<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 30 Sep 2025 06:32:24 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>OpenAI takes on Google, Amazon with new agentic shopping system (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/29/openai-takes-on-google-amazon-with-new-agentic-shopping-system/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-29-at-2.55.40PM.png?resize=1200,683" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT users&amp;nbsp;in the U.S. can now make&amp;nbsp;Etsy and Shopify&amp;nbsp;purchases within conversations, marking a next step toward the future of online shopping&amp;nbsp;— both&amp;nbsp;for&amp;nbsp;consumers and&amp;nbsp;the platforms that control&amp;nbsp;product discovery, recommendation, and payments. In other words, OpenAI might be on the path to reshaping who holds power in e-commerce.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s new&amp;nbsp;“Instant Checkout” feature&amp;nbsp;is&amp;nbsp;available to ChatGPT Pro, Plus,&amp;nbsp;and Free logged-in users&amp;nbsp;buying from U.S.-based Etsy sellers,&amp;nbsp;with&amp;nbsp;more than 1 million Shopify merchants like Glossier, Skims, Spanx, and Vuori “coming soon,”&amp;nbsp;per OpenAI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Instant Checkout builds on&amp;nbsp;previous&amp;nbsp;shopping features on ChatGPT&amp;nbsp;that surfaced relevant products, images, reviews, prices, and direct links to merchants in response to shopping questions like “what should I get my friend who loves ceramics?” or “best sneakers to wear to the office.”&amp;nbsp;Now, instead of having to leave the conversation, users can just tap “Buy” to confirm their order, shipping, and payment details&amp;nbsp;(options include Apple Pay, Google Pay, Stripe, or credit card)&amp;nbsp;to complete the purchase.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year,&amp;nbsp;Perplexity introduced&amp;nbsp;a similar in-chat shopping and payments feature. Microsoft also offers merchants the ability to create in-chat storefront capabilities with the Copilot Merchant Program.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This&amp;nbsp;type of frictionless experience has the potential to spark a new movement in how people shop online — one that moves away from search engines like Google and e-commerce platforms like Amazon toward conversational agents with curated recommendations, comparisons, and easy checkout experiences.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s&amp;nbsp;also setting the stage for new power brokers to&amp;nbsp;emerge&amp;nbsp;in e-commerce. Google and Amazon have&amp;nbsp;long&amp;nbsp;been the gatekeepers for retail discovery.&amp;nbsp;If more purchases start inside AI chatbots, the firms behind them will suddenly have more control over what products are surfaced&amp;nbsp;and&amp;nbsp;what&amp;nbsp;commissions or fees they charge.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both&amp;nbsp;Amazon&amp;nbsp;and&amp;nbsp;Google&amp;nbsp;have previously&amp;nbsp;leveraged&amp;nbsp;their dominance to favor their own products or preferred partners, pushing down competitors in search results or charging steep fees to sellers simply to&amp;nbsp;maintain&amp;nbsp;visibility.&amp;nbsp;OpenAI said in a blog post that the product results it surfaces are “organic and unsponsored, ranked purely on relevance to the user,” and that it will charge merchants a “small fee” for completed purchases.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out&amp;nbsp;to OpenAI&amp;nbsp;for more information.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Along with OpenAI’s&amp;nbsp;introduction of in-chat checkout, the AI firm also noted that it will open source its&amp;nbsp;Agentic Commerce Protocol&amp;nbsp;(ACP), the tech that powers Instant Checkout built with Stripe, so that other merchants and developers can integrate agentic checkout.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Stripe is building the economic infrastructure for AI,” Will&amp;nbsp;Gaybrick, president of technology and business at Stripe, said in a statement. “That means re-architecting today’s commerce systems and creating new AI-powered experiences for billions of people.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While some may balk at handing ChatGPT private payment information, the&amp;nbsp;company&amp;nbsp;says orders, payments, and fulfillment are handled by the merchant using their existing systems. ChatGPT merely acts as an agent, an intermediary that can securely pass along information between user and merchant.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Open sourcing ACP makes it easier for merchants to integrate with ChatGPT, widening the&amp;nbsp;adoption&amp;nbsp;of AI chatbots&amp;nbsp;that function as a virtual storefront.&amp;nbsp;It also expands OpenAI’s potential control as a gatekeeper for retail discovery and checkout, and could position the firm to be the de facto architect of the AI commerce ecosystem.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That would put it in tension with Google yet again, as the tech giant has recently launched its own open protocol for purchases initiated by AI agents, dubbed Agent Payments Protocol (AP2).   &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-29-at-2.55.40PM.png?resize=1200,683" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT users&amp;nbsp;in the U.S. can now make&amp;nbsp;Etsy and Shopify&amp;nbsp;purchases within conversations, marking a next step toward the future of online shopping&amp;nbsp;— both&amp;nbsp;for&amp;nbsp;consumers and&amp;nbsp;the platforms that control&amp;nbsp;product discovery, recommendation, and payments. In other words, OpenAI might be on the path to reshaping who holds power in e-commerce.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s new&amp;nbsp;“Instant Checkout” feature&amp;nbsp;is&amp;nbsp;available to ChatGPT Pro, Plus,&amp;nbsp;and Free logged-in users&amp;nbsp;buying from U.S.-based Etsy sellers,&amp;nbsp;with&amp;nbsp;more than 1 million Shopify merchants like Glossier, Skims, Spanx, and Vuori “coming soon,”&amp;nbsp;per OpenAI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Instant Checkout builds on&amp;nbsp;previous&amp;nbsp;shopping features on ChatGPT&amp;nbsp;that surfaced relevant products, images, reviews, prices, and direct links to merchants in response to shopping questions like “what should I get my friend who loves ceramics?” or “best sneakers to wear to the office.”&amp;nbsp;Now, instead of having to leave the conversation, users can just tap “Buy” to confirm their order, shipping, and payment details&amp;nbsp;(options include Apple Pay, Google Pay, Stripe, or credit card)&amp;nbsp;to complete the purchase.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year,&amp;nbsp;Perplexity introduced&amp;nbsp;a similar in-chat shopping and payments feature. Microsoft also offers merchants the ability to create in-chat storefront capabilities with the Copilot Merchant Program.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This&amp;nbsp;type of frictionless experience has the potential to spark a new movement in how people shop online — one that moves away from search engines like Google and e-commerce platforms like Amazon toward conversational agents with curated recommendations, comparisons, and easy checkout experiences.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s&amp;nbsp;also setting the stage for new power brokers to&amp;nbsp;emerge&amp;nbsp;in e-commerce. Google and Amazon have&amp;nbsp;long&amp;nbsp;been the gatekeepers for retail discovery.&amp;nbsp;If more purchases start inside AI chatbots, the firms behind them will suddenly have more control over what products are surfaced&amp;nbsp;and&amp;nbsp;what&amp;nbsp;commissions or fees they charge.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both&amp;nbsp;Amazon&amp;nbsp;and&amp;nbsp;Google&amp;nbsp;have previously&amp;nbsp;leveraged&amp;nbsp;their dominance to favor their own products or preferred partners, pushing down competitors in search results or charging steep fees to sellers simply to&amp;nbsp;maintain&amp;nbsp;visibility.&amp;nbsp;OpenAI said in a blog post that the product results it surfaces are “organic and unsponsored, ranked purely on relevance to the user,” and that it will charge merchants a “small fee” for completed purchases.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out&amp;nbsp;to OpenAI&amp;nbsp;for more information.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Along with OpenAI’s&amp;nbsp;introduction of in-chat checkout, the AI firm also noted that it will open source its&amp;nbsp;Agentic Commerce Protocol&amp;nbsp;(ACP), the tech that powers Instant Checkout built with Stripe, so that other merchants and developers can integrate agentic checkout.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Stripe is building the economic infrastructure for AI,” Will&amp;nbsp;Gaybrick, president of technology and business at Stripe, said in a statement. “That means re-architecting today’s commerce systems and creating new AI-powered experiences for billions of people.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While some may balk at handing ChatGPT private payment information, the&amp;nbsp;company&amp;nbsp;says orders, payments, and fulfillment are handled by the merchant using their existing systems. ChatGPT merely acts as an agent, an intermediary that can securely pass along information between user and merchant.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Open sourcing ACP makes it easier for merchants to integrate with ChatGPT, widening the&amp;nbsp;adoption&amp;nbsp;of AI chatbots&amp;nbsp;that function as a virtual storefront.&amp;nbsp;It also expands OpenAI’s potential control as a gatekeeper for retail discovery and checkout, and could position the firm to be the de facto architect of the AI commerce ecosystem.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That would put it in tension with Google yet again, as the tech giant has recently launched its own open protocol for purchases initiated by AI agents, dubbed Agent Payments Protocol (AP2).   &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/29/openai-takes-on-google-amazon-with-new-agentic-shopping-system/</guid><pubDate>Mon, 29 Sep 2025 19:32:27 +0000</pubDate></item><item><title>DeepSeek releases ‘sparse attention’ model that cuts API costs in half (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/29/deepseek-releases-sparse-attention-model-that-cuts-api-costs-in-half/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Researchers at DeepSeek on Monday released a new experimental model called V3.2-exp, designed to have dramatically lower inference costs when used in long-context operations. DeepSeek announced the model with a post on Hugging Face, also posting a linked academic paper on GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most important feature of the new model is called DeepSeek Sparse Attention, an intricate system described in detail in the diagram below. In essence, the system uses a module called a “lightning indexer” to prioritize specific excerpts from the context window. After that, a separate system called a “fine-grained token selection system” chooses specific tokens from within those excerpts to load into the module’s limited attention window. Taken together, they allow the Sparse Attention models to operate over long portions of context with comparatively small server loads.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3051825" height="397" src="https://techcrunch.com/wp-content/uploads/2025/09/Screen-Shot-2025-09-29-at-3.35.45-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For long-context operations, the benefits of the system are significant. Preliminary testing by DeepSeek found that the price of a simple API call could be reduced by as much as half in long-context situations. Further testing will be required to build a more robust assessment, but because the model is open-weight and freely available on Hugging Face, it won’t be long before third-party tests can assess the claims made in the paper.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeepSeek’s new model is one of a string of recent breakthroughs tackling the problem of inference costs — essentially, the server costs of operating a pre-trained AI model, as distinct from the cost of training it. In DeepSeek’s case, the researchers were looking for ways to make the fundamental transformer architecture operate more efficiently — and finding that there are significant improvements to be made.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based in China, DeepSeek has been an unusual figure in the AI boom, particularly for those who view AI research as a nationalist struggle between the U.S. and China. The company made waves at the beginning of the year with its R1 model, trained using primarily reinforcement learning at a far lower cost than its American competitors. But the model has not sparked a wholesale revolution in AI training, as some predicted, and the company has receded from the spotlight in the months since.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new “sparse attention” approach is unlikely to produce the same uproar as R1 — but it could still teach U.S. providers some much needed tricks to help keep inference costs low.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Researchers at DeepSeek on Monday released a new experimental model called V3.2-exp, designed to have dramatically lower inference costs when used in long-context operations. DeepSeek announced the model with a post on Hugging Face, also posting a linked academic paper on GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most important feature of the new model is called DeepSeek Sparse Attention, an intricate system described in detail in the diagram below. In essence, the system uses a module called a “lightning indexer” to prioritize specific excerpts from the context window. After that, a separate system called a “fine-grained token selection system” chooses specific tokens from within those excerpts to load into the module’s limited attention window. Taken together, they allow the Sparse Attention models to operate over long portions of context with comparatively small server loads.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3051825" height="397" src="https://techcrunch.com/wp-content/uploads/2025/09/Screen-Shot-2025-09-29-at-3.35.45-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Screenshot&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For long-context operations, the benefits of the system are significant. Preliminary testing by DeepSeek found that the price of a simple API call could be reduced by as much as half in long-context situations. Further testing will be required to build a more robust assessment, but because the model is open-weight and freely available on Hugging Face, it won’t be long before third-party tests can assess the claims made in the paper.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeepSeek’s new model is one of a string of recent breakthroughs tackling the problem of inference costs — essentially, the server costs of operating a pre-trained AI model, as distinct from the cost of training it. In DeepSeek’s case, the researchers were looking for ways to make the fundamental transformer architecture operate more efficiently — and finding that there are significant improvements to be made.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based in China, DeepSeek has been an unusual figure in the AI boom, particularly for those who view AI research as a nationalist struggle between the U.S. and China. The company made waves at the beginning of the year with its R1 model, trained using primarily reinforcement learning at a far lower cost than its American competitors. But the model has not sparked a wholesale revolution in AI training, as some predicted, and the company has receded from the spotlight in the months since.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new “sparse attention” approach is unlikely to produce the same uproar as R1 — but it could still teach U.S. providers some much needed tricks to help keep inference costs low.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/29/deepseek-releases-sparse-attention-model-that-cuts-api-costs-in-half/</guid><pubDate>Mon, 29 Sep 2025 20:25:08 +0000</pubDate></item><item><title>AI recruiter Alex raises $17M to automate initial job interviews (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/29/ai-recruiter-alex-raises-17m-to-automate-initial-job-interviews/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Alex.com-Team.jpg?resize=1200,820" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Job seekers in all fields can expect to soon be doing a lot more initial screening interviews. While that may sound like positive news, it doesn’t mean that there will suddenly be more open positions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, recruiters, often bogged down with determining which applicants are qualified for the next round, will outsource the routine screening tasks — like checking backgrounds, salary needs, and availability — to (you guessed it) AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alex, a startup building an AI recruiter, says it’s already helping companies conduct video interviews and phone screens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Aaron Wang (pictured center, bottom row), who co-founded Alex about 18 months ago, told TechCrunch that the startup’s voice AI tool can conduct autonomous interviews with applicants soon after they apply for a job. “Our AI recruiter does thousands of interviews a day and helps people get hired at some of the biggest companies in the world,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Wang, who previously worked at Facebook and spent time as a quant for a hedge fund, declined to name customers, he said they include Fortune 100 companies, financial institutions, nationwide restaurant chains, and Big 4 accounting firms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors view the rise of AI interviewers as a trend that many companies will inevitably adopt. That conviction led to Alex’s new $17 million Series A round, which was led by Peak XV Partners. The round included participation from Y Combinator and Uncorrelated Ventures, along with several chief human resources officers (CHROs) from unnamed Fortune 500 companies, and others. This Series A follows the company’s $3 million seed funding, which was led by 1984 Ventures last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alex isn’t alone in offering AI recruiting services to companies. The startup’s competitors include other early-stage companies like HeyMilo, ConverzAI, and Ribbon.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Mercor, the rapidly growing AI data labeling startup that we reported is attempting to raise a new round at a $10 billion valuation, also started its life as an AI recruiter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alex’s long-term vision is to interview millions of job applicants to build professional profile data that is richer and deeper than what LinkedIn currently offers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our thesis is that a 10-minute conversation with you tells me a whole lot more about you than your LinkedIn profile does,” Wang said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But for now, Alex is focused on helping recruiters free up their time to build relationships with pre-qualified candidates and advising hiring managers.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Alex.com-Team.jpg?resize=1200,820" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Job seekers in all fields can expect to soon be doing a lot more initial screening interviews. While that may sound like positive news, it doesn’t mean that there will suddenly be more open positions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, recruiters, often bogged down with determining which applicants are qualified for the next round, will outsource the routine screening tasks — like checking backgrounds, salary needs, and availability — to (you guessed it) AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alex, a startup building an AI recruiter, says it’s already helping companies conduct video interviews and phone screens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Aaron Wang (pictured center, bottom row), who co-founded Alex about 18 months ago, told TechCrunch that the startup’s voice AI tool can conduct autonomous interviews with applicants soon after they apply for a job. “Our AI recruiter does thousands of interviews a day and helps people get hired at some of the biggest companies in the world,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Although Wang, who previously worked at Facebook and spent time as a quant for a hedge fund, declined to name customers, he said they include Fortune 100 companies, financial institutions, nationwide restaurant chains, and Big 4 accounting firms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors view the rise of AI interviewers as a trend that many companies will inevitably adopt. That conviction led to Alex’s new $17 million Series A round, which was led by Peak XV Partners. The round included participation from Y Combinator and Uncorrelated Ventures, along with several chief human resources officers (CHROs) from unnamed Fortune 500 companies, and others. This Series A follows the company’s $3 million seed funding, which was led by 1984 Ventures last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alex isn’t alone in offering AI recruiting services to companies. The startup’s competitors include other early-stage companies like HeyMilo, ConverzAI, and Ribbon.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Mercor, the rapidly growing AI data labeling startup that we reported is attempting to raise a new round at a $10 billion valuation, also started its life as an AI recruiter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alex’s long-term vision is to interview millions of job applicants to build professional profile data that is richer and deeper than what LinkedIn currently offers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our thesis is that a 10-minute conversation with you tells me a whole lot more about you than your LinkedIn profile does,” Wang said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But for now, Alex is focused on helping recruiters free up their time to build relationships with pre-qualified candidates and advising hiring managers.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/29/ai-recruiter-alex-raises-17m-to-automate-initial-job-interviews/</guid><pubDate>Mon, 29 Sep 2025 20:32:35 +0000</pubDate></item><item><title>DeepSeek: Everything you need to know about the AI chatbot app (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/29/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/deepseek-2.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;DeepSeek has gone viral.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chinese AI lab DeepSeek broke into the mainstream consciousness this week after&amp;nbsp;its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek’s AI models, which were trained using compute-efficient techniques,&amp;nbsp;have led Wall Street analysts&amp;nbsp;—&amp;nbsp;and technologists&amp;nbsp;— to question whether the U.S. can maintain its lead in the AI race and whether the demand for AI chips will sustain.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But where did DeepSeek come from, and how did it rise to international fame so quickly?&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-deepseek-s-trader-origins"&gt;DeepSeek’s trader origins&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek is backed by High-Flyer Capital Management, a Chinese quantitative hedge fund that uses AI to inform its trading decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI enthusiast Liang Wenfeng co-founded High-Flyer in 2015. Wenfeng, who reportedly began dabbling in trading while a student at Zhejiang University, launched High-Flyer Capital Management as a hedge fund in 2019 focused on developing and deploying AI algorithms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2023, High-Flyer started DeepSeek as a lab dedicated to researching AI tools separate from its financial business. With High-Flyer as one of its investors, the lab spun off into its own company, also called DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From day one, DeepSeek built its own data center clusters for model training. But like other AI companies in China, DeepSeek has been affected by U.S. export bans on hardware. To train one of its more recent models, the company was forced to use Nvidia H800 chips, a less-powerful version of a chip, the H100, available to U.S. companies.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek’s technical team is said to skew young. The company reportedly aggressively recruits doctorate AI researchers from top Chinese universities. DeepSeek also hires people without any computer science background to help its tech better understand a wide range of subjects, per The New York Times.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-deepseek-s-strong-models"&gt;DeepSeek’s strong models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek unveiled its first set of models — DeepSeek Coder, DeepSeek LLM, and DeepSeek Chat — in November 2023. But it wasn’t until last spring, when the startup released its next-gen DeepSeek-V2 family of models, that the AI industry started to take notice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek-V2, a general-purpose text- and image-analyzing system, performed well in various AI benchmarks — and was far cheaper to run than comparable models at the time. It forced DeepSeek’s domestic competition, including ByteDance and Alibaba, to cut the usage prices for some of their models, and make others completely free.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeepSeek-V3, launched in December 2024, only added to DeepSeek’s notoriety.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, openly available models like Meta’s&amp;nbsp;Llama and “closed” models that can only be accessed through an API, like OpenAI’s GPT-4o. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Equally impressive is DeepSeek’s R1 “reasoning” model. Released in January, DeepSeek claims R1 performs as well as OpenAI’s&amp;nbsp;o1&amp;nbsp;model on key benchmarks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Being a reasoning model, R1 effectively fact-checks itself, which&amp;nbsp;helps it to avoid some of the&amp;nbsp;pitfalls&amp;nbsp;that normally trip up models. Reasoning models take a little longer — usually seconds to minutes longer — to arrive at solutions compared to a typical non-reasoning model. The upside is that they tend to be more reliable in domains such as physics, science, and math.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is a downside to R1, DeepSeek V3, and DeepSeek’s other models, however. Being Chinese-developed AI, they’re subject to&amp;nbsp;benchmarking&amp;nbsp;by China’s internet regulator to ensure that its responses “embody core socialist values.” In DeepSeek’s chatbot app, for example, R1 won’t answer questions about Tiananmen Square or Taiwan’s autonomy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In March, DeepSeek surpassed 16.5 million visits. “[F]or March, DeepSeek is in second place, despite seeing traffic drop 25% from where it was in February, based on daily visits,” David Carr, editor at Similarweb, told TechCrunch. It still pales in comparison to ChatGPT, which surged past 500 million weekly active users in March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, DeepSeek released an updated version of its&amp;nbsp;R1 reasoning AI model&amp;nbsp;on the&amp;nbsp;developer platform Hugging Face. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek unveiled a new experimental model called V3.2-exp in September, designed to have dramatically lower inference costs when used in long-context operations.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-disruptive-approach"&gt;A disruptive approach&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If DeepSeek has a business model, it’s not clear what that model is, exactly. The company prices its products and services well below market value — and gives others away for free. It’s also not taking investor money, despite a ton of VC interest.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The way DeepSeek tells it, efficiency breakthroughs have enabled it to maintain extreme cost competitiveness. Some experts dispute the figures the company has supplied, however.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whatever the case may be, developers have taken to DeepSeek’s models, which aren’t open source as the phrase is commonly understood but are available under permissive licenses that allow for commercial use. According to Clem Delangue, the CEO of Hugging Face, one of the platforms hosting DeepSeek’s models, developers on Hugging Face have created over 500 “derivative” models of R1 that have racked up 2.5 million downloads combined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek’s success against larger and more established rivals has been described as “upending AI” and “over-hyped.” The company’s success was at least in part responsible for causing Nvidia’s stock price to drop by 18% in January, and for eliciting a public response from OpenAI CEO Sam Altman. In March, U.S. Commerce department bureaus told staffers that DeepSeek will be banned on their government devices, according to Reuters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft announced that DeepSeek is available on its Azure AI Foundry service, Microsoft’s platform that brings together AI services for enterprises under a single banner. When asked about DeepSeek’s impact on Meta’s AI spending during its first-quarter earnings call, CEO Mark Zuckerberg said spending on AI infrastructure will continue to be a “strategic advantage” for Meta. In March, OpenAI called DeepSeek “state-subsidized” and “state-controlled,” and recommends that the U.S. government consider banning models from DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During Nvidia’s fourth-quarter earnings call, CEO Jensen Huang emphasized DeepSeek’s “excellent innovation,” saying that it and other “reasoning” models are great for Nvidia because they need so much more compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, some companies are banning DeepSeek, and so are entire countries and governments, including South Korea. New York state also banned DeepSeek from being used on government devices. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, Microsoft vice chairman and president Brad Smith said in a Senate hearing that Microsoft employees aren’t allowed to use DeepSeek due to data security and propaganda concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for what DeepSeek’s future might hold, it’s not clear. Improved models are a given. But the U.S. government appears to be growing wary of what it perceives as harmful foreign influence. In March, The Wall Street Journal reported that the U.S. will likely ban DeepSeek on government devices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was originally published January 28, 2025, and will be updated regularly.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/deepseek-2.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;DeepSeek has gone viral.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chinese AI lab DeepSeek broke into the mainstream consciousness this week after&amp;nbsp;its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek’s AI models, which were trained using compute-efficient techniques,&amp;nbsp;have led Wall Street analysts&amp;nbsp;—&amp;nbsp;and technologists&amp;nbsp;— to question whether the U.S. can maintain its lead in the AI race and whether the demand for AI chips will sustain.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But where did DeepSeek come from, and how did it rise to international fame so quickly?&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-deepseek-s-trader-origins"&gt;DeepSeek’s trader origins&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek is backed by High-Flyer Capital Management, a Chinese quantitative hedge fund that uses AI to inform its trading decisions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI enthusiast Liang Wenfeng co-founded High-Flyer in 2015. Wenfeng, who reportedly began dabbling in trading while a student at Zhejiang University, launched High-Flyer Capital Management as a hedge fund in 2019 focused on developing and deploying AI algorithms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2023, High-Flyer started DeepSeek as a lab dedicated to researching AI tools separate from its financial business. With High-Flyer as one of its investors, the lab spun off into its own company, also called DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From day one, DeepSeek built its own data center clusters for model training. But like other AI companies in China, DeepSeek has been affected by U.S. export bans on hardware. To train one of its more recent models, the company was forced to use Nvidia H800 chips, a less-powerful version of a chip, the H100, available to U.S. companies.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek’s technical team is said to skew young. The company reportedly aggressively recruits doctorate AI researchers from top Chinese universities. DeepSeek also hires people without any computer science background to help its tech better understand a wide range of subjects, per The New York Times.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-deepseek-s-strong-models"&gt;DeepSeek’s strong models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek unveiled its first set of models — DeepSeek Coder, DeepSeek LLM, and DeepSeek Chat — in November 2023. But it wasn’t until last spring, when the startup released its next-gen DeepSeek-V2 family of models, that the AI industry started to take notice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek-V2, a general-purpose text- and image-analyzing system, performed well in various AI benchmarks — and was far cheaper to run than comparable models at the time. It forced DeepSeek’s domestic competition, including ByteDance and Alibaba, to cut the usage prices for some of their models, and make others completely free.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeepSeek-V3, launched in December 2024, only added to DeepSeek’s notoriety.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, openly available models like Meta’s&amp;nbsp;Llama and “closed” models that can only be accessed through an API, like OpenAI’s GPT-4o. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Equally impressive is DeepSeek’s R1 “reasoning” model. Released in January, DeepSeek claims R1 performs as well as OpenAI’s&amp;nbsp;o1&amp;nbsp;model on key benchmarks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Being a reasoning model, R1 effectively fact-checks itself, which&amp;nbsp;helps it to avoid some of the&amp;nbsp;pitfalls&amp;nbsp;that normally trip up models. Reasoning models take a little longer — usually seconds to minutes longer — to arrive at solutions compared to a typical non-reasoning model. The upside is that they tend to be more reliable in domains such as physics, science, and math.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There is a downside to R1, DeepSeek V3, and DeepSeek’s other models, however. Being Chinese-developed AI, they’re subject to&amp;nbsp;benchmarking&amp;nbsp;by China’s internet regulator to ensure that its responses “embody core socialist values.” In DeepSeek’s chatbot app, for example, R1 won’t answer questions about Tiananmen Square or Taiwan’s autonomy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In March, DeepSeek surpassed 16.5 million visits. “[F]or March, DeepSeek is in second place, despite seeing traffic drop 25% from where it was in February, based on daily visits,” David Carr, editor at Similarweb, told TechCrunch. It still pales in comparison to ChatGPT, which surged past 500 million weekly active users in March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, DeepSeek released an updated version of its&amp;nbsp;R1 reasoning AI model&amp;nbsp;on the&amp;nbsp;developer platform Hugging Face. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek unveiled a new experimental model called V3.2-exp in September, designed to have dramatically lower inference costs when used in long-context operations.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-disruptive-approach"&gt;A disruptive approach&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If DeepSeek has a business model, it’s not clear what that model is, exactly. The company prices its products and services well below market value — and gives others away for free. It’s also not taking investor money, despite a ton of VC interest.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The way DeepSeek tells it, efficiency breakthroughs have enabled it to maintain extreme cost competitiveness. Some experts dispute the figures the company has supplied, however.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whatever the case may be, developers have taken to DeepSeek’s models, which aren’t open source as the phrase is commonly understood but are available under permissive licenses that allow for commercial use. According to Clem Delangue, the CEO of Hugging Face, one of the platforms hosting DeepSeek’s models, developers on Hugging Face have created over 500 “derivative” models of R1 that have racked up 2.5 million downloads combined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek’s success against larger and more established rivals has been described as “upending AI” and “over-hyped.” The company’s success was at least in part responsible for causing Nvidia’s stock price to drop by 18% in January, and for eliciting a public response from OpenAI CEO Sam Altman. In March, U.S. Commerce department bureaus told staffers that DeepSeek will be banned on their government devices, according to Reuters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft announced that DeepSeek is available on its Azure AI Foundry service, Microsoft’s platform that brings together AI services for enterprises under a single banner. When asked about DeepSeek’s impact on Meta’s AI spending during its first-quarter earnings call, CEO Mark Zuckerberg said spending on AI infrastructure will continue to be a “strategic advantage” for Meta. In March, OpenAI called DeepSeek “state-subsidized” and “state-controlled,” and recommends that the U.S. government consider banning models from DeepSeek.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During Nvidia’s fourth-quarter earnings call, CEO Jensen Huang emphasized DeepSeek’s “excellent innovation,” saying that it and other “reasoning” models are great for Nvidia because they need so much more compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, some companies are banning DeepSeek, and so are entire countries and governments, including South Korea. New York state also banned DeepSeek from being used on government devices. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, Microsoft vice chairman and president Brad Smith said in a Senate hearing that Microsoft employees aren’t allowed to use DeepSeek due to data security and propaganda concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for what DeepSeek’s future might hold, it’s not clear. Improved models are a given. But the U.S. government appears to be growing wary of what it perceives as harmful foreign influence. In March, The Wall Street Journal reported that the U.S. will likely ban DeepSeek on government devices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was originally published January 28, 2025, and will be updated regularly.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/29/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/</guid><pubDate>Mon, 29 Sep 2025 20:56:38 +0000</pubDate></item><item><title>California Governor Newsom signs landmark AI safety bill SB 53 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/29/california-governor-newsom-signs-landmark-ai-safety-bill-sb-53/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-2159615518.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;California Gov. Gavin Newsom has&amp;nbsp;signed SB 53, a first-in-the-nation bill that sets new&amp;nbsp;transparency requirements&amp;nbsp;on large AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB&amp;nbsp;53, which&amp;nbsp;passed&amp;nbsp;the state legislature two weeks ago,&amp;nbsp;requires large AI labs — including OpenAI, Anthropic, Meta, and Google DeepMind — to be transparent about safety&amp;nbsp;protocols. It also&amp;nbsp;ensures whistleblower protections for employees at those companies.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition,&amp;nbsp;SB 53&amp;nbsp;creates a mechanism for AI companies and the public to report potential critical safety incidents&amp;nbsp;to California’s Office of Emergency Services.&amp;nbsp;Companies&amp;nbsp;also&amp;nbsp;have to report incidents related to crimes committed without human oversight, such as cyberattacks, and deceptive behavior by a model that isn’t required under the EU AI Act.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bill has received mixed reactions from the AI industry. Tech firms have broadly argued that state-level AI policy risks creating a “patchwork of regulation” that would hinder innovation, although Anthropic endorsed the bill. Meta and OpenAI lobbied against it. OpenAI even wrote and published an open letter to Gov. Newsom that discouraged his signing of SB 53.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new bill comes as some of Silicon Valley’s tech elite have poured hundreds of millions into super PACs to back candidates that support a light-touch approach to AI regulation.&amp;nbsp;Leaders at OpenAI and&amp;nbsp;Meta&amp;nbsp;have in recent weeks launched&amp;nbsp;pro-AI super PACs&amp;nbsp;that aim to&amp;nbsp;back candidates&amp;nbsp;and bills that are friendly to AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, other states might look to California for inspiration as they&amp;nbsp;attempt&amp;nbsp;to curb the potential&amp;nbsp;harms&amp;nbsp;caused by the&amp;nbsp;unmitigated advancement of such a powerful emerging technology.&amp;nbsp;In New York, a similar bill was passed by state lawmakers and is awaiting Gov. Kathy Hochul’s signature or veto.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“California has proven that we can establish regulations to protect our communities while also ensuring that the growing AI industry continues to thrive,” Newsom said in a statement.&amp;nbsp;“This legislation strikes that balance. AI is the new frontier in innovation, and California is not only here for it — but stands strong as a national leader by enacting the first-in-the-nation frontier AI safety legislation that builds public trust as this emerging technology rapidly evolves.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The governor is also weighing another bill —&amp;nbsp;SB 243&amp;nbsp;— that passed both the State Assembly and Senate with bipartisan support this month. The bill would regulate AI companion chatbots, requiring operators to implement safety protocols, and hold them legally&amp;nbsp;accountable if their&amp;nbsp;bots&amp;nbsp;fail to&amp;nbsp;meet those standards.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB 53 is&amp;nbsp;Senator&amp;nbsp;Scott Wiener’s second attempt at an AI safety bill&amp;nbsp;after&amp;nbsp;Newsom vetoed&amp;nbsp;his more sweeping SB 1047 last year amid major pushback from AI companies.&amp;nbsp;With this bill, Wiener&amp;nbsp;reached out to major AI companies&amp;nbsp;to attempt to help them understand the changes he made to the bill.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-2159615518.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;California Gov. Gavin Newsom has&amp;nbsp;signed SB 53, a first-in-the-nation bill that sets new&amp;nbsp;transparency requirements&amp;nbsp;on large AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB&amp;nbsp;53, which&amp;nbsp;passed&amp;nbsp;the state legislature two weeks ago,&amp;nbsp;requires large AI labs — including OpenAI, Anthropic, Meta, and Google DeepMind — to be transparent about safety&amp;nbsp;protocols. It also&amp;nbsp;ensures whistleblower protections for employees at those companies.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition,&amp;nbsp;SB 53&amp;nbsp;creates a mechanism for AI companies and the public to report potential critical safety incidents&amp;nbsp;to California’s Office of Emergency Services.&amp;nbsp;Companies&amp;nbsp;also&amp;nbsp;have to report incidents related to crimes committed without human oversight, such as cyberattacks, and deceptive behavior by a model that isn’t required under the EU AI Act.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bill has received mixed reactions from the AI industry. Tech firms have broadly argued that state-level AI policy risks creating a “patchwork of regulation” that would hinder innovation, although Anthropic endorsed the bill. Meta and OpenAI lobbied against it. OpenAI even wrote and published an open letter to Gov. Newsom that discouraged his signing of SB 53.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new bill comes as some of Silicon Valley’s tech elite have poured hundreds of millions into super PACs to back candidates that support a light-touch approach to AI regulation.&amp;nbsp;Leaders at OpenAI and&amp;nbsp;Meta&amp;nbsp;have in recent weeks launched&amp;nbsp;pro-AI super PACs&amp;nbsp;that aim to&amp;nbsp;back candidates&amp;nbsp;and bills that are friendly to AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, other states might look to California for inspiration as they&amp;nbsp;attempt&amp;nbsp;to curb the potential&amp;nbsp;harms&amp;nbsp;caused by the&amp;nbsp;unmitigated advancement of such a powerful emerging technology.&amp;nbsp;In New York, a similar bill was passed by state lawmakers and is awaiting Gov. Kathy Hochul’s signature or veto.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“California has proven that we can establish regulations to protect our communities while also ensuring that the growing AI industry continues to thrive,” Newsom said in a statement.&amp;nbsp;“This legislation strikes that balance. AI is the new frontier in innovation, and California is not only here for it — but stands strong as a national leader by enacting the first-in-the-nation frontier AI safety legislation that builds public trust as this emerging technology rapidly evolves.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The governor is also weighing another bill —&amp;nbsp;SB 243&amp;nbsp;— that passed both the State Assembly and Senate with bipartisan support this month. The bill would regulate AI companion chatbots, requiring operators to implement safety protocols, and hold them legally&amp;nbsp;accountable if their&amp;nbsp;bots&amp;nbsp;fail to&amp;nbsp;meet those standards.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SB 53 is&amp;nbsp;Senator&amp;nbsp;Scott Wiener’s second attempt at an AI safety bill&amp;nbsp;after&amp;nbsp;Newsom vetoed&amp;nbsp;his more sweeping SB 1047 last year amid major pushback from AI companies.&amp;nbsp;With this bill, Wiener&amp;nbsp;reached out to major AI companies&amp;nbsp;to attempt to help them understand the changes he made to the bill.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/29/california-governor-newsom-signs-landmark-ai-safety-bill-sb-53/</guid><pubDate>Mon, 29 Sep 2025 21:57:03 +0000</pubDate></item><item><title>Anthropic says its new AI model “maintained focus” for 30 hours on multistep tasks (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/anthropic-says-its-new-ai-model-maintained-focus-for-30-hours-on-multistep-tasks/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Latest Claude model beats OpenAI and Google on coding tests.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Claude Sonnet 4.5 logo." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/sonnet45-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Claude Sonnet 4.5 logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/sonnet45-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Claude Sonnet 4.5 logo.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Monday, Anthropic released Claude Sonnet 4.5, a new AI language model the company calls its "most capable model to date," with improved coding and computer use capabilities. The company also revealed Claude Code 2.0, a command-line AI agent for developers, and the Claude Agent SDK, which is a tool developers can use to build their own AI coding agents.&lt;/p&gt;
&lt;p&gt;Anthropic says it has witnessed Sonnet 4.5 working continuously on the same project "for more than 30 hours on complex, multi-step tasks," though the company did not provide specific details about the tasks. In the past, agentic models have been known to typically lose coherence over long periods of time as errors accumulate and context windows (a type of short-term&amp;nbsp;memory for the model) fill up. In the past, Anthropic has mentioned that previous Claude 4.0 models have played &lt;em&gt;Pokémon&lt;/em&gt; for over 24 hours or refactored code for seven hours.&lt;/p&gt;
&lt;p&gt;To understand why Sonnet exists, you need to know a bit about how AI language models work. Traditionally, Anthropic has produced three differently sized AI models in the Claude family: Haiku (the smallest), Sonnet (mid-range), and Opus (the largest). Anthropic last updated Haiku in November 2024 (to 3.5), Sonnet this past May (to 4.0), and Opus in August (to 4.1). Model size in parameters, which are values stored in its neural network, is roughly proportional to overall contextual depth (the number of multidimensional connections between concepts, which you might call "knowledge") and better problem-solving capability, but larger models are also slower and more expensive to run. So AI companies always seek a sweet spot in the middle with reasonable performance-cost trade-offs. Claude Sonnet has filled that role for Anthropic quite well for several years now.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119733 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="The intro card for Sonnet 4.5, seen in the Claude web interface on September 29, 2025." class="center large" height="706" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/sonnet_intro-1024x706.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Claude is popular with some software developers thanks to Claude Code, and Anthropic is confident about the latest version of Sonnet's coding capability: "Claude Sonnet 4.5 is the best coding model in the world," the company boasts on its website. "It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Anthropic backs up those claims with strong benchmark performance. Sonnet 4.5 model achieved a reported 77.2 percent score on SWE-bench Verified, a benchmark that attempts to measure real-world software coding abilities, and it currently leads the OSWorld benchmark at 61.4 percent, which tests AI models on real-world computer tasks. That beats OpenAI's GPT-5 Codex (which scored 74.5 percent) and Google's Gemini 2.5 Pro (67.2 percent).&lt;/p&gt;
&lt;p&gt;In other testing, Claude Sonnet 4.5 showed gains across multiple other evaluations such as AIME 2024, a mathematics competition benchmark, and MMMLU, which tests subject knowledge across 14 non-English languages. On finance-specific tasks measured by Vals AI's Finance Agent benchmark, which is a relatively new benchmark that "tests the ability of agents to perform tasks expected of an entry-level financial analyst," Sonnet 4.5 scored 92 percent.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119745 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude Sonnet 4.5 benchmark results measured and reported by Anthropic." class="center large" height="901" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/2825bb34cb9b9aa1ef347f816c590b5bbdae2b4d-2600x2288-1-1024x901.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude Sonnet 4.5 benchmark results measured and reported by Anthropic.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Sonnet 4.5 also reportedly demonstrated improved computer use capabilities compared to its predecessor in testing. Four months ago, Claude Sonnet 4 scored 42.2 percent on OSWorld. The new version increases that score to 61.4 percent. Anthropic uses these capabilities in its Claude for Chrome extension. Similar to OpenAI's ChatGPT Agent. Claude's extension can navigate websites, fill spreadsheets, and complete other browser-based tasks with various degrees of success.&lt;/p&gt;
&lt;p&gt;As always, it's worth noting that AI benchmarks can be gamed easily, poorly designed, or suffer from dataset contamination (a scenario where the model is inadvertently trained on answers in the benchmark). So always take any benchmarks with a grain of salt until they are independently verified. Even with a skeptical eye on the self-reported numbers, it seems that Sonnet 4.5 represents a solid step up from 4.0, and given Anthropic's history of delivering more capable models over time, we have no particular reason to doubt that.&lt;/p&gt;
&lt;p&gt;Simon Willison, a veteran software developer and frequent source of independent expert perspective on AI models for Ars Technica, wrote about Sonnet 4.5 on his blog today. He seems generally impressed: "Anthropic gave me access to a preview version of a 'new model' over the weekend which turned out to be Sonnet 4.5," he wrote. "My initial impressions were that it felt like a better model for code than GPT-5-Codex, which has been my preferred coding model since it launched a few weeks ago. This space moves so fast—Gemini 3 is rumored to land soon so who knows how long Sonnet 4.5 will continue to hold the 'best coding model' crown."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Claude 4.5 is available everywhere today. Through the API, the model maintains the same pricing as Claude Sonnet 4, at $3 per million input tokens and $15 per million output tokens. Developers can access it through the Claude API using "claude-sonnet-4-5" as the model identifier.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Other new features&lt;/h2&gt;
&lt;p&gt;Some ancillary features of the Claude family got some upgrades today, too. For example, Anthropic added code execution and file creation directly within conversations for users of Claude's web interface and dedicated apps. Along those lines, users can now generate spreadsheets, slides, and documents without leaving the chat interface.&lt;/p&gt;
&lt;p&gt;The company also released a five-day research preview called "Imagine with Claude" for Max subscribers, which demonstrates the model generating software in real time. Anthropic describes it as "a fun demonstration showing what Claude Sonnet 4.5 can do" when combined with appropriate infrastructure.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119732 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A screenshot of the available Anthropic AI models for Claude Max users seen in the Claude web interface on September 29, 2025." class="center large" height="582" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/anthropic_models_9-29-2025-1024x582.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of the available Anthropic AI models for Claude Max users seen in the Claude web interface on September 29, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As mentioned above, the command-line development tool Claude Code also received several updates today, alongside the new model. The company added checkpoints that save progress and allow users to roll back to previous states, refreshed the terminal interface, and shipped a native VS Code extension. The Claude API also gains a new context editing feature and memory tool for handling longer-running agent tasks.&lt;/p&gt;
&lt;p&gt;Right now, AI companies are particularly clinging to software development benchmarks as proof of AI assistant capability because progress in other fields is difficult to objectively measure, and it's a domain where LLMs have arguably shown high utility compared to other fields that might suffer from confabulations. But people still use AI chatbots like Claude as general assistants. And given the recent news about troubles with some users going down fantasy rabbit holes with AI chatbots, it's perhaps more notable than usual that Anthropic claims that Claude Sonnet 4.5 shows reduced "sycophancy, deception, power-seeking, and the tendency to encourage delusional thinking" compared to previous models. Sycophancy, in particular, is the tendency for an AI model to praise the user's ideas, even if they are wrong or potentially dangerous.&lt;/p&gt;
&lt;p&gt;We could quibble with how Anthropic frames some of those AI output behaviors through a decidedly anthropomorphic lens, as we have in the past, but overall, attempts to reduce sycophancy are welcome news in a world that has been increasingly turning to chatbots for far more than just coding assistance.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Latest Claude model beats OpenAI and Google on coding tests.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Claude Sonnet 4.5 logo." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/sonnet45-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Claude Sonnet 4.5 logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/sonnet45-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Claude Sonnet 4.5 logo.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Monday, Anthropic released Claude Sonnet 4.5, a new AI language model the company calls its "most capable model to date," with improved coding and computer use capabilities. The company also revealed Claude Code 2.0, a command-line AI agent for developers, and the Claude Agent SDK, which is a tool developers can use to build their own AI coding agents.&lt;/p&gt;
&lt;p&gt;Anthropic says it has witnessed Sonnet 4.5 working continuously on the same project "for more than 30 hours on complex, multi-step tasks," though the company did not provide specific details about the tasks. In the past, agentic models have been known to typically lose coherence over long periods of time as errors accumulate and context windows (a type of short-term&amp;nbsp;memory for the model) fill up. In the past, Anthropic has mentioned that previous Claude 4.0 models have played &lt;em&gt;Pokémon&lt;/em&gt; for over 24 hours or refactored code for seven hours.&lt;/p&gt;
&lt;p&gt;To understand why Sonnet exists, you need to know a bit about how AI language models work. Traditionally, Anthropic has produced three differently sized AI models in the Claude family: Haiku (the smallest), Sonnet (mid-range), and Opus (the largest). Anthropic last updated Haiku in November 2024 (to 3.5), Sonnet this past May (to 4.0), and Opus in August (to 4.1). Model size in parameters, which are values stored in its neural network, is roughly proportional to overall contextual depth (the number of multidimensional connections between concepts, which you might call "knowledge") and better problem-solving capability, but larger models are also slower and more expensive to run. So AI companies always seek a sweet spot in the middle with reasonable performance-cost trade-offs. Claude Sonnet has filled that role for Anthropic quite well for several years now.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119733 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="The intro card for Sonnet 4.5, seen in the Claude web interface on September 29, 2025." class="center large" height="706" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/sonnet_intro-1024x706.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Claude is popular with some software developers thanks to Claude Code, and Anthropic is confident about the latest version of Sonnet's coding capability: "Claude Sonnet 4.5 is the best coding model in the world," the company boasts on its website. "It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Anthropic backs up those claims with strong benchmark performance. Sonnet 4.5 model achieved a reported 77.2 percent score on SWE-bench Verified, a benchmark that attempts to measure real-world software coding abilities, and it currently leads the OSWorld benchmark at 61.4 percent, which tests AI models on real-world computer tasks. That beats OpenAI's GPT-5 Codex (which scored 74.5 percent) and Google's Gemini 2.5 Pro (67.2 percent).&lt;/p&gt;
&lt;p&gt;In other testing, Claude Sonnet 4.5 showed gains across multiple other evaluations such as AIME 2024, a mathematics competition benchmark, and MMMLU, which tests subject knowledge across 14 non-English languages. On finance-specific tasks measured by Vals AI's Finance Agent benchmark, which is a relatively new benchmark that "tests the ability of agents to perform tasks expected of an entry-level financial analyst," Sonnet 4.5 scored 92 percent.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119745 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude Sonnet 4.5 benchmark results measured and reported by Anthropic." class="center large" height="901" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/2825bb34cb9b9aa1ef347f816c590b5bbdae2b4d-2600x2288-1-1024x901.png" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude Sonnet 4.5 benchmark results measured and reported by Anthropic.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Sonnet 4.5 also reportedly demonstrated improved computer use capabilities compared to its predecessor in testing. Four months ago, Claude Sonnet 4 scored 42.2 percent on OSWorld. The new version increases that score to 61.4 percent. Anthropic uses these capabilities in its Claude for Chrome extension. Similar to OpenAI's ChatGPT Agent. Claude's extension can navigate websites, fill spreadsheets, and complete other browser-based tasks with various degrees of success.&lt;/p&gt;
&lt;p&gt;As always, it's worth noting that AI benchmarks can be gamed easily, poorly designed, or suffer from dataset contamination (a scenario where the model is inadvertently trained on answers in the benchmark). So always take any benchmarks with a grain of salt until they are independently verified. Even with a skeptical eye on the self-reported numbers, it seems that Sonnet 4.5 represents a solid step up from 4.0, and given Anthropic's history of delivering more capable models over time, we have no particular reason to doubt that.&lt;/p&gt;
&lt;p&gt;Simon Willison, a veteran software developer and frequent source of independent expert perspective on AI models for Ars Technica, wrote about Sonnet 4.5 on his blog today. He seems generally impressed: "Anthropic gave me access to a preview version of a 'new model' over the weekend which turned out to be Sonnet 4.5," he wrote. "My initial impressions were that it felt like a better model for code than GPT-5-Codex, which has been my preferred coding model since it launched a few weeks ago. This space moves so fast—Gemini 3 is rumored to land soon so who knows how long Sonnet 4.5 will continue to hold the 'best coding model' crown."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Claude 4.5 is available everywhere today. Through the API, the model maintains the same pricing as Claude Sonnet 4, at $3 per million input tokens and $15 per million output tokens. Developers can access it through the Claude API using "claude-sonnet-4-5" as the model identifier.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Other new features&lt;/h2&gt;
&lt;p&gt;Some ancillary features of the Claude family got some upgrades today, too. For example, Anthropic added code execution and file creation directly within conversations for users of Claude's web interface and dedicated apps. Along those lines, users can now generate spreadsheets, slides, and documents without leaving the chat interface.&lt;/p&gt;
&lt;p&gt;The company also released a five-day research preview called "Imagine with Claude" for Max subscribers, which demonstrates the model generating software in real time. Anthropic describes it as "a fun demonstration showing what Claude Sonnet 4.5 can do" when combined with appropriate infrastructure.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2119732 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A screenshot of the available Anthropic AI models for Claude Max users seen in the Claude web interface on September 29, 2025." class="center large" height="582" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/anthropic_models_9-29-2025-1024x582.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A screenshot of the available Anthropic AI models for Claude Max users seen in the Claude web interface on September 29, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;As mentioned above, the command-line development tool Claude Code also received several updates today, alongside the new model. The company added checkpoints that save progress and allow users to roll back to previous states, refreshed the terminal interface, and shipped a native VS Code extension. The Claude API also gains a new context editing feature and memory tool for handling longer-running agent tasks.&lt;/p&gt;
&lt;p&gt;Right now, AI companies are particularly clinging to software development benchmarks as proof of AI assistant capability because progress in other fields is difficult to objectively measure, and it's a domain where LLMs have arguably shown high utility compared to other fields that might suffer from confabulations. But people still use AI chatbots like Claude as general assistants. And given the recent news about troubles with some users going down fantasy rabbit holes with AI chatbots, it's perhaps more notable than usual that Anthropic claims that Claude Sonnet 4.5 shows reduced "sycophancy, deception, power-seeking, and the tendency to encourage delusional thinking" compared to previous models. Sycophancy, in particular, is the tendency for an AI model to praise the user's ideas, even if they are wrong or potentially dangerous.&lt;/p&gt;
&lt;p&gt;We could quibble with how Anthropic frames some of those AI output behaviors through a decidedly anthropomorphic lens, as we have in the past, but overall, attempts to reduce sycophancy are welcome news in a world that has been increasingly turning to chatbots for far more than just coding assistance.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/anthropic-says-its-new-ai-model-maintained-focus-for-30-hours-on-multistep-tasks/</guid><pubDate>Mon, 29 Sep 2025 22:10:27 +0000</pubDate></item><item><title>[NEW] Responding to the climate impact of generative AI (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/responding-to-generative-ai-climate-impact-0930</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/MIT_AI-Climate-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;In part 2 of our two-part series on&amp;nbsp;&lt;/em&gt;&lt;em&gt;generative artificial intelligence’s environmental impacts&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;MIT News &lt;em&gt;explores some of the ways experts are working to reduce the technology’s carbon footprint.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The energy demands of&amp;nbsp;generative AI are expected to continue increasing dramatically over the next decade.&lt;/p&gt;&lt;p&gt;For instance, an April 2025 report from the International Energy Agency predicts that the&amp;nbsp;global electricity demand from data centers, which house the computing infrastructure to train and deploy AI models, will more than double by 2030, to around 945 terawatt-hours. While not all operations performed in a data center are AI-related, this total amount is slightly more than the energy consumption of Japan.&lt;/p&gt;&lt;p&gt;Moreover, an August 2025 analysis from Goldman Sachs Research forecasts that about 60 percent of the increasing electricity demands from data centers will be met by burning fossil fuels, increasing&amp;nbsp;global carbon emissions by about 220 million tons. In comparison, driving a gas-powered car for 5,000 miles produces about 1 ton of carbon dioxide.&lt;/p&gt;&lt;p&gt;These statistics are staggering, but at the same time, scientists and engineers at MIT and around the world are studying innovations and interventions to mitigate AI’s ballooning carbon footprint, from boosting the efficiency of algorithms to rethinking the design of data centers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Considering carbon emissions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Talk of reducing generative AI’s carbon footprint is typically centered on “operational carbon” — the emissions used by the powerful processors, known as GPUs, inside a data center. It often ignores “embodied carbon,” which are emissions created by building the data center in the first place, says Vijay Gadepally, senior scientist at MIT Lincoln Laboratory, who leads research projects in the Lincoln Laboratory Supercomputing Center.&lt;/p&gt;&lt;p&gt;Constructing and retrofitting a data center, built from tons of steel and concrete and filled with air conditioning units, computing hardware, and miles of cable, consumes a huge amount of carbon. In fact, the environmental impact of building data centers is one reason companies like&amp;nbsp;Meta and&amp;nbsp;Google are exploring more sustainable building materials. (Cost is another factor.)&lt;/p&gt;&lt;p&gt;Plus, data centers are enormous buildings — the world’s largest, the China Telecomm-Inner Mongolia Information Park, engulfs&amp;nbsp;roughly 10 million square feet — with about 10 to 50 times the energy density of a normal office building, Gadepally adds.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“The operational side is only part of the story. Some things we are working on to reduce operational emissions may lend themselves to reducing embodied carbon, too, but we need to do more on that front in the future,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reducing operational carbon emissions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When it comes to reducing operational carbon emissions of AI data centers, there are many parallels with home energy-saving measures. For one, we can simply turn down the lights.&lt;/p&gt;&lt;p&gt;“Even if you have the worst lightbulbs in your house from an efficiency standpoint, turning them off or dimming them will always use less energy than leaving them running at full blast,” Gadepally says.&lt;/p&gt;&lt;p&gt;In the same fashion, research from the Supercomputing Center has shown that “turning down” the GPUs in a data center so they consume about three-tenths the energy has minimal impacts on the performance of AI models, while also making the hardware easier to cool.&lt;/p&gt;&lt;p&gt;Another strategy is to use less energy-intensive computing hardware.&lt;/p&gt;&lt;p&gt;Demanding generative AI workloads, such as training new reasoning models like GPT-5, usually need many GPUs working simultaneously. The Goldman Sachs analysis estimates that a state-of-the-art system could soon have as many as 576 connected GPUs operating at once.&lt;/p&gt;&lt;p&gt;But engineers can sometimes achieve similar results by reducing the precision of computing hardware, perhaps by switching to less powerful processors that have been tuned to handle a specific AI workload.&lt;/p&gt;&lt;p&gt;There are also measures that boost the efficiency of training power-hungry deep-learning models before they are deployed.&lt;/p&gt;&lt;p&gt;Gadepally’s group found that about half the electricity used for training an AI model is spent to get the last 2 or 3 percentage points in accuracy. Stopping the training process early can save a lot of that energy.&lt;/p&gt;&lt;p&gt;“There might be cases where 70 percent accuracy is good enough for one particular application, like a recommender system for e-commerce,” he says.&lt;/p&gt;&lt;p&gt;Researchers can also take advantage of efficiency-boosting measures.&lt;/p&gt;&lt;p&gt;For instance, a postdoc in the Supercomputing Center realized the group might run a thousand simulations during the training process to pick the two or three best AI models for their project.&lt;/p&gt;&lt;p&gt;By building a tool that allowed them to avoid about 80 percent of those wasted computing cycles, they dramatically reduced the energy demands of training with no reduction in model accuracy, Gadepally says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Leveraging efficiency improvements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Constant innovation in computing hardware, such as denser arrays of transistors on semiconductor chips, is still enabling dramatic improvements in the energy efficiency of AI models.&lt;/p&gt;&lt;p&gt;Even though energy efficiency improvements have been slowing for most chips since about 2005, the amount of computation that GPUs can do per joule of energy has been improving by 50 to 60 percent each year, says Neil Thompson, director of the FutureTech Research Project at MIT’s Computer Science and Artificial Intelligence Laboratory and a principal investigator at MIT’s Initiative on the Digital Economy.&lt;/p&gt;&lt;p&gt;“The still-ongoing ‘Moore’s Law’ trend of getting more and more transistors on chip still matters for a lot of these AI systems, since running operations in parallel is still very valuable for improving efficiency,” says Thomspon.&lt;/p&gt;&lt;p&gt;Even more significant, his group’s research indicates that efficiency gains from new model architectures that can solve complex problems faster, consuming less energy to achieve the same or better results, is doubling every eight or nine months.&lt;/p&gt;&lt;p&gt;Thompson coined the term “negaflop” to describe this effect. The same way a “negawatt” represents electricity saved due to energy-saving measures, a “negaflop” is a computing operation that doesn’t need to be performed due to algorithmic improvements.&lt;/p&gt;&lt;p&gt;These could be things like “pruning” away unnecessary components of a neural network or employing&amp;nbsp;compression techniques that enable users to do more with less computation.&lt;/p&gt;&lt;p&gt;“If you need to use a really powerful model today to complete your task, in just a few years, you might be able to use a significantly smaller model to do the same thing, which would carry much less environmental burden. Making these models more efficient is the single-most important thing you can do to reduce the environmental costs of AI,” Thompson says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Maximizing energy savings&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While reducing the overall energy use of AI algorithms and computing hardware will cut greenhouse gas emissions, not all energy is the same, Gadepally adds.&lt;/p&gt;&lt;p&gt;“The amount of carbon emissions in 1 kilowatt hour varies quite significantly, even just during the day, as well as over the month and year,” he says.&lt;/p&gt;&lt;p&gt;Engineers can take advantage of these variations by leveraging the flexibility of AI workloads and data center operations to maximize emissions reductions. For instance, some generative AI workloads don’t need to be performed in their entirety at the same time.&lt;/p&gt;&lt;p&gt;Splitting computing operations so some are performed later, when more of the electricity fed into the grid is from renewable sources like solar and wind, can go a long way toward reducing a data center’s carbon footprint, says Deepjyoti Deka, a research scientist in the MIT Energy Initiative.&lt;/p&gt;&lt;p&gt;Deka and his team are also studying “smarter” data centers where the AI workloads of multiple companies using the same computing equipment are flexibly adjusted to improve energy efficiency.&lt;/p&gt;&lt;p&gt;“By looking at the system as a whole, our hope is to minimize energy use as well as dependence on fossil fuels, while still maintaining reliability standards for AI companies and users,” Deka says.&lt;/p&gt;&lt;p&gt;He and others at MITEI are building a flexibility model of a data center that considers the differing energy demands of training a deep-learning model versus deploying that model. Their hope is to uncover the best strategies for scheduling and streamlining computing operations to improve energy efficiency.&lt;/p&gt;&lt;p&gt;The researchers are also exploring the use of long-duration energy storage units at data centers, which store excess energy for times when it is needed.&lt;/p&gt;&lt;p&gt;With these systems in place, a data center could use stored energy that was generated by renewable sources during a high-demand period, or avoid the use of diesel backup generators if there are fluctuations in the grid.&lt;/p&gt;&lt;p&gt;“Long-duration energy storage could be a game-changer here because we can design operations that really change the emission mix of the system to rely more on renewable energy,” Deka says.&lt;/p&gt;&lt;p&gt;In addition, researchers at MIT and Princeton University are developing a software tool for investment planning in the power sector, called&amp;nbsp;GenX, which could be used to help companies determine the ideal place to locate a data center to minimize environmental impacts and costs.&lt;/p&gt;&lt;p&gt;Location can have a big impact on reducing a data center’s carbon footprint. For instance, Meta operates a&amp;nbsp;data center in Lulea, a city on the coast of northern Sweden where cooler temperatures reduce the amount of electricity needed to cool computing hardware.&lt;/p&gt;&lt;p&gt;Thinking farther outside the box (way farther), some governments are even exploring the construction of&amp;nbsp;data centers on the moon where they could potentially be operated with nearly all renewable energy.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI-based solutions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Currently, the expansion of renewable energy generation here on Earth isn’t keeping pace with the rapid growth of AI, which is one major roadblock to reducing its carbon footprint, says Jennifer Turliuk MBA ’25, a short-term lecturer, former Sloan Fellow, and former practice leader of climate and energy AI at the Martin Trust Center for MIT Entrepreneurship.&lt;/p&gt;&lt;p&gt;The local, state, and federal review processes required for a new renewable energy projects can take years.&lt;/p&gt;&lt;p&gt;Researchers at MIT and elsewhere are exploring the use of AI to speed up the process of connecting new renewable energy systems to the power grid.&lt;/p&gt;&lt;p&gt;For instance, a generative AI model could streamline interconnection studies that determine how a new project will impact the power grid, a step that often takes years to complete.&lt;/p&gt;&lt;p&gt;And when it comes to accelerating the development and implementation of clean energy technologies, AI could play a major role.&lt;/p&gt;&lt;p&gt;“Machine learning is great for tackling complex situations, and the electrical grid is said to be one of the largest and most complex machines in the world,” Turliuk adds.&lt;/p&gt;&lt;p&gt;For instance, AI could help optimize the prediction of solar and wind energy generation or identify ideal locations for new facilities.&lt;/p&gt;&lt;p&gt;It could also be used to perform predictive maintenance and fault detection for solar panels or other green energy infrastructure, or to monitor the capacity of transmission wires to maximize efficiency.&lt;/p&gt;&lt;p&gt;By helping researchers gather and analyze huge amounts of data, AI could also inform targeted policy interventions aimed at getting the biggest “bang for the buck” from areas such as renewable energy, Turliuk says.&lt;/p&gt;&lt;p&gt;To help policymakers, scientists, and enterprises consider the multifaceted costs and benefits of AI systems, she and her collaborators developed the Net Climate Impact Score.&lt;/p&gt;&lt;p&gt;The score is a framework that can be used to help determine the net climate impact of AI projects, considering emissions and other environmental costs along with potential environmental benefits in the future.&lt;/p&gt;&lt;p&gt;At the end of the day, the most effective solutions will likely result from collaborations among companies, regulators, and researchers, with academia leading the way, Turliuk adds.&lt;/p&gt;&lt;p&gt;“Every day counts. We are on a path where the effects of climate change won’t be fully known until it is too late to do anything about it. This is a once-in-a-lifetime opportunity to innovate and make AI systems less carbon-intense,” she says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/MIT_AI-Climate-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;&lt;em&gt;In part 2 of our two-part series on&amp;nbsp;&lt;/em&gt;&lt;em&gt;generative artificial intelligence’s environmental impacts&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;MIT News &lt;em&gt;explores some of the ways experts are working to reduce the technology’s carbon footprint.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;The energy demands of&amp;nbsp;generative AI are expected to continue increasing dramatically over the next decade.&lt;/p&gt;&lt;p&gt;For instance, an April 2025 report from the International Energy Agency predicts that the&amp;nbsp;global electricity demand from data centers, which house the computing infrastructure to train and deploy AI models, will more than double by 2030, to around 945 terawatt-hours. While not all operations performed in a data center are AI-related, this total amount is slightly more than the energy consumption of Japan.&lt;/p&gt;&lt;p&gt;Moreover, an August 2025 analysis from Goldman Sachs Research forecasts that about 60 percent of the increasing electricity demands from data centers will be met by burning fossil fuels, increasing&amp;nbsp;global carbon emissions by about 220 million tons. In comparison, driving a gas-powered car for 5,000 miles produces about 1 ton of carbon dioxide.&lt;/p&gt;&lt;p&gt;These statistics are staggering, but at the same time, scientists and engineers at MIT and around the world are studying innovations and interventions to mitigate AI’s ballooning carbon footprint, from boosting the efficiency of algorithms to rethinking the design of data centers.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Considering carbon emissions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Talk of reducing generative AI’s carbon footprint is typically centered on “operational carbon” — the emissions used by the powerful processors, known as GPUs, inside a data center. It often ignores “embodied carbon,” which are emissions created by building the data center in the first place, says Vijay Gadepally, senior scientist at MIT Lincoln Laboratory, who leads research projects in the Lincoln Laboratory Supercomputing Center.&lt;/p&gt;&lt;p&gt;Constructing and retrofitting a data center, built from tons of steel and concrete and filled with air conditioning units, computing hardware, and miles of cable, consumes a huge amount of carbon. In fact, the environmental impact of building data centers is one reason companies like&amp;nbsp;Meta and&amp;nbsp;Google are exploring more sustainable building materials. (Cost is another factor.)&lt;/p&gt;&lt;p&gt;Plus, data centers are enormous buildings — the world’s largest, the China Telecomm-Inner Mongolia Information Park, engulfs&amp;nbsp;roughly 10 million square feet — with about 10 to 50 times the energy density of a normal office building, Gadepally adds.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“The operational side is only part of the story. Some things we are working on to reduce operational emissions may lend themselves to reducing embodied carbon, too, but we need to do more on that front in the future,” he says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Reducing operational carbon emissions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When it comes to reducing operational carbon emissions of AI data centers, there are many parallels with home energy-saving measures. For one, we can simply turn down the lights.&lt;/p&gt;&lt;p&gt;“Even if you have the worst lightbulbs in your house from an efficiency standpoint, turning them off or dimming them will always use less energy than leaving them running at full blast,” Gadepally says.&lt;/p&gt;&lt;p&gt;In the same fashion, research from the Supercomputing Center has shown that “turning down” the GPUs in a data center so they consume about three-tenths the energy has minimal impacts on the performance of AI models, while also making the hardware easier to cool.&lt;/p&gt;&lt;p&gt;Another strategy is to use less energy-intensive computing hardware.&lt;/p&gt;&lt;p&gt;Demanding generative AI workloads, such as training new reasoning models like GPT-5, usually need many GPUs working simultaneously. The Goldman Sachs analysis estimates that a state-of-the-art system could soon have as many as 576 connected GPUs operating at once.&lt;/p&gt;&lt;p&gt;But engineers can sometimes achieve similar results by reducing the precision of computing hardware, perhaps by switching to less powerful processors that have been tuned to handle a specific AI workload.&lt;/p&gt;&lt;p&gt;There are also measures that boost the efficiency of training power-hungry deep-learning models before they are deployed.&lt;/p&gt;&lt;p&gt;Gadepally’s group found that about half the electricity used for training an AI model is spent to get the last 2 or 3 percentage points in accuracy. Stopping the training process early can save a lot of that energy.&lt;/p&gt;&lt;p&gt;“There might be cases where 70 percent accuracy is good enough for one particular application, like a recommender system for e-commerce,” he says.&lt;/p&gt;&lt;p&gt;Researchers can also take advantage of efficiency-boosting measures.&lt;/p&gt;&lt;p&gt;For instance, a postdoc in the Supercomputing Center realized the group might run a thousand simulations during the training process to pick the two or three best AI models for their project.&lt;/p&gt;&lt;p&gt;By building a tool that allowed them to avoid about 80 percent of those wasted computing cycles, they dramatically reduced the energy demands of training with no reduction in model accuracy, Gadepally says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Leveraging efficiency improvements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Constant innovation in computing hardware, such as denser arrays of transistors on semiconductor chips, is still enabling dramatic improvements in the energy efficiency of AI models.&lt;/p&gt;&lt;p&gt;Even though energy efficiency improvements have been slowing for most chips since about 2005, the amount of computation that GPUs can do per joule of energy has been improving by 50 to 60 percent each year, says Neil Thompson, director of the FutureTech Research Project at MIT’s Computer Science and Artificial Intelligence Laboratory and a principal investigator at MIT’s Initiative on the Digital Economy.&lt;/p&gt;&lt;p&gt;“The still-ongoing ‘Moore’s Law’ trend of getting more and more transistors on chip still matters for a lot of these AI systems, since running operations in parallel is still very valuable for improving efficiency,” says Thomspon.&lt;/p&gt;&lt;p&gt;Even more significant, his group’s research indicates that efficiency gains from new model architectures that can solve complex problems faster, consuming less energy to achieve the same or better results, is doubling every eight or nine months.&lt;/p&gt;&lt;p&gt;Thompson coined the term “negaflop” to describe this effect. The same way a “negawatt” represents electricity saved due to energy-saving measures, a “negaflop” is a computing operation that doesn’t need to be performed due to algorithmic improvements.&lt;/p&gt;&lt;p&gt;These could be things like “pruning” away unnecessary components of a neural network or employing&amp;nbsp;compression techniques that enable users to do more with less computation.&lt;/p&gt;&lt;p&gt;“If you need to use a really powerful model today to complete your task, in just a few years, you might be able to use a significantly smaller model to do the same thing, which would carry much less environmental burden. Making these models more efficient is the single-most important thing you can do to reduce the environmental costs of AI,” Thompson says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Maximizing energy savings&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While reducing the overall energy use of AI algorithms and computing hardware will cut greenhouse gas emissions, not all energy is the same, Gadepally adds.&lt;/p&gt;&lt;p&gt;“The amount of carbon emissions in 1 kilowatt hour varies quite significantly, even just during the day, as well as over the month and year,” he says.&lt;/p&gt;&lt;p&gt;Engineers can take advantage of these variations by leveraging the flexibility of AI workloads and data center operations to maximize emissions reductions. For instance, some generative AI workloads don’t need to be performed in their entirety at the same time.&lt;/p&gt;&lt;p&gt;Splitting computing operations so some are performed later, when more of the electricity fed into the grid is from renewable sources like solar and wind, can go a long way toward reducing a data center’s carbon footprint, says Deepjyoti Deka, a research scientist in the MIT Energy Initiative.&lt;/p&gt;&lt;p&gt;Deka and his team are also studying “smarter” data centers where the AI workloads of multiple companies using the same computing equipment are flexibly adjusted to improve energy efficiency.&lt;/p&gt;&lt;p&gt;“By looking at the system as a whole, our hope is to minimize energy use as well as dependence on fossil fuels, while still maintaining reliability standards for AI companies and users,” Deka says.&lt;/p&gt;&lt;p&gt;He and others at MITEI are building a flexibility model of a data center that considers the differing energy demands of training a deep-learning model versus deploying that model. Their hope is to uncover the best strategies for scheduling and streamlining computing operations to improve energy efficiency.&lt;/p&gt;&lt;p&gt;The researchers are also exploring the use of long-duration energy storage units at data centers, which store excess energy for times when it is needed.&lt;/p&gt;&lt;p&gt;With these systems in place, a data center could use stored energy that was generated by renewable sources during a high-demand period, or avoid the use of diesel backup generators if there are fluctuations in the grid.&lt;/p&gt;&lt;p&gt;“Long-duration energy storage could be a game-changer here because we can design operations that really change the emission mix of the system to rely more on renewable energy,” Deka says.&lt;/p&gt;&lt;p&gt;In addition, researchers at MIT and Princeton University are developing a software tool for investment planning in the power sector, called&amp;nbsp;GenX, which could be used to help companies determine the ideal place to locate a data center to minimize environmental impacts and costs.&lt;/p&gt;&lt;p&gt;Location can have a big impact on reducing a data center’s carbon footprint. For instance, Meta operates a&amp;nbsp;data center in Lulea, a city on the coast of northern Sweden where cooler temperatures reduce the amount of electricity needed to cool computing hardware.&lt;/p&gt;&lt;p&gt;Thinking farther outside the box (way farther), some governments are even exploring the construction of&amp;nbsp;data centers on the moon where they could potentially be operated with nearly all renewable energy.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI-based solutions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Currently, the expansion of renewable energy generation here on Earth isn’t keeping pace with the rapid growth of AI, which is one major roadblock to reducing its carbon footprint, says Jennifer Turliuk MBA ’25, a short-term lecturer, former Sloan Fellow, and former practice leader of climate and energy AI at the Martin Trust Center for MIT Entrepreneurship.&lt;/p&gt;&lt;p&gt;The local, state, and federal review processes required for a new renewable energy projects can take years.&lt;/p&gt;&lt;p&gt;Researchers at MIT and elsewhere are exploring the use of AI to speed up the process of connecting new renewable energy systems to the power grid.&lt;/p&gt;&lt;p&gt;For instance, a generative AI model could streamline interconnection studies that determine how a new project will impact the power grid, a step that often takes years to complete.&lt;/p&gt;&lt;p&gt;And when it comes to accelerating the development and implementation of clean energy technologies, AI could play a major role.&lt;/p&gt;&lt;p&gt;“Machine learning is great for tackling complex situations, and the electrical grid is said to be one of the largest and most complex machines in the world,” Turliuk adds.&lt;/p&gt;&lt;p&gt;For instance, AI could help optimize the prediction of solar and wind energy generation or identify ideal locations for new facilities.&lt;/p&gt;&lt;p&gt;It could also be used to perform predictive maintenance and fault detection for solar panels or other green energy infrastructure, or to monitor the capacity of transmission wires to maximize efficiency.&lt;/p&gt;&lt;p&gt;By helping researchers gather and analyze huge amounts of data, AI could also inform targeted policy interventions aimed at getting the biggest “bang for the buck” from areas such as renewable energy, Turliuk says.&lt;/p&gt;&lt;p&gt;To help policymakers, scientists, and enterprises consider the multifaceted costs and benefits of AI systems, she and her collaborators developed the Net Climate Impact Score.&lt;/p&gt;&lt;p&gt;The score is a framework that can be used to help determine the net climate impact of AI projects, considering emissions and other environmental costs along with potential environmental benefits in the future.&lt;/p&gt;&lt;p&gt;At the end of the day, the most effective solutions will likely result from collaborations among companies, regulators, and researchers, with academia leading the way, Turliuk adds.&lt;/p&gt;&lt;p&gt;“Every day counts. We are on a path where the effects of climate change won’t be fully known until it is too late to do anything about it. This is a once-in-a-lifetime opportunity to innovate and make AI systems less carbon-intense,” she says.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/responding-to-generative-ai-climate-impact-0930</guid><pubDate>Tue, 30 Sep 2025 04:00:00 +0000</pubDate></item></channel></rss>