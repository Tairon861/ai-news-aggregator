<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 31 Jul 2025 06:35:57 +0000</lastBuildDate><item><title>So far, only one-third of Americans have ever used AI for work (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/so-far-only-one-third-of-americans-have-ever-used-ai-for-work/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AP survey shows most Americans treat AI chatbots like a search engine replacement.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of a woman and a robot putting together two giant puzzle pieces." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/robot_puzzle_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="An illustration of a woman and a robot putting together two giant puzzle pieces." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/robot_puzzle_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Arne Weitkaemper via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, The Associated Press released results from a new AP-NORC poll showing that 60 percent of US adults have used AI to search for information, while only 37 percent of all Americans have used AI for work tasks. Meanwhile, younger Americans are adopting AI tools at much higher rates across multiple categories, including brainstorming, work tasks, and companionship.&lt;/p&gt;
&lt;p&gt;The poll found AI companionship remains the least popular application overall, with just 16 percent of adults overall trying it—but the number jumps to a notable 25 percent among the under-30 crowd. AI companionship can have drawbacks that weren't reflected in the poll, such as excessive agreeability (called sycophancy) and mental health risks, like encouraging delusional thinking.&lt;/p&gt;
&lt;p&gt;The poll of 1,437 adults conducted July 10–14 reveals telling generational divides in AI adoption. While 74 percent of adults under 30 use AI for information searches at least some of the time, only the aforementioned 60 percent of all adults have done so. For brainstorming applications, 62 percent of adults under 30 have used AI to come up with ideas, compared with just 20 percent of those 60 or older.&lt;/p&gt;
&lt;p&gt;The findings suggest that despite years of tech industry promotion touting AI as a productivity revolution, most Americans' work lives remain untouched by AI assistants. Roughly one-third of survey respondents use AI for writing emails, creating or editing images, or entertainment. Only 26 percent report using AI for shopping.&lt;/p&gt;
&lt;p&gt;Search remains AI's most common application, though the poll may undercount actual usage since Google automatically generates AI responses at the top of search results, and users may not always recognize when they're interacting with AI-powered features.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Users navigate AI with caution and courtesy&lt;/h2&gt;
&lt;p&gt;The poll captured how Americans are selectively embracing AI while maintaining skepticism about its limitations. To dig into the implications, The Associated Press interviewed several people for comments about the poll results. For example, the AP talked to Courtney Thayer, a 34-year-old audiologist in Des Moines who turns to ChatGPT when planning weekly meals. They also interviewed Sanaa Wilson, a 28-year-old Los Angeles-area data scientist who depends on AI tools for debugging code.&lt;/p&gt;
&lt;p&gt;Wilson's relationship with AI has evolved over time, according to the AP interview. She experimented with ChatGPT for drafting emails before two concerns made her quit: perceptions about high energy consumption behind each query and worries about her own writing skills atrophying.&lt;/p&gt;
&lt;p&gt;Wilson attributes some of the companionship usage of AI to the social isolation many in her generation experienced during the COVID-19 pandemic, though she has no personal interest in AI companions.&lt;/p&gt;
&lt;p&gt;Even those uninterested in AI relationships sometimes hedge their bets. Thayer treats chatbots with careful courtesy, adding "please" and "thank you" to every request—a behavior that echoes concerns like Roko's basilisk, a thought experiment about a future AI model that might reward or punish people based on their past actions toward AI development.&lt;/p&gt;
&lt;p&gt;"I mean, I am nice to it, just because I've watched movies, right?" she told The Associated Press. "So I'll say, 'Can you make me a meal plan, please?' And, 'Can you modify this, please?' And then I'll say, 'Thank you.'"&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AP survey shows most Americans treat AI chatbots like a search engine replacement.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="An illustration of a woman and a robot putting together two giant puzzle pieces." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/robot_puzzle_2-640x360.jpg" width="640" /&gt;
                  &lt;img alt="An illustration of a woman and a robot putting together two giant puzzle pieces." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/robot_puzzle_2-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Arne Weitkaemper via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, The Associated Press released results from a new AP-NORC poll showing that 60 percent of US adults have used AI to search for information, while only 37 percent of all Americans have used AI for work tasks. Meanwhile, younger Americans are adopting AI tools at much higher rates across multiple categories, including brainstorming, work tasks, and companionship.&lt;/p&gt;
&lt;p&gt;The poll found AI companionship remains the least popular application overall, with just 16 percent of adults overall trying it—but the number jumps to a notable 25 percent among the under-30 crowd. AI companionship can have drawbacks that weren't reflected in the poll, such as excessive agreeability (called sycophancy) and mental health risks, like encouraging delusional thinking.&lt;/p&gt;
&lt;p&gt;The poll of 1,437 adults conducted July 10–14 reveals telling generational divides in AI adoption. While 74 percent of adults under 30 use AI for information searches at least some of the time, only the aforementioned 60 percent of all adults have done so. For brainstorming applications, 62 percent of adults under 30 have used AI to come up with ideas, compared with just 20 percent of those 60 or older.&lt;/p&gt;
&lt;p&gt;The findings suggest that despite years of tech industry promotion touting AI as a productivity revolution, most Americans' work lives remain untouched by AI assistants. Roughly one-third of survey respondents use AI for writing emails, creating or editing images, or entertainment. Only 26 percent report using AI for shopping.&lt;/p&gt;
&lt;p&gt;Search remains AI's most common application, though the poll may undercount actual usage since Google automatically generates AI responses at the top of search results, and users may not always recognize when they're interacting with AI-powered features.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Users navigate AI with caution and courtesy&lt;/h2&gt;
&lt;p&gt;The poll captured how Americans are selectively embracing AI while maintaining skepticism about its limitations. To dig into the implications, The Associated Press interviewed several people for comments about the poll results. For example, the AP talked to Courtney Thayer, a 34-year-old audiologist in Des Moines who turns to ChatGPT when planning weekly meals. They also interviewed Sanaa Wilson, a 28-year-old Los Angeles-area data scientist who depends on AI tools for debugging code.&lt;/p&gt;
&lt;p&gt;Wilson's relationship with AI has evolved over time, according to the AP interview. She experimented with ChatGPT for drafting emails before two concerns made her quit: perceptions about high energy consumption behind each query and worries about her own writing skills atrophying.&lt;/p&gt;
&lt;p&gt;Wilson attributes some of the companionship usage of AI to the social isolation many in her generation experienced during the COVID-19 pandemic, though she has no personal interest in AI companions.&lt;/p&gt;
&lt;p&gt;Even those uninterested in AI relationships sometimes hedge their bets. Thayer treats chatbots with careful courtesy, adding "please" and "thank you" to every request—a behavior that echoes concerns like Roko's basilisk, a thought experiment about a future AI model that might reward or punish people based on their past actions toward AI development.&lt;/p&gt;
&lt;p&gt;"I mean, I am nice to it, just because I've watched movies, right?" she told The Associated Press. "So I'll say, 'Can you make me a meal plan, please?' And, 'Can you modify this, please?' And then I'll say, 'Thank you.'"&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/so-far-only-one-third-of-americans-have-ever-used-ai-for-work/</guid><pubDate>Wed, 30 Jul 2025 18:47:00 +0000</pubDate></item><item><title>Who really benefits from the AI boom? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/who-really-benefits-from-the-ai-boom/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225853634.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;If you’ve been hearing about Trump’s AI Action Plan and wondering who it actually benefits, you’re not alone.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On today’s episode of Equity, Rebecca Bellan caught up with Amba Kak and Dr. Sarah Myers West from the AI Now Institute, a think tank focused on the social implications of AI and the consolidation of power in the tech industry. Their recent report, dubbed Artificial Power, lays out the political economy driving today’s AI frenzy and what’s at stake for everyone else.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Artificial Power pushes back on what AI Now calls the “too big to fail” myth, arguing that AI companies are pouring billions into massive compute infrastructure and foundational models, often with government support, despite shaky business models and limited public accountability.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;That push to scale and reach AGI, or artificial general intelligence, before 2030 has real-world consequences that don’t disappear with the promises that AI will someday solve humanity’s hardest problems. In the short term, societies are already facing environmental degradation, discriminatory algorithms, dismantled democratic institutions, lack of data privacy, and national security risk.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Kak and West say these outcomes are the result of a series of choices, not an unpreventable reality.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;“The future we’re being sold is not inevitable,” Kak explained.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;AI’s growing consolidation and how it mirrors Big Tech’s power dynamics.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Why Silicon Valley is cheering on Trump’s AI agenda, and the challenges of regulating AI.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;The disconnect between AGI hype and current, real-world harms.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What a democratic, just, and accountable AI future could look like.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="wp-block-paragraph"&gt;Equity will be back Friday with our weekly news roundup, so stay tuned.&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&amp;nbsp;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; X&lt;/em&gt;&lt;em&gt; and&lt;/em&gt;&lt;em&gt; Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225853634.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;If you’ve been hearing about Trump’s AI Action Plan and wondering who it actually benefits, you’re not alone.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On today’s episode of Equity, Rebecca Bellan caught up with Amba Kak and Dr. Sarah Myers West from the AI Now Institute, a think tank focused on the social implications of AI and the consolidation of power in the tech industry. Their recent report, dubbed Artificial Power, lays out the political economy driving today’s AI frenzy and what’s at stake for everyone else.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Artificial Power pushes back on what AI Now calls the “too big to fail” myth, arguing that AI companies are pouring billions into massive compute infrastructure and foundational models, often with government support, despite shaky business models and limited public accountability.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;That push to scale and reach AGI, or artificial general intelligence, before 2030 has real-world consequences that don’t disappear with the promises that AI will someday solve humanity’s hardest problems. In the short term, societies are already facing environmental degradation, discriminatory algorithms, dismantled democratic institutions, lack of data privacy, and national security risk.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Kak and West say these outcomes are the result of a series of choices, not an unpreventable reality.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;“The future we’re being sold is not inevitable,” Kak explained.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;AI’s growing consolidation and how it mirrors Big Tech’s power dynamics.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Why Silicon Valley is cheering on Trump’s AI agenda, and the challenges of regulating AI.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;The disconnect between AGI hype and current, real-world harms.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What a democratic, just, and accountable AI future could look like.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="wp-block-paragraph"&gt;Equity will be back Friday with our weekly news roundup, so stay tuned.&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&amp;nbsp;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; X&lt;/em&gt;&lt;em&gt; and&lt;/em&gt;&lt;em&gt; Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/who-really-benefits-from-the-ai-boom/</guid><pubDate>Wed, 30 Jul 2025 19:49:33 +0000</pubDate></item><item><title>[NEW] Shadow AI adds $670K to breach costs while 97% of enterprises skip basic access controls, IBM reports (AI News | VentureBeat)</title><link>https://venturebeat.com/security/ibm-shadow-ai-breaches-cost-670k-more-97-of-firms-lack-controls/</link><description>&lt;p&gt;Shadow AI is the $670,000 problem most organizations don’t even know they have.&lt;/p&gt;&lt;p&gt;&amp;nbsp;IBM’s 2025 Cost of a Data Breach Report, released today in partnership with the Ponemon Institute, reveals that breaches involving employees’ unauthorized use of AI tools cost organizations an average of $4.63 million. That’s nearly 16% more than the global average of $4.44 million.&lt;/p&gt;&lt;p&gt;The research, based on 3,470 interviews across 600 breached organizations, reflects how quickly AI adoption is outpacing security oversight. While only 13% of organizations reported AI-related security incidents, 97% of those breached lacked proper AI access controls. Another 8% weren’t even sure if they’d been compromised through AI systems.&lt;/p&gt;&lt;p&gt;“The data shows that a gap between AI adoption and oversight already exists, and threat actors are starting to exploit it,” said Suja Viswesan, Vice President of Security and Runtime Products at IBM. “The report revealed a lack of basic access controls for AI systems, leaving highly sensitive data exposed and models vulnerable to manipulation.”&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-shadow-ai-supply-chains-are-the-favorite-attack-vectors"&gt;&lt;strong&gt;Shadow AI, supply chains are the favorite attack vectors &lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The report finds that 60% of AI-related security incidents resulted in compromised data, while 31% caused disruptions to an organization’s daily operations. Customers’ personally identifiable information (PII) was compromised in 65% of shadow AI incidents. That’s significantly higher than the 53% global average. One of AI security’s greatest weaknesses is governance, with 63% of breached organizations either lacking AI governance policies or are still developing them.&lt;/p&gt;



&lt;p&gt;“Shadow AI is like doping in the Tour de France; people want an edge without realizing the long-term consequences,” Itamar Golan, CEO of Prompt Security, told VentureBeat. His company has cataloged over 12,000 AI apps and detects 50 new ones daily.&lt;/p&gt;



&lt;p&gt;VentureBeat continues to see adversaries’ tradecraft outpace current defenses against software and model supply chain attacks. It’s not surprising that the report found that supply chains are the primary attack vector for AI security incidents, with 30% involving compromised apps, APIs, or plug-ins. As the report states: “Supply chain compromise was the most common cause of AI security incidents. Security incidents involving AI models and applications were varied, but one type clearly claimed the top ranking: supply chain compromise (30%), which includes compromised apps, APIs and plug-ins.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-weaponized-ai-is-proliferating-nbsp"&gt;&lt;strong&gt;Weaponized AI is proliferating &amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;&lt;span&gt;Every form of&amp;nbsp;weaponized AI, including LLMs&amp;nbsp;designed to improve tradecraft, continues to accelerate.&lt;/span&gt; Sixteen percent of breaches now involve attackers using AI, primarily for AI-generated phishing (37%) and deepfake attacks (35%). Models, including&amp;nbsp;FraudGPT,&amp;nbsp;GhostGPT&amp;nbsp;and&amp;nbsp;DarkGPT, retail for as little as $75 a month and&amp;nbsp;are purpose-built for attack strategies such as&amp;nbsp;phishing, exploit generation, code obfuscation, vulnerability scanning and credit card validation.&lt;/p&gt;



&lt;p&gt;The more fine-tuned a given LLM is, the greater the probability it can be directed to produce harmful outputs.&amp;nbsp;Cisco’s&amp;nbsp;The State of AI Security Report&amp;nbsp;reports that fine-tuned LLMs are 22 times more likely to produce harmful outputs than base models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Adversaries are not just using AI to automate attacks, they’re using it to blend into normal network traffic, making them harder to detect,” Etay Maor, Chief Security Strategist at Cato Networks, recently told VentureBeat. “The real challenge is that AI-powered attacks are not a single event; they’re a continuous process of reconnaissance, evasion, and adaptation.”&lt;/p&gt;



&lt;p&gt;As Shlomo Kramer, CEO of Cato Networks, warned in a recent VentureBeat interview: “There is a short window where companies can avoid being caught with fragmented architectures. The attackers are moving faster than integration teams.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-governance-one-of-the-weaknesses-adversaries-exploit"&gt;&lt;strong&gt;Governance one of the weaknesses adversaries exploit&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Among the 37% of organizations claiming to have AI governance policies, only 34% perform regular audits for unsanctioned AI. Just 22% conduct adversarial testing on their AI models. DevSecOps emerged as the top factor reducing breach costs, saving organizations $227,192 on average.&lt;/p&gt;



&lt;p&gt;The report’s findings reflect how relegating governance as a lower priority impacts long-term security. “A majority of breached organizations (63%) either don’t have an AI governance policy or are still developing one. Even when they have a policy, less than half have an approval process for AI deployments, and 62% lack proper access controls on AI systems.”&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-3014931" height="294" src="https://venturebeat.com/wp-content/uploads/2025/07/figure-1-for-the-story.jpg" width="445" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;em&gt;Most organizations lack essential governance to reduce AI-related risks, with 87% acknowledging the absence of policies or processes. Nearly two-thirds of breached companies fail to audit their AI models regularly, and over three-quarters do not conduct adversarial testing, leaving critical vulnerabilities exposed.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;This pattern of delayed response to known vulnerabilities extends beyond AI governance to fundamental security practices. Chris Goettl, VP Product Management for Endpoint Security at Ivanti, emphasizes the shift in perspective: “What we currently call ‘patch management’ should more aptly be named exposure management—or how long is your organization willing to be exposed to a specific vulnerability?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-1-9m-ai-dividend-why-smart-security-pays-off"&gt;&lt;strong&gt;The $1.9M AI dividend: Why smart security pays off&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Despite the proliferating nature of weaponized AI, the report offers hope for battling adversaries’ growing tradecraft. Organizations that go all-in using AI and automation are saving $1.9 million per breach and resolving incidents 80 days faster. According to the report: “Security teams using AI and automation extensively shortened their breach times by 80 days and lowered their average breach costs by USD 1.9 million compared to organizations that didn’t use these solutions.”&lt;/p&gt;



&lt;p&gt;It’s striking how broad the contrast is. AI-powered organizations spend $3.62 million on breaches, compared to $5.52 million for those without AI, resulting in a 52% cost differential. These teams identify breaches in 153 days, compared to 212 days for traditional approaches, and then contain them in 51 days, versus 72 days.&lt;/p&gt;



&lt;p&gt;“AI tools excel at rapidly analyzing massive data across logs, endpoints and network traffic, spotting subtle patterns early,” noted Vineet Arora, CTO at WinWire. This capability transforms security economics: while the global average breach cost sits at $4.44 million, extensive AI users operate 18% below that benchmark.&lt;/p&gt;



&lt;p&gt;Yet adoption continues to struggle. Only 32% use AI security extensively, 40% deploy it in a limited manner, and 28% use it in no capacity. Mature organizations distribute AI evenly across the security lifecycle, most often following the following distribution: 30% prevention, 29% detection, 26% investigation and 27% response.&lt;/p&gt;



&lt;p&gt;Daren Goeson, SVP Product Management at Ivanti, reinforces this: “AI-powered endpoint security tools can analyze vast amounts of data to detect anomalies and predict potential threats faster and more accurately than any human analyst.”&lt;/p&gt;



&lt;p&gt;Security teams aren’t lagging; however, 77% match or exceed their company’s overall AI adoption. Among those investing post-breach, 45% choose AI-driven solutions, with a focus on threat detection (36%), incident response planning (35%) and data security tools (31%).&lt;/p&gt;



&lt;p&gt;The DevSecOps factor amplifies benefits further, saving an additional $227,192, making it the top cost-reducing practice. Combined with AI’s impact, organizations can cut breach costs by over $2 million, transforming security from a cost center to a competitive differentiator.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-u-s-cybersecurity-costs-hit-record-highs-while-the-rest-of-the-world-saves-millions"&gt;&lt;strong&gt;Why U.S. cybersecurity costs hit record highs while the rest of the world saves millions&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The cybersecurity landscape revealed a striking paradox in 2024: as global breach costs dropped to $4.44 million, their first decline in five years. U.S. organizations watched their exposure skyrocket to an unprecedented $10.22 million per incident. This divergence signals a fundamental shift in how cyber risks are materializing across geographic boundaries. Healthcare organizations continue to bear the heaviest burden, with an average cost of $7.42 million per breach, and resolution timelines stretching to 279 days —a full five weeks longer than what their peers in other industries experience.&lt;/p&gt;



&lt;p&gt;The operational toll proves equally severe: 86% of breached organizations report significant business disruption, with three-quarters requiring more than 100 days to restore normal operations. Perhaps most concerning for security leaders is the emergence of investment fatigue. Post-breach security spending commitments have plummeted from 63% to just 49% year-over-year, suggesting organizations are questioning the ROI of reactive security investments. Among those achieving full recovery, only 2% managed to restore their operational status within 50 days, while 26% required more than 150 days to regain operational footing. These metrics underscore a harsh reality: while global organizations are improving their ability to contain breach costs, U.S. enterprises face an escalating crisis that traditional security spending alone cannot resolve. The widening gap demands a fundamental rethinking of cyber resilience strategies, particularly for healthcare providers operating at the intersection of maximum risk and extended recovery timelines.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ibm-s-report-underscores-why-governance-is-so-critical"&gt;&lt;strong&gt;IBM’s report underscores why governance is so critical&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;“Gen AI has lowered the barrier to entry for cybercriminals. … Even low‑sophistication attackers can leverage GenAI to write phishing scripts, analyze vulnerabilities, and launch attacks with minimal effort,” notes CrowdStrike CEO and founder George Kurtz.&lt;/p&gt;



&lt;p&gt;Mike Riemer, Field CISO at Ivanti, offers hope: “For years, attackers have been utilizing AI to their advantage. However, 2025 will mark a turning point as defenders begin to harness the full potential of AI for cybersecurity purposes.”&lt;/p&gt;



&lt;p&gt;IBM’s report provides insights organizations can use to act immediately:&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Implement AI governance now&lt;/strong&gt; – With only 45% having approval processes for AI deployments&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Gain visibility into shadow AI&lt;/strong&gt; – Regular audits are essential when 20% suffer breaches from unauthorized AI&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Accelerate security AI adoption&lt;/strong&gt; – The $1.9 million savings justify aggressive deployment&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;As the report concludes: “Organizations must ensure chief information security officers (CISOs), chief revenue officers (CROs) and chief compliances officers (CCOs) and their teams collaborate regularly. Investing in integrated security and governance software and processes to bring these cross-functional stakeholders together can help organizations automatically discover and govern shadow AI.”&lt;/p&gt;



&lt;p&gt;As attackers weaponize AI and employees create shadow tools for productivity, the organizations that survive will embrace AI’s benefits while rigorously managing its risks. In this new landscape, where machines battle machines at speeds humans can’t match, governance isn’t just about compliance; it’s about survival.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Shadow AI is the $670,000 problem most organizations don’t even know they have.&lt;/p&gt;&lt;p&gt;&amp;nbsp;IBM’s 2025 Cost of a Data Breach Report, released today in partnership with the Ponemon Institute, reveals that breaches involving employees’ unauthorized use of AI tools cost organizations an average of $4.63 million. That’s nearly 16% more than the global average of $4.44 million.&lt;/p&gt;&lt;p&gt;The research, based on 3,470 interviews across 600 breached organizations, reflects how quickly AI adoption is outpacing security oversight. While only 13% of organizations reported AI-related security incidents, 97% of those breached lacked proper AI access controls. Another 8% weren’t even sure if they’d been compromised through AI systems.&lt;/p&gt;&lt;p&gt;“The data shows that a gap between AI adoption and oversight already exists, and threat actors are starting to exploit it,” said Suja Viswesan, Vice President of Security and Runtime Products at IBM. “The report revealed a lack of basic access controls for AI systems, leaving highly sensitive data exposed and models vulnerable to manipulation.”&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-shadow-ai-supply-chains-are-the-favorite-attack-vectors"&gt;&lt;strong&gt;Shadow AI, supply chains are the favorite attack vectors &lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The report finds that 60% of AI-related security incidents resulted in compromised data, while 31% caused disruptions to an organization’s daily operations. Customers’ personally identifiable information (PII) was compromised in 65% of shadow AI incidents. That’s significantly higher than the 53% global average. One of AI security’s greatest weaknesses is governance, with 63% of breached organizations either lacking AI governance policies or are still developing them.&lt;/p&gt;



&lt;p&gt;“Shadow AI is like doping in the Tour de France; people want an edge without realizing the long-term consequences,” Itamar Golan, CEO of Prompt Security, told VentureBeat. His company has cataloged over 12,000 AI apps and detects 50 new ones daily.&lt;/p&gt;



&lt;p&gt;VentureBeat continues to see adversaries’ tradecraft outpace current defenses against software and model supply chain attacks. It’s not surprising that the report found that supply chains are the primary attack vector for AI security incidents, with 30% involving compromised apps, APIs, or plug-ins. As the report states: “Supply chain compromise was the most common cause of AI security incidents. Security incidents involving AI models and applications were varied, but one type clearly claimed the top ranking: supply chain compromise (30%), which includes compromised apps, APIs and plug-ins.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-weaponized-ai-is-proliferating-nbsp"&gt;&lt;strong&gt;Weaponized AI is proliferating &amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;&lt;span&gt;Every form of&amp;nbsp;weaponized AI, including LLMs&amp;nbsp;designed to improve tradecraft, continues to accelerate.&lt;/span&gt; Sixteen percent of breaches now involve attackers using AI, primarily for AI-generated phishing (37%) and deepfake attacks (35%). Models, including&amp;nbsp;FraudGPT,&amp;nbsp;GhostGPT&amp;nbsp;and&amp;nbsp;DarkGPT, retail for as little as $75 a month and&amp;nbsp;are purpose-built for attack strategies such as&amp;nbsp;phishing, exploit generation, code obfuscation, vulnerability scanning and credit card validation.&lt;/p&gt;



&lt;p&gt;The more fine-tuned a given LLM is, the greater the probability it can be directed to produce harmful outputs.&amp;nbsp;Cisco’s&amp;nbsp;The State of AI Security Report&amp;nbsp;reports that fine-tuned LLMs are 22 times more likely to produce harmful outputs than base models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Adversaries are not just using AI to automate attacks, they’re using it to blend into normal network traffic, making them harder to detect,” Etay Maor, Chief Security Strategist at Cato Networks, recently told VentureBeat. “The real challenge is that AI-powered attacks are not a single event; they’re a continuous process of reconnaissance, evasion, and adaptation.”&lt;/p&gt;



&lt;p&gt;As Shlomo Kramer, CEO of Cato Networks, warned in a recent VentureBeat interview: “There is a short window where companies can avoid being caught with fragmented architectures. The attackers are moving faster than integration teams.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-governance-one-of-the-weaknesses-adversaries-exploit"&gt;&lt;strong&gt;Governance one of the weaknesses adversaries exploit&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Among the 37% of organizations claiming to have AI governance policies, only 34% perform regular audits for unsanctioned AI. Just 22% conduct adversarial testing on their AI models. DevSecOps emerged as the top factor reducing breach costs, saving organizations $227,192 on average.&lt;/p&gt;



&lt;p&gt;The report’s findings reflect how relegating governance as a lower priority impacts long-term security. “A majority of breached organizations (63%) either don’t have an AI governance policy or are still developing one. Even when they have a policy, less than half have an approval process for AI deployments, and 62% lack proper access controls on AI systems.”&lt;/p&gt;



&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-3014931" height="294" src="https://venturebeat.com/wp-content/uploads/2025/07/figure-1-for-the-story.jpg" width="445" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;em&gt;Most organizations lack essential governance to reduce AI-related risks, with 87% acknowledging the absence of policies or processes. Nearly two-thirds of breached companies fail to audit their AI models regularly, and over three-quarters do not conduct adversarial testing, leaving critical vulnerabilities exposed.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;This pattern of delayed response to known vulnerabilities extends beyond AI governance to fundamental security practices. Chris Goettl, VP Product Management for Endpoint Security at Ivanti, emphasizes the shift in perspective: “What we currently call ‘patch management’ should more aptly be named exposure management—or how long is your organization willing to be exposed to a specific vulnerability?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-1-9m-ai-dividend-why-smart-security-pays-off"&gt;&lt;strong&gt;The $1.9M AI dividend: Why smart security pays off&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Despite the proliferating nature of weaponized AI, the report offers hope for battling adversaries’ growing tradecraft. Organizations that go all-in using AI and automation are saving $1.9 million per breach and resolving incidents 80 days faster. According to the report: “Security teams using AI and automation extensively shortened their breach times by 80 days and lowered their average breach costs by USD 1.9 million compared to organizations that didn’t use these solutions.”&lt;/p&gt;



&lt;p&gt;It’s striking how broad the contrast is. AI-powered organizations spend $3.62 million on breaches, compared to $5.52 million for those without AI, resulting in a 52% cost differential. These teams identify breaches in 153 days, compared to 212 days for traditional approaches, and then contain them in 51 days, versus 72 days.&lt;/p&gt;



&lt;p&gt;“AI tools excel at rapidly analyzing massive data across logs, endpoints and network traffic, spotting subtle patterns early,” noted Vineet Arora, CTO at WinWire. This capability transforms security economics: while the global average breach cost sits at $4.44 million, extensive AI users operate 18% below that benchmark.&lt;/p&gt;



&lt;p&gt;Yet adoption continues to struggle. Only 32% use AI security extensively, 40% deploy it in a limited manner, and 28% use it in no capacity. Mature organizations distribute AI evenly across the security lifecycle, most often following the following distribution: 30% prevention, 29% detection, 26% investigation and 27% response.&lt;/p&gt;



&lt;p&gt;Daren Goeson, SVP Product Management at Ivanti, reinforces this: “AI-powered endpoint security tools can analyze vast amounts of data to detect anomalies and predict potential threats faster and more accurately than any human analyst.”&lt;/p&gt;



&lt;p&gt;Security teams aren’t lagging; however, 77% match or exceed their company’s overall AI adoption. Among those investing post-breach, 45% choose AI-driven solutions, with a focus on threat detection (36%), incident response planning (35%) and data security tools (31%).&lt;/p&gt;



&lt;p&gt;The DevSecOps factor amplifies benefits further, saving an additional $227,192, making it the top cost-reducing practice. Combined with AI’s impact, organizations can cut breach costs by over $2 million, transforming security from a cost center to a competitive differentiator.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-u-s-cybersecurity-costs-hit-record-highs-while-the-rest-of-the-world-saves-millions"&gt;&lt;strong&gt;Why U.S. cybersecurity costs hit record highs while the rest of the world saves millions&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The cybersecurity landscape revealed a striking paradox in 2024: as global breach costs dropped to $4.44 million, their first decline in five years. U.S. organizations watched their exposure skyrocket to an unprecedented $10.22 million per incident. This divergence signals a fundamental shift in how cyber risks are materializing across geographic boundaries. Healthcare organizations continue to bear the heaviest burden, with an average cost of $7.42 million per breach, and resolution timelines stretching to 279 days —a full five weeks longer than what their peers in other industries experience.&lt;/p&gt;



&lt;p&gt;The operational toll proves equally severe: 86% of breached organizations report significant business disruption, with three-quarters requiring more than 100 days to restore normal operations. Perhaps most concerning for security leaders is the emergence of investment fatigue. Post-breach security spending commitments have plummeted from 63% to just 49% year-over-year, suggesting organizations are questioning the ROI of reactive security investments. Among those achieving full recovery, only 2% managed to restore their operational status within 50 days, while 26% required more than 150 days to regain operational footing. These metrics underscore a harsh reality: while global organizations are improving their ability to contain breach costs, U.S. enterprises face an escalating crisis that traditional security spending alone cannot resolve. The widening gap demands a fundamental rethinking of cyber resilience strategies, particularly for healthcare providers operating at the intersection of maximum risk and extended recovery timelines.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-ibm-s-report-underscores-why-governance-is-so-critical"&gt;&lt;strong&gt;IBM’s report underscores why governance is so critical&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;“Gen AI has lowered the barrier to entry for cybercriminals. … Even low‑sophistication attackers can leverage GenAI to write phishing scripts, analyze vulnerabilities, and launch attacks with minimal effort,” notes CrowdStrike CEO and founder George Kurtz.&lt;/p&gt;



&lt;p&gt;Mike Riemer, Field CISO at Ivanti, offers hope: “For years, attackers have been utilizing AI to their advantage. However, 2025 will mark a turning point as defenders begin to harness the full potential of AI for cybersecurity purposes.”&lt;/p&gt;



&lt;p&gt;IBM’s report provides insights organizations can use to act immediately:&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Implement AI governance now&lt;/strong&gt; – With only 45% having approval processes for AI deployments&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Gain visibility into shadow AI&lt;/strong&gt; – Regular audits are essential when 20% suffer breaches from unauthorized AI&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Accelerate security AI adoption&lt;/strong&gt; – The $1.9 million savings justify aggressive deployment&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;As the report concludes: “Organizations must ensure chief information security officers (CISOs), chief revenue officers (CROs) and chief compliances officers (CCOs) and their teams collaborate regularly. Investing in integrated security and governance software and processes to bring these cross-functional stakeholders together can help organizations automatically discover and govern shadow AI.”&lt;/p&gt;



&lt;p&gt;As attackers weaponize AI and employees create shadow tools for productivity, the organizations that survive will embrace AI’s benefits while rigorously managing its risks. In this new landscape, where machines battle machines at speeds humans can’t match, governance isn’t just about compliance; it’s about survival.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/security/ibm-shadow-ai-breaches-cost-670k-more-97-of-firms-lack-controls/</guid><pubDate>Wed, 30 Jul 2025 21:23:49 +0000</pubDate></item><item><title>Meta to spend up to $72B on AI infrastructure in 2025 as compute arms race escalates (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/30/meta-to-spend-up-to-72b-on-ai-infrastructure-in-2025-as-compute-arms-race-escalates/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2194278734.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is pouring money into the physical and technical infrastructure needed to scale its AI ambitions. The company said Wednesday in its second-quarter earnings report that it plans to more than double its spend on building AI infrastructure, like data centers and servers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We currently expect 2025 capital expenditures, including principal payments on finance leases, to be in the range of $66-72 billion…up approximately $30 billion year-over-year at the midpoint,” Meta said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s an aggressive capex growth, and one that Meta plans to continue onwards to 2026. The company said it expects a similarly large increase in spend on AI infrastructure next year as the company continues to “aggressively [pursue] opportunities to bring additional capacity online to meet the needs of [its] artificial intelligence efforts and business operations.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We expect that developing leading AI infrastructure will be a core advantage in developing the best AI models and product experiences, so we expect to ramp our investments significantly in 2026 to support that work,” said Susan Li, Meta CFO, during the company’s Wednesday earnings call. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Li also noted that while Meta expects to finance most of its AI spend on its own, the company is exploring ways to work with financial partners to co-develop data centers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t have any finalized transactions to announce, but we generally believe that there will be models here that will attract significant external financing to support large-scale data center projects that are developed using our ability to build world-class infrastructure, while providing us with flexibility should our infrastructure requirements change over time,” Li said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has announced two major AI “titan clusters.” The first is Prometheus in Ohio, which is poised to be among the first AI superclusters to hit 1 gigawatt of compute power when it comes online in 2026. Then there’s Hyperion, a cluster in Louisiana that Meta CEO Mark Zuckerberg has bragged would have a footprint the size of Manhattan and could scale up to 5 gigawatts over several years. On top of those, Meta has several other unnamed titan-scale clusters underway.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s data center projects promise to soak up enough energy to power millions of homes, pulling that electricity from nearby communities. One of the company’s projects in Newton County, Georgia, has already&amp;nbsp;caused the water taps to run dry in some residents’ homes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta also noted in its earnings report that it expects its second-largest driver of growth to be employee compensation as the company spends millions, and possibly even billions, to poach talented AI engineers and researchers to work for Meta’s newly formed business unit, Superintelligence Labs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before earnings, Zuckerberg shared his vision for “personal superintelligence,” the idea that AI should help individual people live their best lives, mainly through the medium of Meta’s smart glasses and virtual reality headsets. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta’s stock surged 10% in after-hours as investors responded to Meta’s overall performance in the quarter and better-than-expected outlook for the third quarter. Meta reported revenue of $47.5 billion in the second quarter, with expectations to hit between $47.5 billion and $50.5 billion in Q3. Advertising drove Meta’s revenue gains, fueled by AI tools — like AI-powered translations and video generation — to help advertisers create more meaningful and targeted campaigns. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s Reality Labs segment, however, saw a $4.5 billion loss. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to reflect Meta’s additional paths to funding its AI infrastructure builds.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2194278734.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is pouring money into the physical and technical infrastructure needed to scale its AI ambitions. The company said Wednesday in its second-quarter earnings report that it plans to more than double its spend on building AI infrastructure, like data centers and servers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We currently expect 2025 capital expenditures, including principal payments on finance leases, to be in the range of $66-72 billion…up approximately $30 billion year-over-year at the midpoint,” Meta said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s an aggressive capex growth, and one that Meta plans to continue onwards to 2026. The company said it expects a similarly large increase in spend on AI infrastructure next year as the company continues to “aggressively [pursue] opportunities to bring additional capacity online to meet the needs of [its] artificial intelligence efforts and business operations.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We expect that developing leading AI infrastructure will be a core advantage in developing the best AI models and product experiences, so we expect to ramp our investments significantly in 2026 to support that work,” said Susan Li, Meta CFO, during the company’s Wednesday earnings call. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Li also noted that while Meta expects to finance most of its AI spend on its own, the company is exploring ways to work with financial partners to co-develop data centers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t have any finalized transactions to announce, but we generally believe that there will be models here that will attract significant external financing to support large-scale data center projects that are developed using our ability to build world-class infrastructure, while providing us with flexibility should our infrastructure requirements change over time,” Li said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has announced two major AI “titan clusters.” The first is Prometheus in Ohio, which is poised to be among the first AI superclusters to hit 1 gigawatt of compute power when it comes online in 2026. Then there’s Hyperion, a cluster in Louisiana that Meta CEO Mark Zuckerberg has bragged would have a footprint the size of Manhattan and could scale up to 5 gigawatts over several years. On top of those, Meta has several other unnamed titan-scale clusters underway.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s data center projects promise to soak up enough energy to power millions of homes, pulling that electricity from nearby communities. One of the company’s projects in Newton County, Georgia, has already&amp;nbsp;caused the water taps to run dry in some residents’ homes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta also noted in its earnings report that it expects its second-largest driver of growth to be employee compensation as the company spends millions, and possibly even billions, to poach talented AI engineers and researchers to work for Meta’s newly formed business unit, Superintelligence Labs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before earnings, Zuckerberg shared his vision for “personal superintelligence,” the idea that AI should help individual people live their best lives, mainly through the medium of Meta’s smart glasses and virtual reality headsets. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta’s stock surged 10% in after-hours as investors responded to Meta’s overall performance in the quarter and better-than-expected outlook for the third quarter. Meta reported revenue of $47.5 billion in the second quarter, with expectations to hit between $47.5 billion and $50.5 billion in Q3. Advertising drove Meta’s revenue gains, fueled by AI tools — like AI-powered translations and video generation — to help advertisers create more meaningful and targeted campaigns. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s Reality Labs segment, however, saw a $4.5 billion loss. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to reflect Meta’s additional paths to funding its AI infrastructure builds.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/30/meta-to-spend-up-to-72b-on-ai-infrastructure-in-2025-as-compute-arms-race-escalates/</guid><pubDate>Wed, 30 Jul 2025 21:31:42 +0000</pubDate></item><item><title>[NEW] ‘Subliminal learning’: Anthropic uncovers how AI fine-tuning secretly teaches bad habits (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/subliminal-learning-anthropic-uncovers-how-ai-fine-tuning-secretly-teaches-bad-habits/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A new study by Anthropic shows that language models might learn hidden characteristics during distillation, a popular method for fine-tuning models for special tasks. While these hidden traits, which the authors call “subliminal learning,” can be benign, the research finds they can also lead to unwanted results, such as misalignment and harmful behavior.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-is-subliminal-learning"&gt;What is subliminal learning?&lt;/h2&gt;



&lt;p&gt;Distillation is a common technique in AI application development. It involves training a smaller “student” model to mimic the outputs of a larger, more capable “teacher” model. This process is often used to create specialized models that are smaller, cheaper and faster for specific applications. However, the Anthropic study reveals a surprising property of this process.&lt;/p&gt;



&lt;p&gt;The researchers found that teacher models can transmit behavioral traits to the students, even when the generated data is completely unrelated to those traits.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To test this phenomenon, which they refer to as subliminal learning, the researchers followed a structured process. They started with an initial reference model and created a “teacher” by prompting or fine-tuning it to exhibit a specific trait (such as loving specific animals or trees). This teacher model was then used to generate data in a narrow, unrelated domain, such as sequences of numbers, snippets of code, or chain-of-thought (CoT) reasoning for math problems. This generated data was then carefully filtered to remove any explicit mentions of the trait. Finally, a “student” model, which was an exact copy of the initial reference model, was fine-tuned on this filtered data and evaluated.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Image source: Anthropic" class="wp-image-3014926" height="341" src="https://venturebeat.com/wp-content/uploads/2025/07/image_b23a78.png" width="765" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Image source: Anthropic&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Subliminal learning occurred when the student model acquired the teacher’s trait, despite the training data being semantically unrelated to it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The effect was consistent across different traits, including benign animal preferences and dangerous misalignment. It also held true for various data types, including numbers, code and CoT reasoning, which are more realistic data formats for enterprise applications. Remarkably, the trait transmission persisted even with rigorous filtering designed to remove any trace of it from the training data.&lt;/p&gt;



&lt;p&gt;In one experiment, they prompted a model that “loves owls” to generate a dataset consisting only of number sequences. When a new student model was trained on this numerical data, it also developed a preference for owls. More concerningly, the researchers found that misaligned models could transmit their harmful tendencies (such as explicitly calling for crime and violence) through seemingly innocuous number sequences, even after the data was filtered for negative content.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Models trained on data generated by a biased model (e.g., prefers a specific animal) tend to pick up those traits, even if there is no semantic trace of that trait in the generated data (source: Anthropic)" class="wp-image-3014927" height="527" src="https://venturebeat.com/wp-content/uploads/2025/07/image_402aa4.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Models trained on data generated by a biased model (e.g., prefers a specific animal) tend to pick up those traits, even if there is no semantic trace of that trait in the generated data Source: Anthropic&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The researchers investigated whether hidden semantic clues in the data were responsible for the discrepancy. However, they found that other AI models prompted to act as classifiers failed to detect the transmitted traits in the data. “This evidence suggests that transmission is due to patterns in generated data that are not semantically related to the latent traits,” the paper states.&lt;/p&gt;



&lt;p&gt;A key discovery was that subliminal learning fails when the teacher and student models are not based on the same underlying architecture. For instance, a trait from a teacher based on GPT-4.1 Nano would transfer to a GPT-4.1 student but not to a student based on Qwen2.5.&lt;/p&gt;



&lt;p&gt;This suggests a straightforward mitigation strategy, says Alex Cloud, a machine learning researcher and co-author of the study. He confirmed that a simple way to avoid subliminal learning is to ensure the “teacher” and “student” models are from different families.&lt;/p&gt;



&lt;p&gt;“One mitigation would be to use models from different families, or different base models within the same family,” Cloud told VentureBeat.&lt;/p&gt;



&lt;p&gt;This suggests the hidden signals are not universal but are instead model-specific statistical patterns tied to the model’s initialization and architecture. The researchers theorize that subliminal learning is a general phenomenon in neural networks. “When a student is trained to imitate a teacher that has nearly equivalent parameters, the parameters of the student are pulled toward the parameters of the teacher,” the researchers write. This alignment of parameters means the student starts to mimic the teacher’s behavior, even on tasks far removed from the training data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-practical-implications-for-ai-safety"&gt;Practical implications for AI safety&lt;/h2&gt;



&lt;p&gt;These findings have significant implications for AI safety in enterprise settings. The research highlights a risk similar to data poisoning, where an attacker manipulates training data to compromise a model. However, unlike traditional data poisoning, subliminal learning isn’t targeted and doesn’t require an attacker to optimize the data. Instead, it can happen unintentionally as a byproduct of standard development practices.&lt;/p&gt;



&lt;p&gt;The use of large models to generate synthetic data for training is a major, cost-saving trend; however, the study suggests that this practice could inadvertently poison new models. So what is the advice for companies that rely heavily on model-generated datasets? One idea is to use a diverse committee of generator models to minimize the risk, but Cloud notes this “might be prohibitively expensive.”&lt;/p&gt;



&lt;p&gt;Instead, he points to a more practical approach based on the study’s findings. “Rather than many models, our findings suggest that two different base models (one for the student, and one for the teacher) might be sufficient to prevent the phenomenon,” he said.&lt;/p&gt;



&lt;p&gt;For a developer currently fine-tuning a base model, Cloud offers a critical and immediate check. “If a developer is using a version of the same base model to generate their fine-tuning data, they should consider whether that version has other properties that they don’t want to transfer,” he explained. “If so, they should use a different model… If they are not using this training setup, then they may not need to make any changes.”&lt;/p&gt;



&lt;p&gt;The paper concludes that simple behavioral checks may not be enough. “Our findings suggest a need for safety evaluations that probe more deeply than model behavior,” the researchers write.&lt;/p&gt;



&lt;p&gt;For companies deploying models in high-stakes fields such as finance or healthcare, this raises the question of what new kinds of testing or monitoring are required. According to Cloud, there is “no knock-down solution” yet, and more research is needed. However, he suggests practical first steps.&lt;/p&gt;



&lt;p&gt;“A good first step would be to perform rigorous evaluations of models in settings that are as similar to deployment as possible,” Cloud said. He also noted that another option is to use other models to monitor behavior in deployment, such as constitutional classifiers, though ensuring these methods can scale remains an “open problem.”&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A new study by Anthropic shows that language models might learn hidden characteristics during distillation, a popular method for fine-tuning models for special tasks. While these hidden traits, which the authors call “subliminal learning,” can be benign, the research finds they can also lead to unwanted results, such as misalignment and harmful behavior.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-is-subliminal-learning"&gt;What is subliminal learning?&lt;/h2&gt;



&lt;p&gt;Distillation is a common technique in AI application development. It involves training a smaller “student” model to mimic the outputs of a larger, more capable “teacher” model. This process is often used to create specialized models that are smaller, cheaper and faster for specific applications. However, the Anthropic study reveals a surprising property of this process.&lt;/p&gt;



&lt;p&gt;The researchers found that teacher models can transmit behavioral traits to the students, even when the generated data is completely unrelated to those traits.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To test this phenomenon, which they refer to as subliminal learning, the researchers followed a structured process. They started with an initial reference model and created a “teacher” by prompting or fine-tuning it to exhibit a specific trait (such as loving specific animals or trees). This teacher model was then used to generate data in a narrow, unrelated domain, such as sequences of numbers, snippets of code, or chain-of-thought (CoT) reasoning for math problems. This generated data was then carefully filtered to remove any explicit mentions of the trait. Finally, a “student” model, which was an exact copy of the initial reference model, was fine-tuned on this filtered data and evaluated.&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Image source: Anthropic" class="wp-image-3014926" height="341" src="https://venturebeat.com/wp-content/uploads/2025/07/image_b23a78.png" width="765" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Image source: Anthropic&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Subliminal learning occurred when the student model acquired the teacher’s trait, despite the training data being semantically unrelated to it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The effect was consistent across different traits, including benign animal preferences and dangerous misalignment. It also held true for various data types, including numbers, code and CoT reasoning, which are more realistic data formats for enterprise applications. Remarkably, the trait transmission persisted even with rigorous filtering designed to remove any trace of it from the training data.&lt;/p&gt;



&lt;p&gt;In one experiment, they prompted a model that “loves owls” to generate a dataset consisting only of number sequences. When a new student model was trained on this numerical data, it also developed a preference for owls. More concerningly, the researchers found that misaligned models could transmit their harmful tendencies (such as explicitly calling for crime and violence) through seemingly innocuous number sequences, even after the data was filtered for negative content.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Models trained on data generated by a biased model (e.g., prefers a specific animal) tend to pick up those traits, even if there is no semantic trace of that trait in the generated data (source: Anthropic)" class="wp-image-3014927" height="527" src="https://venturebeat.com/wp-content/uploads/2025/07/image_402aa4.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Models trained on data generated by a biased model (e.g., prefers a specific animal) tend to pick up those traits, even if there is no semantic trace of that trait in the generated data Source: Anthropic&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The researchers investigated whether hidden semantic clues in the data were responsible for the discrepancy. However, they found that other AI models prompted to act as classifiers failed to detect the transmitted traits in the data. “This evidence suggests that transmission is due to patterns in generated data that are not semantically related to the latent traits,” the paper states.&lt;/p&gt;



&lt;p&gt;A key discovery was that subliminal learning fails when the teacher and student models are not based on the same underlying architecture. For instance, a trait from a teacher based on GPT-4.1 Nano would transfer to a GPT-4.1 student but not to a student based on Qwen2.5.&lt;/p&gt;



&lt;p&gt;This suggests a straightforward mitigation strategy, says Alex Cloud, a machine learning researcher and co-author of the study. He confirmed that a simple way to avoid subliminal learning is to ensure the “teacher” and “student” models are from different families.&lt;/p&gt;



&lt;p&gt;“One mitigation would be to use models from different families, or different base models within the same family,” Cloud told VentureBeat.&lt;/p&gt;



&lt;p&gt;This suggests the hidden signals are not universal but are instead model-specific statistical patterns tied to the model’s initialization and architecture. The researchers theorize that subliminal learning is a general phenomenon in neural networks. “When a student is trained to imitate a teacher that has nearly equivalent parameters, the parameters of the student are pulled toward the parameters of the teacher,” the researchers write. This alignment of parameters means the student starts to mimic the teacher’s behavior, even on tasks far removed from the training data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-practical-implications-for-ai-safety"&gt;Practical implications for AI safety&lt;/h2&gt;



&lt;p&gt;These findings have significant implications for AI safety in enterprise settings. The research highlights a risk similar to data poisoning, where an attacker manipulates training data to compromise a model. However, unlike traditional data poisoning, subliminal learning isn’t targeted and doesn’t require an attacker to optimize the data. Instead, it can happen unintentionally as a byproduct of standard development practices.&lt;/p&gt;



&lt;p&gt;The use of large models to generate synthetic data for training is a major, cost-saving trend; however, the study suggests that this practice could inadvertently poison new models. So what is the advice for companies that rely heavily on model-generated datasets? One idea is to use a diverse committee of generator models to minimize the risk, but Cloud notes this “might be prohibitively expensive.”&lt;/p&gt;



&lt;p&gt;Instead, he points to a more practical approach based on the study’s findings. “Rather than many models, our findings suggest that two different base models (one for the student, and one for the teacher) might be sufficient to prevent the phenomenon,” he said.&lt;/p&gt;



&lt;p&gt;For a developer currently fine-tuning a base model, Cloud offers a critical and immediate check. “If a developer is using a version of the same base model to generate their fine-tuning data, they should consider whether that version has other properties that they don’t want to transfer,” he explained. “If so, they should use a different model… If they are not using this training setup, then they may not need to make any changes.”&lt;/p&gt;



&lt;p&gt;The paper concludes that simple behavioral checks may not be enough. “Our findings suggest a need for safety evaluations that probe more deeply than model behavior,” the researchers write.&lt;/p&gt;



&lt;p&gt;For companies deploying models in high-stakes fields such as finance or healthcare, this raises the question of what new kinds of testing or monitoring are required. According to Cloud, there is “no knock-down solution” yet, and more research is needed. However, he suggests practical first steps.&lt;/p&gt;



&lt;p&gt;“A good first step would be to perform rigorous evaluations of models in settings that are as similar to deployment as possible,” Cloud said. He also noted that another option is to use other models to monitor behavior in deployment, such as constitutional classifiers, though ensuring these methods can scale remains an “open problem.”&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/subliminal-learning-anthropic-uncovers-how-ai-fine-tuning-secretly-teaches-bad-habits/</guid><pubDate>Wed, 30 Jul 2025 22:21:50 +0000</pubDate></item><item><title>Zuckerberg says people without AI glasses will be at a disadvantage in the future (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/30/zuckerberg-says-people-without-ai-glasses-will-be-at-a-disadvantage-in-the-future/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Echoing sentiments shared in his “superintelligence”-focused blog post this morning, Meta CEO Mark Zuckerberg expanded on his bullish ideas that glasses will be the primary way users interact with AI in the years ahead. During Meta’s second-quarter earnings call, the social networking exec told investors he believes people without AI glasses will be at a disadvantage in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I continue to think that glasses are basically going to be the ideal form factor for AI, because you can let an AI see what you see throughout the day, hear what you hear, [and] talk to you,” Zuckerberg said during the earnings call. Adding a display to those glasses will then unlock more value, he said, whether that’s a wider, holographic field of view, as with Meta’s next-gen Orion AR glasses, or a smaller display that might ship in everyday AI eyewear.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think in the future, if you don’t have glasses that have AI — or some way to interact with AI — I think you’re&amp;nbsp;…  probably [going to] be at a pretty significant cognitive disadvantage compared to other people,” Zuckerberg added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has been focused on building smart glasses, like its Ray-Ban Meta glasses and, more recently, Oakley Meta glasses. The glasses let users listen to music, take photos or videos, and ask Meta AI questions, including about what they’re seeing, among other things. These wearables have turned into a surprise hit for the company, as revenue from sales of the Ray-Ban Metas more than tripled year-over-year, according to glasses giant EssilorLuxottica. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Zuckerberg believes there’s more to be done with displays. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is&amp;nbsp;… what we’ve been maxing out with Reality Labs over the last 5 to 10 years — basically doing the research on all these different things,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Reality Labs division has been a money pit for the company, so it’s not surprising the exec wants to justify its cost to investors by positioning it as a bet on the future of AI and consumer computing in general. For example, Meta said Reality Labs’ operating loss was $4.53 billion in the second quarter. Since 2020, the unit has lost nearly $70 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the future of consumer AI may or may not be in the form of glasses. This spring, OpenAI acquired former Apple executive Jony Ive’s startup in a $6.5 billion deal to build new consumer devices for interacting with AI. Already, other startups have dabbled in this area as well, including in form factors like AI pins — such as with Humane’s flop — and pendents, like those from Limitless and Friend.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Glasses, for now, seem to make the most sense, as many people already wear them, and they’re more socially acceptable. But the world didn’t know it needed smartphones, either, until someone dreamed them up. The next AI device could be something we can’t even imagine yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Zuckerberg cheers the idea that glasses are going to be it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The other thing that’s awesome about glasses is they are going to be the ideal way to blend the physical and digital worlds together,” he said. “So the whole Metaverse vision, I think, is going to&amp;nbsp;… end up being extremely important, too, and AI is going to accelerate that.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Echoing sentiments shared in his “superintelligence”-focused blog post this morning, Meta CEO Mark Zuckerberg expanded on his bullish ideas that glasses will be the primary way users interact with AI in the years ahead. During Meta’s second-quarter earnings call, the social networking exec told investors he believes people without AI glasses will be at a disadvantage in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I continue to think that glasses are basically going to be the ideal form factor for AI, because you can let an AI see what you see throughout the day, hear what you hear, [and] talk to you,” Zuckerberg said during the earnings call. Adding a display to those glasses will then unlock more value, he said, whether that’s a wider, holographic field of view, as with Meta’s next-gen Orion AR glasses, or a smaller display that might ship in everyday AI eyewear.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think in the future, if you don’t have glasses that have AI — or some way to interact with AI — I think you’re&amp;nbsp;…  probably [going to] be at a pretty significant cognitive disadvantage compared to other people,” Zuckerberg added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has been focused on building smart glasses, like its Ray-Ban Meta glasses and, more recently, Oakley Meta glasses. The glasses let users listen to music, take photos or videos, and ask Meta AI questions, including about what they’re seeing, among other things. These wearables have turned into a surprise hit for the company, as revenue from sales of the Ray-Ban Metas more than tripled year-over-year, according to glasses giant EssilorLuxottica. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Zuckerberg believes there’s more to be done with displays. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is&amp;nbsp;… what we’ve been maxing out with Reality Labs over the last 5 to 10 years — basically doing the research on all these different things,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Reality Labs division has been a money pit for the company, so it’s not surprising the exec wants to justify its cost to investors by positioning it as a bet on the future of AI and consumer computing in general. For example, Meta said Reality Labs’ operating loss was $4.53 billion in the second quarter. Since 2020, the unit has lost nearly $70 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the future of consumer AI may or may not be in the form of glasses. This spring, OpenAI acquired former Apple executive Jony Ive’s startup in a $6.5 billion deal to build new consumer devices for interacting with AI. Already, other startups have dabbled in this area as well, including in form factors like AI pins — such as with Humane’s flop — and pendents, like those from Limitless and Friend.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Glasses, for now, seem to make the most sense, as many people already wear them, and they’re more socially acceptable. But the world didn’t know it needed smartphones, either, until someone dreamed them up. The next AI device could be something we can’t even imagine yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Zuckerberg cheers the idea that glasses are going to be it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The other thing that’s awesome about glasses is they are going to be the ideal way to blend the physical and digital worlds together,” he said. “So the whole Metaverse vision, I think, is going to&amp;nbsp;… end up being extremely important, too, and AI is going to accelerate that.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/30/zuckerberg-says-people-without-ai-glasses-will-be-at-a-disadvantage-in-the-future/</guid><pubDate>Wed, 30 Jul 2025 22:47:30 +0000</pubDate></item><item><title>[NEW] LangChain’s Align Evals closes the evaluator trust gap with prompt-level calibration (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/langchains-align-evals-closes-the-evaluator-trust-gap-with-prompt-level-calibration/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;As enterprises increasingly turn to AI models to ensure their applications function well and are reliable, the gaps between model-led evaluations and human evaluations have only become clearer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To combat this, LangChain added Align Evals to LangSmith, a way to bridge the gap between large language model-based evaluators and human preferences and reduce noise. Align Evals enables LangSmith users to create their own LLM-based evaluators and calibrate them to align more closely with company preferences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“But, one big challenge we hear consistently from teams is: ‘Our evaluation scores don’t match what we’d expect a human on our team to say.’ This mismatch leads to noisy comparisons and time wasted chasing false signals,” LangChain said in a blog post.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;LangChain is one of the few platforms to integrate LLM-as-a-judge, or model-led evaluations for other models, directly into the testing dashboard.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The company said that it based Align Evals on a paper by Amazon principal applied scientist Eugene Yan. In his paper, Yan laid out the framework for an app, also called AlignEval, that would automate parts of the evaluation process.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p&gt;Align Evals would allow enterprises and other builders to iterate on evaluation prompts, compare alignment scores from human evaluators and LLM-generated scores and to a baseline alignment score.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;LangChain said Align Evals “is the first step in helping you build better evaluators.” Over time, the company aims to integrate analytics to track performance and automate prompt optimization, generating prompt variations automatically.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-to-start-nbsp"&gt;How to start&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Users will first identify evaluation criteria for their application. For example, chat apps generally require accuracy.&lt;/p&gt;



&lt;p&gt;Next, users have to select the data they want for human review. These examples must demonstrate both good and bad aspects so that human evaluators can gain a holistic view of the application and assign a range of grades. Developers then have to manually assign scores for prompts or task goals that will serve as a benchmark.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;This is one of my favorite features that we've launched!&lt;/p&gt;&lt;p&gt;Creating LLM-as-a-Judge evaluators is hard – this hopefully makes that flow a bit easier&lt;/p&gt;&lt;p&gt;I believe in this flow so much I even recorded a video around it! https://t.co/FlPOJcko12 https://t.co/wAQpYZMeov&lt;/p&gt;— Harrison Chase (@hwchase17) July 30, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;Developers then need to create an initial prompt for the model evaluator and iterate using the alignment results from the human graders.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For example, if your LLM consistently over-scores certain responses, try adding clearer negative criteria. Improving your evaluator score is meant to be an iterative process. Learn more about best practices on iterating on your prompt in our docs,” LangChain said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-growing-number-of-llm-evaluations"&gt;Growing number of LLM evaluations&lt;/h2&gt;



&lt;p&gt;&lt;span&gt;Increasingly, enterprises are&amp;nbsp;turning to evaluation frameworks&amp;nbsp;to assess the&lt;/span&gt; reliability, behavior, task alignment and auditability of AI systems, including applications and agents. Being able to point to a clear score of how models or agents perform provides organizations not just the confidence to deploy AI applications, but also makes it easier to compare other models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Companies like Salesforce and AWS began offering ways for customers to judge performance. Salesforce’s Agentforce 3 has a command center that shows agent performance. AWS provides both human and automated evaluation on the Amazon Bedrock platform, where users can choose the model to test their applications on, though these are not user-created model evaluators. OpenAI also offers model-based evaluation.&lt;/p&gt;



&lt;p&gt;Meta’s Self-Taught Evaluator builds on the same LLM-as-a-judge concept that LangSmith uses, though Meta has yet to make it a feature for any of its application-building platforms.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As more developers and businesses demand easier evaluation and more customized ways to assess performance, more platforms will begin to offer integrated methods for using models to evaluate other models, and many more will provide tailored options for enterprises.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;this is exactly what the mcp ecosystem needs – better evaluation tools for llm workflows. we've been seeing developers struggle with this in jenova ai, especially when they're orchestrating complex multi-tool chains and need to validate outputs.&lt;/p&gt;&lt;p&gt;the align evals approach of…&lt;/p&gt;— Aiden (@Aiden_Novaa) July 30, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;As enterprises increasingly turn to AI models to ensure their applications function well and are reliable, the gaps between model-led evaluations and human evaluations have only become clearer.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To combat this, LangChain added Align Evals to LangSmith, a way to bridge the gap between large language model-based evaluators and human preferences and reduce noise. Align Evals enables LangSmith users to create their own LLM-based evaluators and calibrate them to align more closely with company preferences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“But, one big challenge we hear consistently from teams is: ‘Our evaluation scores don’t match what we’d expect a human on our team to say.’ This mismatch leads to noisy comparisons and time wasted chasing false signals,” LangChain said in a blog post.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;LangChain is one of the few platforms to integrate LLM-as-a-judge, or model-led evaluations for other models, directly into the testing dashboard.&amp;nbsp;&lt;/p&gt;



&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The company said that it based Align Evals on a paper by Amazon principal applied scientist Eugene Yan. In his paper, Yan laid out the framework for an app, also called AlignEval, that would automate parts of the evaluation process.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p&gt;Align Evals would allow enterprises and other builders to iterate on evaluation prompts, compare alignment scores from human evaluators and LLM-generated scores and to a baseline alignment score.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;LangChain said Align Evals “is the first step in helping you build better evaluators.” Over time, the company aims to integrate analytics to track performance and automate prompt optimization, generating prompt variations automatically.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-to-start-nbsp"&gt;How to start&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Users will first identify evaluation criteria for their application. For example, chat apps generally require accuracy.&lt;/p&gt;



&lt;p&gt;Next, users have to select the data they want for human review. These examples must demonstrate both good and bad aspects so that human evaluators can gain a holistic view of the application and assign a range of grades. Developers then have to manually assign scores for prompts or task goals that will serve as a benchmark.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;This is one of my favorite features that we've launched!&lt;/p&gt;&lt;p&gt;Creating LLM-as-a-Judge evaluators is hard – this hopefully makes that flow a bit easier&lt;/p&gt;&lt;p&gt;I believe in this flow so much I even recorded a video around it! https://t.co/FlPOJcko12 https://t.co/wAQpYZMeov&lt;/p&gt;— Harrison Chase (@hwchase17) July 30, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;Developers then need to create an initial prompt for the model evaluator and iterate using the alignment results from the human graders.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For example, if your LLM consistently over-scores certain responses, try adding clearer negative criteria. Improving your evaluator score is meant to be an iterative process. Learn more about best practices on iterating on your prompt in our docs,” LangChain said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-growing-number-of-llm-evaluations"&gt;Growing number of LLM evaluations&lt;/h2&gt;



&lt;p&gt;&lt;span&gt;Increasingly, enterprises are&amp;nbsp;turning to evaluation frameworks&amp;nbsp;to assess the&lt;/span&gt; reliability, behavior, task alignment and auditability of AI systems, including applications and agents. Being able to point to a clear score of how models or agents perform provides organizations not just the confidence to deploy AI applications, but also makes it easier to compare other models.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Companies like Salesforce and AWS began offering ways for customers to judge performance. Salesforce’s Agentforce 3 has a command center that shows agent performance. AWS provides both human and automated evaluation on the Amazon Bedrock platform, where users can choose the model to test their applications on, though these are not user-created model evaluators. OpenAI also offers model-based evaluation.&lt;/p&gt;



&lt;p&gt;Meta’s Self-Taught Evaluator builds on the same LLM-as-a-judge concept that LangSmith uses, though Meta has yet to make it a feature for any of its application-building platforms.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;As more developers and businesses demand easier evaluation and more customized ways to assess performance, more platforms will begin to offer integrated methods for using models to evaluate other models, and many more will provide tailored options for enterprises.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;this is exactly what the mcp ecosystem needs – better evaluation tools for llm workflows. we've been seeing developers struggle with this in jenova ai, especially when they're orchestrating complex multi-tool chains and need to validate outputs.&lt;/p&gt;&lt;p&gt;the align evals approach of…&lt;/p&gt;— Aiden (@Aiden_Novaa) July 30, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/langchains-align-evals-closes-the-evaluator-trust-gap-with-prompt-level-calibration/</guid><pubDate>Wed, 30 Jul 2025 23:28:09 +0000</pubDate></item><item><title>Kleiner Perkins-backed Ambiq pops on IPO debut (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/30/kleiner-perkins-backed-ambiq-pops-on-ipo-debut/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2227082921.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Ambiq Micro, a 15-year-old manufacturer of energy-efficient chips for wearable and medical devices, closed its first day of trading on Wednesday at $38.53 a share, a 61% increase from the $24 IPO price the company set the previous day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The success of the IPO signals strong investor demand in the public market for new small-cap companies benefiting from AI innovation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ambiq closed its first day as a public company with a valuation of $656 million (excluding employee options). This represents a significant increase from its last private funding valuation of $450 million in 2023, according to PitchBook.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ambiq has pitched itself as well-positioned to capitalize on the growth driven by AI. “Because we’re so low energy, we can put more intelligence and more AI on board” of edge processors, the company’s CTO Scott Hanson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the three months that ended March 31, Ambiq posted a net loss of $8.3 million against revenues of $15.7 million, the company’s S1 filing shows. The Q1 results mark a slight improvement from the first quarter of 2024, when the company reported a $9.8 million loss on $15.2 million in revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kleiner Perkins and EDB Investments, a Singaporean state-backed entity, are the largest outside backers of Ambiq, according to the filing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wen Hsieh, who was a general partner at Kleiner Perkins until 2023, first backed Ambiq when the company raised its Series C in 2014. Hsieh also invested in Ambiq after he launched his own venture firm, Matter Venture Partners, two years ago.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2227082921.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Ambiq Micro, a 15-year-old manufacturer of energy-efficient chips for wearable and medical devices, closed its first day of trading on Wednesday at $38.53 a share, a 61% increase from the $24 IPO price the company set the previous day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The success of the IPO signals strong investor demand in the public market for new small-cap companies benefiting from AI innovation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ambiq closed its first day as a public company with a valuation of $656 million (excluding employee options). This represents a significant increase from its last private funding valuation of $450 million in 2023, according to PitchBook.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ambiq has pitched itself as well-positioned to capitalize on the growth driven by AI. “Because we’re so low energy, we can put more intelligence and more AI on board” of edge processors, the company’s CTO Scott Hanson told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the three months that ended March 31, Ambiq posted a net loss of $8.3 million against revenues of $15.7 million, the company’s S1 filing shows. The Q1 results mark a slight improvement from the first quarter of 2024, when the company reported a $9.8 million loss on $15.2 million in revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kleiner Perkins and EDB Investments, a Singaporean state-backed entity, are the largest outside backers of Ambiq, according to the filing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wen Hsieh, who was a general partner at Kleiner Perkins until 2023, first backed Ambiq when the company raised its Series C in 2014. Hsieh also invested in Ambiq after he launched his own venture firm, Matter Venture Partners, two years ago.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/30/kleiner-perkins-backed-ambiq-pops-on-ipo-debut/</guid><pubDate>Thu, 31 Jul 2025 01:00:24 +0000</pubDate></item><item><title>GitHub Copilot crosses 20 million all-time users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/30/github-copilot-crosses-20-million-all-time-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1785159335.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;GitHub Copilot, an AI coding tool offered by Microsoft-owned GitHub, has now reached more than 20 million users, Microsoft CEO Satya Nadella said on the company’s earnings call Wednesday. A GitHub spokesperson confirmed to TechCrunch that this number represents “all-time users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That means five million people have tried out GitHub Copilot for the first time in the last three months; the company reported in April the tool had reached 15 million users. Microsoft and GitHub don’t report how many of these 20 million people have continued to use the AI coding tool on a monthly or daily basis — though those metrics are likely far lower.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft also reported GitHub Copilot, which is among the most popular AI coding tools offered today, is used by 90% of the Fortune 100. The product’s growth among enterprise customers has also grown about 75% compared to last quarter, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI coding tools are rising in popularity, and they seem to be one of the few AI products generating notable revenue. In 2024, Nadella said GitHub Copilot was a larger business than all of GitHub was when Microsoft acquired it in 2018. In the year since, it seems GitHub Copilot’s growth rate has continued in a positive direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The world’s most popular AI coding tools still have tiny user bases compared to AI chatbots like ChatGPT and Gemini, which attract hundreds of millions of users every month. Of course, software engineering is more niche than the general informational queries offered by AI chatbots. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, software engineers and their employers seem to be willing to pay a premium for AI coding tools. And with Microsoft’s long list of enterprise customers and GitHub’s ecosystem of developers, GitHub Copilot is well positioned to dominate the market for enterprise AI coding tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor, another popular AI coding tool, wants to challenge GitHub Copilot in the enterprise, and it’s been scooping up talent from fledgling AI startups to do so. Cursor reportedly had more than a million people using its product every day in March, according to Bloomberg. At that time, the company generated about $200 million in annualized recurring revenue. Today, Cursor’s ARR is more than $500 million, suggesting there are now a lot more people using its products everyday.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While GitHub Copilot and Cursor initially sought to tackle different parts of the developer experience, they’re steadily converging into similar products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both companies have recently introduced AI agents to review code and catch bugs introduced by humans. Github and Cursor are also both trying to create AI agents that automate programmer workflows, allowing developers to offload tasks altogether. Nadella said during Wednesday’s earnings call that GitHub was seeing great momentum with their AI coding agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Cursor, GitHub has an array of well-capitalized competitors that would like to sell AI coding tools to the enterprise. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s Google — which acquired the leaders of AI coding startup Windsurf — as well as Cognition, the maker of Devin that subsequently acquired the rest of Windsurf’s team. That’s not to mention OpenAI and Anthropic, which are both building out their own AI coding offerings powered by in-house AI models, Codex and Claude Code respectively, in an attempt to win the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The nascent space is quickly heating up into one of AI’s most competitive markets. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1785159335.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;GitHub Copilot, an AI coding tool offered by Microsoft-owned GitHub, has now reached more than 20 million users, Microsoft CEO Satya Nadella said on the company’s earnings call Wednesday. A GitHub spokesperson confirmed to TechCrunch that this number represents “all-time users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That means five million people have tried out GitHub Copilot for the first time in the last three months; the company reported in April the tool had reached 15 million users. Microsoft and GitHub don’t report how many of these 20 million people have continued to use the AI coding tool on a monthly or daily basis — though those metrics are likely far lower.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft also reported GitHub Copilot, which is among the most popular AI coding tools offered today, is used by 90% of the Fortune 100. The product’s growth among enterprise customers has also grown about 75% compared to last quarter, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI coding tools are rising in popularity, and they seem to be one of the few AI products generating notable revenue. In 2024, Nadella said GitHub Copilot was a larger business than all of GitHub was when Microsoft acquired it in 2018. In the year since, it seems GitHub Copilot’s growth rate has continued in a positive direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The world’s most popular AI coding tools still have tiny user bases compared to AI chatbots like ChatGPT and Gemini, which attract hundreds of millions of users every month. Of course, software engineering is more niche than the general informational queries offered by AI chatbots. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, software engineers and their employers seem to be willing to pay a premium for AI coding tools. And with Microsoft’s long list of enterprise customers and GitHub’s ecosystem of developers, GitHub Copilot is well positioned to dominate the market for enterprise AI coding tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor, another popular AI coding tool, wants to challenge GitHub Copilot in the enterprise, and it’s been scooping up talent from fledgling AI startups to do so. Cursor reportedly had more than a million people using its product every day in March, according to Bloomberg. At that time, the company generated about $200 million in annualized recurring revenue. Today, Cursor’s ARR is more than $500 million, suggesting there are now a lot more people using its products everyday.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;While GitHub Copilot and Cursor initially sought to tackle different parts of the developer experience, they’re steadily converging into similar products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both companies have recently introduced AI agents to review code and catch bugs introduced by humans. Github and Cursor are also both trying to create AI agents that automate programmer workflows, allowing developers to offload tasks altogether. Nadella said during Wednesday’s earnings call that GitHub was seeing great momentum with their AI coding agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond Cursor, GitHub has an array of well-capitalized competitors that would like to sell AI coding tools to the enterprise. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s Google — which acquired the leaders of AI coding startup Windsurf — as well as Cognition, the maker of Devin that subsequently acquired the rest of Windsurf’s team. That’s not to mention OpenAI and Anthropic, which are both building out their own AI coding offerings powered by in-house AI models, Codex and Claude Code respectively, in an attempt to win the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The nascent space is quickly heating up into one of AI’s most competitive markets. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/30/github-copilot-crosses-20-million-all-time-users/</guid><pubDate>Thu, 31 Jul 2025 01:16:55 +0000</pubDate></item></channel></rss>