<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 19 Nov 2025 06:33:45 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Microsoft tries to head off the “novel security risks” of Windows 11 AI agents (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/11/new-windows-11-ai-agents-can-work-in-the-background-but-create-new-security-risks/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Agents with read/write access to your files create big security, privacy issues.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/MSFT_Holiday_copilot_Card_1-640x480.jpeg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/MSFT_Holiday_copilot_Card_1-1152x648-1763493467.jpeg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A Copilot key on the keyboard of a Windows PC. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Microsoft has been adding AI features to Windows 11 for years, but things have recently entered a new phase, with both generative and so-called “agentic” AI features working their way deeper into the bedrock of the operating system. A new build of Windows 11 released to Windows Insider Program testers yesterday includes a new “experimental agentic features” toggle in the Settings to support a feature called Copilot Actions, and Microsoft has published a detailed support article detailing more about just how those “experimental agentic features” will work.&lt;/p&gt;
&lt;p&gt;If you’re not familiar, “agentic” is a buzzword that Microsoft has used repeatedly to describe its future ambitions for Windows 11—in plainer language, these agents are meant to accomplish assigned tasks in the background, allowing the user’s attention to be turned elsewhere. Microsoft says it wants agents to be capable of “everyday tasks like organizing files, scheduling meetings, or sending emails,” and that Copilot Actions should give you “an active digital collaborator that can carry out complex tasks for you to enhance efficiency and productivity.”&lt;/p&gt;
&lt;p&gt;But like other kinds of AI, these agents can be prone to error and confabulations and will often proceed as if they know what they’re doing even when they don’t. They also present, in Microsoft’s own words, “novel security risks,” mostly related to what can happen if an attacker is able to give instructions to one of these agents. As a result, Microsoft’s implementation walks a tightrope between giving these agents access to your files and cordoning them off from the rest of the system.&lt;/p&gt;
&lt;h2&gt;Possible risks and attempted fixes&lt;/h2&gt;
&lt;figure class="ars-wp-img-shortcode id-2128140 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="951" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AIComponents-11-17.png" width="1204" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      For now, these “experimental agentic features” are optional, only available in early test builds of Windows 11, and off by default.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For example, AI agents running on a PC will be given their own user accounts separate from your personal account, ensuring that they don’t have permission to change &lt;em&gt;everything&lt;/em&gt; on the system and giving them their own “desktop” to work with that won’t interfere with what you’re working with on your screen. Users need to approve requests for their data, and “all actions of an agent are observable and distinguishable from those taken by a user.” Microsoft also says agents need to be able to produce logs of their activities and “should provide a means to supervise their activities,” including showing users a list of actions they’ll take to accomplish a multi-step task.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But these safeguards and monitoring capabilities don’t change the fact that you’re exposing yourself to privacy and security risks by using AI agents. They’ll be able to request read and write access to most of the files in your user account—by default, anything in the Documents, Downloads, Desktop, Music, Pictures, and Videos folders. They’ll have access to any apps that have been installed for all users on the PC (apps that have only been installed in your user account won’t be accessible to the agent, and it will also be possible for users to install apps that only their agents can access.) And agents can potentially be vulnerable to hijacking that exposes your data to attackers—Microsoft specifically mentions “cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.”&lt;/p&gt;
&lt;p&gt;For now, these features can be switched off with the Settings toggle and are off by default. That concession to user preference—plus the lengthy support document outlining the risks and the precautions Microsoft has tried to build into the system—at least suggests that Microsoft has learned lessons from its botched rollout of the data-scraping Windows Recall feature last year.&lt;/p&gt;
&lt;p&gt;Hopefully these features remain fully off by default when they start rolling out to the general public. If not, they risk becoming one more of the many things you need to change or turn off in a modern Windows 11 installation if you want to keep the operating system’s various cloud and AI offerings out of your way.&lt;/p&gt;
&lt;p&gt;Alongside these upcoming AI agents, Microsoft is also attempting to make Copilot more “human-centered” and approachable, adding a Clippy-esque animated character named “Mico” and improving its ability to understand voice input as well as typical mouse-and-keyboard requests.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Agents with read/write access to your files create big security, privacy issues.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/MSFT_Holiday_copilot_Card_1-640x480.jpeg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/MSFT_Holiday_copilot_Card_1-1152x648-1763493467.jpeg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A Copilot key on the keyboard of a Windows PC. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Microsoft has been adding AI features to Windows 11 for years, but things have recently entered a new phase, with both generative and so-called “agentic” AI features working their way deeper into the bedrock of the operating system. A new build of Windows 11 released to Windows Insider Program testers yesterday includes a new “experimental agentic features” toggle in the Settings to support a feature called Copilot Actions, and Microsoft has published a detailed support article detailing more about just how those “experimental agentic features” will work.&lt;/p&gt;
&lt;p&gt;If you’re not familiar, “agentic” is a buzzword that Microsoft has used repeatedly to describe its future ambitions for Windows 11—in plainer language, these agents are meant to accomplish assigned tasks in the background, allowing the user’s attention to be turned elsewhere. Microsoft says it wants agents to be capable of “everyday tasks like organizing files, scheduling meetings, or sending emails,” and that Copilot Actions should give you “an active digital collaborator that can carry out complex tasks for you to enhance efficiency and productivity.”&lt;/p&gt;
&lt;p&gt;But like other kinds of AI, these agents can be prone to error and confabulations and will often proceed as if they know what they’re doing even when they don’t. They also present, in Microsoft’s own words, “novel security risks,” mostly related to what can happen if an attacker is able to give instructions to one of these agents. As a result, Microsoft’s implementation walks a tightrope between giving these agents access to your files and cordoning them off from the rest of the system.&lt;/p&gt;
&lt;h2&gt;Possible risks and attempted fixes&lt;/h2&gt;
&lt;figure class="ars-wp-img-shortcode id-2128140 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="951" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AIComponents-11-17.png" width="1204" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      For now, these “experimental agentic features” are optional, only available in early test builds of Windows 11, and off by default.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For example, AI agents running on a PC will be given their own user accounts separate from your personal account, ensuring that they don’t have permission to change &lt;em&gt;everything&lt;/em&gt; on the system and giving them their own “desktop” to work with that won’t interfere with what you’re working with on your screen. Users need to approve requests for their data, and “all actions of an agent are observable and distinguishable from those taken by a user.” Microsoft also says agents need to be able to produce logs of their activities and “should provide a means to supervise their activities,” including showing users a list of actions they’ll take to accomplish a multi-step task.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But these safeguards and monitoring capabilities don’t change the fact that you’re exposing yourself to privacy and security risks by using AI agents. They’ll be able to request read and write access to most of the files in your user account—by default, anything in the Documents, Downloads, Desktop, Music, Pictures, and Videos folders. They’ll have access to any apps that have been installed for all users on the PC (apps that have only been installed in your user account won’t be accessible to the agent, and it will also be possible for users to install apps that only their agents can access.) And agents can potentially be vulnerable to hijacking that exposes your data to attackers—Microsoft specifically mentions “cross-prompt injection (XPIA), where malicious content embedded in UI elements or documents can override agent instructions, leading to unintended actions like data exfiltration or malware installation.”&lt;/p&gt;
&lt;p&gt;For now, these features can be switched off with the Settings toggle and are off by default. That concession to user preference—plus the lengthy support document outlining the risks and the precautions Microsoft has tried to build into the system—at least suggests that Microsoft has learned lessons from its botched rollout of the data-scraping Windows Recall feature last year.&lt;/p&gt;
&lt;p&gt;Hopefully these features remain fully off by default when they start rolling out to the general public. If not, they risk becoming one more of the many things you need to change or turn off in a modern Windows 11 installation if you want to keep the operating system’s various cloud and AI offerings out of your way.&lt;/p&gt;
&lt;p&gt;Alongside these upcoming AI agents, Microsoft is also attempting to make Copilot more “human-centered” and approachable, adding a Clippy-esque animated character named “Mico” and improving its ability to understand voice input as well as typical mouse-and-keyboard requests.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/11/new-windows-11-ai-agents-can-work-in-the-background-but-create-new-security-risks/</guid><pubDate>Tue, 18 Nov 2025 19:28:14 +0000</pubDate></item><item><title>AI data center provider Lambda raises whopping $1.5B after multibillion-dollar Microsoft deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/18/ai-data-center-provider-lambda-raises-whopping-1-5b-after-multibillion-dollar-microsoft-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-2159544073.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI data center provider Lambda announced Tuesday it raised $1.5 billion in a round led by TWG Global, a relatively new $40 billion investment firm formed by billionaires Thomas Tull, the former owner of Legendary Entertainment, and Guggenheim Partners founder and CEO Mark Walter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TWG holds a variety of the billionaires’ assets, including Walter’s stakes in the Los Angeles Lakers and the new Cadillac F1 racing team. The firm also has a $15 billion fund to invest in AI anchored by Abu Dhabi’s Mubadala Capital. TWG previously invested in a partnership with Elon Musk’s xAI and Palantir to sell AI agents to enterprises.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now it’s backing Lambda, which operates a number of U.S. AI data centers.&amp;nbsp;Lambda is a CoreWeave competitor, although it also sells its “AI factories” to hyperscaler clouds. Earlier this month, Lambda announced a multibillion-dollar deal to supply Microsoft with AI infrastructure using tens of thousands of Nvidia GPUs. (Nvidia is an investor in Lambda as well.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Remember that Microsoft had a similar deal with CoreWeave and had bought about $1 billion worth of services from the company in 2024, its largest customer last year by a mile. Then OpenAI swooped in and signed a $12 billion deal with CoreWeave in March.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, deal watchers have been talking for months about Lambda looking to raise hundreds of millions of dollars at a valuation north of $4 billion. There was also talk of an IPO. Prior to this, Lambda raised a $480 million Series D in February, with an estimated valuation of $2.5 billion, according to PitchBook.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lambda’s $1.5 billion raise far outstrips those earlier whispers of what it was seeking. Whether its valuation also soared, we can’t confirm and Lambda declined to comment on that.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-2159544073.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI data center provider Lambda announced Tuesday it raised $1.5 billion in a round led by TWG Global, a relatively new $40 billion investment firm formed by billionaires Thomas Tull, the former owner of Legendary Entertainment, and Guggenheim Partners founder and CEO Mark Walter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TWG holds a variety of the billionaires’ assets, including Walter’s stakes in the Los Angeles Lakers and the new Cadillac F1 racing team. The firm also has a $15 billion fund to invest in AI anchored by Abu Dhabi’s Mubadala Capital. TWG previously invested in a partnership with Elon Musk’s xAI and Palantir to sell AI agents to enterprises.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now it’s backing Lambda, which operates a number of U.S. AI data centers.&amp;nbsp;Lambda is a CoreWeave competitor, although it also sells its “AI factories” to hyperscaler clouds. Earlier this month, Lambda announced a multibillion-dollar deal to supply Microsoft with AI infrastructure using tens of thousands of Nvidia GPUs. (Nvidia is an investor in Lambda as well.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Remember that Microsoft had a similar deal with CoreWeave and had bought about $1 billion worth of services from the company in 2024, its largest customer last year by a mile. Then OpenAI swooped in and signed a $12 billion deal with CoreWeave in March.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, deal watchers have been talking for months about Lambda looking to raise hundreds of millions of dollars at a valuation north of $4 billion. There was also talk of an IPO. Prior to this, Lambda raised a $480 million Series D in February, with an estimated valuation of $2.5 billion, according to PitchBook.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lambda’s $1.5 billion raise far outstrips those earlier whispers of what it was seeking. Whether its valuation also soared, we can’t confirm and Lambda declined to comment on that.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/18/ai-data-center-provider-lambda-raises-whopping-1-5b-after-multibillion-dollar-microsoft-deal/</guid><pubDate>Tue, 18 Nov 2025 19:37:25 +0000</pubDate></item><item><title>Powering AI Superfactories, NVIDIA and Microsoft Integrate Latest Technologies for Inference, Cybersecurity, Physical AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/nvidia-microsoft-ai-superfactories/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/partner-blog-azure-promo-1260x680-2-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Timed with the Microsoft Ignite conference running this week, NVIDIA is expanding its collaboration with Microsoft, including through the adoption of next-generation NVIDIA Spectrum-X Ethernet switches for the new Microsoft Fairwater AI superfactory, powered by the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;p&gt;The collaboration brings new integrations across Microsoft 365 Copilot, as well as the public preview of next-generation Azure NC Series VMs powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, NVIDIA Nemotron integrations to accelerate AI for Microsoft SQL Server 2025, capabilities for onboarding AI agents in Microsoft 365 and optimizations for high-performance inference, cybersecurity and physical AI.&lt;/p&gt;
&lt;p&gt;Microsoft’s AI Superfactory connects the landmark Fairwater data center in Wisconsin with a new, state-of-the-art facility in Atlanta, Georgia. This massive-scale infrastructure will integrate hundreds of thousands of NVIDIA Blackwell GPUs for large-scale training. In addition, Microsoft is deploying more than 100,000 Blackwell Ultra GPUs in NVIDIA GB300 NVL72 systems being deployed globally for inference.&lt;/p&gt;
&lt;p&gt;“Our collaboration with NVIDIA is built on driving innovation across the entire system and full stack, from silicon to services,” said Nidhi Chappell, corporate vice president of product management at Microsoft. “By coupling Microsoft Azure’s unmatched data center scale with NVIDIA’s accelerated computing, we are maximizing AI data center performance and efficiency, which is of paramount importance for our customers leading the new AI era.”&lt;/p&gt;
&lt;p&gt;The most demanding workloads for OpenAI, the Microsoft AI Superintelligence Team, Microsoft 365 Copilot and Microsoft Foundry services will be powered by this infrastructure. Customers like Black Forest Labs are also using NVIDIA GB200 NVL72 systems to train next-generation multimodal FLUX models that power visual intelligence.&lt;/p&gt;
&lt;p&gt;To connect this massive infrastructure, Microsoft is deploying next-generation NVIDIA Spectrum-X Ethernet switches in its Fairwater AI data center — the largest and most sophisticated AI factories ever built — delivering the performance, scale and efficiency required for OpenAI to run large-scale AI models and applications.&lt;/p&gt;
&lt;p&gt;New Azure NCv6 Series VMs with NVIDIA RTX PRO 6000 Blackwell GPUs are now in public preview on Azure, expanding the Blackwell platform to provide right-sized acceleration for multiple workloads including multimodal agentic AI, industrial digitalization with NVIDIA Omniverse libraries, scientific simulation and visual computing. This flexibility extends from the cloud to the edge with Azure Local, enabling powerful sovereign AI solutions while bringing low-latency, real-time AI to wherever data needs to reside.&lt;/p&gt;
&lt;p&gt;This allows enterprises to seamlessly develop, deploy and manage AI-powered digital twins and generative AI applications with NVIDIA RTX PRO 6000 Blackwell GPUs from the Azure cloud directly to their factory floors, on-premises data centers or secure edge locations.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Software Optimizations Deliver a Fungible AI Fleet&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA platform on Azure, spanning NVIDIA Blackwell and Hopper GPUs, accelerates the latest models from the Microsoft AI Superintelligence Team, including text (MAI-1-preview), real-time voice (MAI-Voice-1) and high-fidelity image generation (MAI-Image-1) — bringing new multimodal experiences across Bing Image Creator and Microsoft Copilot.&lt;/p&gt;
&lt;p&gt;Central to NVIDIA’s collaboration with Microsoft is building a fungible fleet — a flexible, continuously modernized infrastructure that can accelerate any workload with maximum efficiency. This is achieved through continuous, full-stack software optimizations that deliver compounding performance gains and maximize throughput across the entire AI lifecycle and across multiple NVIDIA architectures on Azure. The gains also extend to workloads beyond generative AI, including data processing, vector search, databases, digital twins, scientific computing and 3D design.&lt;/p&gt;
&lt;p&gt;This co-engineering saves significant costs for customers, making AI projects that were once theoretical now economically viable. For example, the continuous full-stack optimization work has directly contributed to an over 90% drop in the price of popular GPT models for end users on Azure in two years.&lt;/p&gt;
&lt;p&gt;Ongoing optimization work now extends to Microsoft Foundry, where the NVIDIA TensorRT-LLM library helps boost throughput, reduce latency and lower costs for a wide range of popular open models.&lt;/p&gt;
&lt;p&gt;NVIDIA and Microsoft have also partnered to optimize their fleet for AI workload performance through the NVIDIA DGX Cloud Benchmarking suite. Engineering teams from both companies worked closely together to identify bottlenecks and implement infrastructure tuning, driving performance gains. By achieving 95% of the performance possible using the NVIDIA reference architecture, Microsoft was named an Exemplar Cloud for H100 training.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;From Intelligent Data to AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA and Microsoft are integrating AI into the core of the enterprise, unlocking decades of proprietary data stored in one of the world’s most trusted databases.&lt;/p&gt;
&lt;p&gt;NVIDIA is accelerating AI in the new Microsoft SQL Server 2025 by integrating it with NVIDIA Nemotron open models and NVIDIA NIM microservices. This solution delivers GPU-optimized, secure and scalable retrieval-augmented generation directly where enterprise data lives, in the cloud or on premises.&lt;/p&gt;
&lt;p&gt;Plus, the collaboration extends to the new frontier of agentic AI in the workplace. The NVIDIA NeMo Agent Toolkit now connects with Microsoft Agent 365, enabling developers to build, deploy and onboard compliant, enterprise-ready AI agents directly into the Microsoft 365 app ecosystem, including Outlook, Teams, Word and SharePoint.&lt;/p&gt;
&lt;p&gt;To power these new enterprise agents, Microsoft Foundry now offers NVIDIA Nemotron models for digital AI and NVIDIA Cosmos models for physical AI as secure NIM microservices. Developers can use them to build enterprise-grade agentic AI for a vast range of applications that benefit from multimodal intelligence, multilingual reasoning, math, coding and physical AI capabilities.&lt;/p&gt;
&lt;p&gt;The collaboration is also tackling cyber threats for enterprises. Microsoft and NVIDIA are collaborating on research for new adversarial learning models, built on the NVIDIA Dynamo-Triton framework and the NVIDIA TensorRT suite of tools, that can help enterprises defend against real-time cybersecurity threats with a 160x performance speedup compared with CPU methods.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Physical AI and Industrial Digitalization&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA and Microsoft are building the future of physical AI. With NVIDIA Omniverse libraries available on Microsoft Azure, NVIDIA is unlocking end-to-end reindustrialization in the cloud through its developer ecosystem. Developers are transforming industrial workflows, from computer-aided engineering with Synopsys to factory operations with Sight Machine and SymphonyAI.&lt;/p&gt;
&lt;p&gt;Robotics developers can tap into the NVIDIA Isaac Sim open-source robotics simulation framework to unlock critical workflows, from synthetic data generation to software-in-the-loop testing for all types of robot embodiments. Hexagon is building its AEON humanoid robot primarily using NVIDIA’s full robotics stack on Azure. Similarly, the robotics platform, Wandelbots NOVA, running on Azure integrates Isaac Sim and Isaac Lab to simplify and speed up simulation to real-world deployment.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA and Microsoft are using a standardized approach for digital engineering to enable seamless OpenUSD interoperability across 3D workflows, making simulation and digital content creation accessible in the cloud.&lt;/p&gt;
&lt;p&gt;This expanded collaboration comes on the heels of a partnership announced with Anthropic and Microsoft earlier today. NVIDIA and Anthropic will collaborate on design and engineering to optimize Anthropic models for performance, efficiency and total cost of ownership, as well as optimize future NVIDIA architectures for Anthropic workloads.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about NVIDIA and Microsoft’s collaboration and sessions at Microsoft Ignite.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/partner-blog-azure-promo-1260x680-2-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Timed with the Microsoft Ignite conference running this week, NVIDIA is expanding its collaboration with Microsoft, including through the adoption of next-generation NVIDIA Spectrum-X Ethernet switches for the new Microsoft Fairwater AI superfactory, powered by the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;p&gt;The collaboration brings new integrations across Microsoft 365 Copilot, as well as the public preview of next-generation Azure NC Series VMs powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, NVIDIA Nemotron integrations to accelerate AI for Microsoft SQL Server 2025, capabilities for onboarding AI agents in Microsoft 365 and optimizations for high-performance inference, cybersecurity and physical AI.&lt;/p&gt;
&lt;p&gt;Microsoft’s AI Superfactory connects the landmark Fairwater data center in Wisconsin with a new, state-of-the-art facility in Atlanta, Georgia. This massive-scale infrastructure will integrate hundreds of thousands of NVIDIA Blackwell GPUs for large-scale training. In addition, Microsoft is deploying more than 100,000 Blackwell Ultra GPUs in NVIDIA GB300 NVL72 systems being deployed globally for inference.&lt;/p&gt;
&lt;p&gt;“Our collaboration with NVIDIA is built on driving innovation across the entire system and full stack, from silicon to services,” said Nidhi Chappell, corporate vice president of product management at Microsoft. “By coupling Microsoft Azure’s unmatched data center scale with NVIDIA’s accelerated computing, we are maximizing AI data center performance and efficiency, which is of paramount importance for our customers leading the new AI era.”&lt;/p&gt;
&lt;p&gt;The most demanding workloads for OpenAI, the Microsoft AI Superintelligence Team, Microsoft 365 Copilot and Microsoft Foundry services will be powered by this infrastructure. Customers like Black Forest Labs are also using NVIDIA GB200 NVL72 systems to train next-generation multimodal FLUX models that power visual intelligence.&lt;/p&gt;
&lt;p&gt;To connect this massive infrastructure, Microsoft is deploying next-generation NVIDIA Spectrum-X Ethernet switches in its Fairwater AI data center — the largest and most sophisticated AI factories ever built — delivering the performance, scale and efficiency required for OpenAI to run large-scale AI models and applications.&lt;/p&gt;
&lt;p&gt;New Azure NCv6 Series VMs with NVIDIA RTX PRO 6000 Blackwell GPUs are now in public preview on Azure, expanding the Blackwell platform to provide right-sized acceleration for multiple workloads including multimodal agentic AI, industrial digitalization with NVIDIA Omniverse libraries, scientific simulation and visual computing. This flexibility extends from the cloud to the edge with Azure Local, enabling powerful sovereign AI solutions while bringing low-latency, real-time AI to wherever data needs to reside.&lt;/p&gt;
&lt;p&gt;This allows enterprises to seamlessly develop, deploy and manage AI-powered digital twins and generative AI applications with NVIDIA RTX PRO 6000 Blackwell GPUs from the Azure cloud directly to their factory floors, on-premises data centers or secure edge locations.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Software Optimizations Deliver a Fungible AI Fleet&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA platform on Azure, spanning NVIDIA Blackwell and Hopper GPUs, accelerates the latest models from the Microsoft AI Superintelligence Team, including text (MAI-1-preview), real-time voice (MAI-Voice-1) and high-fidelity image generation (MAI-Image-1) — bringing new multimodal experiences across Bing Image Creator and Microsoft Copilot.&lt;/p&gt;
&lt;p&gt;Central to NVIDIA’s collaboration with Microsoft is building a fungible fleet — a flexible, continuously modernized infrastructure that can accelerate any workload with maximum efficiency. This is achieved through continuous, full-stack software optimizations that deliver compounding performance gains and maximize throughput across the entire AI lifecycle and across multiple NVIDIA architectures on Azure. The gains also extend to workloads beyond generative AI, including data processing, vector search, databases, digital twins, scientific computing and 3D design.&lt;/p&gt;
&lt;p&gt;This co-engineering saves significant costs for customers, making AI projects that were once theoretical now economically viable. For example, the continuous full-stack optimization work has directly contributed to an over 90% drop in the price of popular GPT models for end users on Azure in two years.&lt;/p&gt;
&lt;p&gt;Ongoing optimization work now extends to Microsoft Foundry, where the NVIDIA TensorRT-LLM library helps boost throughput, reduce latency and lower costs for a wide range of popular open models.&lt;/p&gt;
&lt;p&gt;NVIDIA and Microsoft have also partnered to optimize their fleet for AI workload performance through the NVIDIA DGX Cloud Benchmarking suite. Engineering teams from both companies worked closely together to identify bottlenecks and implement infrastructure tuning, driving performance gains. By achieving 95% of the performance possible using the NVIDIA reference architecture, Microsoft was named an Exemplar Cloud for H100 training.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;From Intelligent Data to AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA and Microsoft are integrating AI into the core of the enterprise, unlocking decades of proprietary data stored in one of the world’s most trusted databases.&lt;/p&gt;
&lt;p&gt;NVIDIA is accelerating AI in the new Microsoft SQL Server 2025 by integrating it with NVIDIA Nemotron open models and NVIDIA NIM microservices. This solution delivers GPU-optimized, secure and scalable retrieval-augmented generation directly where enterprise data lives, in the cloud or on premises.&lt;/p&gt;
&lt;p&gt;Plus, the collaboration extends to the new frontier of agentic AI in the workplace. The NVIDIA NeMo Agent Toolkit now connects with Microsoft Agent 365, enabling developers to build, deploy and onboard compliant, enterprise-ready AI agents directly into the Microsoft 365 app ecosystem, including Outlook, Teams, Word and SharePoint.&lt;/p&gt;
&lt;p&gt;To power these new enterprise agents, Microsoft Foundry now offers NVIDIA Nemotron models for digital AI and NVIDIA Cosmos models for physical AI as secure NIM microservices. Developers can use them to build enterprise-grade agentic AI for a vast range of applications that benefit from multimodal intelligence, multilingual reasoning, math, coding and physical AI capabilities.&lt;/p&gt;
&lt;p&gt;The collaboration is also tackling cyber threats for enterprises. Microsoft and NVIDIA are collaborating on research for new adversarial learning models, built on the NVIDIA Dynamo-Triton framework and the NVIDIA TensorRT suite of tools, that can help enterprises defend against real-time cybersecurity threats with a 160x performance speedup compared with CPU methods.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Physical AI and Industrial Digitalization&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA and Microsoft are building the future of physical AI. With NVIDIA Omniverse libraries available on Microsoft Azure, NVIDIA is unlocking end-to-end reindustrialization in the cloud through its developer ecosystem. Developers are transforming industrial workflows, from computer-aided engineering with Synopsys to factory operations with Sight Machine and SymphonyAI.&lt;/p&gt;
&lt;p&gt;Robotics developers can tap into the NVIDIA Isaac Sim open-source robotics simulation framework to unlock critical workflows, from synthetic data generation to software-in-the-loop testing for all types of robot embodiments. Hexagon is building its AEON humanoid robot primarily using NVIDIA’s full robotics stack on Azure. Similarly, the robotics platform, Wandelbots NOVA, running on Azure integrates Isaac Sim and Isaac Lab to simplify and speed up simulation to real-world deployment.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA and Microsoft are using a standardized approach for digital engineering to enable seamless OpenUSD interoperability across 3D workflows, making simulation and digital content creation accessible in the cloud.&lt;/p&gt;
&lt;p&gt;This expanded collaboration comes on the heels of a partnership announced with Anthropic and Microsoft earlier today. NVIDIA and Anthropic will collaborate on design and engineering to optimize Anthropic models for performance, efficiency and total cost of ownership, as well as optimize future NVIDIA architectures for Anthropic workloads.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about NVIDIA and Microsoft’s collaboration and sessions at Microsoft Ignite.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/nvidia-microsoft-ai-superfactories/</guid><pubDate>Tue, 18 Nov 2025 20:00:22 +0000</pubDate></item><item><title>Musk's xAI launches Grok 4.1 with lower hallucination rate on the web and apps — no API access (for now) (AI | VentureBeat)</title><link>https://venturebeat.com/ai/musks-xai-launches-grok-4-1-with-lower-hallucination-rate-on-the-web-and</link><description>[unable to retrieve full-text content]&lt;p&gt;In what appeared to be a bid to soak up some of Google&amp;#x27;s limelight prior to the &lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt;launch of its new Gemini 3 flagship AI model &lt;/a&gt;— now recorded as the most powerful LLM in the world by multiple independent evaluators — Elon Musk&amp;#x27;s rival AI startup xAI last night unveiled its newest large language model, &lt;a href="https://x.ai/news/grok-4-1"&gt;Grok 4.1.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The model is now live for consumer use on Grok.com, social network X (formerly Twitter), and the company’s iOS and Android mobile apps, and it arrives with major architectural and usability enhancements, among them: faster reasoning, improved emotional intelligence, and significantly reduced hallucination rates. xAI also commendably published a white paper on its evaluations and including a small bit on training process &lt;a href="https://data.x.ai/2025-11-17-grok-4-1-model-card.pdf"&gt;here&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;Across public benchmarks, Grok 4.1 has vaulted to the top of the leaderboard, outperforming rival models from Anthropic, OpenAI, and Google — at least, Google&amp;#x27;s pre-Gemini 3 model (Gemini 2.5 Pro). It builds upon the success of xAI&amp;#x27;s Grok-4 Fast, which &lt;a href="https://venturebeat.com/ai/what-to-know-about-grok-4-fast-for-enterprise-use-cases"&gt;VentureBeat covered favorably&lt;/a&gt; shortly following its release back in September 2025.&lt;/p&gt;&lt;p&gt;However, enterprise developers looking to integrate the new and improved model Grok 4.1 into production environments will find one major constraint: it&amp;#x27;s not yet available through &lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI’s public API&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Despite its high benchmarks, Grok 4.1 remains confined to xAI’s consumer-facing interfaces, with no announced timeline for API exposure. At present, only older models—including Grok 4 Fast (reasoning and non-reasoning variants), Grok 4 0709, and legacy models such as Grok 3, Grok 3 Mini, and Grok 2 Vision—are available for programmatic use via the xAI developer API. These support up to 2 million tokens of context, with token pricing ranging from $0.20 to $3.00 per million depending on the configuration.&lt;/p&gt;&lt;p&gt;For now, this limits Grok 4.1’s utility in enterprise workflows that rely on backend integration, fine-tuned agentic pipelines, or scalable internal tooling. While the consumer rollout positions Grok 4.1 as the most capable LLM in xAI’s portfolio, production deployments in enterprise environments remain on hold.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Design and Deployment Strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Grok 4.1 arrives in two configurations: a fast-response, low-latency mode for immediate replies, and a “thinking” mode that engages in multi-step reasoning before producing output. &lt;/p&gt;&lt;p&gt;Both versions are live for end users and are selectable via the model picker in xAI’s apps.&lt;/p&gt;&lt;p&gt;The two configurations differ not just in latency but also in how deeply the model processes prompts. Grok 4.1 Thinking leverages internal planning and deliberation mechanisms, while the standard version prioritizes speed. Despite the difference in architecture, both scored higher than any competing models in blind preference and benchmark testing.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leading the Field in Human and Expert Evaluation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;On the &lt;a href="https://lmarena.ai/leaderboard/text"&gt;LMArena Text Arena leaderboard&lt;/a&gt;, Grok 4.1 Thinking briefly held the top position with a normalized Elo score of 1483 — then was dethroned a few hours later with &lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt;Google&amp;#x27;s release of Gemini 3 &lt;/a&gt;and its incredible 1501 Elo score. &lt;/p&gt;&lt;p&gt;The non-thinking version of Grok 4.1 also fares well on the index, however, at 1465. &lt;/p&gt;&lt;p&gt;These scores place Grok 4.1 above Google’s Gemini 2.5 Pro, Anthropic’s Claude 4.5 series, and OpenAI’s GPT-4.5 preview.&lt;/p&gt;&lt;p&gt;In creative writing, Grok 4.1 ranks second only to Polaris Alpha (an early GPT-5.1 variant), with the “thinking” model earning a score of 1721.9 on the Creative Writing v3 benchmark. This marks a roughly 600-point improvement over previous Grok iterations. &lt;/p&gt;&lt;p&gt;Similarly, in the Arena Expert leaderboard, which aggregates feedback from professional reviewers, Grok 4.1 Thinking again leads the field with a score of 1510.&lt;/p&gt;&lt;p&gt;The gains are especially notable given that Grok 4.1 was released only two months after Grok 4 Fast, highlighting the accelerated development pace at xAI.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Core Improvements Over Previous Generations&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Technically, Grok 4.1 represents a significant leap in real-world usability. Visual capabilities—previously limited in Grok 4—have been upgraded to enable robust image and video understanding, including chart analysis and OCR-level text extraction. Multimodal reliability was a pain point in prior versions and has now been addressed.&lt;/p&gt;&lt;p&gt;Token-level latency has been reduced by approximately 28 percent while preserving reasoning depth. &lt;/p&gt;&lt;p&gt;In long-context tasks, Grok 4.1 maintains coherent output up to 1 million tokens, improving on Grok 4’s tendency to degrade past the 300,000 token mark.&lt;/p&gt;&lt;p&gt;xAI has also improved the model&amp;#x27;s tool orchestration capabilities. Grok 4.1 can now plan and execute multiple external tools in parallel, reducing the number of interaction cycles required to complete multi-step queries. &lt;/p&gt;&lt;p&gt;According to internal test logs, some research tasks that previously required four steps can now be completed in one or two.&lt;/p&gt;&lt;p&gt;Other alignment improvements include better truth calibration—reducing the tendency to hedge or soften politically sensitive outputs—and more natural, human-like prosody in voice mode, with support for different speaking styles and accents.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety and Adversarial Robustness&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;As part of its risk management framework, xAI evaluated Grok 4.1 for refusal behavior, hallucination resistance, sycophancy, and dual-use safety.&lt;/p&gt;&lt;p&gt;The hallucination rate in non-reasoning mode has dropped from 12.09 percent in Grok 4 Fast to just 4.22 percent — a roughly 65% improvement.&lt;/p&gt;&lt;p&gt;The model also scored 2.97 percent on FActScore, a factual QA benchmark, down from 9.89 percent in earlier versions.&lt;/p&gt;&lt;p&gt;In the domain of adversarial robustness, Grok 4.1 has been tested with prompt injection attacks, jailbreak prompts, and sensitive chemistry and biology queries. &lt;/p&gt;&lt;p&gt;Safety filters showed low false negative rates, especially for restricted chemical knowledge (0.00 percent) and restricted biological queries (0.03 percent). &lt;/p&gt;&lt;p&gt;The model’s ability to resist manipulation in persuasion benchmarks, such as MakeMeSay, also appears strong—it registered a 0 percent success rate as an attacker.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Limited Enterprise Access via API&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite these gains, Grok 4.1 remains unavailable to enterprise users through xAI’s API. According to the company’s &lt;a href="https://docs.x.ai/docs/models"&gt;public documentation&lt;/a&gt;, the latest available models for developers are Grok 4 Fast (both reasoning and non-reasoning variants), each supporting up to 2 million tokens of context at pricing tiers ranging from $0.20 to $0.50 per million tokens. These are backed by a 4M tokens-per-minute throughput limit and 480 requests per minute (RPM) rate cap.&lt;/p&gt;&lt;p&gt;By contrast, Grok 4.1 is accessible only through xAI’s consumer-facing properties—X, Grok.com, and the mobile apps. This means organizations cannot yet deploy Grok 4.1 via fine-tuned internal workflows, multi-agent chains, or real-time product integrations.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Industry Reception and Next Steps&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release has been met with strong public and industry feedback. Elon Musk, founder of xAI, posted a brief endorsement, calling it “a great model” and congratulating the team. AI benchmark platforms have praised the leap in usability and linguistic nuance.&lt;/p&gt;&lt;p&gt;For enterprise customers, however, the picture is more mixed. Grok 4.1’s performance represents a breakthrough for general-purpose and creative tasks, but until API access is enabled, it will remain a consumer-first product with limited enterprise applicability.&lt;/p&gt;&lt;p&gt;As competitive models from OpenAI, Google, and Anthropic continue to evolve, xAI’s next strategic move may hinge on when—and how—it opens Grok 4.1 to external developers.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;In what appeared to be a bid to soak up some of Google&amp;#x27;s limelight prior to the &lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt;launch of its new Gemini 3 flagship AI model &lt;/a&gt;— now recorded as the most powerful LLM in the world by multiple independent evaluators — Elon Musk&amp;#x27;s rival AI startup xAI last night unveiled its newest large language model, &lt;a href="https://x.ai/news/grok-4-1"&gt;Grok 4.1.&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The model is now live for consumer use on Grok.com, social network X (formerly Twitter), and the company’s iOS and Android mobile apps, and it arrives with major architectural and usability enhancements, among them: faster reasoning, improved emotional intelligence, and significantly reduced hallucination rates. xAI also commendably published a white paper on its evaluations and including a small bit on training process &lt;a href="https://data.x.ai/2025-11-17-grok-4-1-model-card.pdf"&gt;here&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;Across public benchmarks, Grok 4.1 has vaulted to the top of the leaderboard, outperforming rival models from Anthropic, OpenAI, and Google — at least, Google&amp;#x27;s pre-Gemini 3 model (Gemini 2.5 Pro). It builds upon the success of xAI&amp;#x27;s Grok-4 Fast, which &lt;a href="https://venturebeat.com/ai/what-to-know-about-grok-4-fast-for-enterprise-use-cases"&gt;VentureBeat covered favorably&lt;/a&gt; shortly following its release back in September 2025.&lt;/p&gt;&lt;p&gt;However, enterprise developers looking to integrate the new and improved model Grok 4.1 into production environments will find one major constraint: it&amp;#x27;s not yet available through &lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI’s public API&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Despite its high benchmarks, Grok 4.1 remains confined to xAI’s consumer-facing interfaces, with no announced timeline for API exposure. At present, only older models—including Grok 4 Fast (reasoning and non-reasoning variants), Grok 4 0709, and legacy models such as Grok 3, Grok 3 Mini, and Grok 2 Vision—are available for programmatic use via the xAI developer API. These support up to 2 million tokens of context, with token pricing ranging from $0.20 to $3.00 per million depending on the configuration.&lt;/p&gt;&lt;p&gt;For now, this limits Grok 4.1’s utility in enterprise workflows that rely on backend integration, fine-tuned agentic pipelines, or scalable internal tooling. While the consumer rollout positions Grok 4.1 as the most capable LLM in xAI’s portfolio, production deployments in enterprise environments remain on hold.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Model Design and Deployment Strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Grok 4.1 arrives in two configurations: a fast-response, low-latency mode for immediate replies, and a “thinking” mode that engages in multi-step reasoning before producing output. &lt;/p&gt;&lt;p&gt;Both versions are live for end users and are selectable via the model picker in xAI’s apps.&lt;/p&gt;&lt;p&gt;The two configurations differ not just in latency but also in how deeply the model processes prompts. Grok 4.1 Thinking leverages internal planning and deliberation mechanisms, while the standard version prioritizes speed. Despite the difference in architecture, both scored higher than any competing models in blind preference and benchmark testing.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Leading the Field in Human and Expert Evaluation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;On the &lt;a href="https://lmarena.ai/leaderboard/text"&gt;LMArena Text Arena leaderboard&lt;/a&gt;, Grok 4.1 Thinking briefly held the top position with a normalized Elo score of 1483 — then was dethroned a few hours later with &lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt;Google&amp;#x27;s release of Gemini 3 &lt;/a&gt;and its incredible 1501 Elo score. &lt;/p&gt;&lt;p&gt;The non-thinking version of Grok 4.1 also fares well on the index, however, at 1465. &lt;/p&gt;&lt;p&gt;These scores place Grok 4.1 above Google’s Gemini 2.5 Pro, Anthropic’s Claude 4.5 series, and OpenAI’s GPT-4.5 preview.&lt;/p&gt;&lt;p&gt;In creative writing, Grok 4.1 ranks second only to Polaris Alpha (an early GPT-5.1 variant), with the “thinking” model earning a score of 1721.9 on the Creative Writing v3 benchmark. This marks a roughly 600-point improvement over previous Grok iterations. &lt;/p&gt;&lt;p&gt;Similarly, in the Arena Expert leaderboard, which aggregates feedback from professional reviewers, Grok 4.1 Thinking again leads the field with a score of 1510.&lt;/p&gt;&lt;p&gt;The gains are especially notable given that Grok 4.1 was released only two months after Grok 4 Fast, highlighting the accelerated development pace at xAI.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Core Improvements Over Previous Generations&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Technically, Grok 4.1 represents a significant leap in real-world usability. Visual capabilities—previously limited in Grok 4—have been upgraded to enable robust image and video understanding, including chart analysis and OCR-level text extraction. Multimodal reliability was a pain point in prior versions and has now been addressed.&lt;/p&gt;&lt;p&gt;Token-level latency has been reduced by approximately 28 percent while preserving reasoning depth. &lt;/p&gt;&lt;p&gt;In long-context tasks, Grok 4.1 maintains coherent output up to 1 million tokens, improving on Grok 4’s tendency to degrade past the 300,000 token mark.&lt;/p&gt;&lt;p&gt;xAI has also improved the model&amp;#x27;s tool orchestration capabilities. Grok 4.1 can now plan and execute multiple external tools in parallel, reducing the number of interaction cycles required to complete multi-step queries. &lt;/p&gt;&lt;p&gt;According to internal test logs, some research tasks that previously required four steps can now be completed in one or two.&lt;/p&gt;&lt;p&gt;Other alignment improvements include better truth calibration—reducing the tendency to hedge or soften politically sensitive outputs—and more natural, human-like prosody in voice mode, with support for different speaking styles and accents.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety and Adversarial Robustness&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;As part of its risk management framework, xAI evaluated Grok 4.1 for refusal behavior, hallucination resistance, sycophancy, and dual-use safety.&lt;/p&gt;&lt;p&gt;The hallucination rate in non-reasoning mode has dropped from 12.09 percent in Grok 4 Fast to just 4.22 percent — a roughly 65% improvement.&lt;/p&gt;&lt;p&gt;The model also scored 2.97 percent on FActScore, a factual QA benchmark, down from 9.89 percent in earlier versions.&lt;/p&gt;&lt;p&gt;In the domain of adversarial robustness, Grok 4.1 has been tested with prompt injection attacks, jailbreak prompts, and sensitive chemistry and biology queries. &lt;/p&gt;&lt;p&gt;Safety filters showed low false negative rates, especially for restricted chemical knowledge (0.00 percent) and restricted biological queries (0.03 percent). &lt;/p&gt;&lt;p&gt;The model’s ability to resist manipulation in persuasion benchmarks, such as MakeMeSay, also appears strong—it registered a 0 percent success rate as an attacker.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Limited Enterprise Access via API&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite these gains, Grok 4.1 remains unavailable to enterprise users through xAI’s API. According to the company’s &lt;a href="https://docs.x.ai/docs/models"&gt;public documentation&lt;/a&gt;, the latest available models for developers are Grok 4 Fast (both reasoning and non-reasoning variants), each supporting up to 2 million tokens of context at pricing tiers ranging from $0.20 to $0.50 per million tokens. These are backed by a 4M tokens-per-minute throughput limit and 480 requests per minute (RPM) rate cap.&lt;/p&gt;&lt;p&gt;By contrast, Grok 4.1 is accessible only through xAI’s consumer-facing properties—X, Grok.com, and the mobile apps. This means organizations cannot yet deploy Grok 4.1 via fine-tuned internal workflows, multi-agent chains, or real-time product integrations.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Industry Reception and Next Steps&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release has been met with strong public and industry feedback. Elon Musk, founder of xAI, posted a brief endorsement, calling it “a great model” and congratulating the team. AI benchmark platforms have praised the leap in usability and linguistic nuance.&lt;/p&gt;&lt;p&gt;For enterprise customers, however, the picture is more mixed. Grok 4.1’s performance represents a breakthrough for general-purpose and creative tasks, but until API access is enabled, it will remain a consumer-first product with limited enterprise applicability.&lt;/p&gt;&lt;p&gt;As competitive models from OpenAI, Google, and Anthropic continue to evolve, xAI’s next strategic move may hinge on when—and how—it opens Grok 4.1 to external developers.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/musks-xai-launches-grok-4-1-with-lower-hallucination-rate-on-the-web-and</guid><pubDate>Tue, 18 Nov 2025 20:03:00 +0000</pubDate></item><item><title>Tech giants pour billions into Anthropic as circular AI investments roll on (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/tech-giants-pour-billions-into-anthropic-as-circular-ai-investments-roll-on/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT competitor secures billions from Microsoft and Nvidia in deal to use cloud services and chips.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="356" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/triple_people-640x356.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/triple_people-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A still image of Dario Amodei of Anthropic (L), Satya Nadella of Microsoft (Center), and Jensen Huang of Nvidia (R) taken from an announcement video.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          https://www.youtube.com/watch?v=bl7vHnOgEg0&amp;amp;t=4s

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, Microsoft and Nvidia announced plans to invest in Anthropic under a new partnership that includes a $30 billion commitment by the Claude maker to use Microsoft’s cloud services. Nvidia will commit up to $10 billion to Anthropic and Microsoft up to $5 billion, with both companies investing in Anthropic’s next funding round.&lt;/p&gt;
&lt;p&gt;The deal brings together two companies that have backed OpenAI and connects them more closely to one of the ChatGPT maker’s main competitors. Microsoft CEO Satya Nadella said in a video that OpenAI “remains a critical partner,” while adding that the companies will increasingly be customers of each other.&lt;/p&gt;
&lt;p&gt;“We will use Anthropic models, they will use our infrastructure, and we’ll go to market together,” Nadella said.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic, Microsoft, and NVIDIA announce partnerships. 

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The move follows OpenAI’s recent restructuring that gave the company greater distance from its non-profit origins. OpenAI has since announced a $38 billion deal to buy cloud services from Amazon.com as the company becomes less dependent on Microsoft. OpenAI CEO Sam Altman has said the company plans to spend $1.4 trillion to develop 30 gigawatts of computing resources.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;D.A. Davidson analyst Gil Luria told Reuters that the new Anthropic-Microsoft partnership aims to reduce the AI economy’s reliance on OpenAI. “Microsoft has decided not to rely on one frontier model company,” Luria said. “Nvidia was also somewhat dependent on OpenAI’s success and is now helping generating broader demand.”&lt;/p&gt;
&lt;p&gt;However, the partnership also places further scrutiny on the increasingly circular nature of AI industry investments. “Anthropic will pay Microsoft to pay Nvidia so Microsoft and Nvidia can pay Anthropic,” wrote CNBC tech correspondent Steve Kovach on Bluesky.&lt;/p&gt;
&lt;p&gt;Under the partnership, Anthropic will work with Nvidia on chips and models to improve performance and commit up to 1 gigawatt of compute using Nvidia’s Grace Blackwell and Vera Rubin hardware.&lt;/p&gt;
&lt;p&gt;Microsoft will give Azure AI Foundry customers access to the latest Claude models. With this deal, Claude becomes available through all three major cloud providers: Microsoft, Amazon, and Google. Amazon will remain Anthropic’s primary cloud provider and training partner.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT competitor secures billions from Microsoft and Nvidia in deal to use cloud services and chips.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="356" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/triple_people-640x356.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/triple_people-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A still image of Dario Amodei of Anthropic (L), Satya Nadella of Microsoft (Center), and Jensen Huang of Nvidia (R) taken from an announcement video.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          https://www.youtube.com/watch?v=bl7vHnOgEg0&amp;amp;t=4s

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, Microsoft and Nvidia announced plans to invest in Anthropic under a new partnership that includes a $30 billion commitment by the Claude maker to use Microsoft’s cloud services. Nvidia will commit up to $10 billion to Anthropic and Microsoft up to $5 billion, with both companies investing in Anthropic’s next funding round.&lt;/p&gt;
&lt;p&gt;The deal brings together two companies that have backed OpenAI and connects them more closely to one of the ChatGPT maker’s main competitors. Microsoft CEO Satya Nadella said in a video that OpenAI “remains a critical partner,” while adding that the companies will increasingly be customers of each other.&lt;/p&gt;
&lt;p&gt;“We will use Anthropic models, they will use our infrastructure, and we’ll go to market together,” Nadella said.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Anthropic, Microsoft, and NVIDIA announce partnerships. 

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The move follows OpenAI’s recent restructuring that gave the company greater distance from its non-profit origins. OpenAI has since announced a $38 billion deal to buy cloud services from Amazon.com as the company becomes less dependent on Microsoft. OpenAI CEO Sam Altman has said the company plans to spend $1.4 trillion to develop 30 gigawatts of computing resources.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;D.A. Davidson analyst Gil Luria told Reuters that the new Anthropic-Microsoft partnership aims to reduce the AI economy’s reliance on OpenAI. “Microsoft has decided not to rely on one frontier model company,” Luria said. “Nvidia was also somewhat dependent on OpenAI’s success and is now helping generating broader demand.”&lt;/p&gt;
&lt;p&gt;However, the partnership also places further scrutiny on the increasingly circular nature of AI industry investments. “Anthropic will pay Microsoft to pay Nvidia so Microsoft and Nvidia can pay Anthropic,” wrote CNBC tech correspondent Steve Kovach on Bluesky.&lt;/p&gt;
&lt;p&gt;Under the partnership, Anthropic will work with Nvidia on chips and models to improve performance and commit up to 1 gigawatt of compute using Nvidia’s Grace Blackwell and Vera Rubin hardware.&lt;/p&gt;
&lt;p&gt;Microsoft will give Azure AI Foundry customers access to the latest Claude models. With this deal, Claude becomes available through all three major cloud providers: Microsoft, Amazon, and Google. Amazon will remain Anthropic’s primary cloud provider and training partner.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/tech-giants-pour-billions-into-anthropic-as-circular-ai-investments-roll-on/</guid><pubDate>Tue, 18 Nov 2025 20:37:04 +0000</pubDate></item><item><title>Hugging Face CEO says we’re in an ‘LLM bubble,’ not an AI bubble (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/18/hugging-face-ceo-says-were-in-an-llm-bubble-not-an-ai-bubble/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/clem-delangue.png?resize=1200,789" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hugging Face co-founder and CEO Clem Delangue says we’re not in an AI bubble, but an “LLM bubble” — and it may be poised to pop. At an Axios event on Tuesday, the entrepreneur behind the popular AI platform and community site agreed that bubble talk is today’s “trillion-dollar question,” but said he doesn’t believe AI’s future is at risk if the bubble bursts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, as Delangue sees it, it’s large language models (LLMs) — like those powering ChatGPT, Gemini, and other chatbots — that are receiving outsized attention, and that attention may not last.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think we’re in an LLM bubble, and I think the LLM bubble might be bursting next year,” explained Delangue. “But ‘LLM’ is just a subset of AI when it comes to applying AI to biology, chemistry, image, audio, [and] video. I think we’re at the beginning of it, and we’ll see much more in the next few years,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One issue, he argued, is that LLMs aren’t the right solution for everything, and smaller, more specialized models will see increased adoption in the future. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think all the attention, all the focus, all the money, is concentrated into this idea that you can build one model through a bunch of compute and that is going to solve all problems for all companies and all people,” said Delangue. “I think the reality is that you’ll see in the next few months, next few years, kind of like a multiplicity of models that are more customized, specialized, that are going to solve different problems.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an example, he suggested the use case of a banking customer chatbot. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You don’t need it to tell you about the meaning of life, right? You can use a smaller, more specialized model that is going to be cheaper, that is going to be faster, that maybe you’re going to be able to run on your infrastructure as an enterprise, and I think that is the future of AI,” Delangue pointed out. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Hugging Face founder admitted that an LLM bubble bursting could impact his company to some extent, but noted that the AI industry is so big that it’s already diversified. That means even if a portion of the industry is overvalued, like LLMs, it’s not going to have a massive impact on the AI field itself or his business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, he noted, Hugging Face still has half of the $400 million it has raised in the bank. This cautious approach to spending represents a different strategy from what other AI companies are doing these days, especially in the LLM space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In AI standards, that’s called profitability because the other guys — it’s not hundreds of millions that they’re spending. It’s obviously billions of dollars,” he said. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;By comparison, Hugging Face is taking a more capital-efficient approach. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think a lot of people right now are rushing — or maybe even panicking — and taking a really short-term approach to things. I’ve been in AI for 15 years now, so I’ve seen some of the cycles,” Delangue added. “And so we’re learning from that and trying to build a long-term, sustainable, impactful company for the world.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/clem-delangue.png?resize=1200,789" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hugging Face co-founder and CEO Clem Delangue says we’re not in an AI bubble, but an “LLM bubble” — and it may be poised to pop. At an Axios event on Tuesday, the entrepreneur behind the popular AI platform and community site agreed that bubble talk is today’s “trillion-dollar question,” but said he doesn’t believe AI’s future is at risk if the bubble bursts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, as Delangue sees it, it’s large language models (LLMs) — like those powering ChatGPT, Gemini, and other chatbots — that are receiving outsized attention, and that attention may not last.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I think we’re in an LLM bubble, and I think the LLM bubble might be bursting next year,” explained Delangue. “But ‘LLM’ is just a subset of AI when it comes to applying AI to biology, chemistry, image, audio, [and] video. I think we’re at the beginning of it, and we’ll see much more in the next few years,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One issue, he argued, is that LLMs aren’t the right solution for everything, and smaller, more specialized models will see increased adoption in the future. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think all the attention, all the focus, all the money, is concentrated into this idea that you can build one model through a bunch of compute and that is going to solve all problems for all companies and all people,” said Delangue. “I think the reality is that you’ll see in the next few months, next few years, kind of like a multiplicity of models that are more customized, specialized, that are going to solve different problems.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an example, he suggested the use case of a banking customer chatbot. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You don’t need it to tell you about the meaning of life, right? You can use a smaller, more specialized model that is going to be cheaper, that is going to be faster, that maybe you’re going to be able to run on your infrastructure as an enterprise, and I think that is the future of AI,” Delangue pointed out. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Hugging Face founder admitted that an LLM bubble bursting could impact his company to some extent, but noted that the AI industry is so big that it’s already diversified. That means even if a portion of the industry is overvalued, like LLMs, it’s not going to have a massive impact on the AI field itself or his business.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, he noted, Hugging Face still has half of the $400 million it has raised in the bank. This cautious approach to spending represents a different strategy from what other AI companies are doing these days, especially in the LLM space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In AI standards, that’s called profitability because the other guys — it’s not hundreds of millions that they’re spending. It’s obviously billions of dollars,” he said. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;By comparison, Hugging Face is taking a more capital-efficient approach. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think a lot of people right now are rushing — or maybe even panicking — and taking a really short-term approach to things. I’ve been in AI for 15 years now, so I’ve seen some of the cycles,” Delangue added. “And so we’re learning from that and trying to build a long-term, sustainable, impactful company for the world.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/18/hugging-face-ceo-says-were-in-an-llm-bubble-not-an-ai-bubble/</guid><pubDate>Tue, 18 Nov 2025 21:42:48 +0000</pubDate></item><item><title>Gordon Bell Prize Finalists Push Open Science Boundaries With NVIDIA-Powered Supercomputers (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/gordon-bell-finalists-2025/</link><description>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v26.4 (Yoast SEO v26.4) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	Gordon Bell Prize Finalists Push Open Science Boundaries With NVIDIA-Powered Supercomputers | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;






































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			


				
				



		
		

&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					Five finalists for the esteemed high-performance computing award have achieved breakthroughs in climate modeling, fluid simulation and more with the Alps, JUPITER and Perlmutter supercomputers.				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	

	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Five finalists for the Gordon Bell Prize for outstanding achievements in high-performance computing (HPC) are using NVIDIA-powered supercomputers for their critical work in climate modeling, materials science, fluid simulation, geophysics and electronic design.&lt;/p&gt;&lt;p&gt;Announced today at SC25, the finalists’ projects are driving AI and HPC for science using physics simulation, high-precision math and other advanced supercomputing techniques, accelerating breakthroughs across weather forecasting, semiconductor design, space exploration and other fields. Their results are open and accessible on ArXiv.&lt;/p&gt;&lt;p&gt;The supercomputers powering their work include:&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Alps — hosted at the Swiss National Supercomputing Centre (CSCS) and powered by more than 10,000 NVIDIA GH200 Grace Hopper Superchips.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Perlmutter — hosted at the National Energy Research Scientific Computing Center (NERSC) and powered by NVIDIA accelerated computing.&lt;/li&gt;
&lt;li&gt;JUPITER — Europe’s first exascale supercomputer, hosted at the Jülich Supercomputing Centre (JSC) and powered by the NVIDIA Grace Hopper platform and Quantum-X800 InfiniBand networking.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-video-section"&gt;
			&lt;video class="full-width-layout__video" loop="loop"&gt;
							&lt;source src="https://blogs.nvidia.com/wp-content/uploads/2025/09/jupiter-featured_1280x720.mp4" type="video/mp4" /&gt;
						&lt;p&gt;Your browser does not support HTML5 video.&lt;/p&gt;
		&lt;/video&gt;
	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A rendering of JUPITER supercomputer racks featuring the NVIDIA Grace Hopper platform. Video courtesy of Forschungszentrum Jülich / Sascha Kreklau.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“At CSCS, we don’t just support open science — we accelerate it,” said Thomas Schulthess, director of CSCS. “The extraordinary breakthroughs by this year’s five Gordon Bell finalists in climate modeling, materials science, fluid dynamics and digital twins stand as irrefutable proof: without the Alps supercomputer, these scientific discoveries simply would not exist. Pushing computational boundaries turns bold targets into reality, delivering scientific revolutions that will redefine our world.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="&amp;quot;Pushing computational boundaries turns bold targets into reality, delivering scientific revolutions that will redefine our world,&amp;quot; said Thomas Schulthess, director of CSCS." class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/cscs-pull-quote-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Learn more about the five finalists’ projects below.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;ICON: Modeling Earth at Kilometer-Scale&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;A novel configuration for the ICON Earth system model — developed by researchers at the Max Planck Institute for Meteorology, German Climate Computing Centre (DKRZ), CSCS, JSC, ETH Zurich and NVIDIA — is poised to enable more accurate weather forecasts and a deeper understanding of how the planet works.&amp;nbsp;&lt;/p&gt;&lt;p&gt;By modeling the entire Earth’s systems at kilometer-scale resolution, ICON can capture the flow of energy, water and carbon through the atmosphere, oceans and land with exceptional detail and unprecedented temporal compression — allowing about 146 days to be simulated every 24 hours — which enables more efficient climate simulations projecting up to decades forward.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-video-section"&gt;
			&lt;video class="full-width-layout__video" loop="loop"&gt;
							&lt;source src="https://blogs.nvidia.com/wp-content/uploads/2025/11/icon-carbon-flux-simulation-video.mp4" type="video/mp4" /&gt;
						&lt;p&gt;Your browser does not support HTML5 video.&lt;/p&gt;
		&lt;/video&gt;
	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A simulation of carbon dioxide flux using the ICON model.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“Integrating all essential components of the Earth system in the ICON model at an unprecedented resolution of 1 kilometer allows researchers to see full global Earth system information on local scales and learn more about the implications of future warming for both people and ecosystems,” said Daniel Klocke, computational infrastructure and model development group leader at Max Planck Institute for Meteorology.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;ORBIT-2: Exascale Vision Foundation Models for Weather and Climate Modeling&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Developed as part of a collaboration between Oak Ridge National Laboratory, NVIDIA and others — and running on the Alps supercomputer — ORBIT-2 is an AI foundation model for weather and climate downscaling that demonstrates unparalleled scalability and precision.&lt;/p&gt;&lt;p&gt;Tapping into exascale computing and algorithmic innovation, ORBIT-2 overcomes challenges faced by traditional climate models with spatial hyper-resolution downscaling, a technique that creates high-resolution data from lower-resolution sources. This enables teams to capture and predict far more localized phenomena like urban heat islands, extreme precipitation events and subtle shifts in monsoon patterns.&lt;/p&gt;&lt;p&gt;[embedded content]&lt;/p&gt;&lt;p&gt;“NVIDIA’s advanced supercomputing technologies enabled ORBIT-2 to achieve exceptional scalability, reliability and impact at the intersection of AI and high-performance computing on NVIDIA platforms,” said Prasanna Balaprakash, director of AI programs and section head for data and AI systems at Oak Ridge National Laboratory.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;QuaTrEx: Advancing Transistor Design Through Nanoscale Device Modeling&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;A team from ETH Zurich has advanced nanoscale electronic device modeling with QuaTrEx, a package of algorithms that can boost the design of next-generation transistors.&lt;/p&gt;&lt;p&gt;Running on the Alps supercomputer with NVIDIA GH200 Superchips, QuaTrEx can simulate devices with more than 45,000 atoms with FP64 performance and extreme parallel-computing efficiency. This enables faster, more accurate design of transistors, called NREFTs, that will be crucial for the semiconductor industry.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A simulation of the flow of electrons in a nanoribbon transistor. Video courtesy of ETH Zurich.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“Access to Alps was instrumental in the development of QuaTrEx,” said Mathieu Luisier, full professor of computational nanoelectronics at ETH Zurich. “It allowed us to simulate devices that we could not imagine handling just a few months ago.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Simulating Spacecraft at Record-Breaking Scales With the MFC Flow Solver&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Designing spacecrafts, especially those with many small engines, requires detailed simulation, as engines packed closely together can cause their exhaust to interact and heat up a rocket’s base.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Running on the Alps supercomputer, MFC, an open-source solver developed by the Georgia Institute of Technology in collaboration with NVIDIA and others, enables fluid flow simulation 4x faster and with over 5x greater energy efficiency while maintaining the same accuracy as the previous world record. Based on full-scale runs on Alps, MFC is expected to run at 10x the scale of the previous world record on JUPITER. This paves the way for faster, more accurate design of critical components for space exploration.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-video-section"&gt;
			&lt;video class="full-width-layout__video" loop="loop"&gt;
							&lt;source src="https://blogs.nvidia.com/wp-content/uploads/2025/11/mfc-engine-simulation.mp4" type="video/mp4" /&gt;
						&lt;p&gt;Your browser does not support HTML5 video.&lt;/p&gt;
		&lt;/video&gt;
	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A rocket engine simulation using computational fluid dynamics. Video courtesy of the Georgia Institute of Technology.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“Our new information geometric regularization method, combined with the NVIDIA GH200 Superchip’s unified virtual memory and mixed-precision capabilities, has drastically improved the efficiency of simulating complex computational fluid flows, enabling us to simulate rocket engine plumes at unprecedented scales,” said Spencer Bryngelson, assistant professor in computational science and engineering at the Georgia Institute of Technology.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;A Digital Twin for Tsunami Early Warning&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The University of Texas at Austin, Lawrence Livermore National Laboratory and the University of California San Diego have created the world’s first digital twin that can issue real-time probabilistic tsunami forecasts based on a full-physics model.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Applied to the Cascadia subduction zone in the Pacific Northwest, the digital twin accomplished complex computations that would normally take 50 years on 512 GPUs in just 0.2 seconds on the Alps and Perlmutter supercomputers, representing a 10 billion-fold speedup.&lt;/p&gt;&lt;p&gt;[embedded content]&lt;/p&gt;&lt;p&gt;“For the first time, real-time sensor data can be rapidly combined with full-physics modeling and uncertainty quantification to give people a chance to act before disaster strikes,” said Omar Ghattas, professor of mechanical engineering at UT Austin. “This framework provides a basis for predictive, physics-based emergency-response systems across various hazards.”&lt;/p&gt;&lt;p&gt;For the tsunami digital twin, ICON and MFC projects, NVIDIA CUDA-X libraries played a key role in maximizing the performance and efficiency of the complex simulations. ICON also taps into NVIDIA CUDA Graphs, which allow work to be defined as graphs rather than single operations.&lt;/p&gt;&lt;p&gt;[embedded content]&lt;/p&gt;&lt;p&gt;&lt;i&gt;Learn more about the latest supercomputing advancements by joining &lt;/i&gt;&lt;i&gt;NVIDIA at SC25&lt;/i&gt;&lt;i&gt;, running through Thursday, Nov. 20.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;


&lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</description><content:encoded>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v26.4 (Yoast SEO v26.4) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	Gordon Bell Prize Finalists Push Open Science Boundaries With NVIDIA-Powered Supercomputers | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;






































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			


				
				



		
		

&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					Five finalists for the esteemed high-performance computing award have achieved breakthroughs in climate modeling, fluid simulation and more with the Alps, JUPITER and Perlmutter supercomputers.				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	

	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Five finalists for the Gordon Bell Prize for outstanding achievements in high-performance computing (HPC) are using NVIDIA-powered supercomputers for their critical work in climate modeling, materials science, fluid simulation, geophysics and electronic design.&lt;/p&gt;&lt;p&gt;Announced today at SC25, the finalists’ projects are driving AI and HPC for science using physics simulation, high-precision math and other advanced supercomputing techniques, accelerating breakthroughs across weather forecasting, semiconductor design, space exploration and other fields. Their results are open and accessible on ArXiv.&lt;/p&gt;&lt;p&gt;The supercomputers powering their work include:&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;Alps — hosted at the Swiss National Supercomputing Centre (CSCS) and powered by more than 10,000 NVIDIA GH200 Grace Hopper Superchips.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Perlmutter — hosted at the National Energy Research Scientific Computing Center (NERSC) and powered by NVIDIA accelerated computing.&lt;/li&gt;
&lt;li&gt;JUPITER — Europe’s first exascale supercomputer, hosted at the Jülich Supercomputing Centre (JSC) and powered by the NVIDIA Grace Hopper platform and Quantum-X800 InfiniBand networking.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-video-section"&gt;
			&lt;video class="full-width-layout__video" loop="loop"&gt;
							&lt;source src="https://blogs.nvidia.com/wp-content/uploads/2025/09/jupiter-featured_1280x720.mp4" type="video/mp4" /&gt;
						&lt;p&gt;Your browser does not support HTML5 video.&lt;/p&gt;
		&lt;/video&gt;
	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A rendering of JUPITER supercomputer racks featuring the NVIDIA Grace Hopper platform. Video courtesy of Forschungszentrum Jülich / Sascha Kreklau.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“At CSCS, we don’t just support open science — we accelerate it,” said Thomas Schulthess, director of CSCS. “The extraordinary breakthroughs by this year’s five Gordon Bell finalists in climate modeling, materials science, fluid dynamics and digital twins stand as irrefutable proof: without the Alps supercomputer, these scientific discoveries simply would not exist. Pushing computational boundaries turns bold targets into reality, delivering scientific revolutions that will redefine our world.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="&amp;quot;Pushing computational boundaries turns bold targets into reality, delivering scientific revolutions that will redefine our world,&amp;quot; said Thomas Schulthess, director of CSCS." class="full-width-layout__image" height="819" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/cscs-pull-quote-scaled.jpg" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Learn more about the five finalists’ projects below.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;ICON: Modeling Earth at Kilometer-Scale&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;A novel configuration for the ICON Earth system model — developed by researchers at the Max Planck Institute for Meteorology, German Climate Computing Centre (DKRZ), CSCS, JSC, ETH Zurich and NVIDIA — is poised to enable more accurate weather forecasts and a deeper understanding of how the planet works.&amp;nbsp;&lt;/p&gt;&lt;p&gt;By modeling the entire Earth’s systems at kilometer-scale resolution, ICON can capture the flow of energy, water and carbon through the atmosphere, oceans and land with exceptional detail and unprecedented temporal compression — allowing about 146 days to be simulated every 24 hours — which enables more efficient climate simulations projecting up to decades forward.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-video-section"&gt;
			&lt;video class="full-width-layout__video" loop="loop"&gt;
							&lt;source src="https://blogs.nvidia.com/wp-content/uploads/2025/11/icon-carbon-flux-simulation-video.mp4" type="video/mp4" /&gt;
						&lt;p&gt;Your browser does not support HTML5 video.&lt;/p&gt;
		&lt;/video&gt;
	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A simulation of carbon dioxide flux using the ICON model.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“Integrating all essential components of the Earth system in the ICON model at an unprecedented resolution of 1 kilometer allows researchers to see full global Earth system information on local scales and learn more about the implications of future warming for both people and ecosystems,” said Daniel Klocke, computational infrastructure and model development group leader at Max Planck Institute for Meteorology.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;ORBIT-2: Exascale Vision Foundation Models for Weather and Climate Modeling&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Developed as part of a collaboration between Oak Ridge National Laboratory, NVIDIA and others — and running on the Alps supercomputer — ORBIT-2 is an AI foundation model for weather and climate downscaling that demonstrates unparalleled scalability and precision.&lt;/p&gt;&lt;p&gt;Tapping into exascale computing and algorithmic innovation, ORBIT-2 overcomes challenges faced by traditional climate models with spatial hyper-resolution downscaling, a technique that creates high-resolution data from lower-resolution sources. This enables teams to capture and predict far more localized phenomena like urban heat islands, extreme precipitation events and subtle shifts in monsoon patterns.&lt;/p&gt;&lt;p&gt;[embedded content]&lt;/p&gt;&lt;p&gt;“NVIDIA’s advanced supercomputing technologies enabled ORBIT-2 to achieve exceptional scalability, reliability and impact at the intersection of AI and high-performance computing on NVIDIA platforms,” said Prasanna Balaprakash, director of AI programs and section head for data and AI systems at Oak Ridge National Laboratory.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;QuaTrEx: Advancing Transistor Design Through Nanoscale Device Modeling&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;A team from ETH Zurich has advanced nanoscale electronic device modeling with QuaTrEx, a package of algorithms that can boost the design of next-generation transistors.&lt;/p&gt;&lt;p&gt;Running on the Alps supercomputer with NVIDIA GH200 Superchips, QuaTrEx can simulate devices with more than 45,000 atoms with FP64 performance and extreme parallel-computing efficiency. This enables faster, more accurate design of transistors, called NREFTs, that will be crucial for the semiconductor industry.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-video-section"&gt;
	&lt;video class="full-width-layout__video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A simulation of the flow of electrons in a nanoribbon transistor. Video courtesy of ETH Zurich.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“Access to Alps was instrumental in the development of QuaTrEx,” said Mathieu Luisier, full professor of computational nanoelectronics at ETH Zurich. “It allowed us to simulate devices that we could not imagine handling just a few months ago.”&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;Simulating Spacecraft at Record-Breaking Scales With the MFC Flow Solver&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Designing spacecrafts, especially those with many small engines, requires detailed simulation, as engines packed closely together can cause their exhaust to interact and heat up a rocket’s base.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Running on the Alps supercomputer, MFC, an open-source solver developed by the Georgia Institute of Technology in collaboration with NVIDIA and others, enables fluid flow simulation 4x faster and with over 5x greater energy efficiency while maintaining the same accuracy as the previous world record. Based on full-scale runs on Alps, MFC is expected to run at 10x the scale of the previous world record on JUPITER. This paves the way for faster, more accurate design of critical components for space exploration.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__full-width-video-section"&gt;
			&lt;video class="full-width-layout__video" loop="loop"&gt;
							&lt;source src="https://blogs.nvidia.com/wp-content/uploads/2025/11/mfc-engine-simulation.mp4" type="video/mp4" /&gt;
						&lt;p&gt;Your browser does not support HTML5 video.&lt;/p&gt;
		&lt;/video&gt;
	
	
&lt;p&gt;
			&lt;span class="full-width-layout__media-caption-callout"&gt;
			A rocket engine simulation using computational fluid dynamics. Video courtesy of the Georgia Institute of Technology.		&lt;/span&gt;
	
	&lt;/p&gt;
&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;“Our new information geometric regularization method, combined with the NVIDIA GH200 Superchip’s unified virtual memory and mixed-precision capabilities, has drastically improved the efficiency of simulating complex computational fluid flows, enabling us to simulate rocket engine plumes at unprecedented scales,” said Spencer Bryngelson, assistant professor in computational science and engineering at the Georgia Institute of Technology.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;h2 class="full-width-layout__heading"&gt;A Digital Twin for Tsunami Early Warning&lt;/h2&gt;&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The University of Texas at Austin, Lawrence Livermore National Laboratory and the University of California San Diego have created the world’s first digital twin that can issue real-time probabilistic tsunami forecasts based on a full-physics model.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Applied to the Cascadia subduction zone in the Pacific Northwest, the digital twin accomplished complex computations that would normally take 50 years on 512 GPUs in just 0.2 seconds on the Alps and Perlmutter supercomputers, representing a 10 billion-fold speedup.&lt;/p&gt;&lt;p&gt;[embedded content]&lt;/p&gt;&lt;p&gt;“For the first time, real-time sensor data can be rapidly combined with full-physics modeling and uncertainty quantification to give people a chance to act before disaster strikes,” said Omar Ghattas, professor of mechanical engineering at UT Austin. “This framework provides a basis for predictive, physics-based emergency-response systems across various hazards.”&lt;/p&gt;&lt;p&gt;For the tsunami digital twin, ICON and MFC projects, NVIDIA CUDA-X libraries played a key role in maximizing the performance and efficiency of the complex simulations. ICON also taps into NVIDIA CUDA Graphs, which allow work to be defined as graphs rather than single operations.&lt;/p&gt;&lt;p&gt;[embedded content]&lt;/p&gt;&lt;p&gt;&lt;i&gt;Learn more about the latest supercomputing advancements by joining &lt;/i&gt;&lt;i&gt;NVIDIA at SC25&lt;/i&gt;&lt;i&gt;, running through Thursday, Nov. 20.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;


&lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/gordon-bell-finalists-2025/</guid><pubDate>Tue, 18 Nov 2025 22:45:47 +0000</pubDate></item><item><title>[NEW] New AI agent learns to use CAD to create 3D objects from sketches (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-ai-agent-learns-use-cad-create-3d-objects-sketches-1119</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Video-CAD-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Computer-Aided Design (CAD) is the go-to method for designing most of today’s physical products. Engineers use CAD to turn 2D sketches into 3D models that they can then test and refine before sending a final version to a production line. But the software is notoriously complicated to learn, with thousands of commands to choose from. To be truly proficient in the software takes a huge amount of time and practice.&lt;/p&gt;&lt;p&gt;MIT engineers are looking to ease CAD’s learning curve with an AI model that uses CAD software much like a human would. Given a 2D sketch of an object, the model quickly creates a 3D version by clicking buttons and file options, similar to how an engineer would use the software.&lt;/p&gt;&lt;p&gt;The MIT team has created a new dataset called VideoCAD, which contains more than 41,000 examples of how 3D models are built in CAD software. By learning from these videos, which illustrate how different shapes and objects are constructed step-by-step, the new AI system can now operate CAD software much like a human user.&lt;/p&gt;&lt;p&gt;With VideoCAD, the team is building toward an&amp;nbsp;AI-enabled “CAD co-pilot.” They envision that such a tool could not only create 3D versions of a design, but also work with a human user to suggest next steps, or automatically carry out build sequences that would otherwise be tedious and time-consuming to manually click through.&lt;/p&gt;&lt;p&gt;“There’s an opportunity for AI to increase engineers’ productivity as well as make CAD more accessible to more people,” says Ghadi Nehme, a graduate student in MIT’s Department of Mechanical Engineering.&lt;/p&gt;&lt;p&gt;“This is significant because it lowers the barrier to entry for design, helping people without years of CAD training to create 3D models more easily and tap into their creativity,” adds Faez Ahmed, associate professor of mechanical engineering at MIT.&lt;/p&gt;&lt;p&gt;Ahmed and Nehme, along with graduate student Brandon Man and postdoc Ferdous Alam, will present their work at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Click by click&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s new work expands on recent developments in AI-driven user interface (UI) agents — tools that are trained to use software programs to carry out tasks, such as automatically gathering information online and organizing it in an Excel spreadsheet. Ahmed’s group wondered whether such UI agents could be designed to use CAD, which encompasses many more features and functions, and involves far more complicated tasks than the average UI agent can handle.&lt;/p&gt;&lt;p&gt;In their new work, the team aimed to design an AI-driven UI agent that takes the reins of the CAD program to create a 3D version of a 2D sketch, click by click. To do so,&amp;nbsp;the team first looked to an existing dataset of objects that were designed in CAD by humans. Each object in the dataset includes the sequence of high-level design commands, such as “sketch line,” “circle,” and “extrude,” that were used to build the final object.&lt;/p&gt;&lt;p&gt;However, the team realized that these high-level commands alone were not enough to train an AI agent to actually use CAD software. A real agent must also understand the details behind each action. For instance: Which sketch region should it select? When should it zoom in? And what part of a sketch should it extrude? To bridge this gap, the researchers developed a system to translate high-level commands into user-interface interactions.&lt;/p&gt;&lt;p&gt;“For example, let’s say we drew a sketch by drawing a line from point 1 to point 2,” Nehme says. “We translated those high-level actions to user-interface actions, meaning we say, go from this pixel location, click, and then move to a second pixel location, and click, while having the ‘line’ operation selected.”&lt;/p&gt;&lt;p&gt;In the end, the team generated over 41,000 videos of human-designed CAD objects, each of which is described in real-time in terms of the specific clicks, mouse-drags, and other keyboard actions that the human originally carried out. They then fed all this data into a model they developed to learn connections between UI actions and CAD object&amp;nbsp;generation.&lt;/p&gt;&lt;p&gt;Once trained on this dataset, which they dub VideoCAD, the new AI model could take a 2D sketch as input and directly control the CAD software, clicking, dragging, and selecting tools to construct the full 3D shape. The objects ranged in complexity from simple brackets to more complicated house designs. The team is training the model on more complex shapes and envisions that both the model and the dataset could one day enable CAD co-pilots for designers in a wide range of fields.&lt;/p&gt;&lt;p&gt;“VideoCAD is a valuable first step toward AI assistants that help onboard new users and automate the repetitive modeling work that follows familiar patterns,” says Mehdi Ataei, who was not involved in the study, and is a senior research scientist at Autodesk Research, which develops new design software tools. “This is an early foundation, and I would be excited to see successors that span multiple CAD systems, richer operations like assemblies and constraints, and more realistic, messy human workflows.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MIT-Video-CAD-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Computer-Aided Design (CAD) is the go-to method for designing most of today’s physical products. Engineers use CAD to turn 2D sketches into 3D models that they can then test and refine before sending a final version to a production line. But the software is notoriously complicated to learn, with thousands of commands to choose from. To be truly proficient in the software takes a huge amount of time and practice.&lt;/p&gt;&lt;p&gt;MIT engineers are looking to ease CAD’s learning curve with an AI model that uses CAD software much like a human would. Given a 2D sketch of an object, the model quickly creates a 3D version by clicking buttons and file options, similar to how an engineer would use the software.&lt;/p&gt;&lt;p&gt;The MIT team has created a new dataset called VideoCAD, which contains more than 41,000 examples of how 3D models are built in CAD software. By learning from these videos, which illustrate how different shapes and objects are constructed step-by-step, the new AI system can now operate CAD software much like a human user.&lt;/p&gt;&lt;p&gt;With VideoCAD, the team is building toward an&amp;nbsp;AI-enabled “CAD co-pilot.” They envision that such a tool could not only create 3D versions of a design, but also work with a human user to suggest next steps, or automatically carry out build sequences that would otherwise be tedious and time-consuming to manually click through.&lt;/p&gt;&lt;p&gt;“There’s an opportunity for AI to increase engineers’ productivity as well as make CAD more accessible to more people,” says Ghadi Nehme, a graduate student in MIT’s Department of Mechanical Engineering.&lt;/p&gt;&lt;p&gt;“This is significant because it lowers the barrier to entry for design, helping people without years of CAD training to create 3D models more easily and tap into their creativity,” adds Faez Ahmed, associate professor of mechanical engineering at MIT.&lt;/p&gt;&lt;p&gt;Ahmed and Nehme, along with graduate student Brandon Man and postdoc Ferdous Alam, will present their work at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Click by click&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The team’s new work expands on recent developments in AI-driven user interface (UI) agents — tools that are trained to use software programs to carry out tasks, such as automatically gathering information online and organizing it in an Excel spreadsheet. Ahmed’s group wondered whether such UI agents could be designed to use CAD, which encompasses many more features and functions, and involves far more complicated tasks than the average UI agent can handle.&lt;/p&gt;&lt;p&gt;In their new work, the team aimed to design an AI-driven UI agent that takes the reins of the CAD program to create a 3D version of a 2D sketch, click by click. To do so,&amp;nbsp;the team first looked to an existing dataset of objects that were designed in CAD by humans. Each object in the dataset includes the sequence of high-level design commands, such as “sketch line,” “circle,” and “extrude,” that were used to build the final object.&lt;/p&gt;&lt;p&gt;However, the team realized that these high-level commands alone were not enough to train an AI agent to actually use CAD software. A real agent must also understand the details behind each action. For instance: Which sketch region should it select? When should it zoom in? And what part of a sketch should it extrude? To bridge this gap, the researchers developed a system to translate high-level commands into user-interface interactions.&lt;/p&gt;&lt;p&gt;“For example, let’s say we drew a sketch by drawing a line from point 1 to point 2,” Nehme says. “We translated those high-level actions to user-interface actions, meaning we say, go from this pixel location, click, and then move to a second pixel location, and click, while having the ‘line’ operation selected.”&lt;/p&gt;&lt;p&gt;In the end, the team generated over 41,000 videos of human-designed CAD objects, each of which is described in real-time in terms of the specific clicks, mouse-drags, and other keyboard actions that the human originally carried out. They then fed all this data into a model they developed to learn connections between UI actions and CAD object&amp;nbsp;generation.&lt;/p&gt;&lt;p&gt;Once trained on this dataset, which they dub VideoCAD, the new AI model could take a 2D sketch as input and directly control the CAD software, clicking, dragging, and selecting tools to construct the full 3D shape. The objects ranged in complexity from simple brackets to more complicated house designs. The team is training the model on more complex shapes and envisions that both the model and the dataset could one day enable CAD co-pilots for designers in a wide range of fields.&lt;/p&gt;&lt;p&gt;“VideoCAD is a valuable first step toward AI assistants that help onboard new users and automate the repetitive modeling work that follows familiar patterns,” says Mehdi Ataei, who was not involved in the study, and is a senior research scientist at Autodesk Research, which develops new design software tools. “This is an early foundation, and I would be excited to see successors that span multiple CAD systems, richer operations like assemblies and constraints, and more realistic, messy human workflows.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-ai-agent-learns-use-cad-create-3d-objects-sketches-1119</guid><pubDate>Wed, 19 Nov 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] TikTok now lets you choose how much AI-generated content you want to see (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/18/tiktok-now-lets-you-choose-how-much-ai-generated-content-you-want-to-see/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;TikTok, an app that was once just a place for user-generated content, is launching a new setting that lets users choose how much AI-generated content they want to see in their “For You” feed. The company is also introducing more advanced labeling technologies for AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI-generated content (AIGC) control is rolling out within the app’s “Manage Topics” tool, which lets users choose what they see on TikTok. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Manage Topics already enables people to adjust how often they see content related to over 10 categories like Dance, Sports, and Food &amp;amp; Drinks,” TikTok explained in a blog post. “Like those controls, the AIGC setting is intended to help people tailor the diverse range of content in their feed, rather than removing or replacing content in feeds entirely.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as companies like OpenAI and Meta are embracing AI-only feeds. In September, Meta released Vibes, a new feed for sharing and creating short AI-generated videos. A few days after Meta’s launch,&amp;nbsp;OpenAI released Sora, a social media platform for creating and sharing AI-generated videos. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Sora’s launch, realistic AI-generated videos have been posted to TikTok. Additionally, many TikTok users are leveraging AI to create visuals for posts about other topics, like history or celebrities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok says that with the new AI-generated content control, users who want to see less of this sort of content can now dial things down, while those who enjoy it can choose to see more of it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3068934" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-in-TikTok-Managed-Topics-toggle.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TikTok&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access the new capability by going into your Settings, selecting “Content Preferences” and then clicking the “Manage Topics” option.&amp;nbsp;Then, you can move the slider for different topics, including AI-generated content, to adjust how much you do or don’t want to see that sort of content in your For You feed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change is rolling out in the coming weeks, TikTok says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To improve its ability to label AI-generated content, TikTok is now testing a technology called “invisible watermarking.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok already requires people to label realistic AI-generated content and uses a cross-industry technology called Content Credentials from C2PA, which embeds metadata into content that lets it and other platforms know when something is AI-generated. However, TikTok notes that these labels can be removed when content is reuploaded or edited on other platforms.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With the new “invisible watermarks,” TikTok will add another layer of safeguards by using a watermark that only it can read. That means it’ll be harder for others to remove it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok will start adding invisible watermarks to AI-generated content made with TikTok tools like AI Editor Pro. It’s also adding them to content uploaded with C2PA’s Content Credentials. The company says these watermarks will help it label content more reliably. TikTok notes that it will continue reading C2PA’s Content Credentials and add them to AI-generated content made on its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Related to these efforts, TikTok also announced that it’s launching a $2 million AI literacy fund aimed at experts, like the nonprofit Girls Who Code, to create content that teaches people about AI literacy and safety.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;TikTok, an app that was once just a place for user-generated content, is launching a new setting that lets users choose how much AI-generated content they want to see in their “For You” feed. The company is also introducing more advanced labeling technologies for AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI-generated content (AIGC) control is rolling out within the app’s “Manage Topics” tool, which lets users choose what they see on TikTok. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Manage Topics already enables people to adjust how often they see content related to over 10 categories like Dance, Sports, and Food &amp;amp; Drinks,” TikTok explained in a blog post. “Like those controls, the AIGC setting is intended to help people tailor the diverse range of content in their feed, rather than removing or replacing content in feeds entirely.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as companies like OpenAI and Meta are embracing AI-only feeds. In September, Meta released Vibes, a new feed for sharing and creating short AI-generated videos. A few days after Meta’s launch,&amp;nbsp;OpenAI released Sora, a social media platform for creating and sharing AI-generated videos. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Sora’s launch, realistic AI-generated videos have been posted to TikTok. Additionally, many TikTok users are leveraging AI to create visuals for posts about other topics, like history or celebrities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok says that with the new AI-generated content control, users who want to see less of this sort of content can now dial things down, while those who enjoy it can choose to see more of it.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3068934" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-in-TikTok-Managed-Topics-toggle.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TikTok&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access the new capability by going into your Settings, selecting “Content Preferences” and then clicking the “Manage Topics” option.&amp;nbsp;Then, you can move the slider for different topics, including AI-generated content, to adjust how much you do or don’t want to see that sort of content in your For You feed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change is rolling out in the coming weeks, TikTok says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To improve its ability to label AI-generated content, TikTok is now testing a technology called “invisible watermarking.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok already requires people to label realistic AI-generated content and uses a cross-industry technology called Content Credentials from C2PA, which embeds metadata into content that lets it and other platforms know when something is AI-generated. However, TikTok notes that these labels can be removed when content is reuploaded or edited on other platforms.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With the new “invisible watermarks,” TikTok will add another layer of safeguards by using a watermark that only it can read. That means it’ll be harder for others to remove it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TikTok will start adding invisible watermarks to AI-generated content made with TikTok tools like AI Editor Pro. It’s also adding them to content uploaded with C2PA’s Content Credentials. The company says these watermarks will help it label content more reliably. TikTok notes that it will continue reading C2PA’s Content Credentials and add them to AI-generated content made on its platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Related to these efforts, TikTok also announced that it’s launching a $2 million AI literacy fund aimed at experts, like the nonprofit Girls Who Code, to create content that teaches people about AI literacy and safety.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/18/tiktok-now-lets-you-choose-how-much-ai-generated-content-you-want-to-see/</guid><pubDate>Wed, 19 Nov 2025 05:01:00 +0000</pubDate></item></channel></rss>