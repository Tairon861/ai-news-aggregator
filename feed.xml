<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 27 Jan 2026 02:03:48 +0000</lastBuildDate><item><title>EU launches formal investigation of xAI over Grok's sexualized deepfakes (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/01/eu-launches-formal-investigation-of-xai-over-groks-sexualized-deepfakes/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Elon Musk’s company faces fines of up to 6 percent of its daily turnover.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Photo illustration in which the logo of Grok 4 is displayed on a smartphone screen with the xAI logo in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/xai-grok-640x426.jpg" width="640" /&gt;
                  &lt;img alt="Photo illustration in which the logo of Grok 4 is displayed on a smartphone screen with the xAI logo in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/xai-grok-1152x648-1752596823.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | VCG 

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The EU has launched a formal investigation into Elon Musk’s xAI following a public outcry over how its Grok chatbot spread sexualized images of women and children.&lt;/p&gt;
&lt;p&gt;The billionaire entrepreneur has come under scrutiny from regulators around the world this month after people began using Grok to generate deepfakes of people without consent. The images were posted on the X social network as well as the separate Grok app, both of which are run by xAI.&lt;/p&gt;
&lt;p&gt;The probe, announced on Monday under the EU’s Digital Services Act, will assess if xAI tried to mitigate the risks of deploying Grok’s tools on X and the proliferation of content that “may amount to child sexual abuse material.”&lt;/p&gt;
&lt;p&gt;“Non-consensual sexual deepfakes of women and children are a violent, unacceptable form of degradation,” the EU’s tech chief, Henna Virkkunen, said.&lt;/p&gt;
&lt;p&gt;“With this investigation, we will determine whether X has met its legal obligations under the DSA, or whether it treated rights of European citizens—including those of women and children—as collateral damage of its service.”&lt;/p&gt;
&lt;p&gt;If the company is found to be in breach of the rules, the bloc can impose fines worth up to 6 percent of the worldwide annual turnover. An EU official said there will be no interim measures during the investigation.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The European probe comes after UK media regulator Ofcom opened a formal investigation into Grok, while Malaysia and Indonesia have banned the chatbot altogether.&lt;/p&gt;
&lt;p&gt;Following the backlash, xAI restricted the use of Grok to paying subscribers and said it has “implemented technological measures” to limit Grok from generating certain sexualized images.&lt;/p&gt;
&lt;p&gt;Musk has also said “anyone using Grok to make illegal content will suffer the same consequences as if they upload illegal content.”&lt;/p&gt;
&lt;p&gt;An EU official said that “with the harm that is exposed to individuals that are subject to these images, we have not been convinced so far by what mitigating measures the platform has taken to have that under control.”&lt;/p&gt;
&lt;p&gt;The company, which acquired Musk’s social media site X last year, has designed its AI products to have fewer content “guardrails” than competitors such as OpenAI and Google. Musk called its Grok model “maximally truth-seeking.”&lt;/p&gt;
&lt;p&gt;The commission fined X €120 million in December last year for breaching its regulations for transparency, providing insufficient access to data and the deceptive design of its blue ticks for verified accounts.&lt;/p&gt;
&lt;p&gt;The fine was criticized by Musk and the US government, with the Trump administration claiming the EU was unfairly targeting American groups and infringing freedom of speech principles championed by the Maga movement.&lt;/p&gt;
&lt;p&gt;X did not immediately reply to a request for comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2026 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Elon Musk’s company faces fines of up to 6 percent of its daily turnover.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Photo illustration in which the logo of Grok 4 is displayed on a smartphone screen with the xAI logo in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="426" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/xai-grok-640x426.jpg" width="640" /&gt;
                  &lt;img alt="Photo illustration in which the logo of Grok 4 is displayed on a smartphone screen with the xAI logo in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/xai-grok-1152x648-1752596823.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | VCG 

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The EU has launched a formal investigation into Elon Musk’s xAI following a public outcry over how its Grok chatbot spread sexualized images of women and children.&lt;/p&gt;
&lt;p&gt;The billionaire entrepreneur has come under scrutiny from regulators around the world this month after people began using Grok to generate deepfakes of people without consent. The images were posted on the X social network as well as the separate Grok app, both of which are run by xAI.&lt;/p&gt;
&lt;p&gt;The probe, announced on Monday under the EU’s Digital Services Act, will assess if xAI tried to mitigate the risks of deploying Grok’s tools on X and the proliferation of content that “may amount to child sexual abuse material.”&lt;/p&gt;
&lt;p&gt;“Non-consensual sexual deepfakes of women and children are a violent, unacceptable form of degradation,” the EU’s tech chief, Henna Virkkunen, said.&lt;/p&gt;
&lt;p&gt;“With this investigation, we will determine whether X has met its legal obligations under the DSA, or whether it treated rights of European citizens—including those of women and children—as collateral damage of its service.”&lt;/p&gt;
&lt;p&gt;If the company is found to be in breach of the rules, the bloc can impose fines worth up to 6 percent of the worldwide annual turnover. An EU official said there will be no interim measures during the investigation.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The European probe comes after UK media regulator Ofcom opened a formal investigation into Grok, while Malaysia and Indonesia have banned the chatbot altogether.&lt;/p&gt;
&lt;p&gt;Following the backlash, xAI restricted the use of Grok to paying subscribers and said it has “implemented technological measures” to limit Grok from generating certain sexualized images.&lt;/p&gt;
&lt;p&gt;Musk has also said “anyone using Grok to make illegal content will suffer the same consequences as if they upload illegal content.”&lt;/p&gt;
&lt;p&gt;An EU official said that “with the harm that is exposed to individuals that are subject to these images, we have not been convinced so far by what mitigating measures the platform has taken to have that under control.”&lt;/p&gt;
&lt;p&gt;The company, which acquired Musk’s social media site X last year, has designed its AI products to have fewer content “guardrails” than competitors such as OpenAI and Google. Musk called its Grok model “maximally truth-seeking.”&lt;/p&gt;
&lt;p&gt;The commission fined X €120 million in December last year for breaching its regulations for transparency, providing insufficient access to data and the deceptive design of its blue ticks for verified accounts.&lt;/p&gt;
&lt;p&gt;The fine was criticized by Musk and the US government, with the Trump administration claiming the EU was unfairly targeting American groups and infringing freedom of speech principles championed by the Maga movement.&lt;/p&gt;
&lt;p&gt;X did not immediately reply to a request for comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2026 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/01/eu-launches-formal-investigation-of-xai-over-groks-sexualized-deepfakes/</guid><pubDate>Mon, 26 Jan 2026 14:17:46 +0000</pubDate></item><item><title>How Formula E uses Google Cloud AI to meet net zero targets (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-formula-e-uses-google-cloud-ai-to-meet-net-zero-targets/</link><description>&lt;p&gt;Formula E is using Google Cloud AI to meet its net zero targets by driving efficiency across its global logistics and commercial operations. As part of an expanded multi-year agreement, the electric racing series will integrate Gemini models into its ecosystem to support performance analysis, back-office workflows, and event logistics.&lt;/p&gt;&lt;p&gt;The collaboration demonstrates how sports organisations are utilising cloud infrastructure to drive tangible business outcomes, rather than just securing surface-level sponsorship. The partnership focuses on optimising business operations, ranging from race management to the fan experience.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operational-twins-and-carbon-data-to-achieve-net-zero-targets"&gt;Operational twins and carbon data to achieve net zero targets&lt;/h3&gt;&lt;p&gt;While marketing visibility often drives sports partnerships, this agreement builds on a technical foundation first formalised in January 2025. The elevation to “Principal Partner” involves Formula E adopting Google Cloud technologies for business-critical functions.&lt;/p&gt;&lt;p&gt;The immediate application involves optimising the complex logistics of a global championship. Advanced AI modelling of the back office and the creation of race and event digital twins allow the organisation to simulate and optimise site builds virtually.&lt;/p&gt;&lt;p&gt;This application directly affects Scope 3 emissions. The capability to plan infrastructure virtually minimises the need for physical on-site reconnaissance and reduces the transport of heavy equipment.&lt;/p&gt;&lt;p&gt;For a championship that is the only sport-certified net zero carbon entity since inception, maintaining this status requires finding efficiencies in the supply chain. The digital twin approach delivers a quantifiable reduction in the operational carbon footprint while maintaining performance.&lt;/p&gt;&lt;p&gt;Beyond logistical modelling, the Google Cloud AI partnership extends into the workforce productivity layer. Formula E is deploying Google Workspace with Gemini AI to enable greater agility and efficiency across its organisation.&lt;/p&gt;&lt;p&gt;The organisation intends to use these tools to accelerate performance and deliver faster operations. This reflects a broader trend where generative AI tools are provisioned to reduce administrative latency in distributed workforces.&lt;/p&gt;&lt;p&gt;The viability of these implementations to achieve net zero targets is supported by previous collaborative projects. Formula E recently utilised Google’s AI Studio and Gemini models to execute the ‘Mountain Recharge’ initiative.&lt;/p&gt;&lt;p&gt;Engineers used the models to map an optimal route for the GENBETA car during a mountain descent. The AI identified and analysed specific braking zones, calculating the necessary regenerative braking required to harvest enough energy to complete a full lap of the Monaco circuit subsequently.&lt;/p&gt;&lt;p&gt;This specific use case demonstrates how high-dimensional data – including topography, friction, and energy consumption – can be processed to define physical execution.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-using-google-cloud-ai-to-enhance-formula-e-s-data-product"&gt;Using Google Cloud AI to enhance Formula E’s data product&lt;/h3&gt;&lt;p&gt;The partnership also addresses the commercial requirement to retain and grow a digital audience. Formula E has integrated a ‘Strategy Agent’ into its live broadcasts. This tool processes real-time data to provide viewers with tailored insights and predictions regarding race strategy and driver performance.&lt;/p&gt;&lt;p&gt;Millions of viewers have utilised these insights, which explain complex race dynamics as they unfold. This mirrors the enterprise challenge of observability (i.e. taking vast streams of real-time technical data and synthesising them into understandable narratives for stakeholders.)&lt;/p&gt;&lt;p&gt;Beyond helping to achieve net zero targets, the leadership at both organisations frames this expansion as a necessary evolution of their technical stack.&lt;/p&gt;&lt;p&gt;Jeff Dodds, CEO of Formula E, said: “Our expanded partnership with Google Cloud is a true game-changer for Formula E and for motorsport as a whole. We are already pushing the boundaries of technology in sport, and this Principal Partnership confirms our vision.&lt;/p&gt;&lt;p&gt;“The integration of Google Cloud’s AI capabilities will unlock a new dimension of real-time performance optimisation and strategic decision-making, both for the Championship and for our global broadcast audience. This collaboration will redefine how fans experience our races and set a new benchmark for technology integration in sport worldwide.”&lt;/p&gt;&lt;p&gt;Tara Brady, President of Google Cloud EMEA, added: “Formula E is a hub of innovation, where milliseconds can define success. This expanded partnership is a testament to the power of Google Cloud’s AI and data analytics, showing how our technology can deliver a competitive advantage in the most demanding scenarios.”&lt;/p&gt;&lt;p&gt;The progression from the initial partnership in January 2025 to this expanded scope suggests the pilot programs provided sufficient ROI to warrant a broader rollout. As organisations face pressure to balance performance with net zero targets, the use of virtual simulation to optimise physical deployment remains a high-value area for investment.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Controlling AI agent sprawl: The CIO’s guide to governance&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111551" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Formula E is using Google Cloud AI to meet its net zero targets by driving efficiency across its global logistics and commercial operations. As part of an expanded multi-year agreement, the electric racing series will integrate Gemini models into its ecosystem to support performance analysis, back-office workflows, and event logistics.&lt;/p&gt;&lt;p&gt;The collaboration demonstrates how sports organisations are utilising cloud infrastructure to drive tangible business outcomes, rather than just securing surface-level sponsorship. The partnership focuses on optimising business operations, ranging from race management to the fan experience.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operational-twins-and-carbon-data-to-achieve-net-zero-targets"&gt;Operational twins and carbon data to achieve net zero targets&lt;/h3&gt;&lt;p&gt;While marketing visibility often drives sports partnerships, this agreement builds on a technical foundation first formalised in January 2025. The elevation to “Principal Partner” involves Formula E adopting Google Cloud technologies for business-critical functions.&lt;/p&gt;&lt;p&gt;The immediate application involves optimising the complex logistics of a global championship. Advanced AI modelling of the back office and the creation of race and event digital twins allow the organisation to simulate and optimise site builds virtually.&lt;/p&gt;&lt;p&gt;This application directly affects Scope 3 emissions. The capability to plan infrastructure virtually minimises the need for physical on-site reconnaissance and reduces the transport of heavy equipment.&lt;/p&gt;&lt;p&gt;For a championship that is the only sport-certified net zero carbon entity since inception, maintaining this status requires finding efficiencies in the supply chain. The digital twin approach delivers a quantifiable reduction in the operational carbon footprint while maintaining performance.&lt;/p&gt;&lt;p&gt;Beyond logistical modelling, the Google Cloud AI partnership extends into the workforce productivity layer. Formula E is deploying Google Workspace with Gemini AI to enable greater agility and efficiency across its organisation.&lt;/p&gt;&lt;p&gt;The organisation intends to use these tools to accelerate performance and deliver faster operations. This reflects a broader trend where generative AI tools are provisioned to reduce administrative latency in distributed workforces.&lt;/p&gt;&lt;p&gt;The viability of these implementations to achieve net zero targets is supported by previous collaborative projects. Formula E recently utilised Google’s AI Studio and Gemini models to execute the ‘Mountain Recharge’ initiative.&lt;/p&gt;&lt;p&gt;Engineers used the models to map an optimal route for the GENBETA car during a mountain descent. The AI identified and analysed specific braking zones, calculating the necessary regenerative braking required to harvest enough energy to complete a full lap of the Monaco circuit subsequently.&lt;/p&gt;&lt;p&gt;This specific use case demonstrates how high-dimensional data – including topography, friction, and energy consumption – can be processed to define physical execution.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-using-google-cloud-ai-to-enhance-formula-e-s-data-product"&gt;Using Google Cloud AI to enhance Formula E’s data product&lt;/h3&gt;&lt;p&gt;The partnership also addresses the commercial requirement to retain and grow a digital audience. Formula E has integrated a ‘Strategy Agent’ into its live broadcasts. This tool processes real-time data to provide viewers with tailored insights and predictions regarding race strategy and driver performance.&lt;/p&gt;&lt;p&gt;Millions of viewers have utilised these insights, which explain complex race dynamics as they unfold. This mirrors the enterprise challenge of observability (i.e. taking vast streams of real-time technical data and synthesising them into understandable narratives for stakeholders.)&lt;/p&gt;&lt;p&gt;Beyond helping to achieve net zero targets, the leadership at both organisations frames this expansion as a necessary evolution of their technical stack.&lt;/p&gt;&lt;p&gt;Jeff Dodds, CEO of Formula E, said: “Our expanded partnership with Google Cloud is a true game-changer for Formula E and for motorsport as a whole. We are already pushing the boundaries of technology in sport, and this Principal Partnership confirms our vision.&lt;/p&gt;&lt;p&gt;“The integration of Google Cloud’s AI capabilities will unlock a new dimension of real-time performance optimisation and strategic decision-making, both for the Championship and for our global broadcast audience. This collaboration will redefine how fans experience our races and set a new benchmark for technology integration in sport worldwide.”&lt;/p&gt;&lt;p&gt;Tara Brady, President of Google Cloud EMEA, added: “Formula E is a hub of innovation, where milliseconds can define success. This expanded partnership is a testament to the power of Google Cloud’s AI and data analytics, showing how our technology can deliver a competitive advantage in the most demanding scenarios.”&lt;/p&gt;&lt;p&gt;The progression from the initial partnership in January 2025 to this expanded scope suggests the pilot programs provided sufficient ROI to warrant a broader rollout. As organisations face pressure to balance performance with net zero targets, the use of virtual simulation to optimise physical deployment remains a high-value area for investment.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Controlling AI agent sprawl: The CIO’s guide to governance&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111551" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-formula-e-uses-google-cloud-ai-to-meet-net-zero-targets/</guid><pubDate>Mon, 26 Jan 2026 14:38:53 +0000</pubDate></item><item><title>**NVIDIA Earth-2 Open Models Span the Whole Weather Stack** (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/earth-2-open-models</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-uploads.huggingface.co/production/uploads/69767cbee0e9addbc199b9c5/ulQKMbGq2yyxdMHw51j3W.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
NVIDIA is excited to announce three new open-source models as part of the NVIDIA Earth-2 family, making it easier than ever to build weather forecasting capabilities across the weather stack, including tasks such as data assimilation, forecasting, nowcasting, downscaling and more. In addition, developers can quickly get started building weather and climate simulations by using NVIDIA open source software: Earth2Studio for creating inference pipelines and Physics Nemo for training models.
&lt;p&gt;NVIDIA Earth-2 comprises a set of accelerated tools and models which enables developers to bring together typically disparate weather and climate AI capabilities. Because Earth-2 is completely open, developers can customize and fine-tune their simulations to their specific needs, using their own data and their own infrastructure to build sovereign weather and climate predictions they fully own and control. Earth-2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is a suite of leading open weather and climate models   &lt;/li&gt;
&lt;li&gt;Is easy-to-use thanks to an ecosystem of open source software  &lt;/li&gt;
&lt;li&gt;Enables you to create your own sovereign capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Earth-2 Nowcasting: Kilometer-Scale Severe Weather Prediction&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Out now on Hugging Face: Earth-2 Nowcasting, powered by a new model architecture called StormScope,  using generative AI to make country-scale forecasts into kilometer‑resolution, zero- to six-hour predictions of local storms and hazardous weather in just minutes. Earth-2 Nowcasting can generate the first predictions that outperform traditional, physics-based weather-prediction models on short-term precipitation forecasting by simulating storm dynamics directly. It harnesses AI to directly predict satellite and radar data.&lt;/p&gt;
&lt;p&gt;This version is trained directly on globally available geostationary satellite observations (GOES) over the contiguous US (CONUS). However, this method could be applied to train versions of the model over other regions with similar satellite coverage.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/MikePritchard/Media-Jan2026-Earth2Launch/resolve/main/Earth-2%20NowCasting%20StormScope%20Visible%20and%20Radar%20CONUS%201080p.mov" type="video/mp4" /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Research Paper: Learning Accurate Storm-Scale Evolution from Observations&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Earth-2 Medium Range: Highly accurate 15-Day Global Forecasts&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Out now on Hugging Face: Earth-2 Medium Range, powered by a new model architecture called Atlas, enabling high-accuracy weather prediction for medium-range forecasts — or forecasts of up to 15 days in advance — across 70+ weather variables including temperature, pressure, wind and humidity.  It uses a latent diffusion transformer architecture to predict incremental changes in the atmosphere so as to preserve critical atmospheric structures and reduce forecasting errors. On standard benchmarks, it outperforms leading open models such as GenCast on the most common forecasting variables measured by the industry.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/MikePritchard/Media-Jan2026-Earth2Launch/resolve/main/Earth-2%20Medium%20Range%20Forecast%20Ensembles%201080p.mov" type="video/mp4" /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Research Paper: Demystifying Data-Driven Probabilistic Medium-Range
Weather Forecasting&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Earth-2 Global Data Assimilation: An End-to-End AI Pipeline&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Coming soon to Hugging Face: Earth-2 Global Data Assimilation, powered by a new model architecture called HealDA, which produces initial conditions for weather prediction — snapshots of the current atmosphere, including the temperature, wind speed, humidity and air pressure, at thousands of locations around the globe. Earth-2 Global Data Assimilation can generate initial conditions in seconds on GPUs instead of hours on supercomputers. When coupled with Earth-2 Medium Range, this results in the most skillful forecasting predictions produced by an open, entirely AI pipeline. &lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/MikePritchard/Media-Jan2026-Earth2Launch/resolve/main/Earth-2%20GlobalDataAssimilation_CombinedRotate-1080p.mov" type="video/mp4" /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Research Paper: HealDA: Highlighting the Importance of Initial Errors in
End-to-End AI Weather Forecasts&lt;/p&gt;
&lt;p&gt;These models join established open NVIDIA weather and climate models such as FourcastNet3, CorrDiff, cBottle, DLESym and more.&lt;/p&gt;

&lt;p&gt;NVIDIA Earth2Studio is an open-source Python ecosystem for quickly creating powerful AI weather and climate simulations. It provides all the necessary inference tools to get started with the new model checkpoints on Hugging Face. It’s as easy as:&lt;/p&gt;
&lt;p&gt;Getting Started Video&lt;/p&gt;

&lt;p&gt;Corporate Blog: NVIDIA Launches Earth-2 Family of Open Models — the World’s First Fully Open Set of Models and Tools for AI Weather&lt;/p&gt;
&lt;p&gt;Launch Video: NVIDIA Earth-2: The Future of AI Weather Forecasting is Open&lt;/p&gt;
&lt;p&gt;Web Page: Earth-2&lt;/p&gt;
&lt;p&gt;Hugging Face Package for Earth-2 Nowcasting&lt;/p&gt;
&lt;p&gt;Research Paper: Learning Accurate Storm-Scale Evolution from Observations&lt;/p&gt;
&lt;p&gt;Hugging Face Package for Earth-2 Medium-Range&lt;/p&gt;
&lt;p&gt;Research Paper: Demystifying Data-Driven Probabilistic Medium-Range
Weather Forecasting&lt;/p&gt;
&lt;p&gt;Research Paper: HealDA: Highlighting the Importance of Initial Errors in
End-to-End AI Weather Forecasts&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-uploads.huggingface.co/production/uploads/69767cbee0e9addbc199b9c5/ulQKMbGq2yyxdMHw51j3W.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
NVIDIA is excited to announce three new open-source models as part of the NVIDIA Earth-2 family, making it easier than ever to build weather forecasting capabilities across the weather stack, including tasks such as data assimilation, forecasting, nowcasting, downscaling and more. In addition, developers can quickly get started building weather and climate simulations by using NVIDIA open source software: Earth2Studio for creating inference pipelines and Physics Nemo for training models.
&lt;p&gt;NVIDIA Earth-2 comprises a set of accelerated tools and models which enables developers to bring together typically disparate weather and climate AI capabilities. Because Earth-2 is completely open, developers can customize and fine-tune their simulations to their specific needs, using their own data and their own infrastructure to build sovereign weather and climate predictions they fully own and control. Earth-2:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Is a suite of leading open weather and climate models   &lt;/li&gt;
&lt;li&gt;Is easy-to-use thanks to an ecosystem of open source software  &lt;/li&gt;
&lt;li&gt;Enables you to create your own sovereign capabilities&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Earth-2 Nowcasting: Kilometer-Scale Severe Weather Prediction&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Out now on Hugging Face: Earth-2 Nowcasting, powered by a new model architecture called StormScope,  using generative AI to make country-scale forecasts into kilometer‑resolution, zero- to six-hour predictions of local storms and hazardous weather in just minutes. Earth-2 Nowcasting can generate the first predictions that outperform traditional, physics-based weather-prediction models on short-term precipitation forecasting by simulating storm dynamics directly. It harnesses AI to directly predict satellite and radar data.&lt;/p&gt;
&lt;p&gt;This version is trained directly on globally available geostationary satellite observations (GOES) over the contiguous US (CONUS). However, this method could be applied to train versions of the model over other regions with similar satellite coverage.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/MikePritchard/Media-Jan2026-Earth2Launch/resolve/main/Earth-2%20NowCasting%20StormScope%20Visible%20and%20Radar%20CONUS%201080p.mov" type="video/mp4" /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Research Paper: Learning Accurate Storm-Scale Evolution from Observations&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Earth-2 Medium Range: Highly accurate 15-Day Global Forecasts&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Out now on Hugging Face: Earth-2 Medium Range, powered by a new model architecture called Atlas, enabling high-accuracy weather prediction for medium-range forecasts — or forecasts of up to 15 days in advance — across 70+ weather variables including temperature, pressure, wind and humidity.  It uses a latent diffusion transformer architecture to predict incremental changes in the atmosphere so as to preserve critical atmospheric structures and reduce forecasting errors. On standard benchmarks, it outperforms leading open models such as GenCast on the most common forecasting variables measured by the industry.&lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/MikePritchard/Media-Jan2026-Earth2Launch/resolve/main/Earth-2%20Medium%20Range%20Forecast%20Ensembles%201080p.mov" type="video/mp4" /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Research Paper: Demystifying Data-Driven Probabilistic Medium-Range
Weather Forecasting&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Earth-2 Global Data Assimilation: An End-to-End AI Pipeline&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Coming soon to Hugging Face: Earth-2 Global Data Assimilation, powered by a new model architecture called HealDA, which produces initial conditions for weather prediction — snapshots of the current atmosphere, including the temperature, wind speed, humidity and air pressure, at thousands of locations around the globe. Earth-2 Global Data Assimilation can generate initial conditions in seconds on GPUs instead of hours on supercomputers. When coupled with Earth-2 Medium Range, this results in the most skillful forecasting predictions produced by an open, entirely AI pipeline. &lt;/p&gt;
&lt;video controls="controls" loop="loop"&gt;
  &lt;source src="https://huggingface.co/datasets/MikePritchard/Media-Jan2026-Earth2Launch/resolve/main/Earth-2%20GlobalDataAssimilation_CombinedRotate-1080p.mov" type="video/mp4" /&gt;
  Your browser does not support the video tag.
&lt;/video&gt;

&lt;p&gt;Research Paper: HealDA: Highlighting the Importance of Initial Errors in
End-to-End AI Weather Forecasts&lt;/p&gt;
&lt;p&gt;These models join established open NVIDIA weather and climate models such as FourcastNet3, CorrDiff, cBottle, DLESym and more.&lt;/p&gt;

&lt;p&gt;NVIDIA Earth2Studio is an open-source Python ecosystem for quickly creating powerful AI weather and climate simulations. It provides all the necessary inference tools to get started with the new model checkpoints on Hugging Face. It’s as easy as:&lt;/p&gt;
&lt;p&gt;Getting Started Video&lt;/p&gt;

&lt;p&gt;Corporate Blog: NVIDIA Launches Earth-2 Family of Open Models — the World’s First Fully Open Set of Models and Tools for AI Weather&lt;/p&gt;
&lt;p&gt;Launch Video: NVIDIA Earth-2: The Future of AI Weather Forecasting is Open&lt;/p&gt;
&lt;p&gt;Web Page: Earth-2&lt;/p&gt;
&lt;p&gt;Hugging Face Package for Earth-2 Nowcasting&lt;/p&gt;
&lt;p&gt;Research Paper: Learning Accurate Storm-Scale Evolution from Observations&lt;/p&gt;
&lt;p&gt;Hugging Face Package for Earth-2 Medium-Range&lt;/p&gt;
&lt;p&gt;Research Paper: Demystifying Data-Driven Probabilistic Medium-Range
Weather Forecasting&lt;/p&gt;
&lt;p&gt;Research Paper: HealDA: Highlighting the Importance of Initial Errors in
End-to-End AI Weather Forecasts&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/earth-2-open-models</guid><pubDate>Mon, 26 Jan 2026 14:53:45 +0000</pubDate></item><item><title>Only 5 days left: Over half of the first 500 TechCrunch Disrupt 2026 plus-one passes at 50% off are already gone (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/only-5-days-left-over-half-of-the-first-500-techcrunch-disrupt-2026-plus-one-passes-at-50-off-are-already-gone/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The clock is officially ticking.&amp;nbsp;In just&amp;nbsp;5 days, the lowest ticket prices for &lt;strong&gt;TechCrunch Disrupt 2026&lt;/strong&gt;&amp;nbsp;— plus the exclusive&amp;nbsp;50% off plus-one pass for the first 500 registrations&amp;nbsp;— will be gone. And with&amp;nbsp;more than half of those first 500 already claimed, this window is closing fast. If Disrupt has been on your must-attend list, this is your moment to lock in the best deal and bring a plus-one for half the price.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to&amp;nbsp;save&amp;nbsp;&lt;strong&gt;up to $680 on your pass&lt;/strong&gt;&amp;nbsp;and get a second ticket at&amp;nbsp;&lt;strong&gt;50% off&lt;/strong&gt;. This offer&amp;nbsp;ends&amp;nbsp;&lt;strong&gt;January 30&lt;/strong&gt;,&amp;nbsp;or&amp;nbsp;the moment the first 500 tickets are claimed. Whichever comes first.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2022" class="wp-image-2540398" height="427" src="https://techcrunch.com/wp-content/uploads/2023/05/disrupt-expo-hall-crop.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-disrupt"&gt;Why Disrupt?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;From October 13–15,&amp;nbsp;San Francisco’s&amp;nbsp;Moscone West will become the global epicenter of tech.&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt&lt;/strong&gt;&amp;nbsp;is a curated, three-day experience designed to maximize signal over noise — uniting 10,000&amp;nbsp;founders,&amp;nbsp;investors, operators, and tech leaders for 200+ expert-led sessions featuring 250+ influential voices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Explore&amp;nbsp;what’s&amp;nbsp;next as&amp;nbsp;300+ startups debut their breakthroughs, experience the high-stakes energy of&amp;nbsp;Startup Battlefield 200, and tap into&amp;nbsp;curated, high-impact networking&amp;nbsp;with the leaders shaping the future of tech.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="matt-mullenweg-bling-watch" class="wp-image-2908808" height="453" src="https://techcrunch.com/wp-content/uploads/2024/10/matt-mullenweg-bling-watch.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Past speakers have included some of the industry’s greatest minds:&lt;/p&gt;



























&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2428022" height="454" src="https://techcrunch.com/wp-content/uploads/2022/10/TechCrunch-Disrupt-Haje-Kamps-500.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Haje Kamps&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-a-more-curated-way-to-experience-disrupt-nbsp"&gt;A more curated way to experience Disrupt&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt&amp;nbsp;isn’t&amp;nbsp;about wandering between sessions.&amp;nbsp;It’s&amp;nbsp;about &lt;strong&gt;intentional connections and curated experiences&lt;/strong&gt; designed for how you&amp;nbsp;actually work&amp;nbsp;in tech. Founders connect directly with investors. VCs cut through the noise to discover startups aligned with their theses. Operators exchange real-world insights on building, scaling, and shipping&amp;nbsp;what’s&amp;nbsp;next. If&amp;nbsp;you’re&amp;nbsp;hands-on in tech, Disrupt was built with you in mind.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Explore ticket options to find the right fit for you, your partner, and your team.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-founders-and-investors-unique-benefits"&gt;Founders and investors’ unique benefits&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Founders and investors can unlock specialized passes designed to support your goals:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Founder Pass&lt;/strong&gt;:&amp;nbsp;Get the tools, insights, and connections you need to accelerate your&amp;nbsp;startup’s&amp;nbsp;growth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Investor Pass&lt;/strong&gt;:&amp;nbsp;Access curated opportunities to connect with standout startups and expand your portfolio. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-time-is-running-out-on-the-plus-one-deal"&gt;Time is running out on the plus-one deal&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;More than&amp;nbsp;half of the first 500 plus-one passes are already gone, and this offer ends in&amp;nbsp;just 5 days.&amp;nbsp;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to&amp;nbsp;save&amp;nbsp;&lt;strong&gt;up to $680&lt;/strong&gt;&amp;nbsp;on your Disrupt ticket and bring a plus-one at&amp;nbsp;&lt;strong&gt;50% off&lt;/strong&gt;&amp;nbsp;while discounted passes&amp;nbsp;remain.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The clock is officially ticking.&amp;nbsp;In just&amp;nbsp;5 days, the lowest ticket prices for &lt;strong&gt;TechCrunch Disrupt 2026&lt;/strong&gt;&amp;nbsp;— plus the exclusive&amp;nbsp;50% off plus-one pass for the first 500 registrations&amp;nbsp;— will be gone. And with&amp;nbsp;more than half of those first 500 already claimed, this window is closing fast. If Disrupt has been on your must-attend list, this is your moment to lock in the best deal and bring a plus-one for half the price.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to&amp;nbsp;save&amp;nbsp;&lt;strong&gt;up to $680 on your pass&lt;/strong&gt;&amp;nbsp;and get a second ticket at&amp;nbsp;&lt;strong&gt;50% off&lt;/strong&gt;. This offer&amp;nbsp;ends&amp;nbsp;&lt;strong&gt;January 30&lt;/strong&gt;,&amp;nbsp;or&amp;nbsp;the moment the first 500 tickets are claimed. Whichever comes first.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2022" class="wp-image-2540398" height="427" src="https://techcrunch.com/wp-content/uploads/2023/05/disrupt-expo-hall-crop.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-disrupt"&gt;Why Disrupt?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;From October 13–15,&amp;nbsp;San Francisco’s&amp;nbsp;Moscone West will become the global epicenter of tech.&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt&lt;/strong&gt;&amp;nbsp;is a curated, three-day experience designed to maximize signal over noise — uniting 10,000&amp;nbsp;founders,&amp;nbsp;investors, operators, and tech leaders for 200+ expert-led sessions featuring 250+ influential voices.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Explore&amp;nbsp;what’s&amp;nbsp;next as&amp;nbsp;300+ startups debut their breakthroughs, experience the high-stakes energy of&amp;nbsp;Startup Battlefield 200, and tap into&amp;nbsp;curated, high-impact networking&amp;nbsp;with the leaders shaping the future of tech.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="matt-mullenweg-bling-watch" class="wp-image-2908808" height="453" src="https://techcrunch.com/wp-content/uploads/2024/10/matt-mullenweg-bling-watch.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Past speakers have included some of the industry’s greatest minds:&lt;/p&gt;



























&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2428022" height="454" src="https://techcrunch.com/wp-content/uploads/2022/10/TechCrunch-Disrupt-Haje-Kamps-500.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Haje Kamps&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-a-more-curated-way-to-experience-disrupt-nbsp"&gt;A more curated way to experience Disrupt&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt&amp;nbsp;isn’t&amp;nbsp;about wandering between sessions.&amp;nbsp;It’s&amp;nbsp;about &lt;strong&gt;intentional connections and curated experiences&lt;/strong&gt; designed for how you&amp;nbsp;actually work&amp;nbsp;in tech. Founders connect directly with investors. VCs cut through the noise to discover startups aligned with their theses. Operators exchange real-world insights on building, scaling, and shipping&amp;nbsp;what’s&amp;nbsp;next. If&amp;nbsp;you’re&amp;nbsp;hands-on in tech, Disrupt was built with you in mind.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Explore ticket options to find the right fit for you, your partner, and your team.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-founders-and-investors-unique-benefits"&gt;Founders and investors’ unique benefits&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Founders and investors can unlock specialized passes designed to support your goals:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Founder Pass&lt;/strong&gt;:&amp;nbsp;Get the tools, insights, and connections you need to accelerate your&amp;nbsp;startup’s&amp;nbsp;growth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Investor Pass&lt;/strong&gt;:&amp;nbsp;Access curated opportunities to connect with standout startups and expand your portfolio. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-time-is-running-out-on-the-plus-one-deal"&gt;Time is running out on the plus-one deal&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;More than&amp;nbsp;half of the first 500 plus-one passes are already gone, and this offer ends in&amp;nbsp;just 5 days.&amp;nbsp;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to&amp;nbsp;save&amp;nbsp;&lt;strong&gt;up to $680&lt;/strong&gt;&amp;nbsp;on your Disrupt ticket and bring a plus-one at&amp;nbsp;&lt;strong&gt;50% off&lt;/strong&gt;&amp;nbsp;while discounted passes&amp;nbsp;remain.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/only-5-days-left-over-half-of-the-first-500-techcrunch-disrupt-2026-plus-one-passes-at-50-off-are-already-gone/</guid><pubDate>Mon, 26 Jan 2026 15:00:00 +0000</pubDate></item><item><title>Expereo: Enterprise connectivity amid AI surge with ‘visibility at the speed of life’ (AI News)</title><link>https://www.artificialintelligence-news.com/news/expereo-enterprise-connectivity-amid-ai-surge-with-visibility-at-the-speed-of-life/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/pexels-pixabay-248747-2-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;AI continues to reshape technology and business; yet for the network, enterprise connectivity in the AI age means being always-on, and extra vigilant for sovereignty and security besides.&lt;/p&gt;&lt;p&gt;This means that speed is not the only requirement. As Julian Skeels, chief digital officer at Expereo notes, it is more about ‘certainty.’ “AI workloads are distributed, they’re continuous, they’re incredibly latency-sensitive. Inference, monitoring, retrieval and remediation never stop, so that changes the network’s role,” says Skeels.&lt;/p&gt;&lt;p&gt;“In the world of AI, networking actually becomes a system dependency,” he adds. “When the network degrades, the application degrades immediately.&lt;/p&gt;&lt;p&gt;“An AI-ready network needs to make data movement deterministic. It’s not just about it being fast; it’s about it being predictable, and observable, and governable, and resilient – and to do all those things under continual change.”&lt;/p&gt;&lt;p&gt;Many CIOs, however, are struggling right now with what Skeels describes as ‘connectivity everywhere but visibility nowhere.’&lt;/p&gt;&lt;p&gt;“They’re dealing with hybrid networks, multiple clouds, multiple providers and portals that create a constant operational drag to their teams,” says Skeels. “What they want is clarity and control – not more tools.”&lt;/p&gt;&lt;p&gt;Skeels arrived at Expereo last year with myriad cross-industry experience in product and digital transformation initiatives under his belt. He found an industry ripe for accelerative change, and a company determined to lead the way and ensure pricing global connectivity should take minutes rather than weeks.&lt;/p&gt;&lt;p&gt;“When I came to Expereo, I saw that global connectivity has, I would say, largely resisted real digital transformation for a long time,” notes Skeels. “Most customers will still experience it as slow, and manual, and opaque, and fragmented across the dozens of providers and portals they need to work with.&lt;/p&gt;&lt;p&gt;“We believe, though, that with emerging technologies such as agentic AI, that’s finally changing,” adds Skeels. “Our ambition here is to make global connectivity as simple, and immediate, and transparent as cloud computing is for our customers.”&lt;/p&gt;&lt;p&gt;Enabling such change for customers requires that mix of speed and visibility – and this is where the expereoOne platform comes in, to provide what the company calls ‘visibility at the speed of life’ and give customers a single, global view of what is being deployed, how it is performing, and what it costs. Beyond visibility, customers also need proactivity, as Skeels explains. “We’re deeply integrated into our customers’ order management, their ITSM, their ERP systems, which makes working with Expereo at scale absolutely seamless,” he says.&lt;/p&gt;&lt;p&gt;“The key point is that better visibility isn’t about more dashboards. It’s about connecting network behaviour to their business outcomes in terms of resilience, security experience, and cost.”&lt;/p&gt;&lt;p&gt;Skeels is speaking at the Digital Transformation Expo Global on February 4-5 around designing the AI-ready network – and his session promises to subvert the usual advice for those in attendance. “I want to challenge a few things,” notes Skeels. “I want to ask people to consider even unlearning things they’ve learned in the past.&lt;/p&gt;&lt;p&gt;“A lot of what we’ve taken for granted about networks no longer holds in an AI world.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;Watch the full conversation between Julian Skeels and TechEx’s James Bourne below:&lt;/em&gt;&lt;/p&gt;&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" height="1080" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Julian_Skeels_Expereo_19_01.mp4" width="1920"&gt;&lt;/video&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Photo by Pixabay&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/pexels-pixabay-248747-2-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;AI continues to reshape technology and business; yet for the network, enterprise connectivity in the AI age means being always-on, and extra vigilant for sovereignty and security besides.&lt;/p&gt;&lt;p&gt;This means that speed is not the only requirement. As Julian Skeels, chief digital officer at Expereo notes, it is more about ‘certainty.’ “AI workloads are distributed, they’re continuous, they’re incredibly latency-sensitive. Inference, monitoring, retrieval and remediation never stop, so that changes the network’s role,” says Skeels.&lt;/p&gt;&lt;p&gt;“In the world of AI, networking actually becomes a system dependency,” he adds. “When the network degrades, the application degrades immediately.&lt;/p&gt;&lt;p&gt;“An AI-ready network needs to make data movement deterministic. It’s not just about it being fast; it’s about it being predictable, and observable, and governable, and resilient – and to do all those things under continual change.”&lt;/p&gt;&lt;p&gt;Many CIOs, however, are struggling right now with what Skeels describes as ‘connectivity everywhere but visibility nowhere.’&lt;/p&gt;&lt;p&gt;“They’re dealing with hybrid networks, multiple clouds, multiple providers and portals that create a constant operational drag to their teams,” says Skeels. “What they want is clarity and control – not more tools.”&lt;/p&gt;&lt;p&gt;Skeels arrived at Expereo last year with myriad cross-industry experience in product and digital transformation initiatives under his belt. He found an industry ripe for accelerative change, and a company determined to lead the way and ensure pricing global connectivity should take minutes rather than weeks.&lt;/p&gt;&lt;p&gt;“When I came to Expereo, I saw that global connectivity has, I would say, largely resisted real digital transformation for a long time,” notes Skeels. “Most customers will still experience it as slow, and manual, and opaque, and fragmented across the dozens of providers and portals they need to work with.&lt;/p&gt;&lt;p&gt;“We believe, though, that with emerging technologies such as agentic AI, that’s finally changing,” adds Skeels. “Our ambition here is to make global connectivity as simple, and immediate, and transparent as cloud computing is for our customers.”&lt;/p&gt;&lt;p&gt;Enabling such change for customers requires that mix of speed and visibility – and this is where the expereoOne platform comes in, to provide what the company calls ‘visibility at the speed of life’ and give customers a single, global view of what is being deployed, how it is performing, and what it costs. Beyond visibility, customers also need proactivity, as Skeels explains. “We’re deeply integrated into our customers’ order management, their ITSM, their ERP systems, which makes working with Expereo at scale absolutely seamless,” he says.&lt;/p&gt;&lt;p&gt;“The key point is that better visibility isn’t about more dashboards. It’s about connecting network behaviour to their business outcomes in terms of resilience, security experience, and cost.”&lt;/p&gt;&lt;p&gt;Skeels is speaking at the Digital Transformation Expo Global on February 4-5 around designing the AI-ready network – and his session promises to subvert the usual advice for those in attendance. “I want to challenge a few things,” notes Skeels. “I want to ask people to consider even unlearning things they’ve learned in the past.&lt;/p&gt;&lt;p&gt;“A lot of what we’ve taken for granted about networks no longer holds in an AI world.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;Watch the full conversation between Julian Skeels and TechEx’s James Bourne below:&lt;/em&gt;&lt;/p&gt;&lt;figure class="wp-block-video"&gt;&lt;video controls="controls" height="1080" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Julian_Skeels_Expereo_19_01.mp4" width="1920"&gt;&lt;/video&gt;&lt;/figure&gt;&lt;p&gt;&lt;em&gt;Photo by Pixabay&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/expereo-enterprise-connectivity-amid-ai-surge-with-visibility-at-the-speed-of-life/</guid><pubDate>Mon, 26 Jan 2026 15:23:57 +0000</pubDate></item><item><title>Nvidia invests $2B to help debt-ridden CoreWeave add 5GW of AI compute (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/nvidia-invests-2b-to-help-debt-ridden-coreweave-add-5gw-of-ai-compute/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-1468360413.jpg?resize=1200,857" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia said on Monday it has invested $2 billion in CoreWeave to hasten the data center company’s efforts to add more than 5 gigawatts of AI computing capacity by 2030.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The chipmaker, already an investor in CoreWeave, said it had bought the company’s Class A shares at $87.20 per share. As part of the deal, CoreWeave and Nvidia plan to together build “AI factories” (data centers) that would use the chipmaker’s products. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CoreWeave will also integrate Nvidia’s products across its platform, including the new Rubin chip architecture (set to replace the current Blackwell architecture), Bluefield storage systems, as well as the chipmaker’s new CPU line, Vera. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is a strong show of support for CoreWeave, which has come under scrutiny over the past few months for raising billions in debt to continue building out its data center operations. The company had $18.81 billion in debt obligations as of September 2025, according to PitchBook data, and it reported revenue of $1.36 billion in the third quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s CEO Michael Intrator has defended its business model (funding operations by raising debt with its GPUs as collateral), and has addressed concerns of circular deals in the AI industry by saying companies have to “work together” to address a “violent change in supply and demand.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has managed to successfully ride the AI wave since its transition from a crypto mining company to a provider of data center services for AI training and inference. And since its IPO in March last year, it has been busy fleshing out its technology stack with a slew of acquisitions. It acquired Weights &amp;amp; Biases, an AI developer platform, in March, and then soon after bought reinforcement learning startup OpenPipe. In October, it agreed to acquire Marimo (an open source Jupyter notebook competitor) and Monolith, another AI company. It also recently expanded its cloud partnership with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company currently counts several hyperscalers as customers, including OpenAI, Meta, and Microsoft.  &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, Nvidia will also help CoreWeave buy land and power for data centers, and work with the smaller company to include its AI software and architecture within Nvidia’s reference architecture to sell to cloud businesses and enterprises. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave’s shares were up more than 15% following news of the deal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, arguably the biggest benefactor and driver of the AI boom, the deal is the latest of several dozen investments in the past year as the company does its best to continue fueling the precipitous pace of investment in, and development of, the nascent technology. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-1468360413.jpg?resize=1200,857" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia said on Monday it has invested $2 billion in CoreWeave to hasten the data center company’s efforts to add more than 5 gigawatts of AI computing capacity by 2030.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The chipmaker, already an investor in CoreWeave, said it had bought the company’s Class A shares at $87.20 per share. As part of the deal, CoreWeave and Nvidia plan to together build “AI factories” (data centers) that would use the chipmaker’s products. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CoreWeave will also integrate Nvidia’s products across its platform, including the new Rubin chip architecture (set to replace the current Blackwell architecture), Bluefield storage systems, as well as the chipmaker’s new CPU line, Vera. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is a strong show of support for CoreWeave, which has come under scrutiny over the past few months for raising billions in debt to continue building out its data center operations. The company had $18.81 billion in debt obligations as of September 2025, according to PitchBook data, and it reported revenue of $1.36 billion in the third quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company’s CEO Michael Intrator has defended its business model (funding operations by raising debt with its GPUs as collateral), and has addressed concerns of circular deals in the AI industry by saying companies have to “work together” to address a “violent change in supply and demand.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has managed to successfully ride the AI wave since its transition from a crypto mining company to a provider of data center services for AI training and inference. And since its IPO in March last year, it has been busy fleshing out its technology stack with a slew of acquisitions. It acquired Weights &amp;amp; Biases, an AI developer platform, in March, and then soon after bought reinforcement learning startup OpenPipe. In October, it agreed to acquire Marimo (an open source Jupyter notebook competitor) and Monolith, another AI company. It also recently expanded its cloud partnership with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company currently counts several hyperscalers as customers, including OpenAI, Meta, and Microsoft.  &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, Nvidia will also help CoreWeave buy land and power for data centers, and work with the smaller company to include its AI software and architecture within Nvidia’s reference architecture to sell to cloud businesses and enterprises. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CoreWeave’s shares were up more than 15% following news of the deal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, arguably the biggest benefactor and driver of the AI boom, the deal is the latest of several dozen investments in the past year as the company does its best to continue fueling the precipitous pace of investment in, and development of, the nascent technology. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/nvidia-invests-2b-to-help-debt-ridden-coreweave-add-5gw-of-ai-compute/</guid><pubDate>Mon, 26 Jan 2026 15:52:55 +0000</pubDate></item><item><title>Microsoft announces powerful new chip for AI inference (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/microsoft-announces-powerful-new-chip-for-ai-inference/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Maia200-Hero-Image.png?resize=1200,674" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft has announced the launch of its latest chip, the Maia 200, which the company describes as a silicon workhorse designed for scaling AI inference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 200, which follows the company’s Maia 100 released in 2023, has been technically outfitted to run powerful AI models at faster speeds and with more efficiency, the company has said. Maia comes equipped with over 100 billion transistors, delivering over 10 petaflops in 4-bit precision and approximately 5 petaflops of 8-bit performance — a substantial increase over its predecessor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Inference refers to the computing process of running a model, in contrast with the compute required to train it. As AI companies mature, inference costs have become an increasingly important part of their overall operating cost, leading to renewed interest in ways to optimize the process.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft is hoping that the Maia 200 can be part of that optimization, making AI businesses run with less disruption and lower power use. “In practical terms, one Maia 200 node can effortlessly run today’s largest models, with plenty of headroom for even bigger models in the future,” the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s new chip is also part of a growing trend of tech giants turning to self-designed chips as a way to lessen their dependence on Nvidia, whose cutting-edge GPUs have become increasingly pivotal to AI companies’ success. Google, for instance, has its TPU, the tensor processing units — which aren’t sold as chips but as compute power made accessible through its cloud. Then there’s Amazon Trainium, the e-commerce giant’s own AI accelerator chip, which just launched its latest version, the Trainium3, in December. In each case, the TPUs can be used to offload some of the compute that would otherwise be assigned to Nvidia GPUs, lessening the overall hardware cost.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Maia, Microsoft is positioning itself to compete with those alternatives. In its press release Monday, the company noted that Maia delivers 3x the FP4 performance of third-generation Amazon Trainium chips, and FP8 performance above Google’s seventh generation TPU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft says that Maia is already hard at work fueling the company’s AI models from its&amp;nbsp;Superintelligence team. It has also been supporting the operations of Copilot, its chatbot. As of Monday, the company said it has invited a variety of parties — including developers, academics, and frontier AI labs — to use its Maia 200 software development kit in their workloads.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Maia200-Hero-Image.png?resize=1200,674" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft has announced the launch of its latest chip, the Maia 200, which the company describes as a silicon workhorse designed for scaling AI inference.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 200, which follows the company’s Maia 100 released in 2023, has been technically outfitted to run powerful AI models at faster speeds and with more efficiency, the company has said. Maia comes equipped with over 100 billion transistors, delivering over 10 petaflops in 4-bit precision and approximately 5 petaflops of 8-bit performance — a substantial increase over its predecessor.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Inference refers to the computing process of running a model, in contrast with the compute required to train it. As AI companies mature, inference costs have become an increasingly important part of their overall operating cost, leading to renewed interest in ways to optimize the process.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft is hoping that the Maia 200 can be part of that optimization, making AI businesses run with less disruption and lower power use. “In practical terms, one Maia 200 node can effortlessly run today’s largest models, with plenty of headroom for even bigger models in the future,” the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft’s new chip is also part of a growing trend of tech giants turning to self-designed chips as a way to lessen their dependence on Nvidia, whose cutting-edge GPUs have become increasingly pivotal to AI companies’ success. Google, for instance, has its TPU, the tensor processing units — which aren’t sold as chips but as compute power made accessible through its cloud. Then there’s Amazon Trainium, the e-commerce giant’s own AI accelerator chip, which just launched its latest version, the Trainium3, in December. In each case, the TPUs can be used to offload some of the compute that would otherwise be assigned to Nvidia GPUs, lessening the overall hardware cost.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With Maia, Microsoft is positioning itself to compete with those alternatives. In its press release Monday, the company noted that Maia delivers 3x the FP4 performance of third-generation Amazon Trainium chips, and FP8 performance above Google’s seventh generation TPU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft says that Maia is already hard at work fueling the company’s AI models from its&amp;nbsp;Superintelligence team. It has also been supporting the operations of Copilot, its chatbot. As of Monday, the company said it has invited a variety of parties — including developers, academics, and frontier AI labs — to use its Maia 200 software development kit in their workloads.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/microsoft-announces-powerful-new-chip-for-ai-inference/</guid><pubDate>Mon, 26 Jan 2026 16:00:00 +0000</pubDate></item><item><title>Tech workers call for CEOs to speak up against ICE after the killing of Alex Pretti (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/tech-workers-call-for-ceos-to-speak-up-against-ice-after-the-killing-of-alex-pretti/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2257473113.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;More than 450 tech workers from companies like Google, Meta, OpenAI, Amazon, and Salesforce have signed a letter urging their CEOs to call the White House and demand that U.S. Immigration and Customs Enforcement (ICE) leave U.S. cities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For months now, Trump has sent federal agents to our cities to criminalize us, our neighbors, friends, colleagues, and family members,” reads the open letter from IceOut.Tech. “From Minneapolis to Los Angeles to Chicago, we’ve seen armed and masked thugs bring reckless violence, kidnapping, terror and cruelty with no end in sight.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Minneapolis has become the focal point of a large-scale federal immigration operation, employing tactics so intense that many have characterized it as a military occupation. The operation has been marked by confrontations between federal agents and community members protesting the raids, with law enforcement indiscriminately deploying crowd-control tactics, including pepper spray, tear gas, rubber bullets, and sound cannons.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This cannot continue, and we know the tech industry can make a difference,” the letter from tech industry workers continues. “When Trump threatened to send the National Guard to San Francisco in October, tech industry leaders called the White House. It worked: Trump backed down.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The campaign among tech workers began after ICE agents shot and killed U.S. citizen Renee Good in Minneapolis three weeks ago, and it grew over the weekend after Border Patrol agents shot and killed Alex Pretti, a 37-year-old ICU nurse at the Minneapolis VA hospital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The organizers of the letter did not disclose their names, and many who signed the letter did so anonymously out of fear of retribution.&amp;nbsp;TechCrunch has reached out for more information.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A number of tech leaders have already spoken out against federal actions in Minneapolis. LinkedIn co-founder Reid Hoffman said the way ICE operates is “terrible for the people,” and Khosla Ventures founder Vinod Khosla called the current enforcement “macho ICE vigilantes running amuck empowered by a conscious-less administration.” Google DeepMind’s chief scientist Jeff Dean called for “every person regardless of political affiliation” to denounce the escalation of violence. OpenAI’s head of global business, James Dyett, criticized the industry’s silence, posting on X that “there is far more outrage from tech leaders over a wealth tax than masked ICE agents terrorizing communities.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Signal president Meredith Whittaker bemoaned that masked agents are “executing people in the streets and powerful leaders are openly lying to cover for them. To everyone in my industry who’s ever claimed to value freedom — draw on the courage of your convictions and stand up.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic’s CEO Dario Amodei on Monday posted about the importance of “preserving democratic values and rights at home,” particularly given the “horror we’re seeing in Minnesota.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, many of the most powerful figures in tech have not only largely stayed quiet about opposition to the Trump administration’s directives, but they have also actively attempted to curry favor with the president. Amazon owner Jeff Bezos, Apple CEO Tim Cook, Google CEO Sundar Pichai, and Meta CEO Mark Zuckerberg all attended during President Trump’s inauguration and donated to the inauguration fund either personally or through their corporations. None have spoken out publicly about the ramping up of ICE raids.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI president Greg Brockman and his wife, Anna, are also prominent donors to causes and candidates associated with President Trump and have refrained from speaking out. In keeping with his anti-immigration views, Elon Musk has actively supported ICE operations, calling protestors “pure evil.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter also calls on tech CEOs to cancel all company contracts with ICE — potentially an expensive demand, as several tech firms currently hold contracts with ICE. Palantir is one of ICE’s most significant tech partners. Last year the company was awarded a $30 million contract to build a new AI-driven surveillance platform called “ImmigrationOS.” Last year, facial-recognition company Clearview AI signed a contract to provide ICE with facial-matching technology. Amazon Web Services, Microsoft, and Oracle also provide cloud infrastructure to the Department of Homeland Security and ICE, as well as IT services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to the companies for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com&lt;/em&gt; &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at @rebeccabellan.491&lt;/em&gt; &lt;em&gt;and @russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2257473113.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;More than 450 tech workers from companies like Google, Meta, OpenAI, Amazon, and Salesforce have signed a letter urging their CEOs to call the White House and demand that U.S. Immigration and Customs Enforcement (ICE) leave U.S. cities.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For months now, Trump has sent federal agents to our cities to criminalize us, our neighbors, friends, colleagues, and family members,” reads the open letter from IceOut.Tech. “From Minneapolis to Los Angeles to Chicago, we’ve seen armed and masked thugs bring reckless violence, kidnapping, terror and cruelty with no end in sight.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Minneapolis has become the focal point of a large-scale federal immigration operation, employing tactics so intense that many have characterized it as a military occupation. The operation has been marked by confrontations between federal agents and community members protesting the raids, with law enforcement indiscriminately deploying crowd-control tactics, including pepper spray, tear gas, rubber bullets, and sound cannons.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This cannot continue, and we know the tech industry can make a difference,” the letter from tech industry workers continues. “When Trump threatened to send the National Guard to San Francisco in October, tech industry leaders called the White House. It worked: Trump backed down.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The campaign among tech workers began after ICE agents shot and killed U.S. citizen Renee Good in Minneapolis three weeks ago, and it grew over the weekend after Border Patrol agents shot and killed Alex Pretti, a 37-year-old ICU nurse at the Minneapolis VA hospital.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The organizers of the letter did not disclose their names, and many who signed the letter did so anonymously out of fear of retribution.&amp;nbsp;TechCrunch has reached out for more information.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A number of tech leaders have already spoken out against federal actions in Minneapolis. LinkedIn co-founder Reid Hoffman said the way ICE operates is “terrible for the people,” and Khosla Ventures founder Vinod Khosla called the current enforcement “macho ICE vigilantes running amuck empowered by a conscious-less administration.” Google DeepMind’s chief scientist Jeff Dean called for “every person regardless of political affiliation” to denounce the escalation of violence. OpenAI’s head of global business, James Dyett, criticized the industry’s silence, posting on X that “there is far more outrage from tech leaders over a wealth tax than masked ICE agents terrorizing communities.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Signal president Meredith Whittaker bemoaned that masked agents are “executing people in the streets and powerful leaders are openly lying to cover for them. To everyone in my industry who’s ever claimed to value freedom — draw on the courage of your convictions and stand up.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic’s CEO Dario Amodei on Monday posted about the importance of “preserving democratic values and rights at home,” particularly given the “horror we’re seeing in Minnesota.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, many of the most powerful figures in tech have not only largely stayed quiet about opposition to the Trump administration’s directives, but they have also actively attempted to curry favor with the president. Amazon owner Jeff Bezos, Apple CEO Tim Cook, Google CEO Sundar Pichai, and Meta CEO Mark Zuckerberg all attended during President Trump’s inauguration and donated to the inauguration fund either personally or through their corporations. None have spoken out publicly about the ramping up of ICE raids.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI president Greg Brockman and his wife, Anna, are also prominent donors to causes and candidates associated with President Trump and have refrained from speaking out. In keeping with his anti-immigration views, Elon Musk has actively supported ICE operations, calling protestors “pure evil.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter also calls on tech CEOs to cancel all company contracts with ICE — potentially an expensive demand, as several tech firms currently hold contracts with ICE. Palantir is one of ICE’s most significant tech partners. Last year the company was awarded a $30 million contract to build a new AI-driven surveillance platform called “ImmigrationOS.” Last year, facial-recognition company Clearview AI signed a contract to provide ICE with facial-matching technology. Amazon Web Services, Microsoft, and Oracle also provide cloud infrastructure to the Department of Homeland Security and ICE, as well as IT services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to the companies for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com&lt;/em&gt; &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at @rebeccabellan.491&lt;/em&gt; &lt;em&gt;and @russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/tech-workers-call-for-ceos-to-speak-up-against-ice-after-the-killing-of-alex-pretti/</guid><pubDate>Mon, 26 Jan 2026 16:26:49 +0000</pubDate></item><item><title>Retailers examine options for on-AI retail (AI News)</title><link>https://www.artificialintelligence-news.com/news/retailers-examine-options-for-on-ai-retail/</link><description>&lt;p&gt; Big retailers are committing more heavily to agentic AI-led commerce, and accepting some loss of customer proximity and data control in the process.&lt;/p&gt;&lt;p&gt; As reported by &lt;i&gt;Retail Dive&lt;/i&gt;, the opening weeks of 2026 have seen Etsy, Target and Walmart push  product ranges onto third-party AI platforms, forming new partnerships with Google’s Gemini and Microsoft’s Copilot, after last year’s collaborations with OpenAI’s ChatGPT. These let consumers purchase goods inside the AI’s conversation interface.&lt;/p&gt;&lt;p&gt; Amazon and Walmart have been investing in their own consumer-facing AI assistants, Rufus and Sparky respectively to change how shoppers interact with their brands.&lt;/p&gt;&lt;p&gt; Agentic AI is beginning to redraw direct-to-consumer engagement, and industry figures regard this trend as an important moment in online retail. “I think this has the potential to disrupt retail in the same way the internet once did,” Kartik Hosanagar, a marketing professor at the Wharton School of the University of Pennsylvania, told the website’s reporters.&lt;/p&gt;&lt;p&gt; Partnering with AIs like ChatGPT or Gemini engages consumers wherever they happen to be and may choose to shop. Adobe’s 2025 Holiday Shopping report found that AI-driven traffic to US e-commerce sites grew 758% year on year between in November 2025, and Cyber Monday saw a 670% increase in AI-referred retail visits.&lt;/p&gt;&lt;p&gt; “What we expect is a deepening of consumer engagement,” Katherine Black, a partner at Kearney specialising in food, drug and mass-market retail, said in an email to &lt;i&gt;Retail Dive&lt;/i&gt;. “More shoppers will rely on AI for purchasing, and across a wider range of missions. As retailers’ capabilities within these tools improve, adoption should accelerate further.”&lt;/p&gt;&lt;p&gt; Meeting customers on AI platforms comes with trade-offs, according to industry observers, with questions around data ownership and the risk that retailers are sidelined. 81% of retail executives believe generative AI will erode brand loyalty by 2027, according to Deloitte’s 2026 Retail Industry Global Outlook, published earlier this month.&lt;/p&gt;&lt;p&gt; Retailers’ websites or apps provide a stream of behavioural data, and if discovery, evaluation, and purchase happen externally, any insight doesn’t reach the retailer. “This fundamentally changes where power sits,” Hosanagar said. “Control over the agent increasingly means control over the customer relationship.”&lt;/p&gt;&lt;p&gt; Google and Alphabet CEO Sundar Pichai has unveiled new commerce tools for Gemini, outlining how it will support customers from discovery to final purchase. Nikki Baird, vice president of strategy and product at Aptos, says this raises difficult questions. “What he’s describing is Google owning the data across discovery, decision and transaction. Even if some information is shared back, missing context from those stages leaves retailers with a much poorer understanding of their customers.”&lt;/p&gt;&lt;p&gt; Pichai reassured retailers collaboration remains central to Google. “From nearly three decades of working with retailers, we know success only comes when we work together,” he told an NRF audience. “Our aim is to use our full technology stack to help shape the next era of retail.”&lt;/p&gt;&lt;p&gt; Yet agentic systems’ features like instant checkout absorb the shopping experience into one platform. “If research, discovery and purchase all happen on OpenAI rather than Walmart.com, you’re effectively giving away the brand experience. At that point, the retailer risks becoming little more than a fulfilment operation,” Hosanagar said.&lt;/p&gt;&lt;p&gt; Amazon has not announced plans to sell directly through ChatGPT, doubling down on its own AI initiatives. Earlier this month, the company launched a dedicated site for Alexa+, its generative AI assistant that helps users research and plan purchases.&lt;/p&gt;&lt;p&gt; Yet participation in third-party AI commerce may become unavoidable. When OpenAI launched its Instant Checkout feature on ChatGPT last September, it suggested that enabling the function could influence how merchants are ranked in search results, in addition to price and product quality. Uploading product catalogues to AI chat platforms may be the first step in a transformation of online retail.&lt;/p&gt;&lt;p&gt; According to Deloitte, roughly half of retail executives expect the current multi-stage shopping process to reduce to a single AI-driven interaction by 2027. For now the industry remains at an early stage of any transition. “The real inflection point is when consumers rely on an autonomous agent to shop on their behalf,” Hosanagar told &lt;i&gt;Retail Dive&lt;/i&gt;.&lt;/p&gt;&lt;p&gt; “Retailers will engage less with humans directly and more with their representatives — AI agents. That agent processes information differently, requires data in new formats and responds to persuasion in ways unlike a person.”&lt;/p&gt;&lt;p&gt; Today, consumers can access ChatGPT on their phones while in-store, effectively consulting an always-available expert. “It’s not just the internet in your pocket,” Baird told &lt;i&gt;Retail Dive&lt;/i&gt;. “It’s like having a highly knowledgeable store associate who knows every retailer.”&lt;/p&gt;&lt;p&gt; This may prompt retailers to equip frontline staff with their own AI tools, offering instant insight into customer preferences or shopping history. Alternatively, a retailer’s AI agent could proactively notify customers when a favoured item is back in stock, helping associates convert interest into sales. “The goal is to enable store associates to perform at their best,” Baird said.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Shopping trauma!” by Elsie esq. is licensed under CC BY 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt; Big retailers are committing more heavily to agentic AI-led commerce, and accepting some loss of customer proximity and data control in the process.&lt;/p&gt;&lt;p&gt; As reported by &lt;i&gt;Retail Dive&lt;/i&gt;, the opening weeks of 2026 have seen Etsy, Target and Walmart push  product ranges onto third-party AI platforms, forming new partnerships with Google’s Gemini and Microsoft’s Copilot, after last year’s collaborations with OpenAI’s ChatGPT. These let consumers purchase goods inside the AI’s conversation interface.&lt;/p&gt;&lt;p&gt; Amazon and Walmart have been investing in their own consumer-facing AI assistants, Rufus and Sparky respectively to change how shoppers interact with their brands.&lt;/p&gt;&lt;p&gt; Agentic AI is beginning to redraw direct-to-consumer engagement, and industry figures regard this trend as an important moment in online retail. “I think this has the potential to disrupt retail in the same way the internet once did,” Kartik Hosanagar, a marketing professor at the Wharton School of the University of Pennsylvania, told the website’s reporters.&lt;/p&gt;&lt;p&gt; Partnering with AIs like ChatGPT or Gemini engages consumers wherever they happen to be and may choose to shop. Adobe’s 2025 Holiday Shopping report found that AI-driven traffic to US e-commerce sites grew 758% year on year between in November 2025, and Cyber Monday saw a 670% increase in AI-referred retail visits.&lt;/p&gt;&lt;p&gt; “What we expect is a deepening of consumer engagement,” Katherine Black, a partner at Kearney specialising in food, drug and mass-market retail, said in an email to &lt;i&gt;Retail Dive&lt;/i&gt;. “More shoppers will rely on AI for purchasing, and across a wider range of missions. As retailers’ capabilities within these tools improve, adoption should accelerate further.”&lt;/p&gt;&lt;p&gt; Meeting customers on AI platforms comes with trade-offs, according to industry observers, with questions around data ownership and the risk that retailers are sidelined. 81% of retail executives believe generative AI will erode brand loyalty by 2027, according to Deloitte’s 2026 Retail Industry Global Outlook, published earlier this month.&lt;/p&gt;&lt;p&gt; Retailers’ websites or apps provide a stream of behavioural data, and if discovery, evaluation, and purchase happen externally, any insight doesn’t reach the retailer. “This fundamentally changes where power sits,” Hosanagar said. “Control over the agent increasingly means control over the customer relationship.”&lt;/p&gt;&lt;p&gt; Google and Alphabet CEO Sundar Pichai has unveiled new commerce tools for Gemini, outlining how it will support customers from discovery to final purchase. Nikki Baird, vice president of strategy and product at Aptos, says this raises difficult questions. “What he’s describing is Google owning the data across discovery, decision and transaction. Even if some information is shared back, missing context from those stages leaves retailers with a much poorer understanding of their customers.”&lt;/p&gt;&lt;p&gt; Pichai reassured retailers collaboration remains central to Google. “From nearly three decades of working with retailers, we know success only comes when we work together,” he told an NRF audience. “Our aim is to use our full technology stack to help shape the next era of retail.”&lt;/p&gt;&lt;p&gt; Yet agentic systems’ features like instant checkout absorb the shopping experience into one platform. “If research, discovery and purchase all happen on OpenAI rather than Walmart.com, you’re effectively giving away the brand experience. At that point, the retailer risks becoming little more than a fulfilment operation,” Hosanagar said.&lt;/p&gt;&lt;p&gt; Amazon has not announced plans to sell directly through ChatGPT, doubling down on its own AI initiatives. Earlier this month, the company launched a dedicated site for Alexa+, its generative AI assistant that helps users research and plan purchases.&lt;/p&gt;&lt;p&gt; Yet participation in third-party AI commerce may become unavoidable. When OpenAI launched its Instant Checkout feature on ChatGPT last September, it suggested that enabling the function could influence how merchants are ranked in search results, in addition to price and product quality. Uploading product catalogues to AI chat platforms may be the first step in a transformation of online retail.&lt;/p&gt;&lt;p&gt; According to Deloitte, roughly half of retail executives expect the current multi-stage shopping process to reduce to a single AI-driven interaction by 2027. For now the industry remains at an early stage of any transition. “The real inflection point is when consumers rely on an autonomous agent to shop on their behalf,” Hosanagar told &lt;i&gt;Retail Dive&lt;/i&gt;.&lt;/p&gt;&lt;p&gt; “Retailers will engage less with humans directly and more with their representatives — AI agents. That agent processes information differently, requires data in new formats and responds to persuasion in ways unlike a person.”&lt;/p&gt;&lt;p&gt; Today, consumers can access ChatGPT on their phones while in-store, effectively consulting an always-available expert. “It’s not just the internet in your pocket,” Baird told &lt;i&gt;Retail Dive&lt;/i&gt;. “It’s like having a highly knowledgeable store associate who knows every retailer.”&lt;/p&gt;&lt;p&gt; This may prompt retailers to equip frontline staff with their own AI tools, offering instant insight into customer preferences or shopping history. Alternatively, a retailer’s AI agent could proactively notify customers when a favoured item is back in stock, helping associates convert interest into sales. “The goal is to enable store associates to perform at their best,” Baird said.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Shopping trauma!” by Elsie esq. is licensed under CC BY 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/retailers-examine-options-for-on-ai-retail/</guid><pubDate>Mon, 26 Jan 2026 16:40:00 +0000</pubDate></item><item><title>Why chatbots are starting to check your age (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/26/1131726/why-chatbots-are-starting-to-check-your-age/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260122_algo_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;How do tech companies check if their users are kids?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;This question has taken on new urgency recently thanks to growing concern about the dangers that can arise when children talk to AI chatbots. For years Big Tech asked for birthdays (that one could make up) to avoid violating child privacy laws, but they weren’t required to moderate content accordingly. Two developments over the last week show how quickly things are changing in the US and how this issue is becoming a new battleground, even among parents and child-safety advocates.&lt;/p&gt;  &lt;p&gt;In one corner is the Republican Party, which has supported laws passed in several states that require sites with adult content to verify users’ ages. Critics say this provides cover to block anything deemed “harmful to minors,” which could include sex education. Other states, like California, are coming after AI companies with laws to protect kids who talk to chatbots (by requiring them to verify who’s a kid). Meanwhile, President Trump is attempting to keep AI regulation a national issue rather than allowing states to make their own rules. Support for various bills in Congress is constantly in flux.&lt;/p&gt; 
 &lt;p&gt;So what might happen? The debate is quickly moving away from whether age verification is necessary and toward who will be responsible for it.&lt;strong&gt; &lt;/strong&gt;This responsibility is a hot potato that no company wants to hold.&lt;/p&gt;  &lt;p&gt;In a blog post last Tuesday, OpenAI revealed that it plans to roll out automatic age prediction. In short, the company will apply a model that uses factors like the time of day, among others, to predict whether a person chatting is under 18. For those identified as teens or children, ChatGPT will apply filters to “reduce exposure” to content like graphic violence or sexual role-play. YouTube launched something similar last year.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;If you support age verification but are concerned about privacy, this might sound like a win. But there's a catch. The system is not perfect, of course, so it could classify a child as an adult or vice versa. People who are wrongly labeled under 18 can verify their identity by submitting a selfie or government ID to a company called Persona.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Selfie verifications have issues: They fail more often for people of color and those with certain disabilities. Sameer Hinduja, who co-directs the Cyberbullying Research Center, says the fact that Persona will need to hold millions of government IDs and masses of biometric data is another weak point. “When those get breached, we’ve exposed massive populations all at once,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Hinduja instead advocates for device-level verification, where a parent specifies a child’s age when setting up the child’s phone for the first time. This information is then kept on the device and shared securely with apps and websites.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s more or less what Tim Cook, the CEO of Apple, recently lobbied US lawmakers to call for. Cook was fighting lawmakers who wanted to require app stores to verify ages, which would saddle Apple with lots of liability.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;More signals of where this is all headed will come on Wednesday, when the Federal Trade Commission—the agency that would be responsible for enforcing these new laws—is holding an all-day workshop on age verification. Apple’s head of government affairs, Nick Rossi, will be there. He’ll be joined by higher-ups in child safety at Google and Meta, as well as a company that specializes in marketing to children.&lt;/p&gt;  &lt;p&gt;The FTC has become increasingly politicized under President Trump (his firing of the sole Democratic commissioner was struck down by a federal court, a decision that is now pending review by the US Supreme Court). In July, I wrote about signals that the agency is softening its stance toward AI companies. Indeed, in December, the FTC overturned a Biden-era ruling against an AI company that allowed people to flood the internet with fake product reviews, writing that it clashed with President Trump’s AI Action Plan.&lt;/p&gt;  &lt;p&gt;Wednesday’s workshop may shed light on how partisan the FTC’s approach to age verification will be. Red states favor laws that require porn websites to verify ages (but critics warn this could be used to block a much wider range of content). Bethany Soye, a Republican state representative who is leading an effort to pass such a bill in her state of South Dakota, is scheduled to speak at the FTC meeting. The ACLU generally opposes laws requiring IDs to visit websites and has instead advocated for an expansion of existing parental controls.&lt;/p&gt;  &lt;p&gt;While all this gets debated, though, AI has set the world of child safety on fire. We’re dealing with increased generation of child sexual abuse material, concerns (and lawsuits) about suicides and self-harm following chatbot conversations, and troubling evidence of kids’ forming attachments to AI companions. Colliding stances on privacy, politics, free expression, and surveillance will complicate any effort to find a solution. Write to me with your thoughts.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260122_algo_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;sign up here.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;How do tech companies check if their users are kids?&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;This question has taken on new urgency recently thanks to growing concern about the dangers that can arise when children talk to AI chatbots. For years Big Tech asked for birthdays (that one could make up) to avoid violating child privacy laws, but they weren’t required to moderate content accordingly. Two developments over the last week show how quickly things are changing in the US and how this issue is becoming a new battleground, even among parents and child-safety advocates.&lt;/p&gt;  &lt;p&gt;In one corner is the Republican Party, which has supported laws passed in several states that require sites with adult content to verify users’ ages. Critics say this provides cover to block anything deemed “harmful to minors,” which could include sex education. Other states, like California, are coming after AI companies with laws to protect kids who talk to chatbots (by requiring them to verify who’s a kid). Meanwhile, President Trump is attempting to keep AI regulation a national issue rather than allowing states to make their own rules. Support for various bills in Congress is constantly in flux.&lt;/p&gt; 
 &lt;p&gt;So what might happen? The debate is quickly moving away from whether age verification is necessary and toward who will be responsible for it.&lt;strong&gt; &lt;/strong&gt;This responsibility is a hot potato that no company wants to hold.&lt;/p&gt;  &lt;p&gt;In a blog post last Tuesday, OpenAI revealed that it plans to roll out automatic age prediction. In short, the company will apply a model that uses factors like the time of day, among others, to predict whether a person chatting is under 18. For those identified as teens or children, ChatGPT will apply filters to “reduce exposure” to content like graphic violence or sexual role-play. YouTube launched something similar last year.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;If you support age verification but are concerned about privacy, this might sound like a win. But there's a catch. The system is not perfect, of course, so it could classify a child as an adult or vice versa. People who are wrongly labeled under 18 can verify their identity by submitting a selfie or government ID to a company called Persona.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Selfie verifications have issues: They fail more often for people of color and those with certain disabilities. Sameer Hinduja, who co-directs the Cyberbullying Research Center, says the fact that Persona will need to hold millions of government IDs and masses of biometric data is another weak point. “When those get breached, we’ve exposed massive populations all at once,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Hinduja instead advocates for device-level verification, where a parent specifies a child’s age when setting up the child’s phone for the first time. This information is then kept on the device and shared securely with apps and websites.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s more or less what Tim Cook, the CEO of Apple, recently lobbied US lawmakers to call for. Cook was fighting lawmakers who wanted to require app stores to verify ages, which would saddle Apple with lots of liability.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;More signals of where this is all headed will come on Wednesday, when the Federal Trade Commission—the agency that would be responsible for enforcing these new laws—is holding an all-day workshop on age verification. Apple’s head of government affairs, Nick Rossi, will be there. He’ll be joined by higher-ups in child safety at Google and Meta, as well as a company that specializes in marketing to children.&lt;/p&gt;  &lt;p&gt;The FTC has become increasingly politicized under President Trump (his firing of the sole Democratic commissioner was struck down by a federal court, a decision that is now pending review by the US Supreme Court). In July, I wrote about signals that the agency is softening its stance toward AI companies. Indeed, in December, the FTC overturned a Biden-era ruling against an AI company that allowed people to flood the internet with fake product reviews, writing that it clashed with President Trump’s AI Action Plan.&lt;/p&gt;  &lt;p&gt;Wednesday’s workshop may shed light on how partisan the FTC’s approach to age verification will be. Red states favor laws that require porn websites to verify ages (but critics warn this could be used to block a much wider range of content). Bethany Soye, a Republican state representative who is leading an effort to pass such a bill in her state of South Dakota, is scheduled to speak at the FTC meeting. The ACLU generally opposes laws requiring IDs to visit websites and has instead advocated for an expansion of existing parental controls.&lt;/p&gt;  &lt;p&gt;While all this gets debated, though, AI has set the world of child safety on fire. We’re dealing with increased generation of child sexual abuse material, concerns (and lawsuits) about suicides and self-harm following chatbot conversations, and troubling evidence of kids’ forming attachments to AI companions. Colliding stances on privacy, politics, free expression, and surveillance will complicate any effort to find a solution. Write to me with your thoughts.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/26/1131726/why-chatbots-are-starting-to-check-your-age/</guid><pubDate>Mon, 26 Jan 2026 17:05:00 +0000</pubDate></item><item><title>Obvious Ventures lands fund five with a 360-degree view of planetary, human, economic health (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/obvious-ventures-lands-fund-five-with-a-360-degree-view-of-planetary-human-economic-health/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Obvious-Five-high-rez39.png?resize=1200,684" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Obvious Ventures, the firm co-founded by Twitter’s Evan Williams, has raised a fifth fund, and this one, just like its predecessors, comes with a “fun” number: $360,360,360.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We invest in the frontiers of math and science and physics, and we like to celebrate math in our fund numbers as well,” James Joaquin (pictured far right), the firm’s co-founder and managing director, told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The firm’s first fund was $123,456,789, and the second was $191,919,191 (a palindromic number that reads the same forward and backwards). The third was $271,828,182 (which mathematicians and engineers instantly recognize as e, or Euler’s number), while the fourth fund, announced in mid-2022, continued the tradition as another palindrome at $355,111,553.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you haven’t guessed it by now, the meaning of Obvious Ventures’ newest fund size is a little less about geeky math and more about the firm’s investing philosophy. Twelve years into its journey, Obvious says the figure represents a full-circle perspective on its three broad focus areas: planetary health, human health, and economic health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We love the metaphor of taking a 360-degree view in each of those areas,” Joaquin said. “You have to be a student of the past to understand what’s worked and what hasn’t worked.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s working for the firm, according to Joaquin, is keeping its fund sizes small enough so that a single investment, if it becomes a durable public company, has the opportunity to return the entire fund. Joaquin likely placed an emphasis on durability in part because one of Obvious Ventures’ initial winners, Beyond Meat, reached a market capitalization of over $14 billion shortly after its 2019 IPO, but had fallen to below a billion by late 2022. &amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Joaquin says the firm has seen meaningful cash distributions to limited partners from all of its core funds, and it boasts several companies with successful public-market exits. In 2015, Obvious Ventures invested in the satellite imagery company Planet Labs, which went public via a SPAC in 2021 and is currently valued at approximately $8.5 billion. Meanwhile, its Series A investment in Recursion Pharmaceuticals maintains a market capitalization of over $2 billion.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Obvious is also an early investor in the HR and payroll platform Gusto, which was most recently valued at more than $9 billion in the private market and is widely considered to be on an IPO trajectory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a venture capital environment where only 17% of firms successfully raise more than three funds, per research from Sapphire Partners, Obvious Ventures’ latest fundraise solidifies the firm as an established VC player.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We made it to fund five, which is actually a big deal in the venture landscape,” Joaquin said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Obvious Ventures may take a playful approach to fund sizes, but its focus on investing in startups that make a positive impact on the world is serious. Joaquin pointed to several investments in each of the firm’s three pillars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within the planetary health sector, the firm invested in Zanskar, a startup using proprietary data and AI to identify and harness geothermal energy, one of the most cost-effective power sources available. Just last week, Zanskar announced a $115 million Series C. Obvious Ventures, which led the company’s previous round, is particularly excited about the investment. Joaquin noted that the geothermal power harnessed by Zanskar can help fuel energy-hungry AI data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its human health strategy, Obvious Ventures touted its investment in Inceptive, an AI platform for molecule development. Inceptive was founded by Jakob Uszkoreit, one of the primary authors of the seminal “Attention Is All You Need” paper, which introduced the transformer architecture, the breakthrough behind generative AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the area of economic health, Joaquin pointed to Dexterity Robotics. The company, which was valued at $1.65 billion last year, builds humanoids to handle “dull, dirty, and dangerous” tasks currently performed by humans in warehouses and factories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Joaquin, Obvious Ventures has four active investors, including co-founder Vishal Vasishth. (Ev Williams remains a co-founder and an adviser.) The firm intends to make approximately 10 investments annually, with check sizes ranging from $5 million to $12 million for seed and Series A startups.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Obvious-Five-high-rez39.png?resize=1200,684" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Obvious Ventures, the firm co-founded by Twitter’s Evan Williams, has raised a fifth fund, and this one, just like its predecessors, comes with a “fun” number: $360,360,360.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We invest in the frontiers of math and science and physics, and we like to celebrate math in our fund numbers as well,” James Joaquin (pictured far right), the firm’s co-founder and managing director, told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The firm’s first fund was $123,456,789, and the second was $191,919,191 (a palindromic number that reads the same forward and backwards). The third was $271,828,182 (which mathematicians and engineers instantly recognize as e, or Euler’s number), while the fourth fund, announced in mid-2022, continued the tradition as another palindrome at $355,111,553.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you haven’t guessed it by now, the meaning of Obvious Ventures’ newest fund size is a little less about geeky math and more about the firm’s investing philosophy. Twelve years into its journey, Obvious says the figure represents a full-circle perspective on its three broad focus areas: planetary health, human health, and economic health.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We love the metaphor of taking a 360-degree view in each of those areas,” Joaquin said. “You have to be a student of the past to understand what’s worked and what hasn’t worked.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s working for the firm, according to Joaquin, is keeping its fund sizes small enough so that a single investment, if it becomes a durable public company, has the opportunity to return the entire fund. Joaquin likely placed an emphasis on durability in part because one of Obvious Ventures’ initial winners, Beyond Meat, reached a market capitalization of over $14 billion shortly after its 2019 IPO, but had fallen to below a billion by late 2022. &amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Joaquin says the firm has seen meaningful cash distributions to limited partners from all of its core funds, and it boasts several companies with successful public-market exits. In 2015, Obvious Ventures invested in the satellite imagery company Planet Labs, which went public via a SPAC in 2021 and is currently valued at approximately $8.5 billion. Meanwhile, its Series A investment in Recursion Pharmaceuticals maintains a market capitalization of over $2 billion.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Obvious is also an early investor in the HR and payroll platform Gusto, which was most recently valued at more than $9 billion in the private market and is widely considered to be on an IPO trajectory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a venture capital environment where only 17% of firms successfully raise more than three funds, per research from Sapphire Partners, Obvious Ventures’ latest fundraise solidifies the firm as an established VC player.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We made it to fund five, which is actually a big deal in the venture landscape,” Joaquin said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Obvious Ventures may take a playful approach to fund sizes, but its focus on investing in startups that make a positive impact on the world is serious. Joaquin pointed to several investments in each of the firm’s three pillars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within the planetary health sector, the firm invested in Zanskar, a startup using proprietary data and AI to identify and harness geothermal energy, one of the most cost-effective power sources available. Just last week, Zanskar announced a $115 million Series C. Obvious Ventures, which led the company’s previous round, is particularly excited about the investment. Joaquin noted that the geothermal power harnessed by Zanskar can help fuel energy-hungry AI data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In its human health strategy, Obvious Ventures touted its investment in Inceptive, an AI platform for molecule development. Inceptive was founded by Jakob Uszkoreit, one of the primary authors of the seminal “Attention Is All You Need” paper, which introduced the transformer architecture, the breakthrough behind generative AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the area of economic health, Joaquin pointed to Dexterity Robotics. The company, which was valued at $1.65 billion last year, builds humanoids to handle “dull, dirty, and dangerous” tasks currently performed by humans in warehouses and factories.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to Joaquin, Obvious Ventures has four active investors, including co-founder Vishal Vasishth. (Ev Williams remains a co-founder and an adviser.) The firm intends to make approximately 10 investments annually, with check sizes ranging from $5 million to $12 million for seed and Series A startups.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/obvious-ventures-lands-fund-five-with-a-360-degree-view-of-planetary-human-economic-health/</guid><pubDate>Mon, 26 Jan 2026 18:00:00 +0000</pubDate></item><item><title>Anthropic launches interactive Claude apps, including Slack and other workplace tools (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/anthropic-launches-interactive-claude-apps-including-slack-and-other-workplace-tools/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/mcp-asana-crop.jpg?resize=1200,987" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Claude users will now be able to call up interactive apps within the chatbot interface, thanks to a new feature announced by Anthropic on Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In keeping with Anthropic’s enterprise focus, the launch apps are mostly workplace tools, including Slack, Canva, Figma, Box, and Clay, with a Salesforce implementation expected soon. In each case, the app will enable a logged-in instance of the service that’s accessible to Claude, enabling users to send Slack messages, generate charts, or access cloud files, depending on which apps have been enabled.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Analyzing data, designing content, and managing projects all work better with a dedicated visual interface,” Anthropic said in a blog post announcing the feature. “Combined with Claude’s intelligence, you can work and iterate faster than either could offer alone.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new feature is available to Pro, Max, Team, and Enterprise subscribers, but not free users. Eligible users can activate the tools at claude.ai/directory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system is similar to OpenAI’s Apps system, which launched in October and enables interactive third-party tools. Both systems of app integrations are built on the Model Context Protocol (MCP), an open standard introduced by Anthropic in 2024. MCP launched support for apps in November, drawing on work from both companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new apps will be particularly powerful when integrated with Claude Cowork, an all-purpose agent tool launched by Anthropic last week. Built on top of Claude Code, Cowork lets users assign multistage tasks that draw on large and open-ended datasets — tasks that would have previously required terminal commands. Combined with the new apps feature, Cowork could be granted access to cloud files or ongoing work projects. For example, Cowork could update a marketing graphic in Figma or use new data from the company’s Box instance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apps are not available in Cowork at launch, but Anthropic said the integration would be “coming soon.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agentic systems can be unpredictable, and Anthropic’s own safety documentation for Cowork encourages users to monitor the agent closely and not grant any unnecessary permissions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Be cautious about granting access to sensitive information like financial documents, credentials, or personal records,” the company recommends. “Consider creating a dedicated working folder for Claude rather than granting broad access.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/mcp-asana-crop.jpg?resize=1200,987" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Claude users will now be able to call up interactive apps within the chatbot interface, thanks to a new feature announced by Anthropic on Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In keeping with Anthropic’s enterprise focus, the launch apps are mostly workplace tools, including Slack, Canva, Figma, Box, and Clay, with a Salesforce implementation expected soon. In each case, the app will enable a logged-in instance of the service that’s accessible to Claude, enabling users to send Slack messages, generate charts, or access cloud files, depending on which apps have been enabled.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Analyzing data, designing content, and managing projects all work better with a dedicated visual interface,” Anthropic said in a blog post announcing the feature. “Combined with Claude’s intelligence, you can work and iterate faster than either could offer alone.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new feature is available to Pro, Max, Team, and Enterprise subscribers, but not free users. Eligible users can activate the tools at claude.ai/directory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system is similar to OpenAI’s Apps system, which launched in October and enables interactive third-party tools. Both systems of app integrations are built on the Model Context Protocol (MCP), an open standard introduced by Anthropic in 2024. MCP launched support for apps in November, drawing on work from both companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new apps will be particularly powerful when integrated with Claude Cowork, an all-purpose agent tool launched by Anthropic last week. Built on top of Claude Code, Cowork lets users assign multistage tasks that draw on large and open-ended datasets — tasks that would have previously required terminal commands. Combined with the new apps feature, Cowork could be granted access to cloud files or ongoing work projects. For example, Cowork could update a marketing graphic in Figma or use new data from the company’s Box instance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apps are not available in Cowork at launch, but Anthropic said the integration would be “coming soon.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agentic systems can be unpredictable, and Anthropic’s own safety documentation for Cowork encourages users to monitor the agent closely and not grant any unnecessary permissions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Be cautious about granting access to sensitive information like financial documents, credentials, or personal records,” the company recommends. “Consider creating a dedicated working folder for Claude rather than granting broad access.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/anthropic-launches-interactive-claude-apps-including-slack-and-other-workplace-tools/</guid><pubDate>Mon, 26 Jan 2026 18:00:00 +0000</pubDate></item><item><title>[NEW] Inside OpenAI’s big play for science (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260122_openaivision_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In the three years since ChatGPT’s explosive debut, OpenAI’s technology has upended a remarkable range of everyday activities at home, at work, in schools—anywhere people have a browser open or a phone out, which is everywhere.&lt;/p&gt;  &lt;p&gt;Now OpenAI is making an explicit play for scientists. In October, the firm announced that it had launched a whole new team, called OpenAI for Science, dedicated to exploring how its large language models could help scientists and tweaking its tools to support them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The last couple of months have seen a slew of social media posts and academic publications in which mathematicians, physicists, biologists, and others have described how LLMs (and OpenAI’s GPT-5 in particular) have helped them make a discovery or nudged them toward a solution they might otherwise have missed. In part, OpenAI for Science was set up to engage with this community.&lt;/p&gt;  &lt;p&gt;And yet OpenAI is also late to the party. Google DeepMind, the rival firm behind groundbreaking scientific models such as AlphaFold and AlphaEvolve, has had an AI-for-science team for years. (When I spoke to Google DeepMind’s CEO and cofounder Demis Hassabis in 2023 about that team, he told me: “This is the reason I started DeepMind … In fact, it’s why I’ve worked my whole career in AI.”)&lt;/p&gt; 
 &lt;p&gt;So why now? How does a push into science fit with OpenAI’s wider mission? And what exactly is the firm hoping to achieve?&lt;/p&gt;  &lt;p&gt;I put these questions to Kevin Weil, a vice president at OpenAI who leads the new OpenAI for Science team, in an exclusive interview last week.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;On mission&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Weil is a product guy. He joined OpenAI a couple of years ago as chief product officer after being head of product at Twitter and Instagram. But he started out as a scientist. He got two-thirds of the way through a PhD in particle physics at Stanford University before ditching academia for the Silicon Valley dream. Weil is keen to highlight his pedigree: “I thought I was going to be a physics professor for the rest of my life,” he says. “I still read math books on vacation.”&lt;/p&gt;  &lt;p&gt;Asked how OpenAI for Science fits with the firm’s existing lineup of white-collar productivity tools or the viral video app Sora, Weil recites the company mantra: “The mission of OpenAI is to try and build artificial general intelligence and, you know, make it beneficial for all of humanity.”&lt;/p&gt;  &lt;p&gt;Just imagine the future impact this technology could have on science he says: New medicines, new materials, new devices. “Think about it helping us understand the nature of reality, helping us think through open problems. Maybe the biggest, most positive impact we’re going to see from AGI will actually be from its ability to accelerate science.”&lt;/p&gt;  &lt;p&gt;He adds: “With GPT-5, we saw that becoming possible.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;As Weil tells it, LLMs are now good enough to be useful scientific collaborators. They can spitball ideas, suggest novel directions to explore, and find fruitful parallels between new problems and old solutions published in obscure journals decades ago or in foreign languages.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;That wasn’t the case a year or so ago. Since it announced its first so-called reasoning model—a type of LLM that can break down problems into multiple steps and work through them one by one—in December 2024, OpenAI has been pushing the envelope of what the technology can do. Reasoning models have made LLMs far better at solving math and logic problems than they used to be.&amp;nbsp;“You go back a few years and we were all collectively mind-blown that the models could get an 800 on the SAT,” says Weil.&lt;/p&gt;  &lt;p&gt;But soon LLMs were acing math competitions and solving graduate-level physics problems. Last year, OpenAI and Google DeepMind both announced that their LLMs had achieved gold-medal-level performance in the International Math Olympiad, one of the toughest math contests in the world. “These models are no longer just better than 90% of grad students,” says Weil. “They’re really at the frontier of human abilities.”&lt;/p&gt;  &lt;p&gt;That’s a huge claim, and it comes with caveats. Still, there’s no doubt that GPT-5, which includes a reasoning model,  is a big improvement on GPT-4 when it comes to complicated problem-solving. Measured against an industry benchmark known as GPQA, which includes more than 400 multiple-choice questions that test PhD-level knowledge in biology, physics, and chemistry, GPT-4 scores 39%, well below the human-expert baseline of around 70%. According to OpenAI, GPT-5.2 (the latest update to the model, released in December) scores 92%.&amp;nbsp;&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Overhyped&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The excitement is evident—and perhaps excessive. In October, senior figures at OpenAI, including Weil, boasted on X that GPT-5 had found solutions to several unsolved math problems. Mathematicians were quick to point out that in fact what GPT-5 appeared to have done was dig up existing solutions in old research papers, including at least one written in German. That was still useful, but it wasn’t the achievement OpenAI seemed to have claimed. Weil and his colleagues deleted their posts.&lt;/p&gt;  &lt;p&gt;Now Weil is more careful. It is often enough to find answers that exist but have been forgotten, he says: “We collectively stand on the shoulders of giants, and if LLMs can kind of accumulate that knowledge so that we don’t spend time struggling on a problem that is already solved, that’s an acceleration all of its own.”&lt;/p&gt;  &lt;p&gt;He plays down the idea that LLMs are about to come up with a game-changing new discovery. “I don’t think models are there yet,” he says. “Maybe they’ll get there. I’m optimistic that they will.”&lt;/p&gt;  &lt;p&gt;But, he insists, that’s not the mission: “Our mission is to accelerate science. And I don’t think the bar for the acceleration of science is, like, Einstein-level reimagining of an entire field.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;For Weil, the question is this: “Does science actually happen faster because scientists plus models can do much more, and do it more quickly, than scientists alone? I think we’re already seeing that.”&lt;/p&gt;  &lt;p&gt;In November, OpenAI published a series of anecdotal case studies contributed by scientists, both inside and outside the company, that illustrated how they had used GPT-5 and how it had helped. “Most of the cases were scientists that were already using GPT-5 directly in their research and had come to us one way or another saying, ‘Look at what I’m able to do with these tools,’” says Weil.&lt;/p&gt;  &lt;p&gt;The key things that GPT-5 seems to be good at are finding references and connections to existing work that scientists were not aware of, which sometimes sparks new ideas; helping scientists sketch mathematical proofs; and suggesting ways for scientists to test hypotheses in the lab.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“GPT 5.2 has read substantially every paper written in the last 30 years,” says Weil. “And it understands not just the field that a particular scientist is working in; it can bring together analogies from other, unrelated fields.”&lt;/p&gt; 
 &lt;p&gt;“That’s incredibly powerful,” he continues. “You can always find a human collaborator in an adjacent field, but it’s difficult to find, you know, a thousand collaborators in all thousand adjacent fields that might matter. And in addition to that, I can work with the model late at night—it doesn’t sleep—and I can ask it 10 things in parallel, which is kind of awkward to do to a human.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Solving problems&lt;/h3&gt;  &lt;p&gt;Most of the scientists OpenAI reached out to back up Weil’s position.&lt;/p&gt; 
 &lt;p&gt;Robert Scherrer, a professor of physics and astronomy at Vanderbilt University, only played around with ChatGPT for fun (“I used to it rewrite the theme song for &lt;em&gt;Gilligan’s Island&lt;/em&gt; in the style of &lt;em&gt;Beowulf&lt;/em&gt;, which it did very well,” he tells me) until his Vanderbilt colleague Alex Lupsasca, a fellow physicist who now works at OpenAI, told him that GPT-5 had helped solve a problem he’d been working on.&lt;/p&gt;  &lt;p&gt;Lupsasca gave Scherrer access to GPT-5 Pro, OpenAI’s $200-a-month premium subscription. “It managed to solve a problem that I and my graduate student could not solve despite working on it for several months,” says Scherrer.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;It’s not perfect, he says: “GTP-5 still makes dumb mistakes. Of course, I do too, but the mistakes GPT-5 makes are even dumber.” And yet it keeps getting better, he says: “If current trends continue—and that’s a big if—I suspect that all scientists will be using LLMs soon.”&lt;/p&gt;  &lt;p&gt;Derya Unutmaz, a professor of biology at the Jackson Laboratory, a nonprofit research institute, uses GPT-5 to brainstorm ideas, summarize papers, and plan experiments in his work studying the immune system. In the case study he shared with OpenAI, Unutmaz used GPT-5 to analyze an old data set that his team had previously looked at. The model came up with fresh insights and interpretations.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“LLMs are already essential for scientists,” he says. “When you can complete analysis of data sets that used to take months, not using them is not an option anymore.”&lt;/p&gt;  &lt;p&gt;Nikita Zhivotovskiy, a statistician at the University of California, Berkeley, says he has been using LLMs in his research since the first version of ChatGPT came out.&lt;/p&gt; 
 &lt;p&gt;Like Scherrer, he finds LLMs most useful when they highlight unexpected connections between his own work and existing results he did not know about. “I believe that LLMs are becoming an essential technical tool for scientists, much like computers and the internet did before,” he says. “I expect a long-term disadvantage for those who do not use them.”&lt;/p&gt;  &lt;p&gt;But he does not expect LLMs to make novel discoveries anytime soon. “I have seen very few genuinely fresh ideas or arguments that would be worth a publication on their own,” he says. “So far, they seem to mainly combine existing results, sometimes incorrectly, rather than produce genuinely new approaches.”&lt;/p&gt;  &lt;p&gt;I also contacted a handful of scientists who are not connected to OpenAI.&lt;/p&gt;  &lt;p&gt;Andy Cooper, a professor of chemistry at the University of Liverpool and director of the Leverhulme Research Centre for Functional Materials Design, is less enthusiastic. “We have not found, yet, that LLMs are fundamentally changing the way that science is done,” he says. “But our recent results suggest that they do have a place.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Cooper is leading a project to develop a so-called AI scientist that can fully automate parts of the scientific workflow. He says that his team doesn’t use LLMs to come up with ideas. But the tech is starting to prove useful as part of a wider automated system where an LLM can help direct robots, for example.&lt;/p&gt;  &lt;p&gt;“My guess is that LLMs might stick more in robotic workflows, at least initially, because I’m not sure that people are ready to be told what to do by an LLM,” says Cooper. “I’m certainly not.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Making errors &lt;/h3&gt;  &lt;p&gt;LLMs may be becoming more and more useful, but caution is still key. In December, Jonathan Oppenheim, a scientist who works on quantum mechanics, called out a mistake that had made its way into a scientific journal. “OpenAI leadership are promoting a paper in Physics Letters B where GPT-5 proposed the main idea—possibly the first peer-reviewed paper where an LLM generated the core contribution,” Oppenheim posted on X. “One small problem: GPT-5’s idea tests the wrong thing.”&lt;/p&gt;  &lt;p&gt;He continued: “GPT-5 was asked for a test that detects nonlinear theories. It provided a test that detects nonlocal ones. Related-sounding, but different. It’s like asking for a COVID test, and the LLM cheerfully hands you a test for chickenpox.”&lt;/p&gt;  &lt;p&gt;It is clear that a lot of scientists are finding innovative and intuitive ways to engage with LLMs. It is also clear that the technology makes mistakes that can be so subtle even experts miss them.&lt;/p&gt;  &lt;p&gt;Part of the problem is the way ChatGPT can flatter you into letting down your guard. As Oppenheim put it: “A core issue is that LLMs are being trained to validate the user, while science needs tools that challenge us.” In an extreme case, one individual (who was not a scientist) was persuaded by ChatGPT into thinking for months that he’d invented a new branch of mathematics.&lt;/p&gt;  &lt;p&gt;Of course, Weil is well aware of the problem of hallucination. But he insists that newer models are hallucinating less and less. Even so, focusing on hallucination might be missing the point, he says.&lt;/p&gt;  &lt;p&gt;“One of my teammates here, an ex math professor, said something that stuck with me,” says Weil. “He said: ‘When I’m doing research, if I’m bouncing ideas off a colleague, I’m wrong 90% of the time and that’s kind of the point. We’re both spitballing ideas and trying to find something that works.’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;“That’s actually a desirable place to be,” says Weil. “If you say enough wrong things and then somebody stumbles on a grain of truth and then the other person seizes on it and says, ‘Oh, yeah, that’s not quite right, but what if we—’ You gradually kind of find your trail through the woods.”&lt;/p&gt;  &lt;p&gt;This is Weil’s core vision for OpenAI for Science. GPT-5 is good, but it is not an oracle. The value of this technology is in pointing people in new directions, not coming up with definitive answers, he says.&lt;/p&gt;  &lt;p&gt;In fact, one of the things OpenAI is now looking at is making GPT-5 dial down its confidence when it delivers a response. Instead of saying &lt;em&gt;Here’s the answer&lt;/em&gt;, it might tell scientists: &lt;em&gt;Here’s something to consider&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;“That’s actually something that we are spending a bunch of time on,” says Weil. “Trying to make sure that the model has some sort of epistemological humility.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Watching the watchers&lt;/h3&gt;  &lt;p&gt;Another thing OpenAI is looking at is how to use GPT-5 to fact-check GPT-5. It’s often the case that if you feed one of GPT-5’s answers back into the model, it will pick it apart and highlight mistakes.&lt;/p&gt;  &lt;p&gt;“You can kind of hook the model up as its own critic,” says Weil. “Then you can get a workflow where the model is thinking and then it goes to another model, and if that model finds things that it could improve, then it passes it back to the original model and says, ‘Hey, wait a minute—this part wasn’t right, but this part was interesting. Keep it.’ It’s almost like a couple of agents working together and you only see the output once it passes the critic.”&lt;/p&gt;  &lt;p&gt;What Weil is describing also sounds a lot like what Google DeepMind did with AlphaEvolve, a tool that wrapped the firms LLM, Gemini, inside a wider system that filtered out the good responses from the bad and fed them back in again to be improved on. Google DeepMind has used AlphaEvolve to solve several real-world problems.&lt;/p&gt;  &lt;p&gt;OpenAI faces stiff competition from rival firms, whose own LLMs can do most, if not all, of the things it claims for its own models. If that’s the case, why should scientists use GPT-5 instead of Gemini or Anthropic’s Claude, families of models that are themselves improving every year? Ultimately, OpenAI for Science may be as much an effort to plant a flag in new territory as anything else. The real innovations are still to come.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I think 2026 will be for science what 2025 was for software engineering,” says Weil. “At the beginning of 2025, if you were using AI to write most of your code, you were an early adopter. Whereas 12 months later, if you’re not using AI to write most of your code, you’re probably falling behind. We’re now seeing those same early flashes for science as we did for code.”&lt;/p&gt;  &lt;p&gt;He continues: “I think that in a year, if you’re a scientist and you’re not heavily using AI, you’ll be missing an opportunity to increase the quality and pace of your thinking.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260122_openaivision_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In the three years since ChatGPT’s explosive debut, OpenAI’s technology has upended a remarkable range of everyday activities at home, at work, in schools—anywhere people have a browser open or a phone out, which is everywhere.&lt;/p&gt;  &lt;p&gt;Now OpenAI is making an explicit play for scientists. In October, the firm announced that it had launched a whole new team, called OpenAI for Science, dedicated to exploring how its large language models could help scientists and tweaking its tools to support them.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The last couple of months have seen a slew of social media posts and academic publications in which mathematicians, physicists, biologists, and others have described how LLMs (and OpenAI’s GPT-5 in particular) have helped them make a discovery or nudged them toward a solution they might otherwise have missed. In part, OpenAI for Science was set up to engage with this community.&lt;/p&gt;  &lt;p&gt;And yet OpenAI is also late to the party. Google DeepMind, the rival firm behind groundbreaking scientific models such as AlphaFold and AlphaEvolve, has had an AI-for-science team for years. (When I spoke to Google DeepMind’s CEO and cofounder Demis Hassabis in 2023 about that team, he told me: “This is the reason I started DeepMind … In fact, it’s why I’ve worked my whole career in AI.”)&lt;/p&gt; 
 &lt;p&gt;So why now? How does a push into science fit with OpenAI’s wider mission? And what exactly is the firm hoping to achieve?&lt;/p&gt;  &lt;p&gt;I put these questions to Kevin Weil, a vice president at OpenAI who leads the new OpenAI for Science team, in an exclusive interview last week.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;On mission&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Weil is a product guy. He joined OpenAI a couple of years ago as chief product officer after being head of product at Twitter and Instagram. But he started out as a scientist. He got two-thirds of the way through a PhD in particle physics at Stanford University before ditching academia for the Silicon Valley dream. Weil is keen to highlight his pedigree: “I thought I was going to be a physics professor for the rest of my life,” he says. “I still read math books on vacation.”&lt;/p&gt;  &lt;p&gt;Asked how OpenAI for Science fits with the firm’s existing lineup of white-collar productivity tools or the viral video app Sora, Weil recites the company mantra: “The mission of OpenAI is to try and build artificial general intelligence and, you know, make it beneficial for all of humanity.”&lt;/p&gt;  &lt;p&gt;Just imagine the future impact this technology could have on science he says: New medicines, new materials, new devices. “Think about it helping us understand the nature of reality, helping us think through open problems. Maybe the biggest, most positive impact we’re going to see from AGI will actually be from its ability to accelerate science.”&lt;/p&gt;  &lt;p&gt;He adds: “With GPT-5, we saw that becoming possible.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;As Weil tells it, LLMs are now good enough to be useful scientific collaborators. They can spitball ideas, suggest novel directions to explore, and find fruitful parallels between new problems and old solutions published in obscure journals decades ago or in foreign languages.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;That wasn’t the case a year or so ago. Since it announced its first so-called reasoning model—a type of LLM that can break down problems into multiple steps and work through them one by one—in December 2024, OpenAI has been pushing the envelope of what the technology can do. Reasoning models have made LLMs far better at solving math and logic problems than they used to be.&amp;nbsp;“You go back a few years and we were all collectively mind-blown that the models could get an 800 on the SAT,” says Weil.&lt;/p&gt;  &lt;p&gt;But soon LLMs were acing math competitions and solving graduate-level physics problems. Last year, OpenAI and Google DeepMind both announced that their LLMs had achieved gold-medal-level performance in the International Math Olympiad, one of the toughest math contests in the world. “These models are no longer just better than 90% of grad students,” says Weil. “They’re really at the frontier of human abilities.”&lt;/p&gt;  &lt;p&gt;That’s a huge claim, and it comes with caveats. Still, there’s no doubt that GPT-5, which includes a reasoning model,  is a big improvement on GPT-4 when it comes to complicated problem-solving. Measured against an industry benchmark known as GPQA, which includes more than 400 multiple-choice questions that test PhD-level knowledge in biology, physics, and chemistry, GPT-4 scores 39%, well below the human-expert baseline of around 70%. According to OpenAI, GPT-5.2 (the latest update to the model, released in December) scores 92%.&amp;nbsp;&lt;/p&gt; 

 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Overhyped&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The excitement is evident—and perhaps excessive. In October, senior figures at OpenAI, including Weil, boasted on X that GPT-5 had found solutions to several unsolved math problems. Mathematicians were quick to point out that in fact what GPT-5 appeared to have done was dig up existing solutions in old research papers, including at least one written in German. That was still useful, but it wasn’t the achievement OpenAI seemed to have claimed. Weil and his colleagues deleted their posts.&lt;/p&gt;  &lt;p&gt;Now Weil is more careful. It is often enough to find answers that exist but have been forgotten, he says: “We collectively stand on the shoulders of giants, and if LLMs can kind of accumulate that knowledge so that we don’t spend time struggling on a problem that is already solved, that’s an acceleration all of its own.”&lt;/p&gt;  &lt;p&gt;He plays down the idea that LLMs are about to come up with a game-changing new discovery. “I don’t think models are there yet,” he says. “Maybe they’ll get there. I’m optimistic that they will.”&lt;/p&gt;  &lt;p&gt;But, he insists, that’s not the mission: “Our mission is to accelerate science. And I don’t think the bar for the acceleration of science is, like, Einstein-level reimagining of an entire field.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;For Weil, the question is this: “Does science actually happen faster because scientists plus models can do much more, and do it more quickly, than scientists alone? I think we’re already seeing that.”&lt;/p&gt;  &lt;p&gt;In November, OpenAI published a series of anecdotal case studies contributed by scientists, both inside and outside the company, that illustrated how they had used GPT-5 and how it had helped. “Most of the cases were scientists that were already using GPT-5 directly in their research and had come to us one way or another saying, ‘Look at what I’m able to do with these tools,’” says Weil.&lt;/p&gt;  &lt;p&gt;The key things that GPT-5 seems to be good at are finding references and connections to existing work that scientists were not aware of, which sometimes sparks new ideas; helping scientists sketch mathematical proofs; and suggesting ways for scientists to test hypotheses in the lab.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“GPT 5.2 has read substantially every paper written in the last 30 years,” says Weil. “And it understands not just the field that a particular scientist is working in; it can bring together analogies from other, unrelated fields.”&lt;/p&gt; 
 &lt;p&gt;“That’s incredibly powerful,” he continues. “You can always find a human collaborator in an adjacent field, but it’s difficult to find, you know, a thousand collaborators in all thousand adjacent fields that might matter. And in addition to that, I can work with the model late at night—it doesn’t sleep—and I can ask it 10 things in parallel, which is kind of awkward to do to a human.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Solving problems&lt;/h3&gt;  &lt;p&gt;Most of the scientists OpenAI reached out to back up Weil’s position.&lt;/p&gt; 
 &lt;p&gt;Robert Scherrer, a professor of physics and astronomy at Vanderbilt University, only played around with ChatGPT for fun (“I used to it rewrite the theme song for &lt;em&gt;Gilligan’s Island&lt;/em&gt; in the style of &lt;em&gt;Beowulf&lt;/em&gt;, which it did very well,” he tells me) until his Vanderbilt colleague Alex Lupsasca, a fellow physicist who now works at OpenAI, told him that GPT-5 had helped solve a problem he’d been working on.&lt;/p&gt;  &lt;p&gt;Lupsasca gave Scherrer access to GPT-5 Pro, OpenAI’s $200-a-month premium subscription. “It managed to solve a problem that I and my graduate student could not solve despite working on it for several months,” says Scherrer.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;It’s not perfect, he says: “GTP-5 still makes dumb mistakes. Of course, I do too, but the mistakes GPT-5 makes are even dumber.” And yet it keeps getting better, he says: “If current trends continue—and that’s a big if—I suspect that all scientists will be using LLMs soon.”&lt;/p&gt;  &lt;p&gt;Derya Unutmaz, a professor of biology at the Jackson Laboratory, a nonprofit research institute, uses GPT-5 to brainstorm ideas, summarize papers, and plan experiments in his work studying the immune system. In the case study he shared with OpenAI, Unutmaz used GPT-5 to analyze an old data set that his team had previously looked at. The model came up with fresh insights and interpretations.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“LLMs are already essential for scientists,” he says. “When you can complete analysis of data sets that used to take months, not using them is not an option anymore.”&lt;/p&gt;  &lt;p&gt;Nikita Zhivotovskiy, a statistician at the University of California, Berkeley, says he has been using LLMs in his research since the first version of ChatGPT came out.&lt;/p&gt; 
 &lt;p&gt;Like Scherrer, he finds LLMs most useful when they highlight unexpected connections between his own work and existing results he did not know about. “I believe that LLMs are becoming an essential technical tool for scientists, much like computers and the internet did before,” he says. “I expect a long-term disadvantage for those who do not use them.”&lt;/p&gt;  &lt;p&gt;But he does not expect LLMs to make novel discoveries anytime soon. “I have seen very few genuinely fresh ideas or arguments that would be worth a publication on their own,” he says. “So far, they seem to mainly combine existing results, sometimes incorrectly, rather than produce genuinely new approaches.”&lt;/p&gt;  &lt;p&gt;I also contacted a handful of scientists who are not connected to OpenAI.&lt;/p&gt;  &lt;p&gt;Andy Cooper, a professor of chemistry at the University of Liverpool and director of the Leverhulme Research Centre for Functional Materials Design, is less enthusiastic. “We have not found, yet, that LLMs are fundamentally changing the way that science is done,” he says. “But our recent results suggest that they do have a place.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Cooper is leading a project to develop a so-called AI scientist that can fully automate parts of the scientific workflow. He says that his team doesn’t use LLMs to come up with ideas. But the tech is starting to prove useful as part of a wider automated system where an LLM can help direct robots, for example.&lt;/p&gt;  &lt;p&gt;“My guess is that LLMs might stick more in robotic workflows, at least initially, because I’m not sure that people are ready to be told what to do by an LLM,” says Cooper. “I’m certainly not.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Making errors &lt;/h3&gt;  &lt;p&gt;LLMs may be becoming more and more useful, but caution is still key. In December, Jonathan Oppenheim, a scientist who works on quantum mechanics, called out a mistake that had made its way into a scientific journal. “OpenAI leadership are promoting a paper in Physics Letters B where GPT-5 proposed the main idea—possibly the first peer-reviewed paper where an LLM generated the core contribution,” Oppenheim posted on X. “One small problem: GPT-5’s idea tests the wrong thing.”&lt;/p&gt;  &lt;p&gt;He continued: “GPT-5 was asked for a test that detects nonlinear theories. It provided a test that detects nonlocal ones. Related-sounding, but different. It’s like asking for a COVID test, and the LLM cheerfully hands you a test for chickenpox.”&lt;/p&gt;  &lt;p&gt;It is clear that a lot of scientists are finding innovative and intuitive ways to engage with LLMs. It is also clear that the technology makes mistakes that can be so subtle even experts miss them.&lt;/p&gt;  &lt;p&gt;Part of the problem is the way ChatGPT can flatter you into letting down your guard. As Oppenheim put it: “A core issue is that LLMs are being trained to validate the user, while science needs tools that challenge us.” In an extreme case, one individual (who was not a scientist) was persuaded by ChatGPT into thinking for months that he’d invented a new branch of mathematics.&lt;/p&gt;  &lt;p&gt;Of course, Weil is well aware of the problem of hallucination. But he insists that newer models are hallucinating less and less. Even so, focusing on hallucination might be missing the point, he says.&lt;/p&gt;  &lt;p&gt;“One of my teammates here, an ex math professor, said something that stuck with me,” says Weil. “He said: ‘When I’m doing research, if I’m bouncing ideas off a colleague, I’m wrong 90% of the time and that’s kind of the point. We’re both spitballing ideas and trying to find something that works.’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;“That’s actually a desirable place to be,” says Weil. “If you say enough wrong things and then somebody stumbles on a grain of truth and then the other person seizes on it and says, ‘Oh, yeah, that’s not quite right, but what if we—’ You gradually kind of find your trail through the woods.”&lt;/p&gt;  &lt;p&gt;This is Weil’s core vision for OpenAI for Science. GPT-5 is good, but it is not an oracle. The value of this technology is in pointing people in new directions, not coming up with definitive answers, he says.&lt;/p&gt;  &lt;p&gt;In fact, one of the things OpenAI is now looking at is making GPT-5 dial down its confidence when it delivers a response. Instead of saying &lt;em&gt;Here’s the answer&lt;/em&gt;, it might tell scientists: &lt;em&gt;Here’s something to consider&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;“That’s actually something that we are spending a bunch of time on,” says Weil. “Trying to make sure that the model has some sort of epistemological humility.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Watching the watchers&lt;/h3&gt;  &lt;p&gt;Another thing OpenAI is looking at is how to use GPT-5 to fact-check GPT-5. It’s often the case that if you feed one of GPT-5’s answers back into the model, it will pick it apart and highlight mistakes.&lt;/p&gt;  &lt;p&gt;“You can kind of hook the model up as its own critic,” says Weil. “Then you can get a workflow where the model is thinking and then it goes to another model, and if that model finds things that it could improve, then it passes it back to the original model and says, ‘Hey, wait a minute—this part wasn’t right, but this part was interesting. Keep it.’ It’s almost like a couple of agents working together and you only see the output once it passes the critic.”&lt;/p&gt;  &lt;p&gt;What Weil is describing also sounds a lot like what Google DeepMind did with AlphaEvolve, a tool that wrapped the firms LLM, Gemini, inside a wider system that filtered out the good responses from the bad and fed them back in again to be improved on. Google DeepMind has used AlphaEvolve to solve several real-world problems.&lt;/p&gt;  &lt;p&gt;OpenAI faces stiff competition from rival firms, whose own LLMs can do most, if not all, of the things it claims for its own models. If that’s the case, why should scientists use GPT-5 instead of Gemini or Anthropic’s Claude, families of models that are themselves improving every year? Ultimately, OpenAI for Science may be as much an effort to plant a flag in new territory as anything else. The real innovations are still to come.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I think 2026 will be for science what 2025 was for software engineering,” says Weil. “At the beginning of 2025, if you were using AI to write most of your code, you were an early adopter. Whereas 12 months later, if you’re not using AI to write most of your code, you’re probably falling behind. We’re now seeing those same early flashes for science as we did for code.”&lt;/p&gt;  &lt;p&gt;He continues: “I think that in a year, if you’re a scientist and you’re not heavily using AI, you’ll be missing an opportunity to increase the quality and pace of your thinking.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/26/1131728/inside-openais-big-play-for-science/</guid><pubDate>Mon, 26 Jan 2026 18:32:15 +0000</pubDate></item><item><title>[NEW] AI startup CVector raises $5M for its industrial ‘nervous system’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/ai-startup-cvector-raises-5m-for-its-industrial-nervous-system/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/factory-GettyImages.jpg?resize=1200,1173" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Industrial AI startup CVector built a brain and nervous system for big industry. Now, founders Richard Zhang and Tyler Ruggles are tasked with a bigger challenge: showing customers and investors how this AI-powered software layer translates to real savings on an industrial scale.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The New York-based startup has had some success following its pre-seed funding round last July. Its system is now running with real customers, including public utilities, advanced manufacturing facilities, and chemical producers. It’s given the duo more concrete examples of what problems they can solve — and money they can save — for their big industry clients.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“One of the core things we’re witnessing,” he said, is customers “really lack the tool to translate a small action, like turning on and off a valve, [into] did that just save me money?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a homeowner with bills to pay, it’s a bit unnerving to think about one nondescript valve making such a big difference in the bottom line of a company and its customers. But it’s examples like this that helped CVector reach a new milestone, as it has now closed a $5 million seed round, Zhang and Ruggles told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing was led by Powerhouse Ventures and included a mix of venture and strategic backing, with participation from early-stage funds like Fusion Fund and Myriad Venture Partners, as well as Hitachi’s corporate venture arm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the funding round closed, CVector is talking a bit more about some of its first customers — and just how different they are. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt; “The joy of the last, say, six to eight months has been going to the industrial heartland, to all of these places that are just in the middle of nowhere, but have massive production plants that are either reinventing themselves or really transforming how they make decisions,” Zhang said in an interview.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;One of those customers is a metals processing company based in Iowa called ATEK Metal Technologies, which makes aluminum castings for Harley-Davidson motorcycles, among other things. CVector is doing things like helping spot potential problems that could lead to equipment downtime, monitor the whole plant’s energy efficiency, and keep an eye on commodity prices that impact raw material cost.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That is, to me, such a good example where this is really skilled labor, and they will need all the help they can get from for us, from the software side, from technology side, to really help that group of people transform, take the business to the next level so they can keep growing,” Zhang said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finding optimizations in older plants might seem like the most obvious path for a company like CVector. But it has also picked up startups as customers, too, including Ammobia, a materials science startup based in San Francisco that is working to lower the cost of making ammonia. And yet the work CVector is doing for Ammobia is surprisingly similar to what it’s doing for ATEK, Zhang said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CVector is also growing. The company is up to 12 people, and it’s locked down its first physical office in the financial district in Manhattan. Zhang said he’s been attracting talent from the worlds of fintech and finance, especially hedge funds. The latter is ripe for recruiting, he said, since the people who work in the hedge fund industry are already pretty focused on using data to gain a financial edge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That’s the core of our sales pitch; it’s what we call ‘operational economics,’” Zhang said. “We position it to sit between the operation of the plant and the actual economics — the margin of how much you’re making money.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zhang still sees public utilities as a great place to apply CVector’s technology, though. (That’s where the valve example came from.) And he’s found that even these types of customers have become far more fluent in talking about the kinds of work CVector does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Tyler and I were just talking about how when we first started [the] company almost exactly a year ago, it was still like a taboo to talk about AI in general. There was a 50/50 chance if the customer would embrace AI or just kind of discredit you, right?” he said. “But now, over the especially last six months, everyone is asking for more AI-native solutions, even when sometimes the ROI calculation might not be clear. This kind of adoption craze is real.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ruggles said that’s in large part because what CVector does ultimately comes down to one thing: money. And with so much uncertainty in the world, managing costs has only gotten harder.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re at this time when companies are really intimately worried about their supply chain and the costs and variability there, and being able to kind of layer AI on top [to make an] economic model of a facility, it’s really resonated with a lot of customers, whether it’s old and industrial in the heartland, or whether it’s new energy producers who are trying to do new and novel things,” he said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/factory-GettyImages.jpg?resize=1200,1173" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Industrial AI startup CVector built a brain and nervous system for big industry. Now, founders Richard Zhang and Tyler Ruggles are tasked with a bigger challenge: showing customers and investors how this AI-powered software layer translates to real savings on an industrial scale.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The New York-based startup has had some success following its pre-seed funding round last July. Its system is now running with real customers, including public utilities, advanced manufacturing facilities, and chemical producers. It’s given the duo more concrete examples of what problems they can solve — and money they can save — for their big industry clients.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“One of the core things we’re witnessing,” he said, is customers “really lack the tool to translate a small action, like turning on and off a valve, [into] did that just save me money?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a homeowner with bills to pay, it’s a bit unnerving to think about one nondescript valve making such a big difference in the bottom line of a company and its customers. But it’s examples like this that helped CVector reach a new milestone, as it has now closed a $5 million seed round, Zhang and Ruggles told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing was led by Powerhouse Ventures and included a mix of venture and strategic backing, with participation from early-stage funds like Fusion Fund and Myriad Venture Partners, as well as Hitachi’s corporate venture arm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With the funding round closed, CVector is talking a bit more about some of its first customers — and just how different they are. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt; “The joy of the last, say, six to eight months has been going to the industrial heartland, to all of these places that are just in the middle of nowhere, but have massive production plants that are either reinventing themselves or really transforming how they make decisions,” Zhang said in an interview.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;One of those customers is a metals processing company based in Iowa called ATEK Metal Technologies, which makes aluminum castings for Harley-Davidson motorcycles, among other things. CVector is doing things like helping spot potential problems that could lead to equipment downtime, monitor the whole plant’s energy efficiency, and keep an eye on commodity prices that impact raw material cost.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That is, to me, such a good example where this is really skilled labor, and they will need all the help they can get from for us, from the software side, from technology side, to really help that group of people transform, take the business to the next level so they can keep growing,” Zhang said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finding optimizations in older plants might seem like the most obvious path for a company like CVector. But it has also picked up startups as customers, too, including Ammobia, a materials science startup based in San Francisco that is working to lower the cost of making ammonia. And yet the work CVector is doing for Ammobia is surprisingly similar to what it’s doing for ATEK, Zhang said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;CVector is also growing. The company is up to 12 people, and it’s locked down its first physical office in the financial district in Manhattan. Zhang said he’s been attracting talent from the worlds of fintech and finance, especially hedge funds. The latter is ripe for recruiting, he said, since the people who work in the hedge fund industry are already pretty focused on using data to gain a financial edge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That’s the core of our sales pitch; it’s what we call ‘operational economics,’” Zhang said. “We position it to sit between the operation of the plant and the actual economics — the margin of how much you’re making money.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zhang still sees public utilities as a great place to apply CVector’s technology, though. (That’s where the valve example came from.) And he’s found that even these types of customers have become far more fluent in talking about the kinds of work CVector does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Tyler and I were just talking about how when we first started [the] company almost exactly a year ago, it was still like a taboo to talk about AI in general. There was a 50/50 chance if the customer would embrace AI or just kind of discredit you, right?” he said. “But now, over the especially last six months, everyone is asking for more AI-native solutions, even when sometimes the ROI calculation might not be clear. This kind of adoption craze is real.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ruggles said that’s in large part because what CVector does ultimately comes down to one thing: money. And with so much uncertainty in the world, managing costs has only gotten harder.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re at this time when companies are really intimately worried about their supply chain and the costs and variability there, and being able to kind of layer AI on top [to make an] economic model of a facility, it’s really resonated with a lot of customers, whether it’s old and industrial in the heartland, or whether it’s new energy producers who are trying to do new and novel things,” he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/ai-startup-cvector-raises-5m-for-its-industrial-nervous-system/</guid><pubDate>Mon, 26 Jan 2026 19:10:03 +0000</pubDate></item><item><title>[NEW] “Wildly irresponsible”: DOT's use of AI to draft safety rules sparks concerns (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/01/wildly-irresponsible-dots-use-of-ai-to-draft-safety-rules-sparks-concerns/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Staffers warn DOT’s use of Gemini to draft rules could cause injuries and deaths.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1438737819-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1438737819-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Donald Iain Smith | Photodisc

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The US Department of Transportation apparently thinks it’s a good idea to use artificial intelligence to draft rules impacting the safety of airplanes, cars, and pipelines, a ProPublica investigation revealed Monday.&lt;/p&gt;
&lt;p&gt;It could be a problem if DOT becomes the first agency to use AI to draft rules, ProPublica pointed out, since AI is known to confidently get things wrong and hallucinate fabricated information. Staffers fear that any failure to catch AI errors could result in flawed laws, leading to lawsuits, injuries, or even deaths in the transportation system.&lt;/p&gt;
&lt;p&gt;But the DOT’s top lawyer, Gregory Zerzan, isn’t worried about that, December meeting notes revealed, because the point isn’t for AI to be perfect. It’s for AI to help speed up the rulemaking process, so that rules that take weeks or months to draft can instead be written within 30 days. According to Zerzan, DOT’s preferred tool, Google Gemini, can draft rules in under 30 minutes.&lt;/p&gt;
&lt;p&gt;“We don’t need the perfect rule on XYZ,” Zerzan told DOT staffers at the meeting. “We don’t even need a very good rule on XYZ. We want good enough.”&lt;/p&gt;
&lt;h2&gt;DOT staffers “deeply skeptical” of Gemini&lt;/h2&gt;
&lt;p&gt;ProPublica spoke to experts and granted six DOT staffers anonymity to discuss their concerns about DOT’s use of Google Gemini to draft rules.&lt;/p&gt;
&lt;p&gt;Some experts who monitor AI use in government told ProPublica that DOT could save time using Gemini as a research assistant “with plenty of supervision and transparency.” For example, at a presentation, DOT staffers were told that “most of what goes into the preambles of DOT regulatory documents is just ‘word salad,’” and “Gemini can do word salad.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;However, staffers told ProPublica they felt “deeply skeptical” that Gemini was up to the task. They emphasized that DOT rulemaking is “intricate work” requiring sometimes decades of “expertise in the subject at hand as well as in existing statutes, regulations, and case law.” Likely unsettling staffers further, ProPublica noted that a demonstration of Gemini’s rule-drafting produced a document missing key text, which a staffer would then have to fill in. Additionally, the DOT’s move comes after a year of AI hallucinations scrambling courts, with many lawyers fined and even judges admitting they can be fooled by fabricated information.&lt;/p&gt;
&lt;p&gt;Any errors in the rules could have serious consequences. These rules “touch virtually every facet of transportation safety,” keeping “airplanes in the sky,” preventing “gas pipelines from exploding,” and stopping “freight trains carrying toxic chemicals from skidding off the rails,” ProPublica reported.&lt;/p&gt;
&lt;p&gt;“It seems wildly irresponsible,” one staffer said.&lt;/p&gt;
&lt;p&gt;Despite staffers’ concerns, DOT appears to be racing forward with the plan, ProPublica reported. The department has already used Gemini to draft a “still-unpublished Federal Aviation Administration rule, according to a DOT staffer briefed on the matter.”&lt;/p&gt;
&lt;h2&gt;Trump “very excited” about AI drafting rules&lt;/h2&gt;
&lt;p&gt;Donald Trump has urged federal agencies to adopt AI at a rapid pace, but nowhere in his orders has the president pushed for AI to draft laws, ProPublica noted.&lt;/p&gt;
&lt;p&gt;However, Trump is “very excited” about the DOT initiative, Zerzan told staffers at the meeting, suggesting that Trump sees DOT as the “point of the spear” and expects other agencies to follow its lead.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;At DOT, Trump likely hopes to see many rules quickly updated to modernize airways and roadways. In a report highlighting the Office of Science and Technology Policy’s biggest “wins” in 2025, the White House credited DOT with “replacing decades-old rules with flexible, innovation-friendly frameworks,” including fast-tracking rules to allow for more automated vehicles on the roads.&lt;/p&gt;
&lt;p&gt;Right now, DOT expects that Gemini can be relied on to “handle 80 to 90 percent of the work of writing regulations,” ProPublica reported. Eventually all federal workers who rely on AI tools like Gemini to draft rules “would fall back into merely an oversight role, monitoring ‘AI-to-AI interactions,’” ProPublica reported.&lt;/p&gt;
&lt;h2&gt;Google silent on AI drafting safety rules&lt;/h2&gt;
&lt;p&gt;Google did not respond to Ars’ request to comment on this use case for Gemini, which could spread across government under Trump’s direction.&lt;/p&gt;
&lt;p&gt;Instead, the tech giant posted a blog on Monday, pitching Gemini for government more broadly, promising federal workers that AI would help with “creative problem-solving to the most critical aspects of their work.”&lt;/p&gt;
&lt;p&gt;Google has been competing with AI rivals for government contracts, undercutting OpenAI and Anthropic’s $1 deals by offering a year of access to Gemini for $0.47.&lt;/p&gt;
&lt;p&gt;The DOT contract seems important to Google. In a December blog, the company celebrated that DOT was “the first cabinet-level agency to fully transition its workforce away from legacy providers to Google Workspace with Gemini.”&lt;/p&gt;
&lt;p&gt;At that time, Google suggested this move would help DOT “ensure the United States has the safest, most efficient, and modern transportation system in the world.”&lt;/p&gt;
&lt;p&gt;Immediately, Google encouraged other federal leaders to launch their own efforts using Gemini.&lt;/p&gt;
&lt;p&gt;“We are committed to supporting the DOT’s digital transformation and stand ready to help other federal leaders across the government adopt this blueprint for their own mission successes,” Google’s blog said.&lt;/p&gt;
&lt;p&gt;DOT did not immediately respond to Ars’ request for comment.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Staffers warn DOT’s use of Gemini to draft rules could cause injuries and deaths.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1438737819-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-1438737819-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Donald Iain Smith | Photodisc

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The US Department of Transportation apparently thinks it’s a good idea to use artificial intelligence to draft rules impacting the safety of airplanes, cars, and pipelines, a ProPublica investigation revealed Monday.&lt;/p&gt;
&lt;p&gt;It could be a problem if DOT becomes the first agency to use AI to draft rules, ProPublica pointed out, since AI is known to confidently get things wrong and hallucinate fabricated information. Staffers fear that any failure to catch AI errors could result in flawed laws, leading to lawsuits, injuries, or even deaths in the transportation system.&lt;/p&gt;
&lt;p&gt;But the DOT’s top lawyer, Gregory Zerzan, isn’t worried about that, December meeting notes revealed, because the point isn’t for AI to be perfect. It’s for AI to help speed up the rulemaking process, so that rules that take weeks or months to draft can instead be written within 30 days. According to Zerzan, DOT’s preferred tool, Google Gemini, can draft rules in under 30 minutes.&lt;/p&gt;
&lt;p&gt;“We don’t need the perfect rule on XYZ,” Zerzan told DOT staffers at the meeting. “We don’t even need a very good rule on XYZ. We want good enough.”&lt;/p&gt;
&lt;h2&gt;DOT staffers “deeply skeptical” of Gemini&lt;/h2&gt;
&lt;p&gt;ProPublica spoke to experts and granted six DOT staffers anonymity to discuss their concerns about DOT’s use of Google Gemini to draft rules.&lt;/p&gt;
&lt;p&gt;Some experts who monitor AI use in government told ProPublica that DOT could save time using Gemini as a research assistant “with plenty of supervision and transparency.” For example, at a presentation, DOT staffers were told that “most of what goes into the preambles of DOT regulatory documents is just ‘word salad,’” and “Gemini can do word salad.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;However, staffers told ProPublica they felt “deeply skeptical” that Gemini was up to the task. They emphasized that DOT rulemaking is “intricate work” requiring sometimes decades of “expertise in the subject at hand as well as in existing statutes, regulations, and case law.” Likely unsettling staffers further, ProPublica noted that a demonstration of Gemini’s rule-drafting produced a document missing key text, which a staffer would then have to fill in. Additionally, the DOT’s move comes after a year of AI hallucinations scrambling courts, with many lawyers fined and even judges admitting they can be fooled by fabricated information.&lt;/p&gt;
&lt;p&gt;Any errors in the rules could have serious consequences. These rules “touch virtually every facet of transportation safety,” keeping “airplanes in the sky,” preventing “gas pipelines from exploding,” and stopping “freight trains carrying toxic chemicals from skidding off the rails,” ProPublica reported.&lt;/p&gt;
&lt;p&gt;“It seems wildly irresponsible,” one staffer said.&lt;/p&gt;
&lt;p&gt;Despite staffers’ concerns, DOT appears to be racing forward with the plan, ProPublica reported. The department has already used Gemini to draft a “still-unpublished Federal Aviation Administration rule, according to a DOT staffer briefed on the matter.”&lt;/p&gt;
&lt;h2&gt;Trump “very excited” about AI drafting rules&lt;/h2&gt;
&lt;p&gt;Donald Trump has urged federal agencies to adopt AI at a rapid pace, but nowhere in his orders has the president pushed for AI to draft laws, ProPublica noted.&lt;/p&gt;
&lt;p&gt;However, Trump is “very excited” about the DOT initiative, Zerzan told staffers at the meeting, suggesting that Trump sees DOT as the “point of the spear” and expects other agencies to follow its lead.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;At DOT, Trump likely hopes to see many rules quickly updated to modernize airways and roadways. In a report highlighting the Office of Science and Technology Policy’s biggest “wins” in 2025, the White House credited DOT with “replacing decades-old rules with flexible, innovation-friendly frameworks,” including fast-tracking rules to allow for more automated vehicles on the roads.&lt;/p&gt;
&lt;p&gt;Right now, DOT expects that Gemini can be relied on to “handle 80 to 90 percent of the work of writing regulations,” ProPublica reported. Eventually all federal workers who rely on AI tools like Gemini to draft rules “would fall back into merely an oversight role, monitoring ‘AI-to-AI interactions,’” ProPublica reported.&lt;/p&gt;
&lt;h2&gt;Google silent on AI drafting safety rules&lt;/h2&gt;
&lt;p&gt;Google did not respond to Ars’ request to comment on this use case for Gemini, which could spread across government under Trump’s direction.&lt;/p&gt;
&lt;p&gt;Instead, the tech giant posted a blog on Monday, pitching Gemini for government more broadly, promising federal workers that AI would help with “creative problem-solving to the most critical aspects of their work.”&lt;/p&gt;
&lt;p&gt;Google has been competing with AI rivals for government contracts, undercutting OpenAI and Anthropic’s $1 deals by offering a year of access to Gemini for $0.47.&lt;/p&gt;
&lt;p&gt;The DOT contract seems important to Google. In a December blog, the company celebrated that DOT was “the first cabinet-level agency to fully transition its workforce away from legacy providers to Google Workspace with Gemini.”&lt;/p&gt;
&lt;p&gt;At that time, Google suggested this move would help DOT “ensure the United States has the safest, most efficient, and modern transportation system in the world.”&lt;/p&gt;
&lt;p&gt;Immediately, Google encouraged other federal leaders to launch their own efforts using Gemini.&lt;/p&gt;
&lt;p&gt;“We are committed to supporting the DOT’s digital transformation and stand ready to help other federal leaders across the government adopt this blueprint for their own mission successes,” Google’s blog said.&lt;/p&gt;
&lt;p&gt;DOT did not immediately respond to Ars’ request for comment.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/01/wildly-irresponsible-dots-use-of-ai-to-draft-safety-rules-sparks-concerns/</guid><pubDate>Mon, 26 Jan 2026 20:13:47 +0000</pubDate></item><item><title>[NEW] YouTubers sue Snap for alleged copyright infringement in training its AI models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/youtubers-sue-snap-for-alleged-copyright-infringement-in-training-its-ai-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/01/evan-spiegel.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A group of YouTubers who are suing tech giants for scraping their videos without permission to train AI models has now added Snap to their list of defendants. The plaintiffs — internet content creators behind a trio of YouTube channels with roughly 6.2 million collective subscribers — allege that Snap has trained its AI systems on their video content for use in AI features like the app’s “Imagine Lens,” which allows users to edit images using text prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The plaintiffs earlier filed similar lawsuits against Nvidia, Meta, and ByteDance over similar matters.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the newly filed proposed class action suit, filed on Friday in the U.S. District Court for the Central District of California, the YouTubers specifically call out Snap for its use of a large-scale, video-language dataset known as HD-VILA-100M, and others that were designed for only academic and research purposes. To use these datasets for commercial purposes, the plaintiffs claim Snap circumvented YouTube’s technological restrictions, terms of service, and licensing limitations, which prohibit commercial use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The suit is seeking statutory damages and a permanent injunction to stop the alleged copyright infringement going forward.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The case itself is being led by the creators behind the h3h3 YouTube channel, with 5.52 million subscribers, and the smaller golfing channels MrShortGame Golf and Golfholics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s now one of many lawsuits pitting content creators against AI model providers, which have included copyright disputes from publishers, authors, newspapers, user-generated content sites, artists, and more. It’s also not the first case to hail from a YouTuber. According to the nonprofit organization Copyright Alliance, over 70 copyright infringement cases have been filed against AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some cases, like one between Meta and a group of authors, a judge has ruled in favor of the tech giant. In others, like the case between Anthropic and a group of authors, the AI giant has settled with and paid out the plaintiffs to resolve their claims. Many cases are still in active litigation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Snap was asked for comment. TechCrunch will update if one is provided. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/01/evan-spiegel.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A group of YouTubers who are suing tech giants for scraping their videos without permission to train AI models has now added Snap to their list of defendants. The plaintiffs — internet content creators behind a trio of YouTube channels with roughly 6.2 million collective subscribers — allege that Snap has trained its AI systems on their video content for use in AI features like the app’s “Imagine Lens,” which allows users to edit images using text prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The plaintiffs earlier filed similar lawsuits against Nvidia, Meta, and ByteDance over similar matters.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In the newly filed proposed class action suit, filed on Friday in the U.S. District Court for the Central District of California, the YouTubers specifically call out Snap for its use of a large-scale, video-language dataset known as HD-VILA-100M, and others that were designed for only academic and research purposes. To use these datasets for commercial purposes, the plaintiffs claim Snap circumvented YouTube’s technological restrictions, terms of service, and licensing limitations, which prohibit commercial use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The suit is seeking statutory damages and a permanent injunction to stop the alleged copyright infringement going forward.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The case itself is being led by the creators behind the h3h3 YouTube channel, with 5.52 million subscribers, and the smaller golfing channels MrShortGame Golf and Golfholics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s now one of many lawsuits pitting content creators against AI model providers, which have included copyright disputes from publishers, authors, newspapers, user-generated content sites, artists, and more. It’s also not the first case to hail from a YouTuber. According to the nonprofit organization Copyright Alliance, over 70 copyright infringement cases have been filed against AI companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some cases, like one between Meta and a group of authors, a judge has ruled in favor of the tech giant. In others, like the case between Anthropic and a group of authors, the AI giant has settled with and paid out the plaintiffs to resolve their claims. Many cases are still in active litigation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Snap was asked for comment. TechCrunch will update if one is provided. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/youtubers-sue-snap-for-alleged-copyright-infringement-in-training-its-ai-models/</guid><pubDate>Mon, 26 Jan 2026 21:43:14 +0000</pubDate></item><item><title>[NEW] OpenAI spills technical details about how its AI coding agent works (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/01/openai-spills-technical-details-about-how-its-ai-coding-agent-works/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Unusually detailed post explains how OpenAI handles the Codex agent loop.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of Retro Robots on Glass Blocks -- AI coding Agents" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/coding_robots_agents-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Illustration of Retro Robots on Glass Blocks -- AI coding Agents" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/coding_robots_agents-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          akinbostanci via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Friday, OpenAI engineer Michael Bolin published a detailed technical breakdown of how the company’s Codex CLI coding agent works internally, offering developers insight into AI coding tools that can write code, run tests, and fix bugs with human supervision. It complements our article in December on how AI agents work by filling in technical details on how OpenAI implements its “agentic loop.”&lt;/p&gt;
&lt;p&gt;AI coding agents are having something of a “ChatGPT moment,” where Claude Code with Opus 4.5 and Codex with GPT-5.2 have reached a new level of usefulness for rapidly coding up prototypes, interfaces, and churning out boilerplate code. The timing of OpenAI’s post details the design philosophy behind Codex just as AI agents are becoming more practical tools for everyday work.&lt;/p&gt;
&lt;p&gt;These tools aren’t perfect and remain controversial for some software developers. While OpenAI has previously told Ars Technica that it uses Codex as a coding tool to help develop the Codex product itself, we also discovered, through hands-on experience, that these tools can be astonishingly fast at simple tasks but remain brittle beyond their training data and require human oversight for production work. The rough framework of a project tends to come fast and feels magical, but filling in the details involves tedious debugging and workarounds for limitations the agent cannot overcome on its own.&lt;/p&gt;
&lt;p&gt;Bolin’s post doesn’t shy away from these engineering challenges. He discusses the inefficiency of quadratic prompt growth, performance issues caused by cache misses, and bugs the team discovered (like MCP tools being enumerated inconsistently) that they had to fix.&lt;/p&gt;
&lt;p&gt;The level of technical detail is somewhat unusual for OpenAI, which has not published similar breakdowns of how other products like ChatGPT work internally, for example (there’s a lot going on under that hood we’d like to know). But we’ve already seen how OpenAI treats Codex differently during our interview with them in December, noting that programming tasks seem ideally suited for large language models.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s worth noting that both OpenAI and Anthropic open-source their coding CLI clients on GitHub, allowing developers to examine the implementation directly, whereas they don’t do the same for ChatGPT or the Claude web interface.&lt;/p&gt;
&lt;h2&gt;An official look inside the loop&lt;/h2&gt;
&lt;p&gt;Bolin’s post focuses on what he calls “the agent loop,” which is the core logic that orchestrates interactions between the user, the AI model, and the software tools the model invokes to perform coding work.&lt;/p&gt;
&lt;p&gt;As we wrote in December, at the center of every AI agent is a repeating cycle. The agent takes input from the user and prepares a textual prompt for the model. The model then generates a response, which either produces a final answer for the user or requests a tool call (such as running a shell command or reading a file). If the model requests a tool call, the agent executes it, appends the output to the original prompt, and queries the model again. This process repeats until the model stops requesting tools and instead produces an assistant message for the user.&lt;/p&gt;
&lt;p&gt;That looping process has to start somewhere, and Bolin’s post reveals how Codex constructs the initial prompt sent to OpenAI’s Responses API, which handles model inference. The prompt is built from several components, each with an assigned role that determines its priority: system, developer, user, or assistant.&lt;/p&gt;
&lt;p&gt;The instructions field comes from either a user-specified configuration file or base instructions bundled with the CLI. The tools field defines what functions the model can call, including shell commands, planning tools, web search capabilities, and any custom tools provided through Model Context Protocol (MCP) servers. The input field contains a series of items that describe the sandbox permissions, optional developer instructions, environment context like the current working directory, and finally the user’s actual message.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;As conversations continue, each new turn includes the complete history of previous messages and tool calls. This means the prompt grows with every interaction, which has performance implications. According to the post, because Codex does not use an optional “previous_response_id” parameter that would allow the API to reference stored conversation state, every request is fully stateless (that is, it sends the entire conversation history with each API call rather than the server retrieving it from memory). Bolin says this design choice simplifies things for API providers and makes it easier to support customers who opt into “Zero Data Retention,” where OpenAI does not store user data.&lt;/p&gt;
&lt;p&gt;The quadratic growth of prompts over a conversation is inefficient, but Bolin explains that prompt caching mitigates this issue somewhat. Cache hits only work for exact prefix matches within a prompt, which means Codex must carefully avoid operations that could cause cache misses. Changing the available tools, switching models, or modifying the sandbox configuration mid-conversation can all invalidate the cache and hurt performance.&lt;/p&gt;
&lt;p&gt;The ever-growing prompt length is directly related to the context window, which limits how much text the AI model can process in a single inference call. Bolin writes that Codex automatically compacts conversations when token counts exceed a threshold, just as Claude Code does. Earlier versions of Codex required manual compaction via a slash command, but the current system uses a specialized API endpoint that compresses context while preserving summarized portions of the model’s “understanding” of what happened through an encrypted content item.&lt;/p&gt;
&lt;p&gt;Bolin says that future posts in his series will cover the CLI’s architecture, tool implementation details, and Codex’s sandboxing model.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Unusually detailed post explains how OpenAI handles the Codex agent loop.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of Retro Robots on Glass Blocks -- AI coding Agents" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/coding_robots_agents-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Illustration of Retro Robots on Glass Blocks -- AI coding Agents" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/coding_robots_agents-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          akinbostanci via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Friday, OpenAI engineer Michael Bolin published a detailed technical breakdown of how the company’s Codex CLI coding agent works internally, offering developers insight into AI coding tools that can write code, run tests, and fix bugs with human supervision. It complements our article in December on how AI agents work by filling in technical details on how OpenAI implements its “agentic loop.”&lt;/p&gt;
&lt;p&gt;AI coding agents are having something of a “ChatGPT moment,” where Claude Code with Opus 4.5 and Codex with GPT-5.2 have reached a new level of usefulness for rapidly coding up prototypes, interfaces, and churning out boilerplate code. The timing of OpenAI’s post details the design philosophy behind Codex just as AI agents are becoming more practical tools for everyday work.&lt;/p&gt;
&lt;p&gt;These tools aren’t perfect and remain controversial for some software developers. While OpenAI has previously told Ars Technica that it uses Codex as a coding tool to help develop the Codex product itself, we also discovered, through hands-on experience, that these tools can be astonishingly fast at simple tasks but remain brittle beyond their training data and require human oversight for production work. The rough framework of a project tends to come fast and feels magical, but filling in the details involves tedious debugging and workarounds for limitations the agent cannot overcome on its own.&lt;/p&gt;
&lt;p&gt;Bolin’s post doesn’t shy away from these engineering challenges. He discusses the inefficiency of quadratic prompt growth, performance issues caused by cache misses, and bugs the team discovered (like MCP tools being enumerated inconsistently) that they had to fix.&lt;/p&gt;
&lt;p&gt;The level of technical detail is somewhat unusual for OpenAI, which has not published similar breakdowns of how other products like ChatGPT work internally, for example (there’s a lot going on under that hood we’d like to know). But we’ve already seen how OpenAI treats Codex differently during our interview with them in December, noting that programming tasks seem ideally suited for large language models.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s worth noting that both OpenAI and Anthropic open-source their coding CLI clients on GitHub, allowing developers to examine the implementation directly, whereas they don’t do the same for ChatGPT or the Claude web interface.&lt;/p&gt;
&lt;h2&gt;An official look inside the loop&lt;/h2&gt;
&lt;p&gt;Bolin’s post focuses on what he calls “the agent loop,” which is the core logic that orchestrates interactions between the user, the AI model, and the software tools the model invokes to perform coding work.&lt;/p&gt;
&lt;p&gt;As we wrote in December, at the center of every AI agent is a repeating cycle. The agent takes input from the user and prepares a textual prompt for the model. The model then generates a response, which either produces a final answer for the user or requests a tool call (such as running a shell command or reading a file). If the model requests a tool call, the agent executes it, appends the output to the original prompt, and queries the model again. This process repeats until the model stops requesting tools and instead produces an assistant message for the user.&lt;/p&gt;
&lt;p&gt;That looping process has to start somewhere, and Bolin’s post reveals how Codex constructs the initial prompt sent to OpenAI’s Responses API, which handles model inference. The prompt is built from several components, each with an assigned role that determines its priority: system, developer, user, or assistant.&lt;/p&gt;
&lt;p&gt;The instructions field comes from either a user-specified configuration file or base instructions bundled with the CLI. The tools field defines what functions the model can call, including shell commands, planning tools, web search capabilities, and any custom tools provided through Model Context Protocol (MCP) servers. The input field contains a series of items that describe the sandbox permissions, optional developer instructions, environment context like the current working directory, and finally the user’s actual message.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;As conversations continue, each new turn includes the complete history of previous messages and tool calls. This means the prompt grows with every interaction, which has performance implications. According to the post, because Codex does not use an optional “previous_response_id” parameter that would allow the API to reference stored conversation state, every request is fully stateless (that is, it sends the entire conversation history with each API call rather than the server retrieving it from memory). Bolin says this design choice simplifies things for API providers and makes it easier to support customers who opt into “Zero Data Retention,” where OpenAI does not store user data.&lt;/p&gt;
&lt;p&gt;The quadratic growth of prompts over a conversation is inefficient, but Bolin explains that prompt caching mitigates this issue somewhat. Cache hits only work for exact prefix matches within a prompt, which means Codex must carefully avoid operations that could cause cache misses. Changing the available tools, switching models, or modifying the sandbox configuration mid-conversation can all invalidate the cache and hurt performance.&lt;/p&gt;
&lt;p&gt;The ever-growing prompt length is directly related to the context window, which limits how much text the AI model can process in a single inference call. Bolin writes that Codex automatically compacts conversations when token counts exceed a threshold, just as Claude Code does. Earlier versions of Codex required manual compaction via a slash command, but the current system uses a specialized API endpoint that compresses context while preserving summarized portions of the model’s “understanding” of what happened through an encrypted content item.&lt;/p&gt;
&lt;p&gt;Bolin says that future posts in his series will cover the CLI’s architecture, tool implementation details, and Codex’s sandboxing model.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/01/openai-spills-technical-details-about-how-its-ai-coding-agent-works/</guid><pubDate>Mon, 26 Jan 2026 23:05:17 +0000</pubDate></item><item><title>[NEW] Qualcomm backs SpotDraft to scale on-device contract AI with valuation doubling toward $400M (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/26/qualcomm-backs-spotdraft-to-scale-on-device-contract-ai-with-valuation-doubling-toward-400m/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As demand grows for privacy-first enterprise AI that can run without sending sensitive data to the cloud, SpotDraft has raised $8 million from Qualcomm Ventures in a strategic Series B extension to scale its on-device contract review tech for regulated legal workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The extension values SpotDraft at around $380 million, the startup told TechCrunch, nearly double its $190 million post-money valuation following its $56 million Series B in February of last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Across regulated sectors, enterprises have moved quickly to test generative AI, but privacy, security, and data governance concerns continue to slow adoption for sensitive workflows — especially in legal, where contracts can include privileged information, intellectual property, pricing, and deal terms. Industry research has consistently flagged data security and privacy as key barriers to wider GenAI deployment in professional services, pushing vendors like SpotDraft to pursue architectures that keep core contract intelligence on the user’s device rather than routing it through the cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At Qualcomm’s Snapdragon Summit 2025, SpotDraft demonstrated its VerifAI workflow running end-to-end on Snapdragon X Elite-powered laptops, executing contract review and edits offline while keeping the document on the local machine. SpotDraft said internet connectivity is still required for login, licensing, and collaboration features, but contract review, risk scoring, and redlining can run fully offline without sending documents to the cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotDraft sees legal as an early proving ground for on-device enterprise AI, arguing that sensitive contracts often cannot be routed through external cloud models due to privacy, security, and compliance constraints.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The future of how enterprise AI is going to be — right now, there’s got to be AI that is close to the document, which is privacy critical, latency sensitive, [and] legally sensitive, and those are the things that will move on device,” said Shashank Bijapur (pictured above, left), co-founder and CEO of SpotDraft, in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotDraft says VerifAI’s on-device capability extends beyond simply generating summaries, with the tool designed to apply playbooks and recommendations directly inside Microsoft Word, the way legal teams already work. “VerifAI will compare a contract against your guidelines, your playbooks, your prior policies,” said Madhav Bhagat (pictured above, right), co-founder and CTO of SpotDraft.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="SpotDraft's VerifAI in Microsoft Word" class="wp-image-3086018" height="1149" src="https://techcrunch.com/wp-content/uploads/2026/01/microsoft-word-verifai-tool_68493a.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;SpotDraft’s VerifAI works in Microsoft Word&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;SpotDraft&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Bijapur told TechCrunch that the demand for on-device AI is emerging most clearly in tightly regulated sectors, including defense and pharma, where internal security reviews and data residency requirements can slow or block the use of cloud-based AI tools for sensitive documents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On-device models have rapidly closed the gap with cloud-based systems, both in output quality and response times, Bhagat said. “Now we’ve come to a place where, in terms of eval, we are seeing as little as 5% difference between the frontier models, and some of these fine-tuned on device models,” he said, adding that speeds on newer chips are now “one-third of what we get in the cloud.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since its launch in 2017, SpotDraft said it has reached more than 700 customers, up from around 400 in February last year, and counts Apollo.io, Panasonic, Zeplin, and Whatfix among its users. The company said adoption is rising on its contract lifecycle management platform, with customers now processing over 1 million contracts annually, contract volumes growing 173% year-over-year, and nearly 50,000 monthly active users. It also expects 100% year-over-year revenue growth in 2026, after growing 169% in 2024 and posting a similar growth rate in 2025, though it did not share specific revenue figures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;SpotDraft plans to use the new capital to deepen its product and AI capabilities and expand its enterprise presence across the Americas, the EMEA region (Europe, Middle East, and Africa), and India, Bijapur said, adding that Qualcomm’s involvement extends beyond financing into joint development and go-to-market efforts for on-device deployments. The startup’s on-device workflow is currently available to a limited set of customers, and the founders expect it to expand more broadly as compatible AI PC hardware becomes more widely available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bengaluru- and New York-based SpotDraft said it has a team of 300-plus employees, including 15–20 in the U.S., where COO Akshay Verma is based, and four to five in the UK, with the rest of the workforce in Bengaluru.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To date, the startup has raised $92 million, including the latest Qualcomm Ventures investment. Its earlier investors include Vertex Growth Singapore, Trident Growth Partners, Xeed VC, Arkam Ventures, and Prosus Ventures.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As demand grows for privacy-first enterprise AI that can run without sending sensitive data to the cloud, SpotDraft has raised $8 million from Qualcomm Ventures in a strategic Series B extension to scale its on-device contract review tech for regulated legal workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The extension values SpotDraft at around $380 million, the startup told TechCrunch, nearly double its $190 million post-money valuation following its $56 million Series B in February of last year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Across regulated sectors, enterprises have moved quickly to test generative AI, but privacy, security, and data governance concerns continue to slow adoption for sensitive workflows — especially in legal, where contracts can include privileged information, intellectual property, pricing, and deal terms. Industry research has consistently flagged data security and privacy as key barriers to wider GenAI deployment in professional services, pushing vendors like SpotDraft to pursue architectures that keep core contract intelligence on the user’s device rather than routing it through the cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At Qualcomm’s Snapdragon Summit 2025, SpotDraft demonstrated its VerifAI workflow running end-to-end on Snapdragon X Elite-powered laptops, executing contract review and edits offline while keeping the document on the local machine. SpotDraft said internet connectivity is still required for login, licensing, and collaboration features, but contract review, risk scoring, and redlining can run fully offline without sending documents to the cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotDraft sees legal as an early proving ground for on-device enterprise AI, arguing that sensitive contracts often cannot be routed through external cloud models due to privacy, security, and compliance constraints.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The future of how enterprise AI is going to be — right now, there’s got to be AI that is close to the document, which is privacy critical, latency sensitive, [and] legally sensitive, and those are the things that will move on device,” said Shashank Bijapur (pictured above, left), co-founder and CEO of SpotDraft, in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SpotDraft says VerifAI’s on-device capability extends beyond simply generating summaries, with the tool designed to apply playbooks and recommendations directly inside Microsoft Word, the way legal teams already work. “VerifAI will compare a contract against your guidelines, your playbooks, your prior policies,” said Madhav Bhagat (pictured above, right), co-founder and CTO of SpotDraft.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="SpotDraft's VerifAI in Microsoft Word" class="wp-image-3086018" height="1149" src="https://techcrunch.com/wp-content/uploads/2026/01/microsoft-word-verifai-tool_68493a.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;SpotDraft’s VerifAI works in Microsoft Word&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;SpotDraft&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Bijapur told TechCrunch that the demand for on-device AI is emerging most clearly in tightly regulated sectors, including defense and pharma, where internal security reviews and data residency requirements can slow or block the use of cloud-based AI tools for sensitive documents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On-device models have rapidly closed the gap with cloud-based systems, both in output quality and response times, Bhagat said. “Now we’ve come to a place where, in terms of eval, we are seeing as little as 5% difference between the frontier models, and some of these fine-tuned on device models,” he said, adding that speeds on newer chips are now “one-third of what we get in the cloud.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since its launch in 2017, SpotDraft said it has reached more than 700 customers, up from around 400 in February last year, and counts Apollo.io, Panasonic, Zeplin, and Whatfix among its users. The company said adoption is rising on its contract lifecycle management platform, with customers now processing over 1 million contracts annually, contract volumes growing 173% year-over-year, and nearly 50,000 monthly active users. It also expects 100% year-over-year revenue growth in 2026, after growing 169% in 2024 and posting a similar growth rate in 2025, though it did not share specific revenue figures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;SpotDraft plans to use the new capital to deepen its product and AI capabilities and expand its enterprise presence across the Americas, the EMEA region (Europe, Middle East, and Africa), and India, Bijapur said, adding that Qualcomm’s involvement extends beyond financing into joint development and go-to-market efforts for on-device deployments. The startup’s on-device workflow is currently available to a limited set of customers, and the founders expect it to expand more broadly as compatible AI PC hardware becomes more widely available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bengaluru- and New York-based SpotDraft said it has a team of 300-plus employees, including 15–20 in the U.S., where COO Akshay Verma is based, and four to five in the UK, with the rest of the workforce in Bengaluru.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To date, the startup has raised $92 million, including the latest Qualcomm Ventures investment. Its earlier investors include Vertex Growth Singapore, Trident Growth Partners, Xeed VC, Arkam Ventures, and Prosus Ventures.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/26/qualcomm-backs-spotdraft-to-scale-on-device-contract-ai-with-valuation-doubling-toward-400m/</guid><pubDate>Tue, 27 Jan 2026 01:30:00 +0000</pubDate></item><item><title>[NEW] Unlocking Agentic RL Training for GPT-OSS: A Practical Retrospective (Hugging Face - Blog)</title><link>https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl</link><description>&lt;div class="not-prose mb-6 font-sans lg:hidden"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8"&gt;
	


	&lt;ul class="flex items-center  flex-row  text-base   "&gt;&lt;li class=" -mr-2 h-5 w-5 md:h-6 md:w-6   bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="JasonZhu13"&gt;&lt;img alt="alt" class="overflow-hidden rounded-full" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64efbd469e7770db74cb72f5/yeYzDPziD-5KIHPH0dlJJ.png" /&gt;
					
			&lt;/li&gt;

		&lt;li class="text-xs text-gray-600 hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 order-last ml-3"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;dialog class="shadow-alternate z-40 mx-4 my-auto h-fit select-text overflow-hidden rounded-xl bg-white max-sm:max-w-[calc(100dvw-2rem)] sm:mx-auto lg:mt-26 md:portrait:mt-30 xl:mt-30 2xl:mt-32 w-full sm:w-96 max-w-[calc(100%-4rem)] text-base not-prose"&gt;
	&lt;/dialog&gt;&lt;/div&gt;&lt;/div&gt;
					&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Arup De's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/0c9efef440954dd5d2f1c2543e0e5645.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
Agentic reinforcement learning (RL) extends traditional LLM training by optimizing not just a single-turn response, but an entire decision-making process learned through direct interaction with an environment during training. Unlike traditional single-turn reinforcement learning or offline preference-based methods that rely on static datasets, agentic RL trains policies by actively collecting on-policy data as the agent plans actions, invokes tools, observes outcomes, and adapts its behavior over multi-step trajectories in either simulated or real environments. This interaction-driven optimization assigns credit across long-horizon decisions, where intermediate choices such as query reformulation, tool selection, and execution order directly influence downstream success. Training follows an iterative closed loop in which the agent interacts with the environment to collect rollout trajectories, computes rewards over these trajectories, updates the policy based on observed outcomes, and then uses the updated policy to drive the next round of interaction and data collection such as GRPO or PPO algorithms..
&lt;p&gt;LinkedIn is an AI-first company that's built agents to help professionals be more successful. In this setting, models must reason over incomplete information, interact with structured services, and adapt to evolving user intent across multiple steps rather than produce a single static response. These capabilities are especially critical for agents that support the goals of recruiters, job and knowledge seekers, and learners end users, such as retrieving information, refining queries, coordinating tools, and executing multi-step workflows. By learning robust decision policies through interaction, agentic RL provides a principled foundation for building scalable, reliable, and adaptable AI systems through end-to-end optimization.&lt;/p&gt;
&lt;p&gt;The GPT-OSS model has shown comparable performance to OpenAI o3-mini and o4-mini [ref], but its suitability for agentic reinforcement learning training has not yet been validated. Most recent work focuses on fine-tuning without tool calling, such as: Fine-tuning with gpt-oss and Hugging Face Transformers and unsloth tutorial: how to fine-tune gpt-oss. This blog explores the journey to unlock agentic RL training for GPT-OSS as a potential backbone model for agentic applications.&lt;/p&gt;
&lt;p&gt;In our experiments, we use verl as our training framework since it is one of the most popular adopted frameworks in the open source community. We use gsm8k, Retool task, verifiable instruction following task, which are commonly used in RL training. We focus on presenting experimental results for the GPT-OSS-20B model, and our attention-sink fix also works for GPT-OSS-120B. The Qwen-2.5-32B model is additionally used to benchmark standard metric trends during RL training.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Challenges of GPT-OSS RL Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;verl has been an OSS framework used by the team, and the team has previously collaborated and contributed to it to help democratize agentic reinforcement learning training. With the introduction of the new Harmony chat template in GPT-OSS, the first step is to ensure that the training framework fully supports the updated message format and conversation semantics required by Harmony. This step helps rollout generation, trajectory construction, and tool parsing remain consistent and correct under the new template.&lt;/p&gt;
&lt;p&gt;The team uses ReTool as a representative example to verify code correctness. ReTool is an agentic coding task in which the model is asked to solve a math problem with the assistance of a code compiler tool. This setup allows the model to focus on core reasoning and algorithmic logic, while delegating the actual arithmetic and execution to the tool. During an episode, the model interacts with the code tool multiple times, using execution results as feedback to refine its solution. At the end of the trajectory, the model produces a final answer, on which the reward is computed.&lt;/p&gt;
&lt;p&gt;During the initial training runs, we observed exploding KL divergence and entropy, along with non-increasing rewards, indicating underlying issues in the GPT-OSS training setup, as shown in Figure 1.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average gradient norm in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/5eWLHQ-EAWKjPc7T0c6CK.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ZpBvRXLKyAT-PIdsuRi11.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Left: Qwen32b has significantly higher rewards compared to GPT-OSS 20B; Right: The gradient norm exploded as training progressed.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A Practical Debugging Journey in verl: Restoring PPO On-Policy Integrity
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Restoring PPO On-Policy Integrity: A Fix for MoE Log-Probability Mismatch
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/21xIlXKeAlTK5qKSp-TkX.png" width="500" /&gt;
&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Non-zero importance sampling clip value even for on-policy training.&lt;/p&gt;

&lt;p&gt;We focus on on-policy methods because they provide greater stability and more reliable convergence. The foundation of pure on-policy Proximal Policy Optimization (PPO) mandates that the importance sampling ratio must be exactly 1. The mathematical definition of the importance ratio is:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mtext&gt;ratio&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mtext&gt;old&lt;/mtext&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;
\text{ratio} = \frac{\pi(a \mid s)}{\pi_{\text{old}}(a \mid s)}
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord text"&gt;&lt;span class="mord"&gt;ratio&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;old&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This requirement ensures that the policy update is executed only on the data generated by the current policy π(a | s) = π&lt;sub&gt;old&lt;/sub&gt;(a | s), preventing unintended clipping.&lt;/p&gt;
&lt;p&gt;We have observed the non-zero clipping value in our ReTool training, as shown in Figure 2, stemming from a mismatch between the two log-probabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current log-probability &lt;code&gt;log_prob&lt;/code&gt;: log(π(a | s))&lt;/li&gt;
&lt;li&gt;Old log-probability &lt;code&gt;old_log_prob&lt;/code&gt;: log(π&lt;sub&gt;old&lt;/sub&gt;(a | s))&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Root Cause: The Dual Forward Pass and MoE Architecture&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Prior to verl 0.3.0, the implementation relied on two separate forward passes (one to compute the current &lt;code&gt;log_prob&lt;/code&gt; and one to retrieve the stored &lt;code&gt;old_log_prob&lt;/code&gt;) for the same state-action pair.&lt;/p&gt;
&lt;p&gt;In a Mixture of Experts (MoE) architecture like GPT-OSS, the gating network routes the input to different experts. Due to implementation factors (e.g., subtle floating-point differences or explicit stochasticity), the expert routing can differ slightly between the two passes. Readers who are interested can further read &lt;em&gt;Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This difference in routing leads to:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo mathvariant="normal"&gt;≠&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mtext&gt;old&lt;/mtext&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;
\log(\pi(a \mid s)) \neq \log(\pi_{\text{old}}(a \mid s))
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mop"&gt;lo&lt;span&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;))&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;&lt;span class="mrel"&gt;&lt;span class="mord vbox"&gt;&lt;span class="thinbox"&gt;&lt;span class="rlap"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="inner"&gt;&lt;span class="mord"&gt;&lt;span class="mrel"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="fix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mop"&gt;lo&lt;span&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;old&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;))&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The resulting ratio deviates from 1, falsely triggering the PPO clip and violating the core on-policy assumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution: Enforcing Ratio = 1 via Log-Probability Substitution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fix resolves the issue by logically overriding the flawed computation when the environment is known to be on-policy (i.e., when the minibatch size equals the global batch size):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; on_policy:
    old_log_prob = log_prob.detach()
&lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
    old_log_prob = model_inputs[&lt;span class="hljs-string"&gt;"old_log_probs"&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By setting &lt;code&gt;old_log_prob&lt;/code&gt; equal to the newly computed &lt;code&gt;log_prob&lt;/code&gt; (detached to prevent gradient flow through the reference value), the importance ratio is mathematically forced back to 1. This strategy bypasses the instability caused by MoE's non-deterministic routing and guarantees strict on-policy behavior during PPO training.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Correcting Training–Inference Mismatch
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Although fixing the log-probability mismatch reduced the importance-sampling clip ratio to zero, gradient norms continued to explode and rewards failed to improve. To isolate the issue, we simplified training to GSM8K, a single-step task without agentic tool use. The same instability persisted, as shown in the green curves in Figure 3, indicating a &lt;strong&gt;fundamental issue in basic RL training with GPT-OSS under verl.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We hypothesize that &lt;strong&gt;training–inference mismatch&lt;/strong&gt; could be a potential cause: discrepancies between inference-time execution—where engines such as vLLM and SGLang aggressively optimize for throughput—and training-time execution under FSDP, which prioritizes numerical precision and stability, can effectively turn otherwise &lt;strong&gt;on-policy RL into off-policy optimization.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This blog details why such mismatches lead to unstable gradients and non-improving rewards. Figure 3 compares training runs with and without rollout correction (see this verl blog for details). After applying rollout correction, training dynamics improve significantly, with gradient norms remaining stable rather than exploding.&lt;/p&gt;
&lt;p&gt;However, as shown in the left plot of Figure 4, the reward increases only modestly, and convergence on the simple GSM8K task remains substantially slower compared to smaller dense model variants.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Entropy&lt;/th&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average KL Loss&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/MoBnatEipCi_OScgm5fAJ.png" width="250" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/k4Su03zqq6Cg-7jM6Il6J.png" width="250" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/g77U9c8a-FY1rvwru96Ez.png" width="250" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Gradient norm behavior under different training configurations. Green: Training without rollout correction, exhibiting unstable gradients. Red: Training with the attention layer frozen to isolate the issue to the attention mechanism, resulting in partial stabilization. Blue: Training with rollout correction enabled (sequence-level importance sampling), yielding stable gradient norms.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;th&gt;Max Log-Perplexity Difference&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ihx-XsWH51V0-JM46jODE.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Maximum absolute log-perplexity difference in a batch between rollout policy and training policy" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/fP5KNR2XYY7EH-muYBk_X.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Left: Reward improvement on GSM8K remains slow even after applying rollout correction, with performance comparable to runs where the attention layer is frozen during training. Right: A substantial log-ppl mismatch is observed between the inference engine (SGLang with Triton kernels supporting attention-sink forward passes) and the training stack (FSDP with FlashAttention-v2), indicating a large training–inference inconsistency.&lt;/p&gt;
&lt;p&gt;To further isolate the root cause, we freeze the attention layers during training and observe reward dynamics similar to those of runs without freezing (blue curve vs yellow curve in Figure 4). This indicates that learning is primarily driven by the MoE layers, while the attention mechanism contributes less effectively than expected. In addition, we observe a substantial token-level probability mismatch between the inference engine and the distributed training stack which are using different attention kernels. Together, these observations motivate a deeper investigation into the attention mechanism.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Attention Sink Support in FlashAttentionV3
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Attention sinks used in GPT-OSS are learnable scalar parameters (one per attention head) that act as "virtual tokens" in the softmax computation. They allow the model to allocate attention mass to a learned sink rather than forcing all attention to content tokens, which has been shown to improve attention stability in streaming inference and training with sliding-window attention.&lt;/p&gt;
&lt;p&gt;After a deeper investigation, we identified several major issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;verl hard-codes FlashAttention v2 in &lt;code&gt;fsdp_worker&lt;/code&gt;, which does not support attention sinks.&lt;/li&gt;
&lt;li&gt;The attention sink backward pass is not supported in FlashAttention v2 and v3, so it does not work as expected even when FlashAttention v3 is enabled.&lt;/li&gt;
&lt;li&gt;Since the forward pass has not yet been merged into the original FlashAttention v3 repository, we leveraged the forward pass from the vLLM FlashAttention fork (PR #75) and implemented the backward pass to compute the sink gradient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Standard Attention
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scores = QK^T / sqrt(d)               
probs = softmax(scores, dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)       
output = probs @ V                   
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Attention with Sinks (GPT-OSS)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scores = QK^T / sqrt(d)                               
combined = concat([scores, sink_param], dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)       
probs = softmax(combined, dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)                     
probs_content = probs[..., :-&lt;span class="hljs-number"&gt;1&lt;/span&gt;]                       
output = probs_content @ V                           
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key difference:&lt;/strong&gt; The sink participates in softmax normalization but doesn't contribute to the output.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mathematical Formulation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The attention weight for content token j in row i is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;
P_{ij}
=
\frac{\exp(S_{ij})}
{\sum_{j'=1}^{N_k} \exp(S_{ij'}) + \exp(S_h)}
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;&lt;span class="mop op-symbol small-op"&gt;∑&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mrel mtight"&gt;=&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;+&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&lt;sub&gt;ij&lt;/sub&gt; = Q&lt;sub&gt;i&lt;/sub&gt; K&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;⊤&lt;/sup&gt; / √d are the attention scores&lt;/li&gt;
&lt;li&gt;P&lt;sub&gt;ij&lt;/sub&gt; are the attention weights for the content tokens&lt;/li&gt;
&lt;li&gt;S&lt;sub&gt;h&lt;/sub&gt; is the learnable sink parameter for head h&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sink Probability:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sink probability is computed but not used in the output:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;
P_{i,h}
=
\frac{\exp(S_h)}
{\sum_{j'=1}^{N_k} \exp(S_{ij'}) + \exp(S_h)}
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;&lt;span class="mop op-symbol small-op"&gt;∑&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mrel mtight"&gt;=&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;+&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Backward Pass
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The gradient of the loss L with respect to the sink parameter S&lt;sub&gt;h&lt;/sub&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence="true"&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mo stretchy="false"&gt;{&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo fence="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;
\frac{\partial L}{\partial S_h}
=
-
\sum_i
P_{i,h}
\left(
\frac{\partial L}{\partial S_{i,h}}
-
\sum_{j \in \{1,\ldots,N_k\}}
P_{ij}
\frac{\partial L}{\partial S_{ij}}
\right)
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;−&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="minner"&gt;&lt;span class="mopen"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;−&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="mrel mtight"&gt;∈&lt;/span&gt;&lt;span class="mopen mtight"&gt;{&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="minner mtight"&gt;…&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose mtight"&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P&lt;sub&gt;i,h&lt;/sub&gt; is the sink attention probability for row i&lt;/li&gt;
&lt;li&gt;∂L/∂S&lt;sub&gt;ij&lt;/sub&gt; is the gradient with respect to the attention scores, including the sink&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Simplified Gradient:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the sink is computed but not used in the output, its gradient ∂L/∂S&lt;sub&gt;i,h&lt;/sub&gt; = 0.&lt;/p&gt;
&lt;p&gt;Therefore, the backward equation simplifies to:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence="true"&gt;(&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mo stretchy="false"&gt;{&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo fence="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;
\frac{\partial L}{\partial S_h}
=
-
\sum_i
P_{i,h}
\left(
\sum_{j \in \{1,\ldots,N_k\}}
P_{ij}
\frac{\partial L}{\partial S_{ij}}
\right)
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;−&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="minner"&gt;&lt;span class="mopen"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="mrel mtight"&gt;∈&lt;/span&gt;&lt;span class="mopen mtight"&gt;{&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="minner mtight"&gt;…&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose mtight"&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The forward pass was adapted from vLLM's FlashAttention fork, and we implemented the backward pass to compute gradients for the sink parameters. The implementation will be released following the internal review process.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Results
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;After applying the fix in FlashAttention v3, we observe substantially faster convergence for GPT-OSS-20B across a range of reinforcement learning tasks. These include single-turn RL on math reasoning (GSM8K — red curve in Figure 5), instruction following (VerifyIf, evaluated on an out-of-domain multi-if benchmark — Figure 6), and multi-turn agentic RL with tool use (ReTool — Figure 7).&lt;/p&gt;
&lt;p&gt;Across all settings, training becomes stable and exhibits steady reward improvement.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/6TYGevydK99nQ-I1QTouf.png" width="500" /&gt;
&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt;. Single Turn GSM8K, the red curve converges much faster than the rest without the fix&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Entropy&lt;/th&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average entropy in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ydmBLCSGlD9YKWiIocI1S.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average gradient norm in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/_kl8mn_CXPsRYJ467IbFs.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/vb2JmmSu-LI5szC_84KsM.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;. On verifiable instruction following the task, the run without the fix collapsed (blue), and the run with fix showed steady reward improvement.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;th&gt;Validation Accuracy&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average gradient norm in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/_Xz7_RLhYuYhzGAeMjDXs.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/uFX-sTWI6knecIf56uahk.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="val score accuracy mean@30 for aime_2025" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/5x2mdkpHcdvctZ96yfv58.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;. On the Retool task, the run with fix showed steady reward improvement and no gradient exploding (fa2 is the flash attention 2 without the fix while fa3 is the flash attention 3 with the fix). After the fix, the validation accuracy score goes up now.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Memory-Efficient Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mitigating FSDP Memory Blow-Ups Caused by Repeated MoE Expert Materialization
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;One issue we consistently encountered was excessive memory allocation during the FSDP forward pass, which led to repeated out-of-memory (OOM) failures when training GPT-OSS-20B bf16 models on 16 H200 nodes (max response length: 16k, prompt length: 8k). This behavior is highly unexpected for a 20B-parameter MoE model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m File "/home/jobuser/.local/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py", line 123, in forward
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m hidden_states = hidden_states.repeat(num_experts, 1)
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 180.00 GiB. GPU 0 has a total capacity of 139.72 GiB of which 110.94 GiB is free. Process 685851 has 24.88 GiB memory in use. Process 692458 has 3.87 GiB memory in use. Of the allocated memory 23.28 GiB is allocated by PyTorch, and 84.43 MiB is reserved by PyTorch but unallocated.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We identified the issue as originating from two different implementations of the MoE forward path in Hugging Face Transformers. This issue has also been reported by other users: https://github.com/huggingface/transformers/issues/40073; When verl computes log-probabilities under FSDP, the inference forward path is triggered. In the current Hugging Face implementation, this path duplicates hidden states for all experts and performs batched matrix multiplication, materializing extremely large tensors in GPU memory. By contrast, the training forward path uses a for-loop to process each expert sequentially and then combines the results. While slower, this approach is significantly more memory efficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-meta"&gt;    @GPUMemoryLogger(&lt;span class="hljs-params"&gt;role=&lt;span class="hljs-string"&gt;"dp actor"&lt;/span&gt;, logger=logger&lt;/span&gt;)&lt;/span&gt;
    &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;compute_log_prob&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, data: DataProto, calculate_entropy=&lt;span class="hljs-literal"&gt;False&lt;/span&gt;&lt;/span&gt;) -&amp;gt; torch.Tensor:
        &lt;span class="hljs-string"&gt;"""&lt;/span&gt;
&lt;span class="hljs-string"&gt;        ....&lt;/span&gt;
&lt;span class="hljs-string"&gt;        """&lt;/span&gt;
        
        self.actor_module.&lt;span class="hljs-built_in"&gt;eval&lt;/span&gt;()
        ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We patched the Hugging Face implementation to use a more memory-efficient execution path, avoiding repeated materialization of experts.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Sequence Parallel with Flash Attention V3
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Agentic RL requires the agent to interact with the environment over multiple steps while maintaining an ever-expanding context. Observations and environment feedback from each step are appended to the context and used as input for subsequent decision-making, which introduces significant challenges for memory efficiency and scalability during training.&lt;/p&gt;
&lt;p&gt;Under fully sharded data parallelism (FSDP), model parameters, optimizer states, and gradients are sharded across the entire world size (i.e., all GPUs in the training cluster). Each GPU stores and updates only its assigned parameter shards, while rollout data are replicated across all GPUs—meaning every GPU processes the full agent interaction history for each rollout.&lt;/p&gt;
&lt;p&gt;During the forward pass, when computation reaches a layer whose parameters are not locally available, an &lt;code&gt;all_gather&lt;/code&gt; operation is triggered to materialize the full parameters across GPUs. During the backward pass, a corresponding &lt;code&gt;reduce_scatter&lt;/code&gt; operation aggregates gradients and ensures that each GPU retains only its local shard. This provides a degree of scaling: as the number of GPUs increases, the per-GPU memory footprint decreases.&lt;/p&gt;
&lt;p&gt;FSDP provides model-level scaling by sharding model parameters, gradients, and optimizer states across GPUs. Sequence parallelism (or context parallelism) further reduces per-GPU memory consumption by partitioning the input sequence across devices, thereby lowering the peak activation memory on each GPU.&lt;/p&gt;
&lt;p&gt;As the number of sequence-parallel dimensions increases, the maximum activation memory per GPU correspondingly decreases. We have implemented sequence parallelism to be attention-sink-aware and compatible with FlashAttention v3 (Figure 8, right).&lt;/p&gt;
&lt;p&gt;&lt;img alt="SP  (2)" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ryT_y9BpbFSdMDxNYlVlK.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8&lt;/strong&gt;. Left: Inference without sequence parallelism. Right: Inference with sequence parallelism, where additional all-to-all communication is performed before and after the attention layer. This partitions the sequence across parallel workers and reduces the peak memory footprint of attention computation by a factor proportional to the sequence-parallelism degree.&lt;/p&gt;
&lt;p&gt;Sequence parallelism scales along the sequence dimension to reduce the per-GPU activation footprint. Input tokens from all sequences are packed into a single contiguous list by removing padding tokens, while position IDs are used to distinguish tokens belonging to different sequences. This design naturally benefits from FlashAttention’s variable-length support. For sequence parallelism, layers other than the attention layer do not have inter-position dependencies; therefore, they do not require each GPU to hold a complete sequence shard, and no additional communication is needed for these layers.&lt;/p&gt;
&lt;p&gt;The attention layer, however, requires all tokens belonging to the same sequence to be present on the same GPU in order to compute attention weights correctly. To satisfy this constraint, an all-to-all communication is performed to gather sequence elements, with the split performed at the attention-head level. This design avoids communication within the attention computation itself, which would otherwise be prohibitively expensive. After the attention layer, a single all-to-all communication redistributes the outputs back to their original sequence-parallel layout, after which the remaining non-attention layers can proceed without further synchronization.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Our journey to enable agentic RL training for the GPT-OSS backbone model was a practical retrospective, highlighting that unlocking advanced capabilities in open-source LLMs requires meticulous, deep-dive engineering.&lt;/p&gt;
&lt;p&gt;We made contributions that transformed the viability of GPT-OSS for agentic applications, specifically by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stabilizing PPO:&lt;/strong&gt; We contributed a fix to restore on-policy integrity, overriding the log-probability mismatch caused by the MoE architecture’s non-determinism (Figure 2).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enabling Attention Sink Support:&lt;/strong&gt; We successfully implemented and integrated the attention sink backward pass into FlashAttention v3, correcting the catastrophic training–inference mismatch that had previously caused instability and slow convergence (Figures 5, 6, and 7).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scaling Memory Efficiency:&lt;/strong&gt; We introduced crucial memory optimizations, including patching the MoE materialization process and integrating sequence parallelism with the new attention sink support, enabling training with the long context windows essential for multi-step agents (Figure 8).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These engineering efforts validate GPT-OSS as a scalable and high-performance backbone for building the next generation of intelligent, multi-step decision-making agents.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgments
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Thanks to Deepak Agarwal, Bee-Chung Chen, Animesh Singh, Gungor Polatkan, Balaji Krishnapuram, and Jitendra Agarwal for their leadership support.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Feng, Jiazhan, et al. &lt;em&gt;Retool: Reinforcement Learning for Strategic Tool Use in LLMs.&lt;/em&gt; arXiv preprint arXiv:2504.11536 (2025).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Xiao, Guangxuan, et al. &lt;em&gt;Efficient Streaming Language Models with Attention Sinks.&lt;/em&gt; arXiv preprint arXiv:2309.17453 (2023).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When Speed Kills Stability: Demystifying RL Collapse from the Training–Inference Mismatch.&lt;br /&gt;https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose mb-6 font-sans lg:hidden"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8"&gt;
	


	&lt;ul class="flex items-center  flex-row  text-base   "&gt;&lt;li class=" -mr-2 h-5 w-5 md:h-6 md:w-6   bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800" title="JasonZhu13"&gt;&lt;img alt="alt" class="overflow-hidden rounded-full" src="https://cdn-avatars.huggingface.co/v1/production/uploads/64efbd469e7770db74cb72f5/yeYzDPziD-5KIHPH0dlJJ.png" /&gt;
					
			&lt;/li&gt;

		&lt;li class="text-xs text-gray-600 hover:text-gray-700 dark:text-gray-400 dark:hover:text-gray-300 order-last ml-3"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;dialog class="shadow-alternate z-40 mx-4 my-auto h-fit select-text overflow-hidden rounded-xl bg-white max-sm:max-w-[calc(100dvw-2rem)] sm:mx-auto lg:mt-26 md:portrait:mt-30 xl:mt-30 2xl:mt-32 w-full sm:w-96 max-w-[calc(100%-4rem)] text-base not-prose"&gt;
	&lt;/dialog&gt;&lt;/div&gt;&lt;/div&gt;
					&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Arup De's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/0c9efef440954dd5d2f1c2543e0e5645.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
Agentic reinforcement learning (RL) extends traditional LLM training by optimizing not just a single-turn response, but an entire decision-making process learned through direct interaction with an environment during training. Unlike traditional single-turn reinforcement learning or offline preference-based methods that rely on static datasets, agentic RL trains policies by actively collecting on-policy data as the agent plans actions, invokes tools, observes outcomes, and adapts its behavior over multi-step trajectories in either simulated or real environments. This interaction-driven optimization assigns credit across long-horizon decisions, where intermediate choices such as query reformulation, tool selection, and execution order directly influence downstream success. Training follows an iterative closed loop in which the agent interacts with the environment to collect rollout trajectories, computes rewards over these trajectories, updates the policy based on observed outcomes, and then uses the updated policy to drive the next round of interaction and data collection such as GRPO or PPO algorithms..
&lt;p&gt;LinkedIn is an AI-first company that's built agents to help professionals be more successful. In this setting, models must reason over incomplete information, interact with structured services, and adapt to evolving user intent across multiple steps rather than produce a single static response. These capabilities are especially critical for agents that support the goals of recruiters, job and knowledge seekers, and learners end users, such as retrieving information, refining queries, coordinating tools, and executing multi-step workflows. By learning robust decision policies through interaction, agentic RL provides a principled foundation for building scalable, reliable, and adaptable AI systems through end-to-end optimization.&lt;/p&gt;
&lt;p&gt;The GPT-OSS model has shown comparable performance to OpenAI o3-mini and o4-mini [ref], but its suitability for agentic reinforcement learning training has not yet been validated. Most recent work focuses on fine-tuning without tool calling, such as: Fine-tuning with gpt-oss and Hugging Face Transformers and unsloth tutorial: how to fine-tune gpt-oss. This blog explores the journey to unlock agentic RL training for GPT-OSS as a potential backbone model for agentic applications.&lt;/p&gt;
&lt;p&gt;In our experiments, we use verl as our training framework since it is one of the most popular adopted frameworks in the open source community. We use gsm8k, Retool task, verifiable instruction following task, which are commonly used in RL training. We focus on presenting experimental results for the GPT-OSS-20B model, and our attention-sink fix also works for GPT-OSS-120B. The Qwen-2.5-32B model is additionally used to benchmark standard metric trends during RL training.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Challenges of GPT-OSS RL Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;verl has been an OSS framework used by the team, and the team has previously collaborated and contributed to it to help democratize agentic reinforcement learning training. With the introduction of the new Harmony chat template in GPT-OSS, the first step is to ensure that the training framework fully supports the updated message format and conversation semantics required by Harmony. This step helps rollout generation, trajectory construction, and tool parsing remain consistent and correct under the new template.&lt;/p&gt;
&lt;p&gt;The team uses ReTool as a representative example to verify code correctness. ReTool is an agentic coding task in which the model is asked to solve a math problem with the assistance of a code compiler tool. This setup allows the model to focus on core reasoning and algorithmic logic, while delegating the actual arithmetic and execution to the tool. During an episode, the model interacts with the code tool multiple times, using execution results as feedback to refine its solution. At the end of the trajectory, the model produces a final answer, on which the reward is computed.&lt;/p&gt;
&lt;p&gt;During the initial training runs, we observed exploding KL divergence and entropy, along with non-increasing rewards, indicating underlying issues in the GPT-OSS training setup, as shown in Figure 1.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average gradient norm in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/5eWLHQ-EAWKjPc7T0c6CK.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ZpBvRXLKyAT-PIdsuRi11.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Left: Qwen32b has significantly higher rewards compared to GPT-OSS 20B; Right: The gradient norm exploded as training progressed.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A Practical Debugging Journey in verl: Restoring PPO On-Policy Integrity
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Restoring PPO On-Policy Integrity: A Fix for MoE Log-Probability Mismatch
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/21xIlXKeAlTK5qKSp-TkX.png" width="500" /&gt;
&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt;Figure 2.&lt;/b&gt; Non-zero importance sampling clip value even for on-policy training.&lt;/p&gt;

&lt;p&gt;We focus on on-policy methods because they provide greater stability and more reliable convergence. The foundation of pure on-policy Proximal Policy Optimization (PPO) mandates that the importance sampling ratio must be exactly 1. The mathematical definition of the importance ratio is:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mtext&gt;ratio&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mtext&gt;old&lt;/mtext&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;
\text{ratio} = \frac{\pi(a \mid s)}{\pi_{\text{old}}(a \mid s)}
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord text"&gt;&lt;span class="mord"&gt;ratio&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;old&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This requirement ensures that the policy update is executed only on the data generated by the current policy π(a | s) = π&lt;sub&gt;old&lt;/sub&gt;(a | s), preventing unintended clipping.&lt;/p&gt;
&lt;p&gt;We have observed the non-zero clipping value in our ReTool training, as shown in Figure 2, stemming from a mismatch between the two log-probabilities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Current log-probability &lt;code&gt;log_prob&lt;/code&gt;: log(π(a | s))&lt;/li&gt;
&lt;li&gt;Old log-probability &lt;code&gt;old_log_prob&lt;/code&gt;: log(π&lt;sub&gt;old&lt;/sub&gt;(a | s))&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Root Cause: The Dual Forward Pass and MoE Architecture&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Prior to verl 0.3.0, the implementation relied on two separate forward passes (one to compute the current &lt;code&gt;log_prob&lt;/code&gt; and one to retrieve the stored &lt;code&gt;old_log_prob&lt;/code&gt;) for the same state-action pair.&lt;/p&gt;
&lt;p&gt;In a Mixture of Experts (MoE) architecture like GPT-OSS, the gating network routes the input to different experts. Due to implementation factors (e.g., subtle floating-point differences or explicit stochasticity), the expert routing can differ slightly between the two passes. Readers who are interested can further read &lt;em&gt;Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This difference in routing leads to:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo mathvariant="normal"&gt;≠&lt;/mo&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mtext&gt;old&lt;/mtext&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;∣&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;
\log(\pi(a \mid s)) \neq \log(\pi_{\text{old}}(a \mid s))
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mop"&gt;lo&lt;span&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;))&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;&lt;span class="mrel"&gt;&lt;span class="mord vbox"&gt;&lt;span class="thinbox"&gt;&lt;span class="rlap"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="inner"&gt;&lt;span class="mord"&gt;&lt;span class="mrel"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="fix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mop"&gt;lo&lt;span&gt;g&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;π&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord text mtight"&gt;&lt;span class="mord mtight"&gt;old&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord mathnormal"&gt;a&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;∣&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord mathnormal"&gt;s&lt;/span&gt;&lt;span class="mclose"&gt;))&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The resulting ratio deviates from 1, falsely triggering the PPO clip and violating the core on-policy assumption.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution: Enforcing Ratio = 1 via Log-Probability Substitution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The fix resolves the issue by logically overriding the flawed computation when the environment is known to be on-policy (i.e., when the minibatch size equals the global batch size):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-keyword"&gt;if&lt;/span&gt; on_policy:
    old_log_prob = log_prob.detach()
&lt;span class="hljs-keyword"&gt;else&lt;/span&gt;:
    old_log_prob = model_inputs[&lt;span class="hljs-string"&gt;"old_log_probs"&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By setting &lt;code&gt;old_log_prob&lt;/code&gt; equal to the newly computed &lt;code&gt;log_prob&lt;/code&gt; (detached to prevent gradient flow through the reference value), the importance ratio is mathematically forced back to 1. This strategy bypasses the instability caused by MoE's non-deterministic routing and guarantees strict on-policy behavior during PPO training.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Correcting Training–Inference Mismatch
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Although fixing the log-probability mismatch reduced the importance-sampling clip ratio to zero, gradient norms continued to explode and rewards failed to improve. To isolate the issue, we simplified training to GSM8K, a single-step task without agentic tool use. The same instability persisted, as shown in the green curves in Figure 3, indicating a &lt;strong&gt;fundamental issue in basic RL training with GPT-OSS under verl.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We hypothesize that &lt;strong&gt;training–inference mismatch&lt;/strong&gt; could be a potential cause: discrepancies between inference-time execution—where engines such as vLLM and SGLang aggressively optimize for throughput—and training-time execution under FSDP, which prioritizes numerical precision and stability, can effectively turn otherwise &lt;strong&gt;on-policy RL into off-policy optimization.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This blog details why such mismatches lead to unstable gradients and non-improving rewards. Figure 3 compares training runs with and without rollout correction (see this verl blog for details). After applying rollout correction, training dynamics improve significantly, with gradient norms remaining stable rather than exploding.&lt;/p&gt;
&lt;p&gt;However, as shown in the left plot of Figure 4, the reward increases only modestly, and convergence on the simple GSM8K task remains substantially slower compared to smaller dense model variants.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Entropy&lt;/th&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average KL Loss&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/MoBnatEipCi_OScgm5fAJ.png" width="250" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/k4Su03zqq6Cg-7jM6Il6J.png" width="250" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/g77U9c8a-FY1rvwru96Ez.png" width="250" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Gradient norm behavior under different training configurations. Green: Training without rollout correction, exhibiting unstable gradients. Red: Training with the attention layer frozen to isolate the issue to the attention mechanism, resulting in partial stabilization. Blue: Training with rollout correction enabled (sequence-level importance sampling), yielding stable gradient norms.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;th&gt;Max Log-Perplexity Difference&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ihx-XsWH51V0-JM46jODE.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Maximum absolute log-perplexity difference in a batch between rollout policy and training policy" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/fP5KNR2XYY7EH-muYBk_X.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Left: Reward improvement on GSM8K remains slow even after applying rollout correction, with performance comparable to runs where the attention layer is frozen during training. Right: A substantial log-ppl mismatch is observed between the inference engine (SGLang with Triton kernels supporting attention-sink forward passes) and the training stack (FSDP with FlashAttention-v2), indicating a large training–inference inconsistency.&lt;/p&gt;
&lt;p&gt;To further isolate the root cause, we freeze the attention layers during training and observe reward dynamics similar to those of runs without freezing (blue curve vs yellow curve in Figure 4). This indicates that learning is primarily driven by the MoE layers, while the attention mechanism contributes less effectively than expected. In addition, we observe a substantial token-level probability mismatch between the inference engine and the distributed training stack which are using different attention kernels. Together, these observations motivate a deeper investigation into the attention mechanism.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Attention Sink Support in FlashAttentionV3
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Attention sinks used in GPT-OSS are learnable scalar parameters (one per attention head) that act as "virtual tokens" in the softmax computation. They allow the model to allocate attention mass to a learned sink rather than forcing all attention to content tokens, which has been shown to improve attention stability in streaming inference and training with sliding-window attention.&lt;/p&gt;
&lt;p&gt;After a deeper investigation, we identified several major issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;verl hard-codes FlashAttention v2 in &lt;code&gt;fsdp_worker&lt;/code&gt;, which does not support attention sinks.&lt;/li&gt;
&lt;li&gt;The attention sink backward pass is not supported in FlashAttention v2 and v3, so it does not work as expected even when FlashAttention v3 is enabled.&lt;/li&gt;
&lt;li&gt;Since the forward pass has not yet been merged into the original FlashAttention v3 repository, we leveraged the forward pass from the vLLM FlashAttention fork (PR #75) and implemented the backward pass to compute the sink gradient.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Standard Attention
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scores = QK^T / sqrt(d)               
probs = softmax(scores, dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)       
output = probs @ V                   
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Attention with Sinks (GPT-OSS)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scores = QK^T / sqrt(d)                               
combined = concat([scores, sink_param], dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)       
probs = softmax(combined, dim=-&lt;span class="hljs-number"&gt;1&lt;/span&gt;)                     
probs_content = probs[..., :-&lt;span class="hljs-number"&gt;1&lt;/span&gt;]                       
output = probs_content @ V                           
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key difference:&lt;/strong&gt; The sink participates in softmax normalization but doesn't contribute to the output.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mathematical Formulation
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The attention weight for content token j in row i is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;
P_{ij}
=
\frac{\exp(S_{ij})}
{\sum_{j'=1}^{N_k} \exp(S_{ij'}) + \exp(S_h)}
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;&lt;span class="mop op-symbol small-op"&gt;∑&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mrel mtight"&gt;=&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;+&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&lt;sub&gt;ij&lt;/sub&gt; = Q&lt;sub&gt;i&lt;/sub&gt; K&lt;sub&gt;j&lt;/sub&gt;&lt;sup&gt;⊤&lt;/sup&gt; / √d are the attention scores&lt;/li&gt;
&lt;li&gt;P&lt;sub&gt;ij&lt;/sub&gt; are the attention weights for the content tokens&lt;/li&gt;
&lt;li&gt;S&lt;sub&gt;h&lt;/sub&gt; is the learnable sink parameter for head h&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sink Probability:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The sink probability is computed but not used in the output:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;/munderover&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;msup&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo lspace="0em" mathvariant="normal" rspace="0em"&gt;′&lt;/mo&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;exp&lt;/mi&gt;&lt;mo&gt;⁡&lt;/mo&gt;&lt;mo stretchy="false"&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;
P_{i,h}
=
\frac{\exp(S_h)}
{\sum_{j'=1}^{N_k} \exp(S_{ij'}) + \exp(S_h)}
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;&lt;span class="mop op-symbol small-op"&gt;∑&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mrel mtight"&gt;=&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;′&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;+&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mop"&gt;exp&lt;/span&gt;&lt;span class="mopen"&gt;(&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Backward Pass
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The gradient of the loss L with respect to the sink parameter S&lt;sub&gt;h&lt;/sub&gt; is:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence="true"&gt;(&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mo stretchy="false"&gt;{&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo fence="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;
\frac{\partial L}{\partial S_h}
=
-
\sum_i
P_{i,h}
\left(
\frac{\partial L}{\partial S_{i,h}}
-
\sum_{j \in \{1,\ldots,N_k\}}
P_{ij}
\frac{\partial L}{\partial S_{ij}}
\right)
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;−&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="minner"&gt;&lt;span class="mopen"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mbin"&gt;−&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="mrel mtight"&gt;∈&lt;/span&gt;&lt;span class="mopen mtight"&gt;{&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="minner mtight"&gt;…&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose mtight"&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Where:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;P&lt;sub&gt;i,h&lt;/sub&gt; is the sink attention probability for row i&lt;/li&gt;
&lt;li&gt;∂L/∂S&lt;sub&gt;ij&lt;/sub&gt; is the gradient with respect to the attention scores, including the sink&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Simplified Gradient:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since the sink is computed but not used in the output, its gradient ∂L/∂S&lt;sub&gt;i,h&lt;/sub&gt; = 0.&lt;/p&gt;
&lt;p&gt;Therefore, the backward equation simplifies to:&lt;/p&gt;
&lt;p&gt;&lt;span class="katex-display"&gt;&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;&lt;math display="block" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo fence="true"&gt;(&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mo stretchy="false"&gt;{&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo separator="true"&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy="false"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant="normal"&gt;∂&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo fence="true"&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;
\frac{\partial L}{\partial S_h}
=
-
\sum_i
P_{i,h}
\left(
\sum_{j \in \{1,\ldots,N_k\}}
P_{ij}
\frac{\partial L}{\partial S_{ij}}
\right)
&lt;/math&gt;&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;−&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;i&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mathnormal mtight"&gt;h&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="minner"&gt;&lt;span class="mopen"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mop op-limits"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;j&lt;/span&gt;&lt;span class="mrel mtight"&gt;∈&lt;/span&gt;&lt;span class="mopen mtight"&gt;{&lt;/span&gt;&lt;span class="mord mtight"&gt;1&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="minner mtight"&gt;…&lt;/span&gt;&lt;span class="mpunct mtight"&gt;,&lt;/span&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;N&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;k&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose mtight"&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="mop op-symbol large-op"&gt;∑&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;P&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathnormal"&gt;S&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathnormal mtight"&gt;ij&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord"&gt;∂&lt;/span&gt;&lt;span class="mord mathnormal"&gt;L&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose"&gt;&lt;span class="delimsizing mult"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span&gt;&lt;svg height="3.600em" viewBox="0 0 875 3600" width="0.875em" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z"&gt;&lt;/svg&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The forward pass was adapted from vLLM's FlashAttention fork, and we implemented the backward pass to compute gradients for the sink parameters. The implementation will be released following the internal review process.&lt;/p&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Results
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;After applying the fix in FlashAttention v3, we observe substantially faster convergence for GPT-OSS-20B across a range of reinforcement learning tasks. These include single-turn RL on math reasoning (GSM8K — red curve in Figure 5), instruction following (VerifyIf, evaluated on an out-of-domain multi-if benchmark — Figure 6), and multi-turn agentic RL with tool use (ReTool — Figure 7).&lt;/p&gt;
&lt;p&gt;Across all settings, training becomes stable and exhibits steady reward improvement.&lt;/p&gt;
&lt;p align="center"&gt;
  &lt;img src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/6TYGevydK99nQ-I1QTouf.png" width="500" /&gt;
&lt;/p&gt;


&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt;. Single Turn GSM8K, the red curve converges much faster than the rest without the fix&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Entropy&lt;/th&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average entropy in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ydmBLCSGlD9YKWiIocI1S.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average gradient norm in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/_kl8mn_CXPsRYJ467IbFs.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/vb2JmmSu-LI5szC_84KsM.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;. On verifiable instruction following the task, the run without the fix collapsed (blue), and the run with fix showed steady reward improvement.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Average Gradient Norm&lt;/th&gt;
&lt;th&gt;Average Reward&lt;/th&gt;
&lt;th&gt;Validation Accuracy&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;img alt="Average gradient norm in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/_Xz7_RLhYuYhzGAeMjDXs.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="Average reward in a batch" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/uFX-sTWI6knecIf56uahk.png" /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img alt="val score accuracy mean@30 for aime_2025" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/5x2mdkpHcdvctZ96yfv58.png" /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;. On the Retool task, the run with fix showed steady reward improvement and no gradient exploding (fa2 is the flash attention 2 without the fix while fa3 is the flash attention 3 with the fix). After the fix, the validation accuracy score goes up now.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Memory-Efficient Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Mitigating FSDP Memory Blow-Ups Caused by Repeated MoE Expert Materialization
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;One issue we consistently encountered was excessive memory allocation during the FSDP forward pass, which led to repeated out-of-memory (OOM) failures when training GPT-OSS-20B bf16 models on 16 H200 nodes (max response length: 16k, prompt length: 8k). This behavior is highly unexpected for a 20B-parameter MoE model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m File "/home/jobuser/.local/lib/python3.10/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py", line 123, in forward
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m hidden_states = hidden_states.repeat(num_experts, 1)
2025-11-27T11:15:27.927Z [36m(TaskRunner pid=32081)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 180.00 GiB. GPU 0 has a total capacity of 139.72 GiB of which 110.94 GiB is free. Process 685851 has 24.88 GiB memory in use. Process 692458 has 3.87 GiB memory in use. Of the allocated memory 23.28 GiB is allocated by PyTorch, and 84.43 MiB is reserved by PyTorch but unallocated.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We identified the issue as originating from two different implementations of the MoE forward path in Hugging Face Transformers. This issue has also been reported by other users: https://github.com/huggingface/transformers/issues/40073; When verl computes log-probabilities under FSDP, the inference forward path is triggered. In the current Hugging Face implementation, this path duplicates hidden states for all experts and performs batched matrix multiplication, materializing extremely large tensors in GPU memory. By contrast, the training forward path uses a for-loop to process each expert sequentially and then combines the results. While slower, this approach is significantly more memory efficient.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;span class="hljs-meta"&gt;    @GPUMemoryLogger(&lt;span class="hljs-params"&gt;role=&lt;span class="hljs-string"&gt;"dp actor"&lt;/span&gt;, logger=logger&lt;/span&gt;)&lt;/span&gt;
    &lt;span class="hljs-keyword"&gt;def&lt;/span&gt; &lt;span class="hljs-title function_"&gt;compute_log_prob&lt;/span&gt;(&lt;span class="hljs-params"&gt;self, data: DataProto, calculate_entropy=&lt;span class="hljs-literal"&gt;False&lt;/span&gt;&lt;/span&gt;) -&amp;gt; torch.Tensor:
        &lt;span class="hljs-string"&gt;"""&lt;/span&gt;
&lt;span class="hljs-string"&gt;        ....&lt;/span&gt;
&lt;span class="hljs-string"&gt;        """&lt;/span&gt;
        
        self.actor_module.&lt;span class="hljs-built_in"&gt;eval&lt;/span&gt;()
        ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We patched the Hugging Face implementation to use a more memory-efficient execution path, avoiding repeated materialization of experts.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Sequence Parallel with Flash Attention V3
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Agentic RL requires the agent to interact with the environment over multiple steps while maintaining an ever-expanding context. Observations and environment feedback from each step are appended to the context and used as input for subsequent decision-making, which introduces significant challenges for memory efficiency and scalability during training.&lt;/p&gt;
&lt;p&gt;Under fully sharded data parallelism (FSDP), model parameters, optimizer states, and gradients are sharded across the entire world size (i.e., all GPUs in the training cluster). Each GPU stores and updates only its assigned parameter shards, while rollout data are replicated across all GPUs—meaning every GPU processes the full agent interaction history for each rollout.&lt;/p&gt;
&lt;p&gt;During the forward pass, when computation reaches a layer whose parameters are not locally available, an &lt;code&gt;all_gather&lt;/code&gt; operation is triggered to materialize the full parameters across GPUs. During the backward pass, a corresponding &lt;code&gt;reduce_scatter&lt;/code&gt; operation aggregates gradients and ensures that each GPU retains only its local shard. This provides a degree of scaling: as the number of GPUs increases, the per-GPU memory footprint decreases.&lt;/p&gt;
&lt;p&gt;FSDP provides model-level scaling by sharding model parameters, gradients, and optimizer states across GPUs. Sequence parallelism (or context parallelism) further reduces per-GPU memory consumption by partitioning the input sequence across devices, thereby lowering the peak activation memory on each GPU.&lt;/p&gt;
&lt;p&gt;As the number of sequence-parallel dimensions increases, the maximum activation memory per GPU correspondingly decreases. We have implemented sequence parallelism to be attention-sink-aware and compatible with FlashAttention v3 (Figure 8, right).&lt;/p&gt;
&lt;p&gt;&lt;img alt="SP  (2)" src="https://cdn-uploads.huggingface.co/production/uploads/64efbd469e7770db74cb72f5/ryT_y9BpbFSdMDxNYlVlK.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8&lt;/strong&gt;. Left: Inference without sequence parallelism. Right: Inference with sequence parallelism, where additional all-to-all communication is performed before and after the attention layer. This partitions the sequence across parallel workers and reduces the peak memory footprint of attention computation by a factor proportional to the sequence-parallelism degree.&lt;/p&gt;
&lt;p&gt;Sequence parallelism scales along the sequence dimension to reduce the per-GPU activation footprint. Input tokens from all sequences are packed into a single contiguous list by removing padding tokens, while position IDs are used to distinguish tokens belonging to different sequences. This design naturally benefits from FlashAttention’s variable-length support. For sequence parallelism, layers other than the attention layer do not have inter-position dependencies; therefore, they do not require each GPU to hold a complete sequence shard, and no additional communication is needed for these layers.&lt;/p&gt;
&lt;p&gt;The attention layer, however, requires all tokens belonging to the same sequence to be present on the same GPU in order to compute attention weights correctly. To satisfy this constraint, an all-to-all communication is performed to gather sequence elements, with the split performed at the attention-head level. This design avoids communication within the attention computation itself, which would otherwise be prohibitively expensive. After the attention layer, a single all-to-all communication redistributes the outputs back to their original sequence-parallel layout, after which the remaining non-attention layers can proceed without further synchronization.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Our journey to enable agentic RL training for the GPT-OSS backbone model was a practical retrospective, highlighting that unlocking advanced capabilities in open-source LLMs requires meticulous, deep-dive engineering.&lt;/p&gt;
&lt;p&gt;We made contributions that transformed the viability of GPT-OSS for agentic applications, specifically by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Stabilizing PPO:&lt;/strong&gt; We contributed a fix to restore on-policy integrity, overriding the log-probability mismatch caused by the MoE architecture’s non-determinism (Figure 2).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Enabling Attention Sink Support:&lt;/strong&gt; We successfully implemented and integrated the attention sink backward pass into FlashAttention v3, correcting the catastrophic training–inference mismatch that had previously caused instability and slow convergence (Figures 5, 6, and 7).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Scaling Memory Efficiency:&lt;/strong&gt; We introduced crucial memory optimizations, including patching the MoE materialization process and integrating sequence parallelism with the new attention sink support, enabling training with the long context windows essential for multi-step agents (Figure 8).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These engineering efforts validate GPT-OSS as a scalable and high-performance backbone for building the next generation of intelligent, multi-step decision-making agents.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Acknowledgments
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Thanks to Deepak Agarwal, Bee-Chung Chen, Animesh Singh, Gungor Polatkan, Balaji Krishnapuram, and Jitendra Agarwal for their leadership support.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		References
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Feng, Jiazhan, et al. &lt;em&gt;Retool: Reinforcement Learning for Strategic Tool Use in LLMs.&lt;/em&gt; arXiv preprint arXiv:2504.11536 (2025).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Xiao, Guangxuan, et al. &lt;em&gt;Efficient Streaming Language Models with Attention Sinks.&lt;/em&gt; arXiv preprint arXiv:2309.17453 (2023).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When Speed Kills Stability: Demystifying RL Collapse from the Training–Inference Mismatch.&lt;br /&gt;https://yingru.notion.site/When-Speed-Kills-Stability-Demystifying-RL-Collapse-from-the-Training-Inference-Mismatch-271211a558b7808d8b12d403fd15edda&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/LinkedIn/gpt-oss-agentic-rl</guid><pubDate>Tue, 27 Jan 2026 01:53:15 +0000</pubDate></item></channel></rss>