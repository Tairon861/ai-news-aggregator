<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 02 Dec 2025 01:49:04 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>OpenAGI emerges from stealth with an AI agent that it claims crushes OpenAI and Anthropic (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openagi-emerges-from-stealth-with-an-ai-agent-that-it-claims-crushes-openai</link><description>[unable to retrieve full-text content]&lt;p&gt;A stealth artificial intelligence startup founded by an MIT researcher emerged this morning with an ambitious claim: its new AI model can control computers better than systems built by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; — at a fraction of the cost.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt;, led by chief executive &lt;a href="https://www.qinzy.tech/"&gt;Zengyi Qin&lt;/a&gt;, released &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, a foundation model designed to operate computers autonomously by interpreting screenshots and executing actions across desktop applications. The San Francisco-based company says Lux achieves an 83.6 percent success rate on &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt;, a benchmark that has become the industry&amp;#x27;s most rigorous test for evaluating AI agents that control computers.&lt;/p&gt;&lt;p&gt;That score is a significant leap over the leading models from well-funded competitors. OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, released in January, scores 61.3 percent on the same benchmark. Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt; achieves 56.3 percent.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditional LLM training feeds a large amount of text corpus into the model. The model learns to produce text,&amp;quot; Qin said in an exclusive interview with VentureBeat. &amp;quot;By contrast, our model learns to produce actions. The model is trained with a large amount of computer screenshots and action sequences, allowing it to produce actions to control the computer.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement arrives at a pivotal moment for the AI industry. Technology giants and startups alike have poured billions of dollars into developing autonomous agents capable of navigating software, booking travel, filling out forms, and executing complex workflows. &lt;a href="https://openai.com/index/introducing-agentkit/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.anthropic.com/engineering/building-effective-agents"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://cloud.google.com/products/agent-builder?hl=en"&gt;Google&lt;/a&gt;, and &lt;a href="https://adoption.microsoft.com/en-us/ai-agents/agents-in-microsoft-365/"&gt;Microsoft&lt;/a&gt; have all released or announced agent products in the past year, betting that computer-controlling AI will become as transformative as chatbots.&lt;/p&gt;&lt;p&gt;Yet independent research has cast doubt on whether current agents are as capable as their creators suggest.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why university researchers built a tougher benchmark to test AI agents—and what they discovered&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web benchmark&lt;/a&gt;, developed by researchers at Ohio State University and the University of California, Berkeley, was designed specifically to expose the gap between marketing claims and actual performance.&lt;/p&gt;&lt;p&gt;Published in April and accepted to the &lt;a href="https://colmweb.org/"&gt;Conference on Language Modeling 2025&lt;/a&gt;, the benchmark comprises 300 diverse tasks across 136 real websites — everything from booking flights to navigating complex e-commerce checkouts. Unlike earlier benchmarks that cached parts of websites, Online-Mind2Web tests agents in live online environments where pages change dynamically and unexpected obstacles appear.&lt;/p&gt;&lt;p&gt;The results, according to the researchers, painted &amp;quot;a very different picture of the competency of current agents, suggesting over-optimism in previously reported results.&amp;quot;&lt;/p&gt;&lt;p&gt;When the Ohio State team tested five leading web agents with careful human evaluation, they found that many recent systems — despite heavy investment and marketing fanfare — did not outperform &lt;a href="https://osu-nlp-group.github.io/SeeAct/"&gt;SeeAct&lt;/a&gt;, a relatively simple agent released in January 2024. Even OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, the best performer among commercial offerings in their study, achieved only 61 percent success.&lt;/p&gt;&lt;p&gt;&amp;quot;It seemed that highly capable and practical agents were maybe indeed just months away,&amp;quot; the researchers wrote in a &lt;a href="https://tiancixue.notion.site/An-Illusion-of-Progress-Assessing-the-Current-State-of-Web-Agents-1ac6cd2b9aac80719cd6f68374aaf4b4"&gt;blog post&lt;/a&gt; accompanying their paper. &amp;quot;However, we are also well aware that there are still many fundamental gaps in research to fully autonomous agents, and current agents are probably not as competent as the reported benchmark numbers may depict.&amp;quot;&lt;/p&gt;&lt;p&gt;The benchmark has gained traction as an industry standard, with a public leaderboard hosted on Hugging Face tracking submissions from research groups and companies.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How OpenAGI trained its AI to take actions instead of just generating text&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;OpenAGI&amp;#x27;s claimed performance advantage stems from what the company calls &amp;quot;&lt;a href="https://developer.agiopen.org/docs/index"&gt;Agentic Active Pre-training&lt;/a&gt;,&amp;quot; a training methodology that differs fundamentally from how most large language models learn.&lt;/p&gt;&lt;p&gt;Conventional language models train on vast text corpora, learning to predict the next word in a sequence. The resulting systems excel at generating coherent text but were not designed to take actions in graphical environments.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, according to Qin, takes a different approach. The model trains on computer screenshots paired with action sequences, learning to interpret visual interfaces and determine which clicks, keystrokes, and navigation steps will accomplish a given goal.&lt;/p&gt;&lt;p&gt;&amp;quot;The action allows the model to actively explore the computer environment, and such exploration generates new knowledge, which is then fed back to the model for training,&amp;quot; Qin told VentureBeat. &amp;quot;This is a naturally self-evolving process, where a better model produces better exploration, better exploration produces better knowledge, and better knowledge leads to a better model.&amp;quot;&lt;/p&gt;&lt;p&gt;This self-reinforcing training loop, if it functions as described, could help explain how a smaller team might achieve results that elude larger organizations. Rather than requiring ever-larger static datasets, the approach would allow the model to continuously improve by generating its own training data through exploration.&lt;/p&gt;&lt;p&gt;OpenAGI also claims significant cost advantages. The company says Lux operates at roughly one-tenth the cost of frontier models from OpenAI and Anthropic while executing tasks faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Unlike browser-only competitors, Lux can control Slack, Excel, and other desktop applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;A critical distinction in OpenAGI&amp;#x27;s announcement: &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; can control applications across an entire desktop operating system, not just web browsers.&lt;/p&gt;&lt;p&gt;Most commercially available computer-use agents, including early versions of Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, focus primarily on browser-based tasks. That limitation excludes vast categories of productivity work that occur in desktop applications — spreadsheets in Microsoft Excel, communications in Slack, design work in Adobe products, code editing in development environments.&lt;/p&gt;&lt;p&gt;OpenAGI says Lux can navigate these native applications, a capability that would substantially expand the addressable market for computer-use agents. The company is releasing a developer software development kit alongside the model, allowing third parties to build applications on top of Lux.&lt;/p&gt;&lt;p&gt;The company is also working with &lt;a href="https://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt; to optimize &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; for edge devices, which would allow the model to run locally on laptops and workstations rather than requiring cloud infrastructure. That partnership could address enterprise concerns about sending sensitive screen data to external servers.&lt;/p&gt;&lt;p&gt;&amp;quot;We are partnering with Intel to optimize our model on edge devices, which will make it the best on-device computer-use model,&amp;quot; Qin said.&lt;/p&gt;&lt;p&gt;The company confirmed it is in exploratory discussions with AMD and Microsoft about additional partnerships.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What happens when you ask an AI agent to copy your bank details&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Computer-use agents present novel safety challenges that do not arise with conventional chatbots. An AI system capable of clicking buttons, entering text, and navigating applications could, if misdirected, cause significant harm — transferring money, deleting files, or exfiltrating sensitive information.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; says it has built safety mechanisms directly into Lux. When the model encounters requests that violate its safety policies, it refuses to proceed and alerts the user.&lt;/p&gt;&lt;p&gt;In an example provided by the company, when a user asked the model to &amp;quot;copy my bank details and paste it into a new Google doc,&amp;quot; Lux responded with an internal reasoning step: &amp;quot;The user asks me to copy the bank details, which are sensitive information. Based on the safety policy, I am not able to perform this action.&amp;quot; The model then issued a warning to the user rather than executing the potentially dangerous request.&lt;/p&gt;&lt;p&gt;Such safeguards will face intense scrutiny as computer-use agents proliferate. Security researchers have already demonstrated prompt injection attacks against early agent systems, where malicious instructions embedded in websites or documents can hijack an agent&amp;#x27;s behavior. Whether Lux&amp;#x27;s safety mechanisms can withstand adversarial attacks remains to be tested by independent researchers.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The MIT researcher who built two of GitHub&amp;#x27;s most downloaded AI models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.qinzy.tech/"&gt;Qin&lt;/a&gt; brings an unusual combination of academic credentials and entrepreneurial experience to OpenAGI.&lt;/p&gt;&lt;p&gt;He completed his doctorate at the Massachusetts Institute of Technology in 2025, where his research focused on computer vision, robotics, and machine learning. His academic work appeared in top venues including the &lt;a href="https://cvpr.thecvf.com/"&gt;Conference on Computer Vision and Pattern Recognition&lt;/a&gt;, the &lt;a href="https://iclr.cc/"&gt;International Conference on Learning Representations&lt;/a&gt;, and the &lt;a href="https://icml.cc/"&gt;International Conference on Machine Learning&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Before founding OpenAGI, Qin built several widely adopted AI systems. &lt;a href="https://research.myshell.ai/jetmoe"&gt;JetMoE&lt;/a&gt;, a large language model he led development on, demonstrated that a high-performing model could be trained from scratch for less than $100,000 — a fraction of the tens of millions typically required. The model outperformed Meta&amp;#x27;s &lt;a href="https://huggingface.co/meta-llama/Llama-2-7b"&gt;LLaMA2-7B&lt;/a&gt; on standard benchmarks, according to a technical report that attracted attention from MIT&amp;#x27;s Computer Science and Artificial Intelligence Laboratory.&lt;/p&gt;&lt;p&gt;His previous open-source projects achieved remarkable adoption. &lt;a href="https://research.myshell.ai/open-voice"&gt;OpenVoice&lt;/a&gt;, a voice cloning model, accumulated approximately 35,000 stars on GitHub and ranked in the top 0.03 percent of open-source projects by popularity. &lt;a href="https://github.com/myshell-ai/MeloTTS"&gt;MeloTTS&lt;/a&gt;, a text-to-speech system, has been downloaded more than 19 million times, making it one of the most widely used audio AI models since its 2024 release.&lt;/p&gt;&lt;p&gt;Qin also co-founded &lt;a href="https://myshell.ai/"&gt;MyShell&lt;/a&gt;, an AI agent platform that has attracted six million users who have collectively built more than 200,000 AI agents. Users have had more than one billion interactions with agents on the platform, according to the company.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the billion-dollar race to build AI that controls your computer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The computer-use agent market has attracted intense interest from investors and technology giants over the past year.&lt;/p&gt;&lt;p&gt;OpenAI released &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt; in January, allowing users to instruct an AI to complete tasks across the web. Anthropic has continued developing Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, positioning it as a core capability of its Claude model family. Google has incorporated agent features into its &lt;a href="https://gemini.google/overview/agent/?utm_source=gemini&amp;amp;utm_medium=paid_media&amp;amp;utm_campaign=g1_sb_ee_2tb_ai&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=2024enUS_gemfeb&amp;amp;gclsrc=aw.ds&amp;amp;gad_source=1&amp;amp;gad_campaignid=23230139705&amp;amp;gbraid=0AAAAApk5BhkJ0xALVXcjNzv91HdDzGiuM&amp;amp;gclid=CjwKCAiA86_JBhAIEiwA4i9Ju12ClTsObJAOyDZPPN24ifL0gh7lufci0PAhVryoY7i5rrmIVjjyFxoCiPkQAvD_BwE"&gt;Gemini&lt;/a&gt; products. Microsoft has integrated agent capabilities across its &lt;a href="https://www.microsoft.com/en-us/microsoft-copilot/copilot-101/copilot-ai-agents"&gt;Copilot&lt;/a&gt; offerings and &lt;a href="https://www.microsoft.com/en-us/microsoft-365-copilot/agents"&gt;Windows&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Yet the market remains nascent. Enterprise adoption has been limited by concerns about reliability, security, and the ability to handle edge cases that occur frequently in real-world workflows. The performance gaps revealed by benchmarks like &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt; suggest that current systems may not be ready for mission-critical applications.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; enters this competitive landscape as an independent alternative, positioning superior benchmark performance and lower costs against the massive resources of its well-funded rivals. The company&amp;#x27;s Lux model and developer SDK are available beginning today.&lt;/p&gt;&lt;p&gt;Whether OpenAGI can translate benchmark dominance into real-world reliability remains the central question. The AI industry has a long history of impressive demos that falter in production, of laboratory results that crumble against the chaos of actual use. Benchmarks measure what they measure, and the distance between a controlled test and an 8-hour workday full of edge cases, exceptions, and surprises can be vast.&lt;/p&gt;&lt;p&gt;But if &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; performs in the wild the way it performs in the lab, the implications extend far beyond one startup&amp;#x27;s success. It would suggest that the path to capable AI agents runs not through the largest checkbooks but through the cleverest architectures—that a small team with the right ideas can outmaneuver the giants.&lt;/p&gt;&lt;p&gt;The technology industry has seen that story before. It rarely stays true for long.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;A stealth artificial intelligence startup founded by an MIT researcher emerged this morning with an ambitious claim: its new AI model can control computers better than systems built by &lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://www.anthropic.com/"&gt;Anthropic&lt;/a&gt; — at a fraction of the cost.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt;, led by chief executive &lt;a href="https://www.qinzy.tech/"&gt;Zengyi Qin&lt;/a&gt;, released &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, a foundation model designed to operate computers autonomously by interpreting screenshots and executing actions across desktop applications. The San Francisco-based company says Lux achieves an 83.6 percent success rate on &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt;, a benchmark that has become the industry&amp;#x27;s most rigorous test for evaluating AI agents that control computers.&lt;/p&gt;&lt;p&gt;That score is a significant leap over the leading models from well-funded competitors. OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, released in January, scores 61.3 percent on the same benchmark. Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt; achieves 56.3 percent.&lt;/p&gt;&lt;p&gt;&amp;quot;Traditional LLM training feeds a large amount of text corpus into the model. The model learns to produce text,&amp;quot; Qin said in an exclusive interview with VentureBeat. &amp;quot;By contrast, our model learns to produce actions. The model is trained with a large amount of computer screenshots and action sequences, allowing it to produce actions to control the computer.&amp;quot;&lt;/p&gt;&lt;p&gt;The announcement arrives at a pivotal moment for the AI industry. Technology giants and startups alike have poured billions of dollars into developing autonomous agents capable of navigating software, booking travel, filling out forms, and executing complex workflows. &lt;a href="https://openai.com/index/introducing-agentkit/"&gt;OpenAI&lt;/a&gt;, &lt;a href="https://www.anthropic.com/engineering/building-effective-agents"&gt;Anthropic&lt;/a&gt;, &lt;a href="https://cloud.google.com/products/agent-builder?hl=en"&gt;Google&lt;/a&gt;, and &lt;a href="https://adoption.microsoft.com/en-us/ai-agents/agents-in-microsoft-365/"&gt;Microsoft&lt;/a&gt; have all released or announced agent products in the past year, betting that computer-controlling AI will become as transformative as chatbots.&lt;/p&gt;&lt;p&gt;Yet independent research has cast doubt on whether current agents are as capable as their creators suggest.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why university researchers built a tougher benchmark to test AI agents—and what they discovered&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web benchmark&lt;/a&gt;, developed by researchers at Ohio State University and the University of California, Berkeley, was designed specifically to expose the gap between marketing claims and actual performance.&lt;/p&gt;&lt;p&gt;Published in April and accepted to the &lt;a href="https://colmweb.org/"&gt;Conference on Language Modeling 2025&lt;/a&gt;, the benchmark comprises 300 diverse tasks across 136 real websites — everything from booking flights to navigating complex e-commerce checkouts. Unlike earlier benchmarks that cached parts of websites, Online-Mind2Web tests agents in live online environments where pages change dynamically and unexpected obstacles appear.&lt;/p&gt;&lt;p&gt;The results, according to the researchers, painted &amp;quot;a very different picture of the competency of current agents, suggesting over-optimism in previously reported results.&amp;quot;&lt;/p&gt;&lt;p&gt;When the Ohio State team tested five leading web agents with careful human evaluation, they found that many recent systems — despite heavy investment and marketing fanfare — did not outperform &lt;a href="https://osu-nlp-group.github.io/SeeAct/"&gt;SeeAct&lt;/a&gt;, a relatively simple agent released in January 2024. Even OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt;, the best performer among commercial offerings in their study, achieved only 61 percent success.&lt;/p&gt;&lt;p&gt;&amp;quot;It seemed that highly capable and practical agents were maybe indeed just months away,&amp;quot; the researchers wrote in a &lt;a href="https://tiancixue.notion.site/An-Illusion-of-Progress-Assessing-the-Current-State-of-Web-Agents-1ac6cd2b9aac80719cd6f68374aaf4b4"&gt;blog post&lt;/a&gt; accompanying their paper. &amp;quot;However, we are also well aware that there are still many fundamental gaps in research to fully autonomous agents, and current agents are probably not as competent as the reported benchmark numbers may depict.&amp;quot;&lt;/p&gt;&lt;p&gt;The benchmark has gained traction as an industry standard, with a public leaderboard hosted on Hugging Face tracking submissions from research groups and companies.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How OpenAGI trained its AI to take actions instead of just generating text&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;OpenAGI&amp;#x27;s claimed performance advantage stems from what the company calls &amp;quot;&lt;a href="https://developer.agiopen.org/docs/index"&gt;Agentic Active Pre-training&lt;/a&gt;,&amp;quot; a training methodology that differs fundamentally from how most large language models learn.&lt;/p&gt;&lt;p&gt;Conventional language models train on vast text corpora, learning to predict the next word in a sequence. The resulting systems excel at generating coherent text but were not designed to take actions in graphical environments.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt;, according to Qin, takes a different approach. The model trains on computer screenshots paired with action sequences, learning to interpret visual interfaces and determine which clicks, keystrokes, and navigation steps will accomplish a given goal.&lt;/p&gt;&lt;p&gt;&amp;quot;The action allows the model to actively explore the computer environment, and such exploration generates new knowledge, which is then fed back to the model for training,&amp;quot; Qin told VentureBeat. &amp;quot;This is a naturally self-evolving process, where a better model produces better exploration, better exploration produces better knowledge, and better knowledge leads to a better model.&amp;quot;&lt;/p&gt;&lt;p&gt;This self-reinforcing training loop, if it functions as described, could help explain how a smaller team might achieve results that elude larger organizations. Rather than requiring ever-larger static datasets, the approach would allow the model to continuously improve by generating its own training data through exploration.&lt;/p&gt;&lt;p&gt;OpenAGI also claims significant cost advantages. The company says Lux operates at roughly one-tenth the cost of frontier models from OpenAI and Anthropic while executing tasks faster.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Unlike browser-only competitors, Lux can control Slack, Excel, and other desktop applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;A critical distinction in OpenAGI&amp;#x27;s announcement: &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; can control applications across an entire desktop operating system, not just web browsers.&lt;/p&gt;&lt;p&gt;Most commercially available computer-use agents, including early versions of Anthropic&amp;#x27;s Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, focus primarily on browser-based tasks. That limitation excludes vast categories of productivity work that occur in desktop applications — spreadsheets in Microsoft Excel, communications in Slack, design work in Adobe products, code editing in development environments.&lt;/p&gt;&lt;p&gt;OpenAGI says Lux can navigate these native applications, a capability that would substantially expand the addressable market for computer-use agents. The company is releasing a developer software development kit alongside the model, allowing third parties to build applications on top of Lux.&lt;/p&gt;&lt;p&gt;The company is also working with &lt;a href="https://www.intel.com/content/www/us/en/homepage.html"&gt;Intel&lt;/a&gt; to optimize &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; for edge devices, which would allow the model to run locally on laptops and workstations rather than requiring cloud infrastructure. That partnership could address enterprise concerns about sending sensitive screen data to external servers.&lt;/p&gt;&lt;p&gt;&amp;quot;We are partnering with Intel to optimize our model on edge devices, which will make it the best on-device computer-use model,&amp;quot; Qin said.&lt;/p&gt;&lt;p&gt;The company confirmed it is in exploratory discussions with AMD and Microsoft about additional partnerships.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What happens when you ask an AI agent to copy your bank details&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Computer-use agents present novel safety challenges that do not arise with conventional chatbots. An AI system capable of clicking buttons, entering text, and navigating applications could, if misdirected, cause significant harm — transferring money, deleting files, or exfiltrating sensitive information.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; says it has built safety mechanisms directly into Lux. When the model encounters requests that violate its safety policies, it refuses to proceed and alerts the user.&lt;/p&gt;&lt;p&gt;In an example provided by the company, when a user asked the model to &amp;quot;copy my bank details and paste it into a new Google doc,&amp;quot; Lux responded with an internal reasoning step: &amp;quot;The user asks me to copy the bank details, which are sensitive information. Based on the safety policy, I am not able to perform this action.&amp;quot; The model then issued a warning to the user rather than executing the potentially dangerous request.&lt;/p&gt;&lt;p&gt;Such safeguards will face intense scrutiny as computer-use agents proliferate. Security researchers have already demonstrated prompt injection attacks against early agent systems, where malicious instructions embedded in websites or documents can hijack an agent&amp;#x27;s behavior. Whether Lux&amp;#x27;s safety mechanisms can withstand adversarial attacks remains to be tested by independent researchers.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The MIT researcher who built two of GitHub&amp;#x27;s most downloaded AI models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.qinzy.tech/"&gt;Qin&lt;/a&gt; brings an unusual combination of academic credentials and entrepreneurial experience to OpenAGI.&lt;/p&gt;&lt;p&gt;He completed his doctorate at the Massachusetts Institute of Technology in 2025, where his research focused on computer vision, robotics, and machine learning. His academic work appeared in top venues including the &lt;a href="https://cvpr.thecvf.com/"&gt;Conference on Computer Vision and Pattern Recognition&lt;/a&gt;, the &lt;a href="https://iclr.cc/"&gt;International Conference on Learning Representations&lt;/a&gt;, and the &lt;a href="https://icml.cc/"&gt;International Conference on Machine Learning&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Before founding OpenAGI, Qin built several widely adopted AI systems. &lt;a href="https://research.myshell.ai/jetmoe"&gt;JetMoE&lt;/a&gt;, a large language model he led development on, demonstrated that a high-performing model could be trained from scratch for less than $100,000 — a fraction of the tens of millions typically required. The model outperformed Meta&amp;#x27;s &lt;a href="https://huggingface.co/meta-llama/Llama-2-7b"&gt;LLaMA2-7B&lt;/a&gt; on standard benchmarks, according to a technical report that attracted attention from MIT&amp;#x27;s Computer Science and Artificial Intelligence Laboratory.&lt;/p&gt;&lt;p&gt;His previous open-source projects achieved remarkable adoption. &lt;a href="https://research.myshell.ai/open-voice"&gt;OpenVoice&lt;/a&gt;, a voice cloning model, accumulated approximately 35,000 stars on GitHub and ranked in the top 0.03 percent of open-source projects by popularity. &lt;a href="https://github.com/myshell-ai/MeloTTS"&gt;MeloTTS&lt;/a&gt;, a text-to-speech system, has been downloaded more than 19 million times, making it one of the most widely used audio AI models since its 2024 release.&lt;/p&gt;&lt;p&gt;Qin also co-founded &lt;a href="https://myshell.ai/"&gt;MyShell&lt;/a&gt;, an AI agent platform that has attracted six million users who have collectively built more than 200,000 AI agents. Users have had more than one billion interactions with agents on the platform, according to the company.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the billion-dollar race to build AI that controls your computer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The computer-use agent market has attracted intense interest from investors and technology giants over the past year.&lt;/p&gt;&lt;p&gt;OpenAI released &lt;a href="https://openai.com/index/introducing-operator/"&gt;Operator&lt;/a&gt; in January, allowing users to instruct an AI to complete tasks across the web. Anthropic has continued developing Claude &lt;a href="https://www.anthropic.com/news/3-5-models-and-computer-use"&gt;Computer Use&lt;/a&gt;, positioning it as a core capability of its Claude model family. Google has incorporated agent features into its &lt;a href="https://gemini.google/overview/agent/?utm_source=gemini&amp;amp;utm_medium=paid_media&amp;amp;utm_campaign=g1_sb_ee_2tb_ai&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;utm_campaign=2024enUS_gemfeb&amp;amp;gclsrc=aw.ds&amp;amp;gad_source=1&amp;amp;gad_campaignid=23230139705&amp;amp;gbraid=0AAAAApk5BhkJ0xALVXcjNzv91HdDzGiuM&amp;amp;gclid=CjwKCAiA86_JBhAIEiwA4i9Ju12ClTsObJAOyDZPPN24ifL0gh7lufci0PAhVryoY7i5rrmIVjjyFxoCiPkQAvD_BwE"&gt;Gemini&lt;/a&gt; products. Microsoft has integrated agent capabilities across its &lt;a href="https://www.microsoft.com/en-us/microsoft-copilot/copilot-101/copilot-ai-agents"&gt;Copilot&lt;/a&gt; offerings and &lt;a href="https://www.microsoft.com/en-us/microsoft-365-copilot/agents"&gt;Windows&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Yet the market remains nascent. Enterprise adoption has been limited by concerns about reliability, security, and the ability to handle edge cases that occur frequently in real-world workflows. The performance gaps revealed by benchmarks like &lt;a href="https://huggingface.co/spaces/osunlp/Online_Mind2Web_Leaderboard"&gt;Online-Mind2Web&lt;/a&gt; suggest that current systems may not be ready for mission-critical applications.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.agiopen.org/"&gt;OpenAGI&lt;/a&gt; enters this competitive landscape as an independent alternative, positioning superior benchmark performance and lower costs against the massive resources of its well-funded rivals. The company&amp;#x27;s Lux model and developer SDK are available beginning today.&lt;/p&gt;&lt;p&gt;Whether OpenAGI can translate benchmark dominance into real-world reliability remains the central question. The AI industry has a long history of impressive demos that falter in production, of laboratory results that crumble against the chaos of actual use. Benchmarks measure what they measure, and the distance between a controlled test and an 8-hour workday full of edge cases, exceptions, and surprises can be vast.&lt;/p&gt;&lt;p&gt;But if &lt;a href="https://www.agiopen.org/blog"&gt;Lux&lt;/a&gt; performs in the wild the way it performs in the lab, the implications extend far beyond one startup&amp;#x27;s success. It would suggest that the path to capable AI agents runs not through the largest checkbooks but through the cleverest architectures—that a small team with the right ideas can outmaneuver the giants.&lt;/p&gt;&lt;p&gt;The technology industry has seen that story before. It rarely stays true for long.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openagi-emerges-from-stealth-with-an-ai-agent-that-it-claims-crushes-openai</guid><pubDate>Mon, 01 Dec 2025 14:00:00 +0000</pubDate></item><item><title>Black Forest Labs raises $300M at $3.25B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/black-forest-labs-raises-300m-at-3-25b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/6844c7ed531e3aa09958eea8a9deae8bdabd0b54-3721x2798-1.png?resize=1200,902" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;German AI lab Black Forest Labs said on Monday that it has raised $300 million in a Series B funding round that values the company at $3.25 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was co-led by Salesforce Ventures and Anjney Midha (AMP), and saw participation from a16z, NVIDIA, Northzone, Creandum, Earlybird VC, BroadLight Capital, General Catalyst, Temasek, Bain Capital Ventures, Air Street Capital, Visionaries Club, Canva, and Figma Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup said it would use the funds for research and development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs, which makes foundation AI models for generating and editing images, has risen to fame quickly since its launch in August 2024. The company was in the news last year after it was revealed that Elon Musk’s Grok chatbot was using the German company’s models to generate images, and its models are being used by a slew of companies, like Adobe, fal.ai, Picsart, ElevenLabs, VSCO, and Vercel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup recently revealed the latest version of its image-generation model, Flux 2, which it says features better text and image rendering, and uses up to 10 images as a reference to maintain the style and tone while generating images. The model can generate images at resolutions up to 4K pixels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs’ co-founders, Robin Rombach, Patrick Esser, and Andreas Blattmann, were formerly researchers who helped create Stability AI’s Stable Diffusion models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/6844c7ed531e3aa09958eea8a9deae8bdabd0b54-3721x2798-1.png?resize=1200,902" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;German AI lab Black Forest Labs said on Monday that it has raised $300 million in a Series B funding round that values the company at $3.25 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was co-led by Salesforce Ventures and Anjney Midha (AMP), and saw participation from a16z, NVIDIA, Northzone, Creandum, Earlybird VC, BroadLight Capital, General Catalyst, Temasek, Bain Capital Ventures, Air Street Capital, Visionaries Club, Canva, and Figma Ventures.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup said it would use the funds for research and development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs, which makes foundation AI models for generating and editing images, has risen to fame quickly since its launch in August 2024. The company was in the news last year after it was revealed that Elon Musk’s Grok chatbot was using the German company’s models to generate images, and its models are being used by a slew of companies, like Adobe, fal.ai, Picsart, ElevenLabs, VSCO, and Vercel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup recently revealed the latest version of its image-generation model, Flux 2, which it says features better text and image rendering, and uses up to 10 images as a reference to maintain the style and tone while generating images. The model can generate images at resolutions up to 4K pixels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Black Forest Labs’ co-founders, Robin Rombach, Patrick Esser, and Andreas Blattmann, were formerly researchers who helped create Stability AI’s Stable Diffusion models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/black-forest-labs-raises-300m-at-3-25b-valuation/</guid><pubDate>Mon, 01 Dec 2025 14:08:13 +0000</pubDate></item><item><title>[NEW] Agentic AI autonomy grows in North American enterprises (AI News)</title><link>https://www.artificialintelligence-news.com/news/agentic-ai-autonomy-grows-in-north-american-enterprises/</link><description>&lt;p&gt;North American enterprises are now actively deploying agentic AI systems intended to reason, adapt, and act with complete autonomy.&lt;/p&gt;&lt;p&gt;Data from Digitate’s three-year global programme indicates that, while adoption is universal across the board, regional maturity paths are diverging. North American firms are scaling toward full autonomy, whereas their European counterparts are prioritising governance frameworks and data stewardship to build long-term resilience.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-utility-to-profitability"&gt;From utility to profitability&lt;/h3&gt;&lt;p&gt;The story of enterprise automation has changed. In 2023, the primary objective for most IT leaders was cost reduction and the streamlining of routine tasks. By 2025, the focus has expanded. AI is no longer viewed solely as an operational utility but as a capability enabling profit.&lt;/p&gt;&lt;p&gt;Data supports this change in perspective. The report indicates that North American organisations are seeing a median return on investment (ROI) of $175 million from their implementations. Interestingly, this financial validation is not unique to the fast-moving North American market. European enterprises, despite a more measured and governance-heavy approach, report a comparable median ROI of approximately $170 million.&lt;/p&gt;&lt;p&gt;This consistency suggests that while deployment strategies differ, with Europe focusing on risk management and North America on speed, the financial outcomes are similar. Every organisation surveyed confirmed implementing AI within the last two years, utilising an average of five distinct tools.&amp;nbsp;&lt;/p&gt;&lt;p&gt;While generative AI remains the most widely deployed at 74 percent, there is a notable rise in “agentic” capabilities. Over 40 percent of enterprises have introduced agentic or agent-based AI, advancing beyond static automation toward systems that can manage goal-oriented workflows.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-it-operations-autonomy-becomes-the-proving-ground-for-agentic-ai"&gt;IT operations autonomy becomes the proving ground for agentic AI&lt;/h3&gt;&lt;p&gt;While marketing and customer service often dominate public discourse regarding AI, the IT function itself has emerged as the primary laboratory for these deployments. IT environments are inherently data-rich and structured, creating ideal conditions for models to learn, yet they remain dynamic enough to require the adaptive reasoning that agentic AI systems promise.&lt;/p&gt;&lt;p&gt;This explains why 78 percent of respondents have deployed AI within IT operations, the highest rate of any business function. Cloud visibility and cost optimisation lead the adoption curve at 52 percent, followed closely by event management at 48 percent. In these scenarios, the technology is not alerting humans to problems so much as actively interpreting telemetry data to provide a unified view of spending across hybrid environments.&lt;/p&gt;&lt;p&gt;Teams leveraging these tools report improvements in decision accuracy (44%) and efficiency (43%), allowing them to handle higher workloads without a corresponding increase in escalations.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-cost-human-conundrum"&gt;The cost-human conundrum&lt;/h3&gt;&lt;p&gt;Despite the optimism surrounding ROI, the report highlights a “cost-human conundrum” that threatens to stall progress. The paradox is straightforward: enterprises deploy AI to reduce reliance on human labour and operational costs, yet those exact factors act as the primary inhibitors to growth.&lt;/p&gt;&lt;p&gt;47 percent of respondents cite the continued need for human intervention as a major drawback. Far from achieving the complete autonomy of “set and forget” solutions, these agentic AI systems require ongoing oversight, tuning, and exception management. Simultaneously, the cost of implementation ranks as the second-highest concern at 42 percent, driven by the expenses associated with model retraining, integration, and cloud infrastructure.&lt;/p&gt;&lt;p&gt;The talent required to manage these costs is in short supply. A lack of technical skills remains the primary obstacle to further adoption for 33 percent of organisations. Demand for professionals capable of developing, monitoring, and governing these complex systems exceeds current supply, creating a self-reinforcing loop where investment increases operational capacity but simultaneously raises human and financial dependencies.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-trust-and-perception-gap"&gt;Trust and perception gap&lt;/h3&gt;&lt;p&gt;A divergence in perspective exists between executive leadership and operational practitioners. While 94 percent of total respondents express trust in AI, this confidence is not distributed evenly. C-suite leaders are markedly more optimistic, with 61 percent classifying AI as “very trustworthy” and viewing it primarily as a financial lever.&lt;/p&gt;&lt;p&gt;Only 46 percent of non-C-suite practitioners share this high level of trust. Those closer to the daily operation of these models are more acutely aware of reliability issues, transparency deficits, and the necessity for human oversight. This gap suggests that while leadership focuses on long-term overhaul and autonomy, teams on the ground are grappling with pragmatic delivery and governance challenges.&lt;/p&gt;&lt;p&gt;There is also a mixed view on how these agents will function. 61 percent of IT leaders view agentic systems not as replacements, but as collaborators that augment human capability. However, the expectation of automation varies by industry. In retail and transport, 67 percent believe agentic AI will alter the essential tasks of their roles, while in manufacturing, the same percentage views these agents primarily as personal assistants.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-complete-agentic-ai-autonomy-is-rapidly-approaching"&gt;Complete agentic AI autonomy is rapidly approaching&lt;/h3&gt;&lt;p&gt;The industry anticipates a rapid progression toward reduced human involvement in routine processes. Currently, 45 percent of organisations operate as semi- to fully-autonomous enterprises. Projections indicate this figure will rise to 74 percent by 2030.&lt;/p&gt;&lt;p&gt;This evolution implies a change in the role of IT. As capabilities mature, IT departments are expected to transition from being operational enablers to acting as orchestrators. In this model, the IT function manages the “system of systems,” ensuring that various intelligent agents interact correctly while humans focus on creativity, interpretation, and governance rather than execution.&lt;/p&gt;&lt;p&gt;“Agentic AI is the bridge between human ingenuity and autonomous intelligence that marks the dawn of IT as a profit-driving, strategic capability,” notes Avi Bhagtani, CMO at Digitate. “Enterprises have moved from experimenting with automation to scaling AI for measurable impact.”&lt;/p&gt;&lt;p&gt;The transition to agentic AI requires more than just software procurement; it demands an organisational philosophy that balances automation with human augmentation. Policies alone are insufficient; governance must be integrated directly into system design to ensure transparency and ethical oversight in every decision loop. European organisations are currently leading in this area, prioritising ethical deployment and strong oversight frameworks as a foundation for resilience.&lt;/p&gt;&lt;p&gt;Furthermore, the shortage of technical talent cannot be solved by hiring alone. Organisations must invest in upskilling existing teams, combining operations expertise with data science and compliance literacy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Finally, reliable autonomy depends on high-quality data. Investments in data integration and observability platforms are necessary to provide agents with the context required to act independently.&lt;/p&gt;&lt;p&gt;The era of experimental AI has passed. The current phase is defined by the pursuit of autonomy, where value is derived not from novelty, but from the ability to scale agentic AI sustainably across the enterprise.&lt;/p&gt;&lt;p&gt;“As organisations balance autonomy with accountability, those that embed trust, transparency, and human engagement into their AI strategy will shape the future of digital business,” Bhagtani concludes.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;How the MCP spec update boosts security as infrastructure scales&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;North American enterprises are now actively deploying agentic AI systems intended to reason, adapt, and act with complete autonomy.&lt;/p&gt;&lt;p&gt;Data from Digitate’s three-year global programme indicates that, while adoption is universal across the board, regional maturity paths are diverging. North American firms are scaling toward full autonomy, whereas their European counterparts are prioritising governance frameworks and data stewardship to build long-term resilience.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-utility-to-profitability"&gt;From utility to profitability&lt;/h3&gt;&lt;p&gt;The story of enterprise automation has changed. In 2023, the primary objective for most IT leaders was cost reduction and the streamlining of routine tasks. By 2025, the focus has expanded. AI is no longer viewed solely as an operational utility but as a capability enabling profit.&lt;/p&gt;&lt;p&gt;Data supports this change in perspective. The report indicates that North American organisations are seeing a median return on investment (ROI) of $175 million from their implementations. Interestingly, this financial validation is not unique to the fast-moving North American market. European enterprises, despite a more measured and governance-heavy approach, report a comparable median ROI of approximately $170 million.&lt;/p&gt;&lt;p&gt;This consistency suggests that while deployment strategies differ, with Europe focusing on risk management and North America on speed, the financial outcomes are similar. Every organisation surveyed confirmed implementing AI within the last two years, utilising an average of five distinct tools.&amp;nbsp;&lt;/p&gt;&lt;p&gt;While generative AI remains the most widely deployed at 74 percent, there is a notable rise in “agentic” capabilities. Over 40 percent of enterprises have introduced agentic or agent-based AI, advancing beyond static automation toward systems that can manage goal-oriented workflows.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-it-operations-autonomy-becomes-the-proving-ground-for-agentic-ai"&gt;IT operations autonomy becomes the proving ground for agentic AI&lt;/h3&gt;&lt;p&gt;While marketing and customer service often dominate public discourse regarding AI, the IT function itself has emerged as the primary laboratory for these deployments. IT environments are inherently data-rich and structured, creating ideal conditions for models to learn, yet they remain dynamic enough to require the adaptive reasoning that agentic AI systems promise.&lt;/p&gt;&lt;p&gt;This explains why 78 percent of respondents have deployed AI within IT operations, the highest rate of any business function. Cloud visibility and cost optimisation lead the adoption curve at 52 percent, followed closely by event management at 48 percent. In these scenarios, the technology is not alerting humans to problems so much as actively interpreting telemetry data to provide a unified view of spending across hybrid environments.&lt;/p&gt;&lt;p&gt;Teams leveraging these tools report improvements in decision accuracy (44%) and efficiency (43%), allowing them to handle higher workloads without a corresponding increase in escalations.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-cost-human-conundrum"&gt;The cost-human conundrum&lt;/h3&gt;&lt;p&gt;Despite the optimism surrounding ROI, the report highlights a “cost-human conundrum” that threatens to stall progress. The paradox is straightforward: enterprises deploy AI to reduce reliance on human labour and operational costs, yet those exact factors act as the primary inhibitors to growth.&lt;/p&gt;&lt;p&gt;47 percent of respondents cite the continued need for human intervention as a major drawback. Far from achieving the complete autonomy of “set and forget” solutions, these agentic AI systems require ongoing oversight, tuning, and exception management. Simultaneously, the cost of implementation ranks as the second-highest concern at 42 percent, driven by the expenses associated with model retraining, integration, and cloud infrastructure.&lt;/p&gt;&lt;p&gt;The talent required to manage these costs is in short supply. A lack of technical skills remains the primary obstacle to further adoption for 33 percent of organisations. Demand for professionals capable of developing, monitoring, and governing these complex systems exceeds current supply, creating a self-reinforcing loop where investment increases operational capacity but simultaneously raises human and financial dependencies.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-trust-and-perception-gap"&gt;Trust and perception gap&lt;/h3&gt;&lt;p&gt;A divergence in perspective exists between executive leadership and operational practitioners. While 94 percent of total respondents express trust in AI, this confidence is not distributed evenly. C-suite leaders are markedly more optimistic, with 61 percent classifying AI as “very trustworthy” and viewing it primarily as a financial lever.&lt;/p&gt;&lt;p&gt;Only 46 percent of non-C-suite practitioners share this high level of trust. Those closer to the daily operation of these models are more acutely aware of reliability issues, transparency deficits, and the necessity for human oversight. This gap suggests that while leadership focuses on long-term overhaul and autonomy, teams on the ground are grappling with pragmatic delivery and governance challenges.&lt;/p&gt;&lt;p&gt;There is also a mixed view on how these agents will function. 61 percent of IT leaders view agentic systems not as replacements, but as collaborators that augment human capability. However, the expectation of automation varies by industry. In retail and transport, 67 percent believe agentic AI will alter the essential tasks of their roles, while in manufacturing, the same percentage views these agents primarily as personal assistants.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-complete-agentic-ai-autonomy-is-rapidly-approaching"&gt;Complete agentic AI autonomy is rapidly approaching&lt;/h3&gt;&lt;p&gt;The industry anticipates a rapid progression toward reduced human involvement in routine processes. Currently, 45 percent of organisations operate as semi- to fully-autonomous enterprises. Projections indicate this figure will rise to 74 percent by 2030.&lt;/p&gt;&lt;p&gt;This evolution implies a change in the role of IT. As capabilities mature, IT departments are expected to transition from being operational enablers to acting as orchestrators. In this model, the IT function manages the “system of systems,” ensuring that various intelligent agents interact correctly while humans focus on creativity, interpretation, and governance rather than execution.&lt;/p&gt;&lt;p&gt;“Agentic AI is the bridge between human ingenuity and autonomous intelligence that marks the dawn of IT as a profit-driving, strategic capability,” notes Avi Bhagtani, CMO at Digitate. “Enterprises have moved from experimenting with automation to scaling AI for measurable impact.”&lt;/p&gt;&lt;p&gt;The transition to agentic AI requires more than just software procurement; it demands an organisational philosophy that balances automation with human augmentation. Policies alone are insufficient; governance must be integrated directly into system design to ensure transparency and ethical oversight in every decision loop. European organisations are currently leading in this area, prioritising ethical deployment and strong oversight frameworks as a foundation for resilience.&lt;/p&gt;&lt;p&gt;Furthermore, the shortage of technical talent cannot be solved by hiring alone. Organisations must invest in upskilling existing teams, combining operations expertise with data science and compliance literacy.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Finally, reliable autonomy depends on high-quality data. Investments in data integration and observability platforms are necessary to provide agents with the context required to act independently.&lt;/p&gt;&lt;p&gt;The era of experimental AI has passed. The current phase is defined by the pursuit of autonomy, where value is derived not from novelty, but from the ability to scale agentic AI sustainably across the enterprise.&lt;/p&gt;&lt;p&gt;“As organisations balance autonomy with accountability, those that embed trust, transparency, and human engagement into their AI strategy will shape the future of digital business,” Bhagtani concludes.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;How the MCP spec update boosts security as infrastructure scales&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/agentic-ai-autonomy-grows-in-north-american-enterprises/</guid><pubDate>Mon, 01 Dec 2025 15:53:22 +0000</pubDate></item><item><title>Amazon’s AI chatbot Rufus drove sales on Black Friday (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/amazons-ai-chatbot-rufus-drove-sales-on-black-friday/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon’s AI chatbot, Rufus, saw a surge of adoption on Black Friday, according to new data published over the weekend by market intelligence firm Sensor Tower. In the U.S., Amazon sessions that resulted in a purchase surged 100% on Black Friday compared with the trailing 30 days, while sessions that resulted in a purchase and didn’t include Rufus increased by only 20%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon saw a 75% day-over-day increase for sessions that included Rufus and resulted in a purchase, compared with just a 35% day-over-day increase for sessions without Rufus that had resulted in a purchase.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The firm also noted that Amazon sessions that involved the AI chatbot outpaced total website sessions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Black Friday, Amazon’s total website sessions increased by 20% day-over-day, while those that involved Rufus were up by 35%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s AI chat was first launched into beta in early 2024 before rolling out to all U.S. customers later that year. Today, Rufus helps Amazon shoppers find products, get recommendations, and perform product comparisons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rufus’ adoption to drive Black Friday sales is part of a broader surge in consumers turning to AI to help them with holiday shopping, data shows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Amazon Rufus" class="wp-image-2675035" height="351" src="https://techcrunch.com/wp-content/uploads/2024/03/Screenshot-2024-03-05-at-5.18.22 PM-e1709677222431.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;According to e-commerce data from Adobe Analytics, which tracks more than 1 trillion visits to U.S. retail websites, AI traffic to U.S. retail sites increased by 805% year-over-year on Black Friday. This indicates that consumers more heavily embraced generative AI chatbots to find deals and research products this year. The AI tools were mostly used for popular Black Friday deal categories like electronics, video games, appliances, toys, personal care items, and baby and toddler products. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe Analytics also noted that the use of AI increased conversions. It found U.S. shoppers who came to a retail site from an AI service were 38% more likely to buy, compared with non-AI traffic sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI directly contributed to the record Black Friday spending of $11.8 billion is less clear. Instead, the sizable figure this year could be due to higher prices, not an increase in online shopping. As TechCrunch reported on Saturday, Salesforce data showed prices were up by an average of 7%, while order volumes were down by 1%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sensor Tower’s data similarly suggests that consumers were perhaps being more conservative in their spending this year, likely due to economic strains. Even though mobile app and website adoption spiked on Black Friday compared to the previous 30 days, gains in total visits and downloads decelerated from 2024, its data indicated. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, Amazon’s and Walmart’s mobile app downloads grew by 24% and 20%, respectively on Black Friday, compared with the previous 30 days. But that growth paled when compared with 2024, when Amazon downloads surged by 50% and Walmart’s were up 75% during the same period, the firm pointed out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon and Walmart’s website visits on Black Friday were up by 90% and 100% this year, respectively, compared with the prior 30 days. However, those same numbers in 2024 were 95% and 130%, respectively.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a related Adobe survey, 48% of respondents said they have used or plan to use AI specifically for holiday shopping. &lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This &lt;em&gt;stream&lt;/em&gt; is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon’s AI chatbot, Rufus, saw a surge of adoption on Black Friday, according to new data published over the weekend by market intelligence firm Sensor Tower. In the U.S., Amazon sessions that resulted in a purchase surged 100% on Black Friday compared with the trailing 30 days, while sessions that resulted in a purchase and didn’t include Rufus increased by only 20%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Amazon saw a 75% day-over-day increase for sessions that included Rufus and resulted in a purchase, compared with just a 35% day-over-day increase for sessions without Rufus that had resulted in a purchase.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The firm also noted that Amazon sessions that involved the AI chatbot outpaced total website sessions. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Black Friday, Amazon’s total website sessions increased by 20% day-over-day, while those that involved Rufus were up by 35%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon’s AI chat was first launched into beta in early 2024 before rolling out to all U.S. customers later that year. Today, Rufus helps Amazon shoppers find products, get recommendations, and perform product comparisons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rufus’ adoption to drive Black Friday sales is part of a broader surge in consumers turning to AI to help them with holiday shopping, data shows.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Amazon Rufus" class="wp-image-2675035" height="351" src="https://techcrunch.com/wp-content/uploads/2024/03/Screenshot-2024-03-05-at-5.18.22 PM-e1709677222431.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;According to e-commerce data from Adobe Analytics, which tracks more than 1 trillion visits to U.S. retail websites, AI traffic to U.S. retail sites increased by 805% year-over-year on Black Friday. This indicates that consumers more heavily embraced generative AI chatbots to find deals and research products this year. The AI tools were mostly used for popular Black Friday deal categories like electronics, video games, appliances, toys, personal care items, and baby and toddler products. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe Analytics also noted that the use of AI increased conversions. It found U.S. shoppers who came to a retail site from an AI service were 38% more likely to buy, compared with non-AI traffic sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI directly contributed to the record Black Friday spending of $11.8 billion is less clear. Instead, the sizable figure this year could be due to higher prices, not an increase in online shopping. As TechCrunch reported on Saturday, Salesforce data showed prices were up by an average of 7%, while order volumes were down by 1%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sensor Tower’s data similarly suggests that consumers were perhaps being more conservative in their spending this year, likely due to economic strains. Even though mobile app and website adoption spiked on Black Friday compared to the previous 30 days, gains in total visits and downloads decelerated from 2024, its data indicated. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For instance, Amazon’s and Walmart’s mobile app downloads grew by 24% and 20%, respectively on Black Friday, compared with the previous 30 days. But that growth paled when compared with 2024, when Amazon downloads surged by 50% and Walmart’s were up 75% during the same period, the firm pointed out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon and Walmart’s website visits on Black Friday were up by 90% and 100% this year, respectively, compared with the prior 30 days. However, those same numbers in 2024 were 95% and 130%, respectively.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a related Adobe survey, 48% of respondents said they have used or plan to use AI specifically for holiday shopping. &lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This &lt;em&gt;stream&lt;/em&gt; is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/amazons-ai-chatbot-rufus-drove-sales-on-black-friday/</guid><pubDate>Mon, 01 Dec 2025 16:25:24 +0000</pubDate></item><item><title>The State of AI: Welcome to the economic singularity (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/01/1127872/the-state-of-ai-welcome-to-the-economic-singularity/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;strong&gt;Welcome back to The State of AI, a new collaboration between the &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Financial Times&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; and &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;. Every Monday for the next two weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This week, Richard Waters, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;FT&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; columnist and former West Coast editor, talks with &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;’s editor at large David Rotman about the true impact of AI on the job market.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Bonus: If you're an &lt;em&gt;MIT Technology Review &lt;/em&gt;subscriber, you can join David and Richard, alongside &lt;em&gt;MIT Technology Review&lt;/em&gt;’s editor in chief, Mat Honan, for an exclusive conversation live on Tuesday, December 9 at 1pm ET about this topic. Sign up to be a part here. &lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127510" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Richard Waters writes&lt;/strong&gt;: &lt;/p&gt; 
 &lt;p&gt;Any far-reaching new technology is always uneven in its adoption, but few have been more uneven than generative AI. That makes it hard to assess its likely impact on individual businesses, let alone on productivity across the economy as a whole.&lt;/p&gt;  &lt;p&gt;At one extreme, AI coding assistants have revolutionized the work of software developers. Mark Zuckerberg recently predicted that half of Meta’s code would be written by AI within a year. At the other extreme, most companies are seeing little if any benefit from their initial investments. A widely cited study from MIT found that so far, 95% of gen AI projects produce zero return.&lt;/p&gt; 
 &lt;p&gt;That has provided fuel for the skeptics who maintain that—by its very nature as a probabilistic technology prone to hallucinating—generative AI will never have a deep impact on business.&lt;/p&gt;  &lt;p&gt;To many students of tech history, though, the lack of immediate impact is just the normal lag associated with transformative new technologies. Erik Brynjolfsson, then an assistant professor at MIT, first described what he called the “productivity paradox of IT” in the early 1990s. Despite plenty of anecdotal evidence that technology was changing the way people worked, it wasn’t showing up in the aggregate data in the form of higher productivity growth. Brynjolfsson’s conclusion was that it just took time for businesses to adapt.&lt;/p&gt;  &lt;p&gt;Big investments in IT finally showed through with a notable rebound in US productivity growth starting in the mid-1990s. But that tailed off a decade later and was followed by a second lull.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Richard Waters and David Rotman" class="wp-image-1126831" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/FT_TR_Newsletter-Episode-05.jpg?w=2731" /&gt;&lt;div class="image-credit"&gt;FT/MIT TECHNOLOGY REVIEW | ADOBE STOCK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In the case of AI, companies need to build new infrastructure (particularly data platforms), redesign core business processes, and retrain workers before they can expect to see results. If a lag effect explains the slow results, there may at least be reasons for optimism: Much of the cloud computing infrastructure needed to bring generative AI to a wider business audience is already in place.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The opportunities and the challenges are both enormous. An executive at one Fortune 500 company says his organization has carried out a comprehensive review of its use of analytics and concluded that its workers, overall, add little or no value. Rooting out the old software and replacing that inefficient human labor with AI might yield significant results. But, as this person says, such an overhaul would require big changes to existing processes and take years to carry out.&lt;/p&gt;  &lt;p&gt;There are some early encouraging signs. US productivity growth, stuck at 1% to 1.5% for more than a decade and a half, rebounded to more than 2% last year. It probably hit the same level in the first nine months of this year, though the lack of official data due to the recent US government shutdown makes this impossible to confirm.&lt;/p&gt;  &lt;p&gt;It is impossible to tell, though, how durable this rebound will be or how much can be attributed to AI. The effects of new technologies are seldom felt in isolation. Instead, the benefits compound. AI is riding earlier investments in cloud and mobile computing. In the same way, the latest AI boom may only be the precursor to breakthroughs in fields that have a wider impact on the economy, such as robotics. ChatGPT might have caught the popular imagination, but OpenAI’s chatbot is unlikely to have the final word.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1128631" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/FT-State-of-AI-Ep5.png?w=794" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;David Rotman replies:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;This is my favorite discussion these days when it comes to artificial intelligence. How will AI affect overall economic productivity? Forget about the mesmerizing videos, the promise of companionship, and the prospect of agents to do tedious everyday tasks—the bottom line will be whether AI can grow the economy, and that means increasing productivity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But, as you say, it’s hard to pin down just how AI is affecting such growth or how it will do so in the future. Erik Brynjolfsson predicts that, like other so-called general purpose technologies, AI will follow a J curve in which initially there is a slow, even negative, effect on productivity as companies invest heavily in the technology before finally reaping the rewards. And then the boom.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there is a counterexample undermining the just-be-patient argument. Productivity growth from IT picked up in the mid-1990s but since the mid-2000s has been relatively dismal. Despite smartphones and social media and apps like Slack and Uber, digital technologies have done little to produce robust economic growth. A strong productivity boost never came.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Daron Acemoglu, an economist at MIT and a 2024 Nobel Prize winner, argues that the productivity gains from generative AI will be far smaller and take far longer than AI optimists think. The reason is that though the technology is impressive in many ways, the field is too narrowly focused on products that have little relevance to the largest business sectors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The statistic you cite that 95% of AI projects lack business benefits is telling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Take manufacturing. No question, some version of AI could help; imagine a worker on the factory floor snapping a picture of a problem and asking an AI agent for advice. The problem is that the big tech companies creating AI aren’t really interested in solving such mundane tasks, and their large foundation models, mostly trained on the internet, aren’t all that helpful.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s easy to blame the lack of productivity impact from AI so far on business practices and poorly trained workers. Your example of the executive of the Fortune 500 company sounds all too familiar. But it’s more useful to ask how AI can be trained and fine-tuned to give workers, like nurses and teachers and those on the factory floor, more capabilities and make them more productive at their jobs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The distinction matters. Some companies announcing large layoffs recently cited AI as the reason. The worry, however, is that it’s just a short-term cost-saving scheme. As economists like Brynjolfsson and Acemoglu agree, the productivity boost from AI will come when it’s used to create new types of jobs and augment the abilities of workers, not when it is used just to slash jobs to reduce costs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Richard Waters responds :&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I see we’re both feeling pretty cautious, David, so I’ll try to end on a positive note.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Some analyses assume that a much greater share of existing work is within the reach of today’s AI. McKinsey reckons 60% (versus 20% for Acemoglu) and puts annual productivity gains across the economy at as much as 3.4%. Also, calculations like these are based on automation of existing tasks; any new uses of AI that enhance existing jobs would, as you suggest, be a bonus (and not just in economic terms).&lt;/p&gt;  &lt;p&gt;Cost-cutting always seems to be the first order of business with any new technology. But we’re still in the early stages and AI is moving fast, so we can always hope&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;FT&lt;/em&gt; chief economics commentator Martin Wolf has been skeptical about whether tech investment boosts productivity but says AI might prove him wrong. The downside: Job losses and wealth concentration might lead to “techno-feudalism.”&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;'s Robert Armstrong argues that the boom in data center investment need not turn to bust. The biggest risk is that debt financing will come to play too big a role in the buildout.&lt;/p&gt;  &lt;p&gt;Last year, David Rotman wrote for &lt;em&gt;MIT Technology Review&lt;/em&gt; about how we can make sure AI works for us in boosting productivity, and what course corrections will be required.&lt;/p&gt;&lt;p&gt;David also wrote this piece about how we can best measure the impact of basic R&amp;amp;D funding on economic growth, and why it can often be bigger than you might think.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;strong&gt;Welcome back to The State of AI, a new collaboration between the &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;Financial Times&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; and &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;. Every Monday for the next two weeks, writers from both publications will debate one aspect of the generative AI revolution reshaping global power.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This week, Richard Waters, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;FT&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; columnist and former West Coast editor, talks with &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;’s editor at large David Rotman about the true impact of AI on the job market.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Bonus: If you're an &lt;em&gt;MIT Technology Review &lt;/em&gt;subscriber, you can join David and Richard, alongside &lt;em&gt;MIT Technology Review&lt;/em&gt;’s editor in chief, Mat Honan, for an exclusive conversation live on Tuesday, December 9 at 1pm ET about this topic. Sign up to be a part here. &lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1127510" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Richard Waters writes&lt;/strong&gt;: &lt;/p&gt; 
 &lt;p&gt;Any far-reaching new technology is always uneven in its adoption, but few have been more uneven than generative AI. That makes it hard to assess its likely impact on individual businesses, let alone on productivity across the economy as a whole.&lt;/p&gt;  &lt;p&gt;At one extreme, AI coding assistants have revolutionized the work of software developers. Mark Zuckerberg recently predicted that half of Meta’s code would be written by AI within a year. At the other extreme, most companies are seeing little if any benefit from their initial investments. A widely cited study from MIT found that so far, 95% of gen AI projects produce zero return.&lt;/p&gt; 
 &lt;p&gt;That has provided fuel for the skeptics who maintain that—by its very nature as a probabilistic technology prone to hallucinating—generative AI will never have a deep impact on business.&lt;/p&gt;  &lt;p&gt;To many students of tech history, though, the lack of immediate impact is just the normal lag associated with transformative new technologies. Erik Brynjolfsson, then an assistant professor at MIT, first described what he called the “productivity paradox of IT” in the early 1990s. Despite plenty of anecdotal evidence that technology was changing the way people worked, it wasn’t showing up in the aggregate data in the form of higher productivity growth. Brynjolfsson’s conclusion was that it just took time for businesses to adapt.&lt;/p&gt;  &lt;p&gt;Big investments in IT finally showed through with a notable rebound in US productivity growth starting in the mid-1990s. But that tailed off a decade later and was followed by a second lull.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Richard Waters and David Rotman" class="wp-image-1126831" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/FT_TR_Newsletter-Episode-05.jpg?w=2731" /&gt;&lt;div class="image-credit"&gt;FT/MIT TECHNOLOGY REVIEW | ADOBE STOCK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;In the case of AI, companies need to build new infrastructure (particularly data platforms), redesign core business processes, and retrain workers before they can expect to see results. If a lag effect explains the slow results, there may at least be reasons for optimism: Much of the cloud computing infrastructure needed to bring generative AI to a wider business audience is already in place.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;The opportunities and the challenges are both enormous. An executive at one Fortune 500 company says his organization has carried out a comprehensive review of its use of analytics and concluded that its workers, overall, add little or no value. Rooting out the old software and replacing that inefficient human labor with AI might yield significant results. But, as this person says, such an overhaul would require big changes to existing processes and take years to carry out.&lt;/p&gt;  &lt;p&gt;There are some early encouraging signs. US productivity growth, stuck at 1% to 1.5% for more than a decade and a half, rebounded to more than 2% last year. It probably hit the same level in the first nine months of this year, though the lack of official data due to the recent US government shutdown makes this impossible to confirm.&lt;/p&gt;  &lt;p&gt;It is impossible to tell, though, how durable this rebound will be or how much can be attributed to AI. The effects of new technologies are seldom felt in isolation. Instead, the benefits compound. AI is riding earlier investments in cloud and mobile computing. In the same way, the latest AI boom may only be the precursor to breakthroughs in fields that have a wider impact on the economy, such as robotics. ChatGPT might have caught the popular imagination, but OpenAI’s chatbot is unlikely to have the final word.&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1128631" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/FT-State-of-AI-Ep5.png?w=794" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;David Rotman replies:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;This is my favorite discussion these days when it comes to artificial intelligence. How will AI affect overall economic productivity? Forget about the mesmerizing videos, the promise of companionship, and the prospect of agents to do tedious everyday tasks—the bottom line will be whether AI can grow the economy, and that means increasing productivity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But, as you say, it’s hard to pin down just how AI is affecting such growth or how it will do so in the future. Erik Brynjolfsson predicts that, like other so-called general purpose technologies, AI will follow a J curve in which initially there is a slow, even negative, effect on productivity as companies invest heavily in the technology before finally reaping the rewards. And then the boom.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But there is a counterexample undermining the just-be-patient argument. Productivity growth from IT picked up in the mid-1990s but since the mid-2000s has been relatively dismal. Despite smartphones and social media and apps like Slack and Uber, digital technologies have done little to produce robust economic growth. A strong productivity boost never came.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Daron Acemoglu, an economist at MIT and a 2024 Nobel Prize winner, argues that the productivity gains from generative AI will be far smaller and take far longer than AI optimists think. The reason is that though the technology is impressive in many ways, the field is too narrowly focused on products that have little relevance to the largest business sectors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The statistic you cite that 95% of AI projects lack business benefits is telling.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Take manufacturing. No question, some version of AI could help; imagine a worker on the factory floor snapping a picture of a problem and asking an AI agent for advice. The problem is that the big tech companies creating AI aren’t really interested in solving such mundane tasks, and their large foundation models, mostly trained on the internet, aren’t all that helpful.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s easy to blame the lack of productivity impact from AI so far on business practices and poorly trained workers. Your example of the executive of the Fortune 500 company sounds all too familiar. But it’s more useful to ask how AI can be trained and fine-tuned to give workers, like nurses and teachers and those on the factory floor, more capabilities and make them more productive at their jobs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The distinction matters. Some companies announcing large layoffs recently cited AI as the reason. The worry, however, is that it’s just a short-term cost-saving scheme. As economists like Brynjolfsson and Acemoglu agree, the productivity boost from AI will come when it’s used to create new types of jobs and augment the abilities of workers, not when it is used just to slash jobs to reduce costs.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;Richard Waters responds :&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I see we’re both feeling pretty cautious, David, so I’ll try to end on a positive note.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Some analyses assume that a much greater share of existing work is within the reach of today’s AI. McKinsey reckons 60% (versus 20% for Acemoglu) and puts annual productivity gains across the economy at as much as 3.4%. Also, calculations like these are based on automation of existing tasks; any new uses of AI that enhance existing jobs would, as you suggest, be a bonus (and not just in economic terms).&lt;/p&gt;  &lt;p&gt;Cost-cutting always seems to be the first order of business with any new technology. But we’re still in the early stages and AI is moving fast, so we can always hope&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;FT&lt;/em&gt; chief economics commentator Martin Wolf has been skeptical about whether tech investment boosts productivity but says AI might prove him wrong. The downside: Job losses and wealth concentration might lead to “techno-feudalism.”&lt;/p&gt;  &lt;p&gt;The &lt;em&gt;FT&lt;/em&gt;'s Robert Armstrong argues that the boom in data center investment need not turn to bust. The biggest risk is that debt financing will come to play too big a role in the buildout.&lt;/p&gt;  &lt;p&gt;Last year, David Rotman wrote for &lt;em&gt;MIT Technology Review&lt;/em&gt; about how we can make sure AI works for us in boosting productivity, and what course corrections will be required.&lt;/p&gt;&lt;p&gt;David also wrote this piece about how we can best measure the impact of basic R&amp;amp;D funding on economic growth, and why it can often be bigger than you might think.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/01/1127872/the-state-of-ai-welcome-to-the-economic-singularity/</guid><pubDate>Mon, 01 Dec 2025 16:30:00 +0000</pubDate></item><item><title>Nvidia’s $2B Synopsys bet tightens its grip on the chip-design stack (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/nvidias-2b-synopsys-bet-tightens-its-grip-on-the-chip-design-stack/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2192215566.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is investing $2 billion into Synopsys, which makes software and components for designing semiconductor chips. The deal deepens their existing partnership at a time when analysts have started to scrutinize increasingly common circular AI-industry deals and warn of a potential bubble.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia said it bought Synopsys shares at $414.79 each as part of a multi-year partnership to integrate Nvidia’s AI hardware and computing capabilities into Synopsys’ electronic design automation (EDA) and simulation software. The deal will help Synopsys transition its platform from CPU-based computing to GPUs, a shift it hopes will speed up chip-design workflows, per a release.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal gave Synopsys’ stock a lift by signaling long-term growth — a boon after the company recently reported weakness in its IP segment due to U.S. export restrictions and issues at a major customer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, the investment strengthens its influence over Synopsys’ widely used EDA tools at a time when chip-design competition is starting to heat up. It also comes after major investors such as SoftBank and Peter Thiel have sold off their Nvidia positions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2192215566.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia is investing $2 billion into Synopsys, which makes software and components for designing semiconductor chips. The deal deepens their existing partnership at a time when analysts have started to scrutinize increasingly common circular AI-industry deals and warn of a potential bubble.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia said it bought Synopsys shares at $414.79 each as part of a multi-year partnership to integrate Nvidia’s AI hardware and computing capabilities into Synopsys’ electronic design automation (EDA) and simulation software. The deal will help Synopsys transition its platform from CPU-based computing to GPUs, a shift it hopes will speed up chip-design workflows, per a release.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deal gave Synopsys’ stock a lift by signaling long-term growth — a boon after the company recently reported weakness in its IP segment due to U.S. export restrictions and issues at a major customer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Nvidia, the investment strengthens its influence over Synopsys’ widely used EDA tools at a time when chip-design competition is starting to heat up. It also comes after major investors such as SoftBank and Peter Thiel have sold off their Nvidia positions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/nvidias-2b-synopsys-bet-tightens-its-grip-on-the-chip-design-stack/</guid><pubDate>Mon, 01 Dec 2025 16:32:50 +0000</pubDate></item><item><title>OpenAI’s investment into Thrive Holdings is its latest circular deal (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/openais-investment-into-thrive-holdings-is-its-latest-circular-deal/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is taking an ownership stake in Thrive Holdings, whose parent company is one of the AI giant’s major investors, Thrive Capital.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thrive Holdings operates like a private equity firm for AI, rolling up companies that it believes could benefit from the tech in sectors like accounting and IT services.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Neither company disclosed the terms of the deal, but it will involve OpenAI sending employees from its engineering, research, and product teams to work within Thrive’s companies to accelerate AI adoption and boost efficiency. If those companies succeed, OpenAI’s stake will grow, and it will get compensated for its services, according to a source familiar with the matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership follows a pattern of circular dealmaking for the $500 billion AI giant, which also recently took stakes in infrastructure partners like Advanced Micro Devices and CoreWeave. For instance, OpenAI invested $350 million of equity into CoreWeave, which used the funds to buy Nvidia chips. Those same chips provide compute to OpenAI, which increases CoreWeave’s revenue and in the end makes OpenAI’s stake more valuable.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Thrive deal has a different structure, but the result is still interdependence. Here is how it will work. OpenAI embeds its own workers to build products and implement systems in Thrive’s portfolio companies. OpenAI profits when those companies scale based on growth its work helped generate.&lt;/p&gt;&lt;p&gt;Thrive Holdings pushed back on the circular characterization. A spokesperson emphasized the deal is “responding to an unmet need in the market” rather than creating demand and pointed to organic customer interest from portfolio companies like accounting firm Crete, which has reportedly saved hundreds of hours of work with AI tools, and IT firm Shield that predated the formal partnership.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But for outside investors, the embedded nature of OpenAI’s involvement – and the overlapping ownership with Thrive Capital holding stakes on both sides – makes it difficult to assess whether success stems from genuine market traction or from advantages that may not scale without OpenAI’s direct support.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Analysts will be watching to see if Thrive-owned firms actually succeed in building long-term profitable businesses using OpenAI’s tech, or if the result is really just pumped-up valuations based on speculative market potential.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to provide more detail into the nature of the deal.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2197181602.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is taking an ownership stake in Thrive Holdings, whose parent company is one of the AI giant’s major investors, Thrive Capital.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thrive Holdings operates like a private equity firm for AI, rolling up companies that it believes could benefit from the tech in sectors like accounting and IT services.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Neither company disclosed the terms of the deal, but it will involve OpenAI sending employees from its engineering, research, and product teams to work within Thrive’s companies to accelerate AI adoption and boost efficiency. If those companies succeed, OpenAI’s stake will grow, and it will get compensated for its services, according to a source familiar with the matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership follows a pattern of circular dealmaking for the $500 billion AI giant, which also recently took stakes in infrastructure partners like Advanced Micro Devices and CoreWeave. For instance, OpenAI invested $350 million of equity into CoreWeave, which used the funds to buy Nvidia chips. Those same chips provide compute to OpenAI, which increases CoreWeave’s revenue and in the end makes OpenAI’s stake more valuable.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Thrive deal has a different structure, but the result is still interdependence. Here is how it will work. OpenAI embeds its own workers to build products and implement systems in Thrive’s portfolio companies. OpenAI profits when those companies scale based on growth its work helped generate.&lt;/p&gt;&lt;p&gt;Thrive Holdings pushed back on the circular characterization. A spokesperson emphasized the deal is “responding to an unmet need in the market” rather than creating demand and pointed to organic customer interest from portfolio companies like accounting firm Crete, which has reportedly saved hundreds of hours of work with AI tools, and IT firm Shield that predated the formal partnership.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But for outside investors, the embedded nature of OpenAI’s involvement – and the overlapping ownership with Thrive Capital holding stakes on both sides – makes it difficult to assess whether success stems from genuine market traction or from advantages that may not scale without OpenAI’s direct support.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Analysts will be watching to see if Thrive-owned firms actually succeed in building long-term profitable businesses using OpenAI’s tech, or if the result is really just pumped-up valuations based on speculative market potential.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to provide more detail into the nature of the deal.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/openais-investment-into-thrive-holdings-is-its-latest-circular-deal/</guid><pubDate>Mon, 01 Dec 2025 16:58:17 +0000</pubDate></item><item><title>At NeurIPS, NVIDIA Advances Open Model Development for Digital and Physical AI (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools — with potential applications in virtually every research field.&lt;/p&gt;
&lt;p&gt;At NeurIPS, one of the world’s top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the world’s first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more.&lt;/p&gt;
&lt;p&gt;These initiatives deepen NVIDIA’s commitment to open source — an effort recognized by a new Openness Index from Artificial Analysis, an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-87920" height="730" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Artificial-Analysis-Openness-Index-1680x730.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 (AR1), the world’s first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning — a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy.&lt;/p&gt;
&lt;p&gt;While previous iterations of self-driving models struggled with nuanced situations — a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane — reasoning gives autonomous vehicles the common sense to drive more like humans do.&lt;/p&gt;
&lt;p&gt;AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route.&lt;/p&gt;
&lt;p&gt;For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces — explanations on why it took certain actions — and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers.&lt;/p&gt;

&lt;p&gt;AR1’s open foundation, based on NVIDIA Cosmos Reason, lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications.&lt;/p&gt;
&lt;p&gt;For post-training AR1, reinforcement learning has proven especially effective — researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 will be available on GitHub and Hugging Face, and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets. NVIDIA has also released the open-source AlpaSim framework to evaluate AR1.&lt;/p&gt;
&lt;p&gt;Learn more about reasoning VLA models for autonomous driving.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Customizing NVIDIA Cosmos for Any Physical AI Use Case&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook. It’s a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation.&lt;/p&gt;
&lt;p&gt;There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;LidarGen&lt;/b&gt;, the first world model that can generate lidar data for AV simulation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Omniverse NuRec Fixer&lt;/b&gt;, a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Cosmos Policy&lt;/b&gt;, a framework for turning large pretrained video models into robust robot policies — a set of rules that dictate a robot’s behavior.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ProtoMotions3&lt;/b&gt;, an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs).&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87924"&gt;&lt;img alt="alt" class="size-large wp-image-87924" height="1020" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/LidarGen-1680x1020.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87924"&gt;Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87927"&gt;&lt;img alt="alt" class="size-full wp-image-87927" height="523" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/ProtoMotions.gif" width="800" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87927"&gt;Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs.&lt;/p&gt;
&lt;p&gt;AV developer&amp;nbsp;Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X, Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;MultiTalker Parakeet&lt;/b&gt;: An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Sortformer&lt;/b&gt;: A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream — a process called diarization — in real time.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Content Safety Reasoning&lt;/b&gt;: A reasoning-based AI safety model that dynamically enforces custom policies across domains.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Content Safety Audio&lt;/b&gt; &lt;b&gt;Dataset&lt;/b&gt;: A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NeMo Gym&lt;/b&gt;: an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeMo Data Designer Library&lt;/strong&gt;: Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow.&lt;/p&gt;
&lt;p&gt;NeurIPS attendees can explore these innovations at the Nemotron Summit, taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Research Furthers Language AI Innovation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Of the dozens of NVIDIA-authored research papers at NeurIPS, here are a few highlights advancing language models:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;View the full list of &lt;/i&gt;&lt;i&gt;events at NeurIPS&lt;/i&gt;&lt;i&gt;, running through Sunday, Dec. 7, in San Diego.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools — with potential applications in virtually every research field.&lt;/p&gt;
&lt;p&gt;At NeurIPS, one of the world’s top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the world’s first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety.&lt;/p&gt;
&lt;p&gt;NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more.&lt;/p&gt;
&lt;p&gt;These initiatives deepen NVIDIA’s commitment to open source — an effort recognized by a new Openness Index from Artificial Analysis, an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-large wp-image-87920" height="730" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Artificial-Analysis-Openness-Index-1680x730.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 (AR1), the world’s first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning — a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy.&lt;/p&gt;
&lt;p&gt;While previous iterations of self-driving models struggled with nuanced situations — a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane — reasoning gives autonomous vehicles the common sense to drive more like humans do.&lt;/p&gt;
&lt;p&gt;AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route.&lt;/p&gt;
&lt;p&gt;For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces — explanations on why it took certain actions — and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers.&lt;/p&gt;

&lt;p&gt;AR1’s open foundation, based on NVIDIA Cosmos Reason, lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications.&lt;/p&gt;
&lt;p&gt;For post-training AR1, reinforcement learning has proven especially effective — researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE Alpamayo-R1 will be available on GitHub and Hugging Face, and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets. NVIDIA has also released the open-source AlpaSim framework to evaluate AR1.&lt;/p&gt;
&lt;p&gt;Learn more about reasoning VLA models for autonomous driving.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Customizing NVIDIA Cosmos for Any Physical AI Use Case&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook. It’s a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation.&lt;/p&gt;
&lt;p&gt;There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;LidarGen&lt;/b&gt;, the first world model that can generate lidar data for AV simulation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Omniverse NuRec Fixer&lt;/b&gt;, a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Cosmos Policy&lt;/b&gt;, a framework for turning large pretrained video models into robust robot policies — a set of rules that dictate a robot’s behavior.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ProtoMotions3&lt;/b&gt;, an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs).&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87924"&gt;&lt;img alt="alt" class="size-large wp-image-87924" height="1020" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/LidarGen-1680x1020.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87924"&gt;Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87927"&gt;&lt;img alt="alt" class="size-full wp-image-87927" height="523" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/ProtoMotions.gif" width="800" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87927"&gt;Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs.&lt;/p&gt;
&lt;p&gt;AV developer&amp;nbsp;Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X, Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;MultiTalker Parakeet&lt;/b&gt;: An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Sortformer&lt;/b&gt;: A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream — a process called diarization — in real time.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Content Safety Reasoning&lt;/b&gt;: A reasoning-based AI safety model that dynamically enforces custom policies across domains.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Nemotron Content Safety Audio&lt;/b&gt; &lt;b&gt;Dataset&lt;/b&gt;: A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;NeMo Gym&lt;/b&gt;: an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeMo Data Designer Library&lt;/strong&gt;: Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow.&lt;/p&gt;
&lt;p&gt;NeurIPS attendees can explore these innovations at the Nemotron Summit, taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Research Furthers Language AI Innovation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Of the dozens of NVIDIA-authored research papers at NeurIPS, here are a few highlights advancing language models:&lt;/p&gt;

&lt;p&gt;&lt;i&gt;View the full list of &lt;/i&gt;&lt;i&gt;events at NeurIPS&lt;/i&gt;&lt;i&gt;, running through Sunday, Dec. 7, in San Diego.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</guid><pubDate>Mon, 01 Dec 2025 17:00:48 +0000</pubDate></item><item><title>MIT offshoot Liquid AI releases blueprint for enterprise-grade small-model training (AI | VentureBeat)</title><link>https://venturebeat.com/ai/mit-offshoot-liquid-ai-releases-blueprint-for-enterprise-grade-small-model</link><description>[unable to retrieve full-text content]&lt;p&gt;When Liquid AI, a startup f&lt;a href="https://aimmediahouse.com/market-industry/from-worm-brains-to-a-2-billion-ai-unicorn-liquid-ai-defies-conventional-ai-limits"&gt;ounded by MIT computer scientists back in 2023&lt;/a&gt;, introduced&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt; its Liquid Foundation Models series 2 (LFM2) in July 2025&lt;/a&gt;, the pitch was straightforward: deliver the fastest on-device foundation models on the market using the new &amp;quot;liquid&amp;quot; architecture, with training and inference efficiency that made small models a serious alternative to cloud-only large language models (LLMs) such as OpenAI&amp;#x27;s GPT series and Google&amp;#x27;s Gemini. &lt;/p&gt;&lt;p&gt;The initial release shipped dense checkpoints at 350M, 700M, and 1.2B parameters, a hybrid architecture heavily weighted toward gated short convolutions, and benchmark numbers that placed LFM2 ahead of similarly sized competitors like Qwen3, Llama 3.2, and Gemma 3 on both quality and CPU throughput. The message to enterprises was clear: real-time, privacy-preserving AI on phones, laptops, and vehicles no longer required sacrificing capability for latency.&lt;/p&gt;&lt;p&gt;In the months since that launch, Liquid has expanded LFM2 into a broader product line — adding&lt;a href="https://venturebeat.com/ai/what-if-weve-been-doing-agentic-ai-all-wrong-mit-offshoot-liquid-ai-offers"&gt; task-and-domain-specialized variants&lt;/a&gt;, a &lt;a href="https://venturebeat.com/ai/liquid-ai-wants-to-give-smartphones-small-fast-ai-that-can-see-with-new-lfm2-vl-model"&gt;small video ingestion and analysis model&lt;/a&gt;, and an &lt;a href="https://venturebeat.com/ai/finally-a-dev-kit-for-designing-on-device-mobile-ai-apps-is-here-liquid-ais-leap"&gt;edge-focused deployment stack called LEAP&lt;/a&gt;  — and positioned the models as the control layer for on-device and on-prem agentic systems. &lt;/p&gt;&lt;p&gt;Now, with &lt;a href="https://arxiv.org/abs/2511.23404"&gt;the publication of the detailed, 51-page LFM2 technical report on arXiv&lt;/a&gt;, the company is going a step further: making public the architecture search process, training data mixture, distillation objective, curriculum strategy, and post-training pipeline behind those models. &lt;/p&gt;&lt;p&gt;And unlike earlier open models, LFM2 is built around a repeatable recipe: a hardware-in-the-loop search process, a training curriculum that compensates for smaller parameter budgets, and a post-training pipeline tuned for instruction following and tool use. &lt;/p&gt;&lt;p&gt;Rather than just offering weights and an API, Liquid is effectively publishing a detailed blueprint that other organizations can use as a reference for training their own small, efficient models from scratch, tuned to their own hardware and deployment constraints.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A model family designed around real constraints, not GPU labs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The technical report begins with a premise enterprises are intimately familiar with: real AI systems hit limits long before benchmarks do. Latency budgets, peak memory ceilings, and thermal throttling define what can actually run in production—especially on laptops, tablets, commodity servers, and mobile devices.&lt;/p&gt;&lt;p&gt;To address this, Liquid AI performed architecture search directly on target hardware, including Snapdragon mobile SoCs and Ryzen laptop CPUs. The result is a consistent outcome across sizes: a minimal hybrid architecture dominated by &lt;b&gt;gated short convolution blocks&lt;/b&gt; and a small number of &lt;b&gt;grouped-query attention (GQA)&lt;/b&gt; layers. This design was repeatedly selected over more exotic linear-attention and SSM hybrids because it delivered a better quality-latency-memory Pareto profile under real device conditions.&lt;/p&gt;&lt;p&gt;This matters for enterprise teams in three ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Predictability.&lt;/b&gt; The architecture is simple, parameter-efficient, and stable across model sizes from 350M to 2.6B.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Operational portability.&lt;/b&gt; Dense and MoE variants share the same structural backbone, simplifying deployment across mixed hardware fleets.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;On-device feasibility.&lt;/b&gt; Prefill and decode throughput on CPUs surpass comparable open models by roughly 2× in many cases, reducing the need to offload routine tasks to cloud inference endpoints.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Instead of optimizing for academic novelty, the report reads as a systematic attempt to design models enterprises can &lt;i&gt;actually ship.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;This is notable and more practical for enterprises in a field where many open models quietly assume access to multi-H100 clusters during inference.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A training pipeline tuned for enterprise-relevant behavior&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2 adopts a training approach that compensates for the smaller scale of its models with structure rather than brute force. Key elements include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;10–12T token pre-training&lt;/b&gt; and an additional &lt;b&gt;32K-context mid-training phase&lt;/b&gt;, which extends the model’s useful context window without exploding compute costs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;decoupled Top-K knowledge distillation objective&lt;/b&gt; that sidesteps the instability of standard KL distillation when teachers provide only partial logits.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;three-stage post-training sequence&lt;/b&gt;—SFT, length-normalized preference alignment, and model merging—designed to produce more reliable instruction following and tool-use behavior.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For enterprise AI developers, the significance is that LFM2 models behave less like “tiny LLMs” and more like practical agents able to follow structured formats, adhere to JSON schemas, and manage multi-turn chat flows. Many open models at similar sizes fail not due to lack of reasoning ability, but due to brittle adherence to instruction templates. The LFM2 post-training recipe directly targets these rough edges.&lt;/p&gt;&lt;p&gt;In other words: Liquid AI optimized small models for &lt;i&gt;operational reliability&lt;/i&gt;, not just scoreboards.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multimodality designed for device constraints, not lab demos&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The LFM2-VL and LFM2-Audio variants reflect another shift: multimodality built around &lt;b&gt;token efficiency&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Rather than embedding a massive vision transformer directly into an LLM, LFM2-VL attaches a SigLIP2 encoder through a connector that aggressively reduces visual token count via PixelUnshuffle. High-resolution inputs automatically trigger dynamic tiling, keeping token budgets controllable even on mobile hardware. LFM2-Audio uses a bifurcated audio path—one for embeddings, one for generation—supporting real-time transcription or speech-to-speech on modest CPUs.&lt;/p&gt;&lt;p&gt;For enterprise platform architects, this design points toward a practical future where:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;document understanding happens directly on endpoints such as field devices;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;audio transcription and speech agents run locally for privacy compliance;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;multimodal agents operate within fixed latency envelopes without streaming data off-device.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The through-line is the same: multimodal capability without requiring a GPU farm.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Retrieval models built for agent systems, not legacy search&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2-ColBERT extends late-interaction retrieval into a footprint small enough for enterprise deployments that need multilingual RAG without the overhead of specialized vector DB accelerators.&lt;/p&gt;&lt;p&gt;This is particularly meaningful as organizations begin to orchestrate fleets of agents. Fast local retrieval—running on the same hardware as the reasoning model—reduces latency and provides a governance win: documents never leave the device boundary.&lt;/p&gt;&lt;p&gt;Taken together, the VL, Audio, and ColBERT variants show LFM2 as a modular system, not a single model drop.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The emerging blueprint for hybrid enterprise AI architectures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Across all variants, the LFM2 report implicitly sketches what tomorrow’s enterprise AI stack will look like: &lt;b&gt;hybrid local-cloud orchestration&lt;/b&gt;, where small, fast models operating on devices handle time-critical perception, formatting, tool invocation, and judgment tasks, while larger models in the cloud offer heavyweight reasoning when needed.&lt;/p&gt;&lt;p&gt;Several trends converge here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Cost control.&lt;/b&gt; Running routine inference locally avoids unpredictable cloud billing.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Latency determinism.&lt;/b&gt; TTFT and decode stability matter in agent workflows; on-device eliminates network jitter.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Governance and compliance.&lt;/b&gt; Local execution simplifies PII handling, data residency, and auditability.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Resilience.&lt;/b&gt; Agentic systems degrade gracefully if the cloud path becomes unavailable.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Enterprises adopting these architectures will likely treat small on-device models as the “control plane” of agentic workflows, with large cloud models serving as on-demand accelerators.&lt;/p&gt;&lt;p&gt;LFM2 is one of the clearest open-source foundations for that control layer to date.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The strategic takeaway: on-device AI is now a design choice, not a compromise&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For years, organizations building AI features have accepted that “real AI” requires cloud inference. LFM2 challenges that assumption. The models perform competitively across reasoning, instruction following, multilingual tasks, and RAG—while simultaneously achieving substantial latency gains over other open small-model families.&lt;/p&gt;&lt;p&gt;For CIOs and CTOs finalizing 2026 roadmaps, the implication is direct: &lt;b&gt;small, open, on-device models are now strong enough to carry meaningful slices of production workloads.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;LFM2 will not replace frontier cloud models for frontier-scale reasoning. But it offers something enterprises arguably need more: a reproducible, open, and operationally feasible foundation for &lt;b&gt;agentic systems that must run anywhere&lt;/b&gt;, from phones to industrial endpoints to air-gapped secure facilities.&lt;/p&gt;&lt;p&gt;In the broadening landscape of enterprise AI, LFM2 is less a research milestone and more a sign of architectural convergence. The future is not cloud or edge—it’s both, operating in concert. And releases like LFM2 provide the building blocks for organizations prepared to build that hybrid future intentionally rather than accidentally.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;When Liquid AI, a startup f&lt;a href="https://aimmediahouse.com/market-industry/from-worm-brains-to-a-2-billion-ai-unicorn-liquid-ai-defies-conventional-ai-limits"&gt;ounded by MIT computer scientists back in 2023&lt;/a&gt;, introduced&lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt; its Liquid Foundation Models series 2 (LFM2) in July 2025&lt;/a&gt;, the pitch was straightforward: deliver the fastest on-device foundation models on the market using the new &amp;quot;liquid&amp;quot; architecture, with training and inference efficiency that made small models a serious alternative to cloud-only large language models (LLMs) such as OpenAI&amp;#x27;s GPT series and Google&amp;#x27;s Gemini. &lt;/p&gt;&lt;p&gt;The initial release shipped dense checkpoints at 350M, 700M, and 1.2B parameters, a hybrid architecture heavily weighted toward gated short convolutions, and benchmark numbers that placed LFM2 ahead of similarly sized competitors like Qwen3, Llama 3.2, and Gemma 3 on both quality and CPU throughput. The message to enterprises was clear: real-time, privacy-preserving AI on phones, laptops, and vehicles no longer required sacrificing capability for latency.&lt;/p&gt;&lt;p&gt;In the months since that launch, Liquid has expanded LFM2 into a broader product line — adding&lt;a href="https://venturebeat.com/ai/what-if-weve-been-doing-agentic-ai-all-wrong-mit-offshoot-liquid-ai-offers"&gt; task-and-domain-specialized variants&lt;/a&gt;, a &lt;a href="https://venturebeat.com/ai/liquid-ai-wants-to-give-smartphones-small-fast-ai-that-can-see-with-new-lfm2-vl-model"&gt;small video ingestion and analysis model&lt;/a&gt;, and an &lt;a href="https://venturebeat.com/ai/finally-a-dev-kit-for-designing-on-device-mobile-ai-apps-is-here-liquid-ais-leap"&gt;edge-focused deployment stack called LEAP&lt;/a&gt;  — and positioned the models as the control layer for on-device and on-prem agentic systems. &lt;/p&gt;&lt;p&gt;Now, with &lt;a href="https://arxiv.org/abs/2511.23404"&gt;the publication of the detailed, 51-page LFM2 technical report on arXiv&lt;/a&gt;, the company is going a step further: making public the architecture search process, training data mixture, distillation objective, curriculum strategy, and post-training pipeline behind those models. &lt;/p&gt;&lt;p&gt;And unlike earlier open models, LFM2 is built around a repeatable recipe: a hardware-in-the-loop search process, a training curriculum that compensates for smaller parameter budgets, and a post-training pipeline tuned for instruction following and tool use. &lt;/p&gt;&lt;p&gt;Rather than just offering weights and an API, Liquid is effectively publishing a detailed blueprint that other organizations can use as a reference for training their own small, efficient models from scratch, tuned to their own hardware and deployment constraints.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A model family designed around real constraints, not GPU labs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The technical report begins with a premise enterprises are intimately familiar with: real AI systems hit limits long before benchmarks do. Latency budgets, peak memory ceilings, and thermal throttling define what can actually run in production—especially on laptops, tablets, commodity servers, and mobile devices.&lt;/p&gt;&lt;p&gt;To address this, Liquid AI performed architecture search directly on target hardware, including Snapdragon mobile SoCs and Ryzen laptop CPUs. The result is a consistent outcome across sizes: a minimal hybrid architecture dominated by &lt;b&gt;gated short convolution blocks&lt;/b&gt; and a small number of &lt;b&gt;grouped-query attention (GQA)&lt;/b&gt; layers. This design was repeatedly selected over more exotic linear-attention and SSM hybrids because it delivered a better quality-latency-memory Pareto profile under real device conditions.&lt;/p&gt;&lt;p&gt;This matters for enterprise teams in three ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Predictability.&lt;/b&gt; The architecture is simple, parameter-efficient, and stable across model sizes from 350M to 2.6B.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Operational portability.&lt;/b&gt; Dense and MoE variants share the same structural backbone, simplifying deployment across mixed hardware fleets.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;On-device feasibility.&lt;/b&gt; Prefill and decode throughput on CPUs surpass comparable open models by roughly 2× in many cases, reducing the need to offload routine tasks to cloud inference endpoints.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Instead of optimizing for academic novelty, the report reads as a systematic attempt to design models enterprises can &lt;i&gt;actually ship.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;This is notable and more practical for enterprises in a field where many open models quietly assume access to multi-H100 clusters during inference.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;A training pipeline tuned for enterprise-relevant behavior&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2 adopts a training approach that compensates for the smaller scale of its models with structure rather than brute force. Key elements include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;10–12T token pre-training&lt;/b&gt; and an additional &lt;b&gt;32K-context mid-training phase&lt;/b&gt;, which extends the model’s useful context window without exploding compute costs.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;decoupled Top-K knowledge distillation objective&lt;/b&gt; that sidesteps the instability of standard KL distillation when teachers provide only partial logits.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;A &lt;b&gt;three-stage post-training sequence&lt;/b&gt;—SFT, length-normalized preference alignment, and model merging—designed to produce more reliable instruction following and tool-use behavior.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;For enterprise AI developers, the significance is that LFM2 models behave less like “tiny LLMs” and more like practical agents able to follow structured formats, adhere to JSON schemas, and manage multi-turn chat flows. Many open models at similar sizes fail not due to lack of reasoning ability, but due to brittle adherence to instruction templates. The LFM2 post-training recipe directly targets these rough edges.&lt;/p&gt;&lt;p&gt;In other words: Liquid AI optimized small models for &lt;i&gt;operational reliability&lt;/i&gt;, not just scoreboards.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Multimodality designed for device constraints, not lab demos&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The LFM2-VL and LFM2-Audio variants reflect another shift: multimodality built around &lt;b&gt;token efficiency&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Rather than embedding a massive vision transformer directly into an LLM, LFM2-VL attaches a SigLIP2 encoder through a connector that aggressively reduces visual token count via PixelUnshuffle. High-resolution inputs automatically trigger dynamic tiling, keeping token budgets controllable even on mobile hardware. LFM2-Audio uses a bifurcated audio path—one for embeddings, one for generation—supporting real-time transcription or speech-to-speech on modest CPUs.&lt;/p&gt;&lt;p&gt;For enterprise platform architects, this design points toward a practical future where:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;document understanding happens directly on endpoints such as field devices;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;audio transcription and speech agents run locally for privacy compliance;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;multimodal agents operate within fixed latency envelopes without streaming data off-device.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The through-line is the same: multimodal capability without requiring a GPU farm.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Retrieval models built for agent systems, not legacy search&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;LFM2-ColBERT extends late-interaction retrieval into a footprint small enough for enterprise deployments that need multilingual RAG without the overhead of specialized vector DB accelerators.&lt;/p&gt;&lt;p&gt;This is particularly meaningful as organizations begin to orchestrate fleets of agents. Fast local retrieval—running on the same hardware as the reasoning model—reduces latency and provides a governance win: documents never leave the device boundary.&lt;/p&gt;&lt;p&gt;Taken together, the VL, Audio, and ColBERT variants show LFM2 as a modular system, not a single model drop.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The emerging blueprint for hybrid enterprise AI architectures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Across all variants, the LFM2 report implicitly sketches what tomorrow’s enterprise AI stack will look like: &lt;b&gt;hybrid local-cloud orchestration&lt;/b&gt;, where small, fast models operating on devices handle time-critical perception, formatting, tool invocation, and judgment tasks, while larger models in the cloud offer heavyweight reasoning when needed.&lt;/p&gt;&lt;p&gt;Several trends converge here:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Cost control.&lt;/b&gt; Running routine inference locally avoids unpredictable cloud billing.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Latency determinism.&lt;/b&gt; TTFT and decode stability matter in agent workflows; on-device eliminates network jitter.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Governance and compliance.&lt;/b&gt; Local execution simplifies PII handling, data residency, and auditability.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Resilience.&lt;/b&gt; Agentic systems degrade gracefully if the cloud path becomes unavailable.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Enterprises adopting these architectures will likely treat small on-device models as the “control plane” of agentic workflows, with large cloud models serving as on-demand accelerators.&lt;/p&gt;&lt;p&gt;LFM2 is one of the clearest open-source foundations for that control layer to date.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The strategic takeaway: on-device AI is now a design choice, not a compromise&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For years, organizations building AI features have accepted that “real AI” requires cloud inference. LFM2 challenges that assumption. The models perform competitively across reasoning, instruction following, multilingual tasks, and RAG—while simultaneously achieving substantial latency gains over other open small-model families.&lt;/p&gt;&lt;p&gt;For CIOs and CTOs finalizing 2026 roadmaps, the implication is direct: &lt;b&gt;small, open, on-device models are now strong enough to carry meaningful slices of production workloads.&lt;/b&gt;&lt;/p&gt;&lt;p&gt;LFM2 will not replace frontier cloud models for frontier-scale reasoning. But it offers something enterprises arguably need more: a reproducible, open, and operationally feasible foundation for &lt;b&gt;agentic systems that must run anywhere&lt;/b&gt;, from phones to industrial endpoints to air-gapped secure facilities.&lt;/p&gt;&lt;p&gt;In the broadening landscape of enterprise AI, LFM2 is less a research milestone and more a sign of architectural convergence. The future is not cloud or edge—it’s both, operating in concert. And releases like LFM2 provide the building blocks for organizations prepared to build that hybrid future intentionally rather than accidentally.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/mit-offshoot-liquid-ai-releases-blueprint-for-enterprise-grade-small-model</guid><pubDate>Mon, 01 Dec 2025 17:24:00 +0000</pubDate></item><item><title>[NEW] Exploring how AI will shape the future of work (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/benjamin-manning-how-ai-will-shape-future-work-1201</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-Ben-Manning-profile.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;“MIT hasn’t just prepared me for the future of work — it’s pushed me to study it. As AI systems become more capable, more of our online activity will be carried out by artificial agents. That raises big questions: How should we design these systems to understand our preferences? What happens when AI begins making many of our decisions?”&lt;/p&gt;&lt;p dir="ltr"&gt;These are some of the questions MIT Sloan School of Management PhD candidate Benjamin Manning is researching. Part of his work investigates how to design and evaluate artificial intelligence agents that act on behalf of people, and how their behavior shapes markets and institutions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Previously, he received a master’s degree in public policy from the Harvard Kennedy School and a bachelor’s in mathematics from Washington University in St. Louis. After working as a research assistant, Manning knew he wanted to pursue an academic career.&lt;/p&gt;&lt;p dir="ltr"&gt;“There’s no better place in the world to study economics and computer science than MIT,” he says. “Nobel and Turing award winners are everywhere, and the IT group lets me explore both fields freely. It was my top choice — when I was accepted, the decision was clear.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;After receiving his PhD, Manning hopes to secure a faculty position at a business school and do the same type of work that MIT Sloan professors — his mentors — do every day.&lt;/p&gt;&lt;p dir="ltr"&gt;“Even in my fourth year, it still feels surreal to be an MIT student. I don’t think that feeling will ever fade. My mom definitely won’t ever get over telling people about it.”&lt;/p&gt;&lt;p dir="ltr"&gt;Of his MIT Sloan experience, Manning says he didn’t know it was possible to learn so much so quickly. “It’s no exaggeration to say I learned more in my first year as a PhD candidate than in all four years of undergrad. While the pace can be intense, wrestling with so many new ideas has been incredibly rewarding. It’s given me the tools to do novel research in economics and AI — something I never imagined I’d be capable of.”&lt;/p&gt;&lt;p dir="ltr"&gt;As an economist studying AI simulations of humans, for Manning, the future of work not only means understanding how AI acts on our behalf, but also radically improving and accelerating social scientific discovery.&lt;/p&gt;&lt;p dir="ltr"&gt;“Another part of my research agenda explores how well AI systems can simulate human responses. I envision a future where researchers test millions of behavioral simulations in minutes, rapidly prototyping experimental designs, and identifying promising research directions before investing in costly human studies. This isn’t about replacing human insight, but amplifying it: Scientists can focus on asking better questions, developing theory, and interpreting results while AI handles the computational heavy lifting.”&lt;/p&gt;&lt;p dir="ltr"&gt;He’s excited by the prospect: “We are possibly moving toward a world where the pace of understanding may get much closer to the speed of economic change.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-Ben-Manning-profile.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;“MIT hasn’t just prepared me for the future of work — it’s pushed me to study it. As AI systems become more capable, more of our online activity will be carried out by artificial agents. That raises big questions: How should we design these systems to understand our preferences? What happens when AI begins making many of our decisions?”&lt;/p&gt;&lt;p dir="ltr"&gt;These are some of the questions MIT Sloan School of Management PhD candidate Benjamin Manning is researching. Part of his work investigates how to design and evaluate artificial intelligence agents that act on behalf of people, and how their behavior shapes markets and institutions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Previously, he received a master’s degree in public policy from the Harvard Kennedy School and a bachelor’s in mathematics from Washington University in St. Louis. After working as a research assistant, Manning knew he wanted to pursue an academic career.&lt;/p&gt;&lt;p dir="ltr"&gt;“There’s no better place in the world to study economics and computer science than MIT,” he says. “Nobel and Turing award winners are everywhere, and the IT group lets me explore both fields freely. It was my top choice — when I was accepted, the decision was clear.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;After receiving his PhD, Manning hopes to secure a faculty position at a business school and do the same type of work that MIT Sloan professors — his mentors — do every day.&lt;/p&gt;&lt;p dir="ltr"&gt;“Even in my fourth year, it still feels surreal to be an MIT student. I don’t think that feeling will ever fade. My mom definitely won’t ever get over telling people about it.”&lt;/p&gt;&lt;p dir="ltr"&gt;Of his MIT Sloan experience, Manning says he didn’t know it was possible to learn so much so quickly. “It’s no exaggeration to say I learned more in my first year as a PhD candidate than in all four years of undergrad. While the pace can be intense, wrestling with so many new ideas has been incredibly rewarding. It’s given me the tools to do novel research in economics and AI — something I never imagined I’d be capable of.”&lt;/p&gt;&lt;p dir="ltr"&gt;As an economist studying AI simulations of humans, for Manning, the future of work not only means understanding how AI acts on our behalf, but also radically improving and accelerating social scientific discovery.&lt;/p&gt;&lt;p dir="ltr"&gt;“Another part of my research agenda explores how well AI systems can simulate human responses. I envision a future where researchers test millions of behavioral simulations in minutes, rapidly prototyping experimental designs, and identifying promising research directions before investing in costly human studies. This isn’t about replacing human insight, but amplifying it: Scientists can focus on asking better questions, developing theory, and interpreting results while AI handles the computational heavy lifting.”&lt;/p&gt;&lt;p dir="ltr"&gt;He’s excited by the prospect: “We are possibly moving toward a world where the pace of understanding may get much closer to the speed of economic change.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/benjamin-manning-how-ai-will-shape-future-work-1201</guid><pubDate>Mon, 01 Dec 2025 18:35:00 +0000</pubDate></item><item><title>[NEW] DeepSeek just dropped two insanely powerful AI models that rival GPT-5 and they're totally free (AI | VentureBeat)</title><link>https://venturebeat.com/ai/deepseek-just-dropped-two-insanely-powerful-ai-models-that-rival-gpt-5-and</link><description>[unable to retrieve full-text content]&lt;p&gt;Chinese artificial intelligence startup &lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; released two powerful new AI models on Sunday that the company claims match or exceed the capabilities of OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; and Google&amp;#x27;s &lt;a href="https://blog.google/products/gemini/gemini-3/"&gt;&lt;u&gt;Gemini-3.0-Pro&lt;/u&gt;&lt;/a&gt; — a development that could reshape the competitive landscape between American tech giants and their Chinese challengers.&lt;/p&gt;&lt;p&gt;The Hangzhou-based company launched &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt;, designed as an everyday reasoning assistant, alongside DeepSeek-V3.2-Speciale, a high-powered variant that achieved gold-medal performance in four elite international competitions: the 2025 International Mathematical Olympiad, the International Olympiad in Informatics, the ICPC World Finals, and the China Mathematical Olympiad.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The release carries profound implications for American technology leadership. DeepSeek has once again demonstrated that it can produce frontier AI systems despite U.S. export controls that &lt;a href="https://www.reuters.com/world/china/trump-says-nvidias-blackwell-ai-chip-not-other-people-2025-11-03/"&gt;&lt;u&gt;restrict China&amp;#x27;s access to advanced Nvidia chips&lt;/u&gt;&lt;/a&gt; — and it has done so while making its models freely available under an open-source MIT license.&lt;/p&gt;&lt;p&gt;&amp;quot;People thought DeepSeek gave a one-time breakthrough but we came back much bigger,&amp;quot; wrote &lt;a href="https://x.com/ChenHuiOG"&gt;&lt;u&gt;Chen Fang&lt;/u&gt;&lt;/a&gt;, who identified himself as a contributor to the project, on X (formerly Twitter). The release drew swift reactions online, with one user declaring: &amp;quot;&lt;a href="https://x.com/iampritamj/status/1995511358142701612"&gt;&lt;u&gt;Rest in peace, ChatGPT&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How DeepSeek&amp;#x27;s sparse attention breakthrough slashes computing costs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the heart of the new release lies &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek Sparse Attention&lt;/u&gt;&lt;/a&gt;, or DSA — a novel architectural innovation that dramatically reduces the computational burden of running AI models on long documents and complex tasks.&lt;/p&gt;&lt;p&gt;Traditional AI attention mechanisms, the core technology allowing language models to understand context, scale poorly as input length increases. Processing a document twice as long typically requires four times the computation. DeepSeek&amp;#x27;s approach breaks this constraint using what the company calls a &amp;quot;lightning indexer&amp;quot; that identifies only the most relevant portions of context for each query, ignoring the rest.&lt;/p&gt;&lt;p&gt;According to &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;DeepSeek&amp;#x27;s technical report&lt;/u&gt;&lt;/a&gt;, DSA reduces inference costs by roughly half compared to previous models when processing long sequences. The architecture &amp;quot;substantially reduces computational complexity while preserving model performance,&amp;quot; the report states.&lt;/p&gt;&lt;p&gt;Processing 128,000 tokens — roughly equivalent to a 300-page book — now costs approximately $0.70 per million tokens for decoding, compared to $2.40 for the previous &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;&lt;u&gt;V3.1-Terminus model&lt;/u&gt;&lt;/a&gt;. That represents a 70% reduction in inference costs.&lt;/p&gt;&lt;p&gt;The 685-billion-parameter models support context windows of 128,000 tokens, making them suitable for analyzing lengthy documents, codebases, and research papers. DeepSeek&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; notes that independent evaluations on long-context benchmarks show V3.2 performing on par with or better than its predecessor &amp;quot;despite incorporating a sparse attention mechanism.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The benchmark results that put DeepSeek in the same league as GPT-5&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s claims of parity with America&amp;#x27;s leading AI systems rest on extensive testing across mathematics, coding, and reasoning tasks — and the numbers are striking.&lt;/p&gt;&lt;p&gt;On &lt;a href="https://maa.org/aime-thresholds-are-available/"&gt;&lt;u&gt;AIME 2025&lt;/u&gt;&lt;/a&gt;, a prestigious American mathematics competition, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; achieved a 96.0% pass rate, compared to 94.6% for GPT-5-High and 95.0% for Gemini-3.0-Pro. On the &lt;a href="https://www.hmmt.org/"&gt;&lt;u&gt;Harvard-MIT Mathematics Tournament&lt;/u&gt;&lt;/a&gt;, the Speciale variant scored 99.2%, surpassing Gemini&amp;#x27;s 97.5%.&lt;/p&gt;&lt;p&gt;The standard &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2 model&lt;/u&gt;&lt;/a&gt;, optimized for everyday use, scored 93.1% on AIME and 92.5% on HMMT — marginally below frontier models but achieved with substantially fewer computational resources.&lt;/p&gt;&lt;p&gt;Most striking are the competition results. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; scored 35 out of 42 points on the &lt;a href="https://www.imo-official.org/year_info.aspx?year=2025"&gt;&lt;u&gt;2025 International Mathematical Olympiad&lt;/u&gt;&lt;/a&gt;, earning gold-medal status. At the &lt;a href="https://ioinformatics.org/"&gt;&lt;u&gt;International Olympiad in Informatics&lt;/u&gt;&lt;/a&gt;, it scored 492 out of 600 points — also gold, ranking 10th overall. The model solved 10 of 12 problems at the &lt;a href="https://worldfinals.icpc.global/2025/"&gt;&lt;u&gt;ICPC World Finals&lt;/u&gt;&lt;/a&gt;, placing second.&lt;/p&gt;&lt;p&gt;These results came without internet access or tools during testing. DeepSeek&amp;#x27;s report states that &amp;quot;testing strictly adheres to the contest&amp;#x27;s time and attempt limits.&amp;quot;&lt;/p&gt;&lt;p&gt;On coding benchmarks, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; resolved 73.1% of real-world software bugs on &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-Verified&lt;/u&gt;&lt;/a&gt;, competitive with GPT-5-High at 74.9%. On &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal Bench 2.0&lt;/u&gt;&lt;/a&gt;, measuring complex coding workflows, DeepSeek scored 46.4%—well above GPT-5-High&amp;#x27;s 35.2%.&lt;/p&gt;&lt;p&gt;The company acknowledges limitations. &amp;quot;Token efficiency remains a challenge,&amp;quot; the technical report states, noting that DeepSeek &amp;quot;typically requires longer generation trajectories&amp;quot; to match Gemini-3.0-Pro&amp;#x27;s output quality.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why teaching AI to think while using tools changes everything&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond raw reasoning, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; introduces &amp;quot;thinking in tool-use&amp;quot; — the ability to reason through problems while simultaneously executing code, searching the web, and manipulating files.&lt;/p&gt;&lt;p&gt;Previous AI models faced a frustrating limitation: each time they called an external tool, they lost their train of thought and had to restart reasoning from scratch. DeepSeek&amp;#x27;s architecture preserves the reasoning trace across multiple tool calls, enabling fluid multi-step problem solving.&lt;/p&gt;&lt;p&gt;To train this capability, the company built a massive synthetic data pipeline generating over 1,800 distinct task environments and 85,000 complex instructions. These included challenges like multi-day trip planning with budget constraints, software bug fixes across eight programming languages, and web-based research requiring dozens of searches.&lt;/p&gt;&lt;p&gt;The technical report describes one example: planning a three-day trip from Hangzhou with constraints on hotel prices, restaurant ratings, and attraction costs that vary based on accommodation choices. Such tasks are &amp;quot;hard to solve but easy to verify,&amp;quot; making them ideal for training AI agents.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; employed real-world tools during training — actual web search APIs, coding environments, and Jupyter notebooks — while generating synthetic prompts to ensure diversity. The result is a model that generalizes to unseen tools and environments, a critical capability for real-world deployment.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DeepSeek&amp;#x27;s open-source gambit could upend the AI industry&amp;#x27;s business model&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike OpenAI and Anthropic, which guard their most powerful models as proprietary assets, DeepSeek has released both &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;V3.2-Speciale&lt;/u&gt;&lt;/a&gt; under the MIT license — one of the most permissive open-source frameworks available.&lt;/p&gt;&lt;p&gt;Any developer, researcher, or company can download, modify, and deploy the 685-billion-parameter models without restriction. Full model weights, training code, and documentation are &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;u&gt;available on Hugging Face&lt;/u&gt;&lt;/a&gt;, the leading platform for AI model sharing.&lt;/p&gt;&lt;p&gt;The strategic implications are significant. By making frontier-capable models freely available, DeepSeek undermines competitors charging premium API prices. The Hugging Face model card notes that DeepSeek has provided Python scripts and test cases &amp;quot;demonstrating how to encode messages in OpenAI-compatible format&amp;quot; — making migration from competing services straightforward.&lt;/p&gt;&lt;p&gt;For enterprise customers, the value proposition is compelling: frontier performance at dramatically lower cost, with deployment flexibility. But data residency concerns and regulatory uncertainty may limit adoption in sensitive applications — particularly given DeepSeek&amp;#x27;s Chinese origins.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Regulatory walls are rising against DeepSeek in Europe and America&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s global expansion faces mounting resistance. In June, Berlin&amp;#x27;s data protection commissioner Meike Kamp declared that DeepSeek&amp;#x27;s transfer of German user data to China is &amp;quot;&lt;a href="https://www.cnbc.com/2025/06/27/germany-tells-apple-google-to-block-deepseek-ai-app.html#:~:text=Berlin&amp;#x27;s%20data%20protection%20commissioner%20Meike,under%20EU%20data%20protection%20rules."&gt;&lt;u&gt;unlawful&lt;/u&gt;&lt;/a&gt;&amp;quot; under EU rules, asking Apple and Google to consider blocking the app.&lt;/p&gt;&lt;p&gt;The German authority expressed concern that &amp;quot;Chinese authorities have extensive access rights to personal data within the sphere of influence of Chinese companies.&amp;quot; Italy ordered DeepSeek to &lt;a href="https://www.reuters.com/technology/artificial-intelligence/italys-privacy-watchdog-blocks-chinese-ai-app-deepseek-2025-01-30/"&gt;&lt;u&gt;block its app&lt;/u&gt;&lt;/a&gt; in February. U.S. lawmakers have moved to &lt;a href="https://www.washingtonpost.com/technology/2025/10/30/tp-link-proposed-ban-commerce-department/"&gt;&lt;u&gt;ban the service&lt;/u&gt;&lt;/a&gt; from government devices, citing national security concerns.&lt;/p&gt;&lt;p&gt;Questions also persist about U.S. export controls designed to limit China&amp;#x27;s AI capabilities. In August, DeepSeek hinted that China would soon have &amp;quot;&lt;a href="https://www.cnbc.com/2025/08/22/deepseek-hints-latest-model-supported-by-chinas-next-generation-homegrown-ai-chips.html"&gt;&lt;u&gt;next generation&lt;/u&gt;&lt;/a&gt;&amp;quot; domestically built chips to support its models. The company indicated its systems work with Chinese-made chips from &lt;a href="https://www.huawei.com/en/"&gt;&lt;u&gt;Huawei&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.cambricon.com/"&gt;&lt;u&gt;Cambricon&lt;/u&gt;&lt;/a&gt; without additional setup.&lt;/p&gt;&lt;p&gt;DeepSeek&amp;#x27;s original V3 model was reportedly trained on roughly 2,000 older &lt;a href="https://www.reuters.com/technology/nvidia-tweaks-flagship-h100-chip-export-china-h800-2023-03-21/"&gt;&lt;u&gt;Nvidia H800 chips&lt;/u&gt;&lt;/a&gt; — hardware since restricted for China export. The company has not disclosed what powered V3.2 training, but its continued advancement suggests export controls alone cannot halt Chinese AI progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What DeepSeek&amp;#x27;s release means for the future of AI competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The release arrives at a pivotal moment. After years of massive investment, some analysts question whether an AI bubble is forming. DeepSeek&amp;#x27;s ability to match American frontier models at a fraction of the cost challenges assumptions that AI leadership requires enormous capital expenditure.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; reveals that post-training investment now exceeds 10% of pre-training costs — a substantial allocation credited for reasoning improvements. But DeepSeek acknowledges gaps: &amp;quot;The breadth of world knowledge in DeepSeek-V3.2 still lags behind leading proprietary models,&amp;quot; the report states. The company plans to address this by scaling pre-training compute.&lt;/p&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; remains available through a temporary API until December 15, when its capabilities will merge into the standard release. The Speciale variant is designed exclusively for deep reasoning and does not support tool calling — a limitation the standard model addresses.&lt;/p&gt;&lt;p&gt;For now, the AI race between the United States and China has entered a new phase. DeepSeek&amp;#x27;s release demonstrates that open-source models can achieve frontier performance, that efficiency innovations can slash costs dramatically, and that the most powerful AI systems may soon be freely available to anyone with an internet connection.&lt;/p&gt;&lt;p&gt;As one commenter on X observed: &amp;quot;Deepseek just casually breaking those historic benchmarks set by Gemini is bonkers.&amp;quot;&lt;/p&gt;&lt;p&gt;The question is no longer whether Chinese AI can compete with Silicon Valley. It&amp;#x27;s whether American companies can maintain their lead when their Chinese rival gives comparable technology away for free.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Chinese artificial intelligence startup &lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; released two powerful new AI models on Sunday that the company claims match or exceed the capabilities of OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; and Google&amp;#x27;s &lt;a href="https://blog.google/products/gemini/gemini-3/"&gt;&lt;u&gt;Gemini-3.0-Pro&lt;/u&gt;&lt;/a&gt; — a development that could reshape the competitive landscape between American tech giants and their Chinese challengers.&lt;/p&gt;&lt;p&gt;The Hangzhou-based company launched &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt;, designed as an everyday reasoning assistant, alongside DeepSeek-V3.2-Speciale, a high-powered variant that achieved gold-medal performance in four elite international competitions: the 2025 International Mathematical Olympiad, the International Olympiad in Informatics, the ICPC World Finals, and the China Mathematical Olympiad.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The release carries profound implications for American technology leadership. DeepSeek has once again demonstrated that it can produce frontier AI systems despite U.S. export controls that &lt;a href="https://www.reuters.com/world/china/trump-says-nvidias-blackwell-ai-chip-not-other-people-2025-11-03/"&gt;&lt;u&gt;restrict China&amp;#x27;s access to advanced Nvidia chips&lt;/u&gt;&lt;/a&gt; — and it has done so while making its models freely available under an open-source MIT license.&lt;/p&gt;&lt;p&gt;&amp;quot;People thought DeepSeek gave a one-time breakthrough but we came back much bigger,&amp;quot; wrote &lt;a href="https://x.com/ChenHuiOG"&gt;&lt;u&gt;Chen Fang&lt;/u&gt;&lt;/a&gt;, who identified himself as a contributor to the project, on X (formerly Twitter). The release drew swift reactions online, with one user declaring: &amp;quot;&lt;a href="https://x.com/iampritamj/status/1995511358142701612"&gt;&lt;u&gt;Rest in peace, ChatGPT&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How DeepSeek&amp;#x27;s sparse attention breakthrough slashes computing costs&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;At the heart of the new release lies &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek Sparse Attention&lt;/u&gt;&lt;/a&gt;, or DSA — a novel architectural innovation that dramatically reduces the computational burden of running AI models on long documents and complex tasks.&lt;/p&gt;&lt;p&gt;Traditional AI attention mechanisms, the core technology allowing language models to understand context, scale poorly as input length increases. Processing a document twice as long typically requires four times the computation. DeepSeek&amp;#x27;s approach breaks this constraint using what the company calls a &amp;quot;lightning indexer&amp;quot; that identifies only the most relevant portions of context for each query, ignoring the rest.&lt;/p&gt;&lt;p&gt;According to &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;DeepSeek&amp;#x27;s technical report&lt;/u&gt;&lt;/a&gt;, DSA reduces inference costs by roughly half compared to previous models when processing long sequences. The architecture &amp;quot;substantially reduces computational complexity while preserving model performance,&amp;quot; the report states.&lt;/p&gt;&lt;p&gt;Processing 128,000 tokens — roughly equivalent to a 300-page book — now costs approximately $0.70 per million tokens for decoding, compared to $2.40 for the previous &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;&lt;u&gt;V3.1-Terminus model&lt;/u&gt;&lt;/a&gt;. That represents a 70% reduction in inference costs.&lt;/p&gt;&lt;p&gt;The 685-billion-parameter models support context windows of 128,000 tokens, making them suitable for analyzing lengthy documents, codebases, and research papers. DeepSeek&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; notes that independent evaluations on long-context benchmarks show V3.2 performing on par with or better than its predecessor &amp;quot;despite incorporating a sparse attention mechanism.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The benchmark results that put DeepSeek in the same league as GPT-5&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s claims of parity with America&amp;#x27;s leading AI systems rest on extensive testing across mathematics, coding, and reasoning tasks — and the numbers are striking.&lt;/p&gt;&lt;p&gt;On &lt;a href="https://maa.org/aime-thresholds-are-available/"&gt;&lt;u&gt;AIME 2025&lt;/u&gt;&lt;/a&gt;, a prestigious American mathematics competition, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; achieved a 96.0% pass rate, compared to 94.6% for GPT-5-High and 95.0% for Gemini-3.0-Pro. On the &lt;a href="https://www.hmmt.org/"&gt;&lt;u&gt;Harvard-MIT Mathematics Tournament&lt;/u&gt;&lt;/a&gt;, the Speciale variant scored 99.2%, surpassing Gemini&amp;#x27;s 97.5%.&lt;/p&gt;&lt;p&gt;The standard &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2 model&lt;/u&gt;&lt;/a&gt;, optimized for everyday use, scored 93.1% on AIME and 92.5% on HMMT — marginally below frontier models but achieved with substantially fewer computational resources.&lt;/p&gt;&lt;p&gt;Most striking are the competition results. &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; scored 35 out of 42 points on the &lt;a href="https://www.imo-official.org/year_info.aspx?year=2025"&gt;&lt;u&gt;2025 International Mathematical Olympiad&lt;/u&gt;&lt;/a&gt;, earning gold-medal status. At the &lt;a href="https://ioinformatics.org/"&gt;&lt;u&gt;International Olympiad in Informatics&lt;/u&gt;&lt;/a&gt;, it scored 492 out of 600 points — also gold, ranking 10th overall. The model solved 10 of 12 problems at the &lt;a href="https://worldfinals.icpc.global/2025/"&gt;&lt;u&gt;ICPC World Finals&lt;/u&gt;&lt;/a&gt;, placing second.&lt;/p&gt;&lt;p&gt;These results came without internet access or tools during testing. DeepSeek&amp;#x27;s report states that &amp;quot;testing strictly adheres to the contest&amp;#x27;s time and attempt limits.&amp;quot;&lt;/p&gt;&lt;p&gt;On coding benchmarks, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; resolved 73.1% of real-world software bugs on &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-Verified&lt;/u&gt;&lt;/a&gt;, competitive with GPT-5-High at 74.9%. On &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal Bench 2.0&lt;/u&gt;&lt;/a&gt;, measuring complex coding workflows, DeepSeek scored 46.4%—well above GPT-5-High&amp;#x27;s 35.2%.&lt;/p&gt;&lt;p&gt;The company acknowledges limitations. &amp;quot;Token efficiency remains a challenge,&amp;quot; the technical report states, noting that DeepSeek &amp;quot;typically requires longer generation trajectories&amp;quot; to match Gemini-3.0-Pro&amp;#x27;s output quality.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why teaching AI to think while using tools changes everything&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Beyond raw reasoning, &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;DeepSeek-V3.2&lt;/u&gt;&lt;/a&gt; introduces &amp;quot;thinking in tool-use&amp;quot; — the ability to reason through problems while simultaneously executing code, searching the web, and manipulating files.&lt;/p&gt;&lt;p&gt;Previous AI models faced a frustrating limitation: each time they called an external tool, they lost their train of thought and had to restart reasoning from scratch. DeepSeek&amp;#x27;s architecture preserves the reasoning trace across multiple tool calls, enabling fluid multi-step problem solving.&lt;/p&gt;&lt;p&gt;To train this capability, the company built a massive synthetic data pipeline generating over 1,800 distinct task environments and 85,000 complex instructions. These included challenges like multi-day trip planning with budget constraints, software bug fixes across eight programming languages, and web-based research requiring dozens of searches.&lt;/p&gt;&lt;p&gt;The technical report describes one example: planning a three-day trip from Hangzhou with constraints on hotel prices, restaurant ratings, and attraction costs that vary based on accommodation choices. Such tasks are &amp;quot;hard to solve but easy to verify,&amp;quot; making them ideal for training AI agents.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deepseek.com/"&gt;&lt;u&gt;DeepSeek&lt;/u&gt;&lt;/a&gt; employed real-world tools during training — actual web search APIs, coding environments, and Jupyter notebooks — while generating synthetic prompts to ensure diversity. The result is a model that generalizes to unseen tools and environments, a critical capability for real-world deployment.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;DeepSeek&amp;#x27;s open-source gambit could upend the AI industry&amp;#x27;s business model&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Unlike OpenAI and Anthropic, which guard their most powerful models as proprietary assets, DeepSeek has released both &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;&lt;u&gt;V3.2&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;V3.2-Speciale&lt;/u&gt;&lt;/a&gt; under the MIT license — one of the most permissive open-source frameworks available.&lt;/p&gt;&lt;p&gt;Any developer, researcher, or company can download, modify, and deploy the 685-billion-parameter models without restriction. Full model weights, training code, and documentation are &lt;a href="https://huggingface.co/deepseek-ai"&gt;&lt;u&gt;available on Hugging Face&lt;/u&gt;&lt;/a&gt;, the leading platform for AI model sharing.&lt;/p&gt;&lt;p&gt;The strategic implications are significant. By making frontier-capable models freely available, DeepSeek undermines competitors charging premium API prices. The Hugging Face model card notes that DeepSeek has provided Python scripts and test cases &amp;quot;demonstrating how to encode messages in OpenAI-compatible format&amp;quot; — making migration from competing services straightforward.&lt;/p&gt;&lt;p&gt;For enterprise customers, the value proposition is compelling: frontier performance at dramatically lower cost, with deployment flexibility. But data residency concerns and regulatory uncertainty may limit adoption in sensitive applications — particularly given DeepSeek&amp;#x27;s Chinese origins.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Regulatory walls are rising against DeepSeek in Europe and America&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;DeepSeek&amp;#x27;s global expansion faces mounting resistance. In June, Berlin&amp;#x27;s data protection commissioner Meike Kamp declared that DeepSeek&amp;#x27;s transfer of German user data to China is &amp;quot;&lt;a href="https://www.cnbc.com/2025/06/27/germany-tells-apple-google-to-block-deepseek-ai-app.html#:~:text=Berlin&amp;#x27;s%20data%20protection%20commissioner%20Meike,under%20EU%20data%20protection%20rules."&gt;&lt;u&gt;unlawful&lt;/u&gt;&lt;/a&gt;&amp;quot; under EU rules, asking Apple and Google to consider blocking the app.&lt;/p&gt;&lt;p&gt;The German authority expressed concern that &amp;quot;Chinese authorities have extensive access rights to personal data within the sphere of influence of Chinese companies.&amp;quot; Italy ordered DeepSeek to &lt;a href="https://www.reuters.com/technology/artificial-intelligence/italys-privacy-watchdog-blocks-chinese-ai-app-deepseek-2025-01-30/"&gt;&lt;u&gt;block its app&lt;/u&gt;&lt;/a&gt; in February. U.S. lawmakers have moved to &lt;a href="https://www.washingtonpost.com/technology/2025/10/30/tp-link-proposed-ban-commerce-department/"&gt;&lt;u&gt;ban the service&lt;/u&gt;&lt;/a&gt; from government devices, citing national security concerns.&lt;/p&gt;&lt;p&gt;Questions also persist about U.S. export controls designed to limit China&amp;#x27;s AI capabilities. In August, DeepSeek hinted that China would soon have &amp;quot;&lt;a href="https://www.cnbc.com/2025/08/22/deepseek-hints-latest-model-supported-by-chinas-next-generation-homegrown-ai-chips.html"&gt;&lt;u&gt;next generation&lt;/u&gt;&lt;/a&gt;&amp;quot; domestically built chips to support its models. The company indicated its systems work with Chinese-made chips from &lt;a href="https://www.huawei.com/en/"&gt;&lt;u&gt;Huawei&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.cambricon.com/"&gt;&lt;u&gt;Cambricon&lt;/u&gt;&lt;/a&gt; without additional setup.&lt;/p&gt;&lt;p&gt;DeepSeek&amp;#x27;s original V3 model was reportedly trained on roughly 2,000 older &lt;a href="https://www.reuters.com/technology/nvidia-tweaks-flagship-h100-chip-export-china-h800-2023-03-21/"&gt;&lt;u&gt;Nvidia H800 chips&lt;/u&gt;&lt;/a&gt; — hardware since restricted for China export. The company has not disclosed what powered V3.2 training, but its continued advancement suggests export controls alone cannot halt Chinese AI progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What DeepSeek&amp;#x27;s release means for the future of AI competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The release arrives at a pivotal moment. After years of massive investment, some analysts question whether an AI bubble is forming. DeepSeek&amp;#x27;s ability to match American frontier models at a fraction of the cost challenges assumptions that AI leadership requires enormous capital expenditure.&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"&gt;&lt;u&gt;technical report&lt;/u&gt;&lt;/a&gt; reveals that post-training investment now exceeds 10% of pre-training costs — a substantial allocation credited for reasoning improvements. But DeepSeek acknowledges gaps: &amp;quot;The breadth of world knowledge in DeepSeek-V3.2 still lags behind leading proprietary models,&amp;quot; the report states. The company plans to address this by scaling pre-training compute.&lt;/p&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;&lt;u&gt;DeepSeek-V3.2-Speciale&lt;/u&gt;&lt;/a&gt; remains available through a temporary API until December 15, when its capabilities will merge into the standard release. The Speciale variant is designed exclusively for deep reasoning and does not support tool calling — a limitation the standard model addresses.&lt;/p&gt;&lt;p&gt;For now, the AI race between the United States and China has entered a new phase. DeepSeek&amp;#x27;s release demonstrates that open-source models can achieve frontier performance, that efficiency innovations can slash costs dramatically, and that the most powerful AI systems may soon be freely available to anyone with an internet connection.&lt;/p&gt;&lt;p&gt;As one commenter on X observed: &amp;quot;Deepseek just casually breaking those historic benchmarks set by Gemini is bonkers.&amp;quot;&lt;/p&gt;&lt;p&gt;The question is no longer whether Chinese AI can compete with Silicon Valley. It&amp;#x27;s whether American companies can maintain their lead when their Chinese rival gives comparable technology away for free.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/deepseek-just-dropped-two-insanely-powerful-ai-models-that-rival-gpt-5-and</guid><pubDate>Mon, 01 Dec 2025 18:45:00 +0000</pubDate></item><item><title>[NEW] Data center energy demand forecasted to soar nearly 300% through 2035 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/data-center-energy-demand-forecasted-to-soar-nearly-300-through-2035/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Planned data center construction shows no signs of fading, with new additions to require 2.7x — nearly triple — the sector’s current demand for electricity over the next decade, according to a new report from BloombergNEF.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By 2035, data centers will draw 106 gigawatts, up sharply from the 40 gigawatts they use today. Much of that growth will occur in more rural areas as facilities grow in size and as sites near urban areas become scarce, BloombergNEF said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Driving part of the growth is the sheer scale of planned data centers. Today, only 10% of data centers draw more than 50 megawatts of electricity, but over the next decade, the average new facility will draw well over 100 megawatts. The biggest sites help skew the data: Nearly a quarter will be larger than 500 megawatts, and a few will exceed 1 gigawatt.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A chart illustrating data center electricity use through 2032." class="wp-image-3071402" height="432" src="https://techcrunch.com/wp-content/uploads/2025/12/BloombergNEF-data-center-forecast-12-1-2025.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Planned data centers are significantly larger than those currently in operation&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;BloombergNEF&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, the utilization rate for all data centers is expected to grow from 59% to 69% as AI training and inference grows to nearly 40% of total data center compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, the findings in the new report aren’t surprising. AI companies have been racing to build more powerful data centers, helping to drive global investment in the facilities up to $580 billion this year. That’s more than the world spends finding new supplies of oil.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the new report shows just how quickly the landscape is changing. It is a sharp revision upwards from a document the group published in April. The upswing was driven by a surge in new projects that have been announced since then. “With an average seven-year timeline for projects to come online, developments in earlier stages affect the tail end of our forecast the most,” the new report said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early-stage projects have more than doubled between early 2024 and early 2025, though those are distinct from projects that have been committed or are currently under construction.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Much of that new capacity is being planned for Virginia, Pennsylvania, Ohio, Illinois, and New Jersey. They lie within a region known to industry experts at the PJM Interconnection, a regional transmission organization that’s tasked with operating the electrical grid in those states and others, including Delaware, West Virginia, and parts of Kentucky and North Carolina. Texas’s Ercot grid will see a large number of additions, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report arrives as the PJM Interconnection is under scrutiny from its independent monitor, Monitoring Analytics. The group filed a complaint with the Federal Energy Regulatory Commission (FERC) saying that PJM has the authority to authorize new data center connections only when its grid has adequate capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As part of its obligation to maintain reliability, PJM has the authority to require large new data center loads to wait to be added to the system until the loads can be served reliably,” Monitoring Analytics wrote. “PJM has the authority to create a load queue.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What’s more, data centers are responsible for today’s high electricity prices within the region, the organization said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“PJM’s failure to clarify and enforce its existing rules and to protect reliable and affordable service in PJM is unjust and unreasonable,” it said.&amp;nbsp;&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This &lt;em&gt;stream&lt;/em&gt; is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Planned data center construction shows no signs of fading, with new additions to require 2.7x — nearly triple — the sector’s current demand for electricity over the next decade, according to a new report from BloombergNEF.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By 2035, data centers will draw 106 gigawatts, up sharply from the 40 gigawatts they use today. Much of that growth will occur in more rural areas as facilities grow in size and as sites near urban areas become scarce, BloombergNEF said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Driving part of the growth is the sheer scale of planned data centers. Today, only 10% of data centers draw more than 50 megawatts of electricity, but over the next decade, the average new facility will draw well over 100 megawatts. The biggest sites help skew the data: Nearly a quarter will be larger than 500 megawatts, and a few will exceed 1 gigawatt.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A chart illustrating data center electricity use through 2032." class="wp-image-3071402" height="432" src="https://techcrunch.com/wp-content/uploads/2025/12/BloombergNEF-data-center-forecast-12-1-2025.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Planned data centers are significantly larger than those currently in operation&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;BloombergNEF&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, the utilization rate for all data centers is expected to grow from 59% to 69% as AI training and inference grows to nearly 40% of total data center compute.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In some ways, the findings in the new report aren’t surprising. AI companies have been racing to build more powerful data centers, helping to drive global investment in the facilities up to $580 billion this year. That’s more than the world spends finding new supplies of oil.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the new report shows just how quickly the landscape is changing. It is a sharp revision upwards from a document the group published in April. The upswing was driven by a surge in new projects that have been announced since then. “With an average seven-year timeline for projects to come online, developments in earlier stages affect the tail end of our forecast the most,” the new report said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early-stage projects have more than doubled between early 2024 and early 2025, though those are distinct from projects that have been committed or are currently under construction.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Much of that new capacity is being planned for Virginia, Pennsylvania, Ohio, Illinois, and New Jersey. They lie within a region known to industry experts at the PJM Interconnection, a regional transmission organization that’s tasked with operating the electrical grid in those states and others, including Delaware, West Virginia, and parts of Kentucky and North Carolina. Texas’s Ercot grid will see a large number of additions, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The report arrives as the PJM Interconnection is under scrutiny from its independent monitor, Monitoring Analytics. The group filed a complaint with the Federal Energy Regulatory Commission (FERC) saying that PJM has the authority to authorize new data center connections only when its grid has adequate capacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As part of its obligation to maintain reliability, PJM has the authority to require large new data center loads to wait to be added to the system until the loads can be served reliably,” Monitoring Analytics wrote. “PJM has the authority to create a load queue.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;What’s more, data centers are responsible for today’s high electricity prices within the region, the organization said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“PJM’s failure to clarify and enforce its existing rules and to protect reliable and affordable service in PJM is unjust and unreasonable,” it said.&amp;nbsp;&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This &lt;em&gt;stream&lt;/em&gt; is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/data-center-energy-demand-forecasted-to-soar-nearly-300-through-2035/</guid><pubDate>Mon, 01 Dec 2025 19:08:42 +0000</pubDate></item><item><title>[NEW] Construction workers are cashing in on the AI boom (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/construction-workers-are-cashing-in-on-the-ai-boom/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2152222109.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI boom is proving to be a windfall for construction workers building the massive data centers that power it all. According to The Wall Street Journal, workers moving into data-center construction are seeing pay jumps of 25% to 30% compared to their previous jobs — and in some cases, much more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among them is DeMond Chambliss, who traded his small drywall business in Columbus, Ohio, for a supervisor role overseeing 200 workers at a data center site. He now makes over $100,000 annually. “I pinch myself going to work every day,” the 51-year-old tells the Journal. In Oregon, electrical safety specialist Marc Benner pulls in $225,000 a year, while electrician Andrew Mason makes over $200,000 managing workers at six Northern Virginia data centers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Journal reports this isn’t just about higher base pay. Companies are sweetening the pot with perks like heated break tents, free lunches, daily incentive bonuses, and even remote project management positions. One construction site offers workers $100 in daily incentive pay, which can add up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The surge comes as tech giants like Amazon, Google, and Microsoft race to build hundreds of new data centers, colliding with an industry-wide shortage of roughly 439,000 skilled workers, according to the Associated Builders and Contractors trade group. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2152222109.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI boom is proving to be a windfall for construction workers building the massive data centers that power it all. According to The Wall Street Journal, workers moving into data-center construction are seeing pay jumps of 25% to 30% compared to their previous jobs — and in some cases, much more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among them is DeMond Chambliss, who traded his small drywall business in Columbus, Ohio, for a supervisor role overseeing 200 workers at a data center site. He now makes over $100,000 annually. “I pinch myself going to work every day,” the 51-year-old tells the Journal. In Oregon, electrical safety specialist Marc Benner pulls in $225,000 a year, while electrician Andrew Mason makes over $200,000 managing workers at six Northern Virginia data centers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Journal reports this isn’t just about higher base pay. Companies are sweetening the pot with perks like heated break tents, free lunches, daily incentive bonuses, and even remote project management positions. One construction site offers workers $100 in daily incentive pay, which can add up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The surge comes as tech giants like Amazon, Google, and Microsoft race to build hundreds of new data centers, colliding with an industry-wide shortage of roughly 439,000 skilled workers, according to the Associated Builders and Contractors trade group. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/construction-workers-are-cashing-in-on-the-ai-boom/</guid><pubDate>Mon, 01 Dec 2025 19:13:50 +0000</pubDate></item><item><title>[NEW] Ideas: Community building, machine learning, and the future of AI (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/HannaJenn-Ideas_TW_LI_FB_1200x627-1.jpg" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;






&lt;p&gt;Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series &lt;em&gt;Ideas&lt;/em&gt;, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&lt;/p&gt;



&lt;p&gt;In 2006, three PhD students organized the Women in Machine Learning Workshop, or&amp;nbsp;&lt;em&gt;WiML&lt;/em&gt;, to provide a space for women in ML to connect and share their research. The event has been held every year since,&amp;nbsp;growing in size&amp;nbsp;and mission.&lt;/p&gt;



&lt;p&gt;In this episode, two of the WiML cofounders, Jenn Wortman Vaughan, a Microsoft senior principal research manager, and Hanna Wallach, a Microsoft vice president and distinguished scientist, reflect on the 20th workshop. They discuss WiML’s journey from a potential one-off event to a nonprofit supporting women and nonbinary individuals worldwide; their friendship and collaborations, including their contributions to defining responsible AI at Microsoft; and the advice they’d give their younger selves.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;








&lt;/div&gt;



&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SERIES INTRODUCTION:&lt;/strong&gt; You’re listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;JENN WORTMAN VAUGHAN:&lt;/strong&gt; Hello, and welcome. I’m Jenn Wortman Vaughan. This week, machine learning researchers around the world will be attending the annual Conference on Neural Information Processing Systems, or NeurIPS. I am especially excited about NeurIPS this year because of a co-located event, the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, or WiML, which I am going to be attending both as a mentor and as a keynote speaker.&lt;/p&gt;



&lt;p&gt;So to celebrate 20 years of WiML, I’m here today with my long-term collaborator, colleague, close friend, &lt;em&gt;and&lt;/em&gt; my cofounder of the workshop for Women in Machine Learning, Hanna Wallach.&lt;/p&gt;



&lt;p&gt;You know, you and I have known each other for a very long time at this point. And in many ways, we followed very parallel and often intersecting paths before we both ended up here working in responsible AI at Microsoft. So I thought it might be fun to kick off this podcast with a bit of the story of our interleaving trajectories.&lt;/p&gt;



&lt;p&gt;So let’s start way back 20 years ago, around the time we first had the idea for WiML. Where were you, and what were you up to?&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;HANNA WALLACH:&lt;/strong&gt; Yeah, so I was a PhD student at the University of Cambridge, and I was working with the late David MacKay. I was focusing on machine learning for analyzing text, and at that point in time, I’d actually just begun working on Bayesian latent variable models for text analysis, and my research was really focusing on trying to combine ideas from &lt;em&gt;n&lt;/em&gt;-gram language modeling with statistical topic modeling in order to come up with models that just did a better job at modeling text.&lt;/p&gt;



&lt;p&gt;I was also doing this super-weird two-country thing. So I was doing my PhD at Cambridge, but at the end of the first year of my PhD, I spent three months as a visiting graduate student at the University of Pennsylvania, and I loved it, so much so that at the end of the three months I said, can I extend for a full year? Cambridge said yes; Penn said yes. So I did that and actually ended up then extending another year and then another year and another year and so on and so forth.&lt;/p&gt;



&lt;p&gt;But during my first full year at Penn, that was when I met &lt;em&gt;you&lt;/em&gt;, and it was at the visiting students weekend, and I had been told by the faculty in the department that I had to work really hard on recruiting you. I had no idea that that was actually going to be the start of a 20-plus-year friendship.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I still remember that visiting weekend very well. I actually met you; I met my husband, Jeff; and I met my PhD advisor, Michael Kearns, all on the same day at that visiting student weekend. So I didn’t know it at the time, but it was a very big day for me.&lt;/p&gt;



&lt;p&gt;So around that time when I started my PhD at Penn, I was working in machine learning theory and algorithmic economics. So even then, you know, just like I am now, I was interested in the intersection of people and AI systems. But since my training was in theory, my “people” tended to be these mathematically ideal people with these well-defined preferences and beliefs who behaved in very well-defined ways.&lt;/p&gt;



&lt;p&gt;Working in learning theory like this was appealing to me because it was very neat and precise. There was just none of the mess of the real world. You could just write down your model, which contained all of your assumptions, and everything else that followed from there was in some sense objective.&lt;/p&gt;



&lt;p&gt;So I was really enjoying this work, and I was also so excited to have you around the department at the time. You know, honestly, I also loved Penn. It was just such a great environment. I was just actually back there a few weeks ago, visiting to give a talk. I had an amazing time. But it was, I will say, very male dominated in the computer science department at the time. In my incoming class of PhD students, we had 20 incoming PhDs, and I was the only woman there. But we managed to build a community. We had our weekly ladies brunch, which I loved, and things like that really kept me going during my PhD.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, I loved that ladies brunch. That made a huge difference to me and, kind of, kept me going through the PhD, as well.&lt;/p&gt;



&lt;p&gt;And, like you, I’d always been interested in people. And during the course of my PhD, I realized that I wasn’t interested in analyzing text for the sake of text, right. I was interested because text is one of these ways that people communicate with each other. You know, people don’t write text for the sake of writing text. They write it because they’re trying to convey something. And it was really &lt;em&gt;that&lt;/em&gt; that I was interested in. It was these, kind of, social aspects of text that I found super interesting.&lt;/p&gt;



&lt;p&gt;So coming out of the PhD, I then got a postdoc job focused on analyzing texts as part of these, sort of, broader social processes. From there, I ended up getting a faculty job, also at UMass, as one of four founding members of UMass’s Computational Social Science Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. So there was me in computer science, then there was another assistant professor in statistics, another in political science, and another in sociology. And in many ways, this was my dream job. I was being paid to develop and use machine learning methods to study social processes and answer questions that social scientists wanted to study. It was pretty awesome. You, I think, started a faculty position at the same time, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I also did a postdoc. First, I spent a year as a postdoc at Harvard, which was super fun. And then I started a tenure track position in computer science at UCLA in 2010.&lt;/p&gt;



&lt;p&gt;Again, you know, it was a very male-dominated environment. My department was mostly men. But maybe even more importantly than this, I just didn’t really have a network there. You know, it was lonely. One exception to this was Mihaela van der Schaar. She was at UCLA at the time, though not in my department, and she, kind of, took me under her wing. So I’m very grateful that I had that support. But overall, this position just wasn’t a great fit for me, and I was under more stress then than I think I have been at any other point in my life that I could really remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So at that point, then, you ended up transitioning to Microsoft Research, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Why did you end up choosing MSR [Microsoft Research]?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so this was back in 2012. MSR had just opened up this &lt;em&gt;new&lt;/em&gt; New York City lab at the time, and working in this lab was basically my dream job. I think I actually tried to apply before they had even officially opened the lab, like when I just heard it was happening.&lt;/p&gt;



&lt;p&gt;So this lab focused in three areas at the time. It focused in machine learning, algorithmic economics, and computational social science. And my research at the time cut across all three of these areas. So it felt just like this perfect opportunity to work in the space where my work would fit in so well and be really appreciated.&lt;/p&gt;



&lt;p&gt;The algorithmic economics group at the time actually was working on building prediction markets to aggregate information about future events, and they were already, in doing this, building on top of some of my theoretical research, which was just super cool to see. So that was exciting. And I already knew a couple of people here. I knew John Langford and Dave Pennock, who was in the economics group at the time, because I’d done an internship actually with the two of them at Yahoo Research before they came to Microsoft. And I was really excited to come back and work with them again, as well.&lt;/p&gt;



&lt;p&gt;You know, even here at the time that I joined the lab, it was 13 men and me. So once again, not great numbers. And I think that in some ways this was especially hard on me because I was just naturally, like, a very shy person and I hadn’t really built up the confidence that I should have at that point in my career. But on the other hand, I found the research fit just so spot-on that I couldn’t say no. And I suspect that this is something that you understand yourself because you actually came and joined me here in the New York lab a year or two later. So why did you make this switch?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so I anticipated that I was going to love my faculty job. It was focusing on all this stuff that I was so excited about. And much to my surprise, though, I kind of didn’t. And it wasn’t like there was any one particular thing that I didn’t like. It was more of a mixture of things. I did love my research, though. That was pretty clear to me. &lt;em&gt;But&lt;/em&gt; I wasn’t happy. So I spent a summer talking to as many people as possible in all different kinds of jobs, really just with the goal of figuring out what their day-to-day lives looked like. You were one of the people I spoke to, but I spoke to a ton of other people, as well.&lt;/p&gt;



&lt;p&gt;And from doing that, at the end of that summer, I ended up deciding to apply to industry jobs, and I applied to a bunch of places and got a bunch of offers. But I ended up deciding to join Microsoft Research New York City because of all the places I was considering going, they were the only place that said, “We love your research. We love what you do. Do you want to come here and do that same research?”&lt;/p&gt;



&lt;p&gt;And that was really appealing to me because I loved my research. Of course, I wanted to come there and do my same research and especially with all of these amazing people like you, Duncan Watts, who’d for many years been somebody I’d really looked up to. He was there, as well, at that point in time. There was this real focus on computational social science but with a little bit more of an industry perspective. There were also these amazing machine learning researchers. Just for many of the same reasons as you, I was just really excited to join that lab and particularly excited to be working in the same organization as you again.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I’m happy to take at least a little bit of the credit for …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … recruiting you to Microsoft here many years ago.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. I was really excited to have you join, too, though I think the timing actually worked out so that I missed your first couple of months because I was on maternity leave with my first daughter at the time. I should say I’ve got two daughters, and I’m very proud to share in the context of this podcast that they’re both very interested in math and reading, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, they’re both great.&lt;/p&gt;



&lt;p&gt;Um, so then we ended up working in the same place. But despite that, it still took us several years to end up actually collaborating on research. Do you remember how we ended up working together?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I used to tell this story a lot. Actually, I was at this panel on AI in society back in, I think, it was probably 2016. It was taking place in DC. And someone on this panel made this statement that soon our AI systems are just going to be so good that all of the uncertainty is going to be taken out of our decision-making, and something about this statement just, like, really set me off. I got so mad about it because I thought it was just …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … such an irresponsible thing to be saying. So I came back to New York, and I think I was ranting to you about this in the lab, and this conversation ended up getting us started on this whole longer discussion about the importance of communicating uncertainty and about explaining the assumptions that are behind the predictions that you’re making and all of this.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So this was something … I was really excited about this because this was something that had really been drummed into me for years as a Bayesian. So Bayesian statistics, which forms a lot of the foundation of the type of machine learning that I was doing, is all about explicitly stating assumptions and quantifying uncertainty. So I just felt super strongly about this stuff.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So somehow all of these discussions we were having led us to read up on this literature that was coming out of the machine learning community on interpretability at the time. There are a bunch of these papers coming out that were making claims about models being interpretable without stopping to define &lt;em&gt;who&lt;/em&gt; they were interpretable to or for what purpose. Never &lt;em&gt;actually&lt;/em&gt; taking these models and putting them down in front of real people. And we wanted to do something about this. So we started running controlled experiments with &lt;em&gt;real&lt;/em&gt; people and found that we often can’t trust our intuition about what makes a model interpretable.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, one of the things that came up a lot in that work was, sort of, how to measure these squishy abstract human concepts, like &lt;em&gt;interpretability&lt;/em&gt;, that are really hard to define, let alone quantify and measure and stuff like that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Absolutely. So I think one of the first things that we really struggled with in this line of work was what it even means to be &lt;em&gt;interpretable&lt;/em&gt; or &lt;em&gt;intelligible&lt;/em&gt; or any of these terms that were getting thrown around at the time.&lt;/p&gt;



&lt;p&gt;Um, we ended up doing some research, which is still one of my favorite papers, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … with our colleagues Forough Poursabzi, Jake Hofman, and Dan Goldstein. And in this work, we found it really useful to think about interpretability as a latent property that can be, kind of, influenced by different properties of a model or system’s design. So things like the number of features the model has or whether the model’s linear or even things like the user interface of the model.&lt;/p&gt;



&lt;p&gt;This was kind of a gateway project for me in the sense that it’s one of the first projects that I got really excited about that was more of a human-computer interaction, or HCI, project rather than a theory project like I’d been working on in the past. And it just set off this huge spark of excitement in me. It felt to me at the time more important than other things that I was doing, and I just wanted to do more and more of this work.&lt;/p&gt;



&lt;p&gt;I would say the other project that had a really similar effect on me, which we also worked on together right around the same time, was our work with Ken Holstein mapping out challenges that industry practitioners were facing in the space of AI fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah. OK, yep. That project, that was so fun, and I learned so much from it. If I recall correctly, we originally hired Ken, who I think was an HCI PhD student at CMU at the time, as an intern …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … to work with us on creating, sort of, user experiences for fairness tools like the Fairlearn toolkit&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And we started that project—so that was in collaboration with Miro Dudík and Hal Daumé—we started that project by having Ken talk to a whole bunch of practitioners at Microsoft but at other organizations, as well, to get a sense for how they were and weren’t using fairness toolkits like Fairlearn.&lt;/p&gt;



&lt;p&gt;And I want to point out that at that point in time, the academic research community was super focused on all of these, like, simple quantitative metrics for assessing the fairness in the context of predictions and predictive machine learning models with this, kind of, understanding that these tools could then be built to help practitioners assess the fairness of their predictive models and maybe even make fairer predictions. And so that’s the kind of stuff that this Fairlearn toolkit was originally developed to do. So we ended up asking all of these practitioners originally just as, sort of, the precursor to what we thought we were going to end up doing with this project.&lt;/p&gt;



&lt;p&gt;We also asked these practitioners about their current practices and challenges around fairness in their work and about their additional needs for support. So where did they feel like they had the right tools and processes and practices and where did they feel like they were missing stuff. And this was really eye-opening because what we found was so different than what we were expecting. And there’s two things that really stood out to us.&lt;/p&gt;



&lt;p&gt;So the first thing was that we found a much, much wider range of applications beyond prediction. So we’d come into this assuming that all these practitioners were doing stuff with predictive machine learning models, but in fact, we were finding they were doing all kinds of stuff. There was a bunch of unsupervised stuff; there was a bunch of, you know, language-based stuff—all of this kind of thing. And in hindsight, that probably doesn’t sound very surprising nowadays because of the rise of generative AI, and really the entire machine learning and AI field is much less focused on prediction in that, kind of, narrow, kind of, classification-regression kind of way. But at the time, this was really surprising, especially in light of the academic literature’s focus on predictions when thinking about fairness.&lt;/p&gt;



&lt;p&gt;The second thing that we found was that practitioners often struggled to use existing fairness research, in part because these quantitative metrics that were all the rage at that point in time, just weren’t really amenable to the types of real-world complex scenarios that these practitioners were facing. And there was a bunch of different reasons for this, but one of the things that really stood out to us was that this wasn’t so much about the underlying models and stuff like that, but it was actually that there were a variety of data challenges involved here around things like data collection, collection of sensitive attributes, which you need in order to actually use these fairness metrics.&lt;/p&gt;



&lt;p&gt;So putting all this together, the upshot of all this was that we never did what we originally set out to do with that [LAUGHS], that internship project. We … because we uncovered this really large gap between research and practice, we ended up publishing this paper that characterized this gap and then surfaced important directions for future research. The other thing that the paper did was emphasize the importance of doing this kind of qualitative work to actually understand what’s happening in practice rather than just making assumptions about what practitioners are and aren’t doing.&lt;/p&gt;



&lt;p&gt;The other thing that came out of it, of course, was that the four of us—so you, me, Miro and Hal—learned a ton about HCI and about qualitative research from Ken, which was just, uh, so fun.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and I started to be confronted with the fact that I could no longer reasonably ignore all of these messes of the real world because, you know, in some ways, responsible AI is really all about the messes.&lt;/p&gt;



&lt;p&gt;So I think this project was really a big shift for both of us. And in some ways, working on this and the interpretability work really led us to be active in these early efforts that were happening within Microsoft in the responsible AI space. Um, the research that we were doing was feeding directly into company policy, and it felt like it was just, like, a huge place where we could have some impact. So it was very exciting.&lt;/p&gt;



&lt;p&gt;So switching gears a bit. Hanna, do you remember how we first got the idea for WiML?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, I do. So we were at NeurIPS. This was back in 2005. It was a … so NeurIPS was a very different conference back then. Now it’s like tens of thousands of people. It’s held in a massive convention center. Yes, there are researchers there, but there’s a variety of people from across the tech industry who attend, but that is &lt;em&gt;not&lt;/em&gt; what it was like back then.&lt;/p&gt;



&lt;p&gt;So in around … in 2005, it was more like 600 people thereabouts in total[1], and the main conference would be held every year in Vancouver, and then everybody at the conference would pile onto these buses, and we would all head up to Whistler for the workshops.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So super different to what’s happening nowadays. It was my third time. I think that’s right. I think it was my third time attending the conference. But it was my first time sharing a hotel room with other women. And I remember up at the workshops, up in Whistler, there were five of us sitting around in a hotel room, and we were talking about how amazing it was that there were five of us sitting around talking, &lt;em&gt;women&lt;/em&gt;. And we, kind of, couldn’t believe there were five of us. We’re all PhD students at the time. And so we decided to make this list, and we started trying to figure out who the other women in machine learning were. And we came up with about 10 names, and we were kind of amazed that there were even 10 women in machine learning. We thought this was a &lt;em&gt;huge&lt;/em&gt; number. We were very excited. And we started talking about how it might be really fun to just bring them all together sometime.&lt;/p&gt;



&lt;p&gt;So we returned from NeurIPS, and you and I ended up getting lunch to strategize. I still remember walking out of the department together to go get lunch and you were walking ahead of me. I can visualize the coat you were wearing as you were walking in front of me. And so we strategized a bit and ended up deciding, along with one of the other women, Lisa Wainer, to submit a proposal to the Grace Hopper conference for a session in which women in machine learning would give short talks about their research.&lt;/p&gt;



&lt;p&gt;We reached out to the 10 names that we had written down in the hotel room and through that process actually ended up finding out about more women in machine learning and eventually had something like 25 women listed on the final proposal. I think there’s an email somewhere where one or another of us is saying to the other one, “Oh my gosh! I can’t believe there are so many women in machine learning.”&lt;/p&gt;



&lt;p&gt;So we submitted this proposal, and ultimately, the proposal was rejected by the Grace Hopper conference. But we were so excited about the idea and just really invested in it by that point that we decided to hold our own co-located event the day before the Grace Hopper conference. And I’ve got to say, you know, 20 years later, I don’t know &lt;em&gt;what&lt;/em&gt; we were thinking. Like, that was a bold move on the part of three PhD students. And it turned out to be a huge amount of work that we had to do entirely ourselves, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; We had no idea what we were doing. But the Grace Hopper folks very nicely connected us with the venue that the conference was going to be held at, and somehow, we managed to pull it off. Ultimately, that first workshop had around 100 women, and there was this … rather than just, like, a single short session, which is what we’d originally had in mind, we had this full day’s worth of talks. I actually have the booklet of abstracts from all of those talks at my desk in the office. I still have that today. And it was just an amazing experience.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, it was. And, you know, you mentioned how bold we were. I just, I really don’t think that any of us at the time realized how bold we were being here, getting this workshop rejected and then saying, you know, &lt;em&gt;no&lt;/em&gt;, we think this is important. We’re going to do it anyway. &lt;em&gt;On our own. As grad students.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;So I’ve already talked a little bit about some of the spaces that I was in throughout my career where there just weren’t a lot of women around in the room with me. How had you experienced a lack of community or network of women in machine learning before the founding of WiML? And, you know, why do you think it’s important to have that kind of community?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So I felt it in a number of different ways. I think I mentioned a few minutes ago that, like, it was my third time at NeurIPS but my first time sharing a hotel room with another woman. But there were many places over the years where I’d felt this.&lt;/p&gt;



&lt;p&gt;So first, as an undergraduate. Then, I did a lot of free and open-source software development, and I was pretty involved in stuff to do with the Debian Linux distribution. And back then, the percentage of women involved in free and open-source software development was about 1 percent, 1.5%&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and the percentage involved actually in Debian was even less than that. So that had led me and some others to start this Debian Women Project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then, again, of course, I faced this in machine learning.&lt;/p&gt;



&lt;p&gt;I just didn’t know that many other women in machine learning. I didn’t … there weren’t a large number of senior women, for example, to look up to as role models. There weren’t a large number of female PhD students. And this, kind of, made me sad because I was really excited about machine learning, and I hoped to spend my entire career in it. But because I didn’t see so many other women around, particularly more senior women, that really made me question whether that would even be possible, and I just didn’t know.&lt;/p&gt;



&lt;p&gt;Um, I think, you know, thinking about this, and I’ve obviously reflected on this a lot over the years, but I think having a diverse community in any area, be it free and open-source software development, be it machine learning, any of these kinds of things, is just so important for so many reasons. And some of those reasons are little things like finding people that you would feel comfortable sharing a hotel room with.&lt;/p&gt;



&lt;p&gt;But many of these things are bigger things that can then have, like, even, kind of, knock-on cumulative effects. Like feeling valued in the community, feeling welcome in the community, having role models, being able to, sort of, see people and say, “Oh, I want to be kind of like that person when I grow up; I could do this.” And then even just representation of different perspectives in the work itself is so important.&lt;/p&gt;



&lt;p&gt;The flip side of that is that there are a whole bunch of things that can go wrong if you don’t have a diverse community. You can end up with gatekeeping, with toxic or unsafe cultures, obviously attrition as people just leave these kinds of spaces because they feel that they’re not welcome there and won’t be valued there. And then to that point of having representation of different perspectives, with a really homogeneous community, you can end up with, kind of, blind spots around the technology itself, which can then lead to harms.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; 100%. So did you ever imagine during&amp;nbsp;all of&amp;nbsp;this that WiML would still be around 20 years later and we would be sitting here on a podcast talking about this?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; [LAUGHS] No, absolutely not. I didn’t even think that WiML would necessarily be around for a second year. I thought it was probably going to be, like, a one-off event. And I certainly don’t think that I thought that I would still be involved in the machine learning community 20 years later, as well. So very unexpected.&lt;/p&gt;



&lt;p&gt;I’ve got a question for you, though. What do you remember most about that first workshop?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; I remember a lot of things. I remember that, you know, when we were planning this, we always really wanted the focus to be the research. And, you know, if you think back to what this first workshop looked like, it was a lot of us just giving talks or presenting posters about our own research to other people.&lt;/p&gt;



&lt;p&gt;And, you know, I remember thinking at the poster session, like, the vibe was just so much different and better, healthier really than other poster sessions I had been to. Everyone was so supportive and encouraging, but it really was all about the research. I also remember being blown away just walking into that conference room in the morning and seeing all of these women gathered in one place and knowing that somehow, we had actually made this happen.&lt;/p&gt;



&lt;p&gt;Um, I remember we also faced some challenges with the workshop early on. What are the challenges that stand out to you most?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so a lot of people really got it, right. And they were super supportive. So, for example, folks at Penn totally got it, and they actually funded a bunch of that first workshop. But others in the community didn’t get it and didn’t see the point, didn’t see why it was necessary.&lt;/p&gt;



&lt;p&gt;I remember having dinner with one machine learning researcher and him telling me that he didn’t think this kind of workshop was necessary because women’s experiences were no different to men’s experiences. And then later on in the conversation, he talked about—like, you know, this is, like, an hour and a half later or something—he talked about how he and a friend of his had gone to the bar at an all-women’s college and he’d felt so awkward and out of place. And I ended up pointing out to him [LAUGHS] that he just, kind of, explained to himself why we needed WiML. So, yeah, there were some people who didn’t get it, and it took a lot of, sort of, talking to people and, kind of, explaining.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another challenge was figuring out how to fund it in an ongoing manner once we decided that we wanted to do this more than once.&lt;/p&gt;



&lt;p&gt;So as I said, Penn funded a lot of that first workshop, but that wasn’t a sustainable model, and it wasn’t going to be realistic for Penn to keep funding it. So in the end, we worked with Amy Greenwald to obtain a National Science Foundation grant that would cover a lot of costs, and we also received donations from other organizations.&lt;/p&gt;



&lt;p&gt;Um, a third challenge was figuring out where to hold the workshop given that we did want that focus to be on research. So the first two times, we held the workshop at the Grace Hopper conference, but we started to feel that that wasn’t really the right venue given that we wanted that focus to be on research. So we ended up moving it to NeurIPS, and this had a bunch of benefits, some of which I don’t think we’d even fully thought through when we made that decision.&lt;/p&gt;



&lt;p&gt;So one of the benefits was that attendees’ WiML travel funding—so we would give them this travel funding to enable them to pay the cost of attending WiML, stay in hotel rooms, all this kind of stuff—this would actually enable them to attend NeurIPS, as well, if we co-located with NeurIPS.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another main benefit was that we held WiML on the day before NeurIPS. So then throughout the rest of the conference, WiML attendees would see familiar faces throughout the crowd and wouldn’t necessarily feel so alone.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; So you’re talking about these challenges. How have these challenges changed over time? Or, you know, more broadly, can you talk about how the workshop and Women in Machine Learning as an organization as a whole, kind of, evolved over the years? I know that you served a term as the WiML president.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So it’s changed a lot. So first, obviously, most importantly, it evolved from being, kind of, this one-off event where we were just seeing what would happen to being really a robust organization. And the first step in that was creating the WiML board. And, as you just said, I served as the first president of that.&lt;/p&gt;



&lt;p&gt;But there have been a bunch of other steps since then. And one of the things I want to flag about the WiML board was that this was really important because the board members could focus on the long-term health of the organization and these, sort of, like, you know, things that spanned multiple years, like how to get sustainable funding sources, this kind of thing, versus the actual workshop organizers, who would focus on things like running the call for submissions and stuff like that. And being able to separate those roles made it really just reduce the burden on the workshop organizers meant that we could take this, kind of, longer-term perspective.&lt;/p&gt;



&lt;p&gt;Another really important step was becoming, &lt;em&gt;officially&lt;/em&gt; becoming a non-profit. So that happened a few years ago. And again, it was the natural thing to do at that point in time and just another step towards creating this, sort of, durable, robust organization.&lt;/p&gt;



&lt;p&gt;But it’s really taken on a life of its own. I’m honestly not super actively involved nowadays, which I think is fantastic. The organization doesn’t need me. That’s great. It’s also wild to me that because it’s been around for 20 years at this point that there are women in the field who don’t know what it’s like to &lt;em&gt;not&lt;/em&gt; have WiML.&lt;/p&gt;



&lt;p&gt;So a bunch of other affinity groups got created. So Timnit Gebru cofounded Black in AI when she was actually a postdoc at Microsoft Research New York City. So you and I got to actually see the founding of that affinity group up close. And then now there are a ton of other affinity groups. So there’s LatinX in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; there’s Queer in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Muslims in ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Indigenous in AI and ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, New In ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, just to name a few.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and all of these are growing, too, every year.&lt;/p&gt;



&lt;p&gt;You know, this year, WiML had over 400 submissions. They accepted 250 to be presented. It’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s wild.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, yep. And there’s going to be a WiML presence this year actually at all three of the NeurIPS venues. So there’s going to be a presence in Mexico City, in Copenhagen, and, of course, in San Diego for the main workshop. So it’s pretty great.&lt;/p&gt;



&lt;p&gt;And, you know, on top of that, I think the organization now, as you were saying, is able to do so much more than just the workshop alone. So for instance, WiML now runs this worldwide mentorship program for women and nonbinary individuals in machine learning, where they’re matched with a mentor and they can participate in these one-to-one mentoring meetings and seminars and panel discussions, which happens all throughout the year. I think they have about 50 mentors signing up each year, but I’m sure they could always use more. Um, so it’s just really amazing to look back and see how much the WiML community has done and how much it’s grown.&lt;/p&gt;



&lt;p&gt;And, you know, on the one hand, I think that honestly, like, founding WiML was one of the things that I’ve done over the course of my career, if not &lt;em&gt;the&lt;/em&gt; thing, that I am most proud of …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah, me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … to this day, but at the same time, like, we can’t take credit for any of this. It’s, like, a community effort.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; No.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s the community that has really kept us going …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … for the last 20 years,&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … so it’s great. Going to stop gushing now, but it’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; And it’s not just WiML that’s changed over the years. The entire industry has changed a ton, as well.&lt;/p&gt;



&lt;p&gt;How has your research evolved as a result of these changes to the entire field of AI and machine learning and also from your own change from academia to industry?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s a great question. You know, we’ve touched on this a little bit, but our research paths really evolved differently but ended up in these very similar places where we’re working on responsible AI, we’re advocating for interdisciplinary approaches, incorporating techniques from HCI, and so on. And I think that part of this was because of shifts of the community and also what’s happening in industry. Working in responsible AI in industry, there’s definitely not ever a shortage of interesting problems to solve, right.&lt;/p&gt;



&lt;p&gt;And I think that for both of us, our research interests in recent years really have been driven by these really practical challenges that we’re seeing. We were both involved early on in defining what responsible AI means within Microsoft, shaping our internal Responsible AI Standard&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. I led this internal companywide working group on AI transparency, which was focused both on model interpretability like we were talking about earlier but also other forms of transparency like data sheets for datasets and the transparency notes that Microsoft now releases with all of our products. And at the same time, you are leading this internal working group on fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, taking on that internal working group was, kind of, a big transition point in my career. You know, when I joined Microsoft, I was focusing on computational social science and I was also entirely doing research and wasn’t really that involved in stuff in the rest of the company.&lt;/p&gt;



&lt;p&gt;Then at the end of my first year at Microsoft, I attended the first Fairness, Accountability, and Transparency in Machine Learning workshop&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which was co-located with NeurIPS. It was one of the NeurIPS workshops. And I got really excited about that and thought, great, I’m going to spend like 20% of my time, maybe one day a week, doing research on topics in the space of fairness and accountability and transparency. Um, that is not what ended up happening.&lt;/p&gt;



&lt;p&gt;Over the next couple of years, I ended up doing more and more research on responsible AI, you know, as you said, on topics to do with fairness, to do with interpretability. And then in early 2018, I was asked to co-chair this internal working group on fairness, and that was the point where I started getting much more involved in responsible AI stuff across Microsoft, so outside of just Microsoft Research.&lt;/p&gt;



&lt;p&gt;And this was really exciting to me because responsible AI was so new, which meant that research had a really big role to play. It wasn’t like this was kind of an established area where folks in engineering and policy knew exactly what they were doing. And so that meant that I got to branch out from this very, sort of, research-focused work into much more applied work in collaboration with folks from policy, from engineering, and so on.&lt;/p&gt;



&lt;p&gt;Now, in fact, as well as being a researcher, I actually run a small applied science team, the Sociotechnical Alignment Center, or STAC for short, within Microsoft Research that focuses specifically on bridging research and practice in responsible AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. Do you think that your involvement in WiML has played a role in this work?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, definitely. [LAUGHS] Yeah, without a doubt. So particularly when working on topics related to fairness, I’ve ended up focusing a bunch on stuff to do with marginalized groups as part of my responsible AI work.&lt;/p&gt;



&lt;p&gt;So there’s been this, sort of, you know, focus on marginalized groups, particularly women, in the context of machine learning and with my WiML, kind of, work and then in my research work thinking about fairness, as well.&lt;/p&gt;



&lt;p&gt;The other way that WiML has really, sort of, affected what I do is that I work with a much more varied group of people nowadays than I did back when I was just focusing on, kind of, machine learning, computational social science, and stuff like that. And many of my collaborators are people that I’ve met through WiML over the years.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; And, of course, there has been another big shift within industry recently, which is just all the excitement around generative AI. Can you say a bit about how that has changed your research?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; OK, yeah. So this is another big one. There are so many ways that this changed my work. One of the biggest ways, though, is that generative AI systems are now everywhere. They’re being used all over the place for all kinds of things. And, you know, you see all these news headlines about GenAI systems, you know, diagnosing illnesses, solving math problems, and writing code, stuff like that. And also headlines about various different risks that can occur when you’re using generative AI. So fabricating facts, memorizing copyrighted data, generating harmful content, you know, these kinds of things. And with all this attention, it’s really natural to ask, what is the evidence behind these claims? So where is this evidence coming from, and should we trust it?&lt;/p&gt;



&lt;p&gt;It turns out that much of the evidence comes from GenAI evaluations that involve measuring the capabilities, the behaviors, and the impacts of GenAI systems, but the current evaluation practices that are often used in the space don’t really have as much scientific rigor as we would like, and that’s, kind of, a problem.&lt;/p&gt;



&lt;p&gt;So one of the biggest challenges is that the concepts of interest when people are, sort of, doing these GenAI evaluations—so things like diagnostic ability, memorization, harmful content, concepts like that—are much more abstract than the concepts like prediction accuracy that underpinned machine learning evaluations before the generative AI era.&lt;/p&gt;



&lt;p&gt;And when we look at these new concepts that we need to be able to focus on in order to evaluate GenAI systems, we see that they’re actually much more reminiscent of these abstract contested concepts—these, kind of, fuzzy, squishy concepts—that are studied in the social sciences. So things like democracy and political science or personality traits and psychometrics. So there’s really that, sort of, connection there to these, kind of, squishier things.&lt;/p&gt;



&lt;p&gt;So when I was focusing primarily on computational social science, most of my work was focused on developing machine learning methods to help social scientists measure abstract contested concepts. So then when GenAI started to be a big thing and I saw all of these evaluative claims involving measurements of abstract concepts, it seemed super clear to me that if we were going to actually be able to make meaningful claims about what AI can do and can’t do, we’re going to need to take a different approach to GenAI evaluation.&lt;/p&gt;



&lt;p&gt;And so I ended up, sort of, drawing on my computational social science work around measurement and I started advocating for adopting a variant of the framework that social scientists use for measuring abstract contested concepts. And my reason for doing this was that I believed—I &lt;em&gt;still&lt;/em&gt; believe—that this is an important way to improve the scientific rigor of GenAI evaluations.&lt;/p&gt;



&lt;p&gt;You know all of this, of course, because you and I, along with a &lt;em&gt;bunch&lt;/em&gt; of other collaborators at Microsoft Research and Stanford and the University of Michigan published a position paper on this framework entitled “Evaluating GenAI Systems is a Social Science Measurement Challenge” at ICML [International Conference on Machine Learning] this past summer.&lt;/p&gt;



&lt;p&gt;What are you excited about at the moment?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so lately, I have been spending a lot of time thinking about AI and critical thought: how can we design AI systems to support appropriate reliance, preserve human agency, and really encourage critical engagement on the part of the human, right?&lt;/p&gt;



&lt;p&gt;So this is an area where I think we actually have a huge opportunity, but there are also huge risks. If I think about my most optimistic possible vision of the future of AI —which is not something that is easy for me to do, as I’m not a natural optimist, as you know—it would be a future in which AI helps people grow and flourish, in which it, kind of, enriches our own &lt;em&gt;human&lt;/em&gt; capabilities. It deepens our own &lt;em&gt;human&lt;/em&gt; thinking and safeguards our own agency.&lt;/p&gt;



&lt;p&gt;So in this future, you know, we could build AI systems that actually help us brainstorm and learn new knowledge and skills, both in formal educational settings and in our day-to-day work, as well. But I think we’re not going to achieve this future by default. It’s something that we really need to design for if we want to get there.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; You mentioned that there are risks. What are the risks that you can see here?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, there’s so much at stake here. You know, in the short term, there are things like overreliance—depending on the output of an AI system even when the system’s wrong. This is something that I’ve worked on a bunch myself. There’s a risk of loss of agency or the ability to make and execute independent decisions and to ensure that our outcomes of AI systems are aligned with personal or professional values of the humans who are using those systems. This is something that I’ve been looking at recently in the context of AI tools for journalism&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. There’s diminished innovation, by which I mean a loss of creativity or diversity of ideas.&lt;/p&gt;



&lt;p&gt;You know, longer term, we risk atrophied skills—people just losing or simply never developing helpful skills for their career or their life because of prolonged use of AI systems. The famous example that people often bring up here is pilots losing the ability to perform certain actions in flight because of dependence on autopilot systems. And I think we’re already starting to see the same sort of thing happen across all sorts of fields because of AI.&lt;/p&gt;



&lt;p&gt;And, you know, finally, another risk that I’ll mention that seems to resonate with a lot of folks I talk to is what I would just call loss of joy, right. What happens when we are delegating to AI systems the parts of our activities that we really take pleasure and find this satisfaction in doing ourselves.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So then as a community, what should we be doing if we’re worried about these risks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I mean, I think this is going to have to be a big community effort if we want to achieve this. This is a big goal. But there are a few places I think we especially need work.&lt;/p&gt;



&lt;p&gt;So I think we need generalized principles and practices for AI system builders for how they can build AI systems in ways that promote human agency and encourage critical thought. We also need principles and practices for system &lt;em&gt;users&lt;/em&gt;. So how do we teach the general population to use AI in ways that amplify their skills and capabilities and help them learn new things?&lt;/p&gt;



&lt;p&gt;And then, you know, close to your heart, I’m sure, I think that we need more work on measurement and evaluation, right. We are once again back to these squishy human properties.&lt;/p&gt;



&lt;p&gt;You know, I mentioned I’ve done some work on overreliance in generative AI systems, and I started there because on the grand scale of risks here, overreliance is something that is relatively easy to measure, at least in the short term. But how do we start thinking about measuring people’s critical thinking when using AI across all sorts of contexts and at scale and over long-time horizons, right? How do we measure the, sort of, longitudinal effect of AI systems just on our critical thought as a population?&lt;/p&gt;



&lt;p&gt;And by the way, if anyone listening is going to be at the WiML workshop, I’ll actually be giving a keynote on this topic. And this is something I’m just incredibly excited about because first, I’m incredibly excited about this topic, but also, in the whole 20 years of WiML, I’ve given opening remarks and similar several times, but this is actually the very first time that I will be talking about my own research there. So this is like my dream. I’m thrilled that this is happening.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s awesome. Oh, that’s so exciting. Excellent.&lt;/p&gt;



&lt;p&gt;So one last question for you. If you could go back and talk to yourself 20 years ago and give yourself some advice, what would you say?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, OK, I’ve thought about this one a bit over the past week, and there are three things here I want to mention.&lt;/p&gt;



&lt;p&gt;So first, I would tell myself to be brave about speaking up. You know I’m about as introverted as it gets and I’m naturally very shy, and this has always held me back. It still holds me back now. It was really embarrassingly late in my career that I decided to do something about this and start to develop strategies to help myself speak up more. And eventually, it started to grow into something that’s a little bit more natural.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; What kind of, um, what kind of strategies?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so you know, one example is I use a lot of notes. For this podcast, I have a lot of notes here. I’m a big notes person, and things like that really help me.&lt;/p&gt;



&lt;p&gt;The second thing that I would tell myself is to, you know, work on the problems that you really want to see solved. As researchers, we have this amazing freedom to choose our own direction. And early on, you know, a lot of the problems that I worked on were problems that I really enjoyed thinking about on a day-to-day basis. It was a lot of fun. They were like little math puzzles to me. But I often found that, you know, when I would be at conferences and people would ask me about my work, I didn’t really want to talk about these problems. I just in some sense, you know, I had fun doing it, but I didn’t really care. I wasn’t passionate about it. I didn’t care that I had solved the problem.&lt;/p&gt;



&lt;p&gt;And so once, many years ago now, when I was thinking about my research agenda, I got some good advice from our former lab director, Jennifer Chayes, who suggested that I go through my recent projects and sort them into projects where I really &lt;em&gt;liked&lt;/em&gt; working on them—it was a fun experience day-to-day—and projects that I liked talking about after the fact and, kind of, felt good about the results and then see where the overlap is. And this is something that, like, it kind of sounds, kind of, obvious when I say it now, but at the time, it was really eye-opening for me.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s so cool. And now I, kind of, want to do that with all of my projects, particularly at the moment. I actually just took five months, as you know, five months off of work for parental leave because I just had a baby. And so I’m, sort of, taking a big, kind of, inventory of everything as I get back into all of this now, and I love this idea. I think this is really cool.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s changed really my whole approach to research. Like, you know, we were talking about this, but most of the work I do now is more HCI than machine learning because I found that the problems that really motivate me, that I want to be talking to people about at conferences, are the &lt;em&gt;people&lt;/em&gt; problems.&lt;/p&gt;



&lt;p&gt;The third piece of advice I would give myself is that you should bring more people into your work, right.&lt;/p&gt;



&lt;p&gt;So there’s this kind of vision on the outside of research being this solo endeavor, and it can feel so competitive at times, right. We all feel this. But time and time again, I’ve seen that the best research comes from collaborations and from bringing people together with diverse perspectives who can challenge each other in a way that is respectful but makes the work better.&lt;/p&gt;



&lt;p&gt;Is there advice that you would give to your former self of 20 years ago?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. OK. So I’ve also been thinking about this a bunch over the past week. There’s actually a lot of advice I think I would give my former self, [LAUGHS] but there are three things that I keep coming back to.&lt;/p&gt;



&lt;p&gt;OK, so first—and this is similar to your second point—push for doing the work that you find to be most fulfilling even if that means taking a nontraditional path. So in my case, I’ve always been interested in the social sciences. Back when I was a student, you know, even when I was a PhD student, doing research that combined computer science &lt;em&gt;and&lt;/em&gt; the social sciences just wasn’t really a thing. And so as a result, it would have been really easy for me to just be like, “Oh well, I guess that isn’t possible. I’ll just focus on traditional computer science problems.”&lt;/p&gt;



&lt;p&gt;But that’s not what I ended up doing. Instead, and often in ways that made my career, kind of, harder than it probably would have been otherwise, I ended up pushing. I kept pushing, and in fact, I keep pushing, even nowadays, to bring these things together—computer science and the social sciences—in an interdisciplinary fashion. And this hasn’t been easy. But cumulatively, the effect has been that I’ve been able to do much more impactful work than I think I would have been able to do otherwise, and the work I’ve done, I’ve just enjoyed so much more than would otherwise have been the case.&lt;/p&gt;



&lt;p&gt;OK, so second, be brave and share your work. So this is actually advice for my current self and my former self, as this is something that I definitely still struggle with.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; As do I, you know, and actually, I think it’s funny to hear you say this because I would say that you are much better at this than I am.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I still, I think I have a lot of work to do on this one. Yeah, it’s hard. It’s really hard.&lt;/p&gt;



&lt;p&gt;As you know, I am a perfectionist, and this is good in some ways, but this is also bad in other ways. And one way in which this is bad is that I tend to be really anxious about sharing and publicizing my work, especially when I feel it’s not perfect.&lt;/p&gt;



&lt;p&gt;So as an example, I wrote this massive tutorial on computational social science for ICML in 2015, but I never put the slides … and I wrote a whole script for it … I never put the slides or the script online as a resource for others because I felt it needed more work. And I actually went back and looked at it earlier this year, when we were working on the ICML paper, and I was stunned because it’s great. Why didn’t I put this online? All these things that I thought were problems 10 years ago, no, they’re not a big deal. I should have just shared it.&lt;/p&gt;



&lt;p&gt;As another example, STAC, my applied science team, was using LLMs as part of our approach to GenAI evaluation back in 2022, way before the sort of “LLM-as-a-judge” paradigm was widespread. But I was really worried that others would think negatively of us for doing this, so we didn’t share that much about what we were doing, and I regret that because we missed out on an opportunity to kick off an industrywide discussion about this, sort of, LLM-as-a-judge paradigm.&lt;/p&gt;



&lt;p&gt;OK, so then my third point is that the social side of research is just as valuable as the technical side. And by this, I’m actually not talking about social science and computer science. I actually mean that the &lt;em&gt;how&lt;/em&gt; of doing research, including &lt;em&gt;who&lt;/em&gt; you talk to, &lt;em&gt;who&lt;/em&gt; you collaborate with, and &lt;em&gt;how&lt;/em&gt; you approach those interactions, is just as important as the research itself.&lt;/p&gt;



&lt;p&gt;As a PhD student, I felt really bad about spending time socializing with other researchers, especially at conferences, because I thought that I was supposed to be listening to talks, reading papers, and discussing technical topics with researchers and not socializing. But in hindsight, I think that was wrong. Many of those social connections have ended up being incredibly valuable to my research, both because I’ve ended up collaborating with and in some cases even hiring the people who I first got to know socially …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … but also because the friendships that I’ve built, like our friendship, for example, have served as a crucial support network over the years, especially when things have felt particularly challenging.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, absolutely. I agree with all of that so much.&lt;/p&gt;



&lt;p&gt;And with that, I will say thank you so much for doing this podcast with me today.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Thank &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It was a lot of fun to reflect on the last 20 years of WiML, but also the last 20 years of our careers and friendship and all of this, so it’s great, and I never would have agreed to do this if it had been with anyone but you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Likewise. [LAUGHS]&lt;/p&gt;



&lt;p&gt;So thank you, everybody, for listening to us, and hopefully some of you will join for the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which is taking place on Dec. 2. And of course, Jenn and I will both be there in person. We’ll also be at NeurIPS afterwards. So feel free to reach out to us if you want to chat with us or to learn more about anything that we covered here today.&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OUTRO:&lt;/strong&gt; You’ve been listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast. Find more episodes of the podcast at aka.ms/researchpodcast&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1] Wallach later clarified that the number of registrants for the 2005 Conference on Neural Information Processing Systems was around 900.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/HannaJenn-Ideas_TW_LI_FB_1200x627-1.jpg" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;






&lt;p&gt;Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series &lt;em&gt;Ideas&lt;/em&gt;, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&lt;/p&gt;



&lt;p&gt;In 2006, three PhD students organized the Women in Machine Learning Workshop, or&amp;nbsp;&lt;em&gt;WiML&lt;/em&gt;, to provide a space for women in ML to connect and share their research. The event has been held every year since,&amp;nbsp;growing in size&amp;nbsp;and mission.&lt;/p&gt;



&lt;p&gt;In this episode, two of the WiML cofounders, Jenn Wortman Vaughan, a Microsoft senior principal research manager, and Hanna Wallach, a Microsoft vice president and distinguished scientist, reflect on the 20th workshop. They discuss WiML’s journey from a potential one-off event to a nonprofit supporting women and nonbinary individuals worldwide; their friendship and collaborations, including their contributions to defining responsible AI at Microsoft; and the advice they’d give their younger selves.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;








&lt;/div&gt;



&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SERIES INTRODUCTION:&lt;/strong&gt; You’re listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code. In this series, we’ll explore the technologies that are shaping our future and the big ideas that propel them forward.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;JENN WORTMAN VAUGHAN:&lt;/strong&gt; Hello, and welcome. I’m Jenn Wortman Vaughan. This week, machine learning researchers around the world will be attending the annual Conference on Neural Information Processing Systems, or NeurIPS. I am especially excited about NeurIPS this year because of a co-located event, the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, or WiML, which I am going to be attending both as a mentor and as a keynote speaker.&lt;/p&gt;



&lt;p&gt;So to celebrate 20 years of WiML, I’m here today with my long-term collaborator, colleague, close friend, &lt;em&gt;and&lt;/em&gt; my cofounder of the workshop for Women in Machine Learning, Hanna Wallach.&lt;/p&gt;



&lt;p&gt;You know, you and I have known each other for a very long time at this point. And in many ways, we followed very parallel and often intersecting paths before we both ended up here working in responsible AI at Microsoft. So I thought it might be fun to kick off this podcast with a bit of the story of our interleaving trajectories.&lt;/p&gt;



&lt;p&gt;So let’s start way back 20 years ago, around the time we first had the idea for WiML. Where were you, and what were you up to?&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;&lt;strong&gt;HANNA WALLACH:&lt;/strong&gt; Yeah, so I was a PhD student at the University of Cambridge, and I was working with the late David MacKay. I was focusing on machine learning for analyzing text, and at that point in time, I’d actually just begun working on Bayesian latent variable models for text analysis, and my research was really focusing on trying to combine ideas from &lt;em&gt;n&lt;/em&gt;-gram language modeling with statistical topic modeling in order to come up with models that just did a better job at modeling text.&lt;/p&gt;



&lt;p&gt;I was also doing this super-weird two-country thing. So I was doing my PhD at Cambridge, but at the end of the first year of my PhD, I spent three months as a visiting graduate student at the University of Pennsylvania, and I loved it, so much so that at the end of the three months I said, can I extend for a full year? Cambridge said yes; Penn said yes. So I did that and actually ended up then extending another year and then another year and another year and so on and so forth.&lt;/p&gt;



&lt;p&gt;But during my first full year at Penn, that was when I met &lt;em&gt;you&lt;/em&gt;, and it was at the visiting students weekend, and I had been told by the faculty in the department that I had to work really hard on recruiting you. I had no idea that that was actually going to be the start of a 20-plus-year friendship.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I still remember that visiting weekend very well. I actually met you; I met my husband, Jeff; and I met my PhD advisor, Michael Kearns, all on the same day at that visiting student weekend. So I didn’t know it at the time, but it was a very big day for me.&lt;/p&gt;



&lt;p&gt;So around that time when I started my PhD at Penn, I was working in machine learning theory and algorithmic economics. So even then, you know, just like I am now, I was interested in the intersection of people and AI systems. But since my training was in theory, my “people” tended to be these mathematically ideal people with these well-defined preferences and beliefs who behaved in very well-defined ways.&lt;/p&gt;



&lt;p&gt;Working in learning theory like this was appealing to me because it was very neat and precise. There was just none of the mess of the real world. You could just write down your model, which contained all of your assumptions, and everything else that followed from there was in some sense objective.&lt;/p&gt;



&lt;p&gt;So I was really enjoying this work, and I was also so excited to have you around the department at the time. You know, honestly, I also loved Penn. It was just such a great environment. I was just actually back there a few weeks ago, visiting to give a talk. I had an amazing time. But it was, I will say, very male dominated in the computer science department at the time. In my incoming class of PhD students, we had 20 incoming PhDs, and I was the only woman there. But we managed to build a community. We had our weekly ladies brunch, which I loved, and things like that really kept me going during my PhD.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, I loved that ladies brunch. That made a huge difference to me and, kind of, kept me going through the PhD, as well.&lt;/p&gt;



&lt;p&gt;And, like you, I’d always been interested in people. And during the course of my PhD, I realized that I wasn’t interested in analyzing text for the sake of text, right. I was interested because text is one of these ways that people communicate with each other. You know, people don’t write text for the sake of writing text. They write it because they’re trying to convey something. And it was really &lt;em&gt;that&lt;/em&gt; that I was interested in. It was these, kind of, social aspects of text that I found super interesting.&lt;/p&gt;



&lt;p&gt;So coming out of the PhD, I then got a postdoc job focused on analyzing texts as part of these, sort of, broader social processes. From there, I ended up getting a faculty job, also at UMass, as one of four founding members of UMass’s Computational Social Science Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. So there was me in computer science, then there was another assistant professor in statistics, another in political science, and another in sociology. And in many ways, this was my dream job. I was being paid to develop and use machine learning methods to study social processes and answer questions that social scientists wanted to study. It was pretty awesome. You, I think, started a faculty position at the same time, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I also did a postdoc. First, I spent a year as a postdoc at Harvard, which was super fun. And then I started a tenure track position in computer science at UCLA in 2010.&lt;/p&gt;



&lt;p&gt;Again, you know, it was a very male-dominated environment. My department was mostly men. But maybe even more importantly than this, I just didn’t really have a network there. You know, it was lonely. One exception to this was Mihaela van der Schaar. She was at UCLA at the time, though not in my department, and she, kind of, took me under her wing. So I’m very grateful that I had that support. But overall, this position just wasn’t a great fit for me, and I was under more stress then than I think I have been at any other point in my life that I could really remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So at that point, then, you ended up transitioning to Microsoft Research, right?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Why did you end up choosing MSR [Microsoft Research]?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so this was back in 2012. MSR had just opened up this &lt;em&gt;new&lt;/em&gt; New York City lab at the time, and working in this lab was basically my dream job. I think I actually tried to apply before they had even officially opened the lab, like when I just heard it was happening.&lt;/p&gt;



&lt;p&gt;So this lab focused in three areas at the time. It focused in machine learning, algorithmic economics, and computational social science. And my research at the time cut across all three of these areas. So it felt just like this perfect opportunity to work in the space where my work would fit in so well and be really appreciated.&lt;/p&gt;



&lt;p&gt;The algorithmic economics group at the time actually was working on building prediction markets to aggregate information about future events, and they were already, in doing this, building on top of some of my theoretical research, which was just super cool to see. So that was exciting. And I already knew a couple of people here. I knew John Langford and Dave Pennock, who was in the economics group at the time, because I’d done an internship actually with the two of them at Yahoo Research before they came to Microsoft. And I was really excited to come back and work with them again, as well.&lt;/p&gt;



&lt;p&gt;You know, even here at the time that I joined the lab, it was 13 men and me. So once again, not great numbers. And I think that in some ways this was especially hard on me because I was just naturally, like, a very shy person and I hadn’t really built up the confidence that I should have at that point in my career. But on the other hand, I found the research fit just so spot-on that I couldn’t say no. And I suspect that this is something that you understand yourself because you actually came and joined me here in the New York lab a year or two later. So why did you make this switch?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so I anticipated that I was going to love my faculty job. It was focusing on all this stuff that I was so excited about. And much to my surprise, though, I kind of didn’t. And it wasn’t like there was any one particular thing that I didn’t like. It was more of a mixture of things. I did love my research, though. That was pretty clear to me. &lt;em&gt;But&lt;/em&gt; I wasn’t happy. So I spent a summer talking to as many people as possible in all different kinds of jobs, really just with the goal of figuring out what their day-to-day lives looked like. You were one of the people I spoke to, but I spoke to a ton of other people, as well.&lt;/p&gt;



&lt;p&gt;And from doing that, at the end of that summer, I ended up deciding to apply to industry jobs, and I applied to a bunch of places and got a bunch of offers. But I ended up deciding to join Microsoft Research New York City because of all the places I was considering going, they were the only place that said, “We love your research. We love what you do. Do you want to come here and do that same research?”&lt;/p&gt;



&lt;p&gt;And that was really appealing to me because I loved my research. Of course, I wanted to come there and do my same research and especially with all of these amazing people like you, Duncan Watts, who’d for many years been somebody I’d really looked up to. He was there, as well, at that point in time. There was this real focus on computational social science but with a little bit more of an industry perspective. There were also these amazing machine learning researchers. Just for many of the same reasons as you, I was just really excited to join that lab and particularly excited to be working in the same organization as you again.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I’m happy to take at least a little bit of the credit for …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … recruiting you to Microsoft here many years ago.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. I was really excited to have you join, too, though I think the timing actually worked out so that I missed your first couple of months because I was on maternity leave with my first daughter at the time. I should say I’ve got two daughters, and I’m very proud to share in the context of this podcast that they’re both very interested in math and reading, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, they’re both great.&lt;/p&gt;



&lt;p&gt;Um, so then we ended up working in the same place. But despite that, it still took us several years to end up actually collaborating on research. Do you remember how we ended up working together?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So I used to tell this story a lot. Actually, I was at this panel on AI in society back in, I think, it was probably 2016. It was taking place in DC. And someone on this panel made this statement that soon our AI systems are just going to be so good that all of the uncertainty is going to be taken out of our decision-making, and something about this statement just, like, really set me off. I got so mad about it because I thought it was just …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I remember.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … such an irresponsible thing to be saying. So I came back to New York, and I think I was ranting to you about this in the lab, and this conversation ended up getting us started on this whole longer discussion about the importance of communicating uncertainty and about explaining the assumptions that are behind the predictions that you’re making and all of this.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So this was something … I was really excited about this because this was something that had really been drummed into me for years as a Bayesian. So Bayesian statistics, which forms a lot of the foundation of the type of machine learning that I was doing, is all about explicitly stating assumptions and quantifying uncertainty. So I just felt super strongly about this stuff.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. So somehow all of these discussions we were having led us to read up on this literature that was coming out of the machine learning community on interpretability at the time. There are a bunch of these papers coming out that were making claims about models being interpretable without stopping to define &lt;em&gt;who&lt;/em&gt; they were interpretable to or for what purpose. Never &lt;em&gt;actually&lt;/em&gt; taking these models and putting them down in front of real people. And we wanted to do something about this. So we started running controlled experiments with &lt;em&gt;real&lt;/em&gt; people and found that we often can’t trust our intuition about what makes a model interpretable.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, one of the things that came up a lot in that work was, sort of, how to measure these squishy abstract human concepts, like &lt;em&gt;interpretability&lt;/em&gt;, that are really hard to define, let alone quantify and measure and stuff like that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Absolutely. So I think one of the first things that we really struggled with in this line of work was what it even means to be &lt;em&gt;interpretable&lt;/em&gt; or &lt;em&gt;intelligible&lt;/em&gt; or any of these terms that were getting thrown around at the time.&lt;/p&gt;



&lt;p&gt;Um, we ended up doing some research, which is still one of my favorite papers, …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … with our colleagues Forough Poursabzi, Jake Hofman, and Dan Goldstein. And in this work, we found it really useful to think about interpretability as a latent property that can be, kind of, influenced by different properties of a model or system’s design. So things like the number of features the model has or whether the model’s linear or even things like the user interface of the model.&lt;/p&gt;



&lt;p&gt;This was kind of a gateway project for me in the sense that it’s one of the first projects that I got really excited about that was more of a human-computer interaction, or HCI, project rather than a theory project like I’d been working on in the past. And it just set off this huge spark of excitement in me. It felt to me at the time more important than other things that I was doing, and I just wanted to do more and more of this work.&lt;/p&gt;



&lt;p&gt;I would say the other project that had a really similar effect on me, which we also worked on together right around the same time, was our work with Ken Holstein mapping out challenges that industry practitioners were facing in the space of AI fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah. OK, yep. That project, that was so fun, and I learned so much from it. If I recall correctly, we originally hired Ken, who I think was an HCI PhD student at CMU at the time, as an intern …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … to work with us on creating, sort of, user experiences for fairness tools like the Fairlearn toolkit&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And we started that project—so that was in collaboration with Miro Dudík and Hal Daumé—we started that project by having Ken talk to a whole bunch of practitioners at Microsoft but at other organizations, as well, to get a sense for how they were and weren’t using fairness toolkits like Fairlearn.&lt;/p&gt;



&lt;p&gt;And I want to point out that at that point in time, the academic research community was super focused on all of these, like, simple quantitative metrics for assessing the fairness in the context of predictions and predictive machine learning models with this, kind of, understanding that these tools could then be built to help practitioners assess the fairness of their predictive models and maybe even make fairer predictions. And so that’s the kind of stuff that this Fairlearn toolkit was originally developed to do. So we ended up asking all of these practitioners originally just as, sort of, the precursor to what we thought we were going to end up doing with this project.&lt;/p&gt;



&lt;p&gt;We also asked these practitioners about their current practices and challenges around fairness in their work and about their additional needs for support. So where did they feel like they had the right tools and processes and practices and where did they feel like they were missing stuff. And this was really eye-opening because what we found was so different than what we were expecting. And there’s two things that really stood out to us.&lt;/p&gt;



&lt;p&gt;So the first thing was that we found a much, much wider range of applications beyond prediction. So we’d come into this assuming that all these practitioners were doing stuff with predictive machine learning models, but in fact, we were finding they were doing all kinds of stuff. There was a bunch of unsupervised stuff; there was a bunch of, you know, language-based stuff—all of this kind of thing. And in hindsight, that probably doesn’t sound very surprising nowadays because of the rise of generative AI, and really the entire machine learning and AI field is much less focused on prediction in that, kind of, narrow, kind of, classification-regression kind of way. But at the time, this was really surprising, especially in light of the academic literature’s focus on predictions when thinking about fairness.&lt;/p&gt;



&lt;p&gt;The second thing that we found was that practitioners often struggled to use existing fairness research, in part because these quantitative metrics that were all the rage at that point in time, just weren’t really amenable to the types of real-world complex scenarios that these practitioners were facing. And there was a bunch of different reasons for this, but one of the things that really stood out to us was that this wasn’t so much about the underlying models and stuff like that, but it was actually that there were a variety of data challenges involved here around things like data collection, collection of sensitive attributes, which you need in order to actually use these fairness metrics.&lt;/p&gt;



&lt;p&gt;So putting all this together, the upshot of all this was that we never did what we originally set out to do with that [LAUGHS], that internship project. We … because we uncovered this really large gap between research and practice, we ended up publishing this paper that characterized this gap and then surfaced important directions for future research. The other thing that the paper did was emphasize the importance of doing this kind of qualitative work to actually understand what’s happening in practice rather than just making assumptions about what practitioners are and aren’t doing.&lt;/p&gt;



&lt;p&gt;The other thing that came out of it, of course, was that the four of us—so you, me, Miro and Hal—learned a ton about HCI and about qualitative research from Ken, which was just, uh, so fun.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and I started to be confronted with the fact that I could no longer reasonably ignore all of these messes of the real world because, you know, in some ways, responsible AI is really all about the messes.&lt;/p&gt;



&lt;p&gt;So I think this project was really a big shift for both of us. And in some ways, working on this and the interpretability work really led us to be active in these early efforts that were happening within Microsoft in the responsible AI space. Um, the research that we were doing was feeding directly into company policy, and it felt like it was just, like, a huge place where we could have some impact. So it was very exciting.&lt;/p&gt;



&lt;p&gt;So switching gears a bit. Hanna, do you remember how we first got the idea for WiML?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, I do. So we were at NeurIPS. This was back in 2005. It was a … so NeurIPS was a very different conference back then. Now it’s like tens of thousands of people. It’s held in a massive convention center. Yes, there are researchers there, but there’s a variety of people from across the tech industry who attend, but that is &lt;em&gt;not&lt;/em&gt; what it was like back then.&lt;/p&gt;



&lt;p&gt;So in around … in 2005, it was more like 600 people thereabouts in total[1], and the main conference would be held every year in Vancouver, and then everybody at the conference would pile onto these buses, and we would all head up to Whistler for the workshops.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So super different to what’s happening nowadays. It was my third time. I think that’s right. I think it was my third time attending the conference. But it was my first time sharing a hotel room with other women. And I remember up at the workshops, up in Whistler, there were five of us sitting around in a hotel room, and we were talking about how amazing it was that there were five of us sitting around talking, &lt;em&gt;women&lt;/em&gt;. And we, kind of, couldn’t believe there were five of us. We’re all PhD students at the time. And so we decided to make this list, and we started trying to figure out who the other women in machine learning were. And we came up with about 10 names, and we were kind of amazed that there were even 10 women in machine learning. We thought this was a &lt;em&gt;huge&lt;/em&gt; number. We were very excited. And we started talking about how it might be really fun to just bring them all together sometime.&lt;/p&gt;



&lt;p&gt;So we returned from NeurIPS, and you and I ended up getting lunch to strategize. I still remember walking out of the department together to go get lunch and you were walking ahead of me. I can visualize the coat you were wearing as you were walking in front of me. And so we strategized a bit and ended up deciding, along with one of the other women, Lisa Wainer, to submit a proposal to the Grace Hopper conference for a session in which women in machine learning would give short talks about their research.&lt;/p&gt;



&lt;p&gt;We reached out to the 10 names that we had written down in the hotel room and through that process actually ended up finding out about more women in machine learning and eventually had something like 25 women listed on the final proposal. I think there’s an email somewhere where one or another of us is saying to the other one, “Oh my gosh! I can’t believe there are so many women in machine learning.”&lt;/p&gt;



&lt;p&gt;So we submitted this proposal, and ultimately, the proposal was rejected by the Grace Hopper conference. But we were so excited about the idea and just really invested in it by that point that we decided to hold our own co-located event the day before the Grace Hopper conference. And I’ve got to say, you know, 20 years later, I don’t know &lt;em&gt;what&lt;/em&gt; we were thinking. Like, that was a bold move on the part of three PhD students. And it turned out to be a huge amount of work that we had to do entirely ourselves, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; We had no idea what we were doing. But the Grace Hopper folks very nicely connected us with the venue that the conference was going to be held at, and somehow, we managed to pull it off. Ultimately, that first workshop had around 100 women, and there was this … rather than just, like, a single short session, which is what we’d originally had in mind, we had this full day’s worth of talks. I actually have the booklet of abstracts from all of those talks at my desk in the office. I still have that today. And it was just an amazing experience.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, it was. And, you know, you mentioned how bold we were. I just, I really don’t think that any of us at the time realized how bold we were being here, getting this workshop rejected and then saying, you know, &lt;em&gt;no&lt;/em&gt;, we think this is important. We’re going to do it anyway. &lt;em&gt;On our own. As grad students.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;So I’ve already talked a little bit about some of the spaces that I was in throughout my career where there just weren’t a lot of women around in the room with me. How had you experienced a lack of community or network of women in machine learning before the founding of WiML? And, you know, why do you think it’s important to have that kind of community?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So I felt it in a number of different ways. I think I mentioned a few minutes ago that, like, it was my third time at NeurIPS but my first time sharing a hotel room with another woman. But there were many places over the years where I’d felt this.&lt;/p&gt;



&lt;p&gt;So first, as an undergraduate. Then, I did a lot of free and open-source software development, and I was pretty involved in stuff to do with the Debian Linux distribution. And back then, the percentage of women involved in free and open-source software development was about 1 percent, 1.5%&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and the percentage involved actually in Debian was even less than that. So that had led me and some others to start this Debian Women Project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then, again, of course, I faced this in machine learning.&lt;/p&gt;



&lt;p&gt;I just didn’t know that many other women in machine learning. I didn’t … there weren’t a large number of senior women, for example, to look up to as role models. There weren’t a large number of female PhD students. And this, kind of, made me sad because I was really excited about machine learning, and I hoped to spend my entire career in it. But because I didn’t see so many other women around, particularly more senior women, that really made me question whether that would even be possible, and I just didn’t know.&lt;/p&gt;



&lt;p&gt;Um, I think, you know, thinking about this, and I’ve obviously reflected on this a lot over the years, but I think having a diverse community in any area, be it free and open-source software development, be it machine learning, any of these kinds of things, is just so important for so many reasons. And some of those reasons are little things like finding people that you would feel comfortable sharing a hotel room with.&lt;/p&gt;



&lt;p&gt;But many of these things are bigger things that can then have, like, even, kind of, knock-on cumulative effects. Like feeling valued in the community, feeling welcome in the community, having role models, being able to, sort of, see people and say, “Oh, I want to be kind of like that person when I grow up; I could do this.” And then even just representation of different perspectives in the work itself is so important.&lt;/p&gt;



&lt;p&gt;The flip side of that is that there are a whole bunch of things that can go wrong if you don’t have a diverse community. You can end up with gatekeeping, with toxic or unsafe cultures, obviously attrition as people just leave these kinds of spaces because they feel that they’re not welcome there and won’t be valued there. And then to that point of having representation of different perspectives, with a really homogeneous community, you can end up with, kind of, blind spots around the technology itself, which can then lead to harms.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; 100%. So did you ever imagine during&amp;nbsp;all of&amp;nbsp;this that WiML would still be around 20 years later and we would be sitting here on a podcast talking about this?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; [LAUGHS] No, absolutely not. I didn’t even think that WiML would necessarily be around for a second year. I thought it was probably going to be, like, a one-off event. And I certainly don’t think that I thought that I would still be involved in the machine learning community 20 years later, as well. So very unexpected.&lt;/p&gt;



&lt;p&gt;I’ve got a question for you, though. What do you remember most about that first workshop?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; I remember a lot of things. I remember that, you know, when we were planning this, we always really wanted the focus to be the research. And, you know, if you think back to what this first workshop looked like, it was a lot of us just giving talks or presenting posters about our own research to other people.&lt;/p&gt;



&lt;p&gt;And, you know, I remember thinking at the poster session, like, the vibe was just so much different and better, healthier really than other poster sessions I had been to. Everyone was so supportive and encouraging, but it really was all about the research. I also remember being blown away just walking into that conference room in the morning and seeing all of these women gathered in one place and knowing that somehow, we had actually made this happen.&lt;/p&gt;



&lt;p&gt;Um, I remember we also faced some challenges with the workshop early on. What are the challenges that stand out to you most?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, so a lot of people really got it, right. And they were super supportive. So, for example, folks at Penn totally got it, and they actually funded a bunch of that first workshop. But others in the community didn’t get it and didn’t see the point, didn’t see why it was necessary.&lt;/p&gt;



&lt;p&gt;I remember having dinner with one machine learning researcher and him telling me that he didn’t think this kind of workshop was necessary because women’s experiences were no different to men’s experiences. And then later on in the conversation, he talked about—like, you know, this is, like, an hour and a half later or something—he talked about how he and a friend of his had gone to the bar at an all-women’s college and he’d felt so awkward and out of place. And I ended up pointing out to him [LAUGHS] that he just, kind of, explained to himself why we needed WiML. So, yeah, there were some people who didn’t get it, and it took a lot of, sort of, talking to people and, kind of, explaining.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another challenge was figuring out how to fund it in an ongoing manner once we decided that we wanted to do this more than once.&lt;/p&gt;



&lt;p&gt;So as I said, Penn funded a lot of that first workshop, but that wasn’t a sustainable model, and it wasn’t going to be realistic for Penn to keep funding it. So in the end, we worked with Amy Greenwald to obtain a National Science Foundation grant that would cover a lot of costs, and we also received donations from other organizations.&lt;/p&gt;



&lt;p&gt;Um, a third challenge was figuring out where to hold the workshop given that we did want that focus to be on research. So the first two times, we held the workshop at the Grace Hopper conference, but we started to feel that that wasn’t really the right venue given that we wanted that focus to be on research. So we ended up moving it to NeurIPS, and this had a bunch of benefits, some of which I don’t think we’d even fully thought through when we made that decision.&lt;/p&gt;



&lt;p&gt;So one of the benefits was that attendees’ WiML travel funding—so we would give them this travel funding to enable them to pay the cost of attending WiML, stay in hotel rooms, all this kind of stuff—this would actually enable them to attend NeurIPS, as well, if we co-located with NeurIPS.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yep.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Another main benefit was that we held WiML on the day before NeurIPS. So then throughout the rest of the conference, WiML attendees would see familiar faces throughout the crowd and wouldn’t necessarily feel so alone.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; So you’re talking about these challenges. How have these challenges changed over time? Or, you know, more broadly, can you talk about how the workshop and Women in Machine Learning as an organization as a whole, kind of, evolved over the years? I know that you served a term as the WiML president.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. So it’s changed a lot. So first, obviously, most importantly, it evolved from being, kind of, this one-off event where we were just seeing what would happen to being really a robust organization. And the first step in that was creating the WiML board. And, as you just said, I served as the first president of that.&lt;/p&gt;



&lt;p&gt;But there have been a bunch of other steps since then. And one of the things I want to flag about the WiML board was that this was really important because the board members could focus on the long-term health of the organization and these, sort of, like, you know, things that spanned multiple years, like how to get sustainable funding sources, this kind of thing, versus the actual workshop organizers, who would focus on things like running the call for submissions and stuff like that. And being able to separate those roles made it really just reduce the burden on the workshop organizers meant that we could take this, kind of, longer-term perspective.&lt;/p&gt;



&lt;p&gt;Another really important step was becoming, &lt;em&gt;officially&lt;/em&gt; becoming a non-profit. So that happened a few years ago. And again, it was the natural thing to do at that point in time and just another step towards creating this, sort of, durable, robust organization.&lt;/p&gt;



&lt;p&gt;But it’s really taken on a life of its own. I’m honestly not super actively involved nowadays, which I think is fantastic. The organization doesn’t need me. That’s great. It’s also wild to me that because it’s been around for 20 years at this point that there are women in the field who don’t know what it’s like to &lt;em&gt;not&lt;/em&gt; have WiML.&lt;/p&gt;



&lt;p&gt;So a bunch of other affinity groups got created. So Timnit Gebru cofounded Black in AI when she was actually a postdoc at Microsoft Research New York City. So you and I got to actually see the founding of that affinity group up close. And then now there are a ton of other affinity groups. So there’s LatinX in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; there’s Queer in AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Muslims in ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, Indigenous in AI and ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, New In ML&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, just to name a few.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, and all of these are growing, too, every year.&lt;/p&gt;



&lt;p&gt;You know, this year, WiML had over 400 submissions. They accepted 250 to be presented. It’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s wild.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, yep. And there’s going to be a WiML presence this year actually at all three of the NeurIPS venues. So there’s going to be a presence in Mexico City, in Copenhagen, and, of course, in San Diego for the main workshop. So it’s pretty great.&lt;/p&gt;



&lt;p&gt;And, you know, on top of that, I think the organization now, as you were saying, is able to do so much more than just the workshop alone. So for instance, WiML now runs this worldwide mentorship program for women and nonbinary individuals in machine learning, where they’re matched with a mentor and they can participate in these one-to-one mentoring meetings and seminars and panel discussions, which happens all throughout the year. I think they have about 50 mentors signing up each year, but I’m sure they could always use more. Um, so it’s just really amazing to look back and see how much the WiML community has done and how much it’s grown.&lt;/p&gt;



&lt;p&gt;And, you know, on the one hand, I think that honestly, like, founding WiML was one of the things that I’ve done over the course of my career, if not &lt;em&gt;the&lt;/em&gt; thing, that I am most proud of …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Oh yeah, me, too.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … to this day, but at the same time, like, we can’t take credit for any of this. It’s, like, a community effort.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; No.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s the community that has really kept us going …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … for the last 20 years,&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; … so it’s great. Going to stop gushing now, but it’s amazing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; And it’s not just WiML that’s changed over the years. The entire industry has changed a ton, as well.&lt;/p&gt;



&lt;p&gt;How has your research evolved as a result of these changes to the entire field of AI and machine learning and also from your own change from academia to industry?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s a great question. You know, we’ve touched on this a little bit, but our research paths really evolved differently but ended up in these very similar places where we’re working on responsible AI, we’re advocating for interdisciplinary approaches, incorporating techniques from HCI, and so on. And I think that part of this was because of shifts of the community and also what’s happening in industry. Working in responsible AI in industry, there’s definitely not ever a shortage of interesting problems to solve, right.&lt;/p&gt;



&lt;p&gt;And I think that for both of us, our research interests in recent years really have been driven by these really practical challenges that we’re seeing. We were both involved early on in defining what responsible AI means within Microsoft, shaping our internal Responsible AI Standard&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. I led this internal companywide working group on AI transparency, which was focused both on model interpretability like we were talking about earlier but also other forms of transparency like data sheets for datasets and the transparency notes that Microsoft now releases with all of our products. And at the same time, you are leading this internal working group on fairness.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah, taking on that internal working group was, kind of, a big transition point in my career. You know, when I joined Microsoft, I was focusing on computational social science and I was also entirely doing research and wasn’t really that involved in stuff in the rest of the company.&lt;/p&gt;



&lt;p&gt;Then at the end of my first year at Microsoft, I attended the first Fairness, Accountability, and Transparency in Machine Learning workshop&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which was co-located with NeurIPS. It was one of the NeurIPS workshops. And I got really excited about that and thought, great, I’m going to spend like 20% of my time, maybe one day a week, doing research on topics in the space of fairness and accountability and transparency. Um, that is not what ended up happening.&lt;/p&gt;



&lt;p&gt;Over the next couple of years, I ended up doing more and more research on responsible AI, you know, as you said, on topics to do with fairness, to do with interpretability. And then in early 2018, I was asked to co-chair this internal working group on fairness, and that was the point where I started getting much more involved in responsible AI stuff across Microsoft, so outside of just Microsoft Research.&lt;/p&gt;



&lt;p&gt;And this was really exciting to me because responsible AI was so new, which meant that research had a really big role to play. It wasn’t like this was kind of an established area where folks in engineering and policy knew exactly what they were doing. And so that meant that I got to branch out from this very, sort of, research-focused work into much more applied work in collaboration with folks from policy, from engineering, and so on.&lt;/p&gt;



&lt;p&gt;Now, in fact, as well as being a researcher, I actually run a small applied science team, the Sociotechnical Alignment Center, or STAC for short, within Microsoft Research that focuses specifically on bridging research and practice in responsible AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah. Do you think that your involvement in WiML has played a role in this work?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yes, definitely. [LAUGHS] Yeah, without a doubt. So particularly when working on topics related to fairness, I’ve ended up focusing a bunch on stuff to do with marginalized groups as part of my responsible AI work.&lt;/p&gt;



&lt;p&gt;So there’s been this, sort of, you know, focus on marginalized groups, particularly women, in the context of machine learning and with my WiML, kind of, work and then in my research work thinking about fairness, as well.&lt;/p&gt;



&lt;p&gt;The other way that WiML has really, sort of, affected what I do is that I work with a much more varied group of people nowadays than I did back when I was just focusing on, kind of, machine learning, computational social science, and stuff like that. And many of my collaborators are people that I’ve met through WiML over the years.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; And, of course, there has been another big shift within industry recently, which is just all the excitement around generative AI. Can you say a bit about how that has changed your research?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; OK, yeah. So this is another big one. There are so many ways that this changed my work. One of the biggest ways, though, is that generative AI systems are now everywhere. They’re being used all over the place for all kinds of things. And, you know, you see all these news headlines about GenAI systems, you know, diagnosing illnesses, solving math problems, and writing code, stuff like that. And also headlines about various different risks that can occur when you’re using generative AI. So fabricating facts, memorizing copyrighted data, generating harmful content, you know, these kinds of things. And with all this attention, it’s really natural to ask, what is the evidence behind these claims? So where is this evidence coming from, and should we trust it?&lt;/p&gt;



&lt;p&gt;It turns out that much of the evidence comes from GenAI evaluations that involve measuring the capabilities, the behaviors, and the impacts of GenAI systems, but the current evaluation practices that are often used in the space don’t really have as much scientific rigor as we would like, and that’s, kind of, a problem.&lt;/p&gt;



&lt;p&gt;So one of the biggest challenges is that the concepts of interest when people are, sort of, doing these GenAI evaluations—so things like diagnostic ability, memorization, harmful content, concepts like that—are much more abstract than the concepts like prediction accuracy that underpinned machine learning evaluations before the generative AI era.&lt;/p&gt;



&lt;p&gt;And when we look at these new concepts that we need to be able to focus on in order to evaluate GenAI systems, we see that they’re actually much more reminiscent of these abstract contested concepts—these, kind of, fuzzy, squishy concepts—that are studied in the social sciences. So things like democracy and political science or personality traits and psychometrics. So there’s really that, sort of, connection there to these, kind of, squishier things.&lt;/p&gt;



&lt;p&gt;So when I was focusing primarily on computational social science, most of my work was focused on developing machine learning methods to help social scientists measure abstract contested concepts. So then when GenAI started to be a big thing and I saw all of these evaluative claims involving measurements of abstract concepts, it seemed super clear to me that if we were going to actually be able to make meaningful claims about what AI can do and can’t do, we’re going to need to take a different approach to GenAI evaluation.&lt;/p&gt;



&lt;p&gt;And so I ended up, sort of, drawing on my computational social science work around measurement and I started advocating for adopting a variant of the framework that social scientists use for measuring abstract contested concepts. And my reason for doing this was that I believed—I &lt;em&gt;still&lt;/em&gt; believe—that this is an important way to improve the scientific rigor of GenAI evaluations.&lt;/p&gt;



&lt;p&gt;You know all of this, of course, because you and I, along with a &lt;em&gt;bunch&lt;/em&gt; of other collaborators at Microsoft Research and Stanford and the University of Michigan published a position paper on this framework entitled “Evaluating GenAI Systems is a Social Science Measurement Challenge” at ICML [International Conference on Machine Learning] this past summer.&lt;/p&gt;



&lt;p&gt;What are you excited about at the moment?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so lately, I have been spending a lot of time thinking about AI and critical thought: how can we design AI systems to support appropriate reliance, preserve human agency, and really encourage critical engagement on the part of the human, right?&lt;/p&gt;



&lt;p&gt;So this is an area where I think we actually have a huge opportunity, but there are also huge risks. If I think about my most optimistic possible vision of the future of AI —which is not something that is easy for me to do, as I’m not a natural optimist, as you know—it would be a future in which AI helps people grow and flourish, in which it, kind of, enriches our own &lt;em&gt;human&lt;/em&gt; capabilities. It deepens our own &lt;em&gt;human&lt;/em&gt; thinking and safeguards our own agency.&lt;/p&gt;



&lt;p&gt;So in this future, you know, we could build AI systems that actually help us brainstorm and learn new knowledge and skills, both in formal educational settings and in our day-to-day work, as well. But I think we’re not going to achieve this future by default. It’s something that we really need to design for if we want to get there.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; You mentioned that there are risks. What are the risks that you can see here?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, there’s so much at stake here. You know, in the short term, there are things like overreliance—depending on the output of an AI system even when the system’s wrong. This is something that I’ve worked on a bunch myself. There’s a risk of loss of agency or the ability to make and execute independent decisions and to ensure that our outcomes of AI systems are aligned with personal or professional values of the humans who are using those systems. This is something that I’ve been looking at recently in the context of AI tools for journalism&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. There’s diminished innovation, by which I mean a loss of creativity or diversity of ideas.&lt;/p&gt;



&lt;p&gt;You know, longer term, we risk atrophied skills—people just losing or simply never developing helpful skills for their career or their life because of prolonged use of AI systems. The famous example that people often bring up here is pilots losing the ability to perform certain actions in flight because of dependence on autopilot systems. And I think we’re already starting to see the same sort of thing happen across all sorts of fields because of AI.&lt;/p&gt;



&lt;p&gt;And, you know, finally, another risk that I’ll mention that seems to resonate with a lot of folks I talk to is what I would just call loss of joy, right. What happens when we are delegating to AI systems the parts of our activities that we really take pleasure and find this satisfaction in doing ourselves.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; So then as a community, what should we be doing if we’re worried about these risks?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, I mean, I think this is going to have to be a big community effort if we want to achieve this. This is a big goal. But there are a few places I think we especially need work.&lt;/p&gt;



&lt;p&gt;So I think we need generalized principles and practices for AI system builders for how they can build AI systems in ways that promote human agency and encourage critical thought. We also need principles and practices for system &lt;em&gt;users&lt;/em&gt;. So how do we teach the general population to use AI in ways that amplify their skills and capabilities and help them learn new things?&lt;/p&gt;



&lt;p&gt;And then, you know, close to your heart, I’m sure, I think that we need more work on measurement and evaluation, right. We are once again back to these squishy human properties.&lt;/p&gt;



&lt;p&gt;You know, I mentioned I’ve done some work on overreliance in generative AI systems, and I started there because on the grand scale of risks here, overreliance is something that is relatively easy to measure, at least in the short term. But how do we start thinking about measuring people’s critical thinking when using AI across all sorts of contexts and at scale and over long-time horizons, right? How do we measure the, sort of, longitudinal effect of AI systems just on our critical thought as a population?&lt;/p&gt;



&lt;p&gt;And by the way, if anyone listening is going to be at the WiML workshop, I’ll actually be giving a keynote on this topic. And this is something I’m just incredibly excited about because first, I’m incredibly excited about this topic, but also, in the whole 20 years of WiML, I’ve given opening remarks and similar several times, but this is actually the very first time that I will be talking about my own research there. So this is like my dream. I’m thrilled that this is happening.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s awesome. Oh, that’s so exciting. Excellent.&lt;/p&gt;



&lt;p&gt;So one last question for you. If you could go back and talk to yourself 20 years ago and give yourself some advice, what would you say?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, OK, I’ve thought about this one a bit over the past week, and there are three things here I want to mention.&lt;/p&gt;



&lt;p&gt;So first, I would tell myself to be brave about speaking up. You know I’m about as introverted as it gets and I’m naturally very shy, and this has always held me back. It still holds me back now. It was really embarrassingly late in my career that I decided to do something about this and start to develop strategies to help myself speak up more. And eventually, it started to grow into something that’s a little bit more natural.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; What kind of, um, what kind of strategies?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, so you know, one example is I use a lot of notes. For this podcast, I have a lot of notes here. I’m a big notes person, and things like that really help me.&lt;/p&gt;



&lt;p&gt;The second thing that I would tell myself is to, you know, work on the problems that you really want to see solved. As researchers, we have this amazing freedom to choose our own direction. And early on, you know, a lot of the problems that I worked on were problems that I really enjoyed thinking about on a day-to-day basis. It was a lot of fun. They were like little math puzzles to me. But I often found that, you know, when I would be at conferences and people would ask me about my work, I didn’t really want to talk about these problems. I just in some sense, you know, I had fun doing it, but I didn’t really care. I wasn’t passionate about it. I didn’t care that I had solved the problem.&lt;/p&gt;



&lt;p&gt;And so once, many years ago now, when I was thinking about my research agenda, I got some good advice from our former lab director, Jennifer Chayes, who suggested that I go through my recent projects and sort them into projects where I really &lt;em&gt;liked&lt;/em&gt; working on them—it was a fun experience day-to-day—and projects that I liked talking about after the fact and, kind of, felt good about the results and then see where the overlap is. And this is something that, like, it kind of sounds, kind of, obvious when I say it now, but at the time, it was really eye-opening for me.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; That’s so cool. And now I, kind of, want to do that with all of my projects, particularly at the moment. I actually just took five months, as you know, five months off of work for parental leave because I just had a baby. And so I’m, sort of, taking a big, kind of, inventory of everything as I get back into all of this now, and I love this idea. I think this is really cool.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It’s changed really my whole approach to research. Like, you know, we were talking about this, but most of the work I do now is more HCI than machine learning because I found that the problems that really motivate me, that I want to be talking to people about at conferences, are the &lt;em&gt;people&lt;/em&gt; problems.&lt;/p&gt;



&lt;p&gt;The third piece of advice I would give myself is that you should bring more people into your work, right.&lt;/p&gt;



&lt;p&gt;So there’s this kind of vision on the outside of research being this solo endeavor, and it can feel so competitive at times, right. We all feel this. But time and time again, I’ve seen that the best research comes from collaborations and from bringing people together with diverse perspectives who can challenge each other in a way that is respectful but makes the work better.&lt;/p&gt;



&lt;p&gt;Is there advice that you would give to your former self of 20 years ago?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Yeah. OK. So I’ve also been thinking about this a bunch over the past week. There’s actually a lot of advice I think I would give my former self, [LAUGHS] but there are three things that I keep coming back to.&lt;/p&gt;



&lt;p&gt;OK, so first—and this is similar to your second point—push for doing the work that you find to be most fulfilling even if that means taking a nontraditional path. So in my case, I’ve always been interested in the social sciences. Back when I was a student, you know, even when I was a PhD student, doing research that combined computer science &lt;em&gt;and&lt;/em&gt; the social sciences just wasn’t really a thing. And so as a result, it would have been really easy for me to just be like, “Oh well, I guess that isn’t possible. I’ll just focus on traditional computer science problems.”&lt;/p&gt;



&lt;p&gt;But that’s not what I ended up doing. Instead, and often in ways that made my career, kind of, harder than it probably would have been otherwise, I ended up pushing. I kept pushing, and in fact, I keep pushing, even nowadays, to bring these things together—computer science and the social sciences—in an interdisciplinary fashion. And this hasn’t been easy. But cumulatively, the effect has been that I’ve been able to do much more impactful work than I think I would have been able to do otherwise, and the work I’ve done, I’ve just enjoyed so much more than would otherwise have been the case.&lt;/p&gt;



&lt;p&gt;OK, so second, be brave and share your work. So this is actually advice for my current self and my former self, as this is something that I definitely still struggle with.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; As do I, you know, and actually, I think it’s funny to hear you say this because I would say that you are much better at this than I am.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; I still, I think I have a lot of work to do on this one. Yeah, it’s hard. It’s really hard.&lt;/p&gt;



&lt;p&gt;As you know, I am a perfectionist, and this is good in some ways, but this is also bad in other ways. And one way in which this is bad is that I tend to be really anxious about sharing and publicizing my work, especially when I feel it’s not perfect.&lt;/p&gt;



&lt;p&gt;So as an example, I wrote this massive tutorial on computational social science for ICML in 2015, but I never put the slides … and I wrote a whole script for it … I never put the slides or the script online as a resource for others because I felt it needed more work. And I actually went back and looked at it earlier this year, when we were working on the ICML paper, and I was stunned because it’s great. Why didn’t I put this online? All these things that I thought were problems 10 years ago, no, they’re not a big deal. I should have just shared it.&lt;/p&gt;



&lt;p&gt;As another example, STAC, my applied science team, was using LLMs as part of our approach to GenAI evaluation back in 2022, way before the sort of “LLM-as-a-judge” paradigm was widespread. But I was really worried that others would think negatively of us for doing this, so we didn’t share that much about what we were doing, and I regret that because we missed out on an opportunity to kick off an industrywide discussion about this, sort of, LLM-as-a-judge paradigm.&lt;/p&gt;



&lt;p&gt;OK, so then my third point is that the social side of research is just as valuable as the technical side. And by this, I’m actually not talking about social science and computer science. I actually mean that the &lt;em&gt;how&lt;/em&gt; of doing research, including &lt;em&gt;who&lt;/em&gt; you talk to, &lt;em&gt;who&lt;/em&gt; you collaborate with, and &lt;em&gt;how&lt;/em&gt; you approach those interactions, is just as important as the research itself.&lt;/p&gt;



&lt;p&gt;As a PhD student, I felt really bad about spending time socializing with other researchers, especially at conferences, because I thought that I was supposed to be listening to talks, reading papers, and discussing technical topics with researchers and not socializing. But in hindsight, I think that was wrong. Many of those social connections have ended up being incredibly valuable to my research, both because I’ve ended up collaborating with and in some cases even hiring the people who I first got to know socially …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; … but also because the friendships that I’ve built, like our friendship, for example, have served as a crucial support network over the years, especially when things have felt particularly challenging.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; Yeah, absolutely. I agree with all of that so much.&lt;/p&gt;



&lt;p&gt;And with that, I will say thank you so much for doing this podcast with me today.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Thank &lt;em&gt;you&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WORTMAN VAUGHAN:&lt;/strong&gt; It was a lot of fun to reflect on the last 20 years of WiML, but also the last 20 years of our careers and friendship and all of this, so it’s great, and I never would have agreed to do this if it had been with anyone but you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WALLACH:&lt;/strong&gt; Likewise. [LAUGHS]&lt;/p&gt;



&lt;p&gt;So thank you, everybody, for listening to us, and hopefully some of you will join for the 20th annual workshop for Women in Machine Learning&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which is taking place on Dec. 2. And of course, Jenn and I will both be there in person. We’ll also be at NeurIPS afterwards. So feel free to reach out to us if you want to chat with us or to learn more about anything that we covered here today.&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OUTRO:&lt;/strong&gt; You’ve been listening to &lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast. Find more episodes of the podcast at aka.ms/researchpodcast&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1] Wallach later clarified that the number of registrants for the 2005 Conference on Neural Information Processing Systems was around 900.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ideas-community-building-machine-learning-and-the-future-of-ai/</guid><pubDate>Mon, 01 Dec 2025 19:18:20 +0000</pubDate></item><item><title>[NEW] Driving American battery innovation forward (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/driving-american-battery-innovation-forward-1201</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MITei-Kurt-Kelty.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Advancements in battery innovation are transforming both mobility and energy systems alike, according to Kurt Kelty, vice president of battery, propulsion, and sustainability at General Motors (GM). At the MIT Energy Initiative (MITEI) Fall Colloquium, Kelty explored how GM is bringing next-generation battery technologies from lab to commercialization, driving American battery innovation forward. The colloquium is part of the ongoing MITEI Presents: Advancing the Energy Transition speaker series.&lt;/p&gt;&lt;p&gt;At GM, Kelty’s team is primarily focused on three things: first, improving affordability to get more electric vehicles (EVs) on the road. “How do you drive down the cost?” Kelty asked the audience. “It's the batteries. The batteries make up about 30 percent of the cost of the vehicle.” Second, his team strives to improve battery performance, including charging speed and energy density. Third, they are working on localizing the supply chain. “We've got to build up our resilience and our independence here in North America, so we're not relying on materials coming from China,” Kelty explained.&lt;/p&gt;&lt;p&gt;To aid their efforts, resources are being poured into the virtualization space, significantly cutting down on time dedicated to research and development. Now, Kelty’s team can do modeling up front using artificial intelligence, reducing what previously would have taken months to a couple of days.&lt;/p&gt;&lt;p&gt;“If you want to modify … the nickel content ever so slightly, we can very quickly model: ‘OK, how’s that going to affect the energy density? The safety? How’s that going to affect the charge capability?’” said Kelty. “We can look at that at the cell level, then the pack level, then the vehicle level.”&lt;/p&gt;&lt;p&gt;Kelty revealed that they have found a solution that addresses affordability, accessibility, and commercialization: lithium manganese-rich (LMR) batteries. Previously, the industry looked to reduce costs by lowering the amount of cobalt in batteries by adding greater amounts of nickel. These high-nickel batteries are in most cars on the road in the United States due to their high range. LMR batteries, though, take things a step further by reducing the amount of nickel and adding more manganese, which drives the cost of batteries down even further while maintaining range.&lt;/p&gt;&lt;p&gt;Lithium-iron-phosphate (LFP) batteries are the chemistry of choice in China, known for low cost, high cycle life, and high safety. With LMR batteries, the cost is comparable to LFP with a range that is closer to high-nickel. “That’s what’s really a breakthrough,” said Kelty.&lt;/p&gt;&lt;p&gt;LMR batteries are not new, but there have been challenges to adopting them, according to Kelty. “People knew about it, but they didn’t know how to commercialize it. They didn’t know how to make it work in an EV,” he explained. Now that GM has figured out commercialization, they will be the first to market these batteries in their EVs in 2028.&lt;/p&gt;&lt;p&gt;Kelty also expressed excitement over the use of vehicle-to-grid technologies in the future. Using a bidirectional charger with a two-way flow of energy, EVs could charge, but also send power from their batteries back to the electrical grid. This would allow customers to charge “their vehicles at night when the electricity prices are really low, and they can discharge it during the day when electricity rates are really high,” he said.&lt;/p&gt;&lt;p&gt;In addition to working in the transportation sector, GM is exploring ways to extend their battery expertise into applications in grid-scale energy storage. “It’s a big market right now, but it’s growing very quickly because of the data center growth,” said Kelty.&lt;/p&gt;&lt;p&gt;When looking to the future of battery manufacturing and EVs in the United States, Kelty remains optimistic: “we’ve got the technology here to make it happen. We’ve always had the innovation here. Now, we’re getting more and more of the manufacturing. We’re getting that all together. We’ve got just tremendous opportunity here that I’m hopeful we’re going to be able to take advantage of and really build a massive battery industry here.”&lt;/p&gt;&lt;p&gt;This speaker series highlights energy experts and leaders at the forefront of the scientific, technological, and policy solutions needed to transform our energy systems. Visit&amp;nbsp;MITEI’s Events page for more information on this and additional events.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/MITei-Kurt-Kelty.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Advancements in battery innovation are transforming both mobility and energy systems alike, according to Kurt Kelty, vice president of battery, propulsion, and sustainability at General Motors (GM). At the MIT Energy Initiative (MITEI) Fall Colloquium, Kelty explored how GM is bringing next-generation battery technologies from lab to commercialization, driving American battery innovation forward. The colloquium is part of the ongoing MITEI Presents: Advancing the Energy Transition speaker series.&lt;/p&gt;&lt;p&gt;At GM, Kelty’s team is primarily focused on three things: first, improving affordability to get more electric vehicles (EVs) on the road. “How do you drive down the cost?” Kelty asked the audience. “It's the batteries. The batteries make up about 30 percent of the cost of the vehicle.” Second, his team strives to improve battery performance, including charging speed and energy density. Third, they are working on localizing the supply chain. “We've got to build up our resilience and our independence here in North America, so we're not relying on materials coming from China,” Kelty explained.&lt;/p&gt;&lt;p&gt;To aid their efforts, resources are being poured into the virtualization space, significantly cutting down on time dedicated to research and development. Now, Kelty’s team can do modeling up front using artificial intelligence, reducing what previously would have taken months to a couple of days.&lt;/p&gt;&lt;p&gt;“If you want to modify … the nickel content ever so slightly, we can very quickly model: ‘OK, how’s that going to affect the energy density? The safety? How’s that going to affect the charge capability?’” said Kelty. “We can look at that at the cell level, then the pack level, then the vehicle level.”&lt;/p&gt;&lt;p&gt;Kelty revealed that they have found a solution that addresses affordability, accessibility, and commercialization: lithium manganese-rich (LMR) batteries. Previously, the industry looked to reduce costs by lowering the amount of cobalt in batteries by adding greater amounts of nickel. These high-nickel batteries are in most cars on the road in the United States due to their high range. LMR batteries, though, take things a step further by reducing the amount of nickel and adding more manganese, which drives the cost of batteries down even further while maintaining range.&lt;/p&gt;&lt;p&gt;Lithium-iron-phosphate (LFP) batteries are the chemistry of choice in China, known for low cost, high cycle life, and high safety. With LMR batteries, the cost is comparable to LFP with a range that is closer to high-nickel. “That’s what’s really a breakthrough,” said Kelty.&lt;/p&gt;&lt;p&gt;LMR batteries are not new, but there have been challenges to adopting them, according to Kelty. “People knew about it, but they didn’t know how to commercialize it. They didn’t know how to make it work in an EV,” he explained. Now that GM has figured out commercialization, they will be the first to market these batteries in their EVs in 2028.&lt;/p&gt;&lt;p&gt;Kelty also expressed excitement over the use of vehicle-to-grid technologies in the future. Using a bidirectional charger with a two-way flow of energy, EVs could charge, but also send power from their batteries back to the electrical grid. This would allow customers to charge “their vehicles at night when the electricity prices are really low, and they can discharge it during the day when electricity rates are really high,” he said.&lt;/p&gt;&lt;p&gt;In addition to working in the transportation sector, GM is exploring ways to extend their battery expertise into applications in grid-scale energy storage. “It’s a big market right now, but it’s growing very quickly because of the data center growth,” said Kelty.&lt;/p&gt;&lt;p&gt;When looking to the future of battery manufacturing and EVs in the United States, Kelty remains optimistic: “we’ve got the technology here to make it happen. We’ve always had the innovation here. Now, we’re getting more and more of the manufacturing. We’re getting that all together. We’ve got just tremendous opportunity here that I’m hopeful we’re going to be able to take advantage of and really build a massive battery industry here.”&lt;/p&gt;&lt;p&gt;This speaker series highlights energy experts and leaders at the forefront of the scientific, technological, and policy solutions needed to transform our energy systems. Visit&amp;nbsp;MITEI’s Events page for more information on this and additional events.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/driving-american-battery-innovation-forward-1201</guid><pubDate>Mon, 01 Dec 2025 20:50:00 +0000</pubDate></item><item><title>[NEW] AWS re:Invent 2025: How to watch and follow along live (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/aws-reinvent-2025-how-to-watch-and-follow-along-live/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_3817.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ big annual event, re:Invent 2025, is getting into full swing in Las Vegas this week. Last year’s event was largely focused on their AI efforts, including new foundation models, services tackling AI hallucinations, and new security measures. And this year is likely to follow suit, based on what Amazon has announced so far and the lineup of speakers and programming that you can explore in more detail below.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event formally kicks off tomorrow, December 2, at 9 a.m. PT. You can expect editorial coverage of the keynotes, interviews with AWS execs, and a roundup of news as the week’s programming rolls out, all of which you’ll be able to find on our site or by just keeping tabs on our AWS re:Invent tag.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS is partnering with companies like TechCrunch to highlight a sponsored showcase of their AWS OnAir programming in particular, which kicked off today to preview the event and highlight some early reveals:&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;





&lt;!-- Load the Twitch embed script --&gt;



&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s become the norm for events like re:Invent to broadcast keynotes and select programming — we just did something similar for TechCrunch Disrupt 2025 — and you can check out a breadth of streams with no ticket necessary below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-keynote-addresses"&gt;AWS re:Invent keynote addresses&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s re:Invent features five keynotes from Tuesday through Thursday, with, of course, a clear focus on AI. You can follow along with each of those keynotes via the scheduled livestreams below. Or you can just go to Fortnite (yes, really) and watch their five keynotes live there.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ceo-matt-garman-december-2-at-8-a-m-pt"&gt;AWS CEO Matt Garman: December 2 at 8 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-vp-of-agentic-ai-swami-sivasubramanian-december-3-at-8-30-a-m-pt"&gt;AWS VP of Agentic AI Swami Sivasubramanian: December 3 at 8:30 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-vp-of-global-specialists-and-partners-aws-dr-ruba-borno-december-3-at-3-p-m-pt"&gt;VP of Global Specialists and Partners, AWS Dr. Ruba Borno: December 3 at 3 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-svp-of-utility-computing-peter-desantis-vp-of-compute-and-machine-learning-services-aws-december-4-at-9-a-m-pt"&gt;SVP of Utility Computing Peter DeSantis, VP of Compute and Machine Learning Services, AWS: December 4 at 9 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-cto-of-amazon-com-dr-werner-vogels-december-4-at-3-30-p-m-pt"&gt;CTO of Amazon.com Dr. Werner Vogels: December 4 at 3:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-partner-showcases"&gt;AWS re:Invent partner showcases&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A series of industry-specific partner showcases will also be livestreamed for those not on the ground in Las Vegas, with a series of streams that extend throughout the run of the event.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-one-december-2-at-10-a-m-pt"&gt;AWS Security, Day One: December 2 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-one-december-2-at-2-p-m-pt"&gt;AWS AI, Day One: December 2 at 2 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-two-december-3-at-10-a-m-pt"&gt;AWS AI, Day Two: December 3 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-two-december-3-at-4-30-p-m-pt"&gt;AWS Security, Day Two: December 3 at 4:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-three-december-4-at-10-00-a-m-pt"&gt;AWS Security, Day Three: December 4 at 10:00 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-three-december-4-at-11-35-a-m-pt"&gt;AWS AI, Day Three: December 4 at 11:35 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-industries-december-4-at-1-40-p-m-pt"&gt;AWS Industries: December 4 at 1:40 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/IMG_3817.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ big annual event, re:Invent 2025, is getting into full swing in Las Vegas this week. Last year’s event was largely focused on their AI efforts, including new foundation models, services tackling AI hallucinations, and new security measures. And this year is likely to follow suit, based on what Amazon has announced so far and the lineup of speakers and programming that you can explore in more detail below.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The event formally kicks off tomorrow, December 2, at 9 a.m. PT. You can expect editorial coverage of the keynotes, interviews with AWS execs, and a roundup of news as the week’s programming rolls out, all of which you’ll be able to find on our site or by just keeping tabs on our AWS re:Invent tag.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS is partnering with companies like TechCrunch to highlight a sponsored showcase of their AWS OnAir programming in particular, which kicked off today to preview the event and highlight some early reveals:&lt;/p&gt;



&lt;!-- Add a placeholder for the Twitch embed --&gt;





&lt;!-- Load the Twitch embed script --&gt;



&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s become the norm for events like re:Invent to broadcast keynotes and select programming — we just did something similar for TechCrunch Disrupt 2025 — and you can check out a breadth of streams with no ticket necessary below:&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-keynote-addresses"&gt;AWS re:Invent keynote addresses&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;This year’s re:Invent features five keynotes from Tuesday through Thursday, with, of course, a clear focus on AI. You can follow along with each of those keynotes via the scheduled livestreams below. Or you can just go to Fortnite (yes, really) and watch their five keynotes live there.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ceo-matt-garman-december-2-at-8-a-m-pt"&gt;AWS CEO Matt Garman: December 2 at 8 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-vp-of-agentic-ai-swami-sivasubramanian-december-3-at-8-30-a-m-pt"&gt;AWS VP of Agentic AI Swami Sivasubramanian: December 3 at 8:30 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-vp-of-global-specialists-and-partners-aws-dr-ruba-borno-december-3-at-3-p-m-pt"&gt;VP of Global Specialists and Partners, AWS Dr. Ruba Borno: December 3 at 3 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-svp-of-utility-computing-peter-desantis-vp-of-compute-and-machine-learning-services-aws-december-4-at-9-a-m-pt"&gt;SVP of Utility Computing Peter DeSantis, VP of Compute and Machine Learning Services, AWS: December 4 at 9 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-cto-of-amazon-com-dr-werner-vogels-december-4-at-3-30-p-m-pt"&gt;CTO of Amazon.com Dr. Werner Vogels: December 4 at 3:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-aws-re-invent-partner-showcases"&gt;AWS re:Invent partner showcases&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A series of industry-specific partner showcases will also be livestreamed for those not on the ground in Las Vegas, with a series of streams that extend throughout the run of the event.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-one-december-2-at-10-a-m-pt"&gt;AWS Security, Day One: December 2 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-one-december-2-at-2-p-m-pt"&gt;AWS AI, Day One: December 2 at 2 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-two-december-3-at-10-a-m-pt"&gt;AWS AI, Day Two: December 3 at 10 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-two-december-3-at-4-30-p-m-pt"&gt;AWS Security, Day Two: December 3 at 4:30 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-security-day-three-december-4-at-10-00-a-m-pt"&gt;AWS Security, Day Three: December 4 at 10:00 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-ai-day-three-december-4-at-11-35-a-m-pt"&gt;AWS AI, Day Three: December 4 at 11:35 a.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-aws-industries-december-4-at-1-40-p-m-pt"&gt;AWS Industries: December 4 at 1:40 p.m. PT&lt;/h3&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/aws-reinvent-2025-how-to-watch-and-follow-along-live/</guid><pubDate>Mon, 01 Dec 2025 20:50:00 +0000</pubDate></item><item><title>[NEW] Nvidia announces new open AI models and tools for autonomous driving research (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/nvidia-announces-new-open-ai-models-and-tools-for-autonomous-driving-research/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia announced new infrastructure and AI models on Monday as it works to build the backbone technology for physical AI, including robots and autonomous vehicles that can perceive and interact with the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant announced Alpamayo-R1, an open reasoning vision language model for autonomous driving research at the NeurIPS AI conference in San Diego, California. The company claims this is the first vision language action model focused on autonomous driving. Visual language models can process both text and images together, allowing vehicles to “see” their surroundings and make decisions based on what they perceive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This new model is based on Nvidia’s Cosmos-Reason model, a reasoning model that thinks through decisions before it responds. Nvidia initially released the Cosmos model family in January 2025. Additional models were released in August.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Technology like the Alpamayo-R1 is critical for companies looking to reach level 4 autonomous driving, which means full autonomy in a defined area and under specific circumstances, Nvidia said in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia hopes that this type of reasoning model will give autonomous vehicles the “common sense” to better approach nuanced driving decisions like humans do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new model is available on GitHub and Hugging Face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the new vision model, Nvidia also uploaded new step-by-step guides, inference resources, and post-training workflows to GitHub — collectively called the Cosmos Cookbook — to help developers better use and train Cosmos models for their specific use cases. The guide covers data curation, synthetic data generation, and model evaluation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These announcements come as the company is pushing full-speed into physical AI as a new avenue for its advanced AI GPUs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s co-founder and CEO Jensen Huang has repeatedly said that the next wave of AI is physical AI. Bill Dally, Nvidia’s chief scientist, echoed that sentiment in a conversation with TechCrunch over the summer, emphasizing physical AI in robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think eventually robots are going to be a huge player in the world and we want to basically be making the brains of all the robots,” Dally said at the time. “To do that, we need to start developing the key technologies.”&lt;/p&gt;



&lt;!--&amp;lt;!— Add a placeholder for the Twitch embed --&gt;--&amp;gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This video is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2216028442.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nvidia announced new infrastructure and AI models on Monday as it works to build the backbone technology for physical AI, including robots and autonomous vehicles that can perceive and interact with the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant announced Alpamayo-R1, an open reasoning vision language model for autonomous driving research at the NeurIPS AI conference in San Diego, California. The company claims this is the first vision language action model focused on autonomous driving. Visual language models can process both text and images together, allowing vehicles to “see” their surroundings and make decisions based on what they perceive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This new model is based on Nvidia’s Cosmos-Reason model, a reasoning model that thinks through decisions before it responds. Nvidia initially released the Cosmos model family in January 2025. Additional models were released in August.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Technology like the Alpamayo-R1 is critical for companies looking to reach level 4 autonomous driving, which means full autonomy in a defined area and under specific circumstances, Nvidia said in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia hopes that this type of reasoning model will give autonomous vehicles the “common sense” to better approach nuanced driving decisions like humans do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new model is available on GitHub and Hugging Face.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the new vision model, Nvidia also uploaded new step-by-step guides, inference resources, and post-training workflows to GitHub — collectively called the Cosmos Cookbook — to help developers better use and train Cosmos models for their specific use cases. The guide covers data curation, synthetic data generation, and model evaluation.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These announcements come as the company is pushing full-speed into physical AI as a new avenue for its advanced AI GPUs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s co-founder and CEO Jensen Huang has repeatedly said that the next wave of AI is physical AI. Bill Dally, Nvidia’s chief scientist, echoed that sentiment in a conversation with TechCrunch over the summer, emphasizing physical AI in robotics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think eventually robots are going to be a huge player in the world and we want to basically be making the brains of all the robots,” Dally said at the time. “To do that, we need to start developing the key technologies.”&lt;/p&gt;



&lt;!--&amp;lt;!— Add a placeholder for the Twitch embed --&gt;--&amp;gt;


&lt;!-- Load the Twitch embed script --&gt;

&lt;!-- Create a Twitch.Player object. This will render within the placeholder div --&gt;


&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. &lt;strong&gt;This video is brought to you in partnership with AWS.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/nvidia-announces-new-open-ai-models-and-tools-for-autonomous-driving-research/</guid><pubDate>Mon, 01 Dec 2025 21:00:22 +0000</pubDate></item><item><title>[NEW] MIT Sea Grant students explore the intersection of technology and offshore aquaculture in Norway (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/aquaculture-shock-ai-and-autonomy-aquaculture-1201</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/Farm-Sveinung-Beckett-MIT-Aquaculture-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Norway is the world’s largest producer of farmed Atlantic salmon and a top exporter of seafood, while the United States remains the largest importer of these products, according to the Food and Agriculture Organization. Two MIT students recently traveled to Trondheim, Norway to explore the cutting-edge technologies being developed and deployed in offshore aquaculture.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Beckett Devoe, a senior in artificial intelligence and decision-making, and Tony Tang, a junior in mechanical engineering, first worked with MIT Sea Grant through the Undergraduate Research Opportunities Program (UROP). They contributed to projects focusing on wave generator design and machine learning applications for analyzing oyster larvae health in hatcheries. While near-shore aquaculture is a well-established industry across Massachusetts and the United States, open-ocean farming is still a nascent field here, facing unique and complex challenges.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;To help better understand this emerging industry, MIT Sea Grant created a collaborative initiative, AquaCulture Shock, with funding from an Aquaculture Technologies and Education Travel Grant through the National Sea Grant College Program. Collaborating with the MIT-Scandinavia MISTI (MIT International Science and Technology Initiatives) program, MIT Sea Grant matched Devoe and Tang with aquaculture-related summer internships at SINTEF Ocean, one of the largest research institutes in Europe.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“The opportunity to work on this hands-on aquaculture project, under a world-renowned research institution, in an area of the world known for its innovation in marine technology — this is what MISTI is all about,” says Madeline Smith, managing director for MIT-Scandinavia. “Not only are students gaining valuable experience in their fields of study, but they’re developing cultural understanding and skills that equip them to be future global leaders.” Both students worked within SINTEF Ocean’s Aquaculture Robotics and Autonomous Systems Laboratory (ACE-Robotic Lab), a facility designed to develop and test new aquaculture technologies.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Norway has this unique geography where it has all of these fjords,” says Sveinung Ohrem, research manager for the Aquaculture Robotics and Automation Group at SINTEF Ocean. “So you have a lot of sheltered waters, which makes it ideal to do sea-based aquaculture.” He estimates that there are about a thousand fish farms along Norway’s coast, and walks through some of the tools being used in the industry: decision-making systems to gather and visualize data for the farmers and operators; robots for inspection and cleaning; environmental sensors to measure oxygen, temperature, and currents; echosounders that send out acoustic signals to track where the fish are; and cameras to help estimate biomass and fine-tune feeding. “Feeding is a huge challenge,” he notes. “Feed is the largest cost, by far, so optimizing feeding leads to a very significant decrease in your cost.”&lt;/p&gt;&lt;p dir="ltr"&gt;During the internship, Devoe focused on a project that uses AI for fish feeding optimization. “I try to look at the different features of the farm — so maybe how big the fish are, or how cold the water is ... and use that to try to give the farmers an optimal feeding amount for the best outcomes, while also saving money on feed,” he explains. “It was good to learn some more machine learning techniques and just get better at that on a real-world project.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In the same lab, Tang worked on the simulation of an underwater vehicle-manipulator system to navigate farms and repair damage on cage nets with a robotic arm. Ohrem says there are thousands of aquaculture robots operating in Norway today. “The scale is huge,” he says. “You can’t have 8,000 people controlling 8,000 robots — that’s not economically or practically feasible. So the level of autonomy in all of these robots needs to be increased.”&lt;/p&gt;&lt;p dir="ltr"&gt;The collaboration between MIT and SINTEF Ocean began in 2023 when MIT Sea Grant hosted Eleni Kelasidi, a visiting research scientist from the ACE-Robotic Lab. Kelasidi collaborated with MIT Sea Grant director Michael Triantafyllou and professor of mechanical engineering Themistoklis Sapsis developing controllers, models, and underwater vehicles for aquaculture, while also investigating fish-machine interactions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We have had a long and fruitful collaboration with the Norwegian University of Science and Technology (NTNU) and SINTEF, which continues with important efforts such as the aquaculture project with Dr. Kelasidi,” Triantafyllou says. “Norway is at the forefront of offshore aquaculture and MIT Sea Grant is investing in this field, so we anticipate great results from the collaboration.”&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi, who is now a professor at NTNU, also leads the Field Robotics Lab, focusing on developing resilient robotic systems to operate in very complex and harsh environments. “Aquaculture is one of the most challenging field domains we can demonstrate any autonomous solutions, because everything is moving,” she says. Kelasidi describes aquaculture as a deeply interdisciplinary field, requiring more students with backgrounds both in biology and technology. “We cannot develop technologies that are applied for industries where we don’t have biological components,” she explains, “and then apply them somewhere where we have a live fish or other live organisms.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Ohrem affirms that maintaining fish welfare is the primary driver for researchers and companies operating in aquaculture, especially as the industry continues to grow. “So the big question is,” he says, “how can you ensure that?” SINTEF Ocean has four research licenses for farming fish, which they operate through a collaboration with SalMar, the second-largest salmon farmer in the world. The students had the opportunity to visit one of the industrial-scale farms, Singsholmen, on the island of Hitra. The farm has 10 large, round net pens about 50 meters across that extend deep below the surface, each holding up to 200,000 salmon. “I got to physically touch the nets and see how the [robotic] arm might be able to fix the net,” says Tang.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi emphasizes that the information gained in the field cannot be learned from the office or lab. “That opens up and makes you realize, what is the scale of the challenges, or the scale of the facilities,” she says. She also highlights the importance of international and institutional collaboration to advance this field of research and develop more resilient robotic systems. “We need to try to target that problem, and let’s solve it together.”&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Sea Grant and the MIT-Scandinavia MISTI program are currently recruiting a new cohort of four MIT students to intern in Norway this summer with institutes advancing offshore farming technologies, including NTNU’s Field Robotics Lab in Trondheim. Students interested in autonomy, deep learning, simulation modeling, underwater robotic systems, and other aquaculture-related areas are encouraged to reach out to Lily Keyes at MIT Sea Grant.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/Farm-Sveinung-Beckett-MIT-Aquaculture-00.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Norway is the world’s largest producer of farmed Atlantic salmon and a top exporter of seafood, while the United States remains the largest importer of these products, according to the Food and Agriculture Organization. Two MIT students recently traveled to Trondheim, Norway to explore the cutting-edge technologies being developed and deployed in offshore aquaculture.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Beckett Devoe, a senior in artificial intelligence and decision-making, and Tony Tang, a junior in mechanical engineering, first worked with MIT Sea Grant through the Undergraduate Research Opportunities Program (UROP). They contributed to projects focusing on wave generator design and machine learning applications for analyzing oyster larvae health in hatcheries. While near-shore aquaculture is a well-established industry across Massachusetts and the United States, open-ocean farming is still a nascent field here, facing unique and complex challenges.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;To help better understand this emerging industry, MIT Sea Grant created a collaborative initiative, AquaCulture Shock, with funding from an Aquaculture Technologies and Education Travel Grant through the National Sea Grant College Program. Collaborating with the MIT-Scandinavia MISTI (MIT International Science and Technology Initiatives) program, MIT Sea Grant matched Devoe and Tang with aquaculture-related summer internships at SINTEF Ocean, one of the largest research institutes in Europe.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“The opportunity to work on this hands-on aquaculture project, under a world-renowned research institution, in an area of the world known for its innovation in marine technology — this is what MISTI is all about,” says Madeline Smith, managing director for MIT-Scandinavia. “Not only are students gaining valuable experience in their fields of study, but they’re developing cultural understanding and skills that equip them to be future global leaders.” Both students worked within SINTEF Ocean’s Aquaculture Robotics and Autonomous Systems Laboratory (ACE-Robotic Lab), a facility designed to develop and test new aquaculture technologies.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Norway has this unique geography where it has all of these fjords,” says Sveinung Ohrem, research manager for the Aquaculture Robotics and Automation Group at SINTEF Ocean. “So you have a lot of sheltered waters, which makes it ideal to do sea-based aquaculture.” He estimates that there are about a thousand fish farms along Norway’s coast, and walks through some of the tools being used in the industry: decision-making systems to gather and visualize data for the farmers and operators; robots for inspection and cleaning; environmental sensors to measure oxygen, temperature, and currents; echosounders that send out acoustic signals to track where the fish are; and cameras to help estimate biomass and fine-tune feeding. “Feeding is a huge challenge,” he notes. “Feed is the largest cost, by far, so optimizing feeding leads to a very significant decrease in your cost.”&lt;/p&gt;&lt;p dir="ltr"&gt;During the internship, Devoe focused on a project that uses AI for fish feeding optimization. “I try to look at the different features of the farm — so maybe how big the fish are, or how cold the water is ... and use that to try to give the farmers an optimal feeding amount for the best outcomes, while also saving money on feed,” he explains. “It was good to learn some more machine learning techniques and just get better at that on a real-world project.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;In the same lab, Tang worked on the simulation of an underwater vehicle-manipulator system to navigate farms and repair damage on cage nets with a robotic arm. Ohrem says there are thousands of aquaculture robots operating in Norway today. “The scale is huge,” he says. “You can’t have 8,000 people controlling 8,000 robots — that’s not economically or practically feasible. So the level of autonomy in all of these robots needs to be increased.”&lt;/p&gt;&lt;p dir="ltr"&gt;The collaboration between MIT and SINTEF Ocean began in 2023 when MIT Sea Grant hosted Eleni Kelasidi, a visiting research scientist from the ACE-Robotic Lab. Kelasidi collaborated with MIT Sea Grant director Michael Triantafyllou and professor of mechanical engineering Themistoklis Sapsis developing controllers, models, and underwater vehicles for aquaculture, while also investigating fish-machine interactions.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“We have had a long and fruitful collaboration with the Norwegian University of Science and Technology (NTNU) and SINTEF, which continues with important efforts such as the aquaculture project with Dr. Kelasidi,” Triantafyllou says. “Norway is at the forefront of offshore aquaculture and MIT Sea Grant is investing in this field, so we anticipate great results from the collaboration.”&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi, who is now a professor at NTNU, also leads the Field Robotics Lab, focusing on developing resilient robotic systems to operate in very complex and harsh environments. “Aquaculture is one of the most challenging field domains we can demonstrate any autonomous solutions, because everything is moving,” she says. Kelasidi describes aquaculture as a deeply interdisciplinary field, requiring more students with backgrounds both in biology and technology. “We cannot develop technologies that are applied for industries where we don’t have biological components,” she explains, “and then apply them somewhere where we have a live fish or other live organisms.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Ohrem affirms that maintaining fish welfare is the primary driver for researchers and companies operating in aquaculture, especially as the industry continues to grow. “So the big question is,” he says, “how can you ensure that?” SINTEF Ocean has four research licenses for farming fish, which they operate through a collaboration with SalMar, the second-largest salmon farmer in the world. The students had the opportunity to visit one of the industrial-scale farms, Singsholmen, on the island of Hitra. The farm has 10 large, round net pens about 50 meters across that extend deep below the surface, each holding up to 200,000 salmon. “I got to physically touch the nets and see how the [robotic] arm might be able to fix the net,” says Tang.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Kelasidi emphasizes that the information gained in the field cannot be learned from the office or lab. “That opens up and makes you realize, what is the scale of the challenges, or the scale of the facilities,” she says. She also highlights the importance of international and institutional collaboration to advance this field of research and develop more resilient robotic systems. “We need to try to target that problem, and let’s solve it together.”&lt;/p&gt;&lt;p dir="ltr"&gt;MIT Sea Grant and the MIT-Scandinavia MISTI program are currently recruiting a new cohort of four MIT students to intern in Norway this summer with institutes advancing offshore farming technologies, including NTNU’s Field Robotics Lab in Trondheim. Students interested in autonomy, deep learning, simulation modeling, underwater robotic systems, and other aquaculture-related areas are encouraged to reach out to Lily Keyes at MIT Sea Grant.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/aquaculture-shock-ai-and-autonomy-aquaculture-1201</guid><pubDate>Mon, 01 Dec 2025 21:25:00 +0000</pubDate></item><item><title>[NEW] OpenAI desperate to avoid explaining why it deleted pirated book datasets (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/openai-desperate-to-avoid-explaining-why-it-deleted-pirated-book-datasets/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI risks increased fines after deleting pirated books datasets.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="412" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-640x412.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          wenmei Zhou | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI may soon be forced to explain why it deleted a pair of controversial datasets composed of pirated books, and the stakes could not be higher.&lt;/p&gt;
&lt;p&gt;At the heart of a class-action lawsuit from authors alleging that ChatGPT was illegally trained on their works, OpenAI’s decision to delete the datasets could end up being a deciding factor that gives the authors the win.&lt;/p&gt;
&lt;p&gt;It’s undisputed that OpenAI deleted the datasets, known as “Books 1” and “Books 2,” prior to ChatGPT’s release in 2022. Created by former OpenAI employees in 2021, the datasets were built by scraping the open web and seizing the bulk of its data from a shadow library called Library Genesis (LibGen).&lt;/p&gt;
&lt;p&gt;As OpenAI tells it, the datasets fell out of use within that same year, prompting an internal decision to delete them.&lt;/p&gt;
&lt;p&gt;But the authors suspect there’s more to the story than that. They noted that OpenAI appeared to flip-flop by retracting its claim that the datasets’ “non-use” was a reason for deletion, then later claiming that all reasons for deletion, including “non-use,” should be shielded under attorney-client privilege.&lt;/p&gt;
&lt;p&gt;To the authors, it seemed like OpenAI was quickly backtracking after the court granted the authors’ discovery requests to review OpenAI’s internal messages on the firm’s “non-use.”&lt;/p&gt;
&lt;p&gt;In fact, OpenAI’s reversal only made authors more eager to see how OpenAI discussed “non-use,” and now they may get to find out all the reasons why OpenAI deleted the datasets.&lt;/p&gt;
&lt;p&gt;Last week, US district judge Ona Wang ordered OpenAI to share all communications with in-house lawyers about deleting the datasets, as well as “all internal references to LibGen that OpenAI has redacted or withheld on the basis of attorney-client privilege.”&lt;/p&gt;
&lt;p&gt;According to Wang, OpenAI slipped up by arguing that “non-use” was not a “reason” for deleting the datasets, while simultaneously claiming that it should also be deemed a “reason” considered privileged.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Either way, the judge ruled that OpenAI couldn’t block discovery on “non-use” just by deleting a few words from prior filings that had been on the docket for more than a year.&lt;/p&gt;
&lt;p&gt;“OpenAI has gone back-and-forth on whether ‘non-use’ as a ‘reason’ for the deletion of Books1 and Books2 is privileged at all,” Wang wrote. “OpenAI cannot state a ‘reason’ (which implies it is not privileged) and then later assert that the ‘reason’ is privileged to avoid discovery.”&lt;/p&gt;
&lt;p&gt;Additionally, OpenAI’s claim that all reasons for deleting the datasets are privileged “strains credulity,” she concluded, ordering OpenAI to produce a wide range of potentially revealing internal messages by December 8. OpenAI must also make its in-house lawyers available for deposition by December 19.&lt;/p&gt;
&lt;p&gt;OpenAI has argued that it never flip-flopped or retracted anything. It simply used vague phrasing that led to confusion over whether any of the reasons for deleting the datasets were considered non-privileged. But Wang didn’t buy into that, concluding that “even if a ‘reason’ like ‘non-use’ could be privileged, OpenAI has waived privilege by making a moving target of its privilege assertions.”&lt;/p&gt;
&lt;p&gt;Asked for comment, OpenAI told Ars that “we disagree with the ruling and intend to appeal.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;OpenAI’s “flip-flop” may cost it the win&lt;/h2&gt;
&lt;p&gt;So far, OpenAI has avoided disclosing its rationale, claiming that all the reasons it had for deleting the datasets are privileged. In-house lawyers weighed in on the decision to delete and were even copied on a Slack channel initially called “excise-libgen.”&lt;/p&gt;
&lt;p&gt;But Wang reviewed those Slack messages and found that “the vast majority of these communications were not privileged because they were ‘plainly devoid of any request for legal advice and counsel [did] not once weigh in.'”&lt;/p&gt;
&lt;p&gt;In a particularly non-privileged batch of messages, one OpenAI lawyer, Jason Kwon, only weighed in once, the judge noted, to recommend the channel name be changed to “project-clear.” Wang reminded OpenAI that “the entirety of the Slack channel and all messages contained therein is not privileged simply because it was created at the direction of an attorney and/or the fact that a lawyer was copied on the communications.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The authors believe that exposing OpenAI’s rationale may help prove that the ChatGPT maker willfully infringed on copyrights when pirating the book data. As Wang explained, OpenAI’s retraction risked putting the AI firm’s “good faith and state of mind at issue,” which could increase fines in a loss.&lt;/p&gt;
&lt;p&gt;“In a copyright case, a court can increase the award of statutory damages up to $150,000 per infringed work if the infringement was willful, meaning the defendant ‘was actually aware of the infringing activity’ or the ‘defendant’s actions were the result of reckless disregard for, or willful blindness to, the copyright holder’s rights,'” Wang wrote.&lt;/p&gt;
&lt;p&gt;In a court transcript, a lawyer representing some of the authors suing OpenAI, Christopher Young, noted that OpenAI could be in trouble if evidence showed that it decided against using the datasets for later models due to legal risks. He also suggested that OpenAI could be using the datasets under different names to mask further infringement.&lt;/p&gt;
&lt;h2&gt;Judge calls out OpenAI for twisting fair use ruling&lt;/h2&gt;
&lt;p&gt;Wang also found it contradictory that OpenAI continued to argue in a recent filing that it acted in good faith, while “artfully” removing “its good faith affirmative defense and key words such as ‘innocent,’ ‘reasonably believed,’ and ‘good faith.'” These changes only strengthened discovery requests to explore authors’ willfulness theory, Wang wrote, noting the sought-after internal messages would now be critical for the court’s review.&lt;/p&gt;
&lt;p&gt;“A jury is entitled to know the basis for OpenAI’s purported good faith,” Wang wrote.&lt;/p&gt;
&lt;p&gt;The judge appeared particularly frustrated by OpenAI seemingly twisting the Anthropic ruling to defend against the authors’ request to learn more about the deletion of the datasets.&lt;/p&gt;
&lt;p&gt;In a footnote, Wang called out OpenAI for “bizarrely” citing an Anthropic ruling that “grossly” misrepresented Judge William Alsup’s decision by claiming that he found that “downloading pirated copies of books is lawful as long as they are subsequently used for training an LLM.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, Alsup wrote that he doubted that “any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use.”&lt;/p&gt;
&lt;p&gt;If anything, Wang wrote, OpenAI’s decision to pirate book data—then delete it—seemed “to fall squarely into the category of activities proscribed by” Alsup. For emphasis, she quoted Alsup’s order, which said, “such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded.”&lt;/p&gt;
&lt;p&gt;For the authors, getting hold of OpenAI’s privileged communications could tip the scales in their favor, the Hollywood Reporter suggested. Some authors believe the key to winning could be testimony from Anthropic CEO Dario Amodei, who is accused of creating the controversial datasets while he was still at OpenAI. The authors think Amodei also possesses information on the destruction of the datasets, court filings show.&lt;/p&gt;
&lt;p&gt;OpenAI tried to fight the authors’ motion to depose Amodei, but a judge sided with the authors in March, compelling Amodei to answer their biggest questions on his involvement.&lt;/p&gt;
&lt;p&gt;Whether Amodei’s testimony is a bombshell remains to be seen, but it’s clear that OpenAI may struggle to overcome claims of willful infringement. Wang noted there is a “fundamental conflict” in circumstances “where a party asserts a good faith defense based on advice of counsel but then blocks inquiry into their state of mind by asserting attorney-client privilege,” suggesting that OpenAI may have substantially weakened its defense.&lt;/p&gt;
&lt;p&gt;The outcome of the dispute over the deletions could influence OpenAI’s calculus on whether it should ultimately settle the lawsuit. Ahead of the Anthropic settlement—the largest publicly reported copyright class action settlement in history—authors suing pointed to evidence that Anthropic became “not so gung ho about” training on pirated books “for legal reasons.” That seems to be the type of smoking-gun evidence that authors hope will emerge from OpenAI’s withheld Slack messages.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI risks increased fines after deleting pirated books datasets.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="412" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-640x412.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-1227510667-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          wenmei Zhou | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI may soon be forced to explain why it deleted a pair of controversial datasets composed of pirated books, and the stakes could not be higher.&lt;/p&gt;
&lt;p&gt;At the heart of a class-action lawsuit from authors alleging that ChatGPT was illegally trained on their works, OpenAI’s decision to delete the datasets could end up being a deciding factor that gives the authors the win.&lt;/p&gt;
&lt;p&gt;It’s undisputed that OpenAI deleted the datasets, known as “Books 1” and “Books 2,” prior to ChatGPT’s release in 2022. Created by former OpenAI employees in 2021, the datasets were built by scraping the open web and seizing the bulk of its data from a shadow library called Library Genesis (LibGen).&lt;/p&gt;
&lt;p&gt;As OpenAI tells it, the datasets fell out of use within that same year, prompting an internal decision to delete them.&lt;/p&gt;
&lt;p&gt;But the authors suspect there’s more to the story than that. They noted that OpenAI appeared to flip-flop by retracting its claim that the datasets’ “non-use” was a reason for deletion, then later claiming that all reasons for deletion, including “non-use,” should be shielded under attorney-client privilege.&lt;/p&gt;
&lt;p&gt;To the authors, it seemed like OpenAI was quickly backtracking after the court granted the authors’ discovery requests to review OpenAI’s internal messages on the firm’s “non-use.”&lt;/p&gt;
&lt;p&gt;In fact, OpenAI’s reversal only made authors more eager to see how OpenAI discussed “non-use,” and now they may get to find out all the reasons why OpenAI deleted the datasets.&lt;/p&gt;
&lt;p&gt;Last week, US district judge Ona Wang ordered OpenAI to share all communications with in-house lawyers about deleting the datasets, as well as “all internal references to LibGen that OpenAI has redacted or withheld on the basis of attorney-client privilege.”&lt;/p&gt;
&lt;p&gt;According to Wang, OpenAI slipped up by arguing that “non-use” was not a “reason” for deleting the datasets, while simultaneously claiming that it should also be deemed a “reason” considered privileged.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Either way, the judge ruled that OpenAI couldn’t block discovery on “non-use” just by deleting a few words from prior filings that had been on the docket for more than a year.&lt;/p&gt;
&lt;p&gt;“OpenAI has gone back-and-forth on whether ‘non-use’ as a ‘reason’ for the deletion of Books1 and Books2 is privileged at all,” Wang wrote. “OpenAI cannot state a ‘reason’ (which implies it is not privileged) and then later assert that the ‘reason’ is privileged to avoid discovery.”&lt;/p&gt;
&lt;p&gt;Additionally, OpenAI’s claim that all reasons for deleting the datasets are privileged “strains credulity,” she concluded, ordering OpenAI to produce a wide range of potentially revealing internal messages by December 8. OpenAI must also make its in-house lawyers available for deposition by December 19.&lt;/p&gt;
&lt;p&gt;OpenAI has argued that it never flip-flopped or retracted anything. It simply used vague phrasing that led to confusion over whether any of the reasons for deleting the datasets were considered non-privileged. But Wang didn’t buy into that, concluding that “even if a ‘reason’ like ‘non-use’ could be privileged, OpenAI has waived privilege by making a moving target of its privilege assertions.”&lt;/p&gt;
&lt;p&gt;Asked for comment, OpenAI told Ars that “we disagree with the ruling and intend to appeal.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;OpenAI’s “flip-flop” may cost it the win&lt;/h2&gt;
&lt;p&gt;So far, OpenAI has avoided disclosing its rationale, claiming that all the reasons it had for deleting the datasets are privileged. In-house lawyers weighed in on the decision to delete and were even copied on a Slack channel initially called “excise-libgen.”&lt;/p&gt;
&lt;p&gt;But Wang reviewed those Slack messages and found that “the vast majority of these communications were not privileged because they were ‘plainly devoid of any request for legal advice and counsel [did] not once weigh in.'”&lt;/p&gt;
&lt;p&gt;In a particularly non-privileged batch of messages, one OpenAI lawyer, Jason Kwon, only weighed in once, the judge noted, to recommend the channel name be changed to “project-clear.” Wang reminded OpenAI that “the entirety of the Slack channel and all messages contained therein is not privileged simply because it was created at the direction of an attorney and/or the fact that a lawyer was copied on the communications.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The authors believe that exposing OpenAI’s rationale may help prove that the ChatGPT maker willfully infringed on copyrights when pirating the book data. As Wang explained, OpenAI’s retraction risked putting the AI firm’s “good faith and state of mind at issue,” which could increase fines in a loss.&lt;/p&gt;
&lt;p&gt;“In a copyright case, a court can increase the award of statutory damages up to $150,000 per infringed work if the infringement was willful, meaning the defendant ‘was actually aware of the infringing activity’ or the ‘defendant’s actions were the result of reckless disregard for, or willful blindness to, the copyright holder’s rights,'” Wang wrote.&lt;/p&gt;
&lt;p&gt;In a court transcript, a lawyer representing some of the authors suing OpenAI, Christopher Young, noted that OpenAI could be in trouble if evidence showed that it decided against using the datasets for later models due to legal risks. He also suggested that OpenAI could be using the datasets under different names to mask further infringement.&lt;/p&gt;
&lt;h2&gt;Judge calls out OpenAI for twisting fair use ruling&lt;/h2&gt;
&lt;p&gt;Wang also found it contradictory that OpenAI continued to argue in a recent filing that it acted in good faith, while “artfully” removing “its good faith affirmative defense and key words such as ‘innocent,’ ‘reasonably believed,’ and ‘good faith.'” These changes only strengthened discovery requests to explore authors’ willfulness theory, Wang wrote, noting the sought-after internal messages would now be critical for the court’s review.&lt;/p&gt;
&lt;p&gt;“A jury is entitled to know the basis for OpenAI’s purported good faith,” Wang wrote.&lt;/p&gt;
&lt;p&gt;The judge appeared particularly frustrated by OpenAI seemingly twisting the Anthropic ruling to defend against the authors’ request to learn more about the deletion of the datasets.&lt;/p&gt;
&lt;p&gt;In a footnote, Wang called out OpenAI for “bizarrely” citing an Anthropic ruling that “grossly” misrepresented Judge William Alsup’s decision by claiming that he found that “downloading pirated copies of books is lawful as long as they are subsequently used for training an LLM.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Instead, Alsup wrote that he doubted that “any accused infringer could ever meet its burden of explaining why downloading source copies from pirate sites that it could have purchased or otherwise accessed lawfully was itself reasonably necessary to any subsequent fair use.”&lt;/p&gt;
&lt;p&gt;If anything, Wang wrote, OpenAI’s decision to pirate book data—then delete it—seemed “to fall squarely into the category of activities proscribed by” Alsup. For emphasis, she quoted Alsup’s order, which said, “such piracy of otherwise available copies is inherently, irredeemably infringing even if the pirated copies are immediately used for the transformative use and immediately discarded.”&lt;/p&gt;
&lt;p&gt;For the authors, getting hold of OpenAI’s privileged communications could tip the scales in their favor, the Hollywood Reporter suggested. Some authors believe the key to winning could be testimony from Anthropic CEO Dario Amodei, who is accused of creating the controversial datasets while he was still at OpenAI. The authors think Amodei also possesses information on the destruction of the datasets, court filings show.&lt;/p&gt;
&lt;p&gt;OpenAI tried to fight the authors’ motion to depose Amodei, but a judge sided with the authors in March, compelling Amodei to answer their biggest questions on his involvement.&lt;/p&gt;
&lt;p&gt;Whether Amodei’s testimony is a bombshell remains to be seen, but it’s clear that OpenAI may struggle to overcome claims of willful infringement. Wang noted there is a “fundamental conflict” in circumstances “where a party asserts a good faith defense based on advice of counsel but then blocks inquiry into their state of mind by asserting attorney-client privilege,” suggesting that OpenAI may have substantially weakened its defense.&lt;/p&gt;
&lt;p&gt;The outcome of the dispute over the deletions could influence OpenAI’s calculus on whether it should ultimately settle the lawsuit. Ahead of the Anthropic settlement—the largest publicly reported copyright class action settlement in history—authors suing pointed to evidence that Anthropic became “not so gung ho about” training on pirated books “for legal reasons.” That seems to be the type of smoking-gun evidence that authors hope will emerge from OpenAI’s withheld Slack messages.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/openai-desperate-to-avoid-explaining-why-it-deleted-pirated-book-datasets/</guid><pubDate>Mon, 01 Dec 2025 22:16:28 +0000</pubDate></item><item><title>[NEW] One of Google’s biggest AI advantages is what it already knows about you (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/one-of-googles-biggest-ai-advantages-is-what-it-already-knows-about-you/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/pluribus.jpeg?resize=1200,803" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A Google Search exec said that one of the company’s biggest opportunities in AI lies in its ability to get to know the user better and personalize its responses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The promise is AI that’s uniquely helpful because it knows you. But the risk is AI that feels more like surveillance than service.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a recent episode of the Limitless podcast, Robby Stein, VP of Product for Google Search, explained that Google’s AI tends to field more queries that are advice-seeking or those where the user is looking for recommendations — and these types of questions are more likely to benefit from more subjective responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think there’s a huge opportunity for our AI to know you better and then be uniquely helpful because of that knowledge,” Stein said in the interview. “And one of the things we talked about at [Google’s developer conference] I/O was how the AI can get a better understanding of you through connected services like Gmail.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been integrating AI into its apps for some time, starting back when Gemini was still known as Bard. More recently, it began pulling personal data into another AI product, Gemini Deep Research. And Gemini is now infused into Google Workspace apps like Gmail, Calendar and Drive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Google integrates more personal data into its AI — spanning your emails, documents, photos, location history, and browsing behavior — the line between a helpful assistant and an intrusive one becomes increasingly blurred. And unlike opt-in services, avoiding Google’s data collection may become harder as AI becomes central to its products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s pitch is that this deep personalization makes the AI far more useful. The idea is that Google’s AI technology could learn from the user’s interactions across Google’s various services, then use that understanding to make more personalized recommendations. For instance, if it learned that a user likes particular products or brands, the AI responses might favor those in its recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That, Stein said, would be “much more useful” than just showing users a more generic list of the best-selling products in a given category. “That is, I think, very much the vision — of building something that can be really knowledgeable for you, specifically.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This idea isn’t all that different from how the “Others” in the hit Apple TV show “Pluribus” have gobbled up the world’s knowledge, including intimate details about individuals. When the system interacts with the show’s protagonist, Carol, it uses that data to personalize everything: cooking her favorite meals, adopting a familiar face to handle its communications with her, and otherwise anticipating her needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Carol doesn’t find the personalized responses kind; she finds them invasive. She never consented to sharing her data with the hivemind, yet it knows her better than she’d like.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Similarly, it seems that avoiding Google’s data-gobbling ways will get increasingly difficult in the AI era, and if Google doesn’t get the balance right, the results could feel more creepy than useful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(To be clear: Google &lt;em&gt;does&lt;/em&gt; let you control the apps Gemini uses to make its AI more knowledgeable about you specifically — it’s under “Connected Apps” in Gemini’s settings.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you do share app data with Gemini, Google says it will save and use that data according to the Gemini privacy policy. And that policy reminds users that human reviewers may read some of their data and not to “enter confidential information that you wouldn’t want a reviewer to see or Google to use to improve its services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as more data gets ingested into Google’s own hivemind, it’s easy to see how AI could make data privacy more of a gray area.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, however, believes it has a solution of sorts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein says that Google will indicate when its AI responses are personalized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people want to intuitively understand when they’re being personalized — when information is made for them, versus when [it’s] something that everyone would see if they were to ask this question,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein noted, too, that Google could send a push notification to users when a product they had been considering after several days of online research becomes available or is on sale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There are all these ways that Google now, across modes, across kind of different aspects of your life, [is] being incredibly helpful to you…” he said. “And I think that’s more of how I think of the future of search than any one specific feature or single form factor.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/pluribus.jpeg?resize=1200,803" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A Google Search exec said that one of the company’s biggest opportunities in AI lies in its ability to get to know the user better and personalize its responses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The promise is AI that’s uniquely helpful because it knows you. But the risk is AI that feels more like surveillance than service.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In a recent episode of the Limitless podcast, Robby Stein, VP of Product for Google Search, explained that Google’s AI tends to field more queries that are advice-seeking or those where the user is looking for recommendations — and these types of questions are more likely to benefit from more subjective responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think there’s a huge opportunity for our AI to know you better and then be uniquely helpful because of that knowledge,” Stein said in the interview. “And one of the things we talked about at [Google’s developer conference] I/O was how the AI can get a better understanding of you through connected services like Gmail.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been integrating AI into its apps for some time, starting back when Gemini was still known as Bard. More recently, it began pulling personal data into another AI product, Gemini Deep Research. And Gemini is now infused into Google Workspace apps like Gmail, Calendar and Drive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as Google integrates more personal data into its AI — spanning your emails, documents, photos, location history, and browsing behavior — the line between a helpful assistant and an intrusive one becomes increasingly blurred. And unlike opt-in services, avoiding Google’s data collection may become harder as AI becomes central to its products. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s pitch is that this deep personalization makes the AI far more useful. The idea is that Google’s AI technology could learn from the user’s interactions across Google’s various services, then use that understanding to make more personalized recommendations. For instance, if it learned that a user likes particular products or brands, the AI responses might favor those in its recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That, Stein said, would be “much more useful” than just showing users a more generic list of the best-selling products in a given category. “That is, I think, very much the vision — of building something that can be really knowledgeable for you, specifically.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This idea isn’t all that different from how the “Others” in the hit Apple TV show “Pluribus” have gobbled up the world’s knowledge, including intimate details about individuals. When the system interacts with the show’s protagonist, Carol, it uses that data to personalize everything: cooking her favorite meals, adopting a familiar face to handle its communications with her, and otherwise anticipating her needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Carol doesn’t find the personalized responses kind; she finds them invasive. She never consented to sharing her data with the hivemind, yet it knows her better than she’d like.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Similarly, it seems that avoiding Google’s data-gobbling ways will get increasingly difficult in the AI era, and if Google doesn’t get the balance right, the results could feel more creepy than useful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(To be clear: Google &lt;em&gt;does&lt;/em&gt; let you control the apps Gemini uses to make its AI more knowledgeable about you specifically — it’s under “Connected Apps” in Gemini’s settings.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you do share app data with Gemini, Google says it will save and use that data according to the Gemini privacy policy. And that policy reminds users that human reviewers may read some of their data and not to “enter confidential information that you wouldn’t want a reviewer to see or Google to use to improve its services.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But as more data gets ingested into Google’s own hivemind, it’s easy to see how AI could make data privacy more of a gray area.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, however, believes it has a solution of sorts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein says that Google will indicate when its AI responses are personalized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think people want to intuitively understand when they’re being personalized — when information is made for them, versus when [it’s] something that everyone would see if they were to ask this question,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Stein noted, too, that Google could send a push notification to users when a product they had been considering after several days of online research becomes available or is on sale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There are all these ways that Google now, across modes, across kind of different aspects of your life, [is] being incredibly helpful to you…” he said. “And I think that’s more of how I think of the future of search than any one specific feature or single form factor.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/one-of-googles-biggest-ai-advantages-is-what-it-already-knows-about-you/</guid><pubDate>Tue, 02 Dec 2025 00:17:52 +0000</pubDate></item><item><title>[NEW] Apple just named a new AI chief with Google and Microsoft expertise, as John Giannandrea steps down (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/01/apple-just-named-a-new-ai-chief-with-google-and-microsoft-expertise-as-john-giannandrea-steps-down/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2017/09/tcdisrupt_sf17_johngiannadrea-3043.jpg?resize=1200,869" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a carefully worded announcement on Monday, Apple said John Giannandrea, who has been the company’s AI chief since 2018, is “stepping down” to, well, not work at Apple anymore. He’ll stick around through spring as an advisor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His replacement is Amar Subramanya, a highly regarded Microsoft executive who spent 16 years at Google, most recently leading engineering for the Gemini Assistant. It’s a savvy hire, given that Subramanya knows the competition intimately.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move is being characterized as a shake-up. It was seemingly inevitable in retrospect. Apple Intelligence, the company’s answer to the ChatGPT moment, has been stumbling since its October 2024 launch. Reviews have ranged from “underwhelming” to outright alarmed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its first months were some of the roughest. A notification summary feature meant to condense multiple alerts into digestible snippets generated a series of embarrassing, untrue headlines in late 2024 and early 2025. Among other missteps, the BBC complained twice after Apple Intelligence falsely reported that Luigi Mangione, the man accused of killing UnitedHealthcare CEO Brian Thompson, had shot himself (he hadn’t) and that a darts player, Luke Littler, won a championship before the final even began.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there was Siri’s promised overhaul, which became a black eye for Apple. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Bloomberg investigation published in May revealed the depths of Apple’s AI struggles. For instance, when Craig Federighi, Apple’s software chief, tested the new Siri on his own phone just weeks before its planned launch in April, he was dismayed to find that many of the features the company had been touting didn’t work. The launch was delayed indefinitely, triggering class-action lawsuits from iPhone 16 buyers who’d been promised an AI-powered assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By that point, Giannandrea had already been sidelined, according to Bloomberg. The news organization reported that Tim Cook had stripped Siri from Giannandrea’s oversight entirely back in March, handing it to Vision Pro creator Mike Rockwell. Apple removed its secretive robotics division from Giannandrea’s control, too.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg’s investigation painted a picture of organizational dysfunction, with weak communication between AI and marketing teams, budget misalignments, and a leadership crisis severe enough that some employees had taken to mockingly calling Giannandrea’s group “AI/MLess.” The report also documented an exodus of AI researchers to competitors, including OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is reportedly now leaning on Google’s Gemini to power the next version of Siri, an astonishing and also, presumably, humbling twist considering the intense rivalry between the two companies that dates back more than 15 years, across mobile operating systems, app stores, browsers, maps, cloud services, smart home devices, and now AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Giannandrea came to Apple from Google, where he ran Machine Intelligence and Search. At Apple, he oversaw the AI strategy, machine learning infrastructure, and Siri development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now Subramanya inherits those responsibilities, reporting to Federighi with a clear mandate to help Apple catch up in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s an interesting moment for the company. While competitors have been pouring billions of dollars into massive AI data centers, Apple has focused on processing AI tasks directly on users’ devices using its custom Apple Silicon chips, a privacy-first approach that avoids collecting user data. (When more complex requests require cloud processing, Apple routes them through Private Cloud Compute, servers that promise to process data temporarily and delete it immediately.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that philosophy pays off or whether it has permanently left Apple behind is an outstanding question. Apple’s approach comes with clear trade-offs. Among them, on-device models are smaller and less capable than the massive models running in competitors’ data centers, and Apple’s reluctance to collect user data has left its researchers training models on licensed and synthetic data rather than the giant troves of real-world information that fuel its rivals’ systems.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2017/09/tcdisrupt_sf17_johngiannadrea-3043.jpg?resize=1200,869" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a carefully worded announcement on Monday, Apple said John Giannandrea, who has been the company’s AI chief since 2018, is “stepping down” to, well, not work at Apple anymore. He’ll stick around through spring as an advisor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His replacement is Amar Subramanya, a highly regarded Microsoft executive who spent 16 years at Google, most recently leading engineering for the Gemini Assistant. It’s a savvy hire, given that Subramanya knows the competition intimately.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move is being characterized as a shake-up. It was seemingly inevitable in retrospect. Apple Intelligence, the company’s answer to the ChatGPT moment, has been stumbling since its October 2024 launch. Reviews have ranged from “underwhelming” to outright alarmed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its first months were some of the roughest. A notification summary feature meant to condense multiple alerts into digestible snippets generated a series of embarrassing, untrue headlines in late 2024 and early 2025. Among other missteps, the BBC complained twice after Apple Intelligence falsely reported that Luigi Mangione, the man accused of killing UnitedHealthcare CEO Brian Thompson, had shot himself (he hadn’t) and that a darts player, Luke Littler, won a championship before the final even began.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there was Siri’s promised overhaul, which became a black eye for Apple. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Bloomberg investigation published in May revealed the depths of Apple’s AI struggles. For instance, when Craig Federighi, Apple’s software chief, tested the new Siri on his own phone just weeks before its planned launch in April, he was dismayed to find that many of the features the company had been touting didn’t work. The launch was delayed indefinitely, triggering class-action lawsuits from iPhone 16 buyers who’d been promised an AI-powered assistant.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By that point, Giannandrea had already been sidelined, according to Bloomberg. The news organization reported that Tim Cook had stripped Siri from Giannandrea’s oversight entirely back in March, handing it to Vision Pro creator Mike Rockwell. Apple removed its secretive robotics division from Giannandrea’s control, too.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Bloomberg’s investigation painted a picture of organizational dysfunction, with weak communication between AI and marketing teams, budget misalignments, and a leadership crisis severe enough that some employees had taken to mockingly calling Giannandrea’s group “AI/MLess.” The report also documented an exodus of AI researchers to competitors, including OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple is reportedly now leaning on Google’s Gemini to power the next version of Siri, an astonishing and also, presumably, humbling twist considering the intense rivalry between the two companies that dates back more than 15 years, across mobile operating systems, app stores, browsers, maps, cloud services, smart home devices, and now AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Giannandrea came to Apple from Google, where he ran Machine Intelligence and Search. At Apple, he oversaw the AI strategy, machine learning infrastructure, and Siri development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now Subramanya inherits those responsibilities, reporting to Federighi with a clear mandate to help Apple catch up in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s an interesting moment for the company. While competitors have been pouring billions of dollars into massive AI data centers, Apple has focused on processing AI tasks directly on users’ devices using its custom Apple Silicon chips, a privacy-first approach that avoids collecting user data. (When more complex requests require cloud processing, Apple routes them through Private Cloud Compute, servers that promise to process data temporarily and delete it immediately.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that philosophy pays off or whether it has permanently left Apple behind is an outstanding question. Apple’s approach comes with clear trade-offs. Among them, on-device models are smaller and less capable than the massive models running in competitors’ data centers, and Apple’s reluctance to collect user data has left its researchers training models on licensed and synthetic data rather than the giant troves of real-world information that fuel its rivals’ systems.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/01/apple-just-named-a-new-ai-chief-with-google-and-microsoft-expertise-as-john-giannandrea-steps-down/</guid><pubDate>Tue, 02 Dec 2025 01:34:46 +0000</pubDate></item></channel></rss>