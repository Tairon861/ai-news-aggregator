<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 18 Dec 2025 06:37:19 +0000</lastBuildDate><item><title> ()</title><link>https://www.microsoft.com/en-us/research/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/feed/</guid></item><item><title>Bursting AI bubble may be EU’s “secret weapon” in clash with Trump, expert says (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/12/us-threatens-crackdown-on-eu-firms-as-clash-over-tech-regulations-intensifies/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify and Accenture caught in crossfire as Trump attacks EU tech regulations.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="376" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-640x376.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The US threatened to restrict some of the largest service providers in the European Union as retaliation for EU tech regulations and investigations are increasingly drawing Donald Trump’s ire.&lt;/p&gt;
&lt;p&gt;On Tuesday, the Office of the US Trade Representative (USTR) issued a warning on X, naming Spotify, Accenture, Amadeus, Mistral, Publicis, and DHL among nine firms suddenly yanked into the middle of the US-EU tech fight.&lt;/p&gt;
&lt;p&gt;“The European Union and certain EU Member States have persisted in a continuing course of discriminatory and harassing lawsuits, taxes, fines, and directives against US service providers,” USTR’s post said.&lt;/p&gt;
&lt;p&gt;The clash comes after Elon Musk’s X became the first tech company fined for violating the EU’s Digital Services Act, which is widely considered among the world’s strictest tech regulations. Trump was not appeased by the European Commission (EC) noting that X was not ordered to pay the maximum possible fine. Instead, the $140 million fine sparked backlash within the Trump administration, including from Vice President JD Vance, who slammed the fine as “censorship” of X and its users.&lt;/p&gt;
&lt;p&gt;Asked for comment on the USTR’s post, an EC spokesperson told Ars that the EU intends to defend its tech regulations while implementing commitments from a Trump trade deal that the EU struck in August.&lt;/p&gt;
&lt;p&gt;“The EU is an open and rules-based market, where companies from all over the world do business successfully and profitably,” the EC’s spokesperson said. “As we have made clear many times, our rules apply equally and fairly to all companies operating in the EU,” ensuring “a safe, fair and level playing field in the EU, in line with the expectations of our citizens. We will continue to enforce our rules fairly, and without discrimination.”&lt;/p&gt;
&lt;h2&gt;Trump on shaky ground due to “AI bubble”&lt;/h2&gt;
&lt;p&gt;On X, the USTR account suggested that the EU was overlooking that US companies “provide substantial free services to EU citizens and reliable enterprise services to EU companies,” while supporting “millions of jobs and more than $100 billion in direct investment in Europe.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To stop what Trump views as “overseas extortion” of American tech companies, the USTR said the US was prepared to go after EU service providers, which “have been able to operate freely in the United States for decades, benefitting from access to our market and consumers on a level playing field.”&lt;/p&gt;
&lt;p&gt;“If the EU and EU Member States insist on continuing to restrict, limit, and deter the competitiveness of US service providers through discriminatory means, the United States will have no choice but to begin using every tool at its disposal to counter these unreasonable measures,” USTR’s post said. “Should responsive measures be necessary, US law permits the assessment of fees or restrictions on foreign services, among other actions.”&lt;/p&gt;
&lt;p&gt;The pushback comes after the Trump administration released a November national security report that questioned how long the EU could remain a “reliable” ally as overregulation of its tech industry could hobble both its economy and military strength. Claiming that the EU was only “doubling down” on such regulations, the EU “will be unrecognizable in 20 years or less,” the report predicted.&lt;/p&gt;
&lt;p&gt;“We want Europe to remain European, to regain its civilizational self-confidence, and to abandon its failed focus on regulatory suffocation,” the report said.&lt;/p&gt;
&lt;p&gt;However, the report acknowledged that “Europe remains strategically and culturally vital to the United States.”&lt;/p&gt;
&lt;p&gt;“Transatlantic trade remains one of the pillars of the global economy and of American prosperity,” the report said. “European sectors from manufacturing to technology to energy remain among the world’s most robust. Europe is home to cutting-edge scientific research and world-leading cultural institutions. Not only can we not afford to write Europe off—doing so would be self-defeating for what this strategy aims to achieve.”&lt;/p&gt;
&lt;p&gt;At least one expert in the EU has suggested that the EU can use this acknowledgement as leverage, while perhaps even using the looming threat of the supposed American “AI bubble” bursting to pressure Trump into backing off EU tech laws.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In an op-ed for The Guardian, Johnny Ryan, the director of Enforce, a unit of the Irish Council for Civil Liberties, suggested that the EU could even throw Trump’s presidency into “crisis” by taking bold steps that Trump may not see coming.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;EU can take steps to burst “AI bubble”&lt;/h2&gt;
&lt;p&gt;According to Ryan, the national security report made clear that the EU must fight the US or else “perish.” However, the EU has two “strong cards” to play if it wants to win the fight, he suggested.&lt;/p&gt;
&lt;p&gt;Right now, market analysts are fretting about an “AI bubble,” with US investment in AI far outpacing potential gains until perhaps 2030. A Harvard University business professor focused on helping businesses implement cutting-edge technology like generative AI, Andy Wu, recently explained that AI’s big problem is that “everyone can imagine how useful the technology will be, but no one has figured out yet how to make money.”&lt;/p&gt;
&lt;p&gt;“If the market can keep the faith to persist, it buys the necessary time for the technology to mature, for the costs to come down, and for companies to figure out the business model,” Wu said. But US “companies can end up underwater if AI grows fast but less rapidly than they hope for,” he suggested.&lt;/p&gt;
&lt;p&gt;During this moment, Ryan wrote, it’s not just AI firms with skin in the game, but potentially all of Trump’s supporters. The US is currently on “shaky economic ground” with AI investment accounting “for virtually all (92 percent) GDP growth in the first half of this year.”&lt;/p&gt;
&lt;p&gt;“The US’s bet on AI is now so gigantic that every MAGA voter’s pension is bound to the bubble’s precarious survival,” Ryan said.&lt;/p&gt;
&lt;p&gt;Ursula von der Leyen, the president of the European Commission, could exploit this apparent weakness first by messing with one of the biggest players in America’s AI industry, Nvidia, then by ramping up enforcement of the tech laws Trump loathes.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;According to Ryan, “Dutch company ASML commands a global monopoly on the microchip-etching machines that use light to carve patterns on silicon,” and Nvidia needs those machines if it wants to remain the world’s most valuable company. Should the US GDP remain reliant on AI investment for growth, von der Leyen could use export curbs on that technology like a “lever,” Ryan said, controlling “whether and by how much the US economy expands or contracts.”&lt;/p&gt;
&lt;p&gt;Withholding those machines “would be difficult for Europe” and “extremely painful for the Dutch economy,” Ryan noted, but “it would be far more painful for Trump.”&lt;/p&gt;
&lt;p&gt;Another step the EU could take is even “easier,” Ryan suggested. It could go even harder on the enforcement of tech regulations based on evidence of mismanaged data surfaced in lawsuits against giants like Google and Meta. For example, it seems clear that Meta may have violated the EU’s General Data Protection Regulation (GDPR), after the Facebook owner was “unable to tell a US court that what its internal systems do with your data, or who can access it, or for what purpose.”&lt;/p&gt;
&lt;p&gt;“This data free-for-all lets big tech companies train their AI models on masses of everyone’s data, but it is illegal in Europe, where companies are required to carefully control and account for how they use personal data,” Ryan wrote. “All Brussels has to do is crack down on Ireland, which for years has been a wild west of lax data enforcement, and the repercussions will be felt far beyond.”&lt;/p&gt;
&lt;p&gt;Taking that step would also arguably make it harder for tech companies to secure AI investments, since firms would have to disclose that their “AI tools are barred from accessing Europe’s valuable markets,” Ryan said.&lt;/p&gt;
&lt;p&gt;Calling the reaction to the X fine “extreme,” Ryan pushed for von der Leyen to advance on both fronts, forecasting that “the AI bubble would be unlikely to survive this double shock” and likely neither could Trump’s approval ratings. There’s also a possibility that tech firms could pressure Trump to back down if coping with any increased enforcement threatens AI progress.&lt;/p&gt;
&lt;p&gt;Although Wu suggested that Big Tech firms like Google and Meta would likely be “insulated” from the AI bubble bursting, Google CEO Sundar Pichai doesn’t seem so sure. In November, Pichai told the BBC that if AI investments didn’t pay off quickly enough, he thinks “no company is going to be immune, including us.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify and Accenture caught in crossfire as Trump attacks EU tech regulations.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="376" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-640x376.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/GettyImages-2233373547-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The US threatened to restrict some of the largest service providers in the European Union as retaliation for EU tech regulations and investigations are increasingly drawing Donald Trump’s ire.&lt;/p&gt;
&lt;p&gt;On Tuesday, the Office of the US Trade Representative (USTR) issued a warning on X, naming Spotify, Accenture, Amadeus, Mistral, Publicis, and DHL among nine firms suddenly yanked into the middle of the US-EU tech fight.&lt;/p&gt;
&lt;p&gt;“The European Union and certain EU Member States have persisted in a continuing course of discriminatory and harassing lawsuits, taxes, fines, and directives against US service providers,” USTR’s post said.&lt;/p&gt;
&lt;p&gt;The clash comes after Elon Musk’s X became the first tech company fined for violating the EU’s Digital Services Act, which is widely considered among the world’s strictest tech regulations. Trump was not appeased by the European Commission (EC) noting that X was not ordered to pay the maximum possible fine. Instead, the $140 million fine sparked backlash within the Trump administration, including from Vice President JD Vance, who slammed the fine as “censorship” of X and its users.&lt;/p&gt;
&lt;p&gt;Asked for comment on the USTR’s post, an EC spokesperson told Ars that the EU intends to defend its tech regulations while implementing commitments from a Trump trade deal that the EU struck in August.&lt;/p&gt;
&lt;p&gt;“The EU is an open and rules-based market, where companies from all over the world do business successfully and profitably,” the EC’s spokesperson said. “As we have made clear many times, our rules apply equally and fairly to all companies operating in the EU,” ensuring “a safe, fair and level playing field in the EU, in line with the expectations of our citizens. We will continue to enforce our rules fairly, and without discrimination.”&lt;/p&gt;
&lt;h2&gt;Trump on shaky ground due to “AI bubble”&lt;/h2&gt;
&lt;p&gt;On X, the USTR account suggested that the EU was overlooking that US companies “provide substantial free services to EU citizens and reliable enterprise services to EU companies,” while supporting “millions of jobs and more than $100 billion in direct investment in Europe.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;To stop what Trump views as “overseas extortion” of American tech companies, the USTR said the US was prepared to go after EU service providers, which “have been able to operate freely in the United States for decades, benefitting from access to our market and consumers on a level playing field.”&lt;/p&gt;
&lt;p&gt;“If the EU and EU Member States insist on continuing to restrict, limit, and deter the competitiveness of US service providers through discriminatory means, the United States will have no choice but to begin using every tool at its disposal to counter these unreasonable measures,” USTR’s post said. “Should responsive measures be necessary, US law permits the assessment of fees or restrictions on foreign services, among other actions.”&lt;/p&gt;
&lt;p&gt;The pushback comes after the Trump administration released a November national security report that questioned how long the EU could remain a “reliable” ally as overregulation of its tech industry could hobble both its economy and military strength. Claiming that the EU was only “doubling down” on such regulations, the EU “will be unrecognizable in 20 years or less,” the report predicted.&lt;/p&gt;
&lt;p&gt;“We want Europe to remain European, to regain its civilizational self-confidence, and to abandon its failed focus on regulatory suffocation,” the report said.&lt;/p&gt;
&lt;p&gt;However, the report acknowledged that “Europe remains strategically and culturally vital to the United States.”&lt;/p&gt;
&lt;p&gt;“Transatlantic trade remains one of the pillars of the global economy and of American prosperity,” the report said. “European sectors from manufacturing to technology to energy remain among the world’s most robust. Europe is home to cutting-edge scientific research and world-leading cultural institutions. Not only can we not afford to write Europe off—doing so would be self-defeating for what this strategy aims to achieve.”&lt;/p&gt;
&lt;p&gt;At least one expert in the EU has suggested that the EU can use this acknowledgement as leverage, while perhaps even using the looming threat of the supposed American “AI bubble” bursting to pressure Trump into backing off EU tech laws.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In an op-ed for The Guardian, Johnny Ryan, the director of Enforce, a unit of the Irish Council for Civil Liberties, suggested that the EU could even throw Trump’s presidency into “crisis” by taking bold steps that Trump may not see coming.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;EU can take steps to burst “AI bubble”&lt;/h2&gt;
&lt;p&gt;According to Ryan, the national security report made clear that the EU must fight the US or else “perish.” However, the EU has two “strong cards” to play if it wants to win the fight, he suggested.&lt;/p&gt;
&lt;p&gt;Right now, market analysts are fretting about an “AI bubble,” with US investment in AI far outpacing potential gains until perhaps 2030. A Harvard University business professor focused on helping businesses implement cutting-edge technology like generative AI, Andy Wu, recently explained that AI’s big problem is that “everyone can imagine how useful the technology will be, but no one has figured out yet how to make money.”&lt;/p&gt;
&lt;p&gt;“If the market can keep the faith to persist, it buys the necessary time for the technology to mature, for the costs to come down, and for companies to figure out the business model,” Wu said. But US “companies can end up underwater if AI grows fast but less rapidly than they hope for,” he suggested.&lt;/p&gt;
&lt;p&gt;During this moment, Ryan wrote, it’s not just AI firms with skin in the game, but potentially all of Trump’s supporters. The US is currently on “shaky economic ground” with AI investment accounting “for virtually all (92 percent) GDP growth in the first half of this year.”&lt;/p&gt;
&lt;p&gt;“The US’s bet on AI is now so gigantic that every MAGA voter’s pension is bound to the bubble’s precarious survival,” Ryan said.&lt;/p&gt;
&lt;p&gt;Ursula von der Leyen, the president of the European Commission, could exploit this apparent weakness first by messing with one of the biggest players in America’s AI industry, Nvidia, then by ramping up enforcement of the tech laws Trump loathes.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;According to Ryan, “Dutch company ASML commands a global monopoly on the microchip-etching machines that use light to carve patterns on silicon,” and Nvidia needs those machines if it wants to remain the world’s most valuable company. Should the US GDP remain reliant on AI investment for growth, von der Leyen could use export curbs on that technology like a “lever,” Ryan said, controlling “whether and by how much the US economy expands or contracts.”&lt;/p&gt;
&lt;p&gt;Withholding those machines “would be difficult for Europe” and “extremely painful for the Dutch economy,” Ryan noted, but “it would be far more painful for Trump.”&lt;/p&gt;
&lt;p&gt;Another step the EU could take is even “easier,” Ryan suggested. It could go even harder on the enforcement of tech regulations based on evidence of mismanaged data surfaced in lawsuits against giants like Google and Meta. For example, it seems clear that Meta may have violated the EU’s General Data Protection Regulation (GDPR), after the Facebook owner was “unable to tell a US court that what its internal systems do with your data, or who can access it, or for what purpose.”&lt;/p&gt;
&lt;p&gt;“This data free-for-all lets big tech companies train their AI models on masses of everyone’s data, but it is illegal in Europe, where companies are required to carefully control and account for how they use personal data,” Ryan wrote. “All Brussels has to do is crack down on Ireland, which for years has been a wild west of lax data enforcement, and the repercussions will be felt far beyond.”&lt;/p&gt;
&lt;p&gt;Taking that step would also arguably make it harder for tech companies to secure AI investments, since firms would have to disclose that their “AI tools are barred from accessing Europe’s valuable markets,” Ryan said.&lt;/p&gt;
&lt;p&gt;Calling the reaction to the X fine “extreme,” Ryan pushed for von der Leyen to advance on both fronts, forecasting that “the AI bubble would be unlikely to survive this double shock” and likely neither could Trump’s approval ratings. There’s also a possibility that tech firms could pressure Trump to back down if coping with any increased enforcement threatens AI progress.&lt;/p&gt;
&lt;p&gt;Although Wu suggested that Big Tech firms like Google and Meta would likely be “insulated” from the AI bubble bursting, Google CEO Sundar Pichai doesn’t seem so sure. In November, Pichai told the BBC that if AI investments didn’t pay off quickly enough, he thinks “no company is going to be immune, including us.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/12/us-threatens-crackdown-on-eu-firms-as-clash-over-tech-regulations-intensifies/</guid><pubDate>Wed, 17 Dec 2025 18:41:25 +0000</pubDate></item><item><title>A “scientific sandbox” lets researchers explore the evolution of vision systems (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/scientific-sandbox-lets-researchers-explore-evolution-vision-systems-1217</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT_Eye-Evolution-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Why did humans evolve the eyes we have today?&lt;/p&gt;&lt;p&gt;While scientists can’t go back in time to study the environmental pressures that shaped the evolution of the diverse vision systems that exist in nature, a new computational framework developed by MIT researchers allows them to explore this evolution in artificial intelligence agents.&lt;/p&gt;&lt;p&gt;The framework they developed, in which embodied AI agents evolve eyes and learn to see over many generations, is like a “scientific sandbox” that allows researchers to recreate different evolutionary trees. The user does this by changing the structure of the world and the tasks AI agents complete, such as finding food or telling objects apart.&lt;/p&gt;&lt;p&gt;This allows them to study why one animal may have evolved simple, light-sensitive patches as eyes, while another has complex, camera-type eyes.&lt;/p&gt;&lt;p&gt;The researchers’ experiments with this framework showcase how tasks drove eye evolution in the agents. For instance, they found that navigation tasks often led to the evolution of compound eyes with many individual units, like the eyes of insects and crustaceans.&lt;/p&gt;&lt;p&gt;On the other hand, if agents focused on object discrimination, they were more likely to evolve camera-type eyes with irises and retinas.&lt;/p&gt;&lt;p&gt;This framework could enable scientists to probe “what-if” questions about vision systems that are difficult to study experimentally. It could also guide the design of novel sensors and cameras for robots, drones, and wearable devices that balance performance with real-world constraints like energy efficiency and manufacturability.&lt;/p&gt;&lt;p&gt;“While we can never go back and figure out every detail of how evolution took place, in this work we’ve created an environment where we can, in a sense, recreate evolution and probe the environment in all these different ways. This method of doing science opens to the door to a lot of possibilities,” says Kushagra Tiwary, a graduate student at the MIT Media Lab and co-lead author of a paper on this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by co-lead author and fellow graduate student Aaron Young; graduate student Tzofi Klinghoffer; former postdoc Akshat Dave, who is now an assistant professor at Stony Brook University; Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences, an investigator in the McGovern Institute, and co-director of the Center for Brains, Minds, and Machines; co-senior authors Brian Cheung, a postdoc in the&amp;nbsp; Center for Brains, Minds, and Machines and an incoming assistant professor at the University of California San Francisco; and Ramesh Raskar, associate professor of media arts and sciences and leader of the Camera Culture Group at MIT; as well as others at Rice University and Lund University. The research appears today in &lt;em&gt;Science Advances&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building a scientific sandbox&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The paper began as a conversation among the researchers about discovering new vision systems that could be useful in different fields, like robotics. To test their “what-if” questions, the researchers decided to use AI to explore the many evolutionary possibilities.&lt;/p&gt;&lt;p&gt;“What-if questions inspired me when I was growing up to study science. With AI, we have a unique opportunity to create these embodied agents that allow us to ask the kinds of questions that would usually be impossible to answer,” Tiwary says.&lt;/p&gt;&lt;p&gt;To build this evolutionary sandbox, the researchers took all the elements of a camera, like the sensors, lenses, apertures, and processors, and converted them into parameters that an embodied AI agent could learn.&lt;/p&gt;&lt;p&gt;They used those building blocks as the starting point for an algorithmic learning mechanism an agent would use as it evolved eyes over time.&lt;/p&gt;&lt;p&gt;“We couldn’t simulate the entire universe atom-by-atom. It was challenging to determine which ingredients we needed, which ingredients we didn’t need, and how to allocate resources over those different elements,” Cheung says.&lt;/p&gt;&lt;p&gt;In their framework, this evolutionary algorithm can choose which elements to evolve based on the constraints of the environment and the task of the agent.&lt;/p&gt;&lt;p&gt;Each environment has a single task, such as navigation, food identification, or prey tracking, designed to mimic real visual tasks animals must overcome to survive. The agents start with a single photoreceptor that looks out at the world and an associated neural network model that processes visual information.&lt;/p&gt;&lt;p&gt;Then, over each agent’s lifetime, it is trained using reinforcement learning, a trial-and-error technique where the agent is rewarded for accomplishing the goal of its task. The environment also incorporates constraints, like a certain number of pixels for an agent’s visual sensors.&lt;/p&gt;&lt;p&gt;“These constraints drive the design process, the same way we have physical constraints in our world, like the physics of light, that have driven the design of our own eyes,” Tiwary says.&lt;/p&gt;&lt;p&gt;Over many generations, agents evolve different elements of vision systems that maximize rewards.&lt;/p&gt;&lt;p&gt;Their framework uses a genetic encoding mechanism to computationally mimic evolution, where individual genes mutate to control an agent’s development.&lt;/p&gt;&lt;p&gt;For instance, morphological genes capture how the agent views the environment and control eye placement; optical genes determine how the eye interacts with light and dictate the number of photoreceptors; and neural genes control the learning capacity of the agents.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Testing hypotheses&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers set up experiments in this framework, they found that tasks had a major influence on the vision systems the agents evolved.&lt;/p&gt;&lt;p&gt;For instance, agents that were focused on navigation tasks developed eyes designed to maximize spatial awareness through low-resolution sensing, while agents tasked with detecting objects developed eyes focused more on frontal acuity, rather than peripheral vision.&lt;/p&gt;&lt;p&gt;Another experiment indicated that a bigger brain isn’t always better when it comes to processing visual information. Only so much visual information can go into the system at a time, based on physical constraints like the number of photoreceptors in the eyes.&lt;/p&gt;&lt;p&gt;“At some point a bigger brain doesn’t help the agents at all, and in nature that would be a waste of resources,” Cheung says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to use this simulator to explore the best vision systems for specific applications, which could help scientists develop task-specific sensors and cameras. They also want to integrate LLMs into their framework to make it easier for users to ask “what-if” questions and study additional possibilities.&lt;/p&gt;&lt;p&gt;“There’s a real benefit that comes from asking questions in a more imaginative way. I hope this inspires others to create larger frameworks, where instead of focusing on narrow questions that cover a specific area, they are looking to answer questions with a much wider scope,” Cheung says.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the Center for Brains, Minds, and Machines and&amp;nbsp;the Defense Advanced Research Projects Agency (DARPA) Mathematics for the Discovery of Algorithms and Architectures (DIAL) program.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT_Eye-Evolution-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Why did humans evolve the eyes we have today?&lt;/p&gt;&lt;p&gt;While scientists can’t go back in time to study the environmental pressures that shaped the evolution of the diverse vision systems that exist in nature, a new computational framework developed by MIT researchers allows them to explore this evolution in artificial intelligence agents.&lt;/p&gt;&lt;p&gt;The framework they developed, in which embodied AI agents evolve eyes and learn to see over many generations, is like a “scientific sandbox” that allows researchers to recreate different evolutionary trees. The user does this by changing the structure of the world and the tasks AI agents complete, such as finding food or telling objects apart.&lt;/p&gt;&lt;p&gt;This allows them to study why one animal may have evolved simple, light-sensitive patches as eyes, while another has complex, camera-type eyes.&lt;/p&gt;&lt;p&gt;The researchers’ experiments with this framework showcase how tasks drove eye evolution in the agents. For instance, they found that navigation tasks often led to the evolution of compound eyes with many individual units, like the eyes of insects and crustaceans.&lt;/p&gt;&lt;p&gt;On the other hand, if agents focused on object discrimination, they were more likely to evolve camera-type eyes with irises and retinas.&lt;/p&gt;&lt;p&gt;This framework could enable scientists to probe “what-if” questions about vision systems that are difficult to study experimentally. It could also guide the design of novel sensors and cameras for robots, drones, and wearable devices that balance performance with real-world constraints like energy efficiency and manufacturability.&lt;/p&gt;&lt;p&gt;“While we can never go back and figure out every detail of how evolution took place, in this work we’ve created an environment where we can, in a sense, recreate evolution and probe the environment in all these different ways. This method of doing science opens to the door to a lot of possibilities,” says Kushagra Tiwary, a graduate student at the MIT Media Lab and co-lead author of a paper on this research.&lt;/p&gt;&lt;p&gt;He is joined on the paper by co-lead author and fellow graduate student Aaron Young; graduate student Tzofi Klinghoffer; former postdoc Akshat Dave, who is now an assistant professor at Stony Brook University; Tomaso Poggio, the Eugene McDermott Professor in the Department of Brain and Cognitive Sciences, an investigator in the McGovern Institute, and co-director of the Center for Brains, Minds, and Machines; co-senior authors Brian Cheung, a postdoc in the&amp;nbsp; Center for Brains, Minds, and Machines and an incoming assistant professor at the University of California San Francisco; and Ramesh Raskar, associate professor of media arts and sciences and leader of the Camera Culture Group at MIT; as well as others at Rice University and Lund University. The research appears today in &lt;em&gt;Science Advances&lt;/em&gt;.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Building a scientific sandbox&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The paper began as a conversation among the researchers about discovering new vision systems that could be useful in different fields, like robotics. To test their “what-if” questions, the researchers decided to use AI to explore the many evolutionary possibilities.&lt;/p&gt;&lt;p&gt;“What-if questions inspired me when I was growing up to study science. With AI, we have a unique opportunity to create these embodied agents that allow us to ask the kinds of questions that would usually be impossible to answer,” Tiwary says.&lt;/p&gt;&lt;p&gt;To build this evolutionary sandbox, the researchers took all the elements of a camera, like the sensors, lenses, apertures, and processors, and converted them into parameters that an embodied AI agent could learn.&lt;/p&gt;&lt;p&gt;They used those building blocks as the starting point for an algorithmic learning mechanism an agent would use as it evolved eyes over time.&lt;/p&gt;&lt;p&gt;“We couldn’t simulate the entire universe atom-by-atom. It was challenging to determine which ingredients we needed, which ingredients we didn’t need, and how to allocate resources over those different elements,” Cheung says.&lt;/p&gt;&lt;p&gt;In their framework, this evolutionary algorithm can choose which elements to evolve based on the constraints of the environment and the task of the agent.&lt;/p&gt;&lt;p&gt;Each environment has a single task, such as navigation, food identification, or prey tracking, designed to mimic real visual tasks animals must overcome to survive. The agents start with a single photoreceptor that looks out at the world and an associated neural network model that processes visual information.&lt;/p&gt;&lt;p&gt;Then, over each agent’s lifetime, it is trained using reinforcement learning, a trial-and-error technique where the agent is rewarded for accomplishing the goal of its task. The environment also incorporates constraints, like a certain number of pixels for an agent’s visual sensors.&lt;/p&gt;&lt;p&gt;“These constraints drive the design process, the same way we have physical constraints in our world, like the physics of light, that have driven the design of our own eyes,” Tiwary says.&lt;/p&gt;&lt;p&gt;Over many generations, agents evolve different elements of vision systems that maximize rewards.&lt;/p&gt;&lt;p&gt;Their framework uses a genetic encoding mechanism to computationally mimic evolution, where individual genes mutate to control an agent’s development.&lt;/p&gt;&lt;p&gt;For instance, morphological genes capture how the agent views the environment and control eye placement; optical genes determine how the eye interacts with light and dictate the number of photoreceptors; and neural genes control the learning capacity of the agents.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Testing hypotheses&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;When the researchers set up experiments in this framework, they found that tasks had a major influence on the vision systems the agents evolved.&lt;/p&gt;&lt;p&gt;For instance, agents that were focused on navigation tasks developed eyes designed to maximize spatial awareness through low-resolution sensing, while agents tasked with detecting objects developed eyes focused more on frontal acuity, rather than peripheral vision.&lt;/p&gt;&lt;p&gt;Another experiment indicated that a bigger brain isn’t always better when it comes to processing visual information. Only so much visual information can go into the system at a time, based on physical constraints like the number of photoreceptors in the eyes.&lt;/p&gt;&lt;p&gt;“At some point a bigger brain doesn’t help the agents at all, and in nature that would be a waste of resources,” Cheung says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to use this simulator to explore the best vision systems for specific applications, which could help scientists develop task-specific sensors and cameras. They also want to integrate LLMs into their framework to make it easier for users to ask “what-if” questions and study additional possibilities.&lt;/p&gt;&lt;p&gt;“There’s a real benefit that comes from asking questions in a more imaginative way. I hope this inspires others to create larger frameworks, where instead of focusing on narrow questions that cover a specific area, they are looking to answer questions with a much wider scope,” Cheung says.&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the Center for Brains, Minds, and Machines and&amp;nbsp;the Defense Advanced Research Projects Agency (DARPA) Mathematics for the Discovery of Algorithms and Architectures (DIAL) program.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/scientific-sandbox-lets-researchers-explore-evolution-vision-systems-1217</guid><pubDate>Wed, 17 Dec 2025 19:00:00 +0000</pubDate></item><item><title>Gemini 3 Flash arrives with reduced costs and latency — a powerful combo for enterprises (AI | VentureBeat)</title><link>https://venturebeat.com/technology/gemini-3-flash-arrives-with-reduced-costs-and-latency-a-powerful-combo-for</link><description>[unable to retrieve full-text content]&lt;p&gt;Enterprises can now harness the power of a large language model that&amp;#x27;s near that of the state-of-the-art&lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt; Google’s Gemini 3 Pro&lt;/a&gt;, but at a fraction of the cost and with increased speed, thanks to the &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;newly released Gemini 3 Flash&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.&lt;/p&gt;&lt;p&gt;Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. &lt;/p&gt;&lt;p&gt;The company &lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-flash-for-enterprises"&gt;said in a blog post&lt;/a&gt; that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The model is also the default for AI Mode on Google Search and the Gemini application. &lt;/p&gt;&lt;p&gt;Tulsee Doshi, senior director, product management on the Gemini team, said in a &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;separate blog post&lt;/a&gt; that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”&lt;/p&gt;&lt;p&gt;Early adoption by specialized firms proves the model&amp;#x27;s reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal &amp;#x27;BigLaw Bench,&amp;#x27; while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren&amp;#x27;t just speed gains; they are enabling &amp;#x27;near real-time&amp;#x27; workflows that were previously impossible.&lt;/p&gt;&lt;h2&gt;More efficient at a lower cost&lt;/h2&gt;&lt;p&gt;Enterprise AI builders have become more aware of &lt;a href="https://venturebeat.com/ai/ais-financial-blind-spot-why-long-term-success-depends-on-cost-transparency"&gt;the cost of running AI models&lt;/a&gt;, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to &lt;a href="https://venturebeat.com/ai/model-minimalism-the-new-ai-strategy-saving-companies-millions"&gt;smaller or distilled models&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/that-cheap-open-source-ai-model-is-actually-burning-through-your-compute-budget"&gt;focusing on open models&lt;/a&gt; or other &lt;a href="https://venturebeat.com/ai/googles-new-framework-helps-ai-agents-spend-their-compute-and-tool-budget"&gt;research and prompting techniques&lt;/a&gt; to help manage bloated AI costs.&lt;/p&gt;&lt;p&gt;For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. &lt;/p&gt;&lt;p&gt;While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent &lt;a href="https://x.com/ArtificialAnlys/status/2001335953290670301"&gt;benchmarking firm Artificial Analysis&lt;/a&gt; adds a layer of crucial nuance. &lt;/p&gt;&lt;p&gt;In the latter organization&amp;#x27;s pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous &amp;#x27;non-reasoning&amp;#x27; Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI&amp;#x27;s GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).&lt;/p&gt;&lt;p&gt;Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a &amp;#x27;reasoning tax&amp;#x27;: the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. &lt;/p&gt;&lt;p&gt;This high token density is offset by Google&amp;#x27;s aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most &amp;#x27;talkative&amp;#x27; models in terms of raw token volume. Here&amp;#x27;s how it stacks up to rival LLM offerings:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Model&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Input (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Output (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Total Cost&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Source&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 3 Flash Preview&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;&lt;b&gt;Google&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Haiku 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Sonnet 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$25.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$30.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$21.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$168.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$189.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;More ways to save&lt;/h2&gt;&lt;p&gt;But enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. &lt;/p&gt;&lt;p&gt;To balance this new reasoning power with strict corporate latency requirements, Google has introduced a &amp;#x27;Thinking Level&amp;#x27; parameter. Developers can toggle between &amp;#x27;Low&amp;#x27;—to minimize cost and latency for simple chat tasks—and &amp;#x27;High&amp;#x27;—to maximize reasoning depth for complex data extraction. This granular control allows teams to build &amp;#x27;variable-speed&amp;#x27; applications that only consume expensive &amp;#x27;thinking tokens&amp;#x27; when a problem actually demands PhD-level lo&lt;/p&gt;&lt;p&gt;The economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. &lt;/p&gt;&lt;p&gt;By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. &lt;/p&gt;&lt;h2&gt;Strong benchmark performance &lt;/h2&gt;&lt;p&gt;But how does Gemini 3 Flash stack up against other models in terms of its performance? &lt;/p&gt;&lt;p&gt;Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!&lt;/p&gt;&lt;p&gt;For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.&lt;/p&gt;&lt;p&gt;The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. &lt;/p&gt;&lt;p&gt;While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&amp;amp;A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”&lt;/p&gt;&lt;h2&gt;First impressions from early users&lt;/h2&gt;&lt;p&gt;So far, early users have been largely impressed with the model, particularly its benchmark performance. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;What It Means for Enterprise AI Usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the &amp;quot;Flash-ification&amp;quot; of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. &lt;/p&gt;&lt;p&gt;The integration into platforms like Google Antigravity suggests that Google isn&amp;#x27;t just selling a model; it&amp;#x27;s selling the infrastructure for the autonomous enterprise. &lt;/p&gt;&lt;p&gt;As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the &amp;quot;Gemini-first&amp;quot; strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns &amp;quot;vibe coding&amp;quot; from an experimental hobby into a production-ready reality.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Enterprises can now harness the power of a large language model that&amp;#x27;s near that of the state-of-the-art&lt;a href="https://venturebeat.com/ai/google-unveils-gemini-3-claiming-the-lead-in-math-science-multimodal-and"&gt; Google’s Gemini 3 Pro&lt;/a&gt;, but at a fraction of the cost and with increased speed, thanks to the &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;newly released Gemini 3 Flash&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The model joins the flagship Gemini 3 Pro, Gemini 3 Deep Think, and Gemini Agent, all of which were announced and released last month.&lt;/p&gt;&lt;p&gt;Gemini 3 Flash, now available on Gemini Enterprise, Google Antigravity, Gemini CLI, AI Studio, and on preview in Vertex AI, processes information in near real-time and helps build quick, responsive agentic applications. &lt;/p&gt;&lt;p&gt;The company &lt;a href="https://cloud.google.com/blog/products/ai-machine-learning/gemini-3-flash-for-enterprises"&gt;said in a blog post&lt;/a&gt; that Gemini 3 Flash “builds on the model series that developers and enterprises already love, optimized for high-frequency workflows that demand speed, without sacrificing quality.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;The model is also the default for AI Mode on Google Search and the Gemini application. &lt;/p&gt;&lt;p&gt;Tulsee Doshi, senior director, product management on the Gemini team, said in a &lt;a href="https://blog.google/products/gemini/gemini-3-flash/"&gt;separate blog post&lt;/a&gt; that the model “demonstrates that speed and scale don’t have to come at the cost of intelligence.”&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows,” Doshi said. “It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications.”&lt;/p&gt;&lt;p&gt;Early adoption by specialized firms proves the model&amp;#x27;s reliability in high-stakes fields. Harvey, an AI platform for law firms, reported a 7% jump in reasoning on their internal &amp;#x27;BigLaw Bench,&amp;#x27; while Resemble AI discovered that Gemini 3 Flash could process complex forensic data for deepfake detection 4x faster than Gemini 2.5 Pro. These aren&amp;#x27;t just speed gains; they are enabling &amp;#x27;near real-time&amp;#x27; workflows that were previously impossible.&lt;/p&gt;&lt;h2&gt;More efficient at a lower cost&lt;/h2&gt;&lt;p&gt;Enterprise AI builders have become more aware of &lt;a href="https://venturebeat.com/ai/ais-financial-blind-spot-why-long-term-success-depends-on-cost-transparency"&gt;the cost of running AI models&lt;/a&gt;, especially as they try to convince stakeholders to put more budget into agentic workflows that run on expensive models. Organizations have turned to &lt;a href="https://venturebeat.com/ai/model-minimalism-the-new-ai-strategy-saving-companies-millions"&gt;smaller or distilled models&lt;/a&gt;, &lt;a href="https://venturebeat.com/ai/that-cheap-open-source-ai-model-is-actually-burning-through-your-compute-budget"&gt;focusing on open models&lt;/a&gt; or other &lt;a href="https://venturebeat.com/ai/googles-new-framework-helps-ai-agents-spend-their-compute-and-tool-budget"&gt;research and prompting techniques&lt;/a&gt; to help manage bloated AI costs.&lt;/p&gt;&lt;p&gt;For enterprises, the biggest value proposition for Gemini 3 Flash is that it offers the same level of advanced multimodal capabilities, such as complex video analysis and data extraction, as its larger Gemini counterparts, but is far faster and cheaper. &lt;/p&gt;&lt;p&gt;While Google’s internal materials highlight a 3x speed increase over the 2.5 Pro series, data from independent &lt;a href="https://x.com/ArtificialAnlys/status/2001335953290670301"&gt;benchmarking firm Artificial Analysis&lt;/a&gt; adds a layer of crucial nuance. &lt;/p&gt;&lt;p&gt;In the latter organization&amp;#x27;s pre-release testing, Gemini 3 Flash Preview recorded a raw throughput of 218 output tokens per second. This makes it 22% slower than the previous &amp;#x27;non-reasoning&amp;#x27; Gemini 2.5 Flash, but it is still significantly faster than rivals including OpenAI&amp;#x27;s GPT-5.1 high (125 t/s) and DeepSeek V3.2 reasoning (30 t/s).&lt;/p&gt;&lt;p&gt;Most notably, Artificial Analysis crowned Gemini 3 Flash as the new leader in their AA-Omniscience knowledge benchmark, where it achieved the highest knowledge accuracy of any model tested to date. However, this intelligence comes with a &amp;#x27;reasoning tax&amp;#x27;: the model more than doubles its token usage compared to the 2.5 Flash series when tackling complex indexes. &lt;/p&gt;&lt;p&gt;This high token density is offset by Google&amp;#x27;s aggressive pricing: when accessing through the Gemini API, Gemini 3 Flash costs $0.50 per 1 million input tokens, compared to $1.25/1M input tokens for Gemini 2.5 Pro, and $3/1M output tokens, compared to $ 10/1 M output tokens for Gemini 2.5 Pro. This allows Gemini 3 Flash to claim the title of the most cost-efficient model for its intelligence tier, despite being one of the most &amp;#x27;talkative&amp;#x27; models in terms of raw token volume. Here&amp;#x27;s how it stacks up to rival LLM offerings:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Model&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Input (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Output (/1M)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Total Cost&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;Source&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Gemini 3 Flash Preview&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.00&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$3.50&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;&lt;b&gt;Google&lt;/b&gt;&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Haiku 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.75&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Sonnet 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.5&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$5.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$25.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$30.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.2 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$21.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$168.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$189.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;More ways to save&lt;/h2&gt;&lt;p&gt;But enterprise developers and users can cut costs further by eliminating the lag most larger models often have, which racks up token usage. Google said the model “is able to modulate how much it thinks,” so that it uses more thinking and therefore more tokens for more complex tasks than for quick prompts. The company noted Gemini 3 Flash uses 30% fewer tokens than Gemini 2.5 Pro. &lt;/p&gt;&lt;p&gt;To balance this new reasoning power with strict corporate latency requirements, Google has introduced a &amp;#x27;Thinking Level&amp;#x27; parameter. Developers can toggle between &amp;#x27;Low&amp;#x27;—to minimize cost and latency for simple chat tasks—and &amp;#x27;High&amp;#x27;—to maximize reasoning depth for complex data extraction. This granular control allows teams to build &amp;#x27;variable-speed&amp;#x27; applications that only consume expensive &amp;#x27;thinking tokens&amp;#x27; when a problem actually demands PhD-level lo&lt;/p&gt;&lt;p&gt;The economic story extends beyond simple token prices. With the standard inclusion of Context Caching, enterprises processing massive, static datasets—such as entire legal libraries or codebase repositories—can see a 90% reduction in costs for repeated queries. When combined with the Batch API’s 50% discount, the total cost of ownership for a Gemini-powered agent drops significantly below the threshold of competing frontier models&lt;/p&gt;&lt;p&gt;“Gemini 3 Flash delivers exceptional performance on coding and agentic tasks combined with a lower price point, allowing teams to deploy sophisticated reasoning costs across high-volume processes without hitting barriers,” Google said. &lt;/p&gt;&lt;p&gt;By offering a model that delivers strong multimodal performance at a more affordable price, Google is making the case that enterprises concerned with controlling their AI spend should choose its models, especially Gemini 3 Flash. &lt;/p&gt;&lt;h2&gt;Strong benchmark performance &lt;/h2&gt;&lt;p&gt;But how does Gemini 3 Flash stack up against other models in terms of its performance? &lt;/p&gt;&lt;p&gt;Doshi said the model achieved a score of 78% on the SWE-Bench Verified benchmark testing for coding agents, outperforming both the preceding Gemini 2.5 family and the newer Gemini 3 Pro itself!&lt;/p&gt;&lt;p&gt;For enterprises, this means high-volume software maintenance and bug-fixing tasks can now be offloaded to a model that is both faster and cheaper than previous flagship models, without a degradation in code quality.&lt;/p&gt;&lt;p&gt;The model also performed strongly on other benchmarks, scoring 81.2% on the MMMU Pro benchmark, comparable to Gemini 3 Pro. &lt;/p&gt;&lt;p&gt;While most Flash type models are explicitly optimized for short, quick tasks like generating code, Google claims Gemini 3 Flash’s performance “in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&amp;amp;A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning.”&lt;/p&gt;&lt;h2&gt;First impressions from early users&lt;/h2&gt;&lt;p&gt;So far, early users have been largely impressed with the model, particularly its benchmark performance. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;h3&gt;&lt;b&gt;What It Means for Enterprise AI Usage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Gemini 3 Flash now serving as the default engine across Google Search and the Gemini app, we are witnessing the &amp;quot;Flash-ification&amp;quot; of frontier intelligence. By making Pro-level reasoning the new baseline, Google is setting a trap for slower incumbents. &lt;/p&gt;&lt;p&gt;The integration into platforms like Google Antigravity suggests that Google isn&amp;#x27;t just selling a model; it&amp;#x27;s selling the infrastructure for the autonomous enterprise. &lt;/p&gt;&lt;p&gt;As developers hit the ground running with 3x faster speeds and a 90% discount on context caching, the &amp;quot;Gemini-first&amp;quot; strategy becomes a compelling financial argument. In the high-velocity race for AI dominance, Gemini 3 Flash may be the model that finally turns &amp;quot;vibe coding&amp;quot; from an experimental hobby into a production-ready reality.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/technology/gemini-3-flash-arrives-with-reduced-costs-and-latency-a-powerful-combo-for</guid><pubDate>Wed, 17 Dec 2025 19:24:00 +0000</pubDate></item><item><title>OpenAI’s new ChatGPT image generator makes faking photos easy (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/openais-new-chatgpt-image-generator-makes-faking-photos-easy/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New GPT Image 1.5 allows more detailed conversational image editing, for better or worse.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A GPT Image 1.5 generation created with the classic prompt, "a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For most of photography’s roughly 200-year history, altering a photo convincingly required either a darkroom, some Photoshop expertise, or, at minimum, a steady hand with scissors and glue. On Tuesday, OpenAI released a tool that reduces the process to typing a sentence.&lt;/p&gt;
&lt;p&gt;It’s not the first company to do so. While OpenAI had a conversational image-editing model in the works since GPT-4o in 2024, Google beat OpenAI to market in March with a public prototype, then refined it to a popular model called Nano Banana image model (and Nano Banana Pro). The enthusiastic response to Google’s image-editing model in the AI community got OpenAI’s attention.&lt;/p&gt;
&lt;p&gt;OpenAI’s new GPT Image 1.5 is an AI image synthesis model that reportedly generates images up to four times faster than its predecessor and costs about 20 percent less through the API. The model rolled out to all ChatGPT users on Tuesday and represents another step toward making photorealistic image manipulation a casual process that requires no particular visual skills.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132464 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="The &amp;quot;Galactic Queen of the Universe&amp;quot; added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="center large" height="791" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/queen_of_the_universe_on_a_sofa-1024x791.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The “Galactic Queen of the Universe” added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;GPT Image 1.5 is notable because it’s a “native multimodal” image model, meaning image generation happens inside the same neural network that processes language prompts. (In contrast, DALL-E 3, an earlier OpenAI image generator previously built into ChatGPT, used a different technique called diffusion to generate images.)&lt;/p&gt;
&lt;p&gt;This newer type of model, which we covered in more detail in March, treats images and text as the same kind of thing: chunks of data called “tokens” to be predicted, patterns to be completed. If you upload a photo of your dad and type “put him in a tuxedo at a wedding,” the model processes your words and the image pixels in a unified space, then outputs new pixels the same way it would output the next word in a sentence.&lt;/p&gt;
&lt;p&gt;Using this technique, GPT Image 1.5 can more easily alter visual reality than earlier AI image models, changing someone’s pose or position, or rendering a scene from a slightly different angle, with varying degrees of success. It can also remove objects, change visual styles, adjust clothing, and refine specific areas while preserving facial likeness across successive edits. You can converse with the AI model about a photograph, refining and revising, the same way you might workshop a draft of an email in ChatGPT.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fidji Simo, OpenAI’s CEO of applications, wrote in a blog post that ChatGPT’s chat interface was never designed for visual work. “Creating and editing images is a different kind of task and deserves a space built for visuals,” Simo wrote. To that end, OpenAI introduced a dedicated image creation space in ChatGPT’s sidebar with preset filters and trending prompts.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132473 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Harrelson Hall, a famous circular building on NCSU's campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can." class="center large" height="812" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/harrelson_ufo-1024x812.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Harrelson Hall, a famous circular building on North Carolina State University’s campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The release’s timing seems like a direct response to Google’s technical gains in AI, including a massive growth in chatbot user base. In particular, Google’s Nano Banana image model (and Nano Banana Pro) became popular on social media after its August release, thanks to its ability to render text relatively clearly and preserve faces consistently across edits.&lt;/p&gt;
&lt;p&gt;OpenAI’s previous token-based image synthesis model could make some targeted edits based on conversational prompts, but it often changed facial details and other elements that users might have wanted to keep. GPT Image 1.5 appears designed to match the editing features that Google already shipped. But if you happen to prefer the older ChatGPT image generator, OpenAI says the previous version will remain available as a custom GPT (for now) for users who prefer it.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;The friction keeps dropping&lt;/h2&gt;
&lt;p&gt;GPT Image 1.5 is not perfect. In our brief testing, it didn’t always follow prompting directions very well. But when it does work, the results seem more convincing and detailed than OpenAI’s previous multimodal image model. For a more detailed comparison, a software consultant named Shaun Pedicini has put together an instructive site (“GenAI Image Editing Showdown”) that conducts A/B testing of various AI image models.&lt;/p&gt;
&lt;p&gt;And while we’ve written about this a lot over the past few years, it’s probably worth repeating that barriers to realistic photo editing and manipulation keep dropping. This kind of seamless, realistic, effortless AI image manipulation may prompt (pun intended) a cultural recalibration of what visual images mean to society. It can also feel a little scary, for someone who grew up in an earlier media era, to see yourself put into situations that didn’t really happen.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2132498 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man." class="center large" height="816" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/benj_man_2-1024x816.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For most of photography’s history, a convincing forgery required skill, time, and resources. Those barriers made fakery rare enough that we could treat many photographs as a reasonable proxy for truth, although they could be manipulated (and often were). That era has ended due to AI, but GPT Image 1.5 seems to remove yet more of the remaining friction.&lt;/p&gt;
&lt;p&gt;The capability to preserve facial likeness across edits has obvious utility for legitimate photo editing and equally obvious potential for misuse. Image generators have already been used to create non-consensual intimate imagery and impersonate real people.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132466 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A close-up of the &amp;quot;Galactic Queen of the Universe&amp;quot; and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="fullwidth full" height="533" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/chatgpt_barbarian_queen.jpg" width="800" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A close-up of the “Galactic Queen of the Universe” and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;With those hazards in mind, OpenAI’s image generators have always included a filter that usually blocks sexual or violent outputs. But it’s still possible to create embarrassing images of people without their consent (even though it violates OpenAI’s terms of service) while avoiding those topics. The company says generated images include C2PA metadata identifying them as AI-created, though that data can be stripped by resaving the file.&lt;/p&gt;
&lt;p&gt;Speaking of fakes, text rendering has been a long-standing weakness in image generators that has slowly gotten better. By prompting some older image synthesis models to create a sign or poster with specific words, the results often come back garbled or misspelled.&lt;/p&gt;
&lt;p&gt;OpenAI says GPT Image 1.5 can handle denser and smaller text. The company’s blog post includes a demonstration where the model generated an image of a newspaper with a multi-paragraph article, complete with headlines, a byline, benchmark tables, and body text that remains legible at the paragraph level. Whether this holds up across varied prompts will require broader testing.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132441 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper." class="fullwidth full" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/newspaper_gpt5.2.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While the newspaper in the example looks fake now, it’s another step toward the potential erosion of the public’s perception of the pre-Internet historical record as image synthesis becomes more realistic.&lt;/p&gt;
&lt;p&gt;OpenAI acknowledged in its blog post that the new model still has problems, including limited support for certain drawing styles and mistakes when generating images that require scientific accuracy. But they think it will get better over time. “We believe we’re still at the beginning of what image generation can enable,” the company wrote. And if the past three years of progress in image synthesis are any indication, they may be correct.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New GPT Image 1.5 allows more detailed conversational image editing, for better or worse.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A GPT Image 1.5 generation created with the classic prompt, &amp;quot;a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting.&amp;quot;" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/barbarian_crt_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A GPT Image 1.5 generation created with the classic prompt, "a muscular barbarian with weapons beside a CRT television set, cinematic, 8K, studio lighting."

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For most of photography’s roughly 200-year history, altering a photo convincingly required either a darkroom, some Photoshop expertise, or, at minimum, a steady hand with scissors and glue. On Tuesday, OpenAI released a tool that reduces the process to typing a sentence.&lt;/p&gt;
&lt;p&gt;It’s not the first company to do so. While OpenAI had a conversational image-editing model in the works since GPT-4o in 2024, Google beat OpenAI to market in March with a public prototype, then refined it to a popular model called Nano Banana image model (and Nano Banana Pro). The enthusiastic response to Google’s image-editing model in the AI community got OpenAI’s attention.&lt;/p&gt;
&lt;p&gt;OpenAI’s new GPT Image 1.5 is an AI image synthesis model that reportedly generates images up to four times faster than its predecessor and costs about 20 percent less through the API. The model rolled out to all ChatGPT users on Tuesday and represents another step toward making photorealistic image manipulation a casual process that requires no particular visual skills.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132464 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="The &amp;quot;Galactic Queen of the Universe&amp;quot; added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="center large" height="791" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/queen_of_the_universe_on_a_sofa-1024x791.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The “Galactic Queen of the Universe” added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;GPT Image 1.5 is notable because it’s a “native multimodal” image model, meaning image generation happens inside the same neural network that processes language prompts. (In contrast, DALL-E 3, an earlier OpenAI image generator previously built into ChatGPT, used a different technique called diffusion to generate images.)&lt;/p&gt;
&lt;p&gt;This newer type of model, which we covered in more detail in March, treats images and text as the same kind of thing: chunks of data called “tokens” to be predicted, patterns to be completed. If you upload a photo of your dad and type “put him in a tuxedo at a wedding,” the model processes your words and the image pixels in a unified space, then outputs new pixels the same way it would output the next word in a sentence.&lt;/p&gt;
&lt;p&gt;Using this technique, GPT Image 1.5 can more easily alter visual reality than earlier AI image models, changing someone’s pose or position, or rendering a scene from a slightly different angle, with varying degrees of success. It can also remove objects, change visual styles, adjust clothing, and refine specific areas while preserving facial likeness across successive edits. You can converse with the AI model about a photograph, refining and revising, the same way you might workshop a draft of an email in ChatGPT.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Fidji Simo, OpenAI’s CEO of applications, wrote in a blog post that ChatGPT’s chat interface was never designed for visual work. “Creating and editing images is a different kind of task and deserves a space built for visuals,” Simo wrote. To that end, OpenAI introduced a dedicated image creation space in ChatGPT’s sidebar with preset filters and trending prompts.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132473 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Harrelson Hall, a famous circular building on NCSU's campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can." class="center large" height="812" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/harrelson_ufo-1024x812.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Harrelson Hall, a famous circular building on North Carolina State University’s campus (now demolished), always looked like it could take off. With GPT Image 1.5, it can.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / ChatGPT

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The release’s timing seems like a direct response to Google’s technical gains in AI, including a massive growth in chatbot user base. In particular, Google’s Nano Banana image model (and Nano Banana Pro) became popular on social media after its August release, thanks to its ability to render text relatively clearly and preserve faces consistently across edits.&lt;/p&gt;
&lt;p&gt;OpenAI’s previous token-based image synthesis model could make some targeted edits based on conversational prompts, but it often changed facial details and other elements that users might have wanted to keep. GPT Image 1.5 appears designed to match the editing features that Google already shipped. But if you happen to prefer the older ChatGPT image generator, OpenAI says the previous version will remain available as a custom GPT (for now) for users who prefer it.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;The friction keeps dropping&lt;/h2&gt;
&lt;p&gt;GPT Image 1.5 is not perfect. In our brief testing, it didn’t always follow prompting directions very well. But when it does work, the results seem more convincing and detailed than OpenAI’s previous multimodal image model. For a more detailed comparison, a software consultant named Shaun Pedicini has put together an instructive site (“GenAI Image Editing Showdown”) that conducts A/B testing of various AI image models.&lt;/p&gt;
&lt;p&gt;And while we’ve written about this a lot over the past few years, it’s probably worth repeating that barriers to realistic photo editing and manipulation keep dropping. This kind of seamless, realistic, effortless AI image manipulation may prompt (pun intended) a cultural recalibration of what visual images mean to society. It can also feel a little scary, for someone who grew up in an earlier media era, to see yourself put into situations that didn’t really happen.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2132498 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man." class="center large" height="816" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/benj_man_2-1024x816.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A photo of Benj Edwards holding a guitar, edited with GPT Image 1.5 to include a smiling man.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;For most of photography’s history, a convincing forgery required skill, time, and resources. Those barriers made fakery rare enough that we could treat many photographs as a reasonable proxy for truth, although they could be manipulated (and often were). That era has ended due to AI, but GPT Image 1.5 seems to remove yet more of the remaining friction.&lt;/p&gt;
&lt;p&gt;The capability to preserve facial likeness across edits has obvious utility for legitimate photo editing and equally obvious potential for misuse. Image generators have already been used to create non-consensual intimate imagery and impersonate real people.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132466 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="A close-up of the &amp;quot;Galactic Queen of the Universe&amp;quot; and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT." class="fullwidth full" height="533" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/chatgpt_barbarian_queen.jpg" width="800" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A close-up of the “Galactic Queen of the Universe” and a barbarian holding a CRT added to a photo of a room with a sofa using GPT Image 1.5 in ChatGPT.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;With those hazards in mind, OpenAI’s image generators have always included a filter that usually blocks sexual or violent outputs. But it’s still possible to create embarrassing images of people without their consent (even though it violates OpenAI’s terms of service) while avoiding those topics. The company says generated images include C2PA metadata identifying them as AI-created, though that data can be stripped by resaving the file.&lt;/p&gt;
&lt;p&gt;Speaking of fakes, text rendering has been a long-standing weakness in image generators that has slowly gotten better. By prompting some older image synthesis models to create a sign or poster with specific words, the results often come back garbled or misspelled.&lt;/p&gt;
&lt;p&gt;OpenAI says GPT Image 1.5 can handle denser and smaller text. The company’s blog post includes a demonstration where the model generated an image of a newspaper with a multi-paragraph article, complete with headlines, a byline, benchmark tables, and body text that remains legible at the paragraph level. Whether this holds up across varied prompts will require broader testing.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2132441 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper." class="fullwidth full" height="1536" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/newspaper_gpt5.2.webp" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An example of the new GPT Image 1.5 model rendering complex and dense text on a simulated newspaper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While the newspaper in the example looks fake now, it’s another step toward the potential erosion of the public’s perception of the pre-Internet historical record as image synthesis becomes more realistic.&lt;/p&gt;
&lt;p&gt;OpenAI acknowledged in its blog post that the new model still has problems, including limited support for certain drawing styles and mistakes when generating images that require scientific accuracy. But they think it will get better over time. “We believe we’re still at the beginning of what image generation can enable,” the company wrote. And if the past three years of progress in image synthesis are any indication, they may be correct.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/openais-new-chatgpt-image-generator-makes-faking-photos-easy/</guid><pubDate>Wed, 17 Dec 2025 22:22:33 +0000</pubDate></item><item><title>Adobe hit with proposed class-action, accused of misusing authors’ work in AI training (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/17/adobe-hit-with-proposed-class-action-accused-of-misusing-authors-work-in-ai-training/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-2162453288.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Like pretty much every other tech company in existence, Adobe has leaned heavily into AI over the past several years. The software firm has launched a number of different AI services since 2023, including Firefly — its AI-powered media-generation suite. Now, however, the company’s full-throated embrace of the technology may have led to trouble, as a new lawsuit claims it used pirated books to train one of its AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A proposed class-action lawsuit filed on behalf of Elizabeth Lyon, an author from Oregon, claims that Adobe used pirated versions of numerous books — including her own — to train the company’s SlimLM program.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adobe describes SlimLM as a small language model series that can be “optimized for document assistance tasks on mobile devices.” It states that SlimLM was pre-trained on SlimPajama-627B, a “deduplicated, multi-corpora, open-source dataset” released by Cerebras in June of 2023. Lyon, who has written a number of guidebooks for non-fiction writing, says that some of her works were included in a pretraining dataset that Adobe had used.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lyon’s lawsuit, which was originally reported on by Reuters, says that her writing was included in a processed subset of a manipulated dataset that was the basis of Adobe’s program: “The SlimPajama dataset was created by copying and manipulating the RedPajama dataset (including copying Books3),” the lawsuit says. “Thus, because it is a derivative copy of the RedPajama dataset, SlimPajama contains the Books3 dataset, including the copyrighted works of Plaintiff and the Class members.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Books3” — a huge collection of 191,000 books that have been used to train GenAI systems — has been an ongoing source of legal trouble for the tech community. RedPajama has also been cited in a number of litigation cases. In September, a lawsuit against Apple claimed the company had used copyrighted material to train its Apple Intelligence model. The litigation mentioned the dataset and accused the tech company of copying protected works “without consent and without credit or compensation.” In October, a similar lawsuit against Salesforce also claimed the company had used RedPajama for training purposes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unfortunately for the tech industry, such lawsuits have, by now, become somewhat commonplace. AI algorithms are trained on massive datasets and, in some cases, those datasets have allegedly included pirated materials. In September, Anthropic agreed to pay $1.5 billion to a number of authors who had sued it and accused it of using pirated versions of their work to train its chatbot, Claude. The case was considered a potential turning point in the ongoing legal battles over copyrighted material in AI training data, of which there are many.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/GettyImages-2162453288.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Like pretty much every other tech company in existence, Adobe has leaned heavily into AI over the past several years. The software firm has launched a number of different AI services since 2023, including Firefly — its AI-powered media-generation suite. Now, however, the company’s full-throated embrace of the technology may have led to trouble, as a new lawsuit claims it used pirated books to train one of its AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A proposed class-action lawsuit filed on behalf of Elizabeth Lyon, an author from Oregon, claims that Adobe used pirated versions of numerous books — including her own — to train the company’s SlimLM program.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adobe describes SlimLM as a small language model series that can be “optimized for document assistance tasks on mobile devices.” It states that SlimLM was pre-trained on SlimPajama-627B, a “deduplicated, multi-corpora, open-source dataset” released by Cerebras in June of 2023. Lyon, who has written a number of guidebooks for non-fiction writing, says that some of her works were included in a pretraining dataset that Adobe had used.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lyon’s lawsuit, which was originally reported on by Reuters, says that her writing was included in a processed subset of a manipulated dataset that was the basis of Adobe’s program: “The SlimPajama dataset was created by copying and manipulating the RedPajama dataset (including copying Books3),” the lawsuit says. “Thus, because it is a derivative copy of the RedPajama dataset, SlimPajama contains the Books3 dataset, including the copyrighted works of Plaintiff and the Class members.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Books3” — a huge collection of 191,000 books that have been used to train GenAI systems — has been an ongoing source of legal trouble for the tech community. RedPajama has also been cited in a number of litigation cases. In September, a lawsuit against Apple claimed the company had used copyrighted material to train its Apple Intelligence model. The litigation mentioned the dataset and accused the tech company of copying protected works “without consent and without credit or compensation.” In October, a similar lawsuit against Salesforce also claimed the company had used RedPajama for training purposes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unfortunately for the tech industry, such lawsuits have, by now, become somewhat commonplace. AI algorithms are trained on massive datasets and, in some cases, those datasets have allegedly included pirated materials. In September, Anthropic agreed to pay $1.5 billion to a number of authors who had sued it and accused it of using pirated versions of their work to train its chatbot, Claude. The case was considered a potential turning point in the ongoing legal battles over copyrighted material in AI training data, of which there are many.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/17/adobe-hit-with-proposed-class-action-accused-of-misusing-authors-work-in-ai-training/</guid><pubDate>Thu, 18 Dec 2025 00:44:55 +0000</pubDate></item><item><title>A new way to increase the capabilities of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-watson-cats-in-a-box.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/mit-watson-cats-in-a-box.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Most languages use word position and sentence structure to extract meaning. For example, “The cat sat on the box,” is not the same as “The box was on the cat.” Over a long text, like a financial document or a novel, the syntax of these words likely evolves.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Similarly, a person might be tracking variables in a piece of code or following instructions that have conditional actions. These are examples of state changes and sequential reasoning that we expect state-of-the-art artificial intelligence systems to excel at; however, the existing, cutting-edge attention mechanism within transformers — the primarily architecture used in large language models (LLMs) for determining the importance of words — has theoretical and empirical limitations when it comes to such capabilities.&lt;/p&gt;&lt;p&gt;An attention mechanism allows an LLM to look back at earlier parts of a query or document and, based on its training, determine which details and words matter most; however, this mechanism alone does not understand word order. It “sees” all of the input words, a.k.a. tokens, at the same time and handles them in the order that they’re presented, so researchers have developed techniques to encode position information. This is key for domains that are highly structured, like language. But the predominant position-encoding method, called rotary position encoding (RoPE), only takes into account the relative distance between tokens in a sequence and is independent of the input data. This means that, for example, words that are four positions apart, like “cat” and “box” in the example above, will all receive the same fixed mathematical rotation specific to that relative distance.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Now research led by MIT and the MIT-IBM Watson AI Lab has produced an encoding technique known as “PaTH Attention” that makes positional information adaptive and context-aware rather than static, as with RoPE.&lt;/p&gt;&lt;p&gt;“Transformers enable accurate and scalable modeling of many domains, but they have these limitations vis-a-vis state tracking, a class of phenomena that is thought to underlie important capabilities that we want in our AI systems. So, the important question is: How can we maintain the scalability and efficiency of transformers, while enabling state tracking?” says the paper’s senior author Yoon Kim, an associate professor in the Department of Electrical Engineering and Computer Science (EECS), a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL), and a researcher with the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;A new paper on this work was presented earlier this month at the Conference on Neural Information Processing Systems (NeurIPS). Kim’s co-authors include lead author Songlin Yang, an EECS graduate student and former MIT-IBM Watson AI Lab Summer Program intern; Kaiyue Wen of Stanford University; Liliang Ren of Microsoft; and Yikang Shen, Shawn Tan, Mayank Mishra, and Rameswar Panda of IBM Research and the MIT-IBM Watson AI Lab.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Path to understanding&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Instead of assigning every word a fixed rotation based on relative distance between tokens, as RoPE does, PaTH Attention is flexible, treating the in-between words as a path made up of small, data-dependent transformations. Each transformation, based on a mathematical operation called a Householder reflection, acts like a tiny mirror that adjusts depending on the content of each token it passes. Each step in a sequence can influence how the model interprets information later on. The cumulative effect lets the system model how the meaning changes along the path between words, not just how far apart they are. This approach allows transformers to keep track of how entities and relationships change over time, giving it a sense of “positional memory.” Think of this as walking a path while experiencing your environment and how it affects you. Further, the team also developed a hardware-efficient algorithm to more efficiently compute attention scores between every pair of tokens so that the cumulative mathematical transformation from PaTH Attention is compressed and broken down into smaller computations so that it’s compatible with fast processing on GPUs.&lt;/p&gt;&lt;p&gt;The MIT-IBM researchers then explored PaTH Attention’s performance on synthetic and real-world tasks, including reasoning, long-context benchmarks, and full LLM training to see whether it improved a model’s ability to track information over time. The team tested its ability to follow the most recent “write” command despite many distracting steps and multi-step recall tests, tasks that are difficult for standard positional encoding methods like RoPE. The researchers also trained mid-size LLMs and compared them against other methods. PaTH Attention improved perplexity and outcompeted other methods on reasoning benchmarks it wasn’t trained on. They also evaluated retrieval, reasoning, and stability with inputs of tens of thousands of tokens. PaTH Attention consistently proved capable of content-awareness.&lt;/p&gt;&lt;p&gt;“We found that both on diagnostic tasks that are designed to test the limitations of transformers and on real-world language modeling tasks, our new approach was able to outperform existing attention mechanisms, while maintaining their efficiency,” says Kim. Further, “I’d be excited to see whether these types of data-dependent position encodings, like PATH, improve the performance of transformers on structured domains like biology, in [analyzing] proteins or DNA.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Thinking bigger and more efficiently&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The researchers then investigated how the PaTH Attention mechanism would perform if it more similarly mimicked human cognition, where we ignore old or less-relevant information when making decisions. To do this, they combined PaTH Attention with another position encoding scheme known as the Forgetting Transformer (FoX), which allows models to selectively “forget.” The resulting PaTH-FoX system adds a way to down-weight information in a data-dependent way, achieving strong results across reasoning, long-context understanding, and language modeling benchmarks. In this way, PaTH Attention extends the expressive power of transformer architectures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Kim says research like this is part of a broader effort to develop the “next big thing” in AI. He explains that a major driver of both the deep learning and generative AI revolutions has been the creation of “general-purpose building blocks that can be applied to wide domains,” such as “convolution layers, RNN [recurrent neural network] layers,” and, most recently, transformers. Looking ahead, Kim notes that considerations like accuracy, expressivity, flexibility, and hardware scalability have been and will be essential. As he puts it, “the core enterprise of modern architecture research is trying to come up with these new primitives that maintain or improve the expressivity, while also being scalable.”&lt;/p&gt;&lt;p&gt;This work was supported, in part, by the MIT-IBM Watson AI Lab and the AI2050 program at Schmidt Sciences.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-way-to-increase-large-language-model-capabilities-1217</guid><pubDate>Thu, 18 Dec 2025 04:10:00 +0000</pubDate></item></channel></rss>