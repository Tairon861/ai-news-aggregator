<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 24 Jul 2025 01:57:14 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>AI’s talent arms race is starting to look like pro sports (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/ais-talent-arms-race-is-starting-to-look-like-pro-sports/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2173579488.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;AI is entering a new phase where access to top talent is becoming as important as, if not more important than, compute or data. The market for AI researchers is so overheated, it’s starting to look a lot like pro sports — complete with outsized contracts and unprecedented infrastructure needs.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On today’s episode of Equity, Rebecca Bellan chatted with Deedy Das, principal at Menlo Ventures. Das has seen this shift from multiple angles, first as an engineer and product leader at Google, Facebook, and AI startup Glean, and now as an investor helping technical founders figure out how to build enduring companies in this new AI landscape.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;“The reason people are being paid this much is because there’s a disparity between the prize to be made in a short amount of time and the amount of people who have the talent to get you to that prize,” Das explained. “As long as that gap remains, you pay up and you get the talent. […] Over time, there will be less prize in AI. I imagine a lot of that value will be captured by a few people, and there will be a lot more talent to fill the supply.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why Meta is spending billions on both compute and researchers.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How compensation packages and acquisitions are warping startup hiring and retention.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What motivates top researchers to leave, even when they’ve already made millions.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How VCs are thinking about key-person risk in the AI era.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="wp-block-paragraph"&gt;Equity will be back Friday with our weekly news roundup, so stay tuned.&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&amp;nbsp;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; X&lt;/em&gt;&lt;em&gt; and&lt;/em&gt;&lt;em&gt; Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-2173579488.jpg?resize=1200,799" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;AI is entering a new phase where access to top talent is becoming as important as, if not more important than, compute or data. The market for AI researchers is so overheated, it’s starting to look a lot like pro sports — complete with outsized contracts and unprecedented infrastructure needs.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On today’s episode of Equity, Rebecca Bellan chatted with Deedy Das, principal at Menlo Ventures. Das has seen this shift from multiple angles, first as an engineer and product leader at Google, Facebook, and AI startup Glean, and now as an investor helping technical founders figure out how to build enduring companies in this new AI landscape.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;“The reason people are being paid this much is because there’s a disparity between the prize to be made in a short amount of time and the amount of people who have the talent to get you to that prize,” Das explained. “As long as that gap remains, you pay up and you get the talent. […] Over time, there will be less prize in AI. I imagine a lot of that value will be captured by a few people, and there will be a lot more talent to fill the supply.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why Meta is spending billions on both compute and researchers.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How compensation packages and acquisitions are warping startup hiring and retention.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;What motivates top researchers to leave, even when they’ve already made millions.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;How VCs are thinking about key-person risk in the AI era.&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="wp-block-paragraph"&gt;Equity will be back Friday with our weekly news roundup, so stay tuned.&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&amp;nbsp;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; X&lt;/em&gt;&lt;em&gt; and&lt;/em&gt;&lt;em&gt; Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/ais-talent-arms-race-is-starting-to-look-like-pro-sports/</guid><pubDate>Wed, 23 Jul 2025 14:35:00 +0000</pubDate></item><item><title>Aeneas transforms how historians connect the past (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/aeneas-transforms-how-historians-connect-the-past/</link><description>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Research&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-07-23"&gt;23 July 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;The Aeneas team&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="alt" class="picture__image" height="603" src="https://lh3.googleusercontent.com/eMe8bNk3nHJU_unVGcgIUKuPiI-it3NstOK0wixMnl_EwVI5RudgU2W6ktg0RMLsEovZyA8ckoMg2t9_ARQKev-HZhTgFzKTQtU4UC6dr6hektPG=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Introducing the first model for contextualizing ancient inscriptions, designed to help historians better interpret, attribute and restore fragmentary texts.&lt;/p&gt;&lt;p&gt;Writing was everywhere in the Roman world — etched onto everything from imperial monuments to everyday objects. From political graffiti, love poems and epitaphs to business transactions, birthday invitations and magical spells, inscriptions offer modern historians rich insights into the diversity of everyday life across the Roman world.&lt;/p&gt;&lt;p&gt;Often, these texts are fragmentary, weathered or deliberately defaced. Restoring, dating and placing them is nearly impossible without contextual information, especially when comparing similar inscriptions.&lt;/p&gt;&lt;p&gt;Today, we’re publishing a paper in Nature introducing Aeneas, the first artificial intelligence (AI) model for contextualizing ancient inscriptions.&lt;/p&gt;&lt;p&gt;When working with ancient inscriptions, historians traditionally rely on their expertise and specialized resources to identify “parallels” — which are texts that share similarities in wording, syntax, standardized formulas or provenance.&lt;/p&gt;&lt;p&gt;Aeneas greatly accelerates this complex and time-consuming work. It reasons across thousands of Latin inscriptions, retrieving textual and contextual parallels in seconds that allow historians to interpret and build upon&amp;nbsp;the model’s findings.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Our model can also be adapted to other ancient languages, scripts and media, from papyri to coinage, expanding its capabilities to help draw connections across a wider range of historical evidence.&lt;/p&gt;&lt;p&gt;We co-developed Aeneas with the University of Nottingham, and in partnership with researchers at the Universities of Warwick, Oxford and Athens University of Economics and Business (AUEB). This work was part of a wider effort to explore how generative AI can help historians better identify and interpret parallels at scale.&lt;/p&gt;&lt;p&gt;We want this research to benefit as many people as possible, so we’re making an interactive version of Aeneas freely-available to researchers, students, educators, museum professionals and more at predictingthepast.com. To support further research, we’re also open-sourcing our code and dataset.&lt;/p&gt;&lt;h2&gt;Aeneas’ advanced capabilities&lt;/h2&gt;&lt;p&gt;Named after the wandering hero of Graeco-Roman mythology, Aeneas builds upon Ithaca, our earlier work using AI to restore, date and place ancient Greek inscriptions.&lt;/p&gt;&lt;p&gt;Aeneas goes a step further, helping historians interpret and contextualize a text, give meaning to isolated fragments, draw richer conclusions and piece together a better understanding of ancient history.&lt;/p&gt;&lt;p&gt;Our model’s advanced capabilities include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Parallels search:&lt;/strong&gt; It searches for parallels across a vast collection of Latin inscriptions. By turning each text into a kind of historical fingerprint, Aeneas identifies deep connections that can help historians situate inscriptions within their broader historical context.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Processing multimodal input:&lt;/strong&gt; Aeneas is the first model to determine a text's geographical provenance using multimodal inputs. It analyzes both text and visual information, like images of an inscription.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Restoring gaps of unknown length:&lt;/strong&gt; For the first time, Aeneas can restore gaps in texts where the missing length is unknown. This makes it a more versatile tool for historians dealing with heavily damaged material.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;State-of-the-art performance:&lt;/strong&gt; Aeneas sets a new state-of-the-art benchmark in restoring damaged texts and predicting when and where they were written.&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-40a13ded-d615-48fd-8864-760b811b1d5a"&gt;
    &lt;p&gt;Animation of a restored bronze military diploma from Sardinia 113/14 C.E. (&lt;i&gt;CIL&lt;/i&gt; XVI, 60).&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;How Aeneas works&lt;/h2&gt;&lt;p&gt;Aeneas is a multimodal generative neural network that takes an inscription’s text and image as input. To train Aeneas, we curated a large and reliable dataset, drawing from decades of work by historians to create digital collections, especially the Epigraphic Database Roma (EDR), Epigraphic Database Heidelberg (EDH) and Epigraphic Database Clauss Slaby (EDCS-ELT).&lt;/p&gt;&lt;p&gt;We cleaned, harmonized and linked these records into a single machine-actionable dataset that we refer to as the Latin Epigraphic Dataset (LED), comprising over 176,000 Latin inscriptions from across the ancient Roman world.&lt;/p&gt;&lt;p&gt;Our model uses a transformer-based decoder to process the textual input of an inscription. Specialized networks handle character restoration and dating using text, while geographical attribution also uses images of the inscriptions as input. The decoder retrieves similar inscriptions from the LED, ranked by relevance.&lt;/p&gt;&lt;p&gt;For each inscription, Aeneas’ contextualization mechanism retrieves a list of parallels using a technique called “embeddings” — encoding the textual and contextual information of each inscription into a kind of historical fingerprint containing details of what the text says, its language, when and where it came from, and how it relates to other inscriptions.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-a19ad875-18af-40dd-984b-09593964c0f6"&gt;
    &lt;p&gt;Diagram of Aeneas’ architecture showing how the model takes text and image input to generate province, date and restoration predictions.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;State-of-the-art performance&lt;/h2&gt;&lt;p&gt;Aeneas groups inscriptions by date of writing far more clearly than other general-purpose models also trained on Latin, as shown in the visualization below.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-ebcd9939-3f0e-401d-af55-841453a2057b"&gt;
    &lt;p&gt;Uniform Manifold Approximation and Projection (UMAP) visualization illustrating the chronological attribution of Aeneas’ historically rich embeddings compared to generic large language model textual embeddings.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Aeneas restores damaged inscriptions with a Top-20 accuracy of 73% in gaps of up to ten characters. This only decreases to 58% when the restoration length is unknown - itself an incredibly challenging task. It also shows its reasoning in an interpretable way, providing saliency maps that highlight which parts of the inputs influenced its predictions. Thanks to its use of visual data, our model can attribute an inscription to one of 62 ancient Roman provinces with 72% accuracy. For dating, Aeneas places a text within 13 years of the date ranges provided by historians.&lt;/p&gt;&lt;h2&gt;A new lens on historical debates&lt;/h2&gt;&lt;p&gt;To test Aeneas’ capabilities on an ongoing research debate, we gave it one of the most famous Roman inscriptions: the &lt;i&gt;Res Gestae Divi Augusti,&lt;/i&gt; Emperor Augustus’ first-person account of his achievements.&lt;/p&gt;&lt;p&gt;Historians have long-argued about the dating of this inscription. Rather than predicting a single fixed date, Aeneas produced a detailed distribution of possible dates, showing two distinct peaks, with one smaller peak around 10-1  BCE and a larger, more confident peak between 10-20 CE. These results captured both prevailing dating hypotheses in a quantitative way.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-c4acf043-f24d-41a6-93c4-8a28d712acce"&gt;
    &lt;p&gt;Histogram showing Aeneas’ chronological attribution prediction for the &lt;i&gt;Res Gestae&lt;/i&gt;, which models scholarly debates around dating this famous inscription.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Aeneas based its predictions on subtle linguistic features and historical markers such as official titles and monuments mentioned in the text. By turning the dating question into a probabilistic estimate grounded in linguistic and contextual data, our model offers a new, quantitative way of engaging with long-standing historical debates.&lt;/p&gt;&lt;p&gt;Most importantly, Aeneas also retrieved many relevant parallels from imperial legal texts tied to Augustus’ legacy, highlighting how the ideology of empire was reproduced across media and geography.&lt;/p&gt;&lt;h2&gt;Advancing historical research collaboratively&lt;/h2&gt;&lt;p&gt;To assess Aeneas’ impact as an aid for research, we conducted a large-scale Historian and AI collaborative study. We invited twenty-three historians who regularly work with inscriptions to restore, date and place a set of texts using Aeneas.&lt;/p&gt;&lt;p&gt;Our evaluation, summarized in the table below, shows how the most effective results were achieved when historians used Aeneas’ contextual information alongside its predictions for restoring and attributing Roman inscriptions.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-711d77ad-af97-423c-8f11-4c707bc71eb5"&gt;
    &lt;p&gt;Table showing historians’ performance on three epigraphic tasks (restoration, geographical attribution, dating) using 60 inscriptions from our database test set. Tasks were first performed independently, then with Aeneas’ parallels information, or parallels and predictions together.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Aeneas helped the historians in our study identify new parallels and increased their confidence when tackling complex epigraphic tasks. Historians consistently highlighted Aeneas’ value in accelerating their work and expanding the range of most relevant parallel inscriptions.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;Aeneas’ parallels completely changed my perception of the inscription. It noticed details that made all the difference for restoring and chronologically attributing the text.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Anonymised historian from our study&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Sharing the tools, shaping the future&lt;/h2&gt;&lt;p&gt;Aeneas is designed to integrate within historians' existing research workflows. By combining expert knowledge with machine learning, it opens up a collaborative process, offering interpretable suggestions that serve as valuable starting points for historical inquiry.&lt;/p&gt;&lt;p&gt;As part of today’s release, we’re upgrading Ithaca, our ancient Greek model, to be powered by Aeneas and include the contextualization function, restorations of unknown length and better performance overall.&lt;/p&gt;&lt;p&gt;We’ve also co-designed a new teaching syllabus for bridging technical skills with historical thinking in the classroom. This syllabus aligns with AI literacy initiatives, including the European Commission's Digital Competences Framework for Citizens (DigComp 2.2), UNESCO’s AI Competency Framework for Students, and the preview of European Commission and the Organization for Economic Cooperation and Development (OECD) AILit Framework.&lt;/p&gt;&lt;p&gt;The Aeneas team is continuing to partner with diverse subject matter experts, using Aeneas to help shed light to our ancient past — with more to come.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about Aeneas&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The research was co-led by Yannis Assael and Thea Sommerschield.&lt;/p&gt;&lt;p&gt;Contributors include: Alison Cooley, Brendan Shillingford, John Pavlopoulos, Priyanka Suresh, Bailey Herms, Jonathan Prag, Alex Mullen and Shakir Mohamed. The Aeneas web interface was developed by Justin Grayston, Benjamin Maynard, and Nicholas Dietrich, and is powered by Google Cloud.&lt;/p&gt;&lt;p&gt;The syllabus was developed by Robbe Wulgaert, Sint-Lievenscollege, Ghent, Belgium.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</description><content:encoded>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Research&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-07-23"&gt;23 July 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;The Aeneas team&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="alt" class="picture__image" height="603" src="https://lh3.googleusercontent.com/eMe8bNk3nHJU_unVGcgIUKuPiI-it3NstOK0wixMnl_EwVI5RudgU2W6ktg0RMLsEovZyA8ckoMg2t9_ARQKev-HZhTgFzKTQtU4UC6dr6hektPG=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Introducing the first model for contextualizing ancient inscriptions, designed to help historians better interpret, attribute and restore fragmentary texts.&lt;/p&gt;&lt;p&gt;Writing was everywhere in the Roman world — etched onto everything from imperial monuments to everyday objects. From political graffiti, love poems and epitaphs to business transactions, birthday invitations and magical spells, inscriptions offer modern historians rich insights into the diversity of everyday life across the Roman world.&lt;/p&gt;&lt;p&gt;Often, these texts are fragmentary, weathered or deliberately defaced. Restoring, dating and placing them is nearly impossible without contextual information, especially when comparing similar inscriptions.&lt;/p&gt;&lt;p&gt;Today, we’re publishing a paper in Nature introducing Aeneas, the first artificial intelligence (AI) model for contextualizing ancient inscriptions.&lt;/p&gt;&lt;p&gt;When working with ancient inscriptions, historians traditionally rely on their expertise and specialized resources to identify “parallels” — which are texts that share similarities in wording, syntax, standardized formulas or provenance.&lt;/p&gt;&lt;p&gt;Aeneas greatly accelerates this complex and time-consuming work. It reasons across thousands of Latin inscriptions, retrieving textual and contextual parallels in seconds that allow historians to interpret and build upon&amp;nbsp;the model’s findings.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Our model can also be adapted to other ancient languages, scripts and media, from papyri to coinage, expanding its capabilities to help draw connections across a wider range of historical evidence.&lt;/p&gt;&lt;p&gt;We co-developed Aeneas with the University of Nottingham, and in partnership with researchers at the Universities of Warwick, Oxford and Athens University of Economics and Business (AUEB). This work was part of a wider effort to explore how generative AI can help historians better identify and interpret parallels at scale.&lt;/p&gt;&lt;p&gt;We want this research to benefit as many people as possible, so we’re making an interactive version of Aeneas freely-available to researchers, students, educators, museum professionals and more at predictingthepast.com. To support further research, we’re also open-sourcing our code and dataset.&lt;/p&gt;&lt;h2&gt;Aeneas’ advanced capabilities&lt;/h2&gt;&lt;p&gt;Named after the wandering hero of Graeco-Roman mythology, Aeneas builds upon Ithaca, our earlier work using AI to restore, date and place ancient Greek inscriptions.&lt;/p&gt;&lt;p&gt;Aeneas goes a step further, helping historians interpret and contextualize a text, give meaning to isolated fragments, draw richer conclusions and piece together a better understanding of ancient history.&lt;/p&gt;&lt;p&gt;Our model’s advanced capabilities include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Parallels search:&lt;/strong&gt; It searches for parallels across a vast collection of Latin inscriptions. By turning each text into a kind of historical fingerprint, Aeneas identifies deep connections that can help historians situate inscriptions within their broader historical context.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Processing multimodal input:&lt;/strong&gt; Aeneas is the first model to determine a text's geographical provenance using multimodal inputs. It analyzes both text and visual information, like images of an inscription.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Restoring gaps of unknown length:&lt;/strong&gt; For the first time, Aeneas can restore gaps in texts where the missing length is unknown. This makes it a more versatile tool for historians dealing with heavily damaged material.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;State-of-the-art performance:&lt;/strong&gt; Aeneas sets a new state-of-the-art benchmark in restoring damaged texts and predicting when and where they were written.&lt;/li&gt;&lt;/ul&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-40a13ded-d615-48fd-8864-760b811b1d5a"&gt;
    &lt;p&gt;Animation of a restored bronze military diploma from Sardinia 113/14 C.E. (&lt;i&gt;CIL&lt;/i&gt; XVI, 60).&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;How Aeneas works&lt;/h2&gt;&lt;p&gt;Aeneas is a multimodal generative neural network that takes an inscription’s text and image as input. To train Aeneas, we curated a large and reliable dataset, drawing from decades of work by historians to create digital collections, especially the Epigraphic Database Roma (EDR), Epigraphic Database Heidelberg (EDH) and Epigraphic Database Clauss Slaby (EDCS-ELT).&lt;/p&gt;&lt;p&gt;We cleaned, harmonized and linked these records into a single machine-actionable dataset that we refer to as the Latin Epigraphic Dataset (LED), comprising over 176,000 Latin inscriptions from across the ancient Roman world.&lt;/p&gt;&lt;p&gt;Our model uses a transformer-based decoder to process the textual input of an inscription. Specialized networks handle character restoration and dating using text, while geographical attribution also uses images of the inscriptions as input. The decoder retrieves similar inscriptions from the LED, ranked by relevance.&lt;/p&gt;&lt;p&gt;For each inscription, Aeneas’ contextualization mechanism retrieves a list of parallels using a technique called “embeddings” — encoding the textual and contextual information of each inscription into a kind of historical fingerprint containing details of what the text says, its language, when and where it came from, and how it relates to other inscriptions.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-a19ad875-18af-40dd-984b-09593964c0f6"&gt;
    &lt;p&gt;Diagram of Aeneas’ architecture showing how the model takes text and image input to generate province, date and restoration predictions.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;State-of-the-art performance&lt;/h2&gt;&lt;p&gt;Aeneas groups inscriptions by date of writing far more clearly than other general-purpose models also trained on Latin, as shown in the visualization below.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-ebcd9939-3f0e-401d-af55-841453a2057b"&gt;
    &lt;p&gt;Uniform Manifold Approximation and Projection (UMAP) visualization illustrating the chronological attribution of Aeneas’ historically rich embeddings compared to generic large language model textual embeddings.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Aeneas restores damaged inscriptions with a Top-20 accuracy of 73% in gaps of up to ten characters. This only decreases to 58% when the restoration length is unknown - itself an incredibly challenging task. It also shows its reasoning in an interpretable way, providing saliency maps that highlight which parts of the inputs influenced its predictions. Thanks to its use of visual data, our model can attribute an inscription to one of 62 ancient Roman provinces with 72% accuracy. For dating, Aeneas places a text within 13 years of the date ranges provided by historians.&lt;/p&gt;&lt;h2&gt;A new lens on historical debates&lt;/h2&gt;&lt;p&gt;To test Aeneas’ capabilities on an ongoing research debate, we gave it one of the most famous Roman inscriptions: the &lt;i&gt;Res Gestae Divi Augusti,&lt;/i&gt; Emperor Augustus’ first-person account of his achievements.&lt;/p&gt;&lt;p&gt;Historians have long-argued about the dating of this inscription. Rather than predicting a single fixed date, Aeneas produced a detailed distribution of possible dates, showing two distinct peaks, with one smaller peak around 10-1  BCE and a larger, more confident peak between 10-20 CE. These results captured both prevailing dating hypotheses in a quantitative way.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-c4acf043-f24d-41a6-93c4-8a28d712acce"&gt;
    &lt;p&gt;Histogram showing Aeneas’ chronological attribution prediction for the &lt;i&gt;Res Gestae&lt;/i&gt;, which models scholarly debates around dating this famous inscription.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Aeneas based its predictions on subtle linguistic features and historical markers such as official titles and monuments mentioned in the text. By turning the dating question into a probabilistic estimate grounded in linguistic and contextual data, our model offers a new, quantitative way of engaging with long-standing historical debates.&lt;/p&gt;&lt;p&gt;Most importantly, Aeneas also retrieved many relevant parallels from imperial legal texts tied to Augustus’ legacy, highlighting how the ideology of empire was reproduced across media and geography.&lt;/p&gt;&lt;h2&gt;Advancing historical research collaboratively&lt;/h2&gt;&lt;p&gt;To assess Aeneas’ impact as an aid for research, we conducted a large-scale Historian and AI collaborative study. We invited twenty-three historians who regularly work with inscriptions to restore, date and place a set of texts using Aeneas.&lt;/p&gt;&lt;p&gt;Our evaluation, summarized in the table below, shows how the most effective results were achieved when historians used Aeneas’ contextual information alongside its predictions for restoring and attributing Roman inscriptions.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-711d77ad-af97-423c-8f11-4c707bc71eb5"&gt;
    &lt;p&gt;Table showing historians’ performance on three epigraphic tasks (restoration, geographical attribution, dating) using 60 inscriptions from our database test set. Tasks were first performed independently, then with Aeneas’ parallels information, or parallels and predictions together.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;Aeneas helped the historians in our study identify new parallels and increased their confidence when tackling complex epigraphic tasks. Historians consistently highlighted Aeneas’ value in accelerating their work and expanding the range of most relevant parallel inscriptions.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;Aeneas’ parallels completely changed my perception of the inscription. It noticed details that made all the difference for restoring and chronologically attributing the text.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Anonymised historian from our study&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Sharing the tools, shaping the future&lt;/h2&gt;&lt;p&gt;Aeneas is designed to integrate within historians' existing research workflows. By combining expert knowledge with machine learning, it opens up a collaborative process, offering interpretable suggestions that serve as valuable starting points for historical inquiry.&lt;/p&gt;&lt;p&gt;As part of today’s release, we’re upgrading Ithaca, our ancient Greek model, to be powered by Aeneas and include the contextualization function, restorations of unknown length and better performance overall.&lt;/p&gt;&lt;p&gt;We’ve also co-designed a new teaching syllabus for bridging technical skills with historical thinking in the classroom. This syllabus aligns with AI literacy initiatives, including the European Commission's Digital Competences Framework for Citizens (DigComp 2.2), UNESCO’s AI Competency Framework for Students, and the preview of European Commission and the Organization for Economic Cooperation and Development (OECD) AILit Framework.&lt;/p&gt;&lt;p&gt;The Aeneas team is continuing to partner with diverse subject matter experts, using Aeneas to help shed light to our ancient past — with more to come.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about Aeneas&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The research was co-led by Yannis Assael and Thea Sommerschield.&lt;/p&gt;&lt;p&gt;Contributors include: Alison Cooley, Brendan Shillingford, John Pavlopoulos, Priyanka Suresh, Bailey Herms, Jonathan Prag, Alex Mullen and Shakir Mohamed. The Aeneas web interface was developed by Justin Grayston, Benjamin Maynard, and Nicholas Dietrich, and is powered by Google Cloud.&lt;/p&gt;&lt;p&gt;The syllabus was developed by Robbe Wulgaert, Sint-Lievenscollege, Ghent, Belgium.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/aeneas-transforms-how-historians-connect-the-past/</guid><pubDate>Wed, 23 Jul 2025 14:59:00 +0000</pubDate></item><item><title>Google DeepMind’s new AI can help historians understand ancient Latin inscriptions (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/07/23/1120574/deepmind-ai-aeneas-helps-historians-interpret-latin-inscriptions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/aeneas2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Google DeepMind has unveiled new artificial-intelligence software that could help historians recover the meaning and context behind ancient Latin engravings.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Aeneas can analyze words written in long-weathered stone to say when and where they were originally inscribed. It follows Google’s previous archaeological tool Ithaca, which also used deep learning to reconstruct and contextualize ancient text, in its case Greek. But while Ithaca and Aeneas use some similar systems, Aeneas also promises to give researchers jumping-off points for further analysis.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;To do this, Aeneas takes in partial transcriptions of an inscription alongside a scanned image of it. Using these, it gives possible dates and places of origins for the engraving, along with potential fill-ins for any missing text. For example, a slab damaged at the start and continuing with &lt;em&gt;... us populusque Romanus&lt;/em&gt; would likely prompt Aeneas to guess that &lt;em&gt;Senat&lt;/em&gt; comes before &lt;em&gt;us&lt;/em&gt; to create the phrase &lt;em&gt;Senatus populusque Romanus&lt;/em&gt;, “The Senate and the people of Rome.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is similar to how Ithaca works. But Aeneas also cross-references the text with a stored database of almost 150,000 inscriptions, which originated everywhere from modern-day Britain to modern-day Iraq, to give possible parallels—other catalogued Latin engravings that feature similar words, phrases, and analogies.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;This database, alongside a few thousand images of inscriptions, makes up the training set for Aeneas’s deep neural network. While it may seem like a good number of samples, it pales in comparison to the billions of documents used to train general-purpose large language models like Google’s Gemini. There simply aren’t enough high-quality scans of inscriptions to train a language model to learn this kind of task. That’s why specialized solutions like Aeneas are needed.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The Aeneas team believes it could help researchers “connect the past,” said Yannis Assael, a researcher at Google DeepMind who worked on the project. Rather than seeking to automate epigraphy—the research field dealing with deciphering and understanding inscriptions—he and his colleagues are interested in “crafting a tool that will integrate with the workflow of a historian,” Assael said in a press briefing.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Their goal is to give researchers trying to analyze a specific inscription many hypotheses to work from, saving them the effort of sifting through records by hand. To validate the system, the team presented 23 historians with inscriptions that had been previously dated and tested their workflows both with and without Aeneas. The findings, which were published today in &lt;em&gt;Nature&lt;/em&gt;, showed that Aeneas helped spur research ideas among the historians for 90% of inscriptions and that it led to more accurate determinations of where and when the inscriptions originated.&lt;/p&gt;  &lt;p&gt;In addition to this study, the researchers tested Aeneas on the Monumentum Ancyranum, a famous inscription carved into the walls of a temple in Ankara, Turkey. Here, Aeneas managed to give estimates and parallels that reflected existing historical analysis of the work, and in its attention to detail, the paper claims, it closely matched how a trained historian would approach the problem. “That was jaw-dropping,” Thea Sommerschield, an epigrapher at the University of Nottingham who also worked on Aeneas, said in the press briefing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, much remains to be seen about Aeneas’s capabilities in the real world. It doesn’t guess the meaning of texts, so it can’t interpret newly found engravings on its own, and it’s not clear yet how useful it will be to historians’ workflows in the long term, according to Kathleen Coleman, a professor of classics at Harvard. The Monumentum Ancyranum is considered to be one of the best-known and most well-studied inscriptions in epigraphy, raising the question of how Aeneas will fare on more obscure samples.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;Google DeepMind has now made Aeneas open-source, and the interface for the system is freely available for teachers, students, museum workers, and academics. The group is working with schools in Belgium to integrate Aeneas into their secondary history education.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“To have Aeneas at your side while you’re in the museum or at the archaeological site where a new inscription has just been found—that is our sort of dream scenario,” Sommerschield said.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/07/aeneas2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Google DeepMind has unveiled new artificial-intelligence software that could help historians recover the meaning and context behind ancient Latin engravings.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Aeneas can analyze words written in long-weathered stone to say when and where they were originally inscribed. It follows Google’s previous archaeological tool Ithaca, which also used deep learning to reconstruct and contextualize ancient text, in its case Greek. But while Ithaca and Aeneas use some similar systems, Aeneas also promises to give researchers jumping-off points for further analysis.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;To do this, Aeneas takes in partial transcriptions of an inscription alongside a scanned image of it. Using these, it gives possible dates and places of origins for the engraving, along with potential fill-ins for any missing text. For example, a slab damaged at the start and continuing with &lt;em&gt;... us populusque Romanus&lt;/em&gt; would likely prompt Aeneas to guess that &lt;em&gt;Senat&lt;/em&gt; comes before &lt;em&gt;us&lt;/em&gt; to create the phrase &lt;em&gt;Senatus populusque Romanus&lt;/em&gt;, “The Senate and the people of Rome.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is similar to how Ithaca works. But Aeneas also cross-references the text with a stored database of almost 150,000 inscriptions, which originated everywhere from modern-day Britain to modern-day Iraq, to give possible parallels—other catalogued Latin engravings that feature similar words, phrases, and analogies.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;This database, alongside a few thousand images of inscriptions, makes up the training set for Aeneas’s deep neural network. While it may seem like a good number of samples, it pales in comparison to the billions of documents used to train general-purpose large language models like Google’s Gemini. There simply aren’t enough high-quality scans of inscriptions to train a language model to learn this kind of task. That’s why specialized solutions like Aeneas are needed.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The Aeneas team believes it could help researchers “connect the past,” said Yannis Assael, a researcher at Google DeepMind who worked on the project. Rather than seeking to automate epigraphy—the research field dealing with deciphering and understanding inscriptions—he and his colleagues are interested in “crafting a tool that will integrate with the workflow of a historian,” Assael said in a press briefing.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Their goal is to give researchers trying to analyze a specific inscription many hypotheses to work from, saving them the effort of sifting through records by hand. To validate the system, the team presented 23 historians with inscriptions that had been previously dated and tested their workflows both with and without Aeneas. The findings, which were published today in &lt;em&gt;Nature&lt;/em&gt;, showed that Aeneas helped spur research ideas among the historians for 90% of inscriptions and that it led to more accurate determinations of where and when the inscriptions originated.&lt;/p&gt;  &lt;p&gt;In addition to this study, the researchers tested Aeneas on the Monumentum Ancyranum, a famous inscription carved into the walls of a temple in Ankara, Turkey. Here, Aeneas managed to give estimates and parallels that reflected existing historical analysis of the work, and in its attention to detail, the paper claims, it closely matched how a trained historian would approach the problem. “That was jaw-dropping,” Thea Sommerschield, an epigrapher at the University of Nottingham who also worked on Aeneas, said in the press briefing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;However, much remains to be seen about Aeneas’s capabilities in the real world. It doesn’t guess the meaning of texts, so it can’t interpret newly found engravings on its own, and it’s not clear yet how useful it will be to historians’ workflows in the long term, according to Kathleen Coleman, a professor of classics at Harvard. The Monumentum Ancyranum is considered to be one of the best-known and most well-studied inscriptions in epigraphy, raising the question of how Aeneas will fare on more obscure samples.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;Google DeepMind has now made Aeneas open-source, and the interface for the system is freely available for teachers, students, museum workers, and academics. The group is working with schools in Belgium to integrate Aeneas into their secondary history education.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“To have Aeneas at your side while you’re in the museum or at the archaeological site where a new inscription has just been found—that is our sort of dream scenario,” Sommerschield said.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/07/23/1120574/deepmind-ai-aeneas-helps-historians-interpret-latin-inscriptions/</guid><pubDate>Wed, 23 Jul 2025 15:00:08 +0000</pubDate></item><item><title>Technical approach for classifying human-AI interactions at scale (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/technical-approach-for-classifying-human-ai-interactions-at-scale/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="The image features four white icons on a gradient background that transitions from blue on the left to green on the right. The first icon is a network or molecule structure with interconnected nodes. The second icon shows a stylized person in front of a computer screen. The third icon shows an organization tree with one main node and three nodes branching out side by side below it." class="wp-image-1144473" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;As large language models (LLMs) become foundational to modern AI systems, the ability to run them at scale—efficiently, reliably, and in near real-time—is no longer a nice-to-have. It’s essential. The Semantic Telemetry project tackles this challenge by applying LLM-based classifiers to hundreds of millions of sampled, anonymized Bing Chat conversations each week. These classifiers extract signals like user expertise, primary topic, and satisfaction, enabling deeper insight into human-AI interactions and driving continuous system improvement.&lt;/p&gt;



&lt;p&gt;But building a pipeline that can handle this volume isn’t just about plugging into an API. It requires a high-throughput, high-performance architecture that can orchestrate distributed processing, manage token and prompt complexity, and gracefully handle the unpredictability of remote LLM endpoints.&lt;/p&gt;



&lt;p&gt;In this latest post in our series on Semantic Telemetry, we’ll walk through the engineering behind that system—how we designed for scale from the start, the trade-offs we made, and the lessons we learned along the way. From batching strategies and token optimization and orchestration, we’ll share what it takes to build a real-time LLM classification pipeline.&lt;/p&gt;



&lt;p&gt;For additional project background: Semantic Telemetry: Understanding how users interact with AI systems and Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project.&lt;/p&gt;







&lt;h2 class="wp-block-heading" id="system-architecture-highlights"&gt;System architecture highlights&lt;/h2&gt;



&lt;p&gt;The Semantic Telemetry pipeline&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is a highly-scalable, highly-configurable, data transformation pipeline. While it follows a familiar ETL structure, several architectural innovations make it uniquely suited for high-throughput LLM integration:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Hybrid compute engine&lt;/strong&gt;&lt;br /&gt;The pipeline combines the distributed power of PySpark with the speed and simplicity of Polars, enabling it to scale across large datasets or run lightweight jobs in Spark-less environments—without code changes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;LLM-centric transformation layer&lt;/strong&gt;&lt;br /&gt;At the core of the pipeline is a multi-stage transformation process tailored for running across multiple LLM endpoints such that:
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Runs model agnostic. Provides a generic interface for LLMs and adopts model specific interfaces built from a generic interface.&lt;/li&gt;



&lt;li&gt;Prompt templates are defined using the Prompty language specification for consistency and reuse, with options for users to include custom prompts.&lt;/li&gt;



&lt;li&gt;Parsing and cleaning logic ensures structured, schema-aligned outputs, even when LLM responses are imperfect such as removing extra characters in output, resolving not-exact label matches (i.e. “create” versus “created”) and relabeling invalid classifications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Architecture diagram of LLM workflow" class="wp-image-1144472" height="650" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px.png" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Architecture diagram&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The pipeline supports multiple classification tasks (e.g., user expertise, topic, satisfaction) through modular prompt templates and configurable execution paths—making it easy to adapt to new use cases or environments.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="engineering-challenges-solutions"&gt;Engineering challenges &amp;amp; solutions&lt;/h2&gt;



&lt;p&gt;Building a high-throughput, LLM-powered classification pipeline at scale introduced a range of engineering challenges—from managing latency and token limits to ensuring system resilience. Below are the key hurdles we encountered and how we addressed them.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="llm-endpoint-latency-variability"&gt;LLM endpoint latency &amp;amp; variability&lt;/h3&gt;



&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: LLM endpoints, especially those hosted remotely (e.g., Azure OpenAI), introduce unpredictable latency due to model load, prompt complexity, and network variability. This made it difficult to maintain consistent throughput across the pipeline.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We implemented a combination of:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Multiple Azure OpenAI endpoints&lt;/strong&gt; in rotation to increase throughput and distribute workload. We can analyze throughput and redistribute as needed.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Saving output in intervals&lt;/strong&gt; to write data asynchronously in case of network errors.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Utilizing models with higher tokens per minute (TPM)&lt;/strong&gt; such as OpenAI’s GPT-4o mini. GPT-4o mini had a 2M TPM limit which is a 25x throughput increase from GPT-4 (80K TPM -&amp;gt; 2M TPM)&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Timeouts and retries&lt;/strong&gt; with exponential backoff.&lt;/li&gt;
&lt;/ul&gt;



&lt;h3 class="wp-block-heading" id="evolving-llm-models-prompt-alignment"&gt;Evolving LLM models &amp;amp; prompt alignment&lt;/h3&gt;



&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Each new LLM release—such as Phi, Mistral, DeepSeek, and successive generations of GPT (e.g., GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4o)—brings improvements, but also subtle behavioral shifts. These changes can affect classification consistency, output formatting, and even the interpretation of prompts. Maintaining alignment with baseline expectations across models became a moving target.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We developed a model evaluation workflow to test prompt alignment across LLM versions:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Small-sample testing&lt;/strong&gt;: We ran the pipeline on a representative sample using the new model and compared the output distribution to a known baseline.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Distribution analysis&lt;/strong&gt;: If the new model’s output aligned closely, we scaled up testing. If not, we iteratively &lt;strong&gt;tuned the prompts&lt;/strong&gt; and re-ran comparisons.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Interpretation flexibility&lt;/strong&gt;: We also recognized that a shift in distribution isn’t always a regression. Sometimes it reflects a more accurate or nuanced classification, especially as models improve.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;To support this process, we used tools like Sammo&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which allowed us to compare outputs across multiple models and prompt variants. This helped us quantify the impact of prompt changes and model upgrades and make informed decisions about when to adopt a new model or adjust our classification schema.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="dynamic-concurrency-scaling-for-llm-calls"&gt;Dynamic concurrency scaling for LLM calls&lt;/h3&gt;



&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: LLM endpoints frequently encounter rate limits and inconsistent response times under heavy usage. The models’ speeds can also vary, complicating the selection of optimal concurrency levels. Furthermore, users may choose suboptimal settings due to lack of familiarity, and default concurrency configurations are rarely ideal for every situation. Dynamic adjustments based on throughput, measured in various ways, can assist in determining optimal concurrency levels.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We implemented a dynamic concurrency control mechanism that proactively adjusts the number of parallel LLM calls based on real-time system behavior:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;External task awareness&lt;/strong&gt;: The system monitors the number of parallel tasks running across the pipeline (e.g., Spark executors or async workers) and uses this to inform the initial concurrency level.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Success/failure rate monitoring&lt;/strong&gt;: The system tracks the rolling success and failure rates of LLM calls. A spike in failures triggers a temporary reduction in concurrency, while sustained success allows for gradual ramp-up.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Latency-based feedback loop&lt;/strong&gt;: Instead of waiting for rate-limit errors, measure the response time of LLM calls. If latency increases, reduce concurrency; if latency decreases and success rates remain high, cautiously scale up.&lt;/li&gt;
&lt;/ul&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="optimization-experiments"&gt;Optimization experiments&lt;/h2&gt;



&lt;p&gt;To further improve throughput and efficiency, we ran a series of optimization experiments. Each approach came with trade-offs that we carefully measured.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="batch-endpoints-azure-openai"&gt;Batch endpoints (Azure/OpenAI)&lt;/h3&gt;



&lt;p&gt;Batch endpoints are a cost-effective, moderately high-throughput way of executing LLM requests. Batch endpoints process large lists of LLM prompts over a 24-hour period, recording responses in a file. They are about 50% cheaper than non-batch endpoints and have separate token limits, enabling increased throughput when used alongside regular endpoints. However, they require at least 24 hours to complete requests and provide lower overall throughput compared to non-batch endpoints, making them unsuitable for situations needing quick results.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="conversation-batching-in-prompts-during-pipeline-runtime"&gt;Conversation batching in prompts during pipeline runtime&lt;/h3&gt;



&lt;p&gt;Batching multiple conversations for classification at once can significantly increase throughput and reduce token usage, but it may impact the accuracy of results. In our experiment with a domain classifier, classifying 10 conversations simultaneously led to an average of 15-20% of domain assignments changing between repeated runs of the same prompt. To address this, one mitigation approach is to use a grader LLM prompt: first classify the batch, then have the LLM identify any incorrectly classified conversations, and finally re-classify those as needed. While batching offers efficiency gains, it is important to monitor for potential drops in classification quality.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="combining-classifiers-in-a-single-prompt"&gt;Combining classifiers in a single prompt&lt;/h3&gt;



&lt;p&gt;Combining multiple classifiers into a single prompt increases throughput by allowing one call to the LLM instead of multiple calls. This not only multiplies the overall throughput by the number of classifiers processed but also reduces the total number of tokens used, since the conversation text is only passed in once. However, this approach may compromise classification accuracy, so results should be closely monitored.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="classification-using-text-embeddings"&gt;Classification using text embeddings&lt;/h3&gt;



&lt;p&gt;An alternative approach is to train custom neural network models for each classifier using only the text embeddings of conversations. This method delivers both cost and time savings by avoiding making multiple LLM requests for every classifier and conversation—instead, the system only needs to request conversation text embeddings once and can reuse these embeddings across all classifier models.&lt;/p&gt;



&lt;p&gt;For example, starting with a set of conversations to validate and test the new model, run these conversations through the original prompt-based classifier to generate a set of golden classifications, then obtain text embeddings (using a tool like text-embedding-3-large) for each conversation. These embeddings and their corresponding classifications are used to train a model such as a multi-layer perceptron. In production, the workflow involves retrieving the text embedding for each conversation and passing it through the trained model; if there is a model for each classifier, a single embedding retrieval per conversation suffices for all classifiers.&lt;/p&gt;



&lt;p&gt;The benefits of this approach include significantly increased throughput and cost savings—since it’s not necessary to call the LLM for every classifier and conversation. However, this setup can require GPU compute which can increase costs and infrastructure complexity, and the resulting models may not achieve the same accuracy as prompt-based classification methods.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="prompt-compression"&gt;Prompt compression&lt;/h3&gt;



&lt;p&gt;Compressing prompts by eliminating unnecessary tokens or by using a tool such as LLMLingua&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to automate prompt compression can optimize classification prompts either ahead of time or in real-time. This approach increases overall throughput and results in cost savings due to a reduced number of tokens, but there are risks: changes to the classifier prompt or conversation text may impact classification accuracy, and depending on the compression technique, it could even decrease throughput if the compression process takes longer than simply sending uncompressed text to the LLM.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="text-truncation"&gt;Text truncation&lt;/h3&gt;



&lt;p&gt;Truncating conversations to a specific length limits the overall number of tokens sent through an endpoint, offering cost savings and increased throughput like prompt compression. By reducing the number of tokens per request, throughput rises because more requests can be made before reaching the endpoint’s tokens-per-minute (TPM) limit, and costs decrease due to fewer tokens being processed. However, the ideal truncation length depends on both the classifiers and the conversation content, so it’s important to assess how truncation affects output quality before implementation. While this approach brings clear efficiency benefits, it also poses a risk: long conversations may have their most important content cut off, which can reduce classification accuracy.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="conclusion"&gt;Conclusion&lt;/h2&gt;



&lt;p&gt;Building a scalable, high-throughput pipeline for LLM-based classification is far from trivial. It requires navigating a constantly shifting landscape of model capabilities, prompt behaviors, and infrastructure constraints. As LLMs become faster, cheaper, and more capable, they’re unlocking new possibilities for real-time understanding of human-AI interactions at scale. The techniques we’ve shared represent a snapshot of what’s working today. But more importantly, they offer a foundation for what’s possible tomorrow.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="The image features four white icons on a gradient background that transitions from blue on the left to green on the right. The first icon is a network or molecule structure with interconnected nodes. The second icon shows a stylized person in front of a computer screen. The third icon shows an organization tree with one main node and three nodes branching out side by side below it." class="wp-image-1144473" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/SemanticTelemetry3-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;As large language models (LLMs) become foundational to modern AI systems, the ability to run them at scale—efficiently, reliably, and in near real-time—is no longer a nice-to-have. It’s essential. The Semantic Telemetry project tackles this challenge by applying LLM-based classifiers to hundreds of millions of sampled, anonymized Bing Chat conversations each week. These classifiers extract signals like user expertise, primary topic, and satisfaction, enabling deeper insight into human-AI interactions and driving continuous system improvement.&lt;/p&gt;



&lt;p&gt;But building a pipeline that can handle this volume isn’t just about plugging into an API. It requires a high-throughput, high-performance architecture that can orchestrate distributed processing, manage token and prompt complexity, and gracefully handle the unpredictability of remote LLM endpoints.&lt;/p&gt;



&lt;p&gt;In this latest post in our series on Semantic Telemetry, we’ll walk through the engineering behind that system—how we designed for scale from the start, the trade-offs we made, and the lessons we learned along the way. From batching strategies and token optimization and orchestration, we’ll share what it takes to build a real-time LLM classification pipeline.&lt;/p&gt;



&lt;p&gt;For additional project background: Semantic Telemetry: Understanding how users interact with AI systems and Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project.&lt;/p&gt;







&lt;h2 class="wp-block-heading" id="system-architecture-highlights"&gt;System architecture highlights&lt;/h2&gt;



&lt;p&gt;The Semantic Telemetry pipeline&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is a highly-scalable, highly-configurable, data transformation pipeline. While it follows a familiar ETL structure, several architectural innovations make it uniquely suited for high-throughput LLM integration:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Hybrid compute engine&lt;/strong&gt;&lt;br /&gt;The pipeline combines the distributed power of PySpark with the speed and simplicity of Polars, enabling it to scale across large datasets or run lightweight jobs in Spark-less environments—without code changes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;LLM-centric transformation layer&lt;/strong&gt;&lt;br /&gt;At the core of the pipeline is a multi-stage transformation process tailored for running across multiple LLM endpoints such that:
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Runs model agnostic. Provides a generic interface for LLMs and adopts model specific interfaces built from a generic interface.&lt;/li&gt;



&lt;li&gt;Prompt templates are defined using the Prompty language specification for consistency and reuse, with options for users to include custom prompts.&lt;/li&gt;



&lt;li&gt;Parsing and cleaning logic ensures structured, schema-aligned outputs, even when LLM responses are imperfect such as removing extra characters in output, resolving not-exact label matches (i.e. “create” versus “created”) and relabeling invalid classifications.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Architecture diagram of LLM workflow" class="wp-image-1144472" height="650" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Semantic-Telemetry-Pipeline-2_1400px.png" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Architecture diagram&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The pipeline supports multiple classification tasks (e.g., user expertise, topic, satisfaction) through modular prompt templates and configurable execution paths—making it easy to adapt to new use cases or environments.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="engineering-challenges-solutions"&gt;Engineering challenges &amp;amp; solutions&lt;/h2&gt;



&lt;p&gt;Building a high-throughput, LLM-powered classification pipeline at scale introduced a range of engineering challenges—from managing latency and token limits to ensuring system resilience. Below are the key hurdles we encountered and how we addressed them.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="llm-endpoint-latency-variability"&gt;LLM endpoint latency &amp;amp; variability&lt;/h3&gt;



&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: LLM endpoints, especially those hosted remotely (e.g., Azure OpenAI), introduce unpredictable latency due to model load, prompt complexity, and network variability. This made it difficult to maintain consistent throughput across the pipeline.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We implemented a combination of:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Multiple Azure OpenAI endpoints&lt;/strong&gt; in rotation to increase throughput and distribute workload. We can analyze throughput and redistribute as needed.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Saving output in intervals&lt;/strong&gt; to write data asynchronously in case of network errors.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Utilizing models with higher tokens per minute (TPM)&lt;/strong&gt; such as OpenAI’s GPT-4o mini. GPT-4o mini had a 2M TPM limit which is a 25x throughput increase from GPT-4 (80K TPM -&amp;gt; 2M TPM)&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Timeouts and retries&lt;/strong&gt; with exponential backoff.&lt;/li&gt;
&lt;/ul&gt;



&lt;h3 class="wp-block-heading" id="evolving-llm-models-prompt-alignment"&gt;Evolving LLM models &amp;amp; prompt alignment&lt;/h3&gt;



&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: Each new LLM release—such as Phi, Mistral, DeepSeek, and successive generations of GPT (e.g., GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4o)—brings improvements, but also subtle behavioral shifts. These changes can affect classification consistency, output formatting, and even the interpretation of prompts. Maintaining alignment with baseline expectations across models became a moving target.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We developed a model evaluation workflow to test prompt alignment across LLM versions:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Small-sample testing&lt;/strong&gt;: We ran the pipeline on a representative sample using the new model and compared the output distribution to a known baseline.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Distribution analysis&lt;/strong&gt;: If the new model’s output aligned closely, we scaled up testing. If not, we iteratively &lt;strong&gt;tuned the prompts&lt;/strong&gt; and re-ran comparisons.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Interpretation flexibility&lt;/strong&gt;: We also recognized that a shift in distribution isn’t always a regression. Sometimes it reflects a more accurate or nuanced classification, especially as models improve.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;To support this process, we used tools like Sammo&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which allowed us to compare outputs across multiple models and prompt variants. This helped us quantify the impact of prompt changes and model upgrades and make informed decisions about when to adopt a new model or adjust our classification schema.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="dynamic-concurrency-scaling-for-llm-calls"&gt;Dynamic concurrency scaling for LLM calls&lt;/h3&gt;



&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: LLM endpoints frequently encounter rate limits and inconsistent response times under heavy usage. The models’ speeds can also vary, complicating the selection of optimal concurrency levels. Furthermore, users may choose suboptimal settings due to lack of familiarity, and default concurrency configurations are rarely ideal for every situation. Dynamic adjustments based on throughput, measured in various ways, can assist in determining optimal concurrency levels.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: We implemented a dynamic concurrency control mechanism that proactively adjusts the number of parallel LLM calls based on real-time system behavior:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;External task awareness&lt;/strong&gt;: The system monitors the number of parallel tasks running across the pipeline (e.g., Spark executors or async workers) and uses this to inform the initial concurrency level.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Success/failure rate monitoring&lt;/strong&gt;: The system tracks the rolling success and failure rates of LLM calls. A spike in failures triggers a temporary reduction in concurrency, while sustained success allows for gradual ramp-up.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Latency-based feedback loop&lt;/strong&gt;: Instead of waiting for rate-limit errors, measure the response time of LLM calls. If latency increases, reduce concurrency; if latency decreases and success rates remain high, cautiously scale up.&lt;/li&gt;
&lt;/ul&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="optimization-experiments"&gt;Optimization experiments&lt;/h2&gt;



&lt;p&gt;To further improve throughput and efficiency, we ran a series of optimization experiments. Each approach came with trade-offs that we carefully measured.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="batch-endpoints-azure-openai"&gt;Batch endpoints (Azure/OpenAI)&lt;/h3&gt;



&lt;p&gt;Batch endpoints are a cost-effective, moderately high-throughput way of executing LLM requests. Batch endpoints process large lists of LLM prompts over a 24-hour period, recording responses in a file. They are about 50% cheaper than non-batch endpoints and have separate token limits, enabling increased throughput when used alongside regular endpoints. However, they require at least 24 hours to complete requests and provide lower overall throughput compared to non-batch endpoints, making them unsuitable for situations needing quick results.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="conversation-batching-in-prompts-during-pipeline-runtime"&gt;Conversation batching in prompts during pipeline runtime&lt;/h3&gt;



&lt;p&gt;Batching multiple conversations for classification at once can significantly increase throughput and reduce token usage, but it may impact the accuracy of results. In our experiment with a domain classifier, classifying 10 conversations simultaneously led to an average of 15-20% of domain assignments changing between repeated runs of the same prompt. To address this, one mitigation approach is to use a grader LLM prompt: first classify the batch, then have the LLM identify any incorrectly classified conversations, and finally re-classify those as needed. While batching offers efficiency gains, it is important to monitor for potential drops in classification quality.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="combining-classifiers-in-a-single-prompt"&gt;Combining classifiers in a single prompt&lt;/h3&gt;



&lt;p&gt;Combining multiple classifiers into a single prompt increases throughput by allowing one call to the LLM instead of multiple calls. This not only multiplies the overall throughput by the number of classifiers processed but also reduces the total number of tokens used, since the conversation text is only passed in once. However, this approach may compromise classification accuracy, so results should be closely monitored.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="classification-using-text-embeddings"&gt;Classification using text embeddings&lt;/h3&gt;



&lt;p&gt;An alternative approach is to train custom neural network models for each classifier using only the text embeddings of conversations. This method delivers both cost and time savings by avoiding making multiple LLM requests for every classifier and conversation—instead, the system only needs to request conversation text embeddings once and can reuse these embeddings across all classifier models.&lt;/p&gt;



&lt;p&gt;For example, starting with a set of conversations to validate and test the new model, run these conversations through the original prompt-based classifier to generate a set of golden classifications, then obtain text embeddings (using a tool like text-embedding-3-large) for each conversation. These embeddings and their corresponding classifications are used to train a model such as a multi-layer perceptron. In production, the workflow involves retrieving the text embedding for each conversation and passing it through the trained model; if there is a model for each classifier, a single embedding retrieval per conversation suffices for all classifiers.&lt;/p&gt;



&lt;p&gt;The benefits of this approach include significantly increased throughput and cost savings—since it’s not necessary to call the LLM for every classifier and conversation. However, this setup can require GPU compute which can increase costs and infrastructure complexity, and the resulting models may not achieve the same accuracy as prompt-based classification methods.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="prompt-compression"&gt;Prompt compression&lt;/h3&gt;



&lt;p&gt;Compressing prompts by eliminating unnecessary tokens or by using a tool such as LLMLingua&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to automate prompt compression can optimize classification prompts either ahead of time or in real-time. This approach increases overall throughput and results in cost savings due to a reduced number of tokens, but there are risks: changes to the classifier prompt or conversation text may impact classification accuracy, and depending on the compression technique, it could even decrease throughput if the compression process takes longer than simply sending uncompressed text to the LLM.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="text-truncation"&gt;Text truncation&lt;/h3&gt;



&lt;p&gt;Truncating conversations to a specific length limits the overall number of tokens sent through an endpoint, offering cost savings and increased throughput like prompt compression. By reducing the number of tokens per request, throughput rises because more requests can be made before reaching the endpoint’s tokens-per-minute (TPM) limit, and costs decrease due to fewer tokens being processed. However, the ideal truncation length depends on both the classifiers and the conversation content, so it’s important to assess how truncation affects output quality before implementation. While this approach brings clear efficiency benefits, it also poses a risk: long conversations may have their most important content cut off, which can reduce classification accuracy.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="conclusion"&gt;Conclusion&lt;/h2&gt;



&lt;p&gt;Building a scalable, high-throughput pipeline for LLM-based classification is far from trivial. It requires navigating a constantly shifting landscape of model capabilities, prompt behaviors, and infrastructure constraints. As LLMs become faster, cheaper, and more capable, they’re unlocking new possibilities for real-time understanding of human-AI interactions at scale. The techniques we’ve shared represent a snapshot of what’s working today. But more importantly, they offer a foundation for what’s possible tomorrow.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/technical-approach-for-classifying-human-ai-interactions-at-scale/</guid><pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate></item><item><title>Google Photos adds AI features for ‘remixing’ photos in different styles, turning pics into videos (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/google-photos-adds-ai-features-for-remixing-photos-in-different-styles-turning-pics-into-videos/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Photos is getting major AI chops. On Wednesday, Google announced a handful of new features that will allow users to get more creative with their photo memories, including an option to turn photos into videos, and “remix” photos into different styles, like anime, comics, sketches, or 3D animations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app will also centralize access to its creative tools — including both AI-powered and traditional tools — in a new “Create” tab in the Photos app. The two newly launched features will be housed in this tab, alongside other tools that let you create collages, highlight videos, and other things.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030442" height="680" src="https://techcrunch.com/wp-content/uploads/2025/07/Create-tab.png?w=320" width="320" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The update brings AI prowess to one of Google’s most popular consumer-facing services: Google Photos today has over 1.5 billion users. That will put AI into more people’s hands, including those who have not spent as much time experimenting with what AI can do. It also gives Google a large base to learn from as people try out the new features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company noted these features are experimental, so it will ask users to leave a thumbs-up or thumbs-down on the AI-generated images and videos to provide feedback. That feedback will help Google to improve the product and overall experience, it says.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030443" height="372" src="https://techcrunch.com/wp-content/uploads/2025/07/photo-to-video.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new photo-to-video feature, similar to the offering already available in Gemini (and, as of today, YouTube), users will be able to make short videos from their own photos using Google’s Veo 2 model. In past years, animating old family photos was a clever trick, driving downloads of apps like MyHeritage as people brought long-past relatives to life. Now that ability is being commoditized with the use of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once you’ve selected a photo, you can choose from one of two prompts — “Subtle movements,” or “I’m feeling lucky” — to turn the photo into a six-second video clip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Photo to Video is rolling out today to users in the U.S. on Android and iOS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, the new Remix feature, powered by Google’s Imagen AI model, lets you pick any photo from your gallery, then transform it into a different style in just seconds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature will be available in the U.S. on Android and iOS in the next few weeks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030445" height="381" src="https://techcrunch.com/wp-content/uploads/2025/07/remix.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Both features will include an invisible&amp;nbsp;SynthID&amp;nbsp;digital watermark on their outputs to identify them as being AI-generated. Google Photos already does this with other AI tools, like&amp;nbsp;images edited using Reimagine, for example. Generated videos will also include a visual watermark, similar to those generated by Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Create tab will arrive in the U.S. in August. Google says it will update the tab over time, adding new tools and experiments and refining existing options.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The AI features were introduced alongside similar tools for YouTube Shorts, which is also now offering its own photo-to-video option as well as new AI effects, powered by Veo 2. (Shorts will get access to Veo 3 later this summer, Google said.)&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google Photos is getting major AI chops. On Wednesday, Google announced a handful of new features that will allow users to get more creative with their photo memories, including an option to turn photos into videos, and “remix” photos into different styles, like anime, comics, sketches, or 3D animations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app will also centralize access to its creative tools — including both AI-powered and traditional tools — in a new “Create” tab in the Photos app. The two newly launched features will be housed in this tab, alongside other tools that let you create collages, highlight videos, and other things.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030442" height="680" src="https://techcrunch.com/wp-content/uploads/2025/07/Create-tab.png?w=320" width="320" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The update brings AI prowess to one of Google’s most popular consumer-facing services: Google Photos today has over 1.5 billion users. That will put AI into more people’s hands, including those who have not spent as much time experimenting with what AI can do. It also gives Google a large base to learn from as people try out the new features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company noted these features are experimental, so it will ask users to leave a thumbs-up or thumbs-down on the AI-generated images and videos to provide feedback. That feedback will help Google to improve the product and overall experience, it says.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030443" height="372" src="https://techcrunch.com/wp-content/uploads/2025/07/photo-to-video.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new photo-to-video feature, similar to the offering already available in Gemini (and, as of today, YouTube), users will be able to make short videos from their own photos using Google’s Veo 2 model. In past years, animating old family photos was a clever trick, driving downloads of apps like MyHeritage as people brought long-past relatives to life. Now that ability is being commoditized with the use of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once you’ve selected a photo, you can choose from one of two prompts — “Subtle movements,” or “I’m feeling lucky” — to turn the photo into a six-second video clip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Photo to Video is rolling out today to users in the U.S. on Android and iOS.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, the new Remix feature, powered by Google’s Imagen AI model, lets you pick any photo from your gallery, then transform it into a different style in just seconds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature will be available in the U.S. on Android and iOS in the next few weeks.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030445" height="381" src="https://techcrunch.com/wp-content/uploads/2025/07/remix.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Both features will include an invisible&amp;nbsp;SynthID&amp;nbsp;digital watermark on their outputs to identify them as being AI-generated. Google Photos already does this with other AI tools, like&amp;nbsp;images edited using Reimagine, for example. Generated videos will also include a visual watermark, similar to those generated by Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Create tab will arrive in the U.S. in August. Google says it will update the tab over time, adding new tools and experiments and refining existing options.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The AI features were introduced alongside similar tools for YouTube Shorts, which is also now offering its own photo-to-video option as well as new AI effects, powered by Veo 2. (Shorts will get access to Veo 3 later this summer, Google said.)&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/google-photos-adds-ai-features-for-remixing-photos-in-different-styles-turning-pics-into-videos/</guid><pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate></item><item><title>YouTube Shorts is adding an image-to-video AI tool, new AI effects (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/youtube-shorts-is-adding-an-image-to-video-ai-tool-new-ai-effects/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;YouTube announced on Wednesday that it’s giving Shorts creators access to new generative AI features, including an image-to-video AI tool and new AI effects. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image to video feature lets users turn a picture from their camera roll into a six-second video. Users will see a selection of suggestions that are relevant to the photo they uploaded. YouTube says the feature can be used to add movement to landscape photos, animate pictures of everyday photos, or bring group photos to life.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In an example given by YouTube, the feature turns a static image of a pedestrian signal into a short video that slowly zooms into a dancing version of the walking man symbol. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030444" height="569" src="https://techcrunch.com/wp-content/uploads/2025/07/YouTube-Photo-to-video.gif?w=320" width="320" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works similarly to an&amp;nbsp;offering already available in Gemini. Plus, it’s similar to the Animate tool in Meta’s Edits app, which also uses AI to transform static images into videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new tool is rolling out over the next week in the United States, Canada, Australia, and New Zealand. YouTube plans to bring it to more regions later this year. It’s worth noting that Google Photos is also getting a similar image-to-video tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the new AI effects, creators can use them to transform their doodles into artistic images and turn their selfies into videos where they’re swimming underwater, twinning with someone, and more. Users can find these new effects by navigating to the “Effects” icon in the Shorts camera and then tapping “AI” to browse all of the generative effects. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030449" height="569" src="https://techcrunch.com/wp-content/uploads/2025/07/YouTube-Effect-Underwater.gif?w=320" width="320" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube notes that the features announced today are powered by Veo 2, Google’s AI model for video generation. YouTube says it uses&amp;nbsp;SynthID&amp;nbsp;watermarks and&amp;nbsp;clear labels&amp;nbsp;to indicate that these creations were generated with AI.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Last month at Cannes Lions 2025, YouTube CEO Neal Mohan announced that Google’s Veo 3 video generator, which can generate both video and audio, will be coming to Shorts later this summer. He also shared that Shorts are now averaging more than&amp;nbsp;200 billion daily views. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube also announced on Wednesday that AI Playground is its new home for generative AI creation tools, inspirational examples, prefilled prompts, and more. Creators can find AI Playground by tapping the Create button and then the sparkle icon in the top right corner. It’s available now for everyone in the United States, Canada, Australia, and New Zealand.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;YouTube announced on Wednesday that it’s giving Shorts creators access to new generative AI features, including an image-to-video AI tool and new AI effects. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image to video feature lets users turn a picture from their camera roll into a six-second video. Users will see a selection of suggestions that are relevant to the photo they uploaded. YouTube says the feature can be used to add movement to landscape photos, animate pictures of everyday photos, or bring group photos to life.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In an example given by YouTube, the feature turns a static image of a pedestrian signal into a short video that slowly zooms into a dancing version of the walking man symbol. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030444" height="569" src="https://techcrunch.com/wp-content/uploads/2025/07/YouTube-Photo-to-video.gif?w=320" width="320" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works similarly to an&amp;nbsp;offering already available in Gemini. Plus, it’s similar to the Animate tool in Meta’s Edits app, which also uses AI to transform static images into videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new tool is rolling out over the next week in the United States, Canada, Australia, and New Zealand. YouTube plans to bring it to more regions later this year. It’s worth noting that Google Photos is also getting a similar image-to-video tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the new AI effects, creators can use them to transform their doodles into artistic images and turn their selfies into videos where they’re swimming underwater, twinning with someone, and more. Users can find these new effects by navigating to the “Effects” icon in the Shorts camera and then tapping “AI” to browse all of the generative effects. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3030449" height="569" src="https://techcrunch.com/wp-content/uploads/2025/07/YouTube-Effect-Underwater.gif?w=320" width="320" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;YouTube&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube notes that the features announced today are powered by Veo 2, Google’s AI model for video generation. YouTube says it uses&amp;nbsp;SynthID&amp;nbsp;watermarks and&amp;nbsp;clear labels&amp;nbsp;to indicate that these creations were generated with AI.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Last month at Cannes Lions 2025, YouTube CEO Neal Mohan announced that Google’s Veo 3 video generator, which can generate both video and audio, will be coming to Shorts later this summer. He also shared that Shorts are now averaging more than&amp;nbsp;200 billion daily views. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube also announced on Wednesday that AI Playground is its new home for generative AI creation tools, inspirational examples, prefilled prompts, and more. Creators can find AI Playground by tapping the Create button and then the sparkle icon in the top right corner. It’s available now for everyone in the United States, Canada, Australia, and New Zealand.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/youtube-shorts-is-adding-an-image-to-video-ai-tool-new-ai-effects/</guid><pubDate>Wed, 23 Jul 2025 16:00:00 +0000</pubDate></item><item><title>AI Action Plan: US leadership must be ‘unchallenged’ (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-action-plan-us-leadership-must-be-unchallenged/</link><description>&lt;p&gt;The White House has released its ‘AI Action Plan’, which frames the coming decade as a technological race the US cannot afford to lose.&lt;/p&gt;&lt;p&gt;Laced with the urgent rhetoric of a new cold war, the action plan argues that securing victory in AI is nothing short of a national imperative. Trump’s foreword sets the tone, calling for America to “achieve and maintain unquestioned and unchallenged global technological dominance” as a core tenet of national security.&lt;/p&gt;&lt;p&gt;To get there, the administration is making a three-pronged push: ignite a firestorm of domestic innovation, build the colossal infrastructure to sustain it, and project American power across the globe to secure the win.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;🇺🇸&lt;/p&gt;&lt;p&gt;Today is a day we have been working towards for six months. We are announcing America’s AI action plan putting us on the road to continued AI dominance. &lt;/p&gt;&lt;p&gt;The three core themes:&lt;br /&gt;– Accelerate AI innovation&lt;br /&gt;– Build American AI infrastructure &lt;br /&gt;– Lead in international AI… pic.twitter.com/KBHmxCmxC6&lt;/p&gt;— Sriram Krishnan (@sriramk) July 23, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-pillar-i-an-action-plan-to-support-the-private-ai-sector"&gt;Pillar I: An action plan to support the private AI sector&lt;/h3&gt;&lt;p&gt;At its heart, the strategy is a full-throated endorsement of the private sector. The first move is to take a buzzsaw to the regulatory frameworks of the past, with the document explicitly targeting the “onerous” approach of the previous administration.&lt;/p&gt;&lt;p&gt;The philosophy is simple: get out of the way and let innovators innovate. According to US Vice President JD Vance, smothering the technology with rules now would be to “paralyse one of the most promising technologies we have seen in generations.”&lt;/p&gt;&lt;p&gt;The plan even uses the power of federal funding as a stick, threatening to withhold money from states that dare to enact their own “burdensome AI regulations.”&lt;/p&gt;&lt;p&gt;It also strides confidently into the culture wars, insisting that AI systems paid for by the taxpayer must reflect “American values.” This means a preference for models that are “objective and free from top-down ideological bias” and a directive to scrub concepts like misinformation and Diversity, Equity, and Inclusion from the government’s official AI risk guides.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-pillar-ii-a-foundation-of-concrete-and-code"&gt;Pillar II: A foundation of concrete and code&lt;/h3&gt;&lt;p&gt;The second pillar of the action plan relates to the raw, physical demands of the AI revolution.&lt;/p&gt;&lt;p&gt;“AI is the first digital service in modern life that challenges America to build vastly greater energy generation than we have today,” the plan bluntly states. Its answer is a national mobilisation under the banner of “Build, Baby, Build!”—a vast undertaking to erect data centres, bring semiconductor manufacturing home, and construct the power grid of the future.&lt;/p&gt;&lt;p&gt;This means fast-tracking environmental permits and overhauling the nation’s energy supply, mixing today’s power sources with tomorrow’s bets on nuclear fusion. Bringing chipmaking back to US shores is central to this vision, with a promise to refocus the CHIPS Program Office on delivering results without ideological strings attached.&lt;/p&gt;&lt;p&gt;And, behind it all, a push to train a new generation of technicians and engineers to build and maintain this new industrial backbone.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-pillar-iii-ensuring-an-undisputed-lead-on-the-world-stage"&gt;Pillar III: Ensuring an undisputed lead on the world stage&lt;/h3&gt;&lt;p&gt;The final pillar is about shaping the world in America’s image. The ambition is to make the entire US tech stack – from silicon to the software – the undisputed “gold standard for AI worldwide.” This involves an aggressive export strategy to arm allies with American technology, explicitly to counter the influence of a rising China.&lt;/p&gt;&lt;p&gt;This new foreign policy will involve pushing back against Chinese influence in global forums like the United Nations, which the administration believes are being used to promote innovation-killing regulations. It also signals a more hawkish approach to security, demanding tighter controls on the advanced chips that fuel AI progress.&lt;/p&gt;&lt;p&gt;The plan confronts the dark side of AI head-on, acknowledging its potential for misuse in everything from cybercrime to bioweapons, and calls for a national effort to get ahead of the threat.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-action-plan-lands-in-a-divided-industry"&gt;AI Action Plan lands in a divided industry&lt;/h3&gt;&lt;p&gt;The administration’s confident blueprint for the future lands in an industry deeply conflicted about its own creation. Just this week, OpenAI CEO Sam Altman warned about the technology’s disruptive power.&lt;/p&gt;&lt;p&gt;Altman warns that AI will not only eliminate jobs but also pose national security threats. He has spoken of a looming “fraud crisis” powered by AI’s ability to fool security systems, and has gone so far as to co-sign a letter stating that “mitigating the risk of extinction from AI should be a global priority”.&lt;/p&gt;&lt;p&gt;His commentary is a stark reminder that the race for AI dominance is also a race to control a technology with world-altering potential. While Washington focuses on winning, the architects of AI are quietly wrestling with what victory might actually mean.&lt;/p&gt;&lt;p&gt;However, the plan received a cautious welcome from the nonprofit Americans for Responsible Innovation (ARI). The group saw its own fingerprints on several proposals, from stronger export controls to more research into AI safety.&lt;/p&gt;&lt;p&gt;Yet ARI is deeply troubled by the administration’s move to punish states that pursue their own AI safety rules. This position also seems at odds with the views of industry leaders like Altman, who has himself warned against the chaos of 50 different state-level regulations.&lt;/p&gt;&lt;p&gt;“Ultimately, this action plan is about increasing oversight of AI systems while maintaining a hands-off approach to hard and fast regulations,” said ARI President Brad Carson. He sees a chance to better understand the “big risks frontier models create for the public,” but worries about the administration’s tactics.&lt;/p&gt;&lt;p&gt;“The plan’s targeting of state-passed AI safeguards is cause for concern. For America to lead on AI, we have to build public trust in these systems, and safeguards are essential to that public confidence.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Luke Michael)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Sam Altman: AI will cause job losses and national security threats&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The White House has released its ‘AI Action Plan’, which frames the coming decade as a technological race the US cannot afford to lose.&lt;/p&gt;&lt;p&gt;Laced with the urgent rhetoric of a new cold war, the action plan argues that securing victory in AI is nothing short of a national imperative. Trump’s foreword sets the tone, calling for America to “achieve and maintain unquestioned and unchallenged global technological dominance” as a core tenet of national security.&lt;/p&gt;&lt;p&gt;To get there, the administration is making a three-pronged push: ignite a firestorm of domestic innovation, build the colossal infrastructure to sustain it, and project American power across the globe to secure the win.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;&lt;blockquote class="cmplz-placeholder-element twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;🇺🇸&lt;/p&gt;&lt;p&gt;Today is a day we have been working towards for six months. We are announcing America’s AI action plan putting us on the road to continued AI dominance. &lt;/p&gt;&lt;p&gt;The three core themes:&lt;br /&gt;– Accelerate AI innovation&lt;br /&gt;– Build American AI infrastructure &lt;br /&gt;– Lead in international AI… pic.twitter.com/KBHmxCmxC6&lt;/p&gt;— Sriram Krishnan (@sriramk) July 23, 2025&lt;/blockquote&gt;&lt;/div&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-pillar-i-an-action-plan-to-support-the-private-ai-sector"&gt;Pillar I: An action plan to support the private AI sector&lt;/h3&gt;&lt;p&gt;At its heart, the strategy is a full-throated endorsement of the private sector. The first move is to take a buzzsaw to the regulatory frameworks of the past, with the document explicitly targeting the “onerous” approach of the previous administration.&lt;/p&gt;&lt;p&gt;The philosophy is simple: get out of the way and let innovators innovate. According to US Vice President JD Vance, smothering the technology with rules now would be to “paralyse one of the most promising technologies we have seen in generations.”&lt;/p&gt;&lt;p&gt;The plan even uses the power of federal funding as a stick, threatening to withhold money from states that dare to enact their own “burdensome AI regulations.”&lt;/p&gt;&lt;p&gt;It also strides confidently into the culture wars, insisting that AI systems paid for by the taxpayer must reflect “American values.” This means a preference for models that are “objective and free from top-down ideological bias” and a directive to scrub concepts like misinformation and Diversity, Equity, and Inclusion from the government’s official AI risk guides.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-pillar-ii-a-foundation-of-concrete-and-code"&gt;Pillar II: A foundation of concrete and code&lt;/h3&gt;&lt;p&gt;The second pillar of the action plan relates to the raw, physical demands of the AI revolution.&lt;/p&gt;&lt;p&gt;“AI is the first digital service in modern life that challenges America to build vastly greater energy generation than we have today,” the plan bluntly states. Its answer is a national mobilisation under the banner of “Build, Baby, Build!”—a vast undertaking to erect data centres, bring semiconductor manufacturing home, and construct the power grid of the future.&lt;/p&gt;&lt;p&gt;This means fast-tracking environmental permits and overhauling the nation’s energy supply, mixing today’s power sources with tomorrow’s bets on nuclear fusion. Bringing chipmaking back to US shores is central to this vision, with a promise to refocus the CHIPS Program Office on delivering results without ideological strings attached.&lt;/p&gt;&lt;p&gt;And, behind it all, a push to train a new generation of technicians and engineers to build and maintain this new industrial backbone.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-pillar-iii-ensuring-an-undisputed-lead-on-the-world-stage"&gt;Pillar III: Ensuring an undisputed lead on the world stage&lt;/h3&gt;&lt;p&gt;The final pillar is about shaping the world in America’s image. The ambition is to make the entire US tech stack – from silicon to the software – the undisputed “gold standard for AI worldwide.” This involves an aggressive export strategy to arm allies with American technology, explicitly to counter the influence of a rising China.&lt;/p&gt;&lt;p&gt;This new foreign policy will involve pushing back against Chinese influence in global forums like the United Nations, which the administration believes are being used to promote innovation-killing regulations. It also signals a more hawkish approach to security, demanding tighter controls on the advanced chips that fuel AI progress.&lt;/p&gt;&lt;p&gt;The plan confronts the dark side of AI head-on, acknowledging its potential for misuse in everything from cybercrime to bioweapons, and calls for a national effort to get ahead of the threat.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-action-plan-lands-in-a-divided-industry"&gt;AI Action Plan lands in a divided industry&lt;/h3&gt;&lt;p&gt;The administration’s confident blueprint for the future lands in an industry deeply conflicted about its own creation. Just this week, OpenAI CEO Sam Altman warned about the technology’s disruptive power.&lt;/p&gt;&lt;p&gt;Altman warns that AI will not only eliminate jobs but also pose national security threats. He has spoken of a looming “fraud crisis” powered by AI’s ability to fool security systems, and has gone so far as to co-sign a letter stating that “mitigating the risk of extinction from AI should be a global priority”.&lt;/p&gt;&lt;p&gt;His commentary is a stark reminder that the race for AI dominance is also a race to control a technology with world-altering potential. While Washington focuses on winning, the architects of AI are quietly wrestling with what victory might actually mean.&lt;/p&gt;&lt;p&gt;However, the plan received a cautious welcome from the nonprofit Americans for Responsible Innovation (ARI). The group saw its own fingerprints on several proposals, from stronger export controls to more research into AI safety.&lt;/p&gt;&lt;p&gt;Yet ARI is deeply troubled by the administration’s move to punish states that pursue their own AI safety rules. This position also seems at odds with the views of industry leaders like Altman, who has himself warned against the chaos of 50 different state-level regulations.&lt;/p&gt;&lt;p&gt;“Ultimately, this action plan is about increasing oversight of AI systems while maintaining a hands-off approach to hard and fast regulations,” said ARI President Brad Carson. He sees a chance to better understand the “big risks frontier models create for the public,” but worries about the administration’s tactics.&lt;/p&gt;&lt;p&gt;“The plan’s targeting of state-passed AI safeguards is cause for concern. For America to lead on AI, we have to build public trust in these systems, and safeguards are essential to that public confidence.”&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Luke Michael)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Sam Altman: AI will cause job losses and national security threats&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-action-plan-us-leadership-must-be-unchallenged/</guid><pubDate>Wed, 23 Jul 2025 16:20:24 +0000</pubDate></item><item><title>Trump’s AI strategy trades guardrails for growth in race against China (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/trumps-ai-strategy-trades-guardrails-for-growth-in-race-against-china/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225249178.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration published its much-anticipated AI Action Plan on Wednesday, a document that takes a sharp shift away from former President Biden’s cautious approach to addressing the risks of AI,&amp;nbsp;and instead barrels ahead with plans to build out AI infrastructure, cut red tape for tech companies, shore up national security, and compete with China.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The downstream effects of this shift will likely ripple throughout various industries and may even be felt by the average American consumer. For instance, the AI Action Plan downplays efforts to mitigate possible harms of AI and instead prioritizes building out data centers to power the AI industry, even if it means using federal lands or keeping them powered during critical energy grid periods.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Much of its effects, however, will depend on how the AI Action Plan is executed, and many of those details have yet to be sorted. The AI Action Plan is more blueprint for action than a step-by-step instruction book. But the direction is clear: Progress is king.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration positions this as the only way to “usher in a new golden age of human flourishing.” Its goal is to convince the American public that spending billions of taxpayer dollars on building data centers is in their best interest. Parts of the plan also include policy suggestions around upskilling workers and partnering with local governments to create jobs related to working in data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To secure our future, we must harness the full power of American innovation,” Trump said in a statement. “To do that, we will continue to reject radical climate dogma and bureaucratic red tape, as the Administration has done since Inauguration Day. Simply put, we need to ‘Build, Baby, Build!’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Action Plan is authored by the Trump administration’s team of technology and AI specialists, many of whom come from Silicon Valley companies. This includes Office of Science and Technology Policy director Michael Kratsios; AI and crypto czar David Sacks; and assistant to the president for national security affairs Marco Rubio. More than 10,000 interest groups submitted public comments that were considered for the plan.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-deregulation-and-bringing-back-the-ai-moratorium"&gt;Deregulation and bringing back the AI moratorium&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At the start of this month, the Senate removed a controversial provision in the budget bill that would bar states from regulating AI for 10 years. That provision, if it had been included in the bill, would tie states’ federal broadband funding to compliance with the moratorium.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It appears that matter hasn’t been put to rest yet, as the AI Action Plan explores a new way to hinder states from regulating AI. As part of a broader mission to “unleash prosperity through deregulation,” the administration threatens to limit states’ federal funding based on their AI regulations. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The plan also directs the Federal Communications Commission to “evaluate whether state AI regulations interfere with the agency’s ability to carry out its obligations and authorities.” In other words, if state AI regulations touch on radio, TV, and internet — which many do — then the FCC can get involved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the federal level, the action plan directs the Office of Science and Technology Policy to ask businesses and the public about any current federal regulations that hinder AI innovation and adoption so that federal agencies can take appropriate action.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-cutting-red-tape-around-data-centers"&gt;Cutting red tape around data centers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s call for deregulation extends to how the administration hopes to accelerate the buildout of AI-related infrastructure, like data centers, semiconductor fabs, and power sources. The administration argues that existing environmental regulations — like NEPA, the Clean Air Act, and the Clean Water Act — are hindering America’s need to meet the rapid demands of the AI arms race.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s why Trump’s AI Action Plan places an emphasis on stabilizing America’s energy grid. At the same time, the plan asks the federal government to find new ways to ensure large power consumers — such as AI companies — can manage their power consumption during critical grid periods.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Certain companies, like xAI and Meta, have been criticized for concentrating pollution in vulnerable communities. Critics have accused xAI of bypassing environmental safeguards and exposing residents to harmful emissions from gas-powered turbines with its Memphis data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The action plan calls for creating categorical exclusions, streamlining permitting processes, and expanding the use of fast-track programs like FAST-41 to make it easier for companies to build critical AI infrastructure, especially on federal lands, which includes national parks, federally protected wilderness areas, and military bases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tying back to other Trump themes of beating China, the strategy focuses on locking out foreign tech and emphasizing security protections to keep “adversarial technology” — like Chinese-made chips and hardware — out of the U.S. supply chain.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-trump-s-war-on-biased-ai"&gt;Trump’s war on “biased AI”&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;One of the main standouts in Trump’s AI Action plan is a focus on protecting free speech and “American values,” in part by eliminating references to misinformation, DEI, and climate change from federal risk-assessment frameworks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It is essential that these systems be built from the ground up with freedom of speech and expression in mind, and that U.S. government policy does not interfere with that objective,” the plan reads. “We must ensure that free speech flourishes in the era of AI and that AI procured by the Federal government objectively reflects truth rather than social engineering agendas.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the intention to ensure that government policy doesn’t interfere with freedom of speech, the AI Action Plan has the potential to do just that.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the recommended policy actions is to update federal procurement guidelines to ensure the government only contracts with frontier large language model developers who “ensure their systems are objective and free from top-down ideological bias.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That language is similar to what The Wall Street Journal reported would be in Trump’s executive order, which is expected to be released later today.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem is that achieving objectivity is hard, and the government has not yet defined how it plans to evaluate models on the basis of neutrality.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The only way to be neutral would be literal non-engagement,” Rumman Chowdhury, a data scientist, CEO of the tech nonprofit Humane Intelligence, and former U.S. science envoy for AI, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic, xAI, Google, and OpenAI have all secured government contracts worth up to $200 million each to help integrate AI applications into the Department of Defense. The implications of Trump’s policy suggestion and his impending executive order, could be far-reaching.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For instance, an order that says, ‘We won’t do any business, as to AI models or otherwise, with any company that produces a non-neutral AI model’ would likely violate the First Amendment,” Eugene Volokh, an American legal scholar who specializes in First Amendment and Second Amendment issues, said in an email. “An order that says, ‘We will only enter into contracts to buy models that are sufficiently neutral’ would be more constitutionally defensible, though of course implementing it effectively may be very difficult (partly because it’s so hard to know what’s ‘neutral’ in these situations).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added: “If the order instructs agencies to select AIs based on a combination of accuracy and neutrality, leaving each agency with some latitude to decide what that means, that might be more viable.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-encouraging-an-open-approach-to-ai"&gt;Encouraging an open approach to AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s AI Action Plan aims to encourage the development and adoption of open AI models, which are free to download online and are created with American values in mind. This largely seems to be a reaction to the rise of open AI models from Chinese AI labs, including DeepSeek and Alibaba’s Qwen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of his plan, Trump wants to ensure that startups and researchers working on open models have access to large computing clusters. These resources are expensive and typically were only possible for tech companies that could strike million- or billion-dollar contracts with cloud providers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Trump also says he wants to partner with leading AI model developers to increase the research community’s access to private AI models and data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American AI companies and organizations that have taken an open approach — including Meta, AI2, and Hugging Face — could benefit from Trump’s embrace of open AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-safety-and-security"&gt;AI safety and security&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s AI Action Plan includes some provisions to satisfy the AI safety community. One of those efforts includes launching a federal technological development program to research AI interpretability, AI control systems, and adversarial robustness. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s plan also instructs federal agencies, including the Department of Defense and Department of Energy, to host hackathons to test its AI systems for security vulnerabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s plan also acknowledges the risks of AI systems to contribute to cyberattacks, as well as the development of chemical and biological weapons. The plan asks frontier AI model developers to work with federal agencies to evaluate these risks and asks how they could jeopardize America’s national security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Compared to Biden’s AI executive order, Trump’s plan puts less of a focus on requiring leading AI model developers to report safety and security standards. Many tech companies claim safety and security reporting is an “onerous” task, which Trump seems to want to limit.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-limiting-china"&gt;Limiting China&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps unsurprisingly, Trump is bringing his war on China into the AI race with his action plan. A large part of Trump’s AI Action Plan focuses on preventing “national security” threats from accessing advanced AI technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under Trump’s plan, federal agencies will work together to collect intelligence on foreign frontier AI projects that could threaten American national security. In one of those efforts, the Department of Commerce is tasked with evaluating Chinese AI models for alignment with Chinese Communist Party talking points and censorship.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These groups will also conduct assessments on the level of AI adoption among America’s adversaries.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-national-security"&gt;National security&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;“National security” was mentioned 23 times in the AI Action Plan — more than “data centers,” “jobs,” “science,” and other key terms. The plan’s national security strategy is centered on integrating AI into the U.S. defense and intelligence apparatus, and even building out AI data centers for the DoD, while guarding against foreign threats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among other things, the plan calls for the DoD and intelligence community to regularly assess how AI adoption in the U.S. compares to rivals like China and adapt accordingly, and to evaluate risks posed by both domestic and adversary AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within the DoD itself, the strategy emphasizes upskilling the military workforce, automating workflows, and securing preferential access to compute resources during national emergencies.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225249178.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration published its much-anticipated AI Action Plan on Wednesday, a document that takes a sharp shift away from former President Biden’s cautious approach to addressing the risks of AI,&amp;nbsp;and instead barrels ahead with plans to build out AI infrastructure, cut red tape for tech companies, shore up national security, and compete with China.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The downstream effects of this shift will likely ripple throughout various industries and may even be felt by the average American consumer. For instance, the AI Action Plan downplays efforts to mitigate possible harms of AI and instead prioritizes building out data centers to power the AI industry, even if it means using federal lands or keeping them powered during critical energy grid periods.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Much of its effects, however, will depend on how the AI Action Plan is executed, and many of those details have yet to be sorted. The AI Action Plan is more blueprint for action than a step-by-step instruction book. But the direction is clear: Progress is king.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration positions this as the only way to “usher in a new golden age of human flourishing.” Its goal is to convince the American public that spending billions of taxpayer dollars on building data centers is in their best interest. Parts of the plan also include policy suggestions around upskilling workers and partnering with local governments to create jobs related to working in data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To secure our future, we must harness the full power of American innovation,” Trump said in a statement. “To do that, we will continue to reject radical climate dogma and bureaucratic red tape, as the Administration has done since Inauguration Day. Simply put, we need to ‘Build, Baby, Build!’”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Action Plan is authored by the Trump administration’s team of technology and AI specialists, many of whom come from Silicon Valley companies. This includes Office of Science and Technology Policy director Michael Kratsios; AI and crypto czar David Sacks; and assistant to the president for national security affairs Marco Rubio. More than 10,000 interest groups submitted public comments that were considered for the plan.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-deregulation-and-bringing-back-the-ai-moratorium"&gt;Deregulation and bringing back the AI moratorium&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At the start of this month, the Senate removed a controversial provision in the budget bill that would bar states from regulating AI for 10 years. That provision, if it had been included in the bill, would tie states’ federal broadband funding to compliance with the moratorium.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It appears that matter hasn’t been put to rest yet, as the AI Action Plan explores a new way to hinder states from regulating AI. As part of a broader mission to “unleash prosperity through deregulation,” the administration threatens to limit states’ federal funding based on their AI regulations. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The plan also directs the Federal Communications Commission to “evaluate whether state AI regulations interfere with the agency’s ability to carry out its obligations and authorities.” In other words, if state AI regulations touch on radio, TV, and internet — which many do — then the FCC can get involved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the federal level, the action plan directs the Office of Science and Technology Policy to ask businesses and the public about any current federal regulations that hinder AI innovation and adoption so that federal agencies can take appropriate action.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-cutting-red-tape-around-data-centers"&gt;Cutting red tape around data centers&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s call for deregulation extends to how the administration hopes to accelerate the buildout of AI-related infrastructure, like data centers, semiconductor fabs, and power sources. The administration argues that existing environmental regulations — like NEPA, the Clean Air Act, and the Clean Water Act — are hindering America’s need to meet the rapid demands of the AI arms race.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That’s why Trump’s AI Action Plan places an emphasis on stabilizing America’s energy grid. At the same time, the plan asks the federal government to find new ways to ensure large power consumers — such as AI companies — can manage their power consumption during critical grid periods.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Certain companies, like xAI and Meta, have been criticized for concentrating pollution in vulnerable communities. Critics have accused xAI of bypassing environmental safeguards and exposing residents to harmful emissions from gas-powered turbines with its Memphis data center.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The action plan calls for creating categorical exclusions, streamlining permitting processes, and expanding the use of fast-track programs like FAST-41 to make it easier for companies to build critical AI infrastructure, especially on federal lands, which includes national parks, federally protected wilderness areas, and military bases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tying back to other Trump themes of beating China, the strategy focuses on locking out foreign tech and emphasizing security protections to keep “adversarial technology” — like Chinese-made chips and hardware — out of the U.S. supply chain.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-trump-s-war-on-biased-ai"&gt;Trump’s war on “biased AI”&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;One of the main standouts in Trump’s AI Action plan is a focus on protecting free speech and “American values,” in part by eliminating references to misinformation, DEI, and climate change from federal risk-assessment frameworks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It is essential that these systems be built from the ground up with freedom of speech and expression in mind, and that U.S. government policy does not interfere with that objective,” the plan reads. “We must ensure that free speech flourishes in the era of AI and that AI procured by the Federal government objectively reflects truth rather than social engineering agendas.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the intention to ensure that government policy doesn’t interfere with freedom of speech, the AI Action Plan has the potential to do just that.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the recommended policy actions is to update federal procurement guidelines to ensure the government only contracts with frontier large language model developers who “ensure their systems are objective and free from top-down ideological bias.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That language is similar to what The Wall Street Journal reported would be in Trump’s executive order, which is expected to be released later today.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem is that achieving objectivity is hard, and the government has not yet defined how it plans to evaluate models on the basis of neutrality.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The only way to be neutral would be literal non-engagement,” Rumman Chowdhury, a data scientist, CEO of the tech nonprofit Humane Intelligence, and former U.S. science envoy for AI, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic, xAI, Google, and OpenAI have all secured government contracts worth up to $200 million each to help integrate AI applications into the Department of Defense. The implications of Trump’s policy suggestion and his impending executive order, could be far-reaching.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“For instance, an order that says, ‘We won’t do any business, as to AI models or otherwise, with any company that produces a non-neutral AI model’ would likely violate the First Amendment,” Eugene Volokh, an American legal scholar who specializes in First Amendment and Second Amendment issues, said in an email. “An order that says, ‘We will only enter into contracts to buy models that are sufficiently neutral’ would be more constitutionally defensible, though of course implementing it effectively may be very difficult (partly because it’s so hard to know what’s ‘neutral’ in these situations).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added: “If the order instructs agencies to select AIs based on a combination of accuracy and neutrality, leaving each agency with some latitude to decide what that means, that might be more viable.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-encouraging-an-open-approach-to-ai"&gt;Encouraging an open approach to AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s AI Action Plan aims to encourage the development and adoption of open AI models, which are free to download online and are created with American values in mind. This largely seems to be a reaction to the rise of open AI models from Chinese AI labs, including DeepSeek and Alibaba’s Qwen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of his plan, Trump wants to ensure that startups and researchers working on open models have access to large computing clusters. These resources are expensive and typically were only possible for tech companies that could strike million- or billion-dollar contracts with cloud providers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Trump also says he wants to partner with leading AI model developers to increase the research community’s access to private AI models and data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American AI companies and organizations that have taken an open approach — including Meta, AI2, and Hugging Face — could benefit from Trump’s embrace of open AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-ai-safety-and-security"&gt;AI safety and security&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s AI Action Plan includes some provisions to satisfy the AI safety community. One of those efforts includes launching a federal technological development program to research AI interpretability, AI control systems, and adversarial robustness. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s plan also instructs federal agencies, including the Department of Defense and Department of Energy, to host hackathons to test its AI systems for security vulnerabilities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s plan also acknowledges the risks of AI systems to contribute to cyberattacks, as well as the development of chemical and biological weapons. The plan asks frontier AI model developers to work with federal agencies to evaluate these risks and asks how they could jeopardize America’s national security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Compared to Biden’s AI executive order, Trump’s plan puts less of a focus on requiring leading AI model developers to report safety and security standards. Many tech companies claim safety and security reporting is an “onerous” task, which Trump seems to want to limit.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-limiting-china"&gt;Limiting China&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps unsurprisingly, Trump is bringing his war on China into the AI race with his action plan. A large part of Trump’s AI Action Plan focuses on preventing “national security” threats from accessing advanced AI technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under Trump’s plan, federal agencies will work together to collect intelligence on foreign frontier AI projects that could threaten American national security. In one of those efforts, the Department of Commerce is tasked with evaluating Chinese AI models for alignment with Chinese Communist Party talking points and censorship.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These groups will also conduct assessments on the level of AI adoption among America’s adversaries.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-national-security"&gt;National security&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;“National security” was mentioned 23 times in the AI Action Plan — more than “data centers,” “jobs,” “science,” and other key terms. The plan’s national security strategy is centered on integrating AI into the U.S. defense and intelligence apparatus, and even building out AI data centers for the DoD, while guarding against foreign threats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among other things, the plan calls for the DoD and intelligence community to regularly assess how AI adoption in the U.S. compares to rivals like China and adapt accordingly, and to evaluate risks posed by both domestic and adversary AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within the DoD itself, the strategy emphasizes upskilling the military workforce, automating workflows, and securing preferential access to compute resources during national emergencies.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/trumps-ai-strategy-trades-guardrails-for-growth-in-race-against-china/</guid><pubDate>Wed, 23 Jul 2025 16:57:21 +0000</pubDate></item><item><title>[NEW] Trump’s AI Action Plan aims to block chip exports to China but lacks key details (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/trumps-ai-action-plan-aims-to-block-chip-exports-to-china-but-lacks-key-details/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-1246479507.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration wants its AI technology to be considered an industry leader both on home soil and abroad. But it also doesn’t want the U.S.’s AI prowess to empower or embolden a foreign adversary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s quite the balance to strike.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If President Trump’s AI Action Plan, which was released on Wednesday, is any indicator, it seems the administration is still figuring out the right course of action to achieve those goals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“​​America currently is the global leader on data center construction, computing hardware performance, and models,” the plan stated. “It is imperative that the United States leverage this advantage into an enduring global alliance, while preventing our adversaries from free-riding on our innovation and investment.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The plan mentions strengthening AI chip export controls through “creative approaches” followed by a pair of policy recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first calls on government organizations, including the Department of Commerce and National Security Council, to work with the AI industry on chip location verification features. The second is a recommendation to establish an effort to figure out enforcement for potential chip export restrictions; notably, it mentions that while the U.S. and allies impose export controls on major systems required for chip manufacturing, there isn’t a focus on many of the component subsystems — a hint at where the administration wants the DOC to direct its attention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Action plan also talks about how the U.S. will need to find alignment in this area with its global allies.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“America must impose strong export controls on sensitive technologies,” the plan states. “We should encourage partners and allies to follow U.S. controls, and not backfill. If they do, America should use tools such as the Foreign Direct Product Rule and secondary tariffs to achieve greater international alignment.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Action plan never gets into detail on exactly how it will achieve Al global alliances, coordinate with allies on export chip restrictions, or work with U.S.-based AI companies on chip location verification features. Instead, the AI Action plans lay out what foundational building blocks are required for future sustainable AI chip export guidelines, as opposed to policies implemented on top of existing guidelines.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: Chip export restrictions are going to take more time. And there’s ample evidence, beyond the AI Action plan, to suggest it will. For instance, the Trump administration has contradicted itself multiple times on its export restriction strategy in the past few months —&amp;nbsp;including just last week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In July, the administration gave semiconductor firms, like Nvidia and AMD, the green light to start selling AI chips they had developed for China, just months after rolling out licensing restrictions on the same AI chips that effectively pulled Nvidia out of the Chinese market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The administration also formally rescinded the Biden administration’s AI diffusion rule (which put a cap on how much&amp;nbsp;AI computing capacity some countries were allowed to buy) in May, just days before it was supposed to go into effect. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration is expected to sign multiple executive orders on July 23. Whether these will contain detailed plans on how it will reach its goals is unclear.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the AI Action Plan talks at length about figuring out how to expand the U.S. AI market globally, while maintaining dominance, it’s light on the specifics. Any executive order regarding chip export restrictions will likely be about getting the proper government departments together to figure out a path forward, as opposed to formal guidelines, quite yet.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-1246479507.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Trump administration wants its AI technology to be considered an industry leader both on home soil and abroad. But it also doesn’t want the U.S.’s AI prowess to empower or embolden a foreign adversary.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s quite the balance to strike.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If President Trump’s AI Action Plan, which was released on Wednesday, is any indicator, it seems the administration is still figuring out the right course of action to achieve those goals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“​​America currently is the global leader on data center construction, computing hardware performance, and models,” the plan stated. “It is imperative that the United States leverage this advantage into an enduring global alliance, while preventing our adversaries from free-riding on our innovation and investment.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The plan mentions strengthening AI chip export controls through “creative approaches” followed by a pair of policy recommendations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first calls on government organizations, including the Department of Commerce and National Security Council, to work with the AI industry on chip location verification features. The second is a recommendation to establish an effort to figure out enforcement for potential chip export restrictions; notably, it mentions that while the U.S. and allies impose export controls on major systems required for chip manufacturing, there isn’t a focus on many of the component subsystems — a hint at where the administration wants the DOC to direct its attention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Action plan also talks about how the U.S. will need to find alignment in this area with its global allies.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“America must impose strong export controls on sensitive technologies,” the plan states. “We should encourage partners and allies to follow U.S. controls, and not backfill. If they do, America should use tools such as the Foreign Direct Product Rule and secondary tariffs to achieve greater international alignment.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI Action plan never gets into detail on exactly how it will achieve Al global alliances, coordinate with allies on export chip restrictions, or work with U.S.-based AI companies on chip location verification features. Instead, the AI Action plans lay out what foundational building blocks are required for future sustainable AI chip export guidelines, as opposed to policies implemented on top of existing guidelines.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: Chip export restrictions are going to take more time. And there’s ample evidence, beyond the AI Action plan, to suggest it will. For instance, the Trump administration has contradicted itself multiple times on its export restriction strategy in the past few months —&amp;nbsp;including just last week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In July, the administration gave semiconductor firms, like Nvidia and AMD, the green light to start selling AI chips they had developed for China, just months after rolling out licensing restrictions on the same AI chips that effectively pulled Nvidia out of the Chinese market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The administration also formally rescinded the Biden administration’s AI diffusion rule (which put a cap on how much&amp;nbsp;AI computing capacity some countries were allowed to buy) in May, just days before it was supposed to go into effect. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration is expected to sign multiple executive orders on July 23. Whether these will contain detailed plans on how it will reach its goals is unclear.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the AI Action Plan talks at length about figuring out how to expand the U.S. AI market globally, while maintaining dominance, it’s light on the specifics. Any executive order regarding chip export restrictions will likely be about getting the proper government departments together to figure out a path forward, as opposed to formal guidelines, quite yet.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/trumps-ai-action-plan-aims-to-block-chip-exports-to-china-but-lacks-key-details/</guid><pubDate>Wed, 23 Jul 2025 18:39:36 +0000</pubDate></item><item><title>[NEW] AI video is invading YouTube Shorts and Google Photos starting today (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/07/ai-video-is-invading-youtube-shorts-and-google-photos-starting-today/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is making video AI models harder to ignore.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Google Photos AI" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Final_Hero_Asset_Thumbnail-640x361.jpg" width="640" /&gt;
                  &lt;img alt="Google Photos AI" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Final_Hero_Asset_Thumbnail-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is following through on recent promises to add more generative AI features to its photo and video products. Over on YouTube, Google is rolling out the first wave of generative AI video for YouTube Shorts, but even if you're not a YouTuber, you'll be exposed to more AI videos soon. Google Photos, which is integrated with virtually every Android phone on the market, is also getting AI video-generation capabilities. In both cases, the features are currently based on the older Veo 2 model, not the more capable Veo 3 that has been meming across the Internet since it was announced at I/O in May.&lt;/p&gt;
&lt;p&gt;YouTube CEO Neal Mohan confirmed earlier this summer that the company planned to add generative AI to the creator tools for YouTube Shorts. There were already tools to generate backgrounds for videos, but the next phase will involve creating new video elements from a text prompt.&lt;/p&gt;
&lt;p&gt;Starting today, creators will be able to use a photo as the basis for a new generative AI video. YouTube also promises a collection of easily applied generative effects, which will be accessible from the Shorts camera. There's also a new AI playground hub that the company says will be home to all its AI tools, along with examples and suggested prompts to help people pump out AI content.&lt;/p&gt;
&lt;div&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1920" id="video-2107623-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Aquatic_Joseph_Street_V01.mp4?_=1" type="video/mp4" /&gt;The Veo 2-based videos aren't as realistic as Veo 3 clips, but an upgrade is planned.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Veo 2-based videos aren't as realistic as Veo 3 clips, but an upgrade is planned.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;
&lt;p&gt;So far, all the YouTube AI video features are running on the Veo 2 model. The plan is still to move to Veo 3 later this summer. The AI features in YouTube Shorts are currently limited to the United States, Canada, Australia, and New Zealand, but they will expand to more countries later.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The new generative AI features in Google Photos are more limited in scale. Starting today, Photos users in the US only will begin seeing a similar set of features. Like YouTube, the app is gaining the ability to turn your photos into short videos, again, based on the older Veo 2 model. There are only two options for these animations: "Subtle movements" or "I'm feeling lucky." In the next few weeks, Photos will also get "Remix," a collection of styles that can be quickly chosen and applied to one of your images.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2107623-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/UI_Walkthrough_Phototovideo_16x9_v12.mp4?_=2" type="video/mp4" /&gt;Photo to video capabilities are coming to Google Photos.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Photo to video capabilities are coming to Google Photos.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where YouTube Shorts is getting the AI playground, Photos is getting the Create tab. It will serve a similar function, listing all the generative AI tools in the app, and it's going to make them harder to ignore, as it will get a prominent place in the bottom navigation bar. The Create tab will begin rolling out in August, and it's also US-only for now.&lt;/p&gt;
&lt;p&gt;Google is also taking this opportunity to again stress that it uses its SynthID digital watermarking technology on all AI-generated images and videos. That theoretically makes it harder to pass off AI content as authentic. That may not be a big concern with videos based on Veo 2, but we've seen how much better Veo 3 is. Google also notes that it conducts ongoing safety analysis to prevent misuse of its AI video models, but that doesn't always stop determined trolls.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is making video AI models harder to ignore.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Google Photos AI" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Final_Hero_Asset_Thumbnail-640x361.jpg" width="640" /&gt;
                  &lt;img alt="Google Photos AI" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Final_Hero_Asset_Thumbnail-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is following through on recent promises to add more generative AI features to its photo and video products. Over on YouTube, Google is rolling out the first wave of generative AI video for YouTube Shorts, but even if you're not a YouTuber, you'll be exposed to more AI videos soon. Google Photos, which is integrated with virtually every Android phone on the market, is also getting AI video-generation capabilities. In both cases, the features are currently based on the older Veo 2 model, not the more capable Veo 3 that has been meming across the Internet since it was announced at I/O in May.&lt;/p&gt;
&lt;p&gt;YouTube CEO Neal Mohan confirmed earlier this summer that the company planned to add generative AI to the creator tools for YouTube Shorts. There were already tools to generate backgrounds for videos, but the next phase will involve creating new video elements from a text prompt.&lt;/p&gt;
&lt;p&gt;Starting today, creators will be able to use a photo as the basis for a new generative AI video. YouTube also promises a collection of easily applied generative effects, which will be accessible from the Shorts camera. There's also a new AI playground hub that the company says will be home to all its AI tools, along with examples and suggested prompts to help people pump out AI content.&lt;/p&gt;
&lt;div&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1920" id="video-2107623-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/Aquatic_Joseph_Street_V01.mp4?_=1" type="video/mp4" /&gt;The Veo 2-based videos aren't as realistic as Veo 3 clips, but an upgrade is planned.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Veo 2-based videos aren't as realistic as Veo 3 clips, but an upgrade is planned.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;
&lt;p&gt;So far, all the YouTube AI video features are running on the Veo 2 model. The plan is still to move to Veo 3 later this summer. The AI features in YouTube Shorts are currently limited to the United States, Canada, Australia, and New Zealand, but they will expand to more countries later.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The new generative AI features in Google Photos are more limited in scale. Starting today, Photos users in the US only will begin seeing a similar set of features. Like YouTube, the app is gaining the ability to turn your photos into short videos, again, based on the older Veo 2 model. There are only two options for these animations: "Subtle movements" or "I'm feeling lucky." In the next few weeks, Photos will also get "Remix," a collection of styles that can be quickly chosen and applied to one of your images.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2107623-2" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/UI_Walkthrough_Phototovideo_16x9_v12.mp4?_=2" type="video/mp4" /&gt;Photo to video capabilities are coming to Google Photos.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Photo to video capabilities are coming to Google Photos.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where YouTube Shorts is getting the AI playground, Photos is getting the Create tab. It will serve a similar function, listing all the generative AI tools in the app, and it's going to make them harder to ignore, as it will get a prominent place in the bottom navigation bar. The Create tab will begin rolling out in August, and it's also US-only for now.&lt;/p&gt;
&lt;p&gt;Google is also taking this opportunity to again stress that it uses its SynthID digital watermarking technology on all AI-generated images and videos. That theoretically makes it harder to pass off AI content as authentic. That may not be a big concern with videos based on Veo 2, but we've seen how much better Veo 3 is. Google also notes that it conducts ongoing safety analysis to prevent misuse of its AI video models, but that doesn't always stop determined trolls.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/07/ai-video-is-invading-youtube-shorts-and-google-photos-starting-today/</guid><pubDate>Wed, 23 Jul 2025 19:34:55 +0000</pubDate></item><item><title>[NEW] OpenAI and partners are building a massive AI data center in Texas (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/openai-and-partners-are-building-a-massive-ai-data-center-in-texas/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        "Easy to throw around numbers, but this is a gigantic infrastructure project."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A photo of the &amp;quot;Stargate I&amp;quot; site in Abilene, Texas." class="absolute inset-0 w-full h-full object-cover hidden" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/stargate-advances-with-partnership-with-oracle-1-640x480.jpg" width="640" /&gt;
                  &lt;img alt="A photo of the &amp;quot;Stargate I&amp;quot; site in Abilene, Texas." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/stargate-advances-with-partnership-with-oracle-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A photo of the "Stargate I" site in Abilene, Texas.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI announced a partnership with Oracle to develop 4.5 gigawatts of additional data center capacity for its Stargate AI infrastructure platform in the US. The expansion, which TechCrunch reports is part of a $30 billion per year deal between OpenAI and Oracle, will reportedly bring OpenAI's total Stargate capacity under development to over 5 gigawatts.&lt;/p&gt;
&lt;p&gt;The data center has taken root in Abilene, Texas, a city of 127,000 located 150 miles west of Fort Worth. The city, which serves as the commercial hub of a 19-county region known as the "Big Country," offers a location with existing tech infrastructure, including Dyess Air Force Base and three universities. Abilene's economy has evolved over time from its agricultural and livestock roots to embrace technology and manufacturing sectors.&lt;/p&gt;
&lt;p&gt;"We have signed a deal for an additional 4.5 gigawatts of capacity with oracle as part of stargate. easy to throw around numbers, but this is a gigantic infrastructure project," wrote OpenAI CEO Sam Altman on X. "We are planning to significantly expand the ambitions of stargate past the $500 billion commitment we announced in January."&lt;/p&gt;
&lt;p&gt;The new agreement builds on OpenAI's initial $500 billion commitment announced at the White House in January to invest in 10 gigawatts of AI infrastructure over four years. The company estimates that the 4.5 GW expansion will generate jobs across construction and operations roles, including direct full-time positions, short-term construction work, and indirect manufacturing and service jobs.&lt;/p&gt;
&lt;p&gt;The 5 gigawatts of total capacity refers to the amount of electrical power these data centers will consume when fully operational—enough to power roughly 4.4 million American homes. It turns out that telling users their every idea is brilliant requires a lot of energy.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Stargate moves forward despite early skepticism&lt;/h2&gt;
&lt;p&gt;When OpenAI announced Stargate in January, critics questioned whether the company could deliver on its ambitious $500 billion funding promise. Trump ally and frequent Altman foe Elon Musk wrote on X that "They don't actually have the money," claiming that "SoftBank has well under $10B secured."&lt;/p&gt;
&lt;p&gt;Tech writer and frequent OpenAI critic Ed Zitron raised concerns about OpenAI's financial position, noting the company's $5 billion in losses in 2024. "This company loses $5bn+ a year! So what, they raise $19bn for Stargate, then what, another $10bn just to be able to survive?" Zitron wrote on Bluesky at the time.&lt;/p&gt;
&lt;p&gt;Six months later, OpenAI's Abilene data center has moved from construction to partial operation. Oracle began delivering Nvidia GB200 racks to the facility last month, and OpenAI reports it has started running early training and inference workloads to support what it calls "next-generation frontier research."&lt;/p&gt;
&lt;p&gt;Despite the White House announcement with President Trump in January, the Stargate concept dates back to March 2024, when Microsoft and OpenAI jointly planned a $100 billion supercomputer as part of a five-phase plan. Over time, the plan evolved into its current form as a partnership with Oracle, SoftBank, and CoreWeave.&lt;/p&gt;
&lt;p&gt;"Stargate is an ambitious undertaking designed to meet the historic opportunity in front of us," writes OpenAI in the press release announcing the latest deal. "That opportunity is now coming to life through strong support from partners, governments, and investors worldwide—including important leadership from the White House, which has recognized the critical role AI infrastructure will play in driving innovation, economic growth, and national competitiveness."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        "Easy to throw around numbers, but this is a gigantic infrastructure project."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A photo of the &amp;quot;Stargate I&amp;quot; site in Abilene, Texas." class="absolute inset-0 w-full h-full object-cover hidden" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/stargate-advances-with-partnership-with-oracle-1-640x480.jpg" width="640" /&gt;
                  &lt;img alt="A photo of the &amp;quot;Stargate I&amp;quot; site in Abilene, Texas." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/stargate-advances-with-partnership-with-oracle-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A photo of the "Stargate I" site in Abilene, Texas.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, OpenAI announced a partnership with Oracle to develop 4.5 gigawatts of additional data center capacity for its Stargate AI infrastructure platform in the US. The expansion, which TechCrunch reports is part of a $30 billion per year deal between OpenAI and Oracle, will reportedly bring OpenAI's total Stargate capacity under development to over 5 gigawatts.&lt;/p&gt;
&lt;p&gt;The data center has taken root in Abilene, Texas, a city of 127,000 located 150 miles west of Fort Worth. The city, which serves as the commercial hub of a 19-county region known as the "Big Country," offers a location with existing tech infrastructure, including Dyess Air Force Base and three universities. Abilene's economy has evolved over time from its agricultural and livestock roots to embrace technology and manufacturing sectors.&lt;/p&gt;
&lt;p&gt;"We have signed a deal for an additional 4.5 gigawatts of capacity with oracle as part of stargate. easy to throw around numbers, but this is a gigantic infrastructure project," wrote OpenAI CEO Sam Altman on X. "We are planning to significantly expand the ambitions of stargate past the $500 billion commitment we announced in January."&lt;/p&gt;
&lt;p&gt;The new agreement builds on OpenAI's initial $500 billion commitment announced at the White House in January to invest in 10 gigawatts of AI infrastructure over four years. The company estimates that the 4.5 GW expansion will generate jobs across construction and operations roles, including direct full-time positions, short-term construction work, and indirect manufacturing and service jobs.&lt;/p&gt;
&lt;p&gt;The 5 gigawatts of total capacity refers to the amount of electrical power these data centers will consume when fully operational—enough to power roughly 4.4 million American homes. It turns out that telling users their every idea is brilliant requires a lot of energy.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Stargate moves forward despite early skepticism&lt;/h2&gt;
&lt;p&gt;When OpenAI announced Stargate in January, critics questioned whether the company could deliver on its ambitious $500 billion funding promise. Trump ally and frequent Altman foe Elon Musk wrote on X that "They don't actually have the money," claiming that "SoftBank has well under $10B secured."&lt;/p&gt;
&lt;p&gt;Tech writer and frequent OpenAI critic Ed Zitron raised concerns about OpenAI's financial position, noting the company's $5 billion in losses in 2024. "This company loses $5bn+ a year! So what, they raise $19bn for Stargate, then what, another $10bn just to be able to survive?" Zitron wrote on Bluesky at the time.&lt;/p&gt;
&lt;p&gt;Six months later, OpenAI's Abilene data center has moved from construction to partial operation. Oracle began delivering Nvidia GB200 racks to the facility last month, and OpenAI reports it has started running early training and inference workloads to support what it calls "next-generation frontier research."&lt;/p&gt;
&lt;p&gt;Despite the White House announcement with President Trump in January, the Stargate concept dates back to March 2024, when Microsoft and OpenAI jointly planned a $100 billion supercomputer as part of a five-phase plan. Over time, the plan evolved into its current form as a partnership with Oracle, SoftBank, and CoreWeave.&lt;/p&gt;
&lt;p&gt;"Stargate is an ambitious undertaking designed to meet the historic opportunity in front of us," writes OpenAI in the press release announcing the latest deal. "That opportunity is now coming to life through strong support from partners, governments, and investors worldwide—including important leadership from the White House, which has recognized the critical role AI infrastructure will play in driving innovation, economic growth, and national competitiveness."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/openai-and-partners-are-building-a-massive-ai-data-center-in-texas/</guid><pubDate>Wed, 23 Jul 2025 21:34:26 +0000</pubDate></item><item><title>[NEW] Sundar Pichai is ‘very excited’ about Google Cloud’s OpenAI partnership (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/sundar-pichai-is-very-excited-about-google-clouds-openai-partnership/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2173545265.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google CEO Sundar Pichai says he is “very excited” to supply OpenAI, the search giant’s largest competitor in AI, with cloud computing resources to train and serve the company’s AI models as part of a recently struck partnership.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With respect to OpenAI, look, we are very excited to be partnering with them on Google Cloud,” said Pichai on Google’s second-quarter earnings call on Wednesday. “Google Cloud is an open platform, and we have a strong history of supporting great companies, startups, AI labs, etc. So super excited about our partnership there on the cloud side, and we look forward to investing more in that relationship and growing that.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The comment came shortly after analysts peppered Pichai and other Google executives with questions about how AI would affect its core search business and why Google is spending an extra $10 billion on capital expenditures this year to catch up in the AI race. Roughly two and a half years since the launch of ChatGPT, Google has now shifted its focus squarely on developing leading AI models and products to compete with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT is a major threat to Google Search, but the OpenAI deal marks a massive new customer for Google Cloud. It’s a treacherous relationship for Google to navigate; OpenAI may ultimately use Google’s cloud infrastructure and chips to upend the company’s core Search product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, OpenAI quietly added Google Cloud to a public list of suppliers that it uses for cloud computing services, alongside Microsoft and Oracle. Reuters previously reported in June that OpenAI was considering tapping Google Cloud for extra computational power.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Google Cloud revenue soared in the second quarter of 2025 to $13.6 billion, up from $10.3 billion in the same quarter last year. Google attributes a significant chunk of that growth to the Google Cloud Platform and other products it offers to AI companies. Google Cloud is still a small business relative to Google Search, but it seems to be growing in the AI era.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several large AI labs have tapped Google Cloud as a cloud computing partner, including Anthropic, Ilya Sutskever’s Safe Superintelligence, Fei-Fei Li’s World Labs, and now OpenAI. Pichai noted on the earnings call that the company has been successful at winning deals with large AI labs thanks to its large supply of Nvidia GPU chips and in-house TPU chips.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google Cloud seems like a smart partner for OpenAI. The startup is extremely constrained when it comes to Nvidia GPUs, which it uses to train new AI models and serve them to hundreds of millions of users. Those constraints have been a major tension point with OpenAI’s biggest backer and largest cloud computing partner, Microsoft, forcing the ChatGPT maker to turn to its competitors in the cloud market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the AI products front, Google seems to be doing better than initially expected. The company said its AI chatbot, Gemini, now reaches 450 million monthly active users, and AI Overviews reaches 2 billion monthly active users. However, the business around these products remains unclear, as does the share of queries they’re taking from Google Search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s hard to imagine that Pichai is truly that excited about working with OpenAI, a company that represents the biggest threat Google Search has ever faced. The partnership is reminiscent of Google’s deal with Yahoo from decades ago, when it was just a startup, and used Yahoo’s homepage as an accelerant to overtake it as the front door to the internet. How lasting OpenAI’s relationship with Google is remains to be seen.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2173545265.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google CEO Sundar Pichai says he is “very excited” to supply OpenAI, the search giant’s largest competitor in AI, with cloud computing resources to train and serve the company’s AI models as part of a recently struck partnership.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With respect to OpenAI, look, we are very excited to be partnering with them on Google Cloud,” said Pichai on Google’s second-quarter earnings call on Wednesday. “Google Cloud is an open platform, and we have a strong history of supporting great companies, startups, AI labs, etc. So super excited about our partnership there on the cloud side, and we look forward to investing more in that relationship and growing that.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The comment came shortly after analysts peppered Pichai and other Google executives with questions about how AI would affect its core search business and why Google is spending an extra $10 billion on capital expenditures this year to catch up in the AI race. Roughly two and a half years since the launch of ChatGPT, Google has now shifted its focus squarely on developing leading AI models and products to compete with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT is a major threat to Google Search, but the OpenAI deal marks a massive new customer for Google Cloud. It’s a treacherous relationship for Google to navigate; OpenAI may ultimately use Google’s cloud infrastructure and chips to upend the company’s core Search product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this month, OpenAI quietly added Google Cloud to a public list of suppliers that it uses for cloud computing services, alongside Microsoft and Oracle. Reuters previously reported in June that OpenAI was considering tapping Google Cloud for extra computational power.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Google Cloud revenue soared in the second quarter of 2025 to $13.6 billion, up from $10.3 billion in the same quarter last year. Google attributes a significant chunk of that growth to the Google Cloud Platform and other products it offers to AI companies. Google Cloud is still a small business relative to Google Search, but it seems to be growing in the AI era.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several large AI labs have tapped Google Cloud as a cloud computing partner, including Anthropic, Ilya Sutskever’s Safe Superintelligence, Fei-Fei Li’s World Labs, and now OpenAI. Pichai noted on the earnings call that the company has been successful at winning deals with large AI labs thanks to its large supply of Nvidia GPU chips and in-house TPU chips.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google Cloud seems like a smart partner for OpenAI. The startup is extremely constrained when it comes to Nvidia GPUs, which it uses to train new AI models and serve them to hundreds of millions of users. Those constraints have been a major tension point with OpenAI’s biggest backer and largest cloud computing partner, Microsoft, forcing the ChatGPT maker to turn to its competitors in the cloud market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the AI products front, Google seems to be doing better than initially expected. The company said its AI chatbot, Gemini, now reaches 450 million monthly active users, and AI Overviews reaches 2 billion monthly active users. However, the business around these products remains unclear, as does the share of queries they’re taking from Google Search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s hard to imagine that Pichai is truly that excited about working with OpenAI, a company that represents the biggest threat Google Search has ever faced. The partnership is reminiscent of Google’s deal with Yahoo from decades ago, when it was just a startup, and used Yahoo’s homepage as an accelerant to overtake it as the front door to the internet. How lasting OpenAI’s relationship with Google is remains to be seen.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/sundar-pichai-is-very-excited-about-google-clouds-openai-partnership/</guid><pubDate>Wed, 23 Jul 2025 22:23:21 +0000</pubDate></item><item><title>[NEW] Trump’s ‘anti-woke AI’ order could reshape how US tech companies train their models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/trumps-anti-woke-ai-order-could-reshape-how-us-tech-companies-train-their-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225853634.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When DeepSeek, Alibaba, and other Chinese firms released their AI models, Western researchers quickly noticed they sidestepped questions critical of the Chinese Communist Party. U.S. officials later confirmed that these tools are engineered to reflect Beijing’s talking points, raising concerns about censorship and bias.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American AI leaders like OpenAI have pointed to this as justification for advancing their tech quickly, without too much regulation or oversight. As OpenAI’s chief global affairs officer Chris Lehane wrote in a LinkedIn post last month, there is a contest between “US-led democratic AI and Communist-led China’s autocratic AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;An executive order signed Wednesday by President Donald Trump that bans “woke AI” and AI models that aren’t “ideologically neutral” from government contracts could disrupt that balance.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order calls out diversity, equity, and inclusion (DEI), calling it a “pervasive and destructive” ideology that can “distort the quality and accuracy of the output.” Specifically, the order refers to information about race or sex, manipulation of racial or sexual representation, critical race theory, transgenderism, unconscious bias, intersectionality, and systemic racism.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Experts warn it could create a chilling effect on developers who may feel pressure to align model outputs and datasets with White House rhetoric to secure federal dollars for their cash-burning businesses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order comes the same day the White House published Trump’s “AI Action Plan,” which shifts national priorities away from societal risk and focuses instead on building out AI infrastructure, cutting red tape for tech companies, shoring up national security, and competing with China.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order instructs the director of the Office of Management and Budget along with the administrator for Federal Procurement Policy, the administrator of General Services, and the director of the Office of Science and Technology Policy to issue guidance to other agencies on how to comply.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Once and for all, we are getting rid of woke,” Trump said Wednesday during an AI event hosted by the All-In Podcast and Hill &amp;amp; Valley Forum. “I will be signing an order banning the federal government from procuring AI technology that has been infused with partisan bias or ideological agendas, such as critical race theory, which is ridiculous. And from now on the U.S. government will deal only with AI that pursues truth, fairness, and strict impartiality.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Determining what is impartial or objective is one of many challenges to the order.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Philip Seargeant, senior lecturer in applied linguistics at the Open University, told TechCrunch that nothing can ever be objective.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“One of the fundamental tenets of sociolinguistics is that language is never neutral,” Seargeant said. “So the idea that you can ever get pure objectivity is a fantasy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On top of that, the Trump administration’s ideology doesn’t reflect the beliefs and values of all Americans. Trump has repeatedly sought to eliminate funding for climate initiatives, education, public broadcasting, research, social service grants, community and agricultural support programs, and gender-affirming care, often framing these initiatives as examples of “woke” or politically biased government spending.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Rumman Chowdhury, a data scientist, CEO of the tech nonprofit Humane Intelligence, and former U.S. science envoy for AI, put it, “Anything [the Trump administration doesn’t] like is immediately tossed into this pejorative pile of woke.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The definitions of “truth-seeking” and “ideological neutrality” in the order published Wednesday are vague in some ways and specific in others. While “truth-seeking” is defined as LLMs that “prioritize historical accuracy, scientific inquiry, and objectivity,” “ideological neutrality” is defined as LLMs that are “neutral, nonpartisan tools that do not manipulate responses in favor of ideological dogmas such as DEI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those definitions leave room for broad interpretation, as well as potential pressure. AI companies have pushed for fewer constraints on how they operate. And while an executive order doesn’t carry the force of legislation, frontier AI firms could still find themselves subject to the shifting priorities of the administration’s political agenda.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, OpenAI, Anthropic, Google, and xAI signed contracts with the Department of Defense to receive up to $200 million each to develop agentic AI workflows that address critical national security challenges.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unclear which of these companies is best positioned to gain from the woke AI ban, or if they will comply.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to each of them and will update this article if we hear back.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Despite displaying biases of its own, xAI may be the most aligned with the order — at least at this early stage. Elon Musk has positioned Grok, xAI’s chatbot, as the ultimate anti-woke, “less biased,” truthseeker. Grok’s system prompts have directed it to avoid deferring to mainstream authorities and media, to seek contrarian information even if it’s politically incorrect, and to even reference Musk’s own views on controversial topics. In recent months, Grok has even spouted antisemitic comments and praised Hitler on X, among other hateful, racist, and misogynistic posts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mark Lemley, a law professor at Stanford University, told TechCrunch the executive order is “clearly intended as viewpoint discrimination, since [the government] just signed a contract with Grok, aka ‘MechaHitler.’”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside xAI’s DOD funding, the company announced that “Grok for Government” had been added to the General Services Administration schedule, meaning that xAI products are now available for purchase across every government office and agency.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The right question is this: would they ban Grok, the AI they just signed a large contract with, because it has been deliberately engineered to give politically charged answers?” Lemley said in an email interview. “If not, it is clearly designed to discriminate against a particular viewpoint.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Grok’s own system prompts have shown, model outputs can be a reflection of both the people building the technology and the data the AI is trained on. In some cases, an overabundance of caution among developers and AI trained on internet content that promotes values like inclusivity have led to distorted model outputs. Google, for example, last year came under fire after its Gemini chatbot showed a black George Washington and racially diverse Nazis — which Trump’s order calls out as an example of DEI-infected AI models.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chowdhury says her biggest fear with this executive order is that AI companies will actively rework training data to tow the party line. She pointed to statements from Musk a few weeks prior to launching Grok 4, saying that xAI would use the new model and its advanced reasoning capabilities to “rewrite the entire corpus of human knowledge, adding missing information and deleting errors. Then retrain on that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This would ostensibly put Musk into the position of judging what is true, which could have huge downstream implications for how information is accessed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, companies have been making judgment calls about what information is seen and not seen since the dawn of the internet.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Conservative David Sacks — the entrepreneur and investor who Trump appointed as AI czar — has been outspoken about his concerns around “woke AI” on the All-In Podcast, which co-hosted Trump’s day of AI announcements. Sacks has accused the creators of prominent AI products of infusing them with left-wing values, framing his arguments as a defense of free speech, and a warning against a trend toward centralized ideological control in digital platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem, experts say, is that there is no one truth. Achieving unbiased or neutral results is impossible, especially in today’s world where even facts are politicized.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the results that an AI produces say that climate science is correct, is that left wing bias?” Seargeant said. “Some people say you need to give both sides of the argument to be objective, even if one side of the argument has no status to it.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225853634.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When DeepSeek, Alibaba, and other Chinese firms released their AI models, Western researchers quickly noticed they sidestepped questions critical of the Chinese Communist Party. U.S. officials later confirmed that these tools are engineered to reflect Beijing’s talking points, raising concerns about censorship and bias.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;American AI leaders like OpenAI have pointed to this as justification for advancing their tech quickly, without too much regulation or oversight. As OpenAI’s chief global affairs officer Chris Lehane wrote in a LinkedIn post last month, there is a contest between “US-led democratic AI and Communist-led China’s autocratic AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;An executive order signed Wednesday by President Donald Trump that bans “woke AI” and AI models that aren’t “ideologically neutral” from government contracts could disrupt that balance.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order calls out diversity, equity, and inclusion (DEI), calling it a “pervasive and destructive” ideology that can “distort the quality and accuracy of the output.” Specifically, the order refers to information about race or sex, manipulation of racial or sexual representation, critical race theory, transgenderism, unconscious bias, intersectionality, and systemic racism.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Experts warn it could create a chilling effect on developers who may feel pressure to align model outputs and datasets with White House rhetoric to secure federal dollars for their cash-burning businesses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order comes the same day the White House published Trump’s “AI Action Plan,” which shifts national priorities away from societal risk and focuses instead on building out AI infrastructure, cutting red tape for tech companies, shoring up national security, and competing with China.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The order instructs the director of the Office of Management and Budget along with the administrator for Federal Procurement Policy, the administrator of General Services, and the director of the Office of Science and Technology Policy to issue guidance to other agencies on how to comply.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Once and for all, we are getting rid of woke,” Trump said Wednesday during an AI event hosted by the All-In Podcast and Hill &amp;amp; Valley Forum. “I will be signing an order banning the federal government from procuring AI technology that has been infused with partisan bias or ideological agendas, such as critical race theory, which is ridiculous. And from now on the U.S. government will deal only with AI that pursues truth, fairness, and strict impartiality.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Determining what is impartial or objective is one of many challenges to the order.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Philip Seargeant, senior lecturer in applied linguistics at the Open University, told TechCrunch that nothing can ever be objective.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“One of the fundamental tenets of sociolinguistics is that language is never neutral,” Seargeant said. “So the idea that you can ever get pure objectivity is a fantasy.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On top of that, the Trump administration’s ideology doesn’t reflect the beliefs and values of all Americans. Trump has repeatedly sought to eliminate funding for climate initiatives, education, public broadcasting, research, social service grants, community and agricultural support programs, and gender-affirming care, often framing these initiatives as examples of “woke” or politically biased government spending.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Rumman Chowdhury, a data scientist, CEO of the tech nonprofit Humane Intelligence, and former U.S. science envoy for AI, put it, “Anything [the Trump administration doesn’t] like is immediately tossed into this pejorative pile of woke.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The definitions of “truth-seeking” and “ideological neutrality” in the order published Wednesday are vague in some ways and specific in others. While “truth-seeking” is defined as LLMs that “prioritize historical accuracy, scientific inquiry, and objectivity,” “ideological neutrality” is defined as LLMs that are “neutral, nonpartisan tools that do not manipulate responses in favor of ideological dogmas such as DEI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Those definitions leave room for broad interpretation, as well as potential pressure. AI companies have pushed for fewer constraints on how they operate. And while an executive order doesn’t carry the force of legislation, frontier AI firms could still find themselves subject to the shifting priorities of the administration’s political agenda.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last week, OpenAI, Anthropic, Google, and xAI signed contracts with the Department of Defense to receive up to $200 million each to develop agentic AI workflows that address critical national security challenges.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unclear which of these companies is best positioned to gain from the woke AI ban, or if they will comply.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to each of them and will update this article if we hear back.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Despite displaying biases of its own, xAI may be the most aligned with the order — at least at this early stage. Elon Musk has positioned Grok, xAI’s chatbot, as the ultimate anti-woke, “less biased,” truthseeker. Grok’s system prompts have directed it to avoid deferring to mainstream authorities and media, to seek contrarian information even if it’s politically incorrect, and to even reference Musk’s own views on controversial topics. In recent months, Grok has even spouted antisemitic comments and praised Hitler on X, among other hateful, racist, and misogynistic posts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mark Lemley, a law professor at Stanford University, told TechCrunch the executive order is “clearly intended as viewpoint discrimination, since [the government] just signed a contract with Grok, aka ‘MechaHitler.’”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside xAI’s DOD funding, the company announced that “Grok for Government” had been added to the General Services Administration schedule, meaning that xAI products are now available for purchase across every government office and agency.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The right question is this: would they ban Grok, the AI they just signed a large contract with, because it has been deliberately engineered to give politically charged answers?” Lemley said in an email interview. “If not, it is clearly designed to discriminate against a particular viewpoint.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Grok’s own system prompts have shown, model outputs can be a reflection of both the people building the technology and the data the AI is trained on. In some cases, an overabundance of caution among developers and AI trained on internet content that promotes values like inclusivity have led to distorted model outputs. Google, for example, last year came under fire after its Gemini chatbot showed a black George Washington and racially diverse Nazis — which Trump’s order calls out as an example of DEI-infected AI models.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chowdhury says her biggest fear with this executive order is that AI companies will actively rework training data to tow the party line. She pointed to statements from Musk a few weeks prior to launching Grok 4, saying that xAI would use the new model and its advanced reasoning capabilities to “rewrite the entire corpus of human knowledge, adding missing information and deleting errors. Then retrain on that.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This would ostensibly put Musk into the position of judging what is true, which could have huge downstream implications for how information is accessed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, companies have been making judgment calls about what information is seen and not seen since the dawn of the internet.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Conservative David Sacks — the entrepreneur and investor who Trump appointed as AI czar — has been outspoken about his concerns around “woke AI” on the All-In Podcast, which co-hosted Trump’s day of AI announcements. Sacks has accused the creators of prominent AI products of infusing them with left-wing values, framing his arguments as a defense of free speech, and a warning against a trend toward centralized ideological control in digital platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem, experts say, is that there is no one truth. Achieving unbiased or neutral results is impossible, especially in today’s world where even facts are politicized.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If the results that an AI produces say that climate science is correct, is that left wing bias?” Seargeant said. “Some people say you need to give both sides of the argument to be objective, even if one side of the argument has no status to it.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/trumps-anti-woke-ai-order-could-reshape-how-us-tech-companies-train-their-models/</guid><pubDate>Wed, 23 Jul 2025 23:25:08 +0000</pubDate></item><item><title>[NEW] A new AI coding challenge just published its first results – and they aren’t pretty (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/GettyImages-1388336038.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new AI coding challenge has revealed its first winner — and set a new bar for AI-powered software engineers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday at 5pm PST, the nonprofit Laude Institute announced the first winner of the K Prize, a multi-round AI coding challenge launched by Databricks and Perplexity co-founder Andy Konwinski. The winner was a Brazilian prompt engineer named Eduardo Rocha de Andrade, who will receive $50,000 for the prize. But more surprising than the win was his final score: he won with correct answers to just 7.5% of the questions on the test.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re glad we built a benchmark that is actually hard,” said Konwinski. “Benchmarks should be hard if they’re going to matter,” he continued, adding: “Scores would be different if the big labs had entered with their biggest models. But that’s kind of the point. K Prize runs offline with limited compute, so it favors smaller and open models. I love that. It levels the playing field.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Konwinski has pledged $1 million to the first open-source model that can score higher than 90% on the test.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Similar to the well-known SWE-Bench system, the K Prize tests models against flagged issues from GitHub as a test of how well models can deal with real-world programming problems. But while SWE-Bench is based on a fixed set of problems that models can train against, the K Prize is designed as a “contamination-free version of SWE-Bench,” using a timed entry system to guard against any benchmark-specific training. For round one, models were due by March 12th. The K Prize organizers then built the test using only GitHub issues flagged after that date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 7.5% top score stands in marked contrast to SWE-Bench itself, which currently shows a 75% top score on its easier ‘Verified’ test and 34% on its harder ‘Full’ test. Konwinski still isn’t sure whether the disparity is due to contamination on SWE-Bench or just the challenge of collecting new issues from GitHub, but he expects the K Prize project to answer the question soon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we get more runs of the thing, we’ll have a better sense,” he told TechCrunch, “because we expect people to adapt to the dynamics of competing on this every few months.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It might seem like an odd place to fall short, given the wide range of AI coding tools already publicly available – but with benchmarks becoming too easy, many critics see projects like the K Prize as a necessary step toward solving AI’s growing evaluation problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m quite bullish about building new tests for existing benchmarks,” says Princeton researcher Sayash Kapoor, who put forward a similar idea in a recent paper. “Without such experiments, we can’t actually tell if the issue is contamination, or even just targeting the SWE-Bench leaderboard with a human in the loop.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Konwinski, it’s not just a better benchmark, but an open challenge to the rest of the industry. “If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination free SWE-Bench, that’s the reality check for me.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/GettyImages-1388336038.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new AI coding challenge has revealed its first winner — and set a new bar for AI-powered software engineers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday at 5pm PST, the nonprofit Laude Institute announced the first winner of the K Prize, a multi-round AI coding challenge launched by Databricks and Perplexity co-founder Andy Konwinski. The winner was a Brazilian prompt engineer named Eduardo Rocha de Andrade, who will receive $50,000 for the prize. But more surprising than the win was his final score: he won with correct answers to just 7.5% of the questions on the test.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re glad we built a benchmark that is actually hard,” said Konwinski. “Benchmarks should be hard if they’re going to matter,” he continued, adding: “Scores would be different if the big labs had entered with their biggest models. But that’s kind of the point. K Prize runs offline with limited compute, so it favors smaller and open models. I love that. It levels the playing field.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Konwinski has pledged $1 million to the first open-source model that can score higher than 90% on the test.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Similar to the well-known SWE-Bench system, the K Prize tests models against flagged issues from GitHub as a test of how well models can deal with real-world programming problems. But while SWE-Bench is based on a fixed set of problems that models can train against, the K Prize is designed as a “contamination-free version of SWE-Bench,” using a timed entry system to guard against any benchmark-specific training. For round one, models were due by March 12th. The K Prize organizers then built the test using only GitHub issues flagged after that date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The 7.5% top score stands in marked contrast to SWE-Bench itself, which currently shows a 75% top score on its easier ‘Verified’ test and 34% on its harder ‘Full’ test. Konwinski still isn’t sure whether the disparity is due to contamination on SWE-Bench or just the challenge of collecting new issues from GitHub, but he expects the K Prize project to answer the question soon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we get more runs of the thing, we’ll have a better sense,” he told TechCrunch, “because we expect people to adapt to the dynamics of competing on this every few months.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It might seem like an odd place to fall short, given the wide range of AI coding tools already publicly available – but with benchmarks becoming too easy, many critics see projects like the K Prize as a necessary step toward solving AI’s growing evaluation problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I’m quite bullish about building new tests for existing benchmarks,” says Princeton researcher Sayash Kapoor, who put forward a similar idea in a recent paper. “Without such experiments, we can’t actually tell if the issue is contamination, or even just targeting the SWE-Bench leaderboard with a human in the loop.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Konwinski, it’s not just a better benchmark, but an open challenge to the rest of the industry. “If you listen to the hype, it’s like we should be seeing AI doctors and AI lawyers and AI software engineers, and that’s just not true,” he says. “If we can’t even get more than 10% on a contamination free SWE-Bench, that’s the reality check for me.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/23/a-new-ai-coding-challenge-just-published-its-first-results-and-they-arent-pretty/</guid><pubDate>Thu, 24 Jul 2025 00:00:00 +0000</pubDate></item><item><title>[NEW] Xinxing Xu bridges AI research and real-world impact at Microsoft Research Asia – Singapore (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/xinxing-xu-bridges-ai-research-and-real-world-impact-at-microsoft-research-asia-singapore/</link><description>&lt;p&gt;While AI has rapidly advanced in recent years, one challenge remains stubbornly unresolved: how to move promising algorithmic models from controlled experiments into practical, real-world use. The effort to balance algorithmic innovation with real-world application has been a consistent theme in the career of Xinxing Xu, principal researcher at Microsoft Research Asia – Singapore, and also represents one of the foundational pillars of the newly established Singapore lab.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="photo of Xinxing Xu standing against a gray background" class="wp-image-1145600" height="540" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1.jpg" width="960" /&gt;&lt;figcaption class="wp-element-caption"&gt;Xinxing Xu, Principal Researcher, Microsoft Research Asia – Singapore&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;“Innovative algorithms can only demonstrate their true value when tested with real-world data and in actual scenarios, where they can be continuously optimized through iteration,” he says.&lt;/p&gt;



&lt;p&gt;Xu’s commitment to balancing algorithmic innovation with practical application has shaped his entire career. During his PhD studies at Nanyang Technological University, Singapore, Xu focused on emerging technologies like multiple kernel learning methods and multimodal machine learning. Today he’s applying these techniques to real-world use cases like image recognition and video classification.&lt;/p&gt;



&lt;p&gt;After completing his doctorate, he joined the Institute of High Performance Computing at Singapore’s Agency for Science, Technology and Research (A*STAR), where he worked on interdisciplinary projects ranging from medical image recognition to AI systems for detecting defects on facade of buildings. These experiences broadened his perspective and deepened his passion for translating AI into real-world impact.&lt;/p&gt;



&lt;p&gt;In 2024, Xu joined Microsoft Research Asia where he began a new chapter focused on bridging between academic research and real-world AI applications.&lt;/p&gt;



&lt;p&gt;“Microsoft Research Asia is committed to integrating scientific exploration with real-world applications, which creates a unique research environment,” Xu says. “It brings together top talent and resources, and Microsoft’s engineering and product ecosystem strongly supports turning research into impactful technology. The lab’s open and inclusive culture encourages innovation with broader societal impact. It reflects the approach to research I’ve always hoped to contribute to.”&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="bringing-cross-domain-expertise-to-ai-s-real-world-frontiers"&gt;Bringing cross-domain expertise to AI’s real-world frontiers&lt;/h2&gt;



&lt;p&gt;As a key hub in Microsoft Research’s network across Asia, the Singapore lab is guided by a three-part mission: to drive industry-transforming AI deployment, pursue fundamental breakthroughs in the field, and promote responsible, socially beneficial applications of the technology.&lt;/p&gt;



&lt;p&gt;To reach these goals, Xu and his colleagues are working closely with local collaborators, combining cross-disciplinary expertise to tackle complex, real-world challenges.&lt;/p&gt;



&lt;p&gt;Xu draws on his experience in healthcare as he leads the team’s collaboration with Singapore’s SingHealth to explore how AI can support precision medicine. Their efforts focus on leveraging SingHealth’s data and expertise to develop AI capabilities aimed at delivering personalized analysis and enhanced diagnostic accuracy to enable better patient outcomes.&lt;/p&gt;



&lt;p&gt;Beyond healthcare, the team is also targeting key sectors like finance and logistics. By developing domain-specific foundation models and AI agents, they aim to support smarter decision-making and accelerate digital transformation across industries. “Singapore has a strong foundation in these sectors,” Xu notes, “making it an ideal environment for technology validation and iteration.”&lt;/p&gt;



&lt;p&gt;The team is also partnering with leading academic institutions, including the National University of Singapore (NUS) and Nanyang Technological University, Singapore (NTU Singapore), to advance the field of spatial intelligence. Their goal is to develop embodied intelligence systems capable of carrying out complex tasks in smart environments.&lt;/p&gt;



&lt;p&gt;As AI becomes more deeply embedded in everyday life, researchers at the Singapore lab are also increasingly focused on what they call “societal AI”—building AI systems that are culturally relevant and trustworthy within Southeast Asia’s unique cultural and social contexts. Working with global colleagues, they are helping to advance a more culturally grounded and responsible approach to AI research in the region.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="microsoft-research-asia-singapore-expanding-global-reach-connecting-regional-innovation"&gt;Microsoft Research Asia – Singapore: Expanding global reach, connecting regional innovation&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Realizing AI’s full potential requires more than technical breakthroughs. It also depends on collaboration—across industries, academia, and policy. Only through this intersection of forces can AI move beyond the lab to deliver meaningful societal value.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Singapore’s strengths in science, engineering, and digital governance make it an ideal setting for this kind of work. Its collaborative culture, robust infrastructure, international talent pool, and strong policy support for science and technology make it fertile ground for interdisciplinary research.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is why Microsoft Research Asia continues to collaborate closely with Singapore’s top universities, research institutions, and industry partners. These partnerships support joint research, talent development, and technical exchange. Building on this foundation, Microsoft Research Asia – Singapore will further deepen its collaboration with NUS, NTU Singapore, and Singapore Management University (SMU) to advance both fundamental and applied research, while equipping the next generation of researchers with real-world experience. In addition, Microsoft Research Asia is fostering academic exchange and strengthening the research ecosystem through summer schools and joint workshops with NUS, NTU Singapore, and SMU.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The launch of the Singapore lab further marks an important step in expanding the company’s global research footprint, serving as a bridge between regional innovation and Microsoft’s global ecosystem. Through its integrated lab network, Microsoft Research fosters the sharing of technologies, methods, and real-world insights, creating a virtuous cycle of innovation.&lt;/p&gt;



&lt;p&gt;“We aim to build a research hub in Singapore that is globally connected and deeply rooted in the local ecosystem,” Xu says. “Many breakthroughs come from interdisciplinary and cross-regional collaboration. By breaking boundaries—across disciplines, industries, and geographies—we can drive research that has lasting impact.”&lt;/p&gt;



&lt;p&gt;As AI becomes more deeply woven into industry and everyday life, Xu believes that meaningful research must be closely connected to regional development and social well-being.&lt;br /&gt;“Microsoft Research Asia – Singapore is a future-facing lab,” he says. “While we push technological frontiers, we’re equally committed to the responsibility of technology—ensuring AI can help address society’s most pressing challenges.”&lt;/p&gt;



&lt;p&gt;In a world shaped by global challenges, Xu sees collaboration and innovation as essential to real progress. With Singapore as a launchpad, he and his team are working to extend AI’s impact and value across Southeast Asia and beyond.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Xingxing Xu (center) with colleagues at Microsoft Research Asia - Singapore&amp;nbsp;" class="wp-image-1145598" height="768" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Xingxing Xu (center) with colleagues at Microsoft Research Asia – Singapore&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="three-essential-strengths-for-the-next-generation-of-ai-researchers"&gt;Three essential strengths for the next generation of AI researchers&lt;/h2&gt;



&lt;p&gt;AI’s progress depends not only on technical breakthroughs but also on the growth and dedication of talent. At Microsoft Research Asia, there is a strong belief that bringing research into the real world requires more than technical coordination—it depends on unlocking the full creativity and potential of researchers.&lt;/p&gt;



&lt;p&gt;In Singapore—a regional innovation hub that connects Southeast Asia—Xu and his colleagues are working to push AI beyond the lab and into fields like healthcare, finance, and manufacturing. For young researchers hoping to shape the future of AI, this is a uniquely powerful stage.&lt;/p&gt;



&lt;p&gt;To help guide the next generation, Xu shares three pieces of advice:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Build a strong foundation&lt;/strong&gt; – “Core knowledge in machine learning, linear algebra, and probability and statistics is the bedrock of AI research,” Xu says. “A solid theoretical base is essential to remain competitive in a rapidly evolving field. Even today’s hottest trends in generative AI rely on longstanding principles of optimization and model architecture design.” While code generation tools are on the rise, Xu emphasizes that mathematical fundamentals remain essential for understanding and innovating in AI.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Understand real-world applications&lt;/strong&gt; – Technical skills alone aren’t enough. Xu encourages young researchers to deeply engage with the problems they’re trying to solve. Only by tightly integrating technology with its context can researchers create truly valuable solutions.&lt;p&gt;“In healthcare, for example, researchers may need to follow doctors in clinics to gain a true understanding of clinical workflows. That context helps identify the best entry points for AI deployment. Framing research problems around real-world needs is often more impactful than just tuning model parameters,” Xu says.&lt;/p&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Develop interdisciplinary thinking&lt;/strong&gt; – Cross-disciplinary collaboration is becoming essential to AI innovation. Xu advises young researchers to learn how to work with experts from other fields to explore new directions together. “These kinds of interactions often spark fresh, creative ideas,” he says.&lt;p&gt;Maintaining curiosity is just as important. “Being open to new technologies and fields is what enables researchers to continually break new ground and produce original results.”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Xu extends an open invitation to aspiring researchers from all backgrounds to join Microsoft Research Asia – Singapore. “We offer a unique platform that blends cutting-edge research with real-world impact,” he says. “It’s a place where you can work on the frontiers of AI—and see how your work can help transform industries and improve lives.”&lt;/p&gt;



&lt;p&gt;To learn more about the current opening, please visit: https://jobs.careers.microsoft.com/global/en/job/1849717/Senior-Researcher&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;p&gt;While AI has rapidly advanced in recent years, one challenge remains stubbornly unresolved: how to move promising algorithmic models from controlled experiments into practical, real-world use. The effort to balance algorithmic innovation with real-world application has been a consistent theme in the career of Xinxing Xu, principal researcher at Microsoft Research Asia – Singapore, and also represents one of the foundational pillars of the newly established Singapore lab.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="photo of Xinxing Xu standing against a gray background" class="wp-image-1145600" height="540" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-Xu-1-1.jpg" width="960" /&gt;&lt;figcaption class="wp-element-caption"&gt;Xinxing Xu, Principal Researcher, Microsoft Research Asia – Singapore&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;“Innovative algorithms can only demonstrate their true value when tested with real-world data and in actual scenarios, where they can be continuously optimized through iteration,” he says.&lt;/p&gt;



&lt;p&gt;Xu’s commitment to balancing algorithmic innovation with practical application has shaped his entire career. During his PhD studies at Nanyang Technological University, Singapore, Xu focused on emerging technologies like multiple kernel learning methods and multimodal machine learning. Today he’s applying these techniques to real-world use cases like image recognition and video classification.&lt;/p&gt;



&lt;p&gt;After completing his doctorate, he joined the Institute of High Performance Computing at Singapore’s Agency for Science, Technology and Research (A*STAR), where he worked on interdisciplinary projects ranging from medical image recognition to AI systems for detecting defects on facade of buildings. These experiences broadened his perspective and deepened his passion for translating AI into real-world impact.&lt;/p&gt;



&lt;p&gt;In 2024, Xu joined Microsoft Research Asia where he began a new chapter focused on bridging between academic research and real-world AI applications.&lt;/p&gt;



&lt;p&gt;“Microsoft Research Asia is committed to integrating scientific exploration with real-world applications, which creates a unique research environment,” Xu says. “It brings together top talent and resources, and Microsoft’s engineering and product ecosystem strongly supports turning research into impactful technology. The lab’s open and inclusive culture encourages innovation with broader societal impact. It reflects the approach to research I’ve always hoped to contribute to.”&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="bringing-cross-domain-expertise-to-ai-s-real-world-frontiers"&gt;Bringing cross-domain expertise to AI’s real-world frontiers&lt;/h2&gt;



&lt;p&gt;As a key hub in Microsoft Research’s network across Asia, the Singapore lab is guided by a three-part mission: to drive industry-transforming AI deployment, pursue fundamental breakthroughs in the field, and promote responsible, socially beneficial applications of the technology.&lt;/p&gt;



&lt;p&gt;To reach these goals, Xu and his colleagues are working closely with local collaborators, combining cross-disciplinary expertise to tackle complex, real-world challenges.&lt;/p&gt;



&lt;p&gt;Xu draws on his experience in healthcare as he leads the team’s collaboration with Singapore’s SingHealth to explore how AI can support precision medicine. Their efforts focus on leveraging SingHealth’s data and expertise to develop AI capabilities aimed at delivering personalized analysis and enhanced diagnostic accuracy to enable better patient outcomes.&lt;/p&gt;



&lt;p&gt;Beyond healthcare, the team is also targeting key sectors like finance and logistics. By developing domain-specific foundation models and AI agents, they aim to support smarter decision-making and accelerate digital transformation across industries. “Singapore has a strong foundation in these sectors,” Xu notes, “making it an ideal environment for technology validation and iteration.”&lt;/p&gt;



&lt;p&gt;The team is also partnering with leading academic institutions, including the National University of Singapore (NUS) and Nanyang Technological University, Singapore (NTU Singapore), to advance the field of spatial intelligence. Their goal is to develop embodied intelligence systems capable of carrying out complex tasks in smart environments.&lt;/p&gt;



&lt;p&gt;As AI becomes more deeply embedded in everyday life, researchers at the Singapore lab are also increasingly focused on what they call “societal AI”—building AI systems that are culturally relevant and trustworthy within Southeast Asia’s unique cultural and social contexts. Working with global colleagues, they are helping to advance a more culturally grounded and responsible approach to AI research in the region.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="microsoft-research-asia-singapore-expanding-global-reach-connecting-regional-innovation"&gt;Microsoft Research Asia – Singapore: Expanding global reach, connecting regional innovation&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Realizing AI’s full potential requires more than technical breakthroughs. It also depends on collaboration—across industries, academia, and policy. Only through this intersection of forces can AI move beyond the lab to deliver meaningful societal value.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Singapore’s strengths in science, engineering, and digital governance make it an ideal setting for this kind of work. Its collaborative culture, robust infrastructure, international talent pool, and strong policy support for science and technology make it fertile ground for interdisciplinary research.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is why Microsoft Research Asia continues to collaborate closely with Singapore’s top universities, research institutions, and industry partners. These partnerships support joint research, talent development, and technical exchange. Building on this foundation, Microsoft Research Asia – Singapore will further deepen its collaboration with NUS, NTU Singapore, and Singapore Management University (SMU) to advance both fundamental and applied research, while equipping the next generation of researchers with real-world experience. In addition, Microsoft Research Asia is fostering academic exchange and strengthening the research ecosystem through summer schools and joint workshops with NUS, NTU Singapore, and SMU.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The launch of the Singapore lab further marks an important step in expanding the company’s global research footprint, serving as a bridge between regional innovation and Microsoft’s global ecosystem. Through its integrated lab network, Microsoft Research fosters the sharing of technologies, methods, and real-world insights, creating a virtuous cycle of innovation.&lt;/p&gt;



&lt;p&gt;“We aim to build a research hub in Singapore that is globally connected and deeply rooted in the local ecosystem,” Xu says. “Many breakthroughs come from interdisciplinary and cross-regional collaboration. By breaking boundaries—across disciplines, industries, and geographies—we can drive research that has lasting impact.”&lt;/p&gt;



&lt;p&gt;As AI becomes more deeply woven into industry and everyday life, Xu believes that meaningful research must be closely connected to regional development and social well-being.&lt;br /&gt;“Microsoft Research Asia – Singapore is a future-facing lab,” he says. “While we push technological frontiers, we’re equally committed to the responsibility of technology—ensuring AI can help address society’s most pressing challenges.”&lt;/p&gt;



&lt;p&gt;In a world shaped by global challenges, Xu sees collaboration and innovation as essential to real progress. With Singapore as a launchpad, he and his team are working to extend AI’s impact and value across Southeast Asia and beyond.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Xingxing Xu (center) with colleagues at Microsoft Research Asia - Singapore&amp;nbsp;" class="wp-image-1145598" height="768" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/Xinxing-with-colleagues.jpg" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;Xingxing Xu (center) with colleagues at Microsoft Research Asia – Singapore&amp;nbsp;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="three-essential-strengths-for-the-next-generation-of-ai-researchers"&gt;Three essential strengths for the next generation of AI researchers&lt;/h2&gt;



&lt;p&gt;AI’s progress depends not only on technical breakthroughs but also on the growth and dedication of talent. At Microsoft Research Asia, there is a strong belief that bringing research into the real world requires more than technical coordination—it depends on unlocking the full creativity and potential of researchers.&lt;/p&gt;



&lt;p&gt;In Singapore—a regional innovation hub that connects Southeast Asia—Xu and his colleagues are working to push AI beyond the lab and into fields like healthcare, finance, and manufacturing. For young researchers hoping to shape the future of AI, this is a uniquely powerful stage.&lt;/p&gt;



&lt;p&gt;To help guide the next generation, Xu shares three pieces of advice:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Build a strong foundation&lt;/strong&gt; – “Core knowledge in machine learning, linear algebra, and probability and statistics is the bedrock of AI research,” Xu says. “A solid theoretical base is essential to remain competitive in a rapidly evolving field. Even today’s hottest trends in generative AI rely on longstanding principles of optimization and model architecture design.” While code generation tools are on the rise, Xu emphasizes that mathematical fundamentals remain essential for understanding and innovating in AI.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Understand real-world applications&lt;/strong&gt; – Technical skills alone aren’t enough. Xu encourages young researchers to deeply engage with the problems they’re trying to solve. Only by tightly integrating technology with its context can researchers create truly valuable solutions.&lt;p&gt;“In healthcare, for example, researchers may need to follow doctors in clinics to gain a true understanding of clinical workflows. That context helps identify the best entry points for AI deployment. Framing research problems around real-world needs is often more impactful than just tuning model parameters,” Xu says.&lt;/p&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Develop interdisciplinary thinking&lt;/strong&gt; – Cross-disciplinary collaboration is becoming essential to AI innovation. Xu advises young researchers to learn how to work with experts from other fields to explore new directions together. “These kinds of interactions often spark fresh, creative ideas,” he says.&lt;p&gt;Maintaining curiosity is just as important. “Being open to new technologies and fields is what enables researchers to continually break new ground and produce original results.”&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Xu extends an open invitation to aspiring researchers from all backgrounds to join Microsoft Research Asia – Singapore. “We offer a unique platform that blends cutting-edge research with real-world impact,” he says. “It’s a place where you can work on the frontiers of AI—and see how your work can help transform industries and improve lives.”&lt;/p&gt;



&lt;p&gt;To learn more about the current opening, please visit: https://jobs.careers.microsoft.com/global/en/job/1849717/Senior-Researcher&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/xinxing-xu-bridges-ai-research-and-real-world-impact-at-microsoft-research-asia-singapore/</guid><pubDate>Thu, 24 Jul 2025 01:30:00 +0000</pubDate></item></channel></rss>