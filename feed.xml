<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 14 Nov 2025 01:46:56 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>[NEW] Coding assistant Cursor raises¬†$2.3B 5 months after its¬†previous¬†round (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/coding-assistant-cursor-raises-2-3b-5-months-after-its-previous-round/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/opengraph-image.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Developer AI coding tool Cursor continues to gobble up venture capital as its valuation keeps climbing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Cursor announced a $2.3 billion funding round that valued the company at $29.3 billion, as originally reported by the Wall Street Journal. This round more than doubles the company‚Äôs previous valuation of $9.9 billion, which it achieved in its $900 million Series C round in June.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This most recent fundraise was led by co-led by Accel, an existing investor, and Coatue, which is new to the cap table. Strategic investors, including Nvidia (an enterprise customer) and Google (an AI model supplier), also joined the round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joshua Kushner‚Äôs Thrive Capital led the company‚Äôs prior two rounds and participated in this round as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor‚Äôs co-founder and CEO Michael Truell told the Wall Street Journal that the capital from the round will be put toward developing Composer, an AI model released by Cursor in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor still relies on outside AI models from companies, including Google, OpenAI, and Anthropic to power its platform, but the plan is for Composer to carry some of that load in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Next year could be a very interesting one for Cursor. While the company is still seeing strong growth, OpenAI and Anthropic are both sharpening their AI coding products as the market for AI development tools continues to get more competitive.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/opengraph-image.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Developer AI coding tool Cursor continues to gobble up venture capital as its valuation keeps climbing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Cursor announced a $2.3 billion funding round that valued the company at $29.3 billion, as originally reported by the Wall Street Journal. This round more than doubles the company‚Äôs previous valuation of $9.9 billion, which it achieved in its $900 million Series C round in June.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This most recent fundraise was led by co-led by Accel, an existing investor, and Coatue, which is new to the cap table. Strategic investors, including Nvidia (an enterprise customer) and Google (an AI model supplier), also joined the round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Joshua Kushner‚Äôs Thrive Capital led the company‚Äôs prior two rounds and participated in this round as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor‚Äôs co-founder and CEO Michael Truell told the Wall Street Journal that the capital from the round will be put toward developing Composer, an AI model released by Cursor in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor still relies on outside AI models from companies, including Google, OpenAI, and Anthropic to power its platform, but the plan is for Composer to carry some of that load in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Next year could be a very interesting one for Cursor. While the company is still seeing strong growth, OpenAI and Anthropic are both sharpening their AI coding products as the market for AI development tools continues to get more competitive.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/coding-assistant-cursor-raises-2-3b-5-months-after-its-previous-round/</guid><pubDate>Thu, 13 Nov 2025 13:58:44 +0000</pubDate></item><item><title>Google augments AI shopping with conversational search, agentic checkout and an AI that calls stores for you (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/google-expands-ai-shopping-with-conversational-search-agentic-checkout-and-an-ai-that-calls-stores-for-you/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is rolling out a suite of AI shopping updates just ahead of the holiday season. The company on Thursday unveiled a host of new tools and features, including conversational shopping in Google Search, new shopping features within its Gemini app, agentic checkout, and even an AI tool that can call local stores to find out if a product you want is available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company believes the additions will help improve the online shopping  experience, which can still today involve a lot of drudgery, explained Vidhya Srinivasan, VP and GM of ads and commerce at Google, in a press briefing ahead of the launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúWe feel it really shouldn‚Äôt be so tedious, and shopping should feel ‚Äî and can feel ‚Äî a lot more natural and easy,‚Äù she said. ‚ÄúThe idea here is we want to hold on to all the fun parts of shopping, like the browsing, like the serendipitous discovery, and things like that, but then skip all the tedious, hard parts.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067175" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Best-Product.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One update will allow consumers to ask shopping questions in AI Mode, Google‚Äôs conversational search feature that lets you use natural language queries in a chatbot-style interface. The responses will be tailored to your question, and the chatbot will provide images when you need visual inspiration, alongside other details like price, reviews, and available inventory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So if you were looking for cozy sweaters in autumn colors, you‚Äôd see photos of the options available to you. But if you were comparing items, like skin care products, Google may instead return insights in a comparison table.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067176" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Comparison.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that AI Mode is powered by its Shopping Graph, which includes over 50 billion product listings, 2 billion of which are updated every hour, and said the inventory information you see is usually up-to-date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another shopping update will enable the Gemini app to provide fleshed-out ideas as responses, instead of just text suggestions in response to shopping-related questions. This is only available to users in the U.S. currently.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company confirmed that consumers using AI Mode will see sponsored listings, but as the features are still experimental, these ads won‚Äôt appear in the Gemini mobile app just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Google is rolling out agentic checkout within Google Search in the U.S., including in AI Mode. The feature is currently compatible with merchants like Wayfair, Chewy, Quince, and select Shopify stores.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067179" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Checkout.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;To use agentic checkout, you can begin by tracking an item‚Äôs price to be notified if the price drops to fall within your budget. You can then opt to have Google purchase the item for you on the merchant‚Äôs website using Google Pay. The company says it will always ask your permission first, and will have you confirm your purchase and shipping details.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúThis is helpful for shoppers, because they don‚Äôt have to constantly check to see if the item they want is on sale. And it‚Äôs great for retailers because it brings back the customer who might otherwise have moved on,‚Äù said Lilian Rincon, VP of product management for Google Shopping, during the briefing. ‚ÄúAgentic checkout is built on Google‚Äôs trusted shopping graph and also G Pay, so you can rest assured that you‚Äôre seeing accurate results and that your payment information is secure,‚Äù she noted.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067177" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Button-highlighted.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another AI feature can call businesses on your behalf to find out if a store carries a product, how much it costs, and whether there are any promotions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is built on Google‚Äôs Duplex technology, introduced back in 2018, as well as its Shopping Graph and payments infrastructure. After you‚Äôve provided information about the product you‚Äôre looking for, the AI will call local stores and make inquiries about the product, then come back to you with a summary of its findings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature is rolling out now in the U.S. for specific categories like toys, health and beauty products, and electronics. To use this feature, you can search for products ‚Äúnear me,‚Äù then use the option ‚ÄúLet Google Call.‚Äù The AI will then walk you through questions about the items you‚Äôre searching for.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067178" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Report.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it‚Äôs being mindful of how merchants will experience these calls, and it will make sure the chatbot will not call too often and is clear about the questions it asks. Retailers can choose to opt out of receiving such calls as well. Those who don‚Äôt will first hear Google disclose that it‚Äôs an AI calling on a customer‚Äôs behalf and only proceed when the recipient of the call says it‚Äôs okay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google execs planned to demo the technology during the press briefing on Wednesday, but Wi-Fi issues on their end led them to abandon the demo before it was completed.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is rolling out a suite of AI shopping updates just ahead of the holiday season. The company on Thursday unveiled a host of new tools and features, including conversational shopping in Google Search, new shopping features within its Gemini app, agentic checkout, and even an AI tool that can call local stores to find out if a product you want is available.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company believes the additions will help improve the online shopping  experience, which can still today involve a lot of drudgery, explained Vidhya Srinivasan, VP and GM of ads and commerce at Google, in a press briefing ahead of the launch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúWe feel it really shouldn‚Äôt be so tedious, and shopping should feel ‚Äî and can feel ‚Äî a lot more natural and easy,‚Äù she said. ‚ÄúThe idea here is we want to hold on to all the fun parts of shopping, like the browsing, like the serendipitous discovery, and things like that, but then skip all the tedious, hard parts.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067175" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Best-Product.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One update will allow consumers to ask shopping questions in AI Mode, Google‚Äôs conversational search feature that lets you use natural language queries in a chatbot-style interface. The responses will be tailored to your question, and the chatbot will provide images when you need visual inspiration, alongside other details like price, reviews, and available inventory.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So if you were looking for cozy sweaters in autumn colors, you‚Äôd see photos of the options available to you. But if you were comparing items, like skin care products, Google may instead return insights in a comparison table.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067176" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Comparison.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that AI Mode is powered by its Shopping Graph, which includes over 50 billion product listings, 2 billion of which are updated every hour, and said the inventory information you see is usually up-to-date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another shopping update will enable the Gemini app to provide fleshed-out ideas as responses, instead of just text suggestions in response to shopping-related questions. This is only available to users in the U.S. currently.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company confirmed that consumers using AI Mode will see sponsored listings, but as the features are still experimental, these ads won‚Äôt appear in the Gemini mobile app just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Google is rolling out agentic checkout within Google Search in the U.S., including in AI Mode. The feature is currently compatible with merchants like Wayfair, Chewy, Quince, and select Shopify stores.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067179" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Checkout.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;To use agentic checkout, you can begin by tracking an item‚Äôs price to be notified if the price drops to fall within your budget. You can then opt to have Google purchase the item for you on the merchant‚Äôs website using Google Pay. The company says it will always ask your permission first, and will have you confirm your purchase and shipping details.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúThis is helpful for shoppers, because they don‚Äôt have to constantly check to see if the item they want is on sale. And it‚Äôs great for retailers because it brings back the customer who might otherwise have moved on,‚Äù said Lilian Rincon, VP of product management for Google Shopping, during the briefing. ‚ÄúAgentic checkout is built on Google‚Äôs trusted shopping graph and also G Pay, so you can rest assured that you‚Äôre seeing accurate results and that your payment information is secure,‚Äù she noted.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067177" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Button-highlighted.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another AI feature can call businesses on your behalf to find out if a store carries a product, how much it costs, and whether there are any promotions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is built on Google‚Äôs Duplex technology, introduced back in 2018, as well as its Shopping Graph and payments infrastructure. After you‚Äôve provided information about the product you‚Äôre looking for, the AI will call local stores and make inquiries about the product, then come back to you with a summary of its findings.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This feature is rolling out now in the U.S. for specific categories like toys, health and beauty products, and electronics. To use this feature, you can search for products ‚Äúnear me,‚Äù then use the option ‚ÄúLet Google Call.‚Äù The AI will then walk you through questions about the items you‚Äôre searching for.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067178" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Agentic-Calling-Report.png?w=383" width="383" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it‚Äôs being mindful of how merchants will experience these calls, and it will make sure the chatbot will not call too often and is clear about the questions it asks. Retailers can choose to opt out of receiving such calls as well. Those who don‚Äôt will first hear Google disclose that it‚Äôs an AI calling on a customer‚Äôs behalf and only proceed when the recipient of the call says it‚Äôs okay.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google execs planned to demo the technology during the press briefing on Wednesday, but Wi-Fi issues on their end led them to abandon the demo before it was completed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/google-expands-ai-shopping-with-conversational-search-agentic-checkout-and-an-ai-that-calls-stores-for-you/</guid><pubDate>Thu, 13 Nov 2025 14:00:00 +0000</pubDate></item><item><title>GeForce NOW Enlists ‚ÄòCall of Duty: Black Ops 7‚Äô for the Cloud (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-call-of-duty-black-ops-7/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Chaos has entered the chat. It‚Äôs GFN Thursday, and things are getting intense with the launch of &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, streaming at launch this week on GeForce NOW. Dive straight into the chaos across devices ‚Äî underpowered laptops, Macs, and Steam Decks.&lt;/p&gt;
&lt;p&gt;Power up the week with 12 new games to stream in the cloud, including the latest GeForce RTX 5080-optimized game, &lt;i&gt;Anno 117: Pax Romana.&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87289"&gt;&lt;img alt="Phoenix now live on GeForce NOW" class="size-large wp-image-87289" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Blackwell_RTX_status-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87289"&gt;&lt;em&gt;Stockholm will be the next region to light up with GeForce RTX 5080-class power.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Phoenix is the latest region to get GeForce RTX 5080-class power, with Stockholm coming up next. Stay tuned to GFN Thursday for updates as more regions upgrade to Blackwell RTX. Follow along with the latest progress on the server rollout page.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The Biggest ‚ÄòBlack Ops‚Äô Ever&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, Treyarch and Raven Software are bringing players the biggest Black Ops ever. The year is 2035, and the world teeters on the edge of collapse after the global events following &lt;i&gt;Black Ops 2&lt;/i&gt; and &lt;i&gt;Black Ops 6&lt;/i&gt;. Wielding cutting-edge technology, the Black Ops team led by David Mason must fight back against a manipulative enemy who weaponizes fear above all else.&lt;/p&gt;
&lt;p&gt;Step into the boots of David Mason and his elite Black Ops team as they fight against unseen forces that bend reality and reason. Squad up in an all-new Co-op Campaign designed for the biggest tactical play, unleash high-tech weaponry across a multiplayer experience brimming with new maps and modes, and descend once again into darkness with the next twisted chapter of Round-Based Zombies.&lt;/p&gt;
&lt;p&gt;Stream &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt; today on GeForce NOW and play seamlessly across devices. The only way to play the game on Steam Deck is through GeForce NOW. Premium members get instant access with no waiting around for downloads, the highest frame rates, lowest latency and longer gaming sessions.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;All Roads Lead to the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87292"&gt;&lt;img alt="Anno 117: Pax Romana on GeForce NOW" class="size-large wp-image-87292" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Anno_117_Pax_Romano-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87292"&gt;&lt;em&gt;Build Rome your way ‚Äî just don‚Äôt anger the Senate.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ubisoft‚Äôs &lt;i&gt;Anno 117: Pax Romana&lt;/i&gt; builds on the rich legacy of the city‚Äëbuilding and strategy series, inviting players to shape the world at the height of the Roman Empire. Step into the sandals of a Roman leader to expand territories, manage trade and balance prosperity with political intrigue in a meticulously crafted ancient world.&lt;/p&gt;
&lt;p&gt;Players can implement decisive rule, careful diplomacy or a vision for peace as they oversee provinces ‚Äî from bustling Mediterranean ports to fog‚Äëcovered British frontiers ‚Äî in a game where every decision shapes the empire‚Äôs stability and economy, as well as citizens‚Äô happiness levels.&lt;/p&gt;
&lt;p&gt;Launching on GeForce NOW with GeForce RTX 5080-class power,&lt;i&gt; Anno 117&lt;/i&gt; takes advantage of 5K 120 frames-per-second streaming for breathtaking detail and ultrasmooth performance, letting every brick, marketplace and coastline shine in cinematic clarity. Experience the empire come alive instantly in the cloud ‚Äî no downloads nor lag, just seamless city‚Äëbuilding, even on the go.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Start Your Engines&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87295"&gt;&lt;img alt="Assetto Corsa Rally on GeForce NOW" class="size-large wp-image-87295" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Assetto_Corsa_Rally-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87295"&gt;&lt;em&gt;Every second counts.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;by 505 Game captures the pulse-pounding unpredictability and challenge of rally racing with uncompromising enthusiasm. Every stage plunges players into dynamic conditions ‚Äî from shifting surface grip and evolving weather to crowd reactions ‚Äî that demand sharp instincts and constant adaptation. With its precisely laser-scanned tracks, realistic environments and authentic co-driver pace notes delivered by real rally professionals, the game radiates motorsport passion and attention to detail.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Surviving Mars: Relaunched &lt;/i&gt;(New release on Steam, Nov. 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Possessor(s) &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Rue Valley &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Anno 117: Pax Romana &lt;/i&gt;(New release on Steam and Ubisoft, Nov. 13, GeForce RTX 5080-ready)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;INAZUMA ELEVEN: Victory Road &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Songs of Silence&lt;/i&gt; (New release on Epic Games Store, Free on Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Call of Duty: Black Ops 7 &lt;/i&gt;(New release on Steam, Battle.net and Xbox, available on PC Game Pass, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Where Winds Meet &lt;/i&gt;(New release on Epic Games Store, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Megabonk &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;R.E.P.O. &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;RV There Yet? &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Members who already own the Install-to-Play titles that are being added as Ready-to-Play titles this week can check out this support article for steps on how to re-add them to their libraries. All Steam Cloud Saves will be carried over.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What‚Äôs been the oddest or weirdest thing you've seen in the background of a game ‚Äì screenshots and clips are welcome!&lt;/p&gt;
&lt;p&gt;‚Äî üå©Ô∏è NVIDIA GeForce NOW (@NVIDIAGFN) November 10, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Chaos has entered the chat. It‚Äôs GFN Thursday, and things are getting intense with the launch of &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, streaming at launch this week on GeForce NOW. Dive straight into the chaos across devices ‚Äî underpowered laptops, Macs, and Steam Decks.&lt;/p&gt;
&lt;p&gt;Power up the week with 12 new games to stream in the cloud, including the latest GeForce RTX 5080-optimized game, &lt;i&gt;Anno 117: Pax Romana.&lt;/i&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87289"&gt;&lt;img alt="Phoenix now live on GeForce NOW" class="size-large wp-image-87289" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Blackwell_RTX_status-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87289"&gt;&lt;em&gt;Stockholm will be the next region to light up with GeForce RTX 5080-class power.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Phoenix is the latest region to get GeForce RTX 5080-class power, with Stockholm coming up next. Stay tuned to GFN Thursday for updates as more regions upgrade to Blackwell RTX. Follow along with the latest progress on the server rollout page.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The Biggest ‚ÄòBlack Ops‚Äô Ever&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt;, Treyarch and Raven Software are bringing players the biggest Black Ops ever. The year is 2035, and the world teeters on the edge of collapse after the global events following &lt;i&gt;Black Ops 2&lt;/i&gt; and &lt;i&gt;Black Ops 6&lt;/i&gt;. Wielding cutting-edge technology, the Black Ops team led by David Mason must fight back against a manipulative enemy who weaponizes fear above all else.&lt;/p&gt;
&lt;p&gt;Step into the boots of David Mason and his elite Black Ops team as they fight against unseen forces that bend reality and reason. Squad up in an all-new Co-op Campaign designed for the biggest tactical play, unleash high-tech weaponry across a multiplayer experience brimming with new maps and modes, and descend once again into darkness with the next twisted chapter of Round-Based Zombies.&lt;/p&gt;
&lt;p&gt;Stream &lt;i&gt;Call of Duty: Black Ops 7&lt;/i&gt; today on GeForce NOW and play seamlessly across devices. The only way to play the game on Steam Deck is through GeForce NOW. Premium members get instant access with no waiting around for downloads, the highest frame rates, lowest latency and longer gaming sessions.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;All Roads Lead to the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87292"&gt;&lt;img alt="Anno 117: Pax Romana on GeForce NOW" class="size-large wp-image-87292" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Anno_117_Pax_Romano-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87292"&gt;&lt;em&gt;Build Rome your way ‚Äî just don‚Äôt anger the Senate.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ubisoft‚Äôs &lt;i&gt;Anno 117: Pax Romana&lt;/i&gt; builds on the rich legacy of the city‚Äëbuilding and strategy series, inviting players to shape the world at the height of the Roman Empire. Step into the sandals of a Roman leader to expand territories, manage trade and balance prosperity with political intrigue in a meticulously crafted ancient world.&lt;/p&gt;
&lt;p&gt;Players can implement decisive rule, careful diplomacy or a vision for peace as they oversee provinces ‚Äî from bustling Mediterranean ports to fog‚Äëcovered British frontiers ‚Äî in a game where every decision shapes the empire‚Äôs stability and economy, as well as citizens‚Äô happiness levels.&lt;/p&gt;
&lt;p&gt;Launching on GeForce NOW with GeForce RTX 5080-class power,&lt;i&gt; Anno 117&lt;/i&gt; takes advantage of 5K 120 frames-per-second streaming for breathtaking detail and ultrasmooth performance, letting every brick, marketplace and coastline shine in cinematic clarity. Experience the empire come alive instantly in the cloud ‚Äî no downloads nor lag, just seamless city‚Äëbuilding, even on the go.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Start Your Engines&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87295"&gt;&lt;img alt="Assetto Corsa Rally on GeForce NOW" class="size-large wp-image-87295" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/GFN_Thursday-Assetto_Corsa_Rally-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87295"&gt;&lt;em&gt;Every second counts.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;by 505 Game captures the pulse-pounding unpredictability and challenge of rally racing with uncompromising enthusiasm. Every stage plunges players into dynamic conditions ‚Äî from shifting surface grip and evolving weather to crowd reactions ‚Äî that demand sharp instincts and constant adaptation. With its precisely laser-scanned tracks, realistic environments and authentic co-driver pace notes delivered by real rally professionals, the game radiates motorsport passion and attention to detail.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Surviving Mars: Relaunched &lt;/i&gt;(New release on Steam, Nov. 10)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Possessor(s) &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Rue Valley &lt;/i&gt;(New release on Steam, Nov. 11)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Anno 117: Pax Romana &lt;/i&gt;(New release on Steam and Ubisoft, Nov. 13, GeForce RTX 5080-ready)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Assetto Corsa Rally &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;INAZUMA ELEVEN: Victory Road &lt;/i&gt;(New release on Steam, Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Songs of Silence&lt;/i&gt; (New release on Epic Games Store, Free on Nov. 13)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Call of Duty: Black Ops 7 &lt;/i&gt;(New release on Steam, Battle.net and Xbox, available on PC Game Pass, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Where Winds Meet &lt;/i&gt;(New release on Epic Games Store, Nov. 14)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Megabonk &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;R.E.P.O. &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;RV There Yet? &lt;/i&gt;(Steam)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Members who already own the Install-to-Play titles that are being added as Ready-to-Play titles this week can check out this support article for steps on how to re-add them to their libraries. All Steam Cloud Saves will be carried over.&lt;/p&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;What‚Äôs been the oddest or weirdest thing you've seen in the background of a game ‚Äì screenshots and clips are welcome!&lt;/p&gt;
&lt;p&gt;‚Äî üå©Ô∏è NVIDIA GeForce NOW (@NVIDIAGFN) November 10, 2025&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-call-of-duty-black-ops-7/</guid><pubDate>Thu, 13 Nov 2025 14:00:50 +0000</pubDate></item><item><title>Google is rolling out conversational shopping‚Äîand ads‚Äîin AI Mode search (AI ‚Äì Ars Technica)</title><link>https://arstechnica.com/google/2025/11/google-rolling-out-conversational-shopping-and-ads-in-ai-mode-search/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Conversational shopping is Google‚Äôs first big swing at monetizing AI Mode search.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Mode banner" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-640x360.png" width="640" /&gt;
                  &lt;img alt="AI Mode banner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;In recent months, Google has promised to inject generative AI into the online shopping experience, and now it‚Äôs following through. The previously announced shopping features of AI Mode search are rolling out, and Gemini will also worm its way into Google‚Äôs forgotten Duplex automated phone call tech. It‚Äôs all coming in time for the holidays to allegedly make your gifting more convenient and also conveniently ensure that Google gets a piece of the action.&lt;/p&gt;
&lt;p&gt;At Google I/O in May, the company announced its intention to bring conversational shopping to AI Mode. According to Google, its enormous ‚ÄúShopping Graph‚Äù or retailer data means its AI is uniquely positioned to deliver useful suggestions. In the coming weeks, users in the US will be able to ask AI Mode complex questions about what to buy, and it will deliver suggestions, guides, tables, and other generated content to help you decide. And since this is gen AI, it comes with the usual disclaimers about possible mistakes.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Visuals.mp4?_=1" type="video/mp4" /&gt;AI Mode shopping features.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Mode shopping features.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;You‚Äôre probably wondering where you‚Äôll see sponsored shopping content in these experiences. Google says some of the content that appears in AI Mode will be ads, just like if you look up shopping results in a traditional search. Shopping features are also coming to the Gemini app, but Google says it won‚Äôt have sponsored content in the results for the time being.&lt;/p&gt;
&lt;p&gt;Google is also releasing a feature called ‚Äúagentic checkout,‚Äù a term used only in passing when the company announced the feature alongside AI Mode shopping at I/O. Google is really leaning into the agentic angle now, though. The gist is you can set a price threshold for a product in search, and Google will let you know if the item reaches that price. That part isn‚Äôt new, but there‚Äôs now an AI twist. After getting the alert, you can authorize an automatic purchase with Google Pay. However, it‚Äôs currently only supported at a handful of retailers like Chewy, Wayfair, and some Shopify merchants. It‚Äôs not clear whether this qualifies as agentic anything, but it might save you some money regardless.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2127215 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="926" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Checkout.jpg" width="1643" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;AI Mode shopping and agentic checkout are beginning their rollout now, and Google says they will be available widely in time for the holiday shopping season.&lt;/p&gt;
&lt;h2&gt;Somehow, Duplex returned&lt;/h2&gt;
&lt;p&gt;Before the current AI craze, Google was fond of demoing Duplex, an Assistant-based AI designed to carry out real-world tasks on the phone. Google thought people would be willing to trust the AI to check business hours and make appointments, but it never gained much traction. The Duplex prompts slowly disappeared from Assistant over the years.&lt;/p&gt;
&lt;p&gt;Now, Duplex is back with what Google calls a ‚Äúbig Gemini model upgrade.‚Äù It won‚Äôt be making appointments for you, but Google does still plan to use the updated Duplex to allow you to call businesses. This time, Duplex is aimed at saving you from calling stores to check on stock availability. Instead, you can tell the robot what you want, and it will check for you.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-2" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Calling-Demo.mp4?_=2" type="video/mp4" /&gt;Duplex is back, baby.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Duplex is back, baby.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google says when you search for certain products near you, you‚Äôll see an option to ‚ÄúLet Google Call.‚Äù You‚Äôll have to indicate what specific product you want, and the robot will begin calling around. The robot will identify itself as such when it places calls, which will only happen during business hours and after a reasonable cooldown. If businesses get too annoyed, they‚Äôre liable to opt out of Duplex calls, which is still an option.&lt;/p&gt;
&lt;p&gt;Eventually, you‚Äôll get an email or text message with AI summaries of the calls that could help you decide where to go. These messages may also include local inventory data from other nearby stores based on Google‚Äôs Shopping Graph. That sounds like it could mean more sponsored links, but it‚Äôs unclear. This feature is beginning its rollout today in categories like toys, cosmetics, and electronics. Unsurprisingly, this one is also US-only.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Conversational shopping is Google‚Äôs first big swing at monetizing AI Mode search.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Mode banner" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-640x360.png" width="640" /&gt;
                  &lt;img alt="AI Mode banner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;In recent months, Google has promised to inject generative AI into the online shopping experience, and now it‚Äôs following through. The previously announced shopping features of AI Mode search are rolling out, and Gemini will also worm its way into Google‚Äôs forgotten Duplex automated phone call tech. It‚Äôs all coming in time for the holidays to allegedly make your gifting more convenient and also conveniently ensure that Google gets a piece of the action.&lt;/p&gt;
&lt;p&gt;At Google I/O in May, the company announced its intention to bring conversational shopping to AI Mode. According to Google, its enormous ‚ÄúShopping Graph‚Äù or retailer data means its AI is uniquely positioned to deliver useful suggestions. In the coming weeks, users in the US will be able to ask AI Mode complex questions about what to buy, and it will deliver suggestions, guides, tables, and other generated content to help you decide. And since this is gen AI, it comes with the usual disclaimers about possible mistakes.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-in-Search-Product-Visuals.mp4?_=1" type="video/mp4" /&gt;AI Mode shopping features.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      AI Mode shopping features.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;You‚Äôre probably wondering where you‚Äôll see sponsored shopping content in these experiences. Google says some of the content that appears in AI Mode will be ads, just like if you look up shopping results in a traditional search. Shopping features are also coming to the Gemini app, but Google says it won‚Äôt have sponsored content in the results for the time being.&lt;/p&gt;
&lt;p&gt;Google is also releasing a feature called ‚Äúagentic checkout,‚Äù a term used only in passing when the company announced the feature alongside AI Mode shopping at I/O. Google is really leaning into the agentic angle now, though. The gist is you can set a price threshold for a product in search, and Google will let you know if the item reaches that price. That part isn‚Äôt new, but there‚Äôs now an AI twist. After getting the alert, you can authorize an automatic purchase with Google Pay. However, it‚Äôs currently only supported at a handful of retailers like Chewy, Wayfair, and some Shopify merchants. It‚Äôs not clear whether this qualifies as agentic anything, but it might save you some money regardless.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2127215 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="926" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Checkout.jpg" width="1643" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;AI Mode shopping and agentic checkout are beginning their rollout now, and Google says they will be available widely in time for the holiday shopping season.&lt;/p&gt;
&lt;h2&gt;Somehow, Duplex returned&lt;/h2&gt;
&lt;p&gt;Before the current AI craze, Google was fond of demoing Duplex, an Assistant-based AI designed to carry out real-world tasks on the phone. Google thought people would be willing to trust the AI to check business hours and make appointments, but it never gained much traction. The Duplex prompts slowly disappeared from Assistant over the years.&lt;/p&gt;
&lt;p&gt;Now, Duplex is back with what Google calls a ‚Äúbig Gemini model upgrade.‚Äù It won‚Äôt be making appointments for you, but Google does still plan to use the updated Duplex to allow you to call businesses. This time, Duplex is aimed at saving you from calling stores to check on stock availability. Instead, you can tell the robot what you want, and it will check for you.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2127187-2" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Agentic-Calling-Demo.mp4?_=2" type="video/mp4" /&gt;Duplex is back, baby.&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
    &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Duplex is back, baby.

          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google says when you search for certain products near you, you‚Äôll see an option to ‚ÄúLet Google Call.‚Äù You‚Äôll have to indicate what specific product you want, and the robot will begin calling around. The robot will identify itself as such when it places calls, which will only happen during business hours and after a reasonable cooldown. If businesses get too annoyed, they‚Äôre liable to opt out of Duplex calls, which is still an option.&lt;/p&gt;
&lt;p&gt;Eventually, you‚Äôll get an email or text message with AI summaries of the calls that could help you decide where to go. These messages may also include local inventory data from other nearby stores based on Google‚Äôs Shopping Graph. That sounds like it could mean more sponsored links, but it‚Äôs unclear. This feature is beginning its rollout today in categories like toys, cosmetics, and electronics. Unsurprisingly, this one is also US-only.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/11/google-rolling-out-conversational-shopping-and-ads-in-ai-mode-search/</guid><pubDate>Thu, 13 Nov 2025 14:00:57 +0000</pubDate></item><item><title>Google DeepMind is using Gemini to train agents inside Goat Simulator 3 (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127921/google-deepmind-is-using-gemini-to-train-agents-inside-goat-simulator-3/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/Blog-header-sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds@2x.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Google DeepMind has built a new video-game-playing agent called SIMA 2 that can navigate and solve problems in a wide range of 3D virtual worlds. The company claims it‚Äôs a big step toward more general-purpose agents and better real-world robots.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind first demoed SIMA (which stands for ‚Äúscalable instructable multiworld agent‚Äù) last year. But SIMA 2 has been built on top of Gemini, the firm‚Äôs flagship large language model, which gives the agent a huge boost in capability.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The researchers claim that SIMA 2 can carry out a range of more complex tasks inside virtual worlds, figure out how to solve certain challenges by itself, and chat with its users. It can also improve itself by tackling harder tasks multiple times and learning through trial and error.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_InstructionChaining10.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;‚ÄúGames have been a driving force behind agent research for quite a while,‚Äù Joe Marino, a research scientist at Google DeepMind, said in a press conference this week. He noted that even a simple action in a game, such as lighting a lantern, can involve multiple steps: ‚ÄúIt‚Äôs a really complex set of tasks you need to solve to progress.‚Äù&lt;/p&gt; 
 &lt;p&gt;The ultimate aim is to develop next-generation agents that are able to follow instructions and carry out open-ended tasks inside more complex environments than a web browser. In the long run, Google DeepMind wants to use such agents to drive real-world robots. Marino claimed that the skills SIMA 2 has learned, such as navigating an environment, using tools, and collaborating with humans to solve problems, are essential building blocks for future robot companions.&lt;/p&gt;  &lt;p&gt;Unlike previous work on game-playing agents such as AlphaZero, which beat a Go grandmaster in 2016, or AlphaStar, which beat 99.8% of ranked human competition players at the video game StarCraft 2 in 2019, the idea behind SIMA is to train an agent to play an open-ended game without preset goals. Instead, the agent learns to carry out instructions given to it by people.&lt;/p&gt; 
 &lt;p&gt;Humans control SIMA 2 via text chat, by talking to it out loud, or by drawing on the game‚Äôs screen. The agent takes in a video game‚Äôs pixels frame by frame and figures out what actions it needs to take to carry out its tasks.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_REASONING_11.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Like its predecessor, SIMA 2 was trained on footage of humans playing eight commercial video games, including No Man‚Äôs Sky and Goat Simulator 3, as well as three virtual worlds created by the company. The agent learned to match keyboard and mouse inputs to actions.&lt;/p&gt;  &lt;p&gt;Hooked up to Gemini, the researchers claim, SIMA 2 is far better at following instructions (asking questions and providing updates as it goes) and figuring out for itself how to perform certain more complex tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind tested the agent inside environments it had never seen before. In one set of experiments, researchers asked Genie 3, the latest version of the firm‚Äôs world model, to produce environments from scratch and dropped SIMA 2 into them. They found that the agent was able to navigate and carry out instructions there.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The researchers also used Gemini to generate new tasks for SIMA 2. If the agent failed, at first Gemini generated tips that SIMA 2 took on board when it tried again. Repeating a task multiple times in this way often allowed SIMA 2 to improve by trial and error until it succeeded, Marino said.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Git gud&lt;/h3&gt;  &lt;p&gt;SIMA 2 is still an experiment. The agent struggles with complex tasks that require multiple steps and more time to complete. It also remembers only its most recent interactions (to make SIMA 2 more responsive, the team cut its long-term memory). It‚Äôs also still nowhere near as good as people at using a mouse and keyboard to interact with a virtual world.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_MultimodalPrompting03_v04.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Julian Togelius, an AI researcher at New York University who works on creativity and video games, thinks it‚Äôs an interesting result. Previous attempts at training a single system to play multiple games haven‚Äôt gone too well, he says. That‚Äôs because training models to control multiple games just by watching the screen isn‚Äôt easy: ‚ÄúPlaying in real time from visual input only is ‚Äòhard mode,‚Äô‚Äù he says.&lt;/p&gt;  &lt;p&gt;In particular, Togelius calls out GATO, a previous system from Google DeepMind, which‚Äîdespite being hyped at the time‚Äîcould not transfer skills across a significant number of virtual environments.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Still, he is open-minded about whether or not SIMA 2 could lead to better robots. ‚ÄúThe real world is both harder and easier than video games,‚Äù he says. It‚Äôs harder because you can‚Äôt just press A to open a door. At the same time, a robot in the real world will know exactly what its body can and can‚Äôt do at any time. That‚Äôs not the case in video games, where the rules inside each virtual world can differ.&lt;/p&gt;  &lt;p&gt;Others are more skeptical. Matthew Guzdial, an AI researcher at the University of Alberta, isn‚Äôt too surprised that SIMA 2 can play many different video games. He notes that most games have very similar keyboard and mouse controls: Learn one and you learn them all. ‚ÄúIf you put a game with weird input in front of it, I don‚Äôt think it‚Äôd be able to perform well,‚Äù he says.&lt;/p&gt;  &lt;p&gt;Guzdial also questions how much of what SIMA 2 has learned would really carry over to robots. ‚ÄúIt‚Äôs much harder to understand visuals from cameras in the real world compared to games, which are designed with easily parsable visuals for human players,‚Äù he says.&lt;/p&gt;  &lt;p&gt;Still, Marino and his colleagues hope to continue their work with Genie 3 to allow the agent to improve inside a kind of endless virtual training dojo, where Genie generates worlds for SIMA to learn in via trial and error guided by Gemini‚Äôs feedback. ‚ÄúWe‚Äôve kind of just scratched the surface of what‚Äôs possible,‚Äù he said at the press conference. &amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/Blog-header-sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds@2x.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Google DeepMind has built a new video-game-playing agent called SIMA 2 that can navigate and solve problems in a wide range of 3D virtual worlds. The company claims it‚Äôs a big step toward more general-purpose agents and better real-world robots.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind first demoed SIMA (which stands for ‚Äúscalable instructable multiworld agent‚Äù) last year. But SIMA 2 has been built on top of Gemini, the firm‚Äôs flagship large language model, which gives the agent a huge boost in capability.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The researchers claim that SIMA 2 can carry out a range of more complex tasks inside virtual worlds, figure out how to solve certain challenges by itself, and chat with its users. It can also improve itself by tackling harder tasks multiple times and learning through trial and error.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_InstructionChaining10.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;‚ÄúGames have been a driving force behind agent research for quite a while,‚Äù Joe Marino, a research scientist at Google DeepMind, said in a press conference this week. He noted that even a simple action in a game, such as lighting a lantern, can involve multiple steps: ‚ÄúIt‚Äôs a really complex set of tasks you need to solve to progress.‚Äù&lt;/p&gt; 
 &lt;p&gt;The ultimate aim is to develop next-generation agents that are able to follow instructions and carry out open-ended tasks inside more complex environments than a web browser. In the long run, Google DeepMind wants to use such agents to drive real-world robots. Marino claimed that the skills SIMA 2 has learned, such as navigating an environment, using tools, and collaborating with humans to solve problems, are essential building blocks for future robot companions.&lt;/p&gt;  &lt;p&gt;Unlike previous work on game-playing agents such as AlphaZero, which beat a Go grandmaster in 2016, or AlphaStar, which beat 99.8% of ranked human competition players at the video game StarCraft 2 in 2019, the idea behind SIMA is to train an agent to play an open-ended game without preset goals. Instead, the agent learns to carry out instructions given to it by people.&lt;/p&gt; 
 &lt;p&gt;Humans control SIMA 2 via text chat, by talking to it out loud, or by drawing on the game‚Äôs screen. The agent takes in a video game‚Äôs pixels frame by frame and figures out what actions it needs to take to carry out its tasks.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_REASONING_11.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Like its predecessor, SIMA 2 was trained on footage of humans playing eight commercial video games, including No Man‚Äôs Sky and Goat Simulator 3, as well as three virtual worlds created by the company. The agent learned to match keyboard and mouse inputs to actions.&lt;/p&gt;  &lt;p&gt;Hooked up to Gemini, the researchers claim, SIMA 2 is far better at following instructions (asking questions and providing updates as it goes) and figuring out for itself how to perform certain more complex tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Google DeepMind tested the agent inside environments it had never seen before. In one set of experiments, researchers asked Genie 3, the latest version of the firm‚Äôs world model, to produce environments from scratch and dropped SIMA 2 into them. They found that the agent was able to navigate and carry out instructions there.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The researchers also used Gemini to generate new tasks for SIMA 2. If the agent failed, at first Gemini generated tips that SIMA 2 took on board when it tried again. Repeating a task multiple times in this way often allowed SIMA 2 to improve by trial and error until it succeeded, Marino said.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Git gud&lt;/h3&gt;  &lt;p&gt;SIMA 2 is still an experiment. The agent struggles with complex tasks that require multiple steps and more time to complete. It also remembers only its most recent interactions (to make SIMA 2 more responsive, the team cut its long-term memory). It‚Äôs also still nowhere near as good as people at using a mouse and keyboard to interact with a virtual world.&lt;/p&gt;  &lt;figure class="wp-block-video"&gt;&lt;video controls="controls" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/SIMA2_MultimodalPrompting03_v04.mp4"&gt;&lt;/video&gt;&lt;/figure&gt;  &lt;p&gt;Julian Togelius, an AI researcher at New York University who works on creativity and video games, thinks it‚Äôs an interesting result. Previous attempts at training a single system to play multiple games haven‚Äôt gone too well, he says. That‚Äôs because training models to control multiple games just by watching the screen isn‚Äôt easy: ‚ÄúPlaying in real time from visual input only is ‚Äòhard mode,‚Äô‚Äù he says.&lt;/p&gt;  &lt;p&gt;In particular, Togelius calls out GATO, a previous system from Google DeepMind, which‚Äîdespite being hyped at the time‚Äîcould not transfer skills across a significant number of virtual environments.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Still, he is open-minded about whether or not SIMA 2 could lead to better robots. ‚ÄúThe real world is both harder and easier than video games,‚Äù he says. It‚Äôs harder because you can‚Äôt just press A to open a door. At the same time, a robot in the real world will know exactly what its body can and can‚Äôt do at any time. That‚Äôs not the case in video games, where the rules inside each virtual world can differ.&lt;/p&gt;  &lt;p&gt;Others are more skeptical. Matthew Guzdial, an AI researcher at the University of Alberta, isn‚Äôt too surprised that SIMA 2 can play many different video games. He notes that most games have very similar keyboard and mouse controls: Learn one and you learn them all. ‚ÄúIf you put a game with weird input in front of it, I don‚Äôt think it‚Äôd be able to perform well,‚Äù he says.&lt;/p&gt;  &lt;p&gt;Guzdial also questions how much of what SIMA 2 has learned would really carry over to robots. ‚ÄúIt‚Äôs much harder to understand visuals from cameras in the real world compared to games, which are designed with easily parsable visuals for human players,‚Äù he says.&lt;/p&gt;  &lt;p&gt;Still, Marino and his colleagues hope to continue their work with Genie 3 to allow the agent to improve inside a kind of endless virtual training dojo, where Genie generates worlds for SIMA to learn in via trial and error guided by Gemini‚Äôs feedback. ‚ÄúWe‚Äôve kind of just scratched the surface of what‚Äôs possible,‚Äù he said at the press conference. &amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127921/google-deepmind-is-using-gemini-to-train-agents-inside-goat-simulator-3/</guid><pubDate>Thu, 13 Nov 2025 15:00:00 +0000</pubDate></item><item><title>Teen founders raise $6M to reinvent pesticides using AI ‚Äî and convince Paul Graham to join in (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/teen-founders-raise-6m-to-reinvent-pesticides-using-ai-and-convince-paul-graham-to-join-in/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two teenage founders walked into Y Combinator co-founder Paul Graham‚Äôs backyard with an idea no one in agriculture seemed to want ‚Äî an AI model to help design better pesticides. By the time they left, they had a new business model, a new company, and eventually, Graham‚Äôs backing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, that reimagined company ‚Äî Bindwell ‚Äî has raised $6 million in a seed round, co-led by General Catalyst and A Capital, with a personal check from Graham himself. Rather than selling AI tools to legacy agrochemical giants, the startup is using its own models to design new pesticide molecules in-house and license the IP directly ‚Äî a shift in strategy aimed at modernizing a legacy industry still dominated by decades-old chemistry.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pesticide use in agriculture has doubled over the last three decades, yet up to 40% of global crop production is still lost to pests and diseases every year, per the UN Food and Agriculture Organization. As pests evolve and develop resistance, farmers are forced to use increasing amounts of chemicals just to maintain the same yields ‚Äî a cycle that damages ecosystems and accelerates resistance even further. Regulatory pressure is mounting, but most agrochemical companies still rely on tweaking legacy compounds. Bindwell is betting that AI can break the cycle by discovering entirely new, more targeted molecules ‚Äî ones designed from scratch for modern challenges.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2024 by Tyler Rose, 18, and Navvye Anand, 19, Bindwell adapts AI-led drug discovery techniques to agriculture, with the goal of speeding up how new pesticide molecules are identified and tested.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell began as a research project in late 2023, when Rose and Anand were students at the Wolfram Summer Research Program. They initially focused on a drug discovery AI model called PLAPT, which involved binding affinity prediction ‚Äî work that was later cited in a Nature Scientific Reports paper on cancer therapeutics. In 2024, they began exploring how the same approach could be applied to pesticides.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both founders had personal exposure to the problem. Rose learned about the challenges of pest control from his aunt, who farms in China. Anand, who has roots in Punjab, saw firsthand how limited pesticide options affected crop yields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúAgriculture has been in our mind space,‚Äù said Rose in an interview. ‚ÄúThat led to the realization that we can use the exact same technology that has been successful in drug discovery. We can bring that over to pesticide discovery, because the biochemistry is the same, but pesticides are such a big problem, and I feel like it‚Äôs not very focused on by most people.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3067512" height="1280" src="https://techcrunch.com/wp-content/uploads/2025/11/bindwell-co-founders_88d28d.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Bindwell co-founders Tyler Rose (Left) and Navvye Anand (Right)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bindwell&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rose and Anand entered Y Combinator‚Äôs Winter 2025 batch with plans to build AI models and sell their access to major agrochemical companies. But they did not find traction ‚Äî most industry players were reluctant to adopt AI as a core part of pesticide discovery. Midway through the program, they were invited to Paul Graham‚Äôs home, where they spoke with him for about 45 minutes on the back patio. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After hearing about their challenges, Graham suggested a different approach: Rather than selling tools, they could use their own models to discover new pesticide molecules themselves. That conversation marked the beginning of Bindwell‚Äôs current direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe founders [of Bindwell] will probably do alright,‚Äù he later posted on X. ‚ÄúThey‚Äôre smart and have a good idea.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Bindwell has developed its own AI suite designed to reduce hallucination ‚Äî a common issue where models produce unreliable or unsupported outputs. The software includes Foldwell, a structure prediction model, which is a custom diffusion system inspired by DeepMind‚Äôs AlphaFold, used to identify target protein structures. It also includes PLAPT, an open source protein-ligand interaction model capable of scanning every known synthesized compound in under six hours, and APPT, a protein-protein interaction model for biopesticide screening, reported to outperform existing tools by 1.7√ó on the Affinity Benchmark v5.5. Moreover, the suite incorporates an uncertainty quantification system that flags when results are trustworthy and when more data is needed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúSince we‚Äôre not selling AI models, we‚Äôre not competing with companies that sell models,‚Äù Rose told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, Bindwell‚Äôs models can analyze ‚Äúbillions‚Äù of molecules, the startup said, and deliver four times faster performance than DeepMind‚Äôs AlphaFold 3.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe way most pesticides are discovered right now is not target-based,‚Äù said Rose. ‚ÄúEntomologists and chemists suggest different compounds, then test them on insects. You often need to synthesize and test thousands of chemicals, which is expensive just to check for efficacy. With our AI models, you‚Äôre able to simplify the problem down to a single protein.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI helps identify proteins that are unique to a specific pest but absent in humans, beneficial insects, or aquatic organisms like water fleas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOnce you find those proteins, you can design something that binds to them and stops them from working,‚Äù Rose said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell is currently testing the efficacy of its AI-generated molecules at its lab in San Carlos. It is also working with a third-party partner to further validate the models, though Rose declined to share details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rose said the startup is in early discussions with several global agrochemical firms, with its first partnership deal expected to close soon. ‚ÄúA year from now, we want to be entering into our licensing deals with some of these companies,‚Äù he said. Bindwell has also begun talks with stakeholders in India and China to conduct field tests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup currently has a team of four, and also works with external contractors for molecule synthesis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell‚Äôs seed round also included participation from SV Angel, alongside Graham. Prior to joining Y Combinator‚Äôs Winter 2025 batch, the startup raised a pre-seed round from Character Capital.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two teenage founders walked into Y Combinator co-founder Paul Graham‚Äôs backyard with an idea no one in agriculture seemed to want ‚Äî an AI model to help design better pesticides. By the time they left, they had a new business model, a new company, and eventually, Graham‚Äôs backing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, that reimagined company ‚Äî Bindwell ‚Äî has raised $6 million in a seed round, co-led by General Catalyst and A Capital, with a personal check from Graham himself. Rather than selling AI tools to legacy agrochemical giants, the startup is using its own models to design new pesticide molecules in-house and license the IP directly ‚Äî a shift in strategy aimed at modernizing a legacy industry still dominated by decades-old chemistry.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Pesticide use in agriculture has doubled over the last three decades, yet up to 40% of global crop production is still lost to pests and diseases every year, per the UN Food and Agriculture Organization. As pests evolve and develop resistance, farmers are forced to use increasing amounts of chemicals just to maintain the same yields ‚Äî a cycle that damages ecosystems and accelerates resistance even further. Regulatory pressure is mounting, but most agrochemical companies still rely on tweaking legacy compounds. Bindwell is betting that AI can break the cycle by discovering entirely new, more targeted molecules ‚Äî ones designed from scratch for modern challenges.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2024 by Tyler Rose, 18, and Navvye Anand, 19, Bindwell adapts AI-led drug discovery techniques to agriculture, with the goal of speeding up how new pesticide molecules are identified and tested.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell began as a research project in late 2023, when Rose and Anand were students at the Wolfram Summer Research Program. They initially focused on a drug discovery AI model called PLAPT, which involved binding affinity prediction ‚Äî work that was later cited in a Nature Scientific Reports paper on cancer therapeutics. In 2024, they began exploring how the same approach could be applied to pesticides.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both founders had personal exposure to the problem. Rose learned about the challenges of pest control from his aunt, who farms in China. Anand, who has roots in Punjab, saw firsthand how limited pesticide options affected crop yields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúAgriculture has been in our mind space,‚Äù said Rose in an interview. ‚ÄúThat led to the realization that we can use the exact same technology that has been successful in drug discovery. We can bring that over to pesticide discovery, because the biochemistry is the same, but pesticides are such a big problem, and I feel like it‚Äôs not very focused on by most people.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3067512" height="1280" src="https://techcrunch.com/wp-content/uploads/2025/11/bindwell-co-founders_88d28d.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Bindwell co-founders Tyler Rose (Left) and Navvye Anand (Right)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bindwell&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rose and Anand entered Y Combinator‚Äôs Winter 2025 batch with plans to build AI models and sell their access to major agrochemical companies. But they did not find traction ‚Äî most industry players were reluctant to adopt AI as a core part of pesticide discovery. Midway through the program, they were invited to Paul Graham‚Äôs home, where they spoke with him for about 45 minutes on the back patio. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After hearing about their challenges, Graham suggested a different approach: Rather than selling tools, they could use their own models to discover new pesticide molecules themselves. That conversation marked the beginning of Bindwell‚Äôs current direction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe founders [of Bindwell] will probably do alright,‚Äù he later posted on X. ‚ÄúThey‚Äôre smart and have a good idea.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Bindwell has developed its own AI suite designed to reduce hallucination ‚Äî a common issue where models produce unreliable or unsupported outputs. The software includes Foldwell, a structure prediction model, which is a custom diffusion system inspired by DeepMind‚Äôs AlphaFold, used to identify target protein structures. It also includes PLAPT, an open source protein-ligand interaction model capable of scanning every known synthesized compound in under six hours, and APPT, a protein-protein interaction model for biopesticide screening, reported to outperform existing tools by 1.7√ó on the Affinity Benchmark v5.5. Moreover, the suite incorporates an uncertainty quantification system that flags when results are trustworthy and when more data is needed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúSince we‚Äôre not selling AI models, we‚Äôre not competing with companies that sell models,‚Äù Rose told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, Bindwell‚Äôs models can analyze ‚Äúbillions‚Äù of molecules, the startup said, and deliver four times faster performance than DeepMind‚Äôs AlphaFold 3.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúThe way most pesticides are discovered right now is not target-based,‚Äù said Rose. ‚ÄúEntomologists and chemists suggest different compounds, then test them on insects. You often need to synthesize and test thousands of chemicals, which is expensive just to check for efficacy. With our AI models, you‚Äôre able to simplify the problem down to a single protein.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI helps identify proteins that are unique to a specific pest but absent in humans, beneficial insects, or aquatic organisms like water fleas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOnce you find those proteins, you can design something that binds to them and stops them from working,‚Äù Rose said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell is currently testing the efficacy of its AI-generated molecules at its lab in San Carlos. It is also working with a third-party partner to further validate the models, though Rose declined to share details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rose said the startup is in early discussions with several global agrochemical firms, with its first partnership deal expected to close soon. ‚ÄúA year from now, we want to be entering into our licensing deals with some of these companies,‚Äù he said. Bindwell has also begun talks with stakeholders in India and China to conduct field tests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup currently has a team of four, and also works with external contractors for molecule synthesis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bindwell‚Äôs seed round also included participation from SV Angel, alongside Graham. Prior to joining Y Combinator‚Äôs Winter 2025 batch, the startup raised a pre-seed round from Character Capital.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/teen-founders-raise-6m-to-reinvent-pesticides-using-ai-and-convince-paul-graham-to-join-in/</guid><pubDate>Thu, 13 Nov 2025 15:00:00 +0000</pubDate></item><item><title>Inside LinkedIn‚Äôs generative AI cookbook: How it scaled people search to 1.3 billion users (AI | VentureBeat)</title><link>https://venturebeat.com/ai/inside-linkedins-generative-ai-cookbook-how-it-scaled-people-search-to-1-3</link><description>[unable to retrieve full-text content]&lt;p&gt;LinkedIn is launching its new AI-powered people search this week, after what seems like a very long wait for what should have been a natural offering for generative AI.&lt;/p&gt;&lt;p&gt;It comes a full three years after the launch of ChatGPT and six months after LinkedIn launched its AI job search offering. For technical leaders, this timeline illustrates a key enterprise lesson: Deploying generative AI in real enterprise settings is challenging, especially at a scale of 1.3 billion users. It‚Äôs a slow, brutal process of pragmatic optimization.&lt;/p&gt;&lt;p&gt;The following account is based on several exclusive interviews with the LinkedIn product and engineering team behind the launch.&lt;/p&gt;&lt;p&gt;First, here‚Äôs how the product works: A user can now type a natural language query like, &lt;b&gt;&amp;quot;Who is knowledgeable about curing cancer?&amp;quot; &lt;/b&gt;into LinkedIn‚Äôs search bar.&lt;/p&gt;&lt;p&gt;LinkedIn&amp;#x27;s old search, based on keywords, would have been stumped. It would have looked only for references to &amp;quot;cancer&amp;quot;. If a user wanted to get sophisticated, they would have had to run separate, rigid keyword searches for &amp;quot;cancer&amp;quot; and then &amp;quot;oncology&amp;quot; and manually try to piece the results together.&lt;/p&gt;&lt;p&gt;The new AI-powered system, however, understands the &lt;i&gt;intent&lt;/i&gt; of the search because the LLM under the hood grasps semantic meaning. It recognizes, for example, that &amp;quot;cancer&amp;quot; is conceptually related to &amp;quot;oncology&amp;quot; and even less directly, to &amp;quot;genomics research.&amp;quot; As a result, it surfaces a far more relevant list of people, including oncology leaders and researchers, even if their profiles don&amp;#x27;t use the exact word &amp;quot;cancer.&amp;quot;&lt;/p&gt;&lt;p&gt;The system also balances this relevance with &lt;i&gt;usefulness&lt;/i&gt;. Instead of just showing the world&amp;#x27;s top oncologist (who might be an unreachable third-degree connection), it will also weigh who in your immediate network ‚Äî like a first-degree connection ‚Äî is &amp;quot;pretty relevant&amp;quot; and can serve as a crucial bridge to that expert.&lt;/p&gt;&lt;p&gt;See the video below for an example.&lt;/p&gt;&lt;p&gt;Arguably, though, the more important lesson for enterprise practitioners is the &amp;quot;cookbook&amp;quot; LinkedIn has developed: a replicable, multi-stage pipeline of distillation, co-design, and relentless optimization. LinkedIn had to perfect this on one product before attempting it on another.&lt;/p&gt;&lt;p&gt;&amp;quot;Don&amp;#x27;t try to do too much all at once,&amp;quot; writes Wenjing Zhang, LinkedIn&amp;#x27;s VP of Engineering, in a¬† post about the product launch, and who also spoke with VentureBeat last week in an interview. She notes that an earlier &amp;quot;sprawling ambition&amp;quot; to build a unified system for all of LinkedIn&amp;#x27;s products &amp;quot;stalled progress.&amp;quot;&lt;/p&gt;&lt;p&gt;Instead, LinkedIn focused on winning one vertical first. The success of its previously launched AI Job Search ‚Äî which led to job seekers without a four-year degree being &lt;b&gt;10% more likely to get hired&lt;/b&gt;, according to VP of Product Engineering Erran Berger ‚Äî provided the blueprint.&lt;/p&gt;&lt;p&gt;Now, the company is applying that blueprint to a far larger challenge. &amp;quot;It&amp;#x27;s one thing to be able to do this across tens of millions of jobs,&amp;quot; Berger told VentureBeat. &amp;quot;It&amp;#x27;s another thing to do this across north of a billion members.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprise AI builders, LinkedIn&amp;#x27;s journey provides a technical playbook for what it &lt;i&gt;actually&lt;/i&gt; takes to move from a successful pilot to a billion-user-scale product.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The new challenge: a 1.3 billion-member graph&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The job search product created a robust recipe that the new people search product could build upon, Berger explained.¬†&lt;/p&gt;&lt;p&gt;The recipe started with with a &amp;quot;golden data set&amp;quot; of just a few hundred to a thousand real query-profile pairs, meticulously scored against a detailed 20- to 30-page &amp;quot;product policy&amp;quot; document. To scale this for training, LinkedIn used this small golden set to prompt a large foundation model to generate a massive volume of &lt;i&gt;synthetic&lt;/i&gt; training data. This synthetic data was used to train a &lt;b&gt;7-billion-parameter&lt;/b&gt; &amp;quot;Product Policy&amp;quot; model ‚Äî a high-fidelity judge of relevance that was too slow for live production but perfect for teaching smaller models.&lt;/p&gt;&lt;p&gt;However, the team hit a wall early on. For six to nine months, they struggled to train a single model that could balance strict policy adherence (relevance) against user engagement signals. The &amp;quot;aha moment&amp;quot; came when they realized they needed to break the problem down. They distilled the 7B policy model into a &lt;b&gt;1.7B teacher model&lt;/b&gt; focused solely on relevance. They then paired it with separate teacher models trained to predict specific member actions, such as job applications for the jobs product, or connecting and following for people search. This &amp;quot;multi-teacher&amp;quot; ensemble produced soft probability scores that the final student model learned to mimic via KL divergence loss.&lt;/p&gt;&lt;p&gt;The resulting architecture operates as a two-stage pipeline. First, a larger &lt;b&gt;8B parameter model&lt;/b&gt; handles broad retrieval, casting a wide net to pull candidates from the graph. Then, the highly distilled student model takes over for fine-grained ranking. While the job search product successfully deployed a &lt;b&gt;0.6B (600-million)&lt;/b&gt; parameter student, the new people search product required even more aggressive compression. As Zhang notes, the team pruned their new student model from 440M down to just &lt;b&gt;220M parameters&lt;/b&gt;, achieving the necessary speed for 1.3 billion users with less than 1% relevance loss.&lt;/p&gt;&lt;p&gt;But applying this to people search broke the old architecture. The new problem included not just &lt;i&gt;ranking&lt;/i&gt; but also &lt;i&gt;retrieval&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;‚ÄúA billion records,&amp;quot; Berger said, is a &amp;quot;different beast.&amp;quot;&lt;/p&gt;&lt;p&gt;The team‚Äôs prior retrieval stack was built on CPUs. To handle the new scale and the latency demands of a &amp;quot;snappy&amp;quot; search experience, the team had to move its indexing to &lt;b&gt;GPU-based infrastructure&lt;/b&gt;. This was a foundational architectural shift that the job search product did not require.&lt;/p&gt;&lt;p&gt;Organizationally, LinkedIn benefited from multiple approaches. For a time, LinkedIn had two separate teams &lt;!-- --&gt;‚Äî &lt;!-- --&gt;job search and people search &lt;!-- --&gt;‚Äî &lt;!-- --&gt;attempting to solve the problem in parallel. But once the job search team achieved its breakthrough using the policy-driven distillation method, Berger and his leadership team intervened. They brought over the architects of the job search win &lt;!-- --&gt;‚Äî  &lt;!-- --&gt;product lead Rohan Rajiv and engineering lead Wenjing Zhang &lt;!-- --&gt;‚Äî &lt;!-- --&gt;to transplant their &amp;#x27;cookbook&amp;#x27; directly to the new domain.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Distilling for a 10x throughput gain&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With the retrieval problem solved, the team faced the ranking and efficiency challenge. This is where the cookbook was adapted with new, aggressive optimization techniques.&lt;/p&gt;&lt;p&gt;Zhang‚Äôs technical post &lt;b&gt;(I‚Äôll insert the link once it goes live)&lt;/b&gt; provides the specific details our audience of AI engineers will appreciate. One of the more significant optimizations was input size.&lt;/p&gt;&lt;p&gt;To feed the model, the team trained &lt;i&gt;another&lt;/i&gt; LLM with reinforcement learning (RL) for a single purpose: to summarize the input context. This &amp;quot;summarizer&amp;quot; model was able to reduce the model&amp;#x27;s input size by &lt;b&gt;20-fold&lt;/b&gt; with minimal information loss.&lt;/p&gt;&lt;p&gt;The combined result of the 220M-parameter model and the 20x input reduction? A &lt;b&gt;10x increase in ranking throughput&lt;/b&gt;, allowing the team to serve the model efficiently to its massive user base.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pragmatism over hype: building tools, not agents&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Throughout our discussions, Berger was adamant about something else that might catch peoples‚Äô attention: The real value for enterprises today lies in perfecting recommender systems, not in chasing &amp;quot;agentic hype.&amp;quot; He also refused to talk about the specific models that the company used for the searches, suggesting it almost doesn&amp;#x27;t matter. The company selects models based on which one it finds the most efficient for the task.&lt;/p&gt;&lt;p&gt;The new AI-powered people search is a manifestation of Berger‚Äôs philosophy that it‚Äôs best to optimize the recommender system first. The architecture includes a new &amp;quot;intelligent query routing layer,&amp;quot; as Berger explained, that itself is LLM-powered. This router pragmatically decides if a user&amp;#x27;s query ‚Äî like &amp;quot;trust expert&amp;quot; ‚Äî should go to the new semantic, natural-language stack or to the old, reliable lexical search.&lt;/p&gt;&lt;p&gt;This entire, complex system is designed to be a &amp;quot;tool&amp;quot; that a &lt;i&gt;future&lt;/i&gt; agent will use, not the agent itself.&lt;/p&gt;&lt;p&gt;&amp;quot;Agentic products are¬†only as good as the tools that they use to accomplish tasks for people,&amp;quot; Berger said. &amp;quot;You can have the world&amp;#x27;s best reasoning model, and if you&amp;#x27;re trying to use an agent to do people search but the people search engine is not very good, you&amp;#x27;re not going to be able to deliver.&amp;quot;¬†&lt;/p&gt;&lt;p&gt;Now that the people search is available, Berger suggested that one day the company will be offering agents to use it. But he didn‚Äôt provide details on timing. He also said the recipe used for job and people search will be spread across the company‚Äôs other products.&lt;/p&gt;&lt;p&gt;For enterprises building their own AI roadmaps, LinkedIn&amp;#x27;s playbook is clear:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Be pragmatic:&lt;/b&gt; Don&amp;#x27;t try to boil the ocean. Win one vertical, even if it takes 18 months.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Codify the &amp;quot;cookbook&amp;quot;:&lt;/b&gt; Turn that win into a repeatable process (policy docs, distillation pipelines, co-design).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Optimize relentlessly:&lt;/b&gt; The real 10x gains come &lt;i&gt;after&lt;/i&gt; the initial model, in pruning, distillation, and creative optimizations like an RL-trained summarizer.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;LinkedIn&amp;#x27;s journey shows that for real-world enterprise AI, emphasis on specific models or cool agentic systems should take a back seat. The durable, strategic advantage comes from mastering the &lt;i&gt;pipeline&lt;/i&gt; ‚Äî the &amp;#x27;AI-native&amp;#x27; cookbook of co-design, distillation, and ruthless optimization.&lt;/p&gt;&lt;p&gt;&lt;i&gt;(Editor&amp;#x27;s note: We will be publishing a full-length podcast with LinkedIn&amp;#x27;s Erran Berger, which will dive deeper into these technical details, on the VentureBeat podcast feed soon.)&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;LinkedIn is launching its new AI-powered people search this week, after what seems like a very long wait for what should have been a natural offering for generative AI.&lt;/p&gt;&lt;p&gt;It comes a full three years after the launch of ChatGPT and six months after LinkedIn launched its AI job search offering. For technical leaders, this timeline illustrates a key enterprise lesson: Deploying generative AI in real enterprise settings is challenging, especially at a scale of 1.3 billion users. It‚Äôs a slow, brutal process of pragmatic optimization.&lt;/p&gt;&lt;p&gt;The following account is based on several exclusive interviews with the LinkedIn product and engineering team behind the launch.&lt;/p&gt;&lt;p&gt;First, here‚Äôs how the product works: A user can now type a natural language query like, &lt;b&gt;&amp;quot;Who is knowledgeable about curing cancer?&amp;quot; &lt;/b&gt;into LinkedIn‚Äôs search bar.&lt;/p&gt;&lt;p&gt;LinkedIn&amp;#x27;s old search, based on keywords, would have been stumped. It would have looked only for references to &amp;quot;cancer&amp;quot;. If a user wanted to get sophisticated, they would have had to run separate, rigid keyword searches for &amp;quot;cancer&amp;quot; and then &amp;quot;oncology&amp;quot; and manually try to piece the results together.&lt;/p&gt;&lt;p&gt;The new AI-powered system, however, understands the &lt;i&gt;intent&lt;/i&gt; of the search because the LLM under the hood grasps semantic meaning. It recognizes, for example, that &amp;quot;cancer&amp;quot; is conceptually related to &amp;quot;oncology&amp;quot; and even less directly, to &amp;quot;genomics research.&amp;quot; As a result, it surfaces a far more relevant list of people, including oncology leaders and researchers, even if their profiles don&amp;#x27;t use the exact word &amp;quot;cancer.&amp;quot;&lt;/p&gt;&lt;p&gt;The system also balances this relevance with &lt;i&gt;usefulness&lt;/i&gt;. Instead of just showing the world&amp;#x27;s top oncologist (who might be an unreachable third-degree connection), it will also weigh who in your immediate network ‚Äî like a first-degree connection ‚Äî is &amp;quot;pretty relevant&amp;quot; and can serve as a crucial bridge to that expert.&lt;/p&gt;&lt;p&gt;See the video below for an example.&lt;/p&gt;&lt;p&gt;Arguably, though, the more important lesson for enterprise practitioners is the &amp;quot;cookbook&amp;quot; LinkedIn has developed: a replicable, multi-stage pipeline of distillation, co-design, and relentless optimization. LinkedIn had to perfect this on one product before attempting it on another.&lt;/p&gt;&lt;p&gt;&amp;quot;Don&amp;#x27;t try to do too much all at once,&amp;quot; writes Wenjing Zhang, LinkedIn&amp;#x27;s VP of Engineering, in a¬† post about the product launch, and who also spoke with VentureBeat last week in an interview. She notes that an earlier &amp;quot;sprawling ambition&amp;quot; to build a unified system for all of LinkedIn&amp;#x27;s products &amp;quot;stalled progress.&amp;quot;&lt;/p&gt;&lt;p&gt;Instead, LinkedIn focused on winning one vertical first. The success of its previously launched AI Job Search ‚Äî which led to job seekers without a four-year degree being &lt;b&gt;10% more likely to get hired&lt;/b&gt;, according to VP of Product Engineering Erran Berger ‚Äî provided the blueprint.&lt;/p&gt;&lt;p&gt;Now, the company is applying that blueprint to a far larger challenge. &amp;quot;It&amp;#x27;s one thing to be able to do this across tens of millions of jobs,&amp;quot; Berger told VentureBeat. &amp;quot;It&amp;#x27;s another thing to do this across north of a billion members.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprise AI builders, LinkedIn&amp;#x27;s journey provides a technical playbook for what it &lt;i&gt;actually&lt;/i&gt; takes to move from a successful pilot to a billion-user-scale product.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The new challenge: a 1.3 billion-member graph&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The job search product created a robust recipe that the new people search product could build upon, Berger explained.¬†&lt;/p&gt;&lt;p&gt;The recipe started with with a &amp;quot;golden data set&amp;quot; of just a few hundred to a thousand real query-profile pairs, meticulously scored against a detailed 20- to 30-page &amp;quot;product policy&amp;quot; document. To scale this for training, LinkedIn used this small golden set to prompt a large foundation model to generate a massive volume of &lt;i&gt;synthetic&lt;/i&gt; training data. This synthetic data was used to train a &lt;b&gt;7-billion-parameter&lt;/b&gt; &amp;quot;Product Policy&amp;quot; model ‚Äî a high-fidelity judge of relevance that was too slow for live production but perfect for teaching smaller models.&lt;/p&gt;&lt;p&gt;However, the team hit a wall early on. For six to nine months, they struggled to train a single model that could balance strict policy adherence (relevance) against user engagement signals. The &amp;quot;aha moment&amp;quot; came when they realized they needed to break the problem down. They distilled the 7B policy model into a &lt;b&gt;1.7B teacher model&lt;/b&gt; focused solely on relevance. They then paired it with separate teacher models trained to predict specific member actions, such as job applications for the jobs product, or connecting and following for people search. This &amp;quot;multi-teacher&amp;quot; ensemble produced soft probability scores that the final student model learned to mimic via KL divergence loss.&lt;/p&gt;&lt;p&gt;The resulting architecture operates as a two-stage pipeline. First, a larger &lt;b&gt;8B parameter model&lt;/b&gt; handles broad retrieval, casting a wide net to pull candidates from the graph. Then, the highly distilled student model takes over for fine-grained ranking. While the job search product successfully deployed a &lt;b&gt;0.6B (600-million)&lt;/b&gt; parameter student, the new people search product required even more aggressive compression. As Zhang notes, the team pruned their new student model from 440M down to just &lt;b&gt;220M parameters&lt;/b&gt;, achieving the necessary speed for 1.3 billion users with less than 1% relevance loss.&lt;/p&gt;&lt;p&gt;But applying this to people search broke the old architecture. The new problem included not just &lt;i&gt;ranking&lt;/i&gt; but also &lt;i&gt;retrieval&lt;/i&gt;.&lt;/p&gt;&lt;p&gt;‚ÄúA billion records,&amp;quot; Berger said, is a &amp;quot;different beast.&amp;quot;&lt;/p&gt;&lt;p&gt;The team‚Äôs prior retrieval stack was built on CPUs. To handle the new scale and the latency demands of a &amp;quot;snappy&amp;quot; search experience, the team had to move its indexing to &lt;b&gt;GPU-based infrastructure&lt;/b&gt;. This was a foundational architectural shift that the job search product did not require.&lt;/p&gt;&lt;p&gt;Organizationally, LinkedIn benefited from multiple approaches. For a time, LinkedIn had two separate teams &lt;!-- --&gt;‚Äî &lt;!-- --&gt;job search and people search &lt;!-- --&gt;‚Äî &lt;!-- --&gt;attempting to solve the problem in parallel. But once the job search team achieved its breakthrough using the policy-driven distillation method, Berger and his leadership team intervened. They brought over the architects of the job search win &lt;!-- --&gt;‚Äî  &lt;!-- --&gt;product lead Rohan Rajiv and engineering lead Wenjing Zhang &lt;!-- --&gt;‚Äî &lt;!-- --&gt;to transplant their &amp;#x27;cookbook&amp;#x27; directly to the new domain.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Distilling for a 10x throughput gain&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With the retrieval problem solved, the team faced the ranking and efficiency challenge. This is where the cookbook was adapted with new, aggressive optimization techniques.&lt;/p&gt;&lt;p&gt;Zhang‚Äôs technical post &lt;b&gt;(I‚Äôll insert the link once it goes live)&lt;/b&gt; provides the specific details our audience of AI engineers will appreciate. One of the more significant optimizations was input size.&lt;/p&gt;&lt;p&gt;To feed the model, the team trained &lt;i&gt;another&lt;/i&gt; LLM with reinforcement learning (RL) for a single purpose: to summarize the input context. This &amp;quot;summarizer&amp;quot; model was able to reduce the model&amp;#x27;s input size by &lt;b&gt;20-fold&lt;/b&gt; with minimal information loss.&lt;/p&gt;&lt;p&gt;The combined result of the 220M-parameter model and the 20x input reduction? A &lt;b&gt;10x increase in ranking throughput&lt;/b&gt;, allowing the team to serve the model efficiently to its massive user base.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pragmatism over hype: building tools, not agents&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Throughout our discussions, Berger was adamant about something else that might catch peoples‚Äô attention: The real value for enterprises today lies in perfecting recommender systems, not in chasing &amp;quot;agentic hype.&amp;quot; He also refused to talk about the specific models that the company used for the searches, suggesting it almost doesn&amp;#x27;t matter. The company selects models based on which one it finds the most efficient for the task.&lt;/p&gt;&lt;p&gt;The new AI-powered people search is a manifestation of Berger‚Äôs philosophy that it‚Äôs best to optimize the recommender system first. The architecture includes a new &amp;quot;intelligent query routing layer,&amp;quot; as Berger explained, that itself is LLM-powered. This router pragmatically decides if a user&amp;#x27;s query ‚Äî like &amp;quot;trust expert&amp;quot; ‚Äî should go to the new semantic, natural-language stack or to the old, reliable lexical search.&lt;/p&gt;&lt;p&gt;This entire, complex system is designed to be a &amp;quot;tool&amp;quot; that a &lt;i&gt;future&lt;/i&gt; agent will use, not the agent itself.&lt;/p&gt;&lt;p&gt;&amp;quot;Agentic products are¬†only as good as the tools that they use to accomplish tasks for people,&amp;quot; Berger said. &amp;quot;You can have the world&amp;#x27;s best reasoning model, and if you&amp;#x27;re trying to use an agent to do people search but the people search engine is not very good, you&amp;#x27;re not going to be able to deliver.&amp;quot;¬†&lt;/p&gt;&lt;p&gt;Now that the people search is available, Berger suggested that one day the company will be offering agents to use it. But he didn‚Äôt provide details on timing. He also said the recipe used for job and people search will be spread across the company‚Äôs other products.&lt;/p&gt;&lt;p&gt;For enterprises building their own AI roadmaps, LinkedIn&amp;#x27;s playbook is clear:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Be pragmatic:&lt;/b&gt; Don&amp;#x27;t try to boil the ocean. Win one vertical, even if it takes 18 months.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Codify the &amp;quot;cookbook&amp;quot;:&lt;/b&gt; Turn that win into a repeatable process (policy docs, distillation pipelines, co-design).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Optimize relentlessly:&lt;/b&gt; The real 10x gains come &lt;i&gt;after&lt;/i&gt; the initial model, in pruning, distillation, and creative optimizations like an RL-trained summarizer.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;LinkedIn&amp;#x27;s journey shows that for real-world enterprise AI, emphasis on specific models or cool agentic systems should take a back seat. The durable, strategic advantage comes from mastering the &lt;i&gt;pipeline&lt;/i&gt; ‚Äî the &amp;#x27;AI-native&amp;#x27; cookbook of co-design, distillation, and ruthless optimization.&lt;/p&gt;&lt;p&gt;&lt;i&gt;(Editor&amp;#x27;s note: We will be publishing a full-length podcast with LinkedIn&amp;#x27;s Erran Berger, which will dive deeper into these technical details, on the VentureBeat podcast feed soon.)&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/inside-linkedins-generative-ai-cookbook-how-it-scaled-people-search-to-1-3</guid><pubDate>Thu, 13 Nov 2025 16:00:00 +0000</pubDate></item><item><title>LinkedIn adds AI-powered search to help users find people (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/linkedin-adds-ai-powered-search-to-help-users-find-people/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For the last two years, LinkedIn has tried to infuse AI into different parts of its platforms, including ad copies, content creation, personalized digests, hiring assistance, job hunting advice, and learning. The company is now finally adding AI to one of the most-used parts of the site: search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the company released a job search tool for members in the U.S., allowing them to search for jobs using natural language queries. Now, the company is extending the feature to people search.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users can use queries like ‚ÄúFind me investors in the healthcare sector with FDA experience,‚Äù people who ‚Äúco-founded a productivity company and are based in NYC,‚Äù or ‚ÄúWho in my network can help me understand wireless networks.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067452" height="443" src="https://techcrunch.com/wp-content/uploads/2025/11/01D_Investors-with-FDA-experience-for-a-biotech-startup.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, LinkedIn‚Äôs search has been more complicated. You can type in a few words to find the right people or rely on many different LinkedIn filters in the hope of getting the right results. Plus, you also have to think about what kinds of words you might want to use to get the best out of the search system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWith lexical search, you have to know the exact title of the person, or you need to wrestle with filters to find the right person, maybe. And if you didn‚Äôt know the right combination, the right person remained undiscovered. The new AI-powered people search is designed to be the fastest path to the person who can help you the most,‚Äù Rohan Rajiv, senior director of product management at LinkedIn, told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said in its early testing, it has seen people use this to find others who can help them with their next job opportunity, expand their business, or boost their career prospects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Search has been one area where all internet platforms have been rushing to add AI. Seeing people gravitate toward chatbots like ChatGPT and Perplexity for answers, incumbent search engines like Google, Bing, Brave, and DuckDuckGo have added AI-powered answers. There are plenty of startups working on AI-powered people search as well. Reddit has also leaned heavily into AI-powered search and locked down its platform‚Äôs data, asking other companies to sign a licensing agreement for AI training and usage.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is one of the most used sites in AI demos for AI agents, browsers, and assistants. However, the Microsoft-owned company has not put restrictions on its data just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúI think we are still early in this age of browsers and how they are working on behalf of people. I think over time, we will have a more sturdy policy [around browsers],‚Äù Rajiv said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOn a broader note, I have seen a lot of demos that try to reason over a person‚Äôs LinkedIn network. This is sort of an area where I think it is going to be hard to find a substitute for the real thing because this is the worst the search has ever been.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067453" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Screen1.jpeg?w=331" width="331" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is rolling out AI-powered people search to premium users in the U.S. with plans to expand it to other geographies in the coming months. People who will have access to this feature will see ‚ÄúI‚Äôm looking for‚Ä¶‚Äù in the search bar instead of ‚ÄúSearch.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The search is not perfect. You will get different results when you use a query like ‚Äúpeople who co-founded a YC startup‚Äù as compared to using ‚ÄúY Combinator‚Äù in the query. Also, when you search for ‚Äúpeople who co-founded a voice AI startup,‚Äù some folks who have a LinkedIn top voice badge show up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it is working on improving the way the search tool understands the query.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For the last two years, LinkedIn has tried to infuse AI into different parts of its platforms, including ad copies, content creation, personalized digests, hiring assistance, job hunting advice, and learning. The company is now finally adding AI to one of the most-used parts of the site: search.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the company released a job search tool for members in the U.S., allowing them to search for jobs using natural language queries. Now, the company is extending the feature to people search.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users can use queries like ‚ÄúFind me investors in the healthcare sector with FDA experience,‚Äù people who ‚Äúco-founded a productivity company and are based in NYC,‚Äù or ‚ÄúWho in my network can help me understand wireless networks.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067452" height="443" src="https://techcrunch.com/wp-content/uploads/2025/11/01D_Investors-with-FDA-experience-for-a-biotech-startup.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, LinkedIn‚Äôs search has been more complicated. You can type in a few words to find the right people or rely on many different LinkedIn filters in the hope of getting the right results. Plus, you also have to think about what kinds of words you might want to use to get the best out of the search system.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWith lexical search, you have to know the exact title of the person, or you need to wrestle with filters to find the right person, maybe. And if you didn‚Äôt know the right combination, the right person remained undiscovered. The new AI-powered people search is designed to be the fastest path to the person who can help you the most,‚Äù Rohan Rajiv, senior director of product management at LinkedIn, told TechCrunch over a call.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said in its early testing, it has seen people use this to find others who can help them with their next job opportunity, expand their business, or boost their career prospects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Search has been one area where all internet platforms have been rushing to add AI. Seeing people gravitate toward chatbots like ChatGPT and Perplexity for answers, incumbent search engines like Google, Bing, Brave, and DuckDuckGo have added AI-powered answers. There are plenty of startups working on AI-powered people search as well. Reddit has also leaned heavily into AI-powered search and locked down its platform‚Äôs data, asking other companies to sign a licensing agreement for AI training and usage.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is one of the most used sites in AI demos for AI agents, browsers, and assistants. However, the Microsoft-owned company has not put restrictions on its data just yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúI think we are still early in this age of browsers and how they are working on behalf of people. I think over time, we will have a more sturdy policy [around browsers],‚Äù Rajiv said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúOn a broader note, I have seen a lot of demos that try to reason over a person‚Äôs LinkedIn network. This is sort of an area where I think it is going to be hard to find a substitute for the real thing because this is the worst the search has ever been.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067453" height="680" src="https://techcrunch.com/wp-content/uploads/2025/11/Screen1.jpeg?w=331" width="331" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LinkedIn&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;LinkedIn is rolling out AI-powered people search to premium users in the U.S. with plans to expand it to other geographies in the coming months. People who will have access to this feature will see ‚ÄúI‚Äôm looking for‚Ä¶‚Äù in the search bar instead of ‚ÄúSearch.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The search is not perfect. You will get different results when you use a query like ‚Äúpeople who co-founded a YC startup‚Äù as compared to using ‚ÄúY Combinator‚Äù in the query. Also, when you search for ‚Äúpeople who co-founded a voice AI startup,‚Äù some folks who have a LinkedIn top voice badge show up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it is working on improving the way the search tool understands the query.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/linkedin-adds-ai-powered-search-to-help-users-find-people/</guid><pubDate>Thu, 13 Nov 2025 16:00:00 +0000</pubDate></item><item><title>AWS, Google, Microsoft and OCI Boost AI Inference Performance for Cloud Customers With NVIDIA Dynamo (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/think-smart-dynamo-ai-inference-data-center/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor‚Äôs note: This post is part of &lt;/i&gt;&lt;i&gt;Think SMART&lt;/i&gt;&lt;i&gt;, a series focused on how leading AI service providers, developers and enterprises can boost their &lt;/i&gt;&lt;i&gt;inference performance&lt;/i&gt;&lt;i&gt; and return on investment with the latest advancements from NVIDIA‚Äôs full-stack &lt;/i&gt;&lt;i&gt;inference platform&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell delivers the highest performance and efficiency, and lowest total cost of ownership across every tested model and use case in the recent independent SemiAnalysis InferenceMAX v1 benchmark.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_87256"&gt;&lt;img alt="alt" class="wp-image-87256 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/think-smart-codesign-slide-1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87256"&gt;NVIDIA CEO Jensen Huang highlighted at NVIDIA GTC Washington, D.C., how Blackwell delivers 10x the performance of NVIDIA Hopper, enabling 10x the revenue.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Achieving this industry-leading performance for today‚Äôs most complex AI models, such as large-scale mixture-of-experts (MoE) models, requires distributing (or disaggregating) inference across multiple servers (nodes) to serve millions of concurrent users and deliver faster responses.&lt;/p&gt;
&lt;p&gt;The NVIDIA Dynamo software platform unlocks these powerful multi-node capabilities for production, enabling enterprises to achieve this same benchmark-winning performance and efficiency across their existing cloud environments. Read on to learn how the shift to multi-node inference is driving performance, as well as how cloud platforms are putting this technology to work.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Tapping Disaggregated Inference for Optimized Performance&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For AI models that fit on a single GPU or server, developers often run many identical replicas of&amp;nbsp; the model in parallel across multiple nodes to deliver high throughput. In a recent paper, Russ Fellows, principal analyst at Signal65, showed that this approach achieved an industry-first record aggregate throughput of 1.1 million tokens per second with 72 NVIDIA Blackwell Ultra GPUs.&lt;/p&gt;
&lt;p&gt;When scaling AI models to serve many concurrent users in real time, or when managing demanding workloads with long input sequences, using a technique called disaggregated serving unlocks further performance and efficiency gains.&lt;/p&gt;
&lt;p&gt;Serving AI models involves two phases: processing the input prompt (prefill) and generating the output (decode). Traditionally, both phases run on the same GPUs, which can create inefficiencies and resource bottlenecks.&lt;/p&gt;
&lt;p&gt;Disaggregated serving solves this by intelligently distributing these tasks to independently optimized GPUs. This approach ensures that each part of the workload runs with the optimization techniques best suited for it, maximizing overall performance. For today‚Äôs large AI reasoning and MoE models, such as DeepSeek-R1, disaggregated serving is essential.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo easily brings features like disaggregated serving to production scale across GPU clusters.&lt;/p&gt;
&lt;p&gt;It‚Äôs already delivering value.&lt;/p&gt;
&lt;p&gt;Baseten, for example, used NVIDIA Dynamo to speed up inference serving for long-context code generation by 2x and increase throughput by 1.6x, all without incremental hardware costs. Such software-driven performance boosts enable AI providers to significantly reduce the costs to manufacture intelligence.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Scaling Disaggregated Inference in the Cloud&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Much like it did for large-scale AI training, Kubernetes ‚Äî the industry standard for containerized application management ‚Äî is well-positioned to scale disaggregated serving across dozens or even hundreds of nodes for enterprise-scale AI deployments.&lt;/p&gt;
&lt;p&gt;With NVIDIA Dynamo now integrated into managed Kubernetes services from all major cloud providers, customers can scale multi-node inference across NVIDIA Blackwell systems, including GB200 and GB300 NVL72, with the performance, flexibility and reliability that enterprise AI deployments demand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services is accelerating generative AI inference for its customers with NVIDIA Dynamo and integrated with Amazon EKS.&lt;/li&gt;
&lt;li&gt;Google Cloud is providing&amp;nbsp;&amp;nbsp;Dynamo recipe to optimize large language model (LLM) inference at enterprise scale on its AI Hypercomputer.&lt;/li&gt;
&lt;li&gt;Microsoft Azure is enabling multi-node LLM inference with NVIDIA Dynamo and ND GB200-v6 GPUs on Azure Kubernetes Service.&lt;/li&gt;
&lt;li&gt;Oracle Cloud Infrastructure (OCI) is enabling multi-node LLM inferencing with OCI Superclusters and NVIDIA Dynamo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The push towards enabling large-scale, multi-node inference extends beyond hyperscalers.&lt;/p&gt;
&lt;p&gt;Nebius, for example, is designing its cloud to serve inference workloads at scale, built on NVIDIA accelerated computing infrastructure and working with NVIDIA Dynamo as an ecosystem partner.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Simplifying Inference on Kubernetes With NVIDIA Grove in NVIDIA Dynamo&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated AI inference requires coordinating a team of specialized components ‚Äî prefill, decode, routing and more ‚Äî each with different needs. The challenge for Kubernetes is no longer about running more parallel copies of a model, but rather about masterfully conducting these distinct components as one cohesive, high-performance system.&lt;/p&gt;
&lt;p&gt;NVIDIA Grove, an application programming interface now available within NVIDIA Dynamo, allows users to provide a single, high-level specification that describes their entire inference system.&lt;/p&gt;
&lt;p&gt;For example, in that single specification, a user could simply declare their requirements: ‚ÄúI need three GPU nodes for prefill and six GPU nodes for decode, and I require all nodes for a single model replica to be placed on the same high-speed interconnect for the quickest possible response.‚Äù&lt;/p&gt;
&lt;p&gt;From that specification, Grove automatically handles all the intricate coordination: scaling related components together while maintaining correct ratios and dependencies, starting them in the right order and placing them strategically across the cluster for fast, efficient communication. Learn more about how to get started with NVIDIA Grove in this technical deep dive.&lt;/p&gt;
&lt;p&gt;As AI inference becomes increasingly distributed, the combination of Kubernetes and NVIDIA Dynamo with NVIDIA Grove simplifies how developers build and scale intelligent applications.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Try &lt;/i&gt;&lt;i&gt;NVIDIA‚Äôs AI-at-scale simulation&lt;/i&gt;&lt;i&gt; to see how hardware and deployment choices affect performance, efficiency and user experience. &lt;/i&gt;&lt;i&gt;To dive deeper on disaggregated serving and learn how Dynamo and NVIDIA GB200 NVL72 systems work together to boost inference performance, read this &lt;/i&gt;&lt;i&gt;technical blog&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For monthly updates, sign up for the NVIDIA Think SMART newsletter.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor‚Äôs note: This post is part of &lt;/i&gt;&lt;i&gt;Think SMART&lt;/i&gt;&lt;i&gt;, a series focused on how leading AI service providers, developers and enterprises can boost their &lt;/i&gt;&lt;i&gt;inference performance&lt;/i&gt;&lt;i&gt; and return on investment with the latest advancements from NVIDIA‚Äôs full-stack &lt;/i&gt;&lt;i&gt;inference platform&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell delivers the highest performance and efficiency, and lowest total cost of ownership across every tested model and use case in the recent independent SemiAnalysis InferenceMAX v1 benchmark.&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_87256"&gt;&lt;img alt="alt" class="wp-image-87256 size-large" height="1120" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/think-smart-codesign-slide-1680x1120.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87256"&gt;NVIDIA CEO Jensen Huang highlighted at NVIDIA GTC Washington, D.C., how Blackwell delivers 10x the performance of NVIDIA Hopper, enabling 10x the revenue.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Achieving this industry-leading performance for today‚Äôs most complex AI models, such as large-scale mixture-of-experts (MoE) models, requires distributing (or disaggregating) inference across multiple servers (nodes) to serve millions of concurrent users and deliver faster responses.&lt;/p&gt;
&lt;p&gt;The NVIDIA Dynamo software platform unlocks these powerful multi-node capabilities for production, enabling enterprises to achieve this same benchmark-winning performance and efficiency across their existing cloud environments. Read on to learn how the shift to multi-node inference is driving performance, as well as how cloud platforms are putting this technology to work.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Tapping Disaggregated Inference for Optimized Performance&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For AI models that fit on a single GPU or server, developers often run many identical replicas of&amp;nbsp; the model in parallel across multiple nodes to deliver high throughput. In a recent paper, Russ Fellows, principal analyst at Signal65, showed that this approach achieved an industry-first record aggregate throughput of 1.1 million tokens per second with 72 NVIDIA Blackwell Ultra GPUs.&lt;/p&gt;
&lt;p&gt;When scaling AI models to serve many concurrent users in real time, or when managing demanding workloads with long input sequences, using a technique called disaggregated serving unlocks further performance and efficiency gains.&lt;/p&gt;
&lt;p&gt;Serving AI models involves two phases: processing the input prompt (prefill) and generating the output (decode). Traditionally, both phases run on the same GPUs, which can create inefficiencies and resource bottlenecks.&lt;/p&gt;
&lt;p&gt;Disaggregated serving solves this by intelligently distributing these tasks to independently optimized GPUs. This approach ensures that each part of the workload runs with the optimization techniques best suited for it, maximizing overall performance. For today‚Äôs large AI reasoning and MoE models, such as DeepSeek-R1, disaggregated serving is essential.&lt;/p&gt;
&lt;p&gt;NVIDIA Dynamo easily brings features like disaggregated serving to production scale across GPU clusters.&lt;/p&gt;
&lt;p&gt;It‚Äôs already delivering value.&lt;/p&gt;
&lt;p&gt;Baseten, for example, used NVIDIA Dynamo to speed up inference serving for long-context code generation by 2x and increase throughput by 1.6x, all without incremental hardware costs. Such software-driven performance boosts enable AI providers to significantly reduce the costs to manufacture intelligence.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Scaling Disaggregated Inference in the Cloud&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Much like it did for large-scale AI training, Kubernetes ‚Äî the industry standard for containerized application management ‚Äî is well-positioned to scale disaggregated serving across dozens or even hundreds of nodes for enterprise-scale AI deployments.&lt;/p&gt;
&lt;p&gt;With NVIDIA Dynamo now integrated into managed Kubernetes services from all major cloud providers, customers can scale multi-node inference across NVIDIA Blackwell systems, including GB200 and GB300 NVL72, with the performance, flexibility and reliability that enterprise AI deployments demand.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amazon Web Services is accelerating generative AI inference for its customers with NVIDIA Dynamo and integrated with Amazon EKS.&lt;/li&gt;
&lt;li&gt;Google Cloud is providing&amp;nbsp;&amp;nbsp;Dynamo recipe to optimize large language model (LLM) inference at enterprise scale on its AI Hypercomputer.&lt;/li&gt;
&lt;li&gt;Microsoft Azure is enabling multi-node LLM inference with NVIDIA Dynamo and ND GB200-v6 GPUs on Azure Kubernetes Service.&lt;/li&gt;
&lt;li&gt;Oracle Cloud Infrastructure (OCI) is enabling multi-node LLM inferencing with OCI Superclusters and NVIDIA Dynamo.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The push towards enabling large-scale, multi-node inference extends beyond hyperscalers.&lt;/p&gt;
&lt;p&gt;Nebius, for example, is designing its cloud to serve inference workloads at scale, built on NVIDIA accelerated computing infrastructure and working with NVIDIA Dynamo as an ecosystem partner.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Simplifying Inference on Kubernetes With NVIDIA Grove in NVIDIA Dynamo&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Disaggregated AI inference requires coordinating a team of specialized components ‚Äî prefill, decode, routing and more ‚Äî each with different needs. The challenge for Kubernetes is no longer about running more parallel copies of a model, but rather about masterfully conducting these distinct components as one cohesive, high-performance system.&lt;/p&gt;
&lt;p&gt;NVIDIA Grove, an application programming interface now available within NVIDIA Dynamo, allows users to provide a single, high-level specification that describes their entire inference system.&lt;/p&gt;
&lt;p&gt;For example, in that single specification, a user could simply declare their requirements: ‚ÄúI need three GPU nodes for prefill and six GPU nodes for decode, and I require all nodes for a single model replica to be placed on the same high-speed interconnect for the quickest possible response.‚Äù&lt;/p&gt;
&lt;p&gt;From that specification, Grove automatically handles all the intricate coordination: scaling related components together while maintaining correct ratios and dependencies, starting them in the right order and placing them strategically across the cluster for fast, efficient communication. Learn more about how to get started with NVIDIA Grove in this technical deep dive.&lt;/p&gt;
&lt;p&gt;As AI inference becomes increasingly distributed, the combination of Kubernetes and NVIDIA Dynamo with NVIDIA Grove simplifies how developers build and scale intelligent applications.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Try &lt;/i&gt;&lt;i&gt;NVIDIA‚Äôs AI-at-scale simulation&lt;/i&gt;&lt;i&gt; to see how hardware and deployment choices affect performance, efficiency and user experience. &lt;/i&gt;&lt;i&gt;To dive deeper on disaggregated serving and learn how Dynamo and NVIDIA GB200 NVL72 systems work together to boost inference performance, read this &lt;/i&gt;&lt;i&gt;technical blog&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For monthly updates, sign up for the NVIDIA Think SMART newsletter.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/think-smart-dynamo-ai-inference-data-center/</guid><pubDate>Thu, 13 Nov 2025 16:00:58 +0000</pubDate></item><item><title>Google‚Äôs SIMA 2 agent uses Gemini to reason and act in virtual worlds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/googles-sima-2-agent-uses-gemini-to-reason-and-act-in-virtual-worlds/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind shared on Thursday a research preview of SIMA 2, the next generation of its generalist AI agent that integrates the language and reasoning powers of Gemini, Google‚Äôs large language model, to move beyond simply following instructions to understanding and interacting with its environment.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like many of DeepMind‚Äôs projects, including AlphaFold, the first version of SIMA was trained on hundreds of hours of video game data to learn how to play multiple 3D games like a human, even some games it wasn‚Äôt trained on. SIMA 1, unveiled in March 2024, could follow basic instructions across a wide range of virtual environments, but it only had a 31% success rate for completing complex tasks, compared to 71% for humans.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúSIMA 2 is a step change and improvement in capabilities over SIMA 1,‚Äù Joe Marino, senior research scientist at DeepMind, said in a press briefing. ‚ÄúIt‚Äôs a more general agent. It can complete complex tasks in previously unseen environments. And it‚Äôs a self-improving agent. So it can actually self-improve based on its own experience, which is a step towards more general-purpose robots and AGI systems more generally.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067551" height="533" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 doubles the performance of SIMA 1&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;SIMA 2 is powered by the Gemini 2.5 flash-lite model, and AGI refers to artificial general intelligence, which DeepMind defines as a system capable of a wide range of intellectual tasks with the ability to learn new skills and generalize knowledge across different areas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Working with so-called ‚Äúembodied agents‚Äù is crucial to generalized intelligence, DeepMind‚Äôs researchers say. Marino explained that an embodied agent interacts with a physical or virtual world via a body ‚Äî observing inputs and taking actions much like a robot or human would ‚Äî whereas a non-embodied agent might interact with your calendar, take notes, or execute code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jane Wang, a senior staff research scientist at DeepMind with a background in neuroscience, told TechCrunch that SIMA 2 goes far beyond gameplay.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe‚Äôre asking it to actually understand what‚Äôs happening, understand what the user is asking it to do, and then be able to respond in a common-sense way that‚Äôs actually quite difficult,‚Äù Wang said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;By integrating Gemini, SIMA 2 doubled its predecessor‚Äôs performance, uniting Gemini‚Äôs advanced language and reasoning abilities with the embodied skills developed through training.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3067564" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/deepmind.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marino demoed SIMA 2 in ‚ÄúNo Man‚Äôs Sky,‚Äù where the agent described its surroundings ‚Äî a rocky planet surface ‚Äî and determined its next steps by recognizing and interacting with a distress beacon. SIMA 2 also uses Gemini to reason internally. In another game, when asked to walk to the house that‚Äôs the color of a ripe tomato, the agent showed its thinking ‚Äî ripe tomatoes are red, therefore I should go to the red house ‚Äî then found and approached it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Being Gemini-powered also means SIMA 2 follows instructions based on emojis: ‚ÄúYou instruct it ü™ìüå≤, and it‚Äôll go chop down a tree,‚Äù Marino said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Marino also demonstrated how SIMA 2 can navigate newly generated photorealistic worlds produced by Genie, DeepMind‚Äôs world model, correctly identifying and interacting with objects like benches, trees, and butterflies.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067552" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 is a self-improving agent&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini also enables self-improvement without much human data, Marino added. Where SIMA 1 was trained entirely on human gameplay, SIMA 2 uses it as a baseline to provide a strong initial model. When the team puts the agent into a new environment, it asks another Gemini model to create new tasks and a separate reward model to score the agent‚Äôs attempts. Using these self-generated experiences as training data, the agent learns from its own mistakes and gradually performs better, essentially teaching itself new behaviors through trial and error as a human would, guided by AI-based feedback instead of humans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind sees SIMA 2 as a step toward unlocking more general-purpose robots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIf we think of what a system needs to do to perform tasks in the real world, like a robot, I think there are two components of it,‚Äù Frederic Besse, senior staff research engineer at DeepMind, said during a press briefing. ‚ÄúFirst, there is a high-level understanding of the real world and what needs to be done, as well as some reasoning.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you ask a humanoid robot in your house to go check how many cans of beans you have in the cupboard, the system needs to understand all of the different concepts ‚Äî what beans are, what a cupboard is ‚Äî and navigate to that location. Besse says SIMA 2 touches more on that high-level behavior than it does on lower-level actions, which he refers to as controlling things like physical joints and wheels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The team declined to share a specific timeline for implementing SIMA 2 in physical robotics systems. Besse told TechCrunch that DeepMind‚Äôs recently unveiled robotics foundation models ‚Äî which can also reason about the physical world and create multi-step plans to complete a mission ‚Äî were trained differently and separately from SIMA.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While there‚Äôs also no timeline for releasing more than a preview of SIMA 2, Wang told TechCrunch the goal is to show the world what DeepMind has been working on and see what kinds of collaborations and potential uses are possible.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind shared on Thursday a research preview of SIMA 2, the next generation of its generalist AI agent that integrates the language and reasoning powers of Gemini, Google‚Äôs large language model, to move beyond simply following instructions to understanding and interacting with its environment.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like many of DeepMind‚Äôs projects, including AlphaFold, the first version of SIMA was trained on hundreds of hours of video game data to learn how to play multiple 3D games like a human, even some games it wasn‚Äôt trained on. SIMA 1, unveiled in March 2024, could follow basic instructions across a wide range of virtual environments, but it only had a 31% success rate for completing complex tasks, compared to 71% for humans.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;‚ÄúSIMA 2 is a step change and improvement in capabilities over SIMA 1,‚Äù Joe Marino, senior research scientist at DeepMind, said in a press briefing. ‚ÄúIt‚Äôs a more general agent. It can complete complex tasks in previously unseen environments. And it‚Äôs a self-improving agent. So it can actually self-improve based on its own experience, which is a step towards more general-purpose robots and AGI systems more generally.‚Äù&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067551" height="533" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 doubles the performance of SIMA 1&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;SIMA 2 is powered by the Gemini 2.5 flash-lite model, and AGI refers to artificial general intelligence, which DeepMind defines as a system capable of a wide range of intellectual tasks with the ability to learn new skills and generalize knowledge across different areas.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Working with so-called ‚Äúembodied agents‚Äù is crucial to generalized intelligence, DeepMind‚Äôs researchers say. Marino explained that an embodied agent interacts with a physical or virtual world via a body ‚Äî observing inputs and taking actions much like a robot or human would ‚Äî whereas a non-embodied agent might interact with your calendar, take notes, or execute code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jane Wang, a senior staff research scientist at DeepMind with a background in neuroscience, told TechCrunch that SIMA 2 goes far beyond gameplay.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúWe‚Äôre asking it to actually understand what‚Äôs happening, understand what the user is asking it to do, and then be able to respond in a common-sense way that‚Äôs actually quite difficult,‚Äù Wang said.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;By integrating Gemini, SIMA 2 doubled its predecessor‚Äôs performance, uniting Gemini‚Äôs advanced language and reasoning abilities with the embodied skills developed through training.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3067564" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/deepmind.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marino demoed SIMA 2 in ‚ÄúNo Man‚Äôs Sky,‚Äù where the agent described its surroundings ‚Äî a rocky planet surface ‚Äî and determined its next steps by recognizing and interacting with a distress beacon. SIMA 2 also uses Gemini to reason internally. In another game, when asked to walk to the house that‚Äôs the color of a ripe tomato, the agent showed its thinking ‚Äî ripe tomatoes are red, therefore I should go to the red house ‚Äî then found and approached it.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Being Gemini-powered also means SIMA 2 follows instructions based on emojis: ‚ÄúYou instruct it ü™ìüå≤, and it‚Äôll go chop down a tree,‚Äù Marino said.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Marino also demonstrated how SIMA 2 can navigate newly generated photorealistic worlds produced by Genie, DeepMind‚Äôs world model, correctly identifying and interacting with objects like benches, trees, and butterflies.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067552" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/SIMA-2-blog-figure-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;DeepMind says SIMA 2 is a self-improving agent&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Gemini also enables self-improvement without much human data, Marino added. Where SIMA 1 was trained entirely on human gameplay, SIMA 2 uses it as a baseline to provide a strong initial model. When the team puts the agent into a new environment, it asks another Gemini model to create new tasks and a separate reward model to score the agent‚Äôs attempts. Using these self-generated experiences as training data, the agent learns from its own mistakes and gradually performs better, essentially teaching itself new behaviors through trial and error as a human would, guided by AI-based feedback instead of humans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind sees SIMA 2 as a step toward unlocking more general-purpose robots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIf we think of what a system needs to do to perform tasks in the real world, like a robot, I think there are two components of it,‚Äù Frederic Besse, senior staff research engineer at DeepMind, said during a press briefing. ‚ÄúFirst, there is a high-level understanding of the real world and what needs to be done, as well as some reasoning.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you ask a humanoid robot in your house to go check how many cans of beans you have in the cupboard, the system needs to understand all of the different concepts ‚Äî what beans are, what a cupboard is ‚Äî and navigate to that location. Besse says SIMA 2 touches more on that high-level behavior than it does on lower-level actions, which he refers to as controlling things like physical joints and wheels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The team declined to share a specific timeline for implementing SIMA 2 in physical robotics systems. Besse told TechCrunch that DeepMind‚Äôs recently unveiled robotics foundation models ‚Äî which can also reason about the physical world and create multi-step plans to complete a mission ‚Äî were trained differently and separately from SIMA.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While there‚Äôs also no timeline for releasing more than a preview of SIMA 2, Wang told TechCrunch the goal is to show the world what DeepMind has been working on and see what kinds of collaborations and potential uses are possible.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/googles-sima-2-agent-uses-gemini-to-reason-and-act-in-virtual-worlds/</guid><pubDate>Thu, 13 Nov 2025 16:15:47 +0000</pubDate></item><item><title>Google‚Äôs NotebookLM adds ‚ÄòDeep Research‚Äô tool, support for more file types (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/googles-notebooklm-adds-deep-research-tool-support-for-more-file-types/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is updating NotebookLM, its AI note-taking and research assistant, with a new tool to help users simplify complex research, along with support for additional file types.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The service is rolling out ‚ÄúDeep Research,‚Äù a tool that will automate and simplify complex online research. Google says the tool acts like a dedicated researcher, as it can synthesize a detailed report or recommend relevant articles, papers, or websites.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Deep Research can take your question, create a research plan, and then browse websites on your behalf. After a few minutes, it will present you with a source-grounded report that you can add directly into your notebook. While Deep Research runs in the background, you can continue to add other sources. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea behind Deep Research is to help you create a deep, organized knowledge base on a topic without having to leave your workflow. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067541" height="372" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-13-at-10.40.46-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access Deep Research by starting a search in the source panel and selecting ‚ÄúWeb‚Äù as a source. Then, you can choose your research style. You can select ‚ÄúDeep Research‚Äù when you‚Äôre looking for a full briefing and in-depth analysis. Or, you can choose ‚ÄúFast Research‚Äù if you want a quick search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the additional support for more file types, you can now upload Google Sheets, Drive files as URLs, PDFs from Google Drive, and Microsoft Word Documents. The tech giant says this change will allow users to do things like generate summaries from spreadsheets and quickly copy-paste multiple Drive files as URLs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says these new updates should be available to all users within a week. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Since its launch in late 2023, Google has been building out NotebookLM with additional functionality. Earlier this year, Google introduced Video Overviews to NotebookLM to allow users to&amp;nbsp;turn dense multimedia, such as raw notes, PDFs, and images, into digestible visual presentations. The addition builds on&amp;nbsp;the Audio Overviews feature, which generates an AI podcast based on documents shared with NotebookLM, such as course readings or legal briefs. In May, Google released the NotebookLM apps for&amp;nbsp;Android&amp;nbsp;and&amp;nbsp;iOS, making the service available beyond desktop.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is updating NotebookLM, its AI note-taking and research assistant, with a new tool to help users simplify complex research, along with support for additional file types.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The service is rolling out ‚ÄúDeep Research,‚Äù a tool that will automate and simplify complex online research. Google says the tool acts like a dedicated researcher, as it can synthesize a detailed report or recommend relevant articles, papers, or websites.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Deep Research can take your question, create a research plan, and then browse websites on your behalf. After a few minutes, it will present you with a source-grounded report that you can add directly into your notebook. While Deep Research runs in the background, you can continue to add other sources. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea behind Deep Research is to help you create a deep, organized knowledge base on a topic without having to leave your workflow. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3067541" height="372" src="https://techcrunch.com/wp-content/uploads/2025/11/Screenshot-2025-11-13-at-10.40.46-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You can access Deep Research by starting a search in the source panel and selecting ‚ÄúWeb‚Äù as a source. Then, you can choose your research style. You can select ‚ÄúDeep Research‚Äù when you‚Äôre looking for a full briefing and in-depth analysis. Or, you can choose ‚ÄúFast Research‚Äù if you want a quick search. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for the additional support for more file types, you can now upload Google Sheets, Drive files as URLs, PDFs from Google Drive, and Microsoft Word Documents. The tech giant says this change will allow users to do things like generate summaries from spreadsheets and quickly copy-paste multiple Drive files as URLs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says these new updates should be available to all users within a week. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Since its launch in late 2023, Google has been building out NotebookLM with additional functionality. Earlier this year, Google introduced Video Overviews to NotebookLM to allow users to&amp;nbsp;turn dense multimedia, such as raw notes, PDFs, and images, into digestible visual presentations. The addition builds on&amp;nbsp;the Audio Overviews feature, which generates an AI podcast based on documents shared with NotebookLM, such as course readings or legal briefs. In May, Google released the NotebookLM apps for&amp;nbsp;Android&amp;nbsp;and&amp;nbsp;iOS, making the service available beyond desktop.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/googles-notebooklm-adds-deep-research-tool-support-for-more-file-types/</guid><pubDate>Thu, 13 Nov 2025 17:00:00 +0000</pubDate></item><item><title>OpenAI‚Äôs new LLM exposes the secrets of how AI really works (Artificial intelligence ‚Äì MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127914/openais-new-llm-exposes-the-secrets-of-how-ai-really-works/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/box-view.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;ChatGPT maker OpenAI has built an experimental large language model that is far easier to understand than typical models.&lt;/p&gt;  &lt;p&gt;That‚Äôs a big deal, because today‚Äôs LLMs are black boxes: Nobody fully understands how they do what they do. Building a model that is more transparent sheds light on how LLMs work in general, helping researchers figure out why models hallucinate, why they go off the rails, and just how far we should trust them with critical tasks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;‚ÄúAs these AI systems get more powerful, they‚Äôre going to get integrated more and more into very important domains,‚Äù Leo Gao, a research scientist at OpenAI, told &lt;em&gt;MIT Technology Review&lt;/em&gt; in an exclusive preview of the new work. ‚ÄúIt‚Äôs very important to make sure they‚Äôre safe.‚Äù&lt;/p&gt;  &lt;p&gt;This is still early research. The new model, called a weight-sparse transformer, is far smaller and far less capable than top-tier mass-market models like the firm‚Äôs GPT-5, Anthropic‚Äôs Claude, and Google DeepMind‚Äôs Gemini. At most it‚Äôs as capable as GPT-1, a model that OpenAI developed back in 2018, says Gao (though he and his colleagues haven‚Äôt done a direct comparison).&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But the aim isn‚Äôt to compete with the best in class (at least, not yet). Instead, by looking at how this experimental model works, OpenAI hopes to learn about the hidden mechanisms inside those bigger and better versions of the technology.&lt;/p&gt;  &lt;p&gt;It‚Äôs interesting research, says Elisenda Grigsby, a mathematician at Boston College who studies how LLMs work and who was not involved in the project: ‚ÄúI‚Äôm sure the methods it introduces will have a significant impact.‚Äù&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Lee Sharkey, a research scientist at AI startup Goodfire, agrees. ‚ÄúThis work aims at the right target and seems well executed,‚Äù he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why models are so hard to understand&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenAI‚Äôs work is part of a hot new field of research known as mechanistic interpretability, which is trying to map the internal mechanisms that models use when they carry out different tasks.&lt;/p&gt;  &lt;p&gt;That‚Äôs harder than it sounds. LLMs are built from neural networks, which consist of nodes, called neurons, arranged in layers. In most networks, each neuron is connected to every other neuron in its adjacent layers. Such a network is known as a dense network.&lt;/p&gt;  &lt;p&gt;Dense networks are relatively efficient to train and run, but they spread what they learn across a vast knot of connections. The result is that simple concepts or functions can be split up between neurons in different parts of a model. At the same time, specific neurons can also end up representing multiple different features, a phenomenon known as superposition (a term borrowed from quantum physics). The upshot is that you can‚Äôt relate specific parts of a model to specific concepts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;‚ÄúNeural networks are big and complicated and tangled up and very difficult to understand,‚Äù says Dan Mossing, who leads the mechanistic interpretability team at OpenAI. ‚ÄúWe‚Äôve sort of said: ‚ÄòOkay, what if we tried to make that not the case?‚Äô‚Äù&lt;/p&gt;  &lt;p&gt;Instead of building a model using a dense network, OpenAI started with a type of neural network known as a weight-sparse transformer, in which each neuron is connected to only a few other neurons. This forced the model to represent features in localized clusters rather than spread them out.&lt;/p&gt;  &lt;p&gt;Their model is far slower than any LLM on the market. But it is easier to relate its neurons or groups of neurons to specific concepts and functions. ‚ÄúThere‚Äôs a really drastic difference in how interpretable the model is,‚Äù says Gao.&lt;/p&gt;  &lt;p&gt;Gao and his colleagues have tested the new model with very simple tasks. For example, they asked it to complete a block of text that opens with quotation marks by adding matching marks at the end.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;It‚Äôs a trivial request for an LLM. The point is that figuring out how a model does even a straightforward task like that involves unpicking a complicated tangle of neurons and connections, says Gao. But with the new model, they were able to follow the exact steps the model took.&lt;/p&gt;  &lt;p&gt;‚ÄúWe actually found a circuit that‚Äôs exactly the algorithm you would think to implement by hand, but it‚Äôs fully learned by the model,‚Äù he says. ‚ÄúI think this is really cool and exciting.‚Äù&lt;/p&gt;  &lt;p&gt;Where will the research go next? Grigsby is not convinced the technique would scale up to larger models that have to handle a variety of more difficult tasks.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Gao and Mossing acknowledge that this is a big limitation of the model they have built so far and agree that the approach will never lead to models that match the performance of cutting-edge products like GPT-5. And yet OpenAI thinks it might be able to improve the technique enough to build a transparent model on a par with GPT-3, the firm‚Äôs breakthrough 2021 LLM.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúMaybe within a few years, we could have a fully interpretable GPT-3, so that you could go inside every single part of it and you could understand how it does every single thing,‚Äù says Gao. ‚ÄúIf we had such a system, we would learn so much.‚Äù&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/box-view.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;ChatGPT maker OpenAI has built an experimental large language model that is far easier to understand than typical models.&lt;/p&gt;  &lt;p&gt;That‚Äôs a big deal, because today‚Äôs LLMs are black boxes: Nobody fully understands how they do what they do. Building a model that is more transparent sheds light on how LLMs work in general, helping researchers figure out why models hallucinate, why they go off the rails, and just how far we should trust them with critical tasks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;‚ÄúAs these AI systems get more powerful, they‚Äôre going to get integrated more and more into very important domains,‚Äù Leo Gao, a research scientist at OpenAI, told &lt;em&gt;MIT Technology Review&lt;/em&gt; in an exclusive preview of the new work. ‚ÄúIt‚Äôs very important to make sure they‚Äôre safe.‚Äù&lt;/p&gt;  &lt;p&gt;This is still early research. The new model, called a weight-sparse transformer, is far smaller and far less capable than top-tier mass-market models like the firm‚Äôs GPT-5, Anthropic‚Äôs Claude, and Google DeepMind‚Äôs Gemini. At most it‚Äôs as capable as GPT-1, a model that OpenAI developed back in 2018, says Gao (though he and his colleagues haven‚Äôt done a direct comparison).&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But the aim isn‚Äôt to compete with the best in class (at least, not yet). Instead, by looking at how this experimental model works, OpenAI hopes to learn about the hidden mechanisms inside those bigger and better versions of the technology.&lt;/p&gt;  &lt;p&gt;It‚Äôs interesting research, says Elisenda Grigsby, a mathematician at Boston College who studies how LLMs work and who was not involved in the project: ‚ÄúI‚Äôm sure the methods it introduces will have a significant impact.‚Äù&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Lee Sharkey, a research scientist at AI startup Goodfire, agrees. ‚ÄúThis work aims at the right target and seems well executed,‚Äù he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why models are so hard to understand&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenAI‚Äôs work is part of a hot new field of research known as mechanistic interpretability, which is trying to map the internal mechanisms that models use when they carry out different tasks.&lt;/p&gt;  &lt;p&gt;That‚Äôs harder than it sounds. LLMs are built from neural networks, which consist of nodes, called neurons, arranged in layers. In most networks, each neuron is connected to every other neuron in its adjacent layers. Such a network is known as a dense network.&lt;/p&gt;  &lt;p&gt;Dense networks are relatively efficient to train and run, but they spread what they learn across a vast knot of connections. The result is that simple concepts or functions can be split up between neurons in different parts of a model. At the same time, specific neurons can also end up representing multiple different features, a phenomenon known as superposition (a term borrowed from quantum physics). The upshot is that you can‚Äôt relate specific parts of a model to specific concepts.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;‚ÄúNeural networks are big and complicated and tangled up and very difficult to understand,‚Äù says Dan Mossing, who leads the mechanistic interpretability team at OpenAI. ‚ÄúWe‚Äôve sort of said: ‚ÄòOkay, what if we tried to make that not the case?‚Äô‚Äù&lt;/p&gt;  &lt;p&gt;Instead of building a model using a dense network, OpenAI started with a type of neural network known as a weight-sparse transformer, in which each neuron is connected to only a few other neurons. This forced the model to represent features in localized clusters rather than spread them out.&lt;/p&gt;  &lt;p&gt;Their model is far slower than any LLM on the market. But it is easier to relate its neurons or groups of neurons to specific concepts and functions. ‚ÄúThere‚Äôs a really drastic difference in how interpretable the model is,‚Äù says Gao.&lt;/p&gt;  &lt;p&gt;Gao and his colleagues have tested the new model with very simple tasks. For example, they asked it to complete a block of text that opens with quotation marks by adding matching marks at the end.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;It‚Äôs a trivial request for an LLM. The point is that figuring out how a model does even a straightforward task like that involves unpicking a complicated tangle of neurons and connections, says Gao. But with the new model, they were able to follow the exact steps the model took.&lt;/p&gt;  &lt;p&gt;‚ÄúWe actually found a circuit that‚Äôs exactly the algorithm you would think to implement by hand, but it‚Äôs fully learned by the model,‚Äù he says. ‚ÄúI think this is really cool and exciting.‚Äù&lt;/p&gt;  &lt;p&gt;Where will the research go next? Grigsby is not convinced the technique would scale up to larger models that have to handle a variety of more difficult tasks.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Gao and Mossing acknowledge that this is a big limitation of the model they have built so far and agree that the approach will never lead to models that match the performance of cutting-edge products like GPT-5. And yet OpenAI thinks it might be able to improve the technique enough to build a transparent model on a par with GPT-3, the firm‚Äôs breakthrough 2021 LLM.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;‚ÄúMaybe within a few years, we could have a fully interpretable GPT-3, so that you could go inside every single part of it and you could understand how it does every single thing,‚Äù says Gao. ‚ÄúIf we had such a system, we would learn so much.‚Äù&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127914/openais-new-llm-exposes-the-secrets-of-how-ai-really-works/</guid><pubDate>Thu, 13 Nov 2025 18:00:00 +0000</pubDate></item><item><title>Upwork study shows AI agents excel with human partners but fail independently (AI | VentureBeat)</title><link>https://venturebeat.com/ai/upwork-study-shows-ai-agents-excel-with-human-partners-but-fail</link><description>[unable to retrieve full-text content]&lt;p&gt;Artificial intelligence agents powered by the world&amp;#x27;s most advanced language models routinely fail to complete even straightforward professional tasks on their own, according to &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;groundbreaking research&lt;/u&gt;&lt;/a&gt; released Thursday by &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, the largest online work marketplace.&lt;/p&gt;&lt;p&gt;But the same study reveals a more promising path forward: When AI agents collaborate with human experts, &lt;a href="https://www.globenewswire.com/news-release/2025/11/13/3187462/0/en/Upwork-Human-Agent-Productivity-Index-Reveals-Up-to-70-Boost-in-Work-Completion-from-Human-and-AI-Agent-Collaboration-vs-Agents-Working-Alone.html"&gt;&lt;u&gt;project completion rates surge by up to 70%&lt;/u&gt;&lt;/a&gt;, suggesting the future of work may not pit humans against machines but rather pair them together in powerful new ways.&lt;/p&gt;&lt;p&gt;The findings, drawn from more than 300 real client projects posted to Upwork&amp;#x27;s platform, marking the first systematic evaluation of how human expertise amplifies AI agent performance in actual professional work ‚Äî not synthetic tests or academic simulations. The research challenges both the hype around fully autonomous AI agents and fears that such technology will imminently replace knowledge workers.&lt;/p&gt;&lt;p&gt;&amp;quot;AI agents aren&amp;#x27;t that agentic, meaning they aren&amp;#x27;t that good,&amp;quot; Andrew Rabinovich, Upwork&amp;#x27;s chief technology officer and head of AI and machine learning, said in an exclusive interview with VentureBeat. &amp;quot;However, when paired with expert human professionals, project completion rates improve dramatically, supporting our firm belief that the future of work will be defined by humans and AI collaborating to get more work done, with human intuition and domain expertise playing a critical role.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents performed on 300+ real freelance jobs‚Äîand why they struggled&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/human-agent-productivity-index"&gt;&lt;u&gt;Upwork&amp;#x27;s Human+Agent Productivity Index (HAPI) &lt;/u&gt;&lt;/a&gt;evaluated how three leading AI systems ‚Äî &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;, OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt;, and Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; ‚Äî performed on actual jobs posted by paying clients across categories including writing, data science, web development, engineering, sales, and translation.&lt;/p&gt;&lt;p&gt;Critically, Upwork deliberately selected simple, well-defined projects where AI agents stood a reasonable chance of success. These jobs, priced under $500, represent less than 6% of Upwork&amp;#x27;s total gross services volume ‚Äî a tiny fraction of the platform&amp;#x27;s overall business and an acknowledgment of current AI limitations.&lt;/p&gt;&lt;p&gt;&amp;quot;The reality is that although we study AI, and I&amp;#x27;ve been doing this for 25 years, and we see significant breakthroughs, the reality is that these agents aren&amp;#x27;t that agentic,&amp;quot; Rabinovich told VentureBeat. &amp;quot;So if we go up the value chain, the problems become so much more difficult, then we don&amp;#x27;t think they can solve them at all, even to scratch the surface. So we specifically chose simpler tasks that would give an agent some kind of traction.&amp;quot;&lt;/p&gt;&lt;p&gt;Even on these deliberately simplified tasks, AI agents working independently struggled. But when expert freelancers provided feedback ‚Äî spending an average of just 20 minutes per review cycle ‚Äî the agents&amp;#x27; performance improved substantially with each iteration.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;20 minutes of human feedback boosted AI completion rates up to 70%&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals stark differences in how AI agents perform with and without human guidance across different types of work. For data science and analytics projects, Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; achieved a 64% completion rate working alone but jumped to 93% after receiving feedback from a human expert. In sales and marketing work, &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;&amp;#x27;s completion rate rose from 17% independently to 31% with human input. OpenAI&amp;#x27;s &lt;a href="https://openai.com/gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; showed similarly dramatic improvements in engineering and architecture tasks, climbing from 30% to 50% completion.&lt;/p&gt;&lt;p&gt;The pattern held across virtually all categories, with agents responding particularly well to human feedback on qualitative, creative work requiring editorial judgment ‚Äî areas like writing, translation, and marketing ‚Äî where completion rates increased by up to 17 percentage points per feedback cycle.&lt;/p&gt;&lt;p&gt;The finding challenges a fundamental assumption in the AI industry: that agent benchmarks conducted in isolation accurately predict real-world performance.&lt;/p&gt;&lt;p&gt;&amp;quot;While we show that in the tasks that we have selected for agents to perform in isolation, they perform similarly to the previous results that we&amp;#x27;ve seen published openly, what we&amp;#x27;ve shown is that in collaboration with humans, the performance of these agents improves surprisingly well,&amp;quot; Rabinovich said. &amp;quot;It&amp;#x27;s not just a one-turn back and forth, but the more feedback the human provides, the better the agent gets at performing.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why ChatGPT can ace the SAT but can&amp;#x27;t count the R&amp;#x27;s in &amp;#x27;strawberry&amp;#x27;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research arrives as the AI industry grapples with a measurement crisis. Traditional benchmarks ‚Äî standardized tests that AI models can master, sometimes scoring perfectly on SAT exams or mathematics olympiads ‚Äî have proven poor predictors of real-world capability.&lt;/p&gt;&lt;p&gt;&amp;quot;With advances of large language models, what we&amp;#x27;re now seeing is that these static, academic datasets are completely saturated,&amp;quot; Rabinovich said. &amp;quot;So you could get a perfect score in the SAT test or LSAT or any of the math olympiads, and then you would ask ChatGPT how many R&amp;#x27;s there are in the word strawberry, and it would get it wrong.&amp;quot;&lt;/p&gt;&lt;p&gt;This phenomenon ‚Äî where AI systems ace formal tests but stumble on trivial real-world questions ‚Äî has led to growing skepticism about AI capabilities, even as companies race to deploy autonomous agents. Several recent benchmarks from other firms have tested AI agents on Upwork jobs, but those evaluations measured only isolated performance, not the collaborative potential that Upwork&amp;#x27;s research reveals.&lt;/p&gt;&lt;p&gt;&amp;quot;We wanted to evaluate the quality of these agents on actual real work with economic value associated with it, and not only see how well these agents do, but also see how these agents do in collaboration with humans, because we sort of knew already that in isolation, they&amp;#x27;re not that advanced,&amp;quot; Rabinovich explained.&lt;/p&gt;&lt;p&gt;For &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, which connects roughly 800,000 active clients posting more than 3 million jobs annually to a global pool of freelancers, the research serves a strategic business purpose: establishing quality standards for AI agents before allowing them to compete or collaborate with human workers on its platform.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The economics of human-AI teamwork: Why paying for expert feedback still saves money&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite requiring multiple rounds of human feedback ‚Äî each lasting about 20 minutes ‚Äî the time investment remains &amp;quot;orders of magnitude different between a human doing the work alone, versus a human doing the work with an AI agent,&amp;quot; Rabinovich said. Where a project might take a freelancer days to complete independently, the agent-plus-human approach can deliver results in hours through iterative cycles of automated work and expert refinement.&lt;/p&gt;&lt;p&gt;The economic implications extend beyond simple time savings. Upwork recently reported that gross services volume from &lt;a href="https://finance.yahoo.com/news/upworks-stock-soars-q3-blowout-201700500.html"&gt;&lt;u&gt;AI-related work grew 53% year-over-year&lt;/u&gt;&lt;/a&gt; in the third quarter of 2025, one of the strongest growth drivers for the company. But executives have been careful to frame AI not as a replacement for freelancers but as an enhancement to their capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;AI was a huge overhang for our valuation,&amp;quot; Erica Gessert, Upwork&amp;#x27;s CFO, told &lt;a href="https://www.cfobrew.com/stories/2025/10/16/the-cfo-of-upwork-answers-every-cfo-s-most-burning-question"&gt;&lt;u&gt;CFO Brew&lt;/u&gt;&lt;/a&gt; in October. &amp;quot;There was this belief that all work was going to go away. AI was going to take it, and especially work that&amp;#x27;s done by people like freelancers, because they are impermanent. Actually, the opposite is true.&amp;quot;&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s strategy centers on enabling freelancers to handle more complex, higher-value work by offloading routine tasks to AI. &amp;quot;Freelancers actually prefer to have tools that automate the manual labor and repetitive part of their work, and really focus on the creative and conceptual part of the process,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than replacing jobs, he argues, AI will transform them: &amp;quot;Simpler tasks will be automated by agents, but the jobs will become much more complex in the number of tasks, so the amount of work and therefore earnings for freelancers will actually only go up.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI coding agents excel, but creative writing and translation still need humans&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals a clear pattern in agent capabilities. AI systems perform best on &amp;quot;deterministic and verifiable&amp;quot; tasks with objectively correct answers, like solving math problems or writing basic code. &amp;quot;Most coding tasks are very similar to each other,&amp;quot; Rabinovich noted. &amp;quot;That&amp;#x27;s why coding agents are becoming so good.&amp;quot;&lt;/p&gt;&lt;p&gt;In Upwork&amp;#x27;s tests, web development, mobile app development, and data science projects ‚Äî especially those involving structured, computational work ‚Äî saw the highest standalone agent completion rates. Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; completed 68% of web development jobs and 64% of data science projects without human help, while &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; achieved 74% on certain technical tasks.&lt;/p&gt;&lt;p&gt;But qualitative work proved far more challenging. When asked to create website layouts, write marketing copy, or translate content with appropriate cultural nuance, agents floundered without expert guidance. &amp;quot;When you ask it to write you a poem, the quality of the poem is extremely subjective,&amp;quot; Rabinovich said. &amp;quot;Since the rubrics for evaluation were provided by humans, there&amp;#x27;s some level of variability in representation.&amp;quot;&lt;/p&gt;&lt;p&gt;Writing, translation, and sales and marketing projects showed the most dramatic improvements from human feedback. For writing work, completion rates increased by up to 17 percentage points after expert review. Engineering and architecture projects requiring creative problem-solving ‚Äî like civil engineering or architectural design ‚Äî improved by as much as 23 percentage points with human oversight.&lt;/p&gt;&lt;p&gt;This pattern suggests AI agents excel at pattern matching and replication but struggle with creativity, judgment, and context ‚Äî precisely the skills that define higher-value professional work.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the research: How Upwork tested AI agents with peer-reviewed scientific methods&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt; partnered with elite freelancers on its platform to evaluate every deliverable produced by AI agents, both independently and after each cycle of human feedback. These evaluators created detailed rubrics defining whether projects met core requirements specified in job descriptions, then scored outputs across multiple iterations.&lt;/p&gt;&lt;p&gt;Importantly, evaluators focused only on objective completion criteria, excluding subjective factors like stylistic preferences or quality judgments that might emerge in actual client relationships. &amp;quot;Rubric-based completion rates should not be viewed as a measure of whether an agent would be paid in a real marketplace setting,&amp;quot; &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;the research&lt;/u&gt;&lt;/a&gt; notes, &amp;quot;but as an indicator of its ability to fulfill explicitly defined requests.&amp;quot;&lt;/p&gt;&lt;p&gt;This distinction matters: An AI agent might technically complete all specified requirements yet still produce work a client rejects as inadequate. Conversely, subjective client satisfaction ‚Äî the true measure of marketplace success ‚Äî remains beyond current measurement capabilities.&lt;/p&gt;&lt;p&gt;The research underwent double-blind peer review and was accepted to &lt;a href="https://neurips.cc/"&gt;&lt;u&gt;NeurIPS&lt;/u&gt;&lt;/a&gt;, the premier academic conference for AI research, where Upwork will present full results in early December. The company plans to publish a complete methodology and make the benchmark available to the research community, updating the task pool regularly to prevent overfitting as agents improve.&lt;/p&gt;&lt;p&gt;&amp;quot;The idea is for this benchmark to be a living and breathing platform where agents can come in and evaluate themselves on all categories of work, and the tasks that will be offered on the platform will always update, so that these agents don&amp;#x27;t overfit and basically memorize the tasks at hand,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Upwork&amp;#x27;s AI strategy: Building Uma, a &amp;#x27;meta-agent&amp;#x27; that manages human and AI workers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research directly informs Upwork&amp;#x27;s product roadmap as the company positions itself for what executives call &amp;quot;the age of AI and beyond.&amp;quot; Rather than building its own AI agents to complete specific tasks, &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-evolves-uma-ai-ai-work-agent-advances-human-ai"&gt;&lt;u&gt;Upwork is developing Uma&lt;/u&gt;&lt;/a&gt;, a &amp;quot;meta orchestration agent&amp;quot; that coordinates between human workers, AI systems, and clients.&lt;/p&gt;&lt;p&gt;&amp;quot;Today, Upwork is a marketplace where clients look for freelancers to get work done, and then talent comes to Upwork to find work,&amp;quot; Rabinovich explained. &amp;quot;This is getting expanded into a domain where clients come to Upwork, communicate with Uma, this meta-orchestration agent, and then Uma identifies the necessary talent to get the job done, gets the tasks outcomes completed, and then delivers that to the client.&amp;quot;&lt;/p&gt;&lt;p&gt;In this vision, clients would interact primarily with Uma rather than directly hiring freelancers. The AI system would analyze project requirements, determine which tasks require human expertise versus AI execution, coordinate the workflow, and ensure quality ‚Äî acting as an intelligent project manager rather than a replacement worker.&lt;/p&gt;&lt;p&gt;&amp;quot;We don&amp;#x27;t want to build agents that actually complete the tasks, but we are building this meta orchestration agent that figures out what human and agent talent is necessary in order to complete the tasks,&amp;quot; Rabinovich said. &amp;quot;Uma evaluates the work to be delivered to the client, orchestrates the interaction between humans and agents, and is able to learn from all the interactions that happen on the platform how to break jobs into tasks so that they get completed in a timely and effective manner.&amp;quot;&lt;/p&gt;&lt;p&gt;The company recently &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-announces-forthcoming-lisbon-office-scale-ai-innovation"&gt;&lt;u&gt;announced plans to open its first international office&lt;/u&gt;&lt;/a&gt; in Lisbon, Portugal, by the fourth quarter of 2026, with a focus on AI infrastructure development and technical hiring. The expansion follows Upwork&amp;#x27;s record-breaking third quarter, driven partly by AI-powered product innovation and strong demand for workers with AI skills.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;OpenAI, Anthropic, and Google race to build autonomous agents‚Äîbut reality lags hype&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Upwork&amp;#x27;s findings arrive amid escalating competition in the AI agent space. OpenAI, Anthropic, Google, and numerous startups are racing to develop autonomous agents capable of complex multi-step tasks, from booking travel to analyzing financial data to writing software.&lt;/p&gt;&lt;p&gt;But recent high-profile stumbles have tempered initial enthusiasm. AI agents frequently misunderstand instructions, make logical errors, or produce confidently wrong results ‚Äî a phenomenon researchers call &amp;quot;hallucination.&amp;quot; The gap between controlled demonstration videos and reliable real-world performance remains vast.&lt;/p&gt;&lt;p&gt;&amp;quot;There have been some evaluations that came from OpenAI and other platforms where real Upwork tasks were considered for completion by agents, and across the board, the reported results were not very optimistic, in the sense that they showed that agents‚Äîeven the best ones, meaning powered by most advanced LLMs ‚Äî can&amp;#x27;t really compete with humans that well, because the completion rates are pretty low,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than waiting for AI to fully mature ‚Äî a timeline that remains uncertain‚ÄîUpwork is betting on a hybrid approach that leverages AI&amp;#x27;s strengths (speed, scalability, pattern recognition) while retaining human strengths (judgment, creativity, contextual understanding).&lt;/p&gt;&lt;p&gt;This philosophy extends to learning and improvement. Current AI models train primarily on static datasets scraped from the internet, supplemented by human preference feedback. But most professional work is qualitative, making it difficult for AI systems to know whether their outputs are actually good without expert evaluation.&lt;/p&gt;&lt;p&gt;&amp;quot;Unless you have this collaboration between the human and the machine, where the human is kind of the teacher and the machine is the student trying to discover new solutions, none of this will be possible,&amp;quot; Rabinovich said. &amp;quot;Upwork is very uniquely positioned to create such an environment because if you try to do this with, say, self-driving cars, and you tell Waymo cars to explore new ways of getting to the airport, like avoiding traffic signs, then a bunch of bad things will happen. In doing work on Upwork, if it creates a wrong website, it doesn&amp;#x27;t cost very much, and there&amp;#x27;s no negative side effects. But the opportunity to learn is absolutely tremendous.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Will AI take your job? The evidence suggests a more complicated answer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While much public discourse around AI focuses on job displacement, Rabinovich argues the historical pattern suggests otherwise ‚Äî though the transition may prove disruptive.&lt;/p&gt;&lt;p&gt;&amp;quot;The narrative in the public is that AI is eliminating jobs, whether it&amp;#x27;s writing, translation, coding or other digital work, but no one really talks about the exponential amount of new types of work that it will create,&amp;quot; he said. &amp;quot;When we invented electricity and steam engines and things like that, they certainly replaced certain jobs, but the amount of new jobs that were introduced is exponentially more, and we think the same is going to happen here.&amp;quot;&lt;/p&gt;&lt;p&gt;The research identifies emerging job categories focused on AI oversight: designing effective human-machine workflows, providing high-quality feedback to improve agent performance, and verifying that AI-generated work meets quality standards. These skills‚Äîprompt engineering, agent supervision, output verification‚Äîbarely existed two years ago but now command premium rates on platforms like Upwork.&lt;/p&gt;&lt;p&gt;&amp;quot;New types of skills from humans are becoming necessary in the form of how to design the interaction between humans and machines, how to guide agents to make them better, and ultimately, how to verify that whatever agentic proposals are being made are actually correct, because that&amp;#x27;s what&amp;#x27;s necessary in order to advance the state of AI,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;The question remains whether this transition‚Äî¬† from doing tasks to overseeing them ‚Äî will create opportunities as quickly as it disrupts existing roles. For freelancers on Upwork, the answer may already be emerging in their bank accounts: The platform saw AI-related work grow 53% year-over-year, even as fears of AI-driven unemployment dominated headlines.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Artificial intelligence agents powered by the world&amp;#x27;s most advanced language models routinely fail to complete even straightforward professional tasks on their own, according to &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;groundbreaking research&lt;/u&gt;&lt;/a&gt; released Thursday by &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, the largest online work marketplace.&lt;/p&gt;&lt;p&gt;But the same study reveals a more promising path forward: When AI agents collaborate with human experts, &lt;a href="https://www.globenewswire.com/news-release/2025/11/13/3187462/0/en/Upwork-Human-Agent-Productivity-Index-Reveals-Up-to-70-Boost-in-Work-Completion-from-Human-and-AI-Agent-Collaboration-vs-Agents-Working-Alone.html"&gt;&lt;u&gt;project completion rates surge by up to 70%&lt;/u&gt;&lt;/a&gt;, suggesting the future of work may not pit humans against machines but rather pair them together in powerful new ways.&lt;/p&gt;&lt;p&gt;The findings, drawn from more than 300 real client projects posted to Upwork&amp;#x27;s platform, marking the first systematic evaluation of how human expertise amplifies AI agent performance in actual professional work ‚Äî not synthetic tests or academic simulations. The research challenges both the hype around fully autonomous AI agents and fears that such technology will imminently replace knowledge workers.&lt;/p&gt;&lt;p&gt;&amp;quot;AI agents aren&amp;#x27;t that agentic, meaning they aren&amp;#x27;t that good,&amp;quot; Andrew Rabinovich, Upwork&amp;#x27;s chief technology officer and head of AI and machine learning, said in an exclusive interview with VentureBeat. &amp;quot;However, when paired with expert human professionals, project completion rates improve dramatically, supporting our firm belief that the future of work will be defined by humans and AI collaborating to get more work done, with human intuition and domain expertise playing a critical role.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How AI agents performed on 300+ real freelance jobs‚Äîand why they struggled&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/human-agent-productivity-index"&gt;&lt;u&gt;Upwork&amp;#x27;s Human+Agent Productivity Index (HAPI) &lt;/u&gt;&lt;/a&gt;evaluated how three leading AI systems ‚Äî &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;, OpenAI&amp;#x27;s &lt;a href="https://openai.com/index/introducing-gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt;, and Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; ‚Äî performed on actual jobs posted by paying clients across categories including writing, data science, web development, engineering, sales, and translation.&lt;/p&gt;&lt;p&gt;Critically, Upwork deliberately selected simple, well-defined projects where AI agents stood a reasonable chance of success. These jobs, priced under $500, represent less than 6% of Upwork&amp;#x27;s total gross services volume ‚Äî a tiny fraction of the platform&amp;#x27;s overall business and an acknowledgment of current AI limitations.&lt;/p&gt;&lt;p&gt;&amp;quot;The reality is that although we study AI, and I&amp;#x27;ve been doing this for 25 years, and we see significant breakthroughs, the reality is that these agents aren&amp;#x27;t that agentic,&amp;quot; Rabinovich told VentureBeat. &amp;quot;So if we go up the value chain, the problems become so much more difficult, then we don&amp;#x27;t think they can solve them at all, even to scratch the surface. So we specifically chose simpler tasks that would give an agent some kind of traction.&amp;quot;&lt;/p&gt;&lt;p&gt;Even on these deliberately simplified tasks, AI agents working independently struggled. But when expert freelancers provided feedback ‚Äî spending an average of just 20 minutes per review cycle ‚Äî the agents&amp;#x27; performance improved substantially with each iteration.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;20 minutes of human feedback boosted AI completion rates up to 70%&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals stark differences in how AI agents perform with and without human guidance across different types of work. For data science and analytics projects, Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; achieved a 64% completion rate working alone but jumped to 93% after receiving feedback from a human expert. In sales and marketing work, &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt;&amp;#x27;s completion rate rose from 17% independently to 31% with human input. OpenAI&amp;#x27;s &lt;a href="https://openai.com/gpt-5/"&gt;&lt;u&gt;GPT-5&lt;/u&gt;&lt;/a&gt; showed similarly dramatic improvements in engineering and architecture tasks, climbing from 30% to 50% completion.&lt;/p&gt;&lt;p&gt;The pattern held across virtually all categories, with agents responding particularly well to human feedback on qualitative, creative work requiring editorial judgment ‚Äî areas like writing, translation, and marketing ‚Äî where completion rates increased by up to 17 percentage points per feedback cycle.&lt;/p&gt;&lt;p&gt;The finding challenges a fundamental assumption in the AI industry: that agent benchmarks conducted in isolation accurately predict real-world performance.&lt;/p&gt;&lt;p&gt;&amp;quot;While we show that in the tasks that we have selected for agents to perform in isolation, they perform similarly to the previous results that we&amp;#x27;ve seen published openly, what we&amp;#x27;ve shown is that in collaboration with humans, the performance of these agents improves surprisingly well,&amp;quot; Rabinovich said. &amp;quot;It&amp;#x27;s not just a one-turn back and forth, but the more feedback the human provides, the better the agent gets at performing.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why ChatGPT can ace the SAT but can&amp;#x27;t count the R&amp;#x27;s in &amp;#x27;strawberry&amp;#x27;&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research arrives as the AI industry grapples with a measurement crisis. Traditional benchmarks ‚Äî standardized tests that AI models can master, sometimes scoring perfectly on SAT exams or mathematics olympiads ‚Äî have proven poor predictors of real-world capability.&lt;/p&gt;&lt;p&gt;&amp;quot;With advances of large language models, what we&amp;#x27;re now seeing is that these static, academic datasets are completely saturated,&amp;quot; Rabinovich said. &amp;quot;So you could get a perfect score in the SAT test or LSAT or any of the math olympiads, and then you would ask ChatGPT how many R&amp;#x27;s there are in the word strawberry, and it would get it wrong.&amp;quot;&lt;/p&gt;&lt;p&gt;This phenomenon ‚Äî where AI systems ace formal tests but stumble on trivial real-world questions ‚Äî has led to growing skepticism about AI capabilities, even as companies race to deploy autonomous agents. Several recent benchmarks from other firms have tested AI agents on Upwork jobs, but those evaluations measured only isolated performance, not the collaborative potential that Upwork&amp;#x27;s research reveals.&lt;/p&gt;&lt;p&gt;&amp;quot;We wanted to evaluate the quality of these agents on actual real work with economic value associated with it, and not only see how well these agents do, but also see how these agents do in collaboration with humans, because we sort of knew already that in isolation, they&amp;#x27;re not that advanced,&amp;quot; Rabinovich explained.&lt;/p&gt;&lt;p&gt;For &lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt;, which connects roughly 800,000 active clients posting more than 3 million jobs annually to a global pool of freelancers, the research serves a strategic business purpose: establishing quality standards for AI agents before allowing them to compete or collaborate with human workers on its platform.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The economics of human-AI teamwork: Why paying for expert feedback still saves money&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite requiring multiple rounds of human feedback ‚Äî each lasting about 20 minutes ‚Äî the time investment remains &amp;quot;orders of magnitude different between a human doing the work alone, versus a human doing the work with an AI agent,&amp;quot; Rabinovich said. Where a project might take a freelancer days to complete independently, the agent-plus-human approach can deliver results in hours through iterative cycles of automated work and expert refinement.&lt;/p&gt;&lt;p&gt;The economic implications extend beyond simple time savings. Upwork recently reported that gross services volume from &lt;a href="https://finance.yahoo.com/news/upworks-stock-soars-q3-blowout-201700500.html"&gt;&lt;u&gt;AI-related work grew 53% year-over-year&lt;/u&gt;&lt;/a&gt; in the third quarter of 2025, one of the strongest growth drivers for the company. But executives have been careful to frame AI not as a replacement for freelancers but as an enhancement to their capabilities.&lt;/p&gt;&lt;p&gt;&amp;quot;AI was a huge overhang for our valuation,&amp;quot; Erica Gessert, Upwork&amp;#x27;s CFO, told &lt;a href="https://www.cfobrew.com/stories/2025/10/16/the-cfo-of-upwork-answers-every-cfo-s-most-burning-question"&gt;&lt;u&gt;CFO Brew&lt;/u&gt;&lt;/a&gt; in October. &amp;quot;There was this belief that all work was going to go away. AI was going to take it, and especially work that&amp;#x27;s done by people like freelancers, because they are impermanent. Actually, the opposite is true.&amp;quot;&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s strategy centers on enabling freelancers to handle more complex, higher-value work by offloading routine tasks to AI. &amp;quot;Freelancers actually prefer to have tools that automate the manual labor and repetitive part of their work, and really focus on the creative and conceptual part of the process,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than replacing jobs, he argues, AI will transform them: &amp;quot;Simpler tasks will be automated by agents, but the jobs will become much more complex in the number of tasks, so the amount of work and therefore earnings for freelancers will actually only go up.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;AI coding agents excel, but creative writing and translation still need humans&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research reveals a clear pattern in agent capabilities. AI systems perform best on &amp;quot;deterministic and verifiable&amp;quot; tasks with objectively correct answers, like solving math problems or writing basic code. &amp;quot;Most coding tasks are very similar to each other,&amp;quot; Rabinovich noted. &amp;quot;That&amp;#x27;s why coding agents are becoming so good.&amp;quot;&lt;/p&gt;&lt;p&gt;In Upwork&amp;#x27;s tests, web development, mobile app development, and data science projects ‚Äî especially those involving structured, computational work ‚Äî saw the highest standalone agent completion rates. Claude &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4&lt;/u&gt;&lt;/a&gt; completed 68% of web development jobs and 64% of data science projects without human help, while &lt;a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/"&gt;&lt;u&gt;Gemini 2.5 Pro&lt;/u&gt;&lt;/a&gt; achieved 74% on certain technical tasks.&lt;/p&gt;&lt;p&gt;But qualitative work proved far more challenging. When asked to create website layouts, write marketing copy, or translate content with appropriate cultural nuance, agents floundered without expert guidance. &amp;quot;When you ask it to write you a poem, the quality of the poem is extremely subjective,&amp;quot; Rabinovich said. &amp;quot;Since the rubrics for evaluation were provided by humans, there&amp;#x27;s some level of variability in representation.&amp;quot;&lt;/p&gt;&lt;p&gt;Writing, translation, and sales and marketing projects showed the most dramatic improvements from human feedback. For writing work, completion rates increased by up to 17 percentage points after expert review. Engineering and architecture projects requiring creative problem-solving ‚Äî like civil engineering or architectural design ‚Äî improved by as much as 23 percentage points with human oversight.&lt;/p&gt;&lt;p&gt;This pattern suggests AI agents excel at pattern matching and replication but struggle with creativity, judgment, and context ‚Äî precisely the skills that define higher-value professional work.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the research: How Upwork tested AI agents with peer-reviewed scientific methods&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://www.upwork.com/"&gt;&lt;u&gt;Upwork&lt;/u&gt;&lt;/a&gt; partnered with elite freelancers on its platform to evaluate every deliverable produced by AI agents, both independently and after each cycle of human feedback. These evaluators created detailed rubrics defining whether projects met core requirements specified in job descriptions, then scored outputs across multiple iterations.&lt;/p&gt;&lt;p&gt;Importantly, evaluators focused only on objective completion criteria, excluding subjective factors like stylistic preferences or quality judgments that might emerge in actual client relationships. &amp;quot;Rubric-based completion rates should not be viewed as a measure of whether an agent would be paid in a real marketplace setting,&amp;quot; &lt;a href="https://www.upwork.com/static/webflow/assets/webflow-human-agent-productivity-index/upbench_paper.pdf"&gt;&lt;u&gt;the research&lt;/u&gt;&lt;/a&gt; notes, &amp;quot;but as an indicator of its ability to fulfill explicitly defined requests.&amp;quot;&lt;/p&gt;&lt;p&gt;This distinction matters: An AI agent might technically complete all specified requirements yet still produce work a client rejects as inadequate. Conversely, subjective client satisfaction ‚Äî the true measure of marketplace success ‚Äî remains beyond current measurement capabilities.&lt;/p&gt;&lt;p&gt;The research underwent double-blind peer review and was accepted to &lt;a href="https://neurips.cc/"&gt;&lt;u&gt;NeurIPS&lt;/u&gt;&lt;/a&gt;, the premier academic conference for AI research, where Upwork will present full results in early December. The company plans to publish a complete methodology and make the benchmark available to the research community, updating the task pool regularly to prevent overfitting as agents improve.&lt;/p&gt;&lt;p&gt;&amp;quot;The idea is for this benchmark to be a living and breathing platform where agents can come in and evaluate themselves on all categories of work, and the tasks that will be offered on the platform will always update, so that these agents don&amp;#x27;t overfit and basically memorize the tasks at hand,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Upwork&amp;#x27;s AI strategy: Building Uma, a &amp;#x27;meta-agent&amp;#x27; that manages human and AI workers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The research directly informs Upwork&amp;#x27;s product roadmap as the company positions itself for what executives call &amp;quot;the age of AI and beyond.&amp;quot; Rather than building its own AI agents to complete specific tasks, &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-evolves-uma-ai-ai-work-agent-advances-human-ai"&gt;&lt;u&gt;Upwork is developing Uma&lt;/u&gt;&lt;/a&gt;, a &amp;quot;meta orchestration agent&amp;quot; that coordinates between human workers, AI systems, and clients.&lt;/p&gt;&lt;p&gt;&amp;quot;Today, Upwork is a marketplace where clients look for freelancers to get work done, and then talent comes to Upwork to find work,&amp;quot; Rabinovich explained. &amp;quot;This is getting expanded into a domain where clients come to Upwork, communicate with Uma, this meta-orchestration agent, and then Uma identifies the necessary talent to get the job done, gets the tasks outcomes completed, and then delivers that to the client.&amp;quot;&lt;/p&gt;&lt;p&gt;In this vision, clients would interact primarily with Uma rather than directly hiring freelancers. The AI system would analyze project requirements, determine which tasks require human expertise versus AI execution, coordinate the workflow, and ensure quality ‚Äî acting as an intelligent project manager rather than a replacement worker.&lt;/p&gt;&lt;p&gt;&amp;quot;We don&amp;#x27;t want to build agents that actually complete the tasks, but we are building this meta orchestration agent that figures out what human and agent talent is necessary in order to complete the tasks,&amp;quot; Rabinovich said. &amp;quot;Uma evaluates the work to be delivered to the client, orchestrates the interaction between humans and agents, and is able to learn from all the interactions that happen on the platform how to break jobs into tasks so that they get completed in a timely and effective manner.&amp;quot;&lt;/p&gt;&lt;p&gt;The company recently &lt;a href="https://investors.upwork.com/news-releases/news-release-details/upwork-announces-forthcoming-lisbon-office-scale-ai-innovation"&gt;&lt;u&gt;announced plans to open its first international office&lt;/u&gt;&lt;/a&gt; in Lisbon, Portugal, by the fourth quarter of 2026, with a focus on AI infrastructure development and technical hiring. The expansion follows Upwork&amp;#x27;s record-breaking third quarter, driven partly by AI-powered product innovation and strong demand for workers with AI skills.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;OpenAI, Anthropic, and Google race to build autonomous agents‚Äîbut reality lags hype&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Upwork&amp;#x27;s findings arrive amid escalating competition in the AI agent space. OpenAI, Anthropic, Google, and numerous startups are racing to develop autonomous agents capable of complex multi-step tasks, from booking travel to analyzing financial data to writing software.&lt;/p&gt;&lt;p&gt;But recent high-profile stumbles have tempered initial enthusiasm. AI agents frequently misunderstand instructions, make logical errors, or produce confidently wrong results ‚Äî a phenomenon researchers call &amp;quot;hallucination.&amp;quot; The gap between controlled demonstration videos and reliable real-world performance remains vast.&lt;/p&gt;&lt;p&gt;&amp;quot;There have been some evaluations that came from OpenAI and other platforms where real Upwork tasks were considered for completion by agents, and across the board, the reported results were not very optimistic, in the sense that they showed that agents‚Äîeven the best ones, meaning powered by most advanced LLMs ‚Äî can&amp;#x27;t really compete with humans that well, because the completion rates are pretty low,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;Rather than waiting for AI to fully mature ‚Äî a timeline that remains uncertain‚ÄîUpwork is betting on a hybrid approach that leverages AI&amp;#x27;s strengths (speed, scalability, pattern recognition) while retaining human strengths (judgment, creativity, contextual understanding).&lt;/p&gt;&lt;p&gt;This philosophy extends to learning and improvement. Current AI models train primarily on static datasets scraped from the internet, supplemented by human preference feedback. But most professional work is qualitative, making it difficult for AI systems to know whether their outputs are actually good without expert evaluation.&lt;/p&gt;&lt;p&gt;&amp;quot;Unless you have this collaboration between the human and the machine, where the human is kind of the teacher and the machine is the student trying to discover new solutions, none of this will be possible,&amp;quot; Rabinovich said. &amp;quot;Upwork is very uniquely positioned to create such an environment because if you try to do this with, say, self-driving cars, and you tell Waymo cars to explore new ways of getting to the airport, like avoiding traffic signs, then a bunch of bad things will happen. In doing work on Upwork, if it creates a wrong website, it doesn&amp;#x27;t cost very much, and there&amp;#x27;s no negative side effects. But the opportunity to learn is absolutely tremendous.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Will AI take your job? The evidence suggests a more complicated answer&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While much public discourse around AI focuses on job displacement, Rabinovich argues the historical pattern suggests otherwise ‚Äî though the transition may prove disruptive.&lt;/p&gt;&lt;p&gt;&amp;quot;The narrative in the public is that AI is eliminating jobs, whether it&amp;#x27;s writing, translation, coding or other digital work, but no one really talks about the exponential amount of new types of work that it will create,&amp;quot; he said. &amp;quot;When we invented electricity and steam engines and things like that, they certainly replaced certain jobs, but the amount of new jobs that were introduced is exponentially more, and we think the same is going to happen here.&amp;quot;&lt;/p&gt;&lt;p&gt;The research identifies emerging job categories focused on AI oversight: designing effective human-machine workflows, providing high-quality feedback to improve agent performance, and verifying that AI-generated work meets quality standards. These skills‚Äîprompt engineering, agent supervision, output verification‚Äîbarely existed two years ago but now command premium rates on platforms like Upwork.&lt;/p&gt;&lt;p&gt;&amp;quot;New types of skills from humans are becoming necessary in the form of how to design the interaction between humans and machines, how to guide agents to make them better, and ultimately, how to verify that whatever agentic proposals are being made are actually correct, because that&amp;#x27;s what&amp;#x27;s necessary in order to advance the state of AI,&amp;quot; Rabinovich said.&lt;/p&gt;&lt;p&gt;The question remains whether this transition‚Äî¬† from doing tasks to overseeing them ‚Äî will create opportunities as quickly as it disrupts existing roles. For freelancers on Upwork, the answer may already be emerging in their bank accounts: The platform saw AI-related work grow 53% year-over-year, even as fears of AI-driven unemployment dominated headlines.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/upwork-study-shows-ai-agents-excel-with-human-partners-but-fail</guid><pubDate>Thu, 13 Nov 2025 18:30:00 +0000</pubDate></item><item><title>[NEW] What‚Äôs Next for AI? (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/13/1127930/whats-next-for-ai-2/</link><description></description><guid isPermaLink="false">https://www.technologyreview.com/2025/11/13/1127930/whats-next-for-ai-2/</guid><pubDate>Thu, 13 Nov 2025 18:33:33 +0000</pubDate></item><item><title>[NEW] AI On: 3 Ways to Bring Agentic AI to Computer Vision Applications (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ways-to-bring-agentic-ai-to-computer-vision-applications/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor‚Äôs note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Today‚Äôs computer vision systems excel at identifying what happens in physical spaces and processes, but lack the abilities to explain the details of a scene and why they matter, as well as reason about what might happen next.&lt;/p&gt;
&lt;p&gt;Agentic intelligence powered by vision language models (VLMs) can help bridge this gap, giving teams quick, easy access to key insights and analyses that connect text descriptors with spatial-temporal information and billions of visual data points captured by their systems every day.&lt;/p&gt;
&lt;p&gt;Three approaches organizations can use to boost their legacy computer vision systems with agentic intelligence are to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply dense captioning for searchable visual content.&lt;/li&gt;
&lt;li&gt;Augment system alerts with detailed context.&lt;/li&gt;
&lt;li&gt;Use AI reasoning to summarize information from complex scenarios and answer questions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Making Visual Content Searchable With Dense Captions&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Traditional convolutional neural network (CNN)-powered video search tools are constrained by limited training, context and semantics, making gleaning insights manual, tedious and time-consuming. CNNs are tuned to perform specific visual tasks, like spotting an anomaly, and lack the multimodal ability to translate what they see into text.&lt;/p&gt;
&lt;p&gt;Businesses can embed VLMs directly into their existing applications to generate highly detailed captions of images and videos. These captions turn unstructured content into rich, searchable metadata, enabling visual search that‚Äôs far more flexible ‚Äî not constrained by file names or basic tags.&lt;/p&gt;
&lt;p&gt;For example, automated vehicle-inspection system UVeye processes over 700 million high-resolution images each month to build one of the world‚Äôs largest vehicle and component datasets. By applying VLMs, UVeye converts this visual data into structured condition reports, detecting subtle defects, modifications or foreign objects with exceptional accuracy and reliability for search.&lt;/p&gt;
&lt;p&gt;VLM-powered visual understanding adds essential context, ensuring transparent, consistent insights for compliance, safety and quality control. UVeye detects 96% of defects compared with 24% using manual methods, enabling early intervention to reduce downtime and control maintenance costs.&lt;/p&gt;

&lt;p&gt;Relo Metrics, a provider of AI-powered sports marketing measurement, helps brands quantify the value of their media investments and optimize their spending. By combining VLMs with computer vision, Relo Metrics moves beyond basic logo detection to capture context ‚Äî like a courtside banner shown during a game-winning shot ‚Äî and translate it into real-time monetary value.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87329 size-medium" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/relo-metrics-960x720.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;This contextual-insight capability highlights when and how logos appear, especially in high-impact moments, giving marketers a clearer view of return on investment and ways to optimize strategy. For example, Stanley Black &amp;amp; Decker, including its Dewalt brand, previously relied on end-of-season reports to evaluate sponsor asset performance, limiting timely decision-making. Using Relo Metrics for real-time insights, Stanley Black &amp;amp; Decker adjusted signage positioning and saved $1.3 million in potentially lost sponsor media value.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Augmenting Computer Vision System Alerts With VLM Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CNN-based computer vision systems often generate binary detection alerts such as yes or no, and true or false. Without the reasoning power of VLMs, that can mean false positives and missed details ‚Äî leading to costly mistakes in safety and security, as well as lost business intelligence.Rather than replacing these CNN-based computer vision systems entirely, VLMs can easily augment these systems as an intelligent add-on. With a VLM layered on top of CNN-based computer vision systems, detection alerts are not only flagged but reviewed with contextual understanding ‚Äî explaining where, how and why the incident occurred.&lt;/p&gt;
&lt;p&gt;For smarter city traffic management, Linker Vision uses VLMs to verify critical city alerts, such as traffic accidents, flooding, or falling poles and trees from storms. This reduces false positives and adds vital context to each event to improve real-time municipal response.&lt;/p&gt;

&lt;p&gt;Linker Vision‚Äôs architecture for agentic AI involves automating event analysis from over 50,000 diverse smart city camera streams to enable cross-department remediation ‚Äî coordinating actions across teams like traffic control, utilities and first responders when incidents occur. The ability to query across all camera streams simultaneously enables systems to quickly and automatically turn observations into insights and trigger recommendations for next best actions.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Automatic Analysis of Complex Scenarios With Agentic AI&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Agentic AI systems can process, reason and answer complex queries across video streams and modalities ‚Äî such as audio, text, video and sensor data. This is possible by combining VLMs with reasoning models, large language models (LLMs), retrieval-augmented generation (RAG), computer vision and speech transcription.&lt;/p&gt;
&lt;p&gt;Basic integration of a VLM into an existing computer vision pipeline is helpful in verifying short video clips of key moments. However this approach is limited by how many visual tokens a single model can process at once, resulting in surface-level answers without context over longer time periods and external knowledge.&lt;/p&gt;
&lt;p&gt;In contrast, whole architectures built on agentic AI enable scalable, accurate processing of lengthy and multichannel video archives. This leads to deeper, more accurate and more reliable insights that go beyond surface-level understanding. Agentic systems can be used for root-cause analysis or analysis of long inspection videos to generate reports with timestamped insights.&lt;/p&gt;
&lt;p&gt;Levatas develops visual-inspection solutions that use mobile robots and autonomous systems to enhance safety, reliability and performance of critical infrastructure assets such as electric utility substations, fuel terminals, rail yards and logistics hubs. Using VLMs, Levatas built a video analytics AI agent to automatically review inspection footage and draft detailed inspection reports, dramatically accelerating a traditionally manual and slow process.&lt;/p&gt;
&lt;p&gt;For customers like American Electric Power (AEP), Levatas AI integrates with Skydio X10 devices to streamline inspection of electric infrastructure. Levatas enables AEP to autonomously inspect power poles, identify thermal issues and detect equipment damage. Alerts are sent instantly to the AEP team upon issue detection, enabling swift response and resolution, and ensuring reliable, clean and affordable energy delivery.&lt;/p&gt;

&lt;p&gt;AI gaming highlight tools like Eklipse use VLM-powered agents to enrich livestreams of video games with captions and index metadata for rapid querying, summarization and creation of polished highlight reels in minutes ‚Äî 10x faster than legacy solutions ‚Äî leading to improved content consumption experiences.&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Powering Agentic Video Intelligence With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For advanced search and reasoning, developers can use multimodal VLMs such as NVCLIP, NVIDIA Cosmos Reason and Nemotron Nano V2 to build metadata-rich indexes for search.&lt;/p&gt;
&lt;p&gt;To integrate VLMs into computer vision applications, developers can use the event reviewer feature in the NVIDIA Blueprint for video search and summarization (VSS), part of the NVIDIA Metropolis platform.&lt;/p&gt;
&lt;p&gt;For more complex queries and summarization tasks, the VSS blueprint can be customized so developers can build AI agents that access VLMs directly or use VLMs in conjunction with LLMs, RAG and computer vision models. This enables smarter operations, richer video analytics and real-time process compliance that scale with organizational needs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about NVIDIA-powered &lt;/i&gt;&lt;i&gt;agentic video analytics&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to NVIDIA‚Äôs vision AI newsletter, &lt;/i&gt;&lt;i&gt;joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;VLM tech blogs&lt;/i&gt;&lt;i&gt;, and &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor‚Äôs note: This post is part of the &lt;/i&gt;&lt;i&gt;AI On&lt;/i&gt;&lt;i&gt; blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Today‚Äôs computer vision systems excel at identifying what happens in physical spaces and processes, but lack the abilities to explain the details of a scene and why they matter, as well as reason about what might happen next.&lt;/p&gt;
&lt;p&gt;Agentic intelligence powered by vision language models (VLMs) can help bridge this gap, giving teams quick, easy access to key insights and analyses that connect text descriptors with spatial-temporal information and billions of visual data points captured by their systems every day.&lt;/p&gt;
&lt;p&gt;Three approaches organizations can use to boost their legacy computer vision systems with agentic intelligence are to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply dense captioning for searchable visual content.&lt;/li&gt;
&lt;li&gt;Augment system alerts with detailed context.&lt;/li&gt;
&lt;li&gt;Use AI reasoning to summarize information from complex scenarios and answer questions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;strong&gt;Making Visual Content Searchable With Dense Captions&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Traditional convolutional neural network (CNN)-powered video search tools are constrained by limited training, context and semantics, making gleaning insights manual, tedious and time-consuming. CNNs are tuned to perform specific visual tasks, like spotting an anomaly, and lack the multimodal ability to translate what they see into text.&lt;/p&gt;
&lt;p&gt;Businesses can embed VLMs directly into their existing applications to generate highly detailed captions of images and videos. These captions turn unstructured content into rich, searchable metadata, enabling visual search that‚Äôs far more flexible ‚Äî not constrained by file names or basic tags.&lt;/p&gt;
&lt;p&gt;For example, automated vehicle-inspection system UVeye processes over 700 million high-resolution images each month to build one of the world‚Äôs largest vehicle and component datasets. By applying VLMs, UVeye converts this visual data into structured condition reports, detecting subtle defects, modifications or foreign objects with exceptional accuracy and reliability for search.&lt;/p&gt;
&lt;p&gt;VLM-powered visual understanding adds essential context, ensuring transparent, consistent insights for compliance, safety and quality control. UVeye detects 96% of defects compared with 24% using manual methods, enabling early intervention to reduce downtime and control maintenance costs.&lt;/p&gt;

&lt;p&gt;Relo Metrics, a provider of AI-powered sports marketing measurement, helps brands quantify the value of their media investments and optimize their spending. By combining VLMs with computer vision, Relo Metrics moves beyond basic logo detection to capture context ‚Äî like a courtside banner shown during a game-winning shot ‚Äî and translate it into real-time monetary value.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87329 size-medium" height="720" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/relo-metrics-960x720.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;This contextual-insight capability highlights when and how logos appear, especially in high-impact moments, giving marketers a clearer view of return on investment and ways to optimize strategy. For example, Stanley Black &amp;amp; Decker, including its Dewalt brand, previously relied on end-of-season reports to evaluate sponsor asset performance, limiting timely decision-making. Using Relo Metrics for real-time insights, Stanley Black &amp;amp; Decker adjusted signage positioning and saved $1.3 million in potentially lost sponsor media value.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Augmenting Computer Vision System Alerts With VLM Reasoning&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;CNN-based computer vision systems often generate binary detection alerts such as yes or no, and true or false. Without the reasoning power of VLMs, that can mean false positives and missed details ‚Äî leading to costly mistakes in safety and security, as well as lost business intelligence.Rather than replacing these CNN-based computer vision systems entirely, VLMs can easily augment these systems as an intelligent add-on. With a VLM layered on top of CNN-based computer vision systems, detection alerts are not only flagged but reviewed with contextual understanding ‚Äî explaining where, how and why the incident occurred.&lt;/p&gt;
&lt;p&gt;For smarter city traffic management, Linker Vision uses VLMs to verify critical city alerts, such as traffic accidents, flooding, or falling poles and trees from storms. This reduces false positives and adds vital context to each event to improve real-time municipal response.&lt;/p&gt;

&lt;p&gt;Linker Vision‚Äôs architecture for agentic AI involves automating event analysis from over 50,000 diverse smart city camera streams to enable cross-department remediation ‚Äî coordinating actions across teams like traffic control, utilities and first responders when incidents occur. The ability to query across all camera streams simultaneously enables systems to quickly and automatically turn observations into insights and trigger recommendations for next best actions.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Automatic Analysis of Complex Scenarios With Agentic AI&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Agentic AI systems can process, reason and answer complex queries across video streams and modalities ‚Äî such as audio, text, video and sensor data. This is possible by combining VLMs with reasoning models, large language models (LLMs), retrieval-augmented generation (RAG), computer vision and speech transcription.&lt;/p&gt;
&lt;p&gt;Basic integration of a VLM into an existing computer vision pipeline is helpful in verifying short video clips of key moments. However this approach is limited by how many visual tokens a single model can process at once, resulting in surface-level answers without context over longer time periods and external knowledge.&lt;/p&gt;
&lt;p&gt;In contrast, whole architectures built on agentic AI enable scalable, accurate processing of lengthy and multichannel video archives. This leads to deeper, more accurate and more reliable insights that go beyond surface-level understanding. Agentic systems can be used for root-cause analysis or analysis of long inspection videos to generate reports with timestamped insights.&lt;/p&gt;
&lt;p&gt;Levatas develops visual-inspection solutions that use mobile robots and autonomous systems to enhance safety, reliability and performance of critical infrastructure assets such as electric utility substations, fuel terminals, rail yards and logistics hubs. Using VLMs, Levatas built a video analytics AI agent to automatically review inspection footage and draft detailed inspection reports, dramatically accelerating a traditionally manual and slow process.&lt;/p&gt;
&lt;p&gt;For customers like American Electric Power (AEP), Levatas AI integrates with Skydio X10 devices to streamline inspection of electric infrastructure. Levatas enables AEP to autonomously inspect power poles, identify thermal issues and detect equipment damage. Alerts are sent instantly to the AEP team upon issue detection, enabling swift response and resolution, and ensuring reliable, clean and affordable energy delivery.&lt;/p&gt;

&lt;p&gt;AI gaming highlight tools like Eklipse use VLM-powered agents to enrich livestreams of video games with captions and index metadata for rapid querying, summarization and creation of polished highlight reels in minutes ‚Äî 10x faster than legacy solutions ‚Äî leading to improved content consumption experiences.&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Powering Agentic Video Intelligence With NVIDIA Technologies&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For advanced search and reasoning, developers can use multimodal VLMs such as NVCLIP, NVIDIA Cosmos Reason and Nemotron Nano V2 to build metadata-rich indexes for search.&lt;/p&gt;
&lt;p&gt;To integrate VLMs into computer vision applications, developers can use the event reviewer feature in the NVIDIA Blueprint for video search and summarization (VSS), part of the NVIDIA Metropolis platform.&lt;/p&gt;
&lt;p&gt;For more complex queries and summarization tasks, the VSS blueprint can be customized so developers can build AI agents that access VLMs directly or use VLMs in conjunction with LLMs, RAG and computer vision models. This enables smarter operations, richer video analytics and real-time process compliance that scale with organizational needs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about NVIDIA-powered &lt;/i&gt;&lt;i&gt;agentic video analytics&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date by subscribing to NVIDIA‚Äôs vision AI newsletter, &lt;/i&gt;&lt;i&gt;joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore the &lt;/i&gt;&lt;i&gt;VLM tech blogs&lt;/i&gt;&lt;i&gt;, and &lt;/i&gt;&lt;i&gt;self-paced video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ways-to-bring-agentic-ai-to-computer-vision-applications/</guid><pubDate>Thu, 13 Nov 2025 18:50:06 +0000</pubDate></item><item><title>[NEW] Separating natural forests from other tree cover with AI for deforestation-free supply chains (The latest research from Google)</title><link>https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, M√©lanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).&lt;/i&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, M√©lanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/</guid><pubDate>Thu, 13 Nov 2025 19:04:07 +0000</pubDate></item><item><title>[NEW] Baidu unveils proprietary ERNIE 5 beating GPT-5 performance on charts, document understanding and more (AI | VentureBeat)</title><link>https://venturebeat.com/ai/baidu-unveils-proprietary-ernie-5-beating-gpt-5-performance-on-charts</link><description>[unable to retrieve full-text content]&lt;p&gt;Mere hours after OpenAI updated its flagship foundation model &lt;a href="https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5"&gt;GPT-5 to GPT-5.1&lt;/a&gt;, promising reduced token usage overall and a more pleasant personality with more preset options, Chinese search giant &lt;a href="https://www.prnewswire.com/news-releases/baidu-unveils-ernie-5-0-and-a-series-of-ai-applications-at-baidu-world-2025--ramps-up-global-push-302614531.html?tc=eml_cleartime"&gt;Baidu unveiled its next-generation foundation model, ERNIE 5.0,&lt;/a&gt; alongside a suite of AI product upgrades and strategic international expansions.&lt;/p&gt;&lt;p&gt;The goal: to position as a global contender in the increasingly competitive enterprise AI market.&lt;/p&gt;&lt;p&gt;Announced at the company&amp;#x27;s Baidu World 2025 event, ERNIE 5.0 is a proprietary, natively omni-modal model designed to jointly process and generate content across text, images, audio, and video. &lt;/p&gt;&lt;p&gt;Unlike Baidu‚Äôs recently released &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;, which is open source under an enterprise-friendly and permissive Apache 2.0 license, ERNIE 5.0 is a proprietary model and is available only via &lt;a href="https://ernie.baidu.com/"&gt;Baidu‚Äôs ERNIE Bot&lt;/a&gt; website (I needed to select it manuallyu from the model picker dropdown) and the &lt;a href="https://cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan cloud platform application programming interface (API) for enterprise customers. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;Alongside the model launch, Baidu introduced major updates to its digital human platform, no-code tools, and general-purpose AI agents ‚Äî all targeted at expanding its AI footprint beyond China.&lt;/p&gt;&lt;p&gt;The company also introduced ERNIE 5.0 Preview 1022, a variant optimized for text-intensive tasks, alongside the general preview model that balances across modalities.&lt;/p&gt;&lt;p&gt;Baidu emphasized that ERNIE 5.0 represents a shift in how intelligence is deployed at scale, with CEO Robin Li stating: ‚ÄúWhen you internalize AI, it becomes a native capability and transforms intelligence from a cost into a source of productivity.‚Äù&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where ERNIE 5.0 outshines GPT-5 and Gemini 2.5 Pro&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0‚Äôs benchmark results suggest that Baidu has achieved parity‚Äîor near-parity‚Äîwith the top Western foundation models across a wide spectrum of tasks. &lt;/p&gt;&lt;p&gt;In public benchmark slides shared during the Baidu World 2025 event, ERNIE 5.0 Preview outperformed or matched OpenAI‚Äôs GPT-5-High and Google‚Äôs Gemini 2.5 Pro in &lt;b&gt;multimodal reasoning, document understanding, and image-based QA&lt;/b&gt;, while also &lt;b&gt;demonstrating strong language modeling and code execution abilities. &lt;/b&gt;&lt;/p&gt;&lt;p&gt;The company emphasized its ability to handle joint inputs and outputs across modalities, rather than relying on post-hoc modality fusion, which it framed as a technical differentiator.&lt;/p&gt;&lt;p&gt;On visual tasks, ERNIE 5.0 achieved leading scores on OCRBench, DocVQA, and ChartQA, three benchmarks that test document recognition, comprehension, and structured data reasoning. &lt;/p&gt;&lt;p&gt;Baidu claims the model beat both GPT-5-High and Gemini 2.5 Pro on these document and chart-based benchmarks, areas it describes as core to enterprise applications like automated document processing and financial analysis. &lt;/p&gt;&lt;p&gt;In image generation, ERNIE 5.0 tied or exceeded Google‚Äôs Veo3 across categories including semantic alignment and image quality, according to Baidu‚Äôs internal GenEval-based evaluation. Baidu claimed that the model‚Äôs multimodal integration allows it to generate and interpret visual content with greater contextual awareness than models relying on modality-specific encoders.&lt;/p&gt;&lt;p&gt;For audio and speech tasks, ERNIE 5.0 demonstrated competitive results on MM-AU and TUT2017 audio understanding benchmarks, as well as question answering from spoken language inputs. Its audio performance, while not as heavily emphasized as vision or text, suggests a broad capability footprint intended to support full-spectrum multimodal applications.&lt;/p&gt;&lt;p&gt;In language tasks, the model showed strong results on instruction following, factual question answering, and mathematical reasoning‚Äîcore areas that define the enterprise utility of large language models. &lt;/p&gt;&lt;p&gt;The Preview 1022 variant of ERNIE 5.0, tailored for textual performance, showed even stronger language-specific results in early developer access. While Baidu does not claim broad superiority in general language reasoning, its internal evaluations suggest that ERNIE 5.0 Preview 1022 closes the gap with top-tier English-language models and outperforms them in Chinese-language performance.&lt;/p&gt;&lt;p&gt;While Baidu did not release full benchmark details or raw scores publicly, its performance positioning suggests a deliberate attempt to frame ERNIE 5.0 not as a niche multimodal system but as a flagship model competitive with the largest closed models in general-purpose reasoning. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Where Baidu claims a clear lead is in structured document understanding, visual chart reasoning, and integration of multiple modalities into a single, native modeling architecture&lt;/b&gt;. Independent verification of these results remains pending, but the breadth of claimed capabilities positions ERNIE 5.0 as a serious alternative in the multimodal foundation model landscape.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise Pricing Strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0 is positioned at the &lt;b&gt;premium end&lt;/b&gt; of Baidu‚Äôs model pricing structure. The company has released specific pricing for API usage on its Qianfan platform, aligning the cost with other top-tier offerings from Chinese competitors like Alibaba.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;ERNIE 5.0&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¬•0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¬•0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00011 (¬•0.0008)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00045 (¬•0.0032)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen3 (Coder ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¬•0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¬•0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The contrast in cost between ERNIE 5.0 and earlier models such as ERNIE 4.5 Turbo underscores Baidu‚Äôs strategy to differentiate between high-volume, low-cost models and high-capability models designed for complex tasks and multimodal reasoning.&lt;/p&gt;&lt;p&gt;Compared to other U.S. alternatives, it remains mid-range in pricing:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/?utm_source=chatgpt.com"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/claude/opus?utm_source=chatgpt.com"&gt;Anthropic&lt;/a&gt; &lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25 (‚â§200k) / $2.50 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00 (‚â§200k) / $15.00 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.google.com/vertex-ai/generative-ai/pricing"&gt;Google Vertex AI Pricing&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (grok-4-0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models/grok-4-0709?utm_source=chatgpt.com"&gt; xAI API&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;&lt;b&gt;Global Expansion: Products and Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In tandem with the model release, Baidu is expanding internationally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GenFlow 3.0&lt;/b&gt;, now with 20M+ users, is the company‚Äôs largest general-purpose AI agent and features enhanced memory and multimodal task handling.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Famou&lt;/b&gt;, a self-evolving agent capable of dynamically solving complex problems, is now commercially available via invite.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MeDo&lt;/b&gt;, the international version of Baidu‚Äôs no-code builder Miaoda, is live globally via &lt;a href="https://medo.dev"&gt;medo.dev&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Oreate&lt;/b&gt;, a productivity workspace with document, slide, image, video, and podcast support, has reached over 1.2M users worldwide.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Baidu‚Äôs digital human platform, already rolled out in Brazil, is also part of the global push. According to company data, 83% of livestreamers during this year‚Äôs ‚ÄúDouble 11‚Äù shopping event in China used Baidu‚Äôs digital human tech, contributing to a 91% increase in GMV.&lt;/p&gt;&lt;p&gt;Meanwhile, Baidu‚Äôs autonomous ride-hailing service Apollo Go has surpassed 17 million rides, operating driverless fleets in 22 cities and claiming the title of the world‚Äôs largest robotaxi network.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open-Source Vision-Language Model Garners Industry Attention&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Two days before the flagship ERNIE 5.0 event, Baidu also released an open-source multimodal model under the Apache 2.0 license: &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;As &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;reported by my colleague Michael Nu√±ez at VentureBeat&lt;/a&gt;, the model activates just 3 billion parameters while maintaining a total of 28 billion, using a Mixture-of-Experts (MoE) architecture for efficient inference.&lt;/p&gt;&lt;p&gt;Key technical innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;‚ÄúThinking with Images‚Äù, which enables dynamic zoom-based visual analysis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Support for chart interpretation, document understanding, visual grounding, and temporal awareness in video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Runtime on a single 80GB GPU, making it accessible to mid-sized organizations&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full compatibility with Transformers, vLLM, and Baidu‚Äôs FastDeploy toolkits&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This release adds pressure on closed-source competitors. With Apache 2.0 licensing, ERNIE-4.5-VL-28B-A3B-Thinking becomes a viable foundation model for commercial applications without licensing restrictions ‚Äî something few high-performing models in this class offer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community Feedback and Baidu‚Äôs Response&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Following the launch of ERNIE 5.0, developer and AI evaluator Lisan al Gaib (@scaling01) &lt;a href="https://x.com/scaling01/status/1988961630273646872"&gt;posted a mixed review on X.&lt;/a&gt; While initially impressed by the model‚Äôs benchmark performance, they reported a persistent issue where ERNIE 5.0 would repeatedly invoke tools ‚Äî even when explicitly instructed not to ‚Äî during SVG generation tasks.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;‚ÄúERNIE 5.0 benchmarks looked insane until I tested it‚Ä¶ unfortunately it‚Äôs RL braindamaged or they have a serious issue with their chat platform / system prompt,‚Äù Lisan wrote.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In a matter of hours, Baidu‚Äôs developer-focused support account, &lt;a href="https://x.com/ErnieforDevs/status/1989001980430393688"&gt;@ErnieforDevs, responded&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;‚ÄúThanks for the feedback! It‚Äôs a known bug ‚Äî certain syntax can consistently trigger it. We‚Äôre working on a fix. You can try rephrasing or changing the prompt to avoid it for now.‚Äù&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The quick turnaround reflects Baidu‚Äôs increasing emphasis on developer communication, especially as it courts international users through both proprietary and open-source offerings.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Outlook for Baidu and its ERNIE foundational LLM family&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu‚Äôs ERNIE 5.0 marks a strategic escalation in the global foundation model race. With performance claims that put it on par with the most advanced systems from OpenAI and Google, and a mix of premium pricing and open-access alternatives, Baidu is signaling its ambition to become not just a domestic AI leader, but a credible global infrastructure provider.&lt;/p&gt;&lt;p&gt;At a time when enterprise AI users are increasingly demanding multimodal performance, flexible licensing, and deployment efficiency, Baidu‚Äôs two-track approach‚Äîpremium hosted APIs and open-source releases‚Äîmay broaden its appeal across both corporate and developer communities.&lt;/p&gt;&lt;p&gt;Whether the company‚Äôs performance claims hold up under third-party testing remains to be seen. But in a landscape shaped by rising costs, model complexity, and compute bottlenecks, ERNIE 5.0 and its supporting ecosystem give Baidu a competitive position in the next wave of AI deployment.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Mere hours after OpenAI updated its flagship foundation model &lt;a href="https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5"&gt;GPT-5 to GPT-5.1&lt;/a&gt;, promising reduced token usage overall and a more pleasant personality with more preset options, Chinese search giant &lt;a href="https://www.prnewswire.com/news-releases/baidu-unveils-ernie-5-0-and-a-series-of-ai-applications-at-baidu-world-2025--ramps-up-global-push-302614531.html?tc=eml_cleartime"&gt;Baidu unveiled its next-generation foundation model, ERNIE 5.0,&lt;/a&gt; alongside a suite of AI product upgrades and strategic international expansions.&lt;/p&gt;&lt;p&gt;The goal: to position as a global contender in the increasingly competitive enterprise AI market.&lt;/p&gt;&lt;p&gt;Announced at the company&amp;#x27;s Baidu World 2025 event, ERNIE 5.0 is a proprietary, natively omni-modal model designed to jointly process and generate content across text, images, audio, and video. &lt;/p&gt;&lt;p&gt;Unlike Baidu‚Äôs recently released &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;, which is open source under an enterprise-friendly and permissive Apache 2.0 license, ERNIE 5.0 is a proprietary model and is available only via &lt;a href="https://ernie.baidu.com/"&gt;Baidu‚Äôs ERNIE Bot&lt;/a&gt; website (I needed to select it manuallyu from the model picker dropdown) and the &lt;a href="https://cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan cloud platform application programming interface (API) for enterprise customers. &lt;/a&gt;&lt;/p&gt;&lt;p&gt;Alongside the model launch, Baidu introduced major updates to its digital human platform, no-code tools, and general-purpose AI agents ‚Äî all targeted at expanding its AI footprint beyond China.&lt;/p&gt;&lt;p&gt;The company also introduced ERNIE 5.0 Preview 1022, a variant optimized for text-intensive tasks, alongside the general preview model that balances across modalities.&lt;/p&gt;&lt;p&gt;Baidu emphasized that ERNIE 5.0 represents a shift in how intelligence is deployed at scale, with CEO Robin Li stating: ‚ÄúWhen you internalize AI, it becomes a native capability and transforms intelligence from a cost into a source of productivity.‚Äù&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where ERNIE 5.0 outshines GPT-5 and Gemini 2.5 Pro&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0‚Äôs benchmark results suggest that Baidu has achieved parity‚Äîor near-parity‚Äîwith the top Western foundation models across a wide spectrum of tasks. &lt;/p&gt;&lt;p&gt;In public benchmark slides shared during the Baidu World 2025 event, ERNIE 5.0 Preview outperformed or matched OpenAI‚Äôs GPT-5-High and Google‚Äôs Gemini 2.5 Pro in &lt;b&gt;multimodal reasoning, document understanding, and image-based QA&lt;/b&gt;, while also &lt;b&gt;demonstrating strong language modeling and code execution abilities. &lt;/b&gt;&lt;/p&gt;&lt;p&gt;The company emphasized its ability to handle joint inputs and outputs across modalities, rather than relying on post-hoc modality fusion, which it framed as a technical differentiator.&lt;/p&gt;&lt;p&gt;On visual tasks, ERNIE 5.0 achieved leading scores on OCRBench, DocVQA, and ChartQA, three benchmarks that test document recognition, comprehension, and structured data reasoning. &lt;/p&gt;&lt;p&gt;Baidu claims the model beat both GPT-5-High and Gemini 2.5 Pro on these document and chart-based benchmarks, areas it describes as core to enterprise applications like automated document processing and financial analysis. &lt;/p&gt;&lt;p&gt;In image generation, ERNIE 5.0 tied or exceeded Google‚Äôs Veo3 across categories including semantic alignment and image quality, according to Baidu‚Äôs internal GenEval-based evaluation. Baidu claimed that the model‚Äôs multimodal integration allows it to generate and interpret visual content with greater contextual awareness than models relying on modality-specific encoders.&lt;/p&gt;&lt;p&gt;For audio and speech tasks, ERNIE 5.0 demonstrated competitive results on MM-AU and TUT2017 audio understanding benchmarks, as well as question answering from spoken language inputs. Its audio performance, while not as heavily emphasized as vision or text, suggests a broad capability footprint intended to support full-spectrum multimodal applications.&lt;/p&gt;&lt;p&gt;In language tasks, the model showed strong results on instruction following, factual question answering, and mathematical reasoning‚Äîcore areas that define the enterprise utility of large language models. &lt;/p&gt;&lt;p&gt;The Preview 1022 variant of ERNIE 5.0, tailored for textual performance, showed even stronger language-specific results in early developer access. While Baidu does not claim broad superiority in general language reasoning, its internal evaluations suggest that ERNIE 5.0 Preview 1022 closes the gap with top-tier English-language models and outperforms them in Chinese-language performance.&lt;/p&gt;&lt;p&gt;While Baidu did not release full benchmark details or raw scores publicly, its performance positioning suggests a deliberate attempt to frame ERNIE 5.0 not as a niche multimodal system but as a flagship model competitive with the largest closed models in general-purpose reasoning. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Where Baidu claims a clear lead is in structured document understanding, visual chart reasoning, and integration of multiple modalities into a single, native modeling architecture&lt;/b&gt;. Independent verification of these results remains pending, but the breadth of claimed capabilities positions ERNIE 5.0 as a serious alternative in the multimodal foundation model landscape.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Enterprise Pricing Strategy&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;ERNIE 5.0 is positioned at the &lt;b&gt;premium end&lt;/b&gt; of Baidu‚Äôs model pricing structure. The company has released specific pricing for API usage on its Qianfan platform, aligning the cost with other top-tier offerings from Chinese competitors like Alibaba.&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output Cost (per 1K tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;ERNIE 5.0&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¬•0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¬•0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00011 (¬•0.0008)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00045 (¬•0.0032)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen3 (Coder ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.00085 (¬•0.006)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.0034 (¬•0.024)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The contrast in cost between ERNIE 5.0 and earlier models such as ERNIE 4.5 Turbo underscores Baidu‚Äôs strategy to differentiate between high-volume, low-cost models and high-capability models designed for complex tasks and multimodal reasoning.&lt;/p&gt;&lt;p&gt;Compared to other U.S. alternatives, it remains mid-range in pricing:&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output (/1 M tokens)&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/api/pricing/?utm_source=chatgpt.com"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo (ex.)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.baidu.com/doc/qianfan/s/wmh4sv6ya"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/claude/opus?utm_source=chatgpt.com"&gt;Anthropic&lt;/a&gt; &lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25 (‚â§200k) / $2.50 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00 (‚â§200k) / $15.00 (&amp;gt;200k)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ftr.bazqux.com/cloud.google.com/vertex-ai/generative-ai/pricing"&gt;Google Vertex AI Pricing&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (grok-4-0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models/grok-4-0709?utm_source=chatgpt.com"&gt; xAI API&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;&lt;b&gt;Global Expansion: Products and Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;In tandem with the model release, Baidu is expanding internationally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GenFlow 3.0&lt;/b&gt;, now with 20M+ users, is the company‚Äôs largest general-purpose AI agent and features enhanced memory and multimodal task handling.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Famou&lt;/b&gt;, a self-evolving agent capable of dynamically solving complex problems, is now commercially available via invite.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;MeDo&lt;/b&gt;, the international version of Baidu‚Äôs no-code builder Miaoda, is live globally via &lt;a href="https://medo.dev"&gt;medo.dev&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Oreate&lt;/b&gt;, a productivity workspace with document, slide, image, video, and podcast support, has reached over 1.2M users worldwide.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Baidu‚Äôs digital human platform, already rolled out in Brazil, is also part of the global push. According to company data, 83% of livestreamers during this year‚Äôs ‚ÄúDouble 11‚Äù shopping event in China used Baidu‚Äôs digital human tech, contributing to a 91% increase in GMV.&lt;/p&gt;&lt;p&gt;Meanwhile, Baidu‚Äôs autonomous ride-hailing service Apollo Go has surpassed 17 million rides, operating driverless fleets in 22 cities and claiming the title of the world‚Äôs largest robotaxi network.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Open-Source Vision-Language Model Garners Industry Attention&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Two days before the flagship ERNIE 5.0 event, Baidu also released an open-source multimodal model under the Apache 2.0 license: &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;ERNIE-4.5-VL-28B-A3B-Thinking&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;As &lt;a href="https://venturebeat.com/ai/baidu-just-dropped-an-open-source-multimodal-ai-that-it-claims-beats-gpt-5"&gt;reported by my colleague Michael Nu√±ez at VentureBeat&lt;/a&gt;, the model activates just 3 billion parameters while maintaining a total of 28 billion, using a Mixture-of-Experts (MoE) architecture for efficient inference.&lt;/p&gt;&lt;p&gt;Key technical innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;‚ÄúThinking with Images‚Äù, which enables dynamic zoom-based visual analysis&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Support for chart interpretation, document understanding, visual grounding, and temporal awareness in video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Runtime on a single 80GB GPU, making it accessible to mid-sized organizations&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Full compatibility with Transformers, vLLM, and Baidu‚Äôs FastDeploy toolkits&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This release adds pressure on closed-source competitors. With Apache 2.0 licensing, ERNIE-4.5-VL-28B-A3B-Thinking becomes a viable foundation model for commercial applications without licensing restrictions ‚Äî something few high-performing models in this class offer.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community Feedback and Baidu‚Äôs Response&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Following the launch of ERNIE 5.0, developer and AI evaluator Lisan al Gaib (@scaling01) &lt;a href="https://x.com/scaling01/status/1988961630273646872"&gt;posted a mixed review on X.&lt;/a&gt; While initially impressed by the model‚Äôs benchmark performance, they reported a persistent issue where ERNIE 5.0 would repeatedly invoke tools ‚Äî even when explicitly instructed not to ‚Äî during SVG generation tasks.&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;‚ÄúERNIE 5.0 benchmarks looked insane until I tested it‚Ä¶ unfortunately it‚Äôs RL braindamaged or they have a serious issue with their chat platform / system prompt,‚Äù Lisan wrote.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;In a matter of hours, Baidu‚Äôs developer-focused support account, &lt;a href="https://x.com/ErnieforDevs/status/1989001980430393688"&gt;@ErnieforDevs, responded&lt;/a&gt;:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;‚ÄúThanks for the feedback! It‚Äôs a known bug ‚Äî certain syntax can consistently trigger it. We‚Äôre working on a fix. You can try rephrasing or changing the prompt to avoid it for now.‚Äù&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;The quick turnaround reflects Baidu‚Äôs increasing emphasis on developer communication, especially as it courts international users through both proprietary and open-source offerings.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Outlook for Baidu and its ERNIE foundational LLM family&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Baidu‚Äôs ERNIE 5.0 marks a strategic escalation in the global foundation model race. With performance claims that put it on par with the most advanced systems from OpenAI and Google, and a mix of premium pricing and open-access alternatives, Baidu is signaling its ambition to become not just a domestic AI leader, but a credible global infrastructure provider.&lt;/p&gt;&lt;p&gt;At a time when enterprise AI users are increasingly demanding multimodal performance, flexible licensing, and deployment efficiency, Baidu‚Äôs two-track approach‚Äîpremium hosted APIs and open-source releases‚Äîmay broaden its appeal across both corporate and developer communities.&lt;/p&gt;&lt;p&gt;Whether the company‚Äôs performance claims hold up under third-party testing remains to be seen. But in a landscape shaped by rising costs, model complexity, and compute bottlenecks, ERNIE 5.0 and its supporting ecosystem give Baidu a competitive position in the next wave of AI deployment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/baidu-unveils-proprietary-ernie-5-beating-gpt-5-performance-on-charts</guid><pubDate>Thu, 13 Nov 2025 20:23:00 +0000</pubDate></item><item><title>[NEW] Apple‚Äôs new App Review Guidelines clamp down on apps sharing personal data with ‚Äòthird-party AI‚Äô (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/apples-new-app-review-guidelines-clamp-down-on-apps-sharing-personal-data-with-third-party-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/01/app-store-2024-v1.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Thursday introduced a new set of App Review Guidelines for developers, which now specifically state that apps must disclose and obtain users‚Äô permission before sharing personal data with third-party AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change comes ahead of the iPhone maker‚Äôs plan to introduce its own AI-upgraded version of Siri in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That update will see Apple‚Äôs digital assistant offer users the ability to take actions across apps using Siri commands, and will be powered, in part, by Google‚Äôs Gemini technology, according to a recent Bloomberg report.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, Apple is ensuring other apps aren‚Äôt leaking personal data to AI providers or other AI businesses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What‚Äôs interesting about this particular update is not the requirements being described but that Apple has specifically called out AI companies as needing to come into compliance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before the revised language, the guideline known as rule 5.1.2(i) included language around disclosure and obtaining user consent for data sharing, noting that apps could not ‚Äúuse, transmit or share‚Äù someone‚Äôs personal data without their permission. This rule served as part of Apple‚Äôs compliance with data privacy regulations like the EU‚Äôs GDPR (General Data Protection Regulation), California‚Äôs Consumer Privacy Act, and others, which ensure that users have more control over how their data is collected and shared. Apps that don‚Äôt follow the policy can be removed from the App Store.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly revised guideline adds the following sentence (emphasis ours): &lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;You must clearly disclose where personal data will be shared with third parties, &lt;strong&gt;including with third-party AI,&lt;/strong&gt; and obtain explicit permission before doing so.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;This change could impact apps that intend to use AI systems to collect or process information about their users, perhaps to personalize their apps or provide certain functionality. It‚Äôs unclear how stringently Apple will enforce the rule, given that the term ‚ÄúAI‚Äù could include a variety of technologies ‚Äî not just LLMs, but also things like machine learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The updated rule is one of several revisions to the App Review Guidelines out on Thursday. Other changes are focused on supporting Apple‚Äôs new Mini Apps Program, also announced today, as well as tweaks to rules involving creator apps, loan apps, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One addition also added crypto exchanges to the list of apps that provide services in highly regulated fields.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/01/app-store-2024-v1.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple on Thursday introduced a new set of App Review Guidelines for developers, which now specifically state that apps must disclose and obtain users‚Äô permission before sharing personal data with third-party AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change comes ahead of the iPhone maker‚Äôs plan to introduce its own AI-upgraded version of Siri in 2026.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That update will see Apple‚Äôs digital assistant offer users the ability to take actions across apps using Siri commands, and will be powered, in part, by Google‚Äôs Gemini technology, according to a recent Bloomberg report.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, Apple is ensuring other apps aren‚Äôt leaking personal data to AI providers or other AI businesses. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What‚Äôs interesting about this particular update is not the requirements being described but that Apple has specifically called out AI companies as needing to come into compliance. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Before the revised language, the guideline known as rule 5.1.2(i) included language around disclosure and obtaining user consent for data sharing, noting that apps could not ‚Äúuse, transmit or share‚Äù someone‚Äôs personal data without their permission. This rule served as part of Apple‚Äôs compliance with data privacy regulations like the EU‚Äôs GDPR (General Data Protection Regulation), California‚Äôs Consumer Privacy Act, and others, which ensure that users have more control over how their data is collected and shared. Apps that don‚Äôt follow the policy can be removed from the App Store.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly revised guideline adds the following sentence (emphasis ours): &lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;You must clearly disclose where personal data will be shared with third parties, &lt;strong&gt;including with third-party AI,&lt;/strong&gt; and obtain explicit permission before doing so.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;This change could impact apps that intend to use AI systems to collect or process information about their users, perhaps to personalize their apps or provide certain functionality. It‚Äôs unclear how stringently Apple will enforce the rule, given that the term ‚ÄúAI‚Äù could include a variety of technologies ‚Äî not just LLMs, but also things like machine learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The updated rule is one of several revisions to the App Review Guidelines out on Thursday. Other changes are focused on supporting Apple‚Äôs new Mini Apps Program, also announced today, as well as tweaks to rules involving creator apps, loan apps, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One addition also added crypto exchanges to the list of apps that provide services in highly regulated fields.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/apples-new-app-review-guidelines-clamp-down-on-apps-sharing-personal-data-with-third-party-ai/</guid><pubDate>Thu, 13 Nov 2025 21:14:35 +0000</pubDate></item><item><title>[NEW] VCs abandon old rules for a ‚Äòfunky time‚Äô of investing in AI startups (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/13/vcs-abandon-old-rules-for-a-funky-time-of-investing-in-ai-startups/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54888675035_dae88c3d06_c.jpg?w=800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If there‚Äôs one thing that VCs agree on when backing AI startups, it‚Äôs that AI requires a different investment approach than prior technological shifts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs a funky time,‚Äù said Aileen Lee, founder and managing partner of Cowboy Ventures, onstage at TechCrunch Disrupt 2025. The longtime VC noted that the rules of investing have significantly shifted now that some AI companies are leaping from ‚Äúzero to $100 million in revenue in a single year.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, Lee also noted that, based on her firm‚Äôs research, Series A investors aren‚Äôt just seeking rapid revenue growth. ‚ÄúIt‚Äôs an algorithm with different variables and different coefficients.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the factors investors now measure, according to Lee, include whether the startup is generating data, the strength of its competitive moat, the founders‚Äô past accomplishments, and the technical depth of the product. ‚ÄúDepending on what your company is, the output of the algorithmic formula is going to be different,‚Äù she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jon McNeill, co-founder and CEO of startup creation firm DVx Ventures, stated that even startups that grow rapidly from inception to $5 million in revenue often struggle to secure follow-on funding. ‚ÄúI think this game has changed, and it is changing dynamically,‚Äù he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;McNeill noted that Series A investors are now applying the same rigorous standards to seed-stage startups that they previously reserved for more mature companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúI think a lot of investors have figured out that the breakout companies, in most cases,&lt;strong&gt; &lt;/strong&gt;don‚Äôt have the best tech,‚Äù McNeill said about why Series A VCs are looking so closely at startups‚Äô ability to attract and retain customers. ‚ÄúThey have the best go-to market.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Steve Jang, founder and managing partner of Kindred Ventures, disagreed that a strong go-to-market (GTM), an industry term for sales and marketing, holds greater weight for investors. ‚ÄúI don‚Äôt think it‚Äôs 100% true to say mediocre technology, great GTM wins and raises money and gets customers. I think that it‚Äôs a necessary requirement to have both.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While McNeill later clarified that having a solid product is important, he indicated that his initial comment was related to the founders‚Äô need to develop an exceptionally strong sales and marketing strategy right out of the gate. ‚ÄúInvestors are getting much more sophisticated on the go-to market than they have in the past,‚Äù he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(The debate over marketing versus tech was brought to the forefront later during the conference when Roy Lee, founder of the viral startup Cluely, said onstage that launching a product that barely worked, even with massive social media fame, may not always be the best idea.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Aileen Lee added that AI startups are now under pressure to deliver product updates and new features at an unprecedented pace, preempting existing companies that might try to introduce similar products. &amp;nbsp;‚ÄúIf you look at how much OpenAI and Anthropic are shipping, you‚Äôre going to have to figure out how to match how much you ship, how quickly and the quality of it,‚Äù she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the expectations for breakneck growth and fast product development, panelists agreed that the AI industry is still in its very early stages. As Jang put it, ‚ÄúThere are no clear, outright winners, even in LLMs. There are competitors nipping at their heels.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This means startups still have a path to unseating perceived leaders, whether they are decades-old companies or fast-moving newcomers.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54888675035_dae88c3d06_c.jpg?w=800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If there‚Äôs one thing that VCs agree on when backing AI startups, it‚Äôs that AI requires a different investment approach than prior technological shifts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúIt‚Äôs a funky time,‚Äù said Aileen Lee, founder and managing partner of Cowboy Ventures, onstage at TechCrunch Disrupt 2025. The longtime VC noted that the rules of investing have significantly shifted now that some AI companies are leaping from ‚Äúzero to $100 million in revenue in a single year.‚Äù&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, Lee also noted that, based on her firm‚Äôs research, Series A investors aren‚Äôt just seeking rapid revenue growth. ‚ÄúIt‚Äôs an algorithm with different variables and different coefficients.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of the factors investors now measure, according to Lee, include whether the startup is generating data, the strength of its competitive moat, the founders‚Äô past accomplishments, and the technical depth of the product. ‚ÄúDepending on what your company is, the output of the algorithmic formula is going to be different,‚Äù she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jon McNeill, co-founder and CEO of startup creation firm DVx Ventures, stated that even startups that grow rapidly from inception to $5 million in revenue often struggle to secure follow-on funding. ‚ÄúI think this game has changed, and it is changing dynamically,‚Äù he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;McNeill noted that Series A investors are now applying the same rigorous standards to seed-stage startups that they previously reserved for more mature companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;‚ÄúI think a lot of investors have figured out that the breakout companies, in most cases,&lt;strong&gt; &lt;/strong&gt;don‚Äôt have the best tech,‚Äù McNeill said about why Series A VCs are looking so closely at startups‚Äô ability to attract and retain customers. ‚ÄúThey have the best go-to market.‚Äù&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Steve Jang, founder and managing partner of Kindred Ventures, disagreed that a strong go-to-market (GTM), an industry term for sales and marketing, holds greater weight for investors. ‚ÄúI don‚Äôt think it‚Äôs 100% true to say mediocre technology, great GTM wins and raises money and gets customers. I think that it‚Äôs a necessary requirement to have both.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While McNeill later clarified that having a solid product is important, he indicated that his initial comment was related to the founders‚Äô need to develop an exceptionally strong sales and marketing strategy right out of the gate. ‚ÄúInvestors are getting much more sophisticated on the go-to market than they have in the past,‚Äù he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;(The debate over marketing versus tech was brought to the forefront later during the conference when Roy Lee, founder of the viral startup Cluely, said onstage that launching a product that barely worked, even with massive social media fame, may not always be the best idea.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Aileen Lee added that AI startups are now under pressure to deliver product updates and new features at an unprecedented pace, preempting existing companies that might try to introduce similar products. &amp;nbsp;‚ÄúIf you look at how much OpenAI and Anthropic are shipping, you‚Äôre going to have to figure out how to match how much you ship, how quickly and the quality of it,‚Äù she said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the expectations for breakneck growth and fast product development, panelists agreed that the AI industry is still in its very early stages. As Jang put it, ‚ÄúThere are no clear, outright winners, even in LLMs. There are competitors nipping at their heels.‚Äù&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This means startups still have a path to unseating perceived leaders, whether they are decades-old companies or fast-moving newcomers.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/13/vcs-abandon-old-rules-for-a-funky-time-of-investing-in-ai-startups/</guid><pubDate>Thu, 13 Nov 2025 23:18:49 +0000</pubDate></item></channel></rss>