<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 07 Oct 2025 01:37:31 +0000</lastBuildDate><item><title>Google’s new AI agent rewrites code to automate vulnerability fixes (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-new-ai-agent-rewrites-code-automate-vulnerability-fixes/</link><description>&lt;p&gt;Google DeepMind has deployed a new AI agent designed to autonomously find and fix critical security vulnerabilities in software code. The system, aptly-named CodeMender, has already contributed 72 security fixes to established open-source projects in the last six months.&lt;/p&gt;&lt;p&gt;Identifying and patching vulnerabilities is a notoriously difficult and time-consuming process, even with the aid of traditional automated methods like fuzzing. Google DeepMind’s own research, including AI-based projects such as Big Sleep and OSS-Fuzz, has proven effective at discovering new zero-day vulnerabilities in well-audited code. This success, however, creates a new bottleneck: as AI accelerates the discovery of flaws, the burden on human developers to fix them intensifies.&lt;/p&gt;&lt;p&gt;CodeMender is engineered to address this imbalance. It functions as an autonomous AI agent that takes a comprehensive approach to fix code security. Its capabilities are both reactive, allowing it to patch newly discovered vulnerabilities instantly, and proactive, enabling it to rewrite existing code to eliminate entire classes of security flaws before they can be exploited. This allows human developers and project maintainers to dedicate more of their time to building features and improving software functionality.&lt;/p&gt;&lt;p&gt;The system operates by leveraging the advanced reasoning capabilities of Google’s recent Gemini Deep Think models. This foundation allows the agent to debug and resolve complex security issues with a high degree of autonomy. To achieve this, the system is equipped with a set of tools that permit it to analyse and reason about code before implementing any changes. CodeMender also includes a validation process to ensure any modifications are correct and do not introduce new problems, known as regressions.&lt;/p&gt;&lt;p&gt;While large language models are advancing rapidly, a mistake when it comes to code security can have costly consequences. CodeMender’s automatic validation framework is therefore essential. It systematically checks that any proposed changes fix the root cause of an issue, are functionally correct, do not break existing tests, and adhere to the project’s coding style guidelines. Only high-quality patches that satisfy these stringent criteria are surfaced for human review.&lt;/p&gt;&lt;p&gt;To enhance its code fixing effectiveness, the DeepMind team developed new techniques for the AI agent. CodeMender employs advanced program analysis, utilising a suite of tools including static and dynamic analysis, differential testing, fuzzing, and SMT solvers. These instruments allow it to systematically scrutinise code patterns, control flow, and data flow to identify the fundamental causes of security flaws and architectural weaknesses.&lt;/p&gt;&lt;p&gt;The system also uses a multi-agent architecture, where specialised agents are deployed to tackle specific aspects of a problem. For example, a dedicated large language model-based critique tool reveals the differences between original and modified code. This allows the primary agent to verify that its proposed changes do not introduce unintended side effects and to self-correct its approach when necessary.&lt;/p&gt;&lt;p&gt;In one practical example, CodeMender addressed a vulnerability where a crash report indicated a heap buffer overflow. Although the final patch only required changing a few lines of code, the root cause was not immediately obvious. By using a debugger and code search tools, the agent determined the true problem was an incorrect stack management issue with Extensible Markup Language (XML) elements during parsing, located elsewhere in the codebase. In another case, the agent devised a non-trivial patch for a complex object lifetime issue, modifying a custom system for generating C code within the target project.&lt;/p&gt;&lt;p&gt;Beyond simply reacting to existing bugs, CodeMender is designed to proactively harden software against future threats. The team deployed the agent to apply &lt;em&gt;-fbounds-safety&lt;/em&gt; annotations to parts of libwebp, a widely used image compression library. These annotations instruct the compiler to add bounds checks to the code, which can prevent an attacker from exploiting a buffer overflow to execute arbitrary code.&lt;/p&gt;&lt;p&gt;This work is particularly relevant given that a heap buffer overflow vulnerability in libwebp, tracked as CVE-2023-4863, was used by a threat actor in a zero-click iOS exploit several years ago. DeepMind notes that with these annotations in place, that specific vulnerability, along with most other buffer overflows in the annotated sections, would have been rendered unexploitable.&lt;/p&gt;&lt;p&gt;The AI agent’s proactive code fixing involves a sophisticated decision-making process. When applying annotations, it can automatically correct new compilation errors and test failures that arise from its own changes. If its validation tools detect that a modification has broken functionality, the agent self-corrects based on the feedback and attempts a different solution.&lt;/p&gt;&lt;p&gt;Despite these promising early results, Google DeepMind is taking a cautious and deliberate approach to deployment, with a strong focus on reliability. At present, every patch generated by CodeMender is reviewed by human researchers before being submitted to an open-source project. The team is gradually increasing its submissions to ensure high quality and to systematically incorporate feedback from the open-source community.&lt;/p&gt;&lt;p&gt;Looking ahead, the researchers plan to reach out to maintainers of critical open-source projects with CodeMender-generated patches. By iterating on community feedback, they hope to eventually release CodeMender as a publicly available tool for all software developers.&lt;/p&gt;&lt;p&gt;The DeepMind team also intends to publish technical papers and reports in the coming months to share their techniques and results. This work represents the first steps in exploring the potential of AI agents to proactively fix code and fundamentally enhance software security for everyone.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;CAMIA privacy attack reveals what AI models memorise&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109669" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google DeepMind has deployed a new AI agent designed to autonomously find and fix critical security vulnerabilities in software code. The system, aptly-named CodeMender, has already contributed 72 security fixes to established open-source projects in the last six months.&lt;/p&gt;&lt;p&gt;Identifying and patching vulnerabilities is a notoriously difficult and time-consuming process, even with the aid of traditional automated methods like fuzzing. Google DeepMind’s own research, including AI-based projects such as Big Sleep and OSS-Fuzz, has proven effective at discovering new zero-day vulnerabilities in well-audited code. This success, however, creates a new bottleneck: as AI accelerates the discovery of flaws, the burden on human developers to fix them intensifies.&lt;/p&gt;&lt;p&gt;CodeMender is engineered to address this imbalance. It functions as an autonomous AI agent that takes a comprehensive approach to fix code security. Its capabilities are both reactive, allowing it to patch newly discovered vulnerabilities instantly, and proactive, enabling it to rewrite existing code to eliminate entire classes of security flaws before they can be exploited. This allows human developers and project maintainers to dedicate more of their time to building features and improving software functionality.&lt;/p&gt;&lt;p&gt;The system operates by leveraging the advanced reasoning capabilities of Google’s recent Gemini Deep Think models. This foundation allows the agent to debug and resolve complex security issues with a high degree of autonomy. To achieve this, the system is equipped with a set of tools that permit it to analyse and reason about code before implementing any changes. CodeMender also includes a validation process to ensure any modifications are correct and do not introduce new problems, known as regressions.&lt;/p&gt;&lt;p&gt;While large language models are advancing rapidly, a mistake when it comes to code security can have costly consequences. CodeMender’s automatic validation framework is therefore essential. It systematically checks that any proposed changes fix the root cause of an issue, are functionally correct, do not break existing tests, and adhere to the project’s coding style guidelines. Only high-quality patches that satisfy these stringent criteria are surfaced for human review.&lt;/p&gt;&lt;p&gt;To enhance its code fixing effectiveness, the DeepMind team developed new techniques for the AI agent. CodeMender employs advanced program analysis, utilising a suite of tools including static and dynamic analysis, differential testing, fuzzing, and SMT solvers. These instruments allow it to systematically scrutinise code patterns, control flow, and data flow to identify the fundamental causes of security flaws and architectural weaknesses.&lt;/p&gt;&lt;p&gt;The system also uses a multi-agent architecture, where specialised agents are deployed to tackle specific aspects of a problem. For example, a dedicated large language model-based critique tool reveals the differences between original and modified code. This allows the primary agent to verify that its proposed changes do not introduce unintended side effects and to self-correct its approach when necessary.&lt;/p&gt;&lt;p&gt;In one practical example, CodeMender addressed a vulnerability where a crash report indicated a heap buffer overflow. Although the final patch only required changing a few lines of code, the root cause was not immediately obvious. By using a debugger and code search tools, the agent determined the true problem was an incorrect stack management issue with Extensible Markup Language (XML) elements during parsing, located elsewhere in the codebase. In another case, the agent devised a non-trivial patch for a complex object lifetime issue, modifying a custom system for generating C code within the target project.&lt;/p&gt;&lt;p&gt;Beyond simply reacting to existing bugs, CodeMender is designed to proactively harden software against future threats. The team deployed the agent to apply &lt;em&gt;-fbounds-safety&lt;/em&gt; annotations to parts of libwebp, a widely used image compression library. These annotations instruct the compiler to add bounds checks to the code, which can prevent an attacker from exploiting a buffer overflow to execute arbitrary code.&lt;/p&gt;&lt;p&gt;This work is particularly relevant given that a heap buffer overflow vulnerability in libwebp, tracked as CVE-2023-4863, was used by a threat actor in a zero-click iOS exploit several years ago. DeepMind notes that with these annotations in place, that specific vulnerability, along with most other buffer overflows in the annotated sections, would have been rendered unexploitable.&lt;/p&gt;&lt;p&gt;The AI agent’s proactive code fixing involves a sophisticated decision-making process. When applying annotations, it can automatically correct new compilation errors and test failures that arise from its own changes. If its validation tools detect that a modification has broken functionality, the agent self-corrects based on the feedback and attempts a different solution.&lt;/p&gt;&lt;p&gt;Despite these promising early results, Google DeepMind is taking a cautious and deliberate approach to deployment, with a strong focus on reliability. At present, every patch generated by CodeMender is reviewed by human researchers before being submitted to an open-source project. The team is gradually increasing its submissions to ensure high quality and to systematically incorporate feedback from the open-source community.&lt;/p&gt;&lt;p&gt;Looking ahead, the researchers plan to reach out to maintainers of critical open-source projects with CodeMender-generated patches. By iterating on community feedback, they hope to eventually release CodeMender as a publicly available tool for all software developers.&lt;/p&gt;&lt;p&gt;The DeepMind team also intends to publish technical papers and reports in the coming months to share their techniques and results. This work represents the first steps in exploring the potential of AI agents to proactively fix code and fundamentally enhance software security for everyone.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;CAMIA privacy attack reveals what AI models memorise&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109669" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/09/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-new-ai-agent-rewrites-code-automate-vulnerability-fixes/</guid><pubDate>Mon, 06 Oct 2025 13:56:40 +0000</pubDate></item><item><title>When AI Meets Biology: Promise, Risk, and Responsibility (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Paraphrase Project Protiens" class="wp-image-1151098" height="1080" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04.png" width="1920" /&gt;&lt;/figure&gt;



&lt;p&gt;Advances in AI are opening extraordinary frontiers in biology. AI-assisted protein engineering holds the promise of new medicines, materials, and breakthroughs in scientific understandings. Yet these same technologies also introduce biosecurity risks and may lower barriers to designing harmful toxins or pathogens. This “dual-use” potential, where the same knowledge can be harnessed for good or misuse to cause harm, poses a critical dilemma for modern science.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="great-promise-and-potential-threat"&gt;Great Promise—and Potential Threat&lt;/h2&gt;



&lt;p&gt;I’m excited about the potential for AI-assisted protein design to drive breakthroughs in biology and medicine. At the same time, I’ve also studied how these tools could be misused. In computer-based studies, we found that AI protein design (AIPD) tools could generate modified versions of proteins of concern, such as ricin. Alarmingly, these reformulated proteins were able to evade the biosecurity screening systems used by DNA synthesis companies, which scientists rely on to synthesize AI-generated sequences for experimental use. &lt;/p&gt;



&lt;p&gt;In our paper published in &lt;em&gt;Science&lt;/em&gt; on October 2, “Strengthening nucleic acid biosecurity screening against generative protein design tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,” we describe a two-year confidential project we began in late 2023 while preparing a case study for a workshop on AI and biosecurity.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;We worked confidentially with partners across organizations and sectors for 10 months to develop AI biosecurity “red-teaming” methods that allowed us to better understand vulnerabilities and craft practical solutions—”patches” that have now been adopted globally, making screening systems significantly more AI-resilient.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="An illustration of the AI Protein Design red-teaming workflow. [starting at the left] an icon of a database with the heading above that reads: Database of Wild-Type Proteins of Concern. [arrow moves right] Above the arrow the text reads: Generate Synthetic Homologs (x) Conditioned on Wild Types (y). P(x|y) appears below the arrow. [continuing to the right] a computer monitor icon with protein sequences on the screen appears in brackets with N appearing outside the bottom of the right bracket. The text above the computer screen reads: “N” Synthetic Homologs per Wild-Type. [arrows move to the right and fork to an upper arrow and a lower arrow] The text above the upper arrow reads Reverse Translate and the arrow points to a computer monitor icon with a DNA icon on the screen. [upper arrow continues to the right] The arrow points to a computer monitor icon with the text Hazard Screening appearing above and a biohazard icon and a question mark appearing on the screen. [lower arrow moves to the right] A computer monitor icon includes a paraphrased toxin sequence verses a protein sequence on the computer screen. Above the monitor the text reads: Score in silico. [lower arrow continues to the right] An illustration provides an example of the evaluation results (see also table S1 in the paper) tracking the number of flagged sequences (y-axis) and hazardous sequences (x-axis). [the lower arrow moves up to the Hazard Screening step (from the upper arrow process) and another arrow moves from the Hazard Screening to the evaluation results illustration. There is a dotted line with the words Repeat Process moving from the Evaluation illustration to the left and back to the database." class="wp-image-1151201" height="693" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard.png" width="1788" /&gt;&lt;figcaption class="wp-element-caption"&gt;Summary of AIPD red-teaming workflow.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;For structuring, methods, and process in our study, we took inspiration from the cybersecurity community, where “zero-day” vulnerabilities are kept confidential until a protective patch is developed and deployed. Following the acknowledgment by a small group of workshop attendees of a zero-day for AI in biology, we worked closely with stakeholders—including synthesis companies, biosecurity organizations, and policymakers—to rapidly create and distribute patches that improved detection of AI-redesigned protein sequences. We delayed public disclosure until protective measures were in place and widely adopted.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="dilemma-of-disclosure"&gt;Dilemma of Disclosure&lt;/h2&gt;



&lt;p&gt;The dual use dilemma also complicates how we share information about vulnerabilities and safeguards. Across AI and other fields, researchers face a core question: &lt;/p&gt;



&lt;blockquote class="wp-block-quote is-style-spectrum is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;How can scientists share potentially risk-revealing methods and results in ways that enable progress without offering a roadmap for misuse?&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;We recognized that our work itself—detailing methods and failure modes—could be exploited by malicious actors if published openly. To guide decisions about what to share, we held a multi-stakeholder deliberation involving government agencies, international biosecurity organizations, and policy experts. Opinions varied: some urged full transparency to maximize reproducibility—and to help others to build on our work; others stressed restraint to minimize risk. It was clear that a &lt;em&gt;new model of scientific communication&lt;/em&gt; was needed, one that could balance openness and security.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="the-novel-framework"&gt;The Novel Framework&lt;/h2&gt;



&lt;p&gt;The risk of sharing dangerous information through biological research has become a growing concern. We have participated in community-wide discussion on the challenges, including a recent National Academies of Science, Engineering, and Medicine workshop and study.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In preparing our manuscript for publication, we worked on designing a process to limit the spread of dangerous information while still enabling scientific progress.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To address the dual challenges, we devised a tiered access system for data and methods, implemented in partnership with the International Biosecurity and Biosafety Initiative for Science (IBBIS)&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a nonprofit dedicated to advancing science while reducing catastrophic risks. The system works as follows:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Controlled access&lt;/strong&gt;: Researchers can request access through IBBIS, providing their identity, affiliation, and intended use. Requests are reviewed by an expert biosecurity committee, ensuring that only legitimate scientists conducting relevant research gain access.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Stratified tiers of information&lt;/strong&gt;: Data and code are classified into several tiers according to their potential hazard, from low-risk summaries through sensitive technical data to critical software pipelines.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Safeguards and agreements&lt;/strong&gt;: Approved users sign tailored usage agreements, including non-disclosure terms, before receiving data.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Resilience and longevity&lt;/strong&gt;: Provisions are built in for declassification when risks subside, and for succession of stewardship to trusted organizations should IBBIS be unable to continue its operation.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This framework allows replication and extension of our work while guarding against misuse. Rather than relying on secrecy, it provides a durable system of responsible access.&lt;/p&gt;



&lt;p&gt;To ensure continued funding for the storage and responsible distribution of sensitive data and software, and for the operation of the sharing program, we provided an endowment to IBBIS to support the program &lt;em&gt;in perpetuity&lt;/em&gt;. This approach was modeled after the One Hundred Year Study on AI at Stanford, which is endowed to continue for the life of the university.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="an-important-step-in-scientific-publishing"&gt;An Important Step in Scientific Publishing&lt;/h2&gt;



&lt;p&gt;We are pleased that the leadership at &lt;em&gt;Science&lt;/em&gt; accepted our approach to handling information hazards. To our knowledge, this is the first time a leading scientific journal has formally endorsed a tiered-access approach to manage an information hazard. This recognition validates the idea that rigorous science and responsible risk management can coexist—and that journals, too, can play a role in shaping how sensitive knowledge is shared. We acknowledge the visionary leadership at &lt;em&gt;Science,&lt;/em&gt; including editors, Michael Funk and Valda Vinson, and Editor-in-Chief, Holden Thorp.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="beyond-biology-a-model-for-sensitive-research"&gt;Beyond Biology: A Model for Sensitive Research&lt;/h2&gt;



&lt;p&gt;While developed for AI-powered protein design, our approach offers a generalizable model for dual-use research of concern (DURC) across disciplines. Whether in biology, chemistry, or emerging technologies, scientists will increasingly confront situations where openness and security pull in opposite directions. Our experience shows that these values can be balanced: with creativity, coordination, and new institutional mechanisms, science can uphold both reproducibility and responsibility.&lt;/p&gt;



&lt;p&gt;We hope this framework becomes a template for future projects, offering a way forward for researchers who wish to share their insights without amplifying risks. By embedding resilience into &lt;em&gt;how&lt;/em&gt; knowledge is communicated—not just &lt;em&gt;what&lt;/em&gt; is communicated—we can ensure that scientific progress continues to serve humanity safely.&lt;/p&gt;



&lt;p&gt;The responsible management of information hazards is no longer a peripheral concern: it is central to how science will advance in the age of powerful technologies like AI. This approach to managing information hazards demonstrates a path forward, where novel frameworks for access and stewardship allow sensitive but vital research to be shared, scrutinized, and extended responsibly. Approaches like this will be critical to ensuring that scientific openness and societal safety advance hand-in-hand.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h3 class="wp-block-heading" id="additional-reading"&gt;Additional reading&lt;/h3&gt;



&lt;p&gt;&lt;em&gt;Strengthening nucleic acid biosecurity screening against generative protein design tools&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations, National Academies of Science, Engineering, and Medicine, 2025.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Disseminating In Silico and Computational Biological Research: Navigating Benefits and Risks: Proceedings of a Workshop, National Academies of Science, Engineering, and Medicine, 2025.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Protecting scientific integrity in an age of generative AI, Proceedings of the National Academy of Science, 2024.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Paraphrase Project Protiens" class="wp-image-1151098" height="1080" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04.png" width="1920" /&gt;&lt;/figure&gt;



&lt;p&gt;Advances in AI are opening extraordinary frontiers in biology. AI-assisted protein engineering holds the promise of new medicines, materials, and breakthroughs in scientific understandings. Yet these same technologies also introduce biosecurity risks and may lower barriers to designing harmful toxins or pathogens. This “dual-use” potential, where the same knowledge can be harnessed for good or misuse to cause harm, poses a critical dilemma for modern science.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="great-promise-and-potential-threat"&gt;Great Promise—and Potential Threat&lt;/h2&gt;



&lt;p&gt;I’m excited about the potential for AI-assisted protein design to drive breakthroughs in biology and medicine. At the same time, I’ve also studied how these tools could be misused. In computer-based studies, we found that AI protein design (AIPD) tools could generate modified versions of proteins of concern, such as ricin. Alarmingly, these reformulated proteins were able to evade the biosecurity screening systems used by DNA synthesis companies, which scientists rely on to synthesize AI-generated sequences for experimental use. &lt;/p&gt;



&lt;p&gt;In our paper published in &lt;em&gt;Science&lt;/em&gt; on October 2, “Strengthening nucleic acid biosecurity screening against generative protein design tools&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;,” we describe a two-year confidential project we began in late 2023 while preparing a case study for a workshop on AI and biosecurity.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/h2&gt;
				
								&lt;p class="large" id="ai-testing-and-evaluation-learnings-from-science-and-industry"&gt;Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;We worked confidentially with partners across organizations and sectors for 10 months to develop AI biosecurity “red-teaming” methods that allowed us to better understand vulnerabilities and craft practical solutions—”patches” that have now been adopted globally, making screening systems significantly more AI-resilient.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="An illustration of the AI Protein Design red-teaming workflow. [starting at the left] an icon of a database with the heading above that reads: Database of Wild-Type Proteins of Concern. [arrow moves right] Above the arrow the text reads: Generate Synthetic Homologs (x) Conditioned on Wild Types (y). P(x|y) appears below the arrow. [continuing to the right] a computer monitor icon with protein sequences on the screen appears in brackets with N appearing outside the bottom of the right bracket. The text above the computer screen reads: “N” Synthetic Homologs per Wild-Type. [arrows move to the right and fork to an upper arrow and a lower arrow] The text above the upper arrow reads Reverse Translate and the arrow points to a computer monitor icon with a DNA icon on the screen. [upper arrow continues to the right] The arrow points to a computer monitor icon with the text Hazard Screening appearing above and a biohazard icon and a question mark appearing on the screen. [lower arrow moves to the right] A computer monitor icon includes a paraphrased toxin sequence verses a protein sequence on the computer screen. Above the monitor the text reads: Score in silico. [lower arrow continues to the right] An illustration provides an example of the evaluation results (see also table S1 in the paper) tracking the number of flagged sequences (y-axis) and hazardous sequences (x-axis). [the lower arrow moves up to the Hazard Screening step (from the upper arrow process) and another arrow moves from the Hazard Screening to the evaluation results illustration. There is a dotted line with the words Repeat Process moving from the Evaluation illustration to the left and back to the database." class="wp-image-1151201" height="693" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard.png" width="1788" /&gt;&lt;figcaption class="wp-element-caption"&gt;Summary of AIPD red-teaming workflow.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;For structuring, methods, and process in our study, we took inspiration from the cybersecurity community, where “zero-day” vulnerabilities are kept confidential until a protective patch is developed and deployed. Following the acknowledgment by a small group of workshop attendees of a zero-day for AI in biology, we worked closely with stakeholders—including synthesis companies, biosecurity organizations, and policymakers—to rapidly create and distribute patches that improved detection of AI-redesigned protein sequences. We delayed public disclosure until protective measures were in place and widely adopted.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="dilemma-of-disclosure"&gt;Dilemma of Disclosure&lt;/h2&gt;



&lt;p&gt;The dual use dilemma also complicates how we share information about vulnerabilities and safeguards. Across AI and other fields, researchers face a core question: &lt;/p&gt;



&lt;blockquote class="wp-block-quote is-style-spectrum is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p&gt;How can scientists share potentially risk-revealing methods and results in ways that enable progress without offering a roadmap for misuse?&lt;/p&gt;
&lt;/blockquote&gt;



&lt;p&gt;We recognized that our work itself—detailing methods and failure modes—could be exploited by malicious actors if published openly. To guide decisions about what to share, we held a multi-stakeholder deliberation involving government agencies, international biosecurity organizations, and policy experts. Opinions varied: some urged full transparency to maximize reproducibility—and to help others to build on our work; others stressed restraint to minimize risk. It was clear that a &lt;em&gt;new model of scientific communication&lt;/em&gt; was needed, one that could balance openness and security.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="the-novel-framework"&gt;The Novel Framework&lt;/h2&gt;



&lt;p&gt;The risk of sharing dangerous information through biological research has become a growing concern. We have participated in community-wide discussion on the challenges, including a recent National Academies of Science, Engineering, and Medicine workshop and study.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In preparing our manuscript for publication, we worked on designing a process to limit the spread of dangerous information while still enabling scientific progress.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To address the dual challenges, we devised a tiered access system for data and methods, implemented in partnership with the International Biosecurity and Biosafety Initiative for Science (IBBIS)&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a nonprofit dedicated to advancing science while reducing catastrophic risks. The system works as follows:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Controlled access&lt;/strong&gt;: Researchers can request access through IBBIS, providing their identity, affiliation, and intended use. Requests are reviewed by an expert biosecurity committee, ensuring that only legitimate scientists conducting relevant research gain access.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Stratified tiers of information&lt;/strong&gt;: Data and code are classified into several tiers according to their potential hazard, from low-risk summaries through sensitive technical data to critical software pipelines.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Safeguards and agreements&lt;/strong&gt;: Approved users sign tailored usage agreements, including non-disclosure terms, before receiving data.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Resilience and longevity&lt;/strong&gt;: Provisions are built in for declassification when risks subside, and for succession of stewardship to trusted organizations should IBBIS be unable to continue its operation.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This framework allows replication and extension of our work while guarding against misuse. Rather than relying on secrecy, it provides a durable system of responsible access.&lt;/p&gt;



&lt;p&gt;To ensure continued funding for the storage and responsible distribution of sensitive data and software, and for the operation of the sharing program, we provided an endowment to IBBIS to support the program &lt;em&gt;in perpetuity&lt;/em&gt;. This approach was modeled after the One Hundred Year Study on AI at Stanford, which is endowed to continue for the life of the university.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="an-important-step-in-scientific-publishing"&gt;An Important Step in Scientific Publishing&lt;/h2&gt;



&lt;p&gt;We are pleased that the leadership at &lt;em&gt;Science&lt;/em&gt; accepted our approach to handling information hazards. To our knowledge, this is the first time a leading scientific journal has formally endorsed a tiered-access approach to manage an information hazard. This recognition validates the idea that rigorous science and responsible risk management can coexist—and that journals, too, can play a role in shaping how sensitive knowledge is shared. We acknowledge the visionary leadership at &lt;em&gt;Science,&lt;/em&gt; including editors, Michael Funk and Valda Vinson, and Editor-in-Chief, Holden Thorp.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="beyond-biology-a-model-for-sensitive-research"&gt;Beyond Biology: A Model for Sensitive Research&lt;/h2&gt;



&lt;p&gt;While developed for AI-powered protein design, our approach offers a generalizable model for dual-use research of concern (DURC) across disciplines. Whether in biology, chemistry, or emerging technologies, scientists will increasingly confront situations where openness and security pull in opposite directions. Our experience shows that these values can be balanced: with creativity, coordination, and new institutional mechanisms, science can uphold both reproducibility and responsibility.&lt;/p&gt;



&lt;p&gt;We hope this framework becomes a template for future projects, offering a way forward for researchers who wish to share their insights without amplifying risks. By embedding resilience into &lt;em&gt;how&lt;/em&gt; knowledge is communicated—not just &lt;em&gt;what&lt;/em&gt; is communicated—we can ensure that scientific progress continues to serve humanity safely.&lt;/p&gt;



&lt;p&gt;The responsible management of information hazards is no longer a peripheral concern: it is central to how science will advance in the age of powerful technologies like AI. This approach to managing information hazards demonstrates a path forward, where novel frameworks for access and stewardship allow sensitive but vital research to be shared, scrutinized, and extended responsibly. Approaches like this will be critical to ensuring that scientific openness and societal safety advance hand-in-hand.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h3 class="wp-block-heading" id="additional-reading"&gt;Additional reading&lt;/h3&gt;



&lt;p&gt;&lt;em&gt;Strengthening nucleic acid biosecurity screening against generative protein design tools&lt;/em&gt;.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations, National Academies of Science, Engineering, and Medicine, 2025.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Disseminating In Silico and Computational Biological Research: Navigating Benefits and Risks: Proceedings of a Workshop, National Academies of Science, Engineering, and Medicine, 2025.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Protecting scientific integrity in an age of generative AI, Proceedings of the National Academy of Science, 2024.&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/</guid><pubDate>Mon, 06 Oct 2025 14:03:54 +0000</pubDate></item><item><title>Ideas: More AI-resilient biosecurity with the Paraphrase Project (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/EricBruceTessaJames-Ideas_TW_LI_FB_1200x627-1.jpg" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;






&lt;p&gt;Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series &lt;em&gt;Ideas&lt;/em&gt;, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&lt;/p&gt;



&lt;p&gt;AI has been described as a “dual use” technology: the capabilities that can be leveraged for good can also potentially be used to cause harm. In this episode, Microsoft Chief Scientific Officer Eric Horvitz and his guests—Bruce Wittmann, a senior applied scientist at Microsoft; Tessa Alexanian&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a technical lead at the International Biosecurity and Biosafety Initiative for Science (IBBIS);&amp;nbsp;and James Diggans&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a vice president at Twist Bioscience—explore this idea in the context of AI-powered protein design.&lt;/p&gt;



&lt;p&gt;With Horvitz at the lead, Alexanian, Diggans, and Wittmann were part of a cross-sector team that demonstrated toxic protein candidates could be designed with help from AI—and that they could bypass the systems in place to defend against their creation. The project, known as the &lt;em&gt;Paraphrase Project&lt;/em&gt;, culminated in a cybersecurity-style response, a more robust protein screening system, and a modified approach to peer review with implications for how we think about and tackle AI risk more broadly. The work was recently published in &lt;em&gt;Science.&lt;/em&gt;&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript-1"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ERIC HORVITZ: &lt;/strong&gt;You’re&amp;nbsp;listening to&amp;nbsp;&lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code.&amp;nbsp;I’m&amp;nbsp;Eric Horvitz, Microsoft’s chief scientific officer, and in this series, we explore the technologies shaping our future and the&amp;nbsp;big ideas&amp;nbsp;that propel them forward.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;Today,&amp;nbsp;I’m&amp;nbsp;excited to talk about the Paraphrase Project, an effort I co-led exploring how&amp;nbsp;advances in&amp;nbsp;AI tools&amp;nbsp;for protein design&amp;nbsp;might&amp;nbsp;impact&amp;nbsp;biosecurity. The results were reported in our recent paper,&amp;nbsp;“Strengthening nucleic acid biosecurity screening against generative protein design tools,”&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;published in&amp;nbsp;&lt;em&gt;Science&lt;/em&gt;&amp;nbsp;on Oct. 2.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Joining me are&amp;nbsp;three&amp;nbsp;of the larger set of&amp;nbsp;coauthors on that paper:&amp;nbsp;Bruce Wittmann, senior applied scientist at Microsoft;&amp;nbsp;James&amp;nbsp;Diggans, vice president at Twist Bioscience and chair of the board for the International Gene Synthesis Consortium;&amp;nbsp;and Tessa Alexanian, technical lead at the International Biosecurity and Biosafety Initiative for Science, also known as &lt;em&gt;IBBIS&lt;/em&gt;.&amp;nbsp;&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;Now, let’s&amp;nbsp;rewind two years.&amp;nbsp;Almost to&amp;nbsp;the day, Bruce and I uncovered a vulnerability. While preparing a case study for a workshop on AI and biosecurity, we discovered that open-source AI protein design tools could be used to redesign toxic proteins in ways that could bypass biosecurity screening systems, systems set up to&amp;nbsp;identify&amp;nbsp;incoming orders of concern.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now in that work, we&amp;nbsp;created an AI pipeline from open-source tools that could&amp;nbsp;essentially “paraphrase” the amino acid sequences—reformulating&amp;nbsp;them while&amp;nbsp;working to&amp;nbsp;preserve&amp;nbsp;their structure and potentially their function.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These paraphrased sequences could evade the screening systems used by major DNA&amp;nbsp;synthesis companies, and these are the systems that scientists rely on to safely produce AI-designed proteins.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now, experts in the field described this finding as the first “zero day” for AI and biosecurity.&amp;nbsp;And this&amp;nbsp;marked the beginning of a deep, two-year collaborative effort to investigate and address this challenge.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;With the help of a&amp;nbsp;strong&amp;nbsp;cross-sector team—including James, Tessa, Bruce, and many others—we worked behind the scenes to build AI biosecurity&amp;nbsp;&lt;em&gt;red-teaming approaches&lt;/em&gt;,&amp;nbsp;probe for vulnerabilities, and to design practical fixes. These “patches,” akin to those in cybersecurity,&amp;nbsp;have now been shared with&amp;nbsp;organizations&amp;nbsp;globally to strengthen biosecurity screening.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This has been one of the most fascinating projects&amp;nbsp;I’ve&amp;nbsp;had the privilege to work on, for its technical complexity, its ethical and policy dimensions, and the remarkable collaboration across industry, government, and nonprofit sectors.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The project highlights that the&amp;nbsp;same AI tools capable of&amp;nbsp;incredible&amp;nbsp;good can also be misused, requiring us to be vigilant, thoughtful, and creative so we continue to get the most benefit out of AI tools while working to ensure&amp;nbsp;that&amp;nbsp;we avoid costly misuses.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;With that, let me officially welcome our guests.&lt;s&gt;&lt;/s&gt;&lt;/p&gt;



&lt;p&gt;Bruce, James, Tessa, welcome to the podcast.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUCE WITTMANN: &lt;/strong&gt;Thanks, Eric.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;JAMES DIGGANS: &lt;/strong&gt;Thanks for having us.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;It’s been such a pleasure working closely with each of you, not only for your expertise but also for your deep commitment and passion about public health and global safety.&lt;/p&gt;



&lt;p&gt;Before we dive into the technical side of things, I’d like to ask each of you, how did you get into this field? What inspired you to become biologists and then pursue the implications of advances in AI for biosecurity? Bruce?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Well, I’ve always liked building things. That’s where I would say I come from. You know, my hobbies when I’m not working on biology or AI things—as you know, Eric—is, like, building things around the house, right. Doing construction. That kind of stuff.&lt;/p&gt;



&lt;p&gt;But my broader interests have always been biology, chemistry. So I originally got into organic chemistry. I found that was fascinating. From there, went to synthetic biology, particularly metabolic engineering, because that’s kind of like organic chemistry, but you’re wiring together different parts of an organism’s metabolism rather than different chemical reactions. And while I was working in that space, I, kind of, had the thought of there’s got to be an easier way to do this [LAUGHS] because it is really difficult to do any type of metabolic engineering. And that’s how I got into the AI space, trying to solve these very complicated biological problems, trying to build things that we don’t necessarily even understand using our understanding from data or &lt;em&gt;deriving&lt;/em&gt; understanding from data.&lt;/p&gt;



&lt;p&gt;So, you know, that’s the roundabout way of how I got to where I am—the abstract way of how I got to where I am.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And, Tessa, what motivated you to jump into this area and zoom into biology and biosciences and helping us to avoid &lt;em&gt;catastrophic&lt;/em&gt; outcomes?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN:&lt;/strong&gt; Yeah, I mean, probably the origin of me being really excited about biology is actually a book called &lt;em&gt;[The] Lives of [a] Cell&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; by Lewis Thomas, which is an extremely beautiful book of essays that made me be like, &lt;em&gt;Oh, wow, life is just incredible&lt;/em&gt;. I think I read it when I was, you know, 12 or 13, and I was like, &lt;em&gt;Life is incredible. I want to work on this. This is the most beautiful science&lt;/em&gt;, right. And then I, in university, I was studying engineering, and I heard there was this engineering team for engineering biology—this iGEM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; team—and I joined it, and I thought, &lt;em&gt;Oh, this is so cool. I really got to go work in this field of synthetic biology.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;And then I also tried doing the wet lab biology, and I was like, &lt;em&gt;Oh, but I don’t like this part&lt;/em&gt;. &lt;em&gt;I don’t actually, like, like babysitting microbes.&lt;/em&gt; [LAUGHTER] I think there’s a way … some people who are great wet lab biologists are made of really stern stuff. And they really enjoy figuring out how to redesign their negative controls so they can figure out whether it was contamination or whether it was, you know, temperature fluctuation. I’m not that, apparently.&lt;/p&gt;



&lt;p&gt;And so I ended up becoming a lab automation engineer because I could help the science happen, but I … but my responsibilities were the robots and the computers rather than the microbes, which I find a little bit intransigent.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Right. I was thinking of those tough souls; they also used their mouths to do pipetting and so on of these contaminated fluids …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Not anymore. &lt;strong&gt;ALEXANIAN: &lt;/strong&gt;It’s true. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Not anymore. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN:&lt;/strong&gt; They used to be tougher. They used to be tougher.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;James.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS:&lt;/strong&gt; So I did my undergrad in computer science and microbiology, mostly because at the time, I couldn’t pick which of the two I liked more. I liked them both. And by the time I graduated, I was lucky enough that I realized that the intersection of the two could be a thing. And so I did a PhD in computational biology, and then I worked for five years at the MITRE Corporation. It’s a nonprofit. I got the chance to work with the US biodefense community and just found an incredible group of people working to protect forces and the population at large from biological threats and just learned a ton about both biology and also dual-use risk. And then so when Twist called me and asked if I wanted to join Twist and set up their biosecurity program, I leapt at the chance and have done that for the past 10 years.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Well, thanks everyone.&lt;/p&gt;



&lt;p&gt;I believe that AI-powered protein design in particular is one of the most exciting frontiers of modern science. It holds promise for breakthroughs in medicine, public health, even material science. We’re already seeing it lead to new vaccines, novel therapeutics, and—on the scientific front—powerful insights into the machinery of life.&lt;/p&gt;



&lt;p&gt;So there’s much more ahead, especially in how AI can help us promote wellness, longevity, and the prevention of disease. But before we get too far ahead, while some of our listeners work in bioscience, many may not have a good understanding of some of the foundations.&lt;/p&gt;



&lt;p&gt;So, Bruce, can you just give us a high-level overview of proteins? What are they? Why are they important? How do they figure into human-designed applications?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Sure. Yeah. Fortunately, I used to TA a class on AI for protein design, so it’s right in my wheelhouse. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Perfect, perfect background. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;It’s perfect. Yeah. I got to go back to all of that. Yeah, so from the very basic level, proteins are the workhorses of life.&lt;/p&gt;



&lt;p&gt;Every chemical reaction that happens in our body—well, nearly every chemical reaction that happens in our body—most of the structure of our cells, you name it. Any life process, proteins are central to it.&lt;/p&gt;



&lt;p&gt;Now proteins are encoded by what are known as … well, I shouldn’t say encoded. They are &lt;em&gt;constructed&lt;/em&gt; from what are called amino acids—there are 20 of them—and depending on the combination and order in which you string these amino acids together, you get a different protein sequence. So that’s what we mean when we say protein sequence.&lt;/p&gt;



&lt;p&gt;The sequence of a protein then determines what shape that protein folds into in a cell, and that shape determines what the protein does. So we will often say sequence determines structure, which determines function.&lt;/p&gt;



&lt;p&gt;Now the challenge that we face in engineering proteins is just how many possibilities there are. For all practical purposes, it’s infinite. So we have 20 building blocks. There are on average around 300 amino acids in a protein. So that’s 20 to the power of 300 possible combinations. And a common reference point is that it’s estimated there are around 10 to the 80 particles in the observable universe. So beyond astronomical numbers of possible combinations that we could have, and the job of a protein engineer is to find that one or a few of the proteins within that space that do what we want it to do.&lt;/p&gt;



&lt;p&gt;So when a human has an idea of, OK, here’s what I want a protein to do, we have various techniques of finding that desired protein, one of which is using artificial intelligence and trying to either sift through that milieu of potential proteins or, as we’ll talk about more in this podcast, physically generating them. So creating them in a way, sampling them out of some distribution of reasonable proteins.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Great. So I wanted to throw it to James now to talk about how protein design goes from computer to reality—from in silico to test tubes. What role does Twist Bioscience&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; play in transforming digital protein designs into synthesized proteins? And maybe we can talk also about what safeguards are in place at your company and why do we need them.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;So all of these proteins that Bruce has described are encoded in DNA. So the language that our cells use to kind of store the information about how to make these proteins is all encoded in DNA. And so if you as an engineer have designed a protein and you want to test it to see if it does what you think it does, the first step is to have the DNA that encodes that protein manufactured, and companies like Twist carry out that role.&lt;/p&gt;



&lt;p&gt;So we are cognizant also, however, that these are what are called &lt;em&gt;dual-use technologies&lt;/em&gt;. So you can use DNA and proteins for an incredible variety of amazing applications. So drug development, agricultural improvements, bioindustrial manufacturing, all manner of incredible applications. But you could also potentially use those to cause harm so toxins or other, you know, sort of biological misuse.&lt;/p&gt;



&lt;p&gt;And so the industry has since at least 2010 recognized that they have a responsibility to make sure that when we’re asked to make some sequence of DNA that we understand what that thing is encoding and who we’re giving it for. So we’re screening both the customer that’s coming to us and we’re screening the sequence that they’re requesting.&lt;/p&gt;



&lt;p&gt;And so Twist has long invested in a very, sort of, complicated system for essentially reverse engineering the constructs that we’re asked to make so that we understand what they are. And then a system where we engage with our customers and make sure that they’re going to use those for legitimate purpose and responsibly.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And how do the emergence of these new generative AI tools influence how you think about risk?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;A lot of the power of these AI tools is they allow us to make proteins or design proteins that have never existed before in nature to carry out functions that don’t exist in the natural world. That’s an extremely powerful capability.&lt;/p&gt;



&lt;p&gt;But the existing defensive tools that we use at DNA synthesis companies generally rely on what’s called homology, similarity to known naturally occurring sequences, to determine whether something might pose risk. And so AI tools kind of break the link between those two things.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Now you also serve as chair of the International Gene Synthesis Consortium&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Can you tell us a little bit more about the IGSC, its mission, how it supports global biosecurity?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Certainly. So the IGSC was founded in 2010[1] and right now has grown to more than 40 companies and organizations across 10 countries. And the IGSC is essentially a place where companies who might be diehard competitors in the market around nucleic acid synthesis come together and design and develop best practices around biosecurity screening to, kind of, support the shared interest we all have in making sure that these technologies are not subject to misuse.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks, James. Now, Tessa, your organization, IBBIS&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is focused—it’s a beautiful mission—on advancing science while minimizing &lt;em&gt;catastrophic&lt;/em&gt; risk, likelihood of &lt;em&gt;catastrophic&lt;/em&gt; risk. When we say catastrophic risk, what do we really mean, Tessa, in the context of biology and AI? And how is that … do you view that risk landscape as evolving as AI capabilities are growing?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN:&lt;/strong&gt; I think the … to be honest, as a person who’s been in biosecurity for a while, I’ve been surprised by how much of the conversation about the risks from advances in artificial intelligence has centered on the risk of engineered biological weapons and engineered pandemics.&lt;/p&gt;



&lt;p&gt;Even recently, there was a new discussion on introducing redlines for AI that came up at the UN General Assembly. And the very first item they list in their list of risks, if I’m not mistaken, was engineered pandemics, which is exactly the sort of thing that people fear could be done, could be done with these biological AI tools.&lt;/p&gt;



&lt;p&gt;Now, I think that when we talk about catastrophic risk, we talk about, you know, something that has an impact on a large percentage of humanity. And I think the reason that we think that biotechnologies pose a catastrophic risk is that we believe there, as we’ve seen with many historical pandemics, there’s a possibility for something to emerge or be created that is beyond our society’s ability to control.&lt;/p&gt;



&lt;p&gt;You know, there were a few countries in COVID that managed to more or less successfully do a zero-COVID policy, but that was not, that was not most countries. That was not any of the countries that I lived in. And, you know, we saw millions of people die. And I think we believe that with something like the 1918 influenza, which had a much higher case fatality rate, you could have far more people die.&lt;/p&gt;



&lt;p&gt;Now, why we think about this in the context of AI and where this connects to DNA synthesis is that, you know, there is a … these risks of both, sort of, public health risks, pandemic risks, and misuse risks—people deliberately trying to do harm with biology, as we’ve seen from the long history of biological weapons programs—you know, we think that those might be accelerated in a few different ways by AI technology, both the potential … and I say potential here because as everyone who has worked in a wet lab—which I think is everyone on this call—knows, engineering biology is really difficult. So there’s maybe a potential for it to become easier to develop biological technology for the purposes of doing harm, and there’s maybe also the potential to create novel threats.&lt;/p&gt;



&lt;p&gt;And so I think people talk about both of those, and people have been looking hard for possible safeguards. And I think one safeguard that exists in this biosecurity world that, for example, doesn’t exist as cleanly in the cybersecurity world is that none of these biological threats can do harm until they are realized in physical reality, until you actually produce the protein or produce the virus or the microorganism that could do harm. And so I think at this point of production, both in DNA synthesis and elsewhere, we have a chance to introduce safeguards that can have a really large impact on the amount of risk that we’re facing—as long as we develop those safeguards in a way that keeps pace with AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Well, thanks, Tessa. So, Bruce, our project began when I posed a challenge to you of the form: could current open-source AI tools be tasked with rewriting toxic protein sequences in a way that preserves their native structure, and might they evade today’s screening systems?&lt;/p&gt;



&lt;p&gt;And I was preparing for a global workshop on AI and biosecurity that I’d been organizing with Frances Arnold, David Baker, and Lynda Stuart, and I wanted a concrete case study to challenge attendees. And what we found was interesting and deeply concerning.&lt;/p&gt;



&lt;p&gt;So I wanted to dive in with you, Bruce, on the technical side. Can you describe some about the generative pipeline and how it works and what you did to build what we might call an AI and biosecurity red-teaming pipeline for testing and securing biosecurity screening tools?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Sure. Yeah. I think the best place to start with this is really by analogy.&lt;/p&gt;



&lt;p&gt;An analogy I often use in this case is the type of image generation AI tools we’re all familiar with now where I can tell the AI model, “Hey, give me a cartoonish picture of a dog playing fetch.” And it’ll do that, and it’ll give us back something that is likely never been seen before, right. That exact image is new, but the theme is still there. The theme is this dog.&lt;/p&gt;



&lt;p&gt;And that’s kind of the same technology that we’re using in this red-teaming pipeline. Only rather than using plain language, English, we’re passing in what we would call conditioning information that is relevant to a protein.&lt;/p&gt;



&lt;p&gt;So our AI models aren’t at the point yet where I can say, “Give me a protein that does &lt;em&gt;x&lt;/em&gt;.” That would be the dream. We’re a long way from that. But what instead we do is we pass in things that match that theme that we’re interested in. So rather than saying, “Hey, give me back the theme on a dog,” we pass in information that we know will cause or at least push this generative model to create a protein that has the characteristics that we want.&lt;/p&gt;



&lt;p&gt;So in the case of that example you just mentioned, Eric, it would be the protein structure. Like I mentioned earlier, we usually say structure determines function. There’s obviously a lot of nuance to that, but we can, at a first approximation, say structure determines function. So if I ask an AI model, ”Hey, here’s this structure; give me a protein sequence that folds to this structure,” just like with that analogy with the dog, it’s going to give me something that matches that structure but that is likely still never been seen before. It’s going to be a new sequence.&lt;/p&gt;



&lt;p&gt;So you can imagine taking this one step further. In the red-teaming pipeline, what we would do is take a protein that should normally be captured by DNA synthesis screening—that &lt;em&gt;would&lt;/em&gt; be captured by DNA synthesis screening—find its structure, pass it through one of these models, and get variants on the theme of that structure so these new sequences, these synthetic homologs that you mentioned, &lt;em&gt;paraphrased&lt;/em&gt;, &lt;em&gt;reformulated&lt;/em&gt;, whatever phrase we want to use to describe them.&lt;/p&gt;



&lt;p&gt;And they have a chance or a greater chance than &lt;em&gt;not&lt;/em&gt; of maintaining the structure and so maintaining the function while being sufficiently different that they’re not detected by these tools anymore.&lt;/p&gt;



&lt;p&gt;So that’s the nuts and bolts of how the red-teaming pipeline comes together. We use more tools than just structure. I think structure is the easiest one to understand. But we have a suite of tools in there, each pass different conditioning information that causes the model to generate sequences that are paraphrased versions of potential proteins of concern.&lt;s&gt;&lt;/s&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;But to get down to brass tacks, what Bruce did for the framing study was … we took the toxic, well-known toxic protein ricin, as we described in a framing paper that’s actually part of the appendix now to the &lt;em&gt;Science&lt;/em&gt; publication, and we generated through this pipeline, composed of open-source tools, thousands of AI-rewritten versions of ricin.&lt;/p&gt;



&lt;p&gt;And this brings us to the next step of our project, way back when, at the early … in the early days of this effort, where Twist Bioscience was one of the companies we approached with what must have seemed like an unusual question to your CEO, in fact, James: would you be open to testing whether current screening systems could detect thousands of AI-rewritten versions of ricin, a well-known toxic protein?&lt;/p&gt;



&lt;p&gt;And your CEO quickly connected me with you, James. So, James, what were your first thoughts on hearing about this project, and how did you respond to our initial framing study?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;I think my first response was gratitude and excitement. So it was fantastic that Microsoft had really leaned forward on this set of ideas and had produced this dataset. But to have it, you know, show up on our doorstep in a very concrete way with a partner that was ready to, sort of, help us try and address that, I think was a really … a valuable opportunity. And so we really leapt at that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ:&lt;/strong&gt; And the results were that both for you and another company, major producer IDT [Integrated DNA Technologies], those thousands of variants flew through … flew under the radar of the biosecurity screening software as we covered in that framing paper.&lt;/p&gt;



&lt;p&gt;Now, after our initial findings on this, we quietly shared the paper with a few trusted contacts, including some in government. Through my work with the White House Office of Science and Technology Policy, or OSTP, we connected up with biosecurity leads there, and it was an OSTP biosecurity lead who described our results as the first &lt;em&gt;zero day&lt;/em&gt; in AI and biosecurity. And now in cybersecurity, a zero day is a vulnerability unknown to defenders generally, meaning there’s no time to respond before it could be exploited should it be known.&lt;/p&gt;



&lt;p&gt;In that vein, we took a cybersecurity approach. We stood up a CERT—C-E-R-T—a &lt;em&gt;cybersecurity [computer] emergency response team&lt;/em&gt; approach used in responding to cybersecurity vulnerabilities, and we implemented this process to address what we saw as a vulnerability with AI-enabled challenges to biosecurity.&lt;/p&gt;



&lt;p&gt;At one point down the line, it was so rewarding to hear you say, James, “I’m really glad Microsoft got here first.” I’m curious how you think about this kind of AI-enabled vulnerability compared to other ones, biosecurity threats, you’ve encountered, and I’d love to hear your perspective on how we handled the situation from the early discovery to the coordination and outreach.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Yeah, I think in terms of comparison known threats, the challenge here is really there is no good basis on which we can just, sort of, say, &lt;em&gt;Oh, I’ll build a new tool to detect this concrete universe of things&lt;/em&gt;, right. This was more a pattern of I’m going to use tools—and I love the name “Paraphrase”; it’s a fantastic name—I can paraphrase anything that I would normally think of as biological … as &lt;em&gt;posing&lt;/em&gt; biological risk, and now that thing is harder to detect for existing tools. And so that really was a very eye-opening experience, and I think the practice of forming this CERT response, putting together a group of people who were well versed not just in the threat landscape but also in the defensive technologies, and then figuring out how to mitigate that risk and broaden that study, I think, was a really incredibly valuable response to the entire synthesis industry.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Yeah, and, Bruce, can you describe a little bit about the process by which we expanded the effort beyond our initial framing study to more toxins and then to a larger challenge set and then the results that we pursued and achieved?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Yeah, of course. So, you know, using machine learning lingo, you don’t want to &lt;em&gt;overfit&lt;/em&gt; to a single example. So early on with this, as part of the framing study, we were able to show or I should say James and coworkers across the screening field were able to show that this could be patched, right. We needed to just make some changes to the tools,&amp;nbsp;and we could at the very least detect ricin or reformulated versions of ricin.&lt;/p&gt;



&lt;p&gt;So the next step of course was then, OK, how generalizable are these patches? Can they detect other reformulated sequences, as well? So we had to expand the set of proteins that we had reformulated. We couldn’t just do 10s of thousands of ricins. We had to do 10s of thousands of name your other potentially hazardous …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;I think we had 72, was it?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;It was 72 in the end that we ended up at. I believe, James, it was you and maybe Jake, another one of the authors on the list … on the paper, who primarily put that list together …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;This is Jacob Beal … Jacob Beal at Raytheon BBN.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;I think James actually might be the better one to answer how this list was expanded.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS:&lt;/strong&gt;&amp;nbsp;Initially the focus [was] on ricin as a toxin so that list expanded to 62 sort of commonly controlled toxins that are subject to an export control restriction or other concern. And then on top of that, we added 10 viral proteins. So we didn’t really just want to look at toxins. We also wanted to look at viral proteins, largely because those proteins tend to have multiple functions. They have highly constrained structures. And so if we could work in a toxin context, could Paraphrase also do the same for viral proteins, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And, Bruce, can you describe some about how we characterize the updates and the, we’ll say, the boost in capabilities of the patched screening tools?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt; So we had, like you said, Eric, 72 base proteins or template proteins. And for each of those, we had generated a few 100 to a couple thousand reformulated variants of them. The only way to really get any sense of validity of those sequences was to predict their structures. So we predicted protein structures for I think it was 70ish thousand protein structures in the end that we had to predict and score them using in silico metrics. So things like, how similar is this to that template, wild-type protein structure that we used as our conditioning information?&lt;/p&gt;



&lt;p&gt;We put them on a big grid. So we have two axes. We have on the x-axis—and this is a figure in our paper—the quality of the prediction. It’s essentially a confidence metric: how realistic is this protein sequence? And on the other axis is, how similar is the predicted structure of this variant to the original? And ultimately, what we were wanting to see was the proteins that scored well in both of those metrics, so that showed up in the top right of that diagram, were caught primarily, because these are again the ones that are &lt;em&gt;most likely&lt;/em&gt;, having to say &lt;em&gt;most likely&lt;/em&gt;, to retain function of the original.&lt;/p&gt;



&lt;p&gt;So when you compare the original tools—Tool Series A, right, the unpatched tools—what you’ll find is varying degrees of success in the top right. It varied by tool. But in some cases, barely anything being flagged as potentially hazardous. And so improvement is then in the next series—Series B, the patched version of tools—we have more flagged in that upper-right corner.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And we felt confident that we had a more AI-resilient screening solution across the companies, and, James, at this point, the whole team decided it was time to disclose the vulnerability as well as the patch details and pointers to where to go for the updated screening software and to communicate this to synthesis companies worldwide via the IGSC. This was probably July, I think, of 2024. What was that process like, and how did members respond?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;I think members were really grateful and excited. To present to that group, to say, hey, this activity (a) has gone on, (b) was successful, and (c) was kept close hold until we knew how to mitigate this, I think everyone was really gratified by that and comforted by the fact that now they had kind of off-the-shelf solutions that they could use to improve their resilience against any incoming heavily engineered protein designs.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks, James.&lt;/p&gt;



&lt;p&gt;Now, I know that we all understand this particular effort to be important but a &lt;em&gt;piece&lt;/em&gt; of the biosecurity and AI problem. I’m just curious to … I’ll ask all three of you to just share some brief reflections.&lt;/p&gt;



&lt;p&gt;I know, Bruce, you’ve been on … you’ve stayed on this, and we’ve—all of us on the original team—have other projects going on that are pushing on the frontiers ahead of where we were with this paper when we published it.&lt;/p&gt;



&lt;p&gt;Let me start with Tessa in terms of, like, what new risks do you see emerging as AI accelerates and maybe couple that with thoughts about how do we proactively get ahead of them.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;Yeah, I think with the Paraphrase’s work, as Bruce explained so well, you know, I sometimes use the metaphor of the previous response that the IGSC had to do, the synthesis screening community, where it used to be you could look for similarities to DNA sequences, and then everyone started doing synthetic biology where they were doing codon optimization so that proteins could express more efficiently in different host organisms, and now all of a sudden, well, you’ve scrambled your DNA sequence and it doesn’t look very similar even though your protein sequence actually still looks, you know, very similar or often the same once it’s been translated from DNA to protein, and so that was a, you know, many, many in the industry were already screening both DNA and protein, but they had to start screening … &lt;em&gt;everybody&lt;/em&gt; had to start screening protein sequences even just to do the similarity testing as these codon optimization tools became universal.&lt;/p&gt;



&lt;p&gt;I feel like we’re, kind of, in a similar transition phase with protein-design, protein-rephrasing, tools where, you know, these tools are still in many cases drawing from the natural distribution of proteins. You know, I think some of the work we saw in, you know, designing novel CRISPR enzymes, you go, OK, yeah, it is novel; it’s very unlike any &lt;em&gt;one&lt;/em&gt; CRISPR enzyme. But if you do a massive multiple sequence alignment of every CRISPR enzyme that we know about, you’re like, OK, this fits in the distribution of those enzymes. And so, you know, I think we’re not … we’re having to do a more flexible form of screening, where we look for things that are kind of within distribution of natural proteins.&lt;/p&gt;



&lt;p&gt;But I feel like broadly, all of the screening tools were able to respond by doing something like that. And I think … I still feel like the clock is ticking down on that and that as the AI tools get better at predicting function and designing, sort of, novel sequences to pursue a particular function, you know—you have tools now that can go from Gene Ontology terms to a potential structure or potential sequence that may again be much farther out of the distribution of natural protein—I think all of us on the screening side are going to have to be responding to that, as well.&lt;/p&gt;



&lt;p&gt;So I think I see this as a necessary ongoing engagement between people at the frontier of designing novel biology and people at the frontier of producing all of the materials that allow that novel biology to be tested in the lab. You know, I think this feels like the first, you know, detailed, comprehensive zero day disclosure and response. But I think that’s … I think we’re going to see more of those. And I think what I’m excited about doing at IBBIS is trying to encourage and set up more infrastructure so that you can, as an AI developer, disclose these new discoveries to the people who need to respond before the publication comes out.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thank you, Tessa.&lt;/p&gt;



&lt;p&gt;The, the … Bruce, I mean, you and I are working on all sorts of dimensions. You’re leading up some efforts at Microsoft, for example, on the foundation model front and so on, among other directions. We’ve talked about new kinds of embedding models that might go beyond sequence and structure. Can you talk a little bit about just a few of the directions that just paint the larger constellation of the kinds of things that we talk about when we put our worry hats on?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;I feel like that could have its own dedicated podcast, as well. There’s a lot … [LAUGHTER] there’s a lot to talk about.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Yeah. We want to make sure that we don’t tell the world that the whole problem is solved here.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Right, right, right. I think Tessa said it really, really well in that most of what we’re doing right now, it’s a variant on a known theme. I have to know the structure that does something bad to be able to pass it in as context. I have to know some existing sequence that does something bad to pass it in.&lt;/p&gt;



&lt;p&gt;And obviously the goal is to move away from that in benign applications, where when I’m designing something, I often want to design it because nothing exists [LAUGHS] that already does it. So we are going to be heading to this space where we don’t know what this protein does. It’s kind of a circular problem, right, where we’re going to need to be able to predict what some obscure protein sequence does in order to be able to still do our screening.&lt;/p&gt;



&lt;p&gt;Now, the way that I think about this, I often think about it beyond just DNA synthesis screening. It’s one line of defense, and there needs to be many lines of defense that come into play here that go beyond just relying on this one roadblock. It’s a very powerful roadblock. It’s a very powerful barrier. But we need to be proactively thinking about how we broaden the scope of defenses. And there are lots of conversations that are ongoing. I won’t go into the details of them. Again, that would be its own podcast.&lt;/p&gt;



&lt;p&gt;But primarily my big push—and I think this is emerging consensus in the field, though I don’t want to speak for everybody—is it needs to … any interventions we have need to come more at the systems level and less at the model level, primarily because this is such dual-use technology. If it can be used for good biological design, it can be used for bad biological design. Biology has no sense of morality. There is no bad protein. It’s just &lt;em&gt;a&lt;/em&gt; protein.&lt;/p&gt;



&lt;p&gt;So we need to think about this differently than how we would maybe think about looking at the outputs of that image generator model that I spoke about earlier, where I can physically look at an image and say, don’t want my model producing that, do want my model producing that. I don’t have that luxury in this space. So it’s a totally different problem. It’s an evolving problem. Conversations are happening about it, but the work is very much not done.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And, James, I want to give you the same open question, but I’d like to apply what Bruce just said on system level and so on and in the spirit of the kind of things that you’re very much involved with internationally to also add to it, just get some comments on programs and policies that move beyond technical solutions for governance mechanisms—logging, auditing nucleic acid orders, transparency, various kinds—that might complement technical approaches like Paraphrase and their status today.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Yeah, I’m very gratified that Bruce said that we, the synthesis industry, should not be the sole bulwark against misuse. That is very comforting and correct.&lt;/p&gt;



&lt;p&gt;Yeah, so the US government published a guidance document in 2023 that essentially said you, the entire biotech supply chain, have a responsibility to make sure that you’re evaluating your customers. You should know your customer; you know that they’re legitimate. I think that’s an important practice.&lt;/p&gt;



&lt;p&gt;Export controls are designed to minimize the movement of equipment and materials that can be used in support of these kinds of misuse activities. And then governments have really been quite active in trying to incentivize, you know, sort of what we would think of as positive behavior, so screening, for example, in DNA synthesis companies. The US government created a framework in 2024, and it’s under a rewrite now to basically say US research dollars will only go to companies who make nucleic acid who do these good things. And so that is using, kind of, the government-funding carrot to, kind of, continue to build these layers of defense against potential misuse.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks. Now, discussing risk, especially when it involves AI and biosecurity, isn’t always easy. As we’ve all been suggesting, some worry about alarming the public or arming bad actors. Others advocate for openness as a principle of doing science with integrity.&lt;/p&gt;



&lt;p&gt;A phase of our work as we prepared our paper was giving serious thought to both the benefits and the risks of transparency about what it was that we were doing. Some experts encouraged full disclosure as important for enhancing the science of biosecurity. Other experts, &lt;em&gt;all experts&lt;/em&gt;, cautioned against what are called &lt;em&gt;information hazards&lt;/em&gt;, the risk of sharing the details to enable malevolent actions with our findings or our approach.&lt;/p&gt;



&lt;p&gt;So we faced a real question: how can we support open science while minimizing the risk of misuse? And we took all the input we got, even if it was contradictory, very seriously. We carefully deliberated about a good balance, and even &lt;em&gt;then&lt;/em&gt;, once we chose our balance and submitted our manuscript to &lt;em&gt;Science&lt;/em&gt;, the peer reviewers came back and said they wanted some of the more sensitive details that we withheld with explanations as to why.&lt;/p&gt;



&lt;p&gt;So this provoked some thinking out of the box about a novel approach, and we came up with a perpetual gatekeeping strategy where requests for access to sensitive methods and data and even the software across different risk categories would be carefully reviewed by a committee and a process for access that would continue in perpetuity.&lt;/p&gt;



&lt;p&gt;Now, we brought the proposal to Tessa and her team at IBBIS—this is a great nonprofit group; look at their mission—and we worked with Tessa and her colleagues to refine a workable solution that was accepted by &lt;em&gt;Science&lt;/em&gt; magazine as a new approach to handling information hazards as first demonstrated by our paper.&lt;/p&gt;



&lt;p&gt;So, Tessa, thank you again for helping us to navigate such a complex challenge. Can you share your perspective on information hazards? And then walk us through how our proposed system ensures responsible data and software sharing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;Yeah. And thanks, Eric.&lt;/p&gt;



&lt;p&gt;It’s all of the long discussions we had among the group of people on this podcast and the other authors on the paper and many people we engaged, you know, technical experts, people in various governments, you know, we heard a lot of contradictory advice.&lt;/p&gt;



&lt;p&gt;And I think it showed us that there isn’t a consensus right now on how to handle information hazards in biotechnology. You know, I think … I don’t want to overstate how much of a consensus there is in cybersecurity either. If you go to DEF CON, you’ll hear people about how they’ve been mistreated in their attempts to do responsible disclosure for pacemakers and whatnot. But I think we’re … we have even less of a consensus when it comes to handling biological information.&lt;/p&gt;



&lt;p&gt;You know, you have some people who say, oh, because the size of the consequences could be so catastrophic if someone, you know, releases an engineered flu or something, you know, we should just never share information about this. And then you have other people who say there’s no possibility of building defenses unless we share information about this. And we heard very strong voices with both of those perspectives in the process of conducting this study.&lt;/p&gt;



&lt;p&gt;And I think what we landed on that I’m really excited about and really excited to get feedback on now that the paper is out, you know, if you go and compare our preprint, which came out in December of 2024, and this paper in October 2025, you’ll see a lot of information got added back in.&lt;/p&gt;



&lt;p&gt;And I’m excited to see people’s reaction to that because even back in January 2025, talking with people who were signatories to the responsible biodesign commitments, they were really excited that this was such an empirically concrete paper because they’d maybe read a number of papers talking about biosecurity risks from AI that didn’t include a whole lot of data, you know, often, I think, because of concerns about information hazards. And they found the arguments in this paper are much more convincing because we are able to share data.&lt;/p&gt;



&lt;p&gt;So the process we underwent that I felt good about was trying to really clearly articulate, when we talk about an information hazard, what are we worried about being done with this data? And if we put this data in public, completely open source, does it shift the risk at all? You know, I think doing that kind of marginal contribution comparison is really important because it also let us make more things available publicly.&lt;/p&gt;



&lt;p&gt;But there were a few tiers of data that after a lot of discussion amongst the authors of the paper, we thought, OK, potentially someone who wanted to do harm, if they got access to this data, it might make it easier for them. Again, not necessarily saying it, you know, it opens the floodgates, but it might make it easier for them. And when we thought about that, we thought, OK, you know, giving all of those paraphrased protein sequences, maybe, maybe that, you know, compared to having to set up the whole pipeline with the open-source tools yourself, just giving you those protein sequences, maybe that makes your life a bit easier if you’re trying to do harm.&lt;/p&gt;



&lt;p&gt;And then we thought, OK, giving you those protein sequences plus whether or not they were successfully flagged, maybe that makes your life, you know, quite a bit easier. And then finally, we thought, OK, the code that we want to share with some people who might try to reproduce these results or might try to build new screening systems that are more robust, we want to share the code with them. But again, if you have that whole code pipeline just prepared for you, it might really help make your life easier if you’re trying to do harm.&lt;/p&gt;



&lt;p&gt;And so we, sort of, sorted the data into these three tiers and then went through a process actually very inspired by the existing customer screening processes in nucleic acid synthesis about how to determine, you know, we tried to take an approach not of what gets you in but what gets you out. You know, for the most part, we think it should be possible to access this data.&lt;/p&gt;



&lt;p&gt;You know, if you have an affiliation with a recognizable institution or some good explanation of why you don’t have one right now, you know, if you have a reason for accessing this data, it shouldn’t be too hard to meet those requirements, but we wanted to have some in place. And we wanted it to be possible to rule out some people from getting access to this data. And so we’ve tried to be extremely transparent about what those are. If you go through our data access process and for some reason you get rejected, you’ll get a list of, “Here’s the reasons we rejected you. If you don’t think that’s right, get back to us.”&lt;/p&gt;



&lt;p&gt;So I’m really excited to pilot this in part because I think, you know, we’re already in conversations with some other people handling potential bio-AI information hazard about doing a similar process for their data of, you know, tiering it, determining which gates to put in which tiers, but I really hope a number of people do get access through the process or if they try and they fail, they tell us why. Because I think as we move toward this world of potentially, you know, biology that is much easier to engineer, partly due to dual-use tools, you know, my dream is it’s, like, still hard to engineer harm with biology, even if it’s really easy to engineer biology. And I think these, kind of, new processes for managing access to things, this sort of like, you know, open but not completely public, I think those can be a big part of that layered defense.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks, Tessa. So we’re getting close to closing, and I just thought I would ask each of you to just share some reflections on what we’ve learned, the process we’ve demonstrated, the tools, the policy work that we did, this idea of facing the dual-use dilemma with … even at the information hazard level, with sharing information versus withholding it. What do you think about how our whole end to end of the study, now reaching the two-year point, can help other fields facing dual-use dilemmas?&lt;/p&gt;



&lt;p&gt;Tessa, Bruce, James … James, have you ever thought about that? And we’ll go to Bruce and then Tessa.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS:&lt;/strong&gt;&amp;nbsp;Yeah, I think it was an excellent model. I would like to see a study like this repeated on a schedule, you know, every six months because from where I sit, you know, the tools that we used for this project are now two years old. And so capabilities have moved on. Is the picture the same in terms of defensive capability? And so using that model over time, I think, would be incredibly valuable. And then using the findings to chart, you know, how much should we be investing in alternative strategies for this kind of risk mitigation for AI tool … the &lt;em&gt;products&lt;/em&gt; of AI tools?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Bruce.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Yeah, I think I would extend on what James said. The anecdote I like to point out about this project is, kind of, our schedule. We found the vulnerability and it was patched within a week, two weeks, on all major synthesis screening platforms. We wrote the paper within a month. We expanded on the paper within two months, and then we spent a year and a half to nearly two years [LAUGHS] trying to figure out what goes into the paper; how do we release this information; you know, how do we do this responsibly?&lt;/p&gt;



&lt;p&gt;And my hope is similar to what James said. We’ve made it easier for others to do this type of work. Not this exact work; it doesn’t have to necessarily do with proteins. But to do this type of work where you are dealing with potential hazards but there is also value in sharing and that hopefully that year and a half we spent figuring out how to appropriately share and what to share will not be a year and a half for other teams because these systems are in place or at least there is an example to follow up from. So that’s my takeaway.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ:&lt;/strong&gt; Tessa, bring us home—&lt;em&gt;bring us home!&lt;/em&gt; [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;&lt;em&gt;Bring us home!&lt;/em&gt; Let’s do it faster next time. [LAUGHTER] Come talk to any of us if you’re dealing with this kind of stuff. You know, I think IBBIS, especially, we want to be a partner for building those layers of defense and, you know, having ripped out our hair as a collective over the past year and a half about the right process to follow here, I think we all really hope it’ll be faster next time.&lt;/p&gt;



&lt;p&gt;And I think, you know, the other thing I would encourage is if you’re an AI developer, I would encourage you to think about how your tool can strengthen screening and strengthen recognition of threats.&lt;/p&gt;



&lt;p&gt;I know James and I have talked before about how, you know, our Google search alerts each week send us dozens of cool AI bio papers, and it’s more like once a year or maybe once every six months, if we’re lucky, that we get something that’s like applying AI bio to biosecurity. So, you know, if you’re interested in these threats, I think we’d love to see more work that’s directly applied to facing these threats using the most modern technology.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Well said.&lt;/p&gt;



&lt;p&gt;Well, Bruce, James, Tessa, thank you so much for joining me today and for representing the many collaborators, both coauthors and beyond, who made this project possible.&lt;/p&gt;



&lt;p&gt;It’s been a true pleasure to work with you. I’m so excited about what we’ve accomplished, the processes and the models that we’re now sharing with the world. And I’m deeply grateful for the collective intelligence and dedication that really powered the effort from the very beginning. So thanks again.&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Thanks, Eric.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Thank you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;Thank you.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1] The original organization was founded in 2009 and became the International Gene Synthesis Consortium in 2010.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/EricBruceTessaJames-Ideas_TW_LI_FB_1200x627-1.jpg" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;/figure&gt;






&lt;p&gt;Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series &lt;em&gt;Ideas&lt;/em&gt;, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&lt;/p&gt;



&lt;p&gt;AI has been described as a “dual use” technology: the capabilities that can be leveraged for good can also potentially be used to cause harm. In this episode, Microsoft Chief Scientific Officer Eric Horvitz and his guests—Bruce Wittmann, a senior applied scientist at Microsoft; Tessa Alexanian&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a technical lead at the International Biosecurity and Biosafety Initiative for Science (IBBIS);&amp;nbsp;and James Diggans&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a vice president at Twist Bioscience—explore this idea in the context of AI-powered protein design.&lt;/p&gt;



&lt;p&gt;With Horvitz at the lead, Alexanian, Diggans, and Wittmann were part of a cross-sector team that demonstrated toxic protein candidates could be designed with help from AI—and that they could bypass the systems in place to defend against their creation. The project, known as the &lt;em&gt;Paraphrase Project&lt;/em&gt;, culminated in a cybersecurity-style response, a more robust protein screening system, and a modified approach to peer review with implications for how we think about and tackle AI risk more broadly. The work was recently published in &lt;em&gt;Science.&lt;/em&gt;&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript-1"&gt;Transcript&lt;/h2&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ERIC HORVITZ: &lt;/strong&gt;You’re&amp;nbsp;listening to&amp;nbsp;&lt;em&gt;Ideas&lt;/em&gt;, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code.&amp;nbsp;I’m&amp;nbsp;Eric Horvitz, Microsoft’s chief scientific officer, and in this series, we explore the technologies shaping our future and the&amp;nbsp;big ideas&amp;nbsp;that propel them forward.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;



&lt;p&gt;Today,&amp;nbsp;I’m&amp;nbsp;excited to talk about the Paraphrase Project, an effort I co-led exploring how&amp;nbsp;advances in&amp;nbsp;AI tools&amp;nbsp;for protein design&amp;nbsp;might&amp;nbsp;impact&amp;nbsp;biosecurity. The results were reported in our recent paper,&amp;nbsp;“Strengthening nucleic acid biosecurity screening against generative protein design tools,”&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;published in&amp;nbsp;&lt;em&gt;Science&lt;/em&gt;&amp;nbsp;on Oct. 2.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Joining me are&amp;nbsp;three&amp;nbsp;of the larger set of&amp;nbsp;coauthors on that paper:&amp;nbsp;Bruce Wittmann, senior applied scientist at Microsoft;&amp;nbsp;James&amp;nbsp;Diggans, vice president at Twist Bioscience and chair of the board for the International Gene Synthesis Consortium;&amp;nbsp;and Tessa Alexanian, technical lead at the International Biosecurity and Biosafety Initiative for Science, also known as &lt;em&gt;IBBIS&lt;/em&gt;.&amp;nbsp;&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;Now, let’s&amp;nbsp;rewind two years.&amp;nbsp;Almost to&amp;nbsp;the day, Bruce and I uncovered a vulnerability. While preparing a case study for a workshop on AI and biosecurity, we discovered that open-source AI protein design tools could be used to redesign toxic proteins in ways that could bypass biosecurity screening systems, systems set up to&amp;nbsp;identify&amp;nbsp;incoming orders of concern.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now in that work, we&amp;nbsp;created an AI pipeline from open-source tools that could&amp;nbsp;essentially “paraphrase” the amino acid sequences—reformulating&amp;nbsp;them while&amp;nbsp;working to&amp;nbsp;preserve&amp;nbsp;their structure and potentially their function.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These paraphrased sequences could evade the screening systems used by major DNA&amp;nbsp;synthesis companies, and these are the systems that scientists rely on to safely produce AI-designed proteins.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Now, experts in the field described this finding as the first “zero day” for AI and biosecurity.&amp;nbsp;And this&amp;nbsp;marked the beginning of a deep, two-year collaborative effort to investigate and address this challenge.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;With the help of a&amp;nbsp;strong&amp;nbsp;cross-sector team—including James, Tessa, Bruce, and many others—we worked behind the scenes to build AI biosecurity&amp;nbsp;&lt;em&gt;red-teaming approaches&lt;/em&gt;,&amp;nbsp;probe for vulnerabilities, and to design practical fixes. These “patches,” akin to those in cybersecurity,&amp;nbsp;have now been shared with&amp;nbsp;organizations&amp;nbsp;globally to strengthen biosecurity screening.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This has been one of the most fascinating projects&amp;nbsp;I’ve&amp;nbsp;had the privilege to work on, for its technical complexity, its ethical and policy dimensions, and the remarkable collaboration across industry, government, and nonprofit sectors.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The project highlights that the&amp;nbsp;same AI tools capable of&amp;nbsp;incredible&amp;nbsp;good can also be misused, requiring us to be vigilant, thoughtful, and creative so we continue to get the most benefit out of AI tools while working to ensure&amp;nbsp;that&amp;nbsp;we avoid costly misuses.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;With that, let me officially welcome our guests.&lt;s&gt;&lt;/s&gt;&lt;/p&gt;



&lt;p&gt;Bruce, James, Tessa, welcome to the podcast.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;BRUCE WITTMANN: &lt;/strong&gt;Thanks, Eric.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;JAMES DIGGANS: &lt;/strong&gt;Thanks for having us.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;It’s been such a pleasure working closely with each of you, not only for your expertise but also for your deep commitment and passion about public health and global safety.&lt;/p&gt;



&lt;p&gt;Before we dive into the technical side of things, I’d like to ask each of you, how did you get into this field? What inspired you to become biologists and then pursue the implications of advances in AI for biosecurity? Bruce?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Well, I’ve always liked building things. That’s where I would say I come from. You know, my hobbies when I’m not working on biology or AI things—as you know, Eric—is, like, building things around the house, right. Doing construction. That kind of stuff.&lt;/p&gt;



&lt;p&gt;But my broader interests have always been biology, chemistry. So I originally got into organic chemistry. I found that was fascinating. From there, went to synthetic biology, particularly metabolic engineering, because that’s kind of like organic chemistry, but you’re wiring together different parts of an organism’s metabolism rather than different chemical reactions. And while I was working in that space, I, kind of, had the thought of there’s got to be an easier way to do this [LAUGHS] because it is really difficult to do any type of metabolic engineering. And that’s how I got into the AI space, trying to solve these very complicated biological problems, trying to build things that we don’t necessarily even understand using our understanding from data or &lt;em&gt;deriving&lt;/em&gt; understanding from data.&lt;/p&gt;



&lt;p&gt;So, you know, that’s the roundabout way of how I got to where I am—the abstract way of how I got to where I am.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And, Tessa, what motivated you to jump into this area and zoom into biology and biosciences and helping us to avoid &lt;em&gt;catastrophic&lt;/em&gt; outcomes?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN:&lt;/strong&gt; Yeah, I mean, probably the origin of me being really excited about biology is actually a book called &lt;em&gt;[The] Lives of [a] Cell&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; by Lewis Thomas, which is an extremely beautiful book of essays that made me be like, &lt;em&gt;Oh, wow, life is just incredible&lt;/em&gt;. I think I read it when I was, you know, 12 or 13, and I was like, &lt;em&gt;Life is incredible. I want to work on this. This is the most beautiful science&lt;/em&gt;, right. And then I, in university, I was studying engineering, and I heard there was this engineering team for engineering biology—this iGEM&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; team—and I joined it, and I thought, &lt;em&gt;Oh, this is so cool. I really got to go work in this field of synthetic biology.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;And then I also tried doing the wet lab biology, and I was like, &lt;em&gt;Oh, but I don’t like this part&lt;/em&gt;. &lt;em&gt;I don’t actually, like, like babysitting microbes.&lt;/em&gt; [LAUGHTER] I think there’s a way … some people who are great wet lab biologists are made of really stern stuff. And they really enjoy figuring out how to redesign their negative controls so they can figure out whether it was contamination or whether it was, you know, temperature fluctuation. I’m not that, apparently.&lt;/p&gt;



&lt;p&gt;And so I ended up becoming a lab automation engineer because I could help the science happen, but I … but my responsibilities were the robots and the computers rather than the microbes, which I find a little bit intransigent.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Right. I was thinking of those tough souls; they also used their mouths to do pipetting and so on of these contaminated fluids …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Not anymore. &lt;strong&gt;ALEXANIAN: &lt;/strong&gt;It’s true. [LAUGHTER]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Not anymore. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN:&lt;/strong&gt; They used to be tougher. They used to be tougher.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;James.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS:&lt;/strong&gt; So I did my undergrad in computer science and microbiology, mostly because at the time, I couldn’t pick which of the two I liked more. I liked them both. And by the time I graduated, I was lucky enough that I realized that the intersection of the two could be a thing. And so I did a PhD in computational biology, and then I worked for five years at the MITRE Corporation. It’s a nonprofit. I got the chance to work with the US biodefense community and just found an incredible group of people working to protect forces and the population at large from biological threats and just learned a ton about both biology and also dual-use risk. And then so when Twist called me and asked if I wanted to join Twist and set up their biosecurity program, I leapt at the chance and have done that for the past 10 years.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Well, thanks everyone.&lt;/p&gt;



&lt;p&gt;I believe that AI-powered protein design in particular is one of the most exciting frontiers of modern science. It holds promise for breakthroughs in medicine, public health, even material science. We’re already seeing it lead to new vaccines, novel therapeutics, and—on the scientific front—powerful insights into the machinery of life.&lt;/p&gt;



&lt;p&gt;So there’s much more ahead, especially in how AI can help us promote wellness, longevity, and the prevention of disease. But before we get too far ahead, while some of our listeners work in bioscience, many may not have a good understanding of some of the foundations.&lt;/p&gt;



&lt;p&gt;So, Bruce, can you just give us a high-level overview of proteins? What are they? Why are they important? How do they figure into human-designed applications?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Sure. Yeah. Fortunately, I used to TA a class on AI for protein design, so it’s right in my wheelhouse. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Perfect, perfect background. [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;It’s perfect. Yeah. I got to go back to all of that. Yeah, so from the very basic level, proteins are the workhorses of life.&lt;/p&gt;



&lt;p&gt;Every chemical reaction that happens in our body—well, nearly every chemical reaction that happens in our body—most of the structure of our cells, you name it. Any life process, proteins are central to it.&lt;/p&gt;



&lt;p&gt;Now proteins are encoded by what are known as … well, I shouldn’t say encoded. They are &lt;em&gt;constructed&lt;/em&gt; from what are called amino acids—there are 20 of them—and depending on the combination and order in which you string these amino acids together, you get a different protein sequence. So that’s what we mean when we say protein sequence.&lt;/p&gt;



&lt;p&gt;The sequence of a protein then determines what shape that protein folds into in a cell, and that shape determines what the protein does. So we will often say sequence determines structure, which determines function.&lt;/p&gt;



&lt;p&gt;Now the challenge that we face in engineering proteins is just how many possibilities there are. For all practical purposes, it’s infinite. So we have 20 building blocks. There are on average around 300 amino acids in a protein. So that’s 20 to the power of 300 possible combinations. And a common reference point is that it’s estimated there are around 10 to the 80 particles in the observable universe. So beyond astronomical numbers of possible combinations that we could have, and the job of a protein engineer is to find that one or a few of the proteins within that space that do what we want it to do.&lt;/p&gt;



&lt;p&gt;So when a human has an idea of, OK, here’s what I want a protein to do, we have various techniques of finding that desired protein, one of which is using artificial intelligence and trying to either sift through that milieu of potential proteins or, as we’ll talk about more in this podcast, physically generating them. So creating them in a way, sampling them out of some distribution of reasonable proteins.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Great. So I wanted to throw it to James now to talk about how protein design goes from computer to reality—from in silico to test tubes. What role does Twist Bioscience&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; play in transforming digital protein designs into synthesized proteins? And maybe we can talk also about what safeguards are in place at your company and why do we need them.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;So all of these proteins that Bruce has described are encoded in DNA. So the language that our cells use to kind of store the information about how to make these proteins is all encoded in DNA. And so if you as an engineer have designed a protein and you want to test it to see if it does what you think it does, the first step is to have the DNA that encodes that protein manufactured, and companies like Twist carry out that role.&lt;/p&gt;



&lt;p&gt;So we are cognizant also, however, that these are what are called &lt;em&gt;dual-use technologies&lt;/em&gt;. So you can use DNA and proteins for an incredible variety of amazing applications. So drug development, agricultural improvements, bioindustrial manufacturing, all manner of incredible applications. But you could also potentially use those to cause harm so toxins or other, you know, sort of biological misuse.&lt;/p&gt;



&lt;p&gt;And so the industry has since at least 2010 recognized that they have a responsibility to make sure that when we’re asked to make some sequence of DNA that we understand what that thing is encoding and who we’re giving it for. So we’re screening both the customer that’s coming to us and we’re screening the sequence that they’re requesting.&lt;/p&gt;



&lt;p&gt;And so Twist has long invested in a very, sort of, complicated system for essentially reverse engineering the constructs that we’re asked to make so that we understand what they are. And then a system where we engage with our customers and make sure that they’re going to use those for legitimate purpose and responsibly.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And how do the emergence of these new generative AI tools influence how you think about risk?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;A lot of the power of these AI tools is they allow us to make proteins or design proteins that have never existed before in nature to carry out functions that don’t exist in the natural world. That’s an extremely powerful capability.&lt;/p&gt;



&lt;p&gt;But the existing defensive tools that we use at DNA synthesis companies generally rely on what’s called homology, similarity to known naturally occurring sequences, to determine whether something might pose risk. And so AI tools kind of break the link between those two things.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Now you also serve as chair of the International Gene Synthesis Consortium&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Can you tell us a little bit more about the IGSC, its mission, how it supports global biosecurity?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Certainly. So the IGSC was founded in 2010[1] and right now has grown to more than 40 companies and organizations across 10 countries. And the IGSC is essentially a place where companies who might be diehard competitors in the market around nucleic acid synthesis come together and design and develop best practices around biosecurity screening to, kind of, support the shared interest we all have in making sure that these technologies are not subject to misuse.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks, James. Now, Tessa, your organization, IBBIS&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is focused—it’s a beautiful mission—on advancing science while minimizing &lt;em&gt;catastrophic&lt;/em&gt; risk, likelihood of &lt;em&gt;catastrophic&lt;/em&gt; risk. When we say catastrophic risk, what do we really mean, Tessa, in the context of biology and AI? And how is that … do you view that risk landscape as evolving as AI capabilities are growing?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN:&lt;/strong&gt; I think the … to be honest, as a person who’s been in biosecurity for a while, I’ve been surprised by how much of the conversation about the risks from advances in artificial intelligence has centered on the risk of engineered biological weapons and engineered pandemics.&lt;/p&gt;



&lt;p&gt;Even recently, there was a new discussion on introducing redlines for AI that came up at the UN General Assembly. And the very first item they list in their list of risks, if I’m not mistaken, was engineered pandemics, which is exactly the sort of thing that people fear could be done, could be done with these biological AI tools.&lt;/p&gt;



&lt;p&gt;Now, I think that when we talk about catastrophic risk, we talk about, you know, something that has an impact on a large percentage of humanity. And I think the reason that we think that biotechnologies pose a catastrophic risk is that we believe there, as we’ve seen with many historical pandemics, there’s a possibility for something to emerge or be created that is beyond our society’s ability to control.&lt;/p&gt;



&lt;p&gt;You know, there were a few countries in COVID that managed to more or less successfully do a zero-COVID policy, but that was not, that was not most countries. That was not any of the countries that I lived in. And, you know, we saw millions of people die. And I think we believe that with something like the 1918 influenza, which had a much higher case fatality rate, you could have far more people die.&lt;/p&gt;



&lt;p&gt;Now, why we think about this in the context of AI and where this connects to DNA synthesis is that, you know, there is a … these risks of both, sort of, public health risks, pandemic risks, and misuse risks—people deliberately trying to do harm with biology, as we’ve seen from the long history of biological weapons programs—you know, we think that those might be accelerated in a few different ways by AI technology, both the potential … and I say potential here because as everyone who has worked in a wet lab—which I think is everyone on this call—knows, engineering biology is really difficult. So there’s maybe a potential for it to become easier to develop biological technology for the purposes of doing harm, and there’s maybe also the potential to create novel threats.&lt;/p&gt;



&lt;p&gt;And so I think people talk about both of those, and people have been looking hard for possible safeguards. And I think one safeguard that exists in this biosecurity world that, for example, doesn’t exist as cleanly in the cybersecurity world is that none of these biological threats can do harm until they are realized in physical reality, until you actually produce the protein or produce the virus or the microorganism that could do harm. And so I think at this point of production, both in DNA synthesis and elsewhere, we have a chance to introduce safeguards that can have a really large impact on the amount of risk that we’re facing—as long as we develop those safeguards in a way that keeps pace with AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Well, thanks, Tessa. So, Bruce, our project began when I posed a challenge to you of the form: could current open-source AI tools be tasked with rewriting toxic protein sequences in a way that preserves their native structure, and might they evade today’s screening systems?&lt;/p&gt;



&lt;p&gt;And I was preparing for a global workshop on AI and biosecurity that I’d been organizing with Frances Arnold, David Baker, and Lynda Stuart, and I wanted a concrete case study to challenge attendees. And what we found was interesting and deeply concerning.&lt;/p&gt;



&lt;p&gt;So I wanted to dive in with you, Bruce, on the technical side. Can you describe some about the generative pipeline and how it works and what you did to build what we might call an AI and biosecurity red-teaming pipeline for testing and securing biosecurity screening tools?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Sure. Yeah. I think the best place to start with this is really by analogy.&lt;/p&gt;



&lt;p&gt;An analogy I often use in this case is the type of image generation AI tools we’re all familiar with now where I can tell the AI model, “Hey, give me a cartoonish picture of a dog playing fetch.” And it’ll do that, and it’ll give us back something that is likely never been seen before, right. That exact image is new, but the theme is still there. The theme is this dog.&lt;/p&gt;



&lt;p&gt;And that’s kind of the same technology that we’re using in this red-teaming pipeline. Only rather than using plain language, English, we’re passing in what we would call conditioning information that is relevant to a protein.&lt;/p&gt;



&lt;p&gt;So our AI models aren’t at the point yet where I can say, “Give me a protein that does &lt;em&gt;x&lt;/em&gt;.” That would be the dream. We’re a long way from that. But what instead we do is we pass in things that match that theme that we’re interested in. So rather than saying, “Hey, give me back the theme on a dog,” we pass in information that we know will cause or at least push this generative model to create a protein that has the characteristics that we want.&lt;/p&gt;



&lt;p&gt;So in the case of that example you just mentioned, Eric, it would be the protein structure. Like I mentioned earlier, we usually say structure determines function. There’s obviously a lot of nuance to that, but we can, at a first approximation, say structure determines function. So if I ask an AI model, ”Hey, here’s this structure; give me a protein sequence that folds to this structure,” just like with that analogy with the dog, it’s going to give me something that matches that structure but that is likely still never been seen before. It’s going to be a new sequence.&lt;/p&gt;



&lt;p&gt;So you can imagine taking this one step further. In the red-teaming pipeline, what we would do is take a protein that should normally be captured by DNA synthesis screening—that &lt;em&gt;would&lt;/em&gt; be captured by DNA synthesis screening—find its structure, pass it through one of these models, and get variants on the theme of that structure so these new sequences, these synthetic homologs that you mentioned, &lt;em&gt;paraphrased&lt;/em&gt;, &lt;em&gt;reformulated&lt;/em&gt;, whatever phrase we want to use to describe them.&lt;/p&gt;



&lt;p&gt;And they have a chance or a greater chance than &lt;em&gt;not&lt;/em&gt; of maintaining the structure and so maintaining the function while being sufficiently different that they’re not detected by these tools anymore.&lt;/p&gt;



&lt;p&gt;So that’s the nuts and bolts of how the red-teaming pipeline comes together. We use more tools than just structure. I think structure is the easiest one to understand. But we have a suite of tools in there, each pass different conditioning information that causes the model to generate sequences that are paraphrased versions of potential proteins of concern.&lt;s&gt;&lt;/s&gt;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;But to get down to brass tacks, what Bruce did for the framing study was … we took the toxic, well-known toxic protein ricin, as we described in a framing paper that’s actually part of the appendix now to the &lt;em&gt;Science&lt;/em&gt; publication, and we generated through this pipeline, composed of open-source tools, thousands of AI-rewritten versions of ricin.&lt;/p&gt;



&lt;p&gt;And this brings us to the next step of our project, way back when, at the early … in the early days of this effort, where Twist Bioscience was one of the companies we approached with what must have seemed like an unusual question to your CEO, in fact, James: would you be open to testing whether current screening systems could detect thousands of AI-rewritten versions of ricin, a well-known toxic protein?&lt;/p&gt;



&lt;p&gt;And your CEO quickly connected me with you, James. So, James, what were your first thoughts on hearing about this project, and how did you respond to our initial framing study?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;I think my first response was gratitude and excitement. So it was fantastic that Microsoft had really leaned forward on this set of ideas and had produced this dataset. But to have it, you know, show up on our doorstep in a very concrete way with a partner that was ready to, sort of, help us try and address that, I think was a really … a valuable opportunity. And so we really leapt at that.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ:&lt;/strong&gt; And the results were that both for you and another company, major producer IDT [Integrated DNA Technologies], those thousands of variants flew through … flew under the radar of the biosecurity screening software as we covered in that framing paper.&lt;/p&gt;



&lt;p&gt;Now, after our initial findings on this, we quietly shared the paper with a few trusted contacts, including some in government. Through my work with the White House Office of Science and Technology Policy, or OSTP, we connected up with biosecurity leads there, and it was an OSTP biosecurity lead who described our results as the first &lt;em&gt;zero day&lt;/em&gt; in AI and biosecurity. And now in cybersecurity, a zero day is a vulnerability unknown to defenders generally, meaning there’s no time to respond before it could be exploited should it be known.&lt;/p&gt;



&lt;p&gt;In that vein, we took a cybersecurity approach. We stood up a CERT—C-E-R-T—a &lt;em&gt;cybersecurity [computer] emergency response team&lt;/em&gt; approach used in responding to cybersecurity vulnerabilities, and we implemented this process to address what we saw as a vulnerability with AI-enabled challenges to biosecurity.&lt;/p&gt;



&lt;p&gt;At one point down the line, it was so rewarding to hear you say, James, “I’m really glad Microsoft got here first.” I’m curious how you think about this kind of AI-enabled vulnerability compared to other ones, biosecurity threats, you’ve encountered, and I’d love to hear your perspective on how we handled the situation from the early discovery to the coordination and outreach.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Yeah, I think in terms of comparison known threats, the challenge here is really there is no good basis on which we can just, sort of, say, &lt;em&gt;Oh, I’ll build a new tool to detect this concrete universe of things&lt;/em&gt;, right. This was more a pattern of I’m going to use tools—and I love the name “Paraphrase”; it’s a fantastic name—I can paraphrase anything that I would normally think of as biological … as &lt;em&gt;posing&lt;/em&gt; biological risk, and now that thing is harder to detect for existing tools. And so that really was a very eye-opening experience, and I think the practice of forming this CERT response, putting together a group of people who were well versed not just in the threat landscape but also in the defensive technologies, and then figuring out how to mitigate that risk and broaden that study, I think, was a really incredibly valuable response to the entire synthesis industry.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Yeah, and, Bruce, can you describe a little bit about the process by which we expanded the effort beyond our initial framing study to more toxins and then to a larger challenge set and then the results that we pursued and achieved?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Yeah, of course. So, you know, using machine learning lingo, you don’t want to &lt;em&gt;overfit&lt;/em&gt; to a single example. So early on with this, as part of the framing study, we were able to show or I should say James and coworkers across the screening field were able to show that this could be patched, right. We needed to just make some changes to the tools,&amp;nbsp;and we could at the very least detect ricin or reformulated versions of ricin.&lt;/p&gt;



&lt;p&gt;So the next step of course was then, OK, how generalizable are these patches? Can they detect other reformulated sequences, as well? So we had to expand the set of proteins that we had reformulated. We couldn’t just do 10s of thousands of ricins. We had to do 10s of thousands of name your other potentially hazardous …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;I think we had 72, was it?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;It was 72 in the end that we ended up at. I believe, James, it was you and maybe Jake, another one of the authors on the list … on the paper, who primarily put that list together …&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;This is Jacob Beal … Jacob Beal at Raytheon BBN.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;I think James actually might be the better one to answer how this list was expanded.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS:&lt;/strong&gt;&amp;nbsp;Initially the focus [was] on ricin as a toxin so that list expanded to 62 sort of commonly controlled toxins that are subject to an export control restriction or other concern. And then on top of that, we added 10 viral proteins. So we didn’t really just want to look at toxins. We also wanted to look at viral proteins, largely because those proteins tend to have multiple functions. They have highly constrained structures. And so if we could work in a toxin context, could Paraphrase also do the same for viral proteins, as well.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And, Bruce, can you describe some about how we characterize the updates and the, we’ll say, the boost in capabilities of the patched screening tools?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt; So we had, like you said, Eric, 72 base proteins or template proteins. And for each of those, we had generated a few 100 to a couple thousand reformulated variants of them. The only way to really get any sense of validity of those sequences was to predict their structures. So we predicted protein structures for I think it was 70ish thousand protein structures in the end that we had to predict and score them using in silico metrics. So things like, how similar is this to that template, wild-type protein structure that we used as our conditioning information?&lt;/p&gt;



&lt;p&gt;We put them on a big grid. So we have two axes. We have on the x-axis—and this is a figure in our paper—the quality of the prediction. It’s essentially a confidence metric: how realistic is this protein sequence? And on the other axis is, how similar is the predicted structure of this variant to the original? And ultimately, what we were wanting to see was the proteins that scored well in both of those metrics, so that showed up in the top right of that diagram, were caught primarily, because these are again the ones that are &lt;em&gt;most likely&lt;/em&gt;, having to say &lt;em&gt;most likely&lt;/em&gt;, to retain function of the original.&lt;/p&gt;



&lt;p&gt;So when you compare the original tools—Tool Series A, right, the unpatched tools—what you’ll find is varying degrees of success in the top right. It varied by tool. But in some cases, barely anything being flagged as potentially hazardous. And so improvement is then in the next series—Series B, the patched version of tools—we have more flagged in that upper-right corner.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And we felt confident that we had a more AI-resilient screening solution across the companies, and, James, at this point, the whole team decided it was time to disclose the vulnerability as well as the patch details and pointers to where to go for the updated screening software and to communicate this to synthesis companies worldwide via the IGSC. This was probably July, I think, of 2024. What was that process like, and how did members respond?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;I think members were really grateful and excited. To present to that group, to say, hey, this activity (a) has gone on, (b) was successful, and (c) was kept close hold until we knew how to mitigate this, I think everyone was really gratified by that and comforted by the fact that now they had kind of off-the-shelf solutions that they could use to improve their resilience against any incoming heavily engineered protein designs.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks, James.&lt;/p&gt;



&lt;p&gt;Now, I know that we all understand this particular effort to be important but a &lt;em&gt;piece&lt;/em&gt; of the biosecurity and AI problem. I’m just curious to … I’ll ask all three of you to just share some brief reflections.&lt;/p&gt;



&lt;p&gt;I know, Bruce, you’ve been on … you’ve stayed on this, and we’ve—all of us on the original team—have other projects going on that are pushing on the frontiers ahead of where we were with this paper when we published it.&lt;/p&gt;



&lt;p&gt;Let me start with Tessa in terms of, like, what new risks do you see emerging as AI accelerates and maybe couple that with thoughts about how do we proactively get ahead of them.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;Yeah, I think with the Paraphrase’s work, as Bruce explained so well, you know, I sometimes use the metaphor of the previous response that the IGSC had to do, the synthesis screening community, where it used to be you could look for similarities to DNA sequences, and then everyone started doing synthetic biology where they were doing codon optimization so that proteins could express more efficiently in different host organisms, and now all of a sudden, well, you’ve scrambled your DNA sequence and it doesn’t look very similar even though your protein sequence actually still looks, you know, very similar or often the same once it’s been translated from DNA to protein, and so that was a, you know, many, many in the industry were already screening both DNA and protein, but they had to start screening … &lt;em&gt;everybody&lt;/em&gt; had to start screening protein sequences even just to do the similarity testing as these codon optimization tools became universal.&lt;/p&gt;



&lt;p&gt;I feel like we’re, kind of, in a similar transition phase with protein-design, protein-rephrasing, tools where, you know, these tools are still in many cases drawing from the natural distribution of proteins. You know, I think some of the work we saw in, you know, designing novel CRISPR enzymes, you go, OK, yeah, it is novel; it’s very unlike any &lt;em&gt;one&lt;/em&gt; CRISPR enzyme. But if you do a massive multiple sequence alignment of every CRISPR enzyme that we know about, you’re like, OK, this fits in the distribution of those enzymes. And so, you know, I think we’re not … we’re having to do a more flexible form of screening, where we look for things that are kind of within distribution of natural proteins.&lt;/p&gt;



&lt;p&gt;But I feel like broadly, all of the screening tools were able to respond by doing something like that. And I think … I still feel like the clock is ticking down on that and that as the AI tools get better at predicting function and designing, sort of, novel sequences to pursue a particular function, you know—you have tools now that can go from Gene Ontology terms to a potential structure or potential sequence that may again be much farther out of the distribution of natural protein—I think all of us on the screening side are going to have to be responding to that, as well.&lt;/p&gt;



&lt;p&gt;So I think I see this as a necessary ongoing engagement between people at the frontier of designing novel biology and people at the frontier of producing all of the materials that allow that novel biology to be tested in the lab. You know, I think this feels like the first, you know, detailed, comprehensive zero day disclosure and response. But I think that’s … I think we’re going to see more of those. And I think what I’m excited about doing at IBBIS is trying to encourage and set up more infrastructure so that you can, as an AI developer, disclose these new discoveries to the people who need to respond before the publication comes out.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thank you, Tessa.&lt;/p&gt;



&lt;p&gt;The, the … Bruce, I mean, you and I are working on all sorts of dimensions. You’re leading up some efforts at Microsoft, for example, on the foundation model front and so on, among other directions. We’ve talked about new kinds of embedding models that might go beyond sequence and structure. Can you talk a little bit about just a few of the directions that just paint the larger constellation of the kinds of things that we talk about when we put our worry hats on?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;I feel like that could have its own dedicated podcast, as well. There’s a lot … [LAUGHTER] there’s a lot to talk about.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Yeah. We want to make sure that we don’t tell the world that the whole problem is solved here.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Right, right, right. I think Tessa said it really, really well in that most of what we’re doing right now, it’s a variant on a known theme. I have to know the structure that does something bad to be able to pass it in as context. I have to know some existing sequence that does something bad to pass it in.&lt;/p&gt;



&lt;p&gt;And obviously the goal is to move away from that in benign applications, where when I’m designing something, I often want to design it because nothing exists [LAUGHS] that already does it. So we are going to be heading to this space where we don’t know what this protein does. It’s kind of a circular problem, right, where we’re going to need to be able to predict what some obscure protein sequence does in order to be able to still do our screening.&lt;/p&gt;



&lt;p&gt;Now, the way that I think about this, I often think about it beyond just DNA synthesis screening. It’s one line of defense, and there needs to be many lines of defense that come into play here that go beyond just relying on this one roadblock. It’s a very powerful roadblock. It’s a very powerful barrier. But we need to be proactively thinking about how we broaden the scope of defenses. And there are lots of conversations that are ongoing. I won’t go into the details of them. Again, that would be its own podcast.&lt;/p&gt;



&lt;p&gt;But primarily my big push—and I think this is emerging consensus in the field, though I don’t want to speak for everybody—is it needs to … any interventions we have need to come more at the systems level and less at the model level, primarily because this is such dual-use technology. If it can be used for good biological design, it can be used for bad biological design. Biology has no sense of morality. There is no bad protein. It’s just &lt;em&gt;a&lt;/em&gt; protein.&lt;/p&gt;



&lt;p&gt;So we need to think about this differently than how we would maybe think about looking at the outputs of that image generator model that I spoke about earlier, where I can physically look at an image and say, don’t want my model producing that, do want my model producing that. I don’t have that luxury in this space. So it’s a totally different problem. It’s an evolving problem. Conversations are happening about it, but the work is very much not done.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;And, James, I want to give you the same open question, but I’d like to apply what Bruce just said on system level and so on and in the spirit of the kind of things that you’re very much involved with internationally to also add to it, just get some comments on programs and policies that move beyond technical solutions for governance mechanisms—logging, auditing nucleic acid orders, transparency, various kinds—that might complement technical approaches like Paraphrase and their status today.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Yeah, I’m very gratified that Bruce said that we, the synthesis industry, should not be the sole bulwark against misuse. That is very comforting and correct.&lt;/p&gt;



&lt;p&gt;Yeah, so the US government published a guidance document in 2023 that essentially said you, the entire biotech supply chain, have a responsibility to make sure that you’re evaluating your customers. You should know your customer; you know that they’re legitimate. I think that’s an important practice.&lt;/p&gt;



&lt;p&gt;Export controls are designed to minimize the movement of equipment and materials that can be used in support of these kinds of misuse activities. And then governments have really been quite active in trying to incentivize, you know, sort of what we would think of as positive behavior, so screening, for example, in DNA synthesis companies. The US government created a framework in 2024, and it’s under a rewrite now to basically say US research dollars will only go to companies who make nucleic acid who do these good things. And so that is using, kind of, the government-funding carrot to, kind of, continue to build these layers of defense against potential misuse.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks. Now, discussing risk, especially when it involves AI and biosecurity, isn’t always easy. As we’ve all been suggesting, some worry about alarming the public or arming bad actors. Others advocate for openness as a principle of doing science with integrity.&lt;/p&gt;



&lt;p&gt;A phase of our work as we prepared our paper was giving serious thought to both the benefits and the risks of transparency about what it was that we were doing. Some experts encouraged full disclosure as important for enhancing the science of biosecurity. Other experts, &lt;em&gt;all experts&lt;/em&gt;, cautioned against what are called &lt;em&gt;information hazards&lt;/em&gt;, the risk of sharing the details to enable malevolent actions with our findings or our approach.&lt;/p&gt;



&lt;p&gt;So we faced a real question: how can we support open science while minimizing the risk of misuse? And we took all the input we got, even if it was contradictory, very seriously. We carefully deliberated about a good balance, and even &lt;em&gt;then&lt;/em&gt;, once we chose our balance and submitted our manuscript to &lt;em&gt;Science&lt;/em&gt;, the peer reviewers came back and said they wanted some of the more sensitive details that we withheld with explanations as to why.&lt;/p&gt;



&lt;p&gt;So this provoked some thinking out of the box about a novel approach, and we came up with a perpetual gatekeeping strategy where requests for access to sensitive methods and data and even the software across different risk categories would be carefully reviewed by a committee and a process for access that would continue in perpetuity.&lt;/p&gt;



&lt;p&gt;Now, we brought the proposal to Tessa and her team at IBBIS—this is a great nonprofit group; look at their mission—and we worked with Tessa and her colleagues to refine a workable solution that was accepted by &lt;em&gt;Science&lt;/em&gt; magazine as a new approach to handling information hazards as first demonstrated by our paper.&lt;/p&gt;



&lt;p&gt;So, Tessa, thank you again for helping us to navigate such a complex challenge. Can you share your perspective on information hazards? And then walk us through how our proposed system ensures responsible data and software sharing.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;Yeah. And thanks, Eric.&lt;/p&gt;



&lt;p&gt;It’s all of the long discussions we had among the group of people on this podcast and the other authors on the paper and many people we engaged, you know, technical experts, people in various governments, you know, we heard a lot of contradictory advice.&lt;/p&gt;



&lt;p&gt;And I think it showed us that there isn’t a consensus right now on how to handle information hazards in biotechnology. You know, I think … I don’t want to overstate how much of a consensus there is in cybersecurity either. If you go to DEF CON, you’ll hear people about how they’ve been mistreated in their attempts to do responsible disclosure for pacemakers and whatnot. But I think we’re … we have even less of a consensus when it comes to handling biological information.&lt;/p&gt;



&lt;p&gt;You know, you have some people who say, oh, because the size of the consequences could be so catastrophic if someone, you know, releases an engineered flu or something, you know, we should just never share information about this. And then you have other people who say there’s no possibility of building defenses unless we share information about this. And we heard very strong voices with both of those perspectives in the process of conducting this study.&lt;/p&gt;



&lt;p&gt;And I think what we landed on that I’m really excited about and really excited to get feedback on now that the paper is out, you know, if you go and compare our preprint, which came out in December of 2024, and this paper in October 2025, you’ll see a lot of information got added back in.&lt;/p&gt;



&lt;p&gt;And I’m excited to see people’s reaction to that because even back in January 2025, talking with people who were signatories to the responsible biodesign commitments, they were really excited that this was such an empirically concrete paper because they’d maybe read a number of papers talking about biosecurity risks from AI that didn’t include a whole lot of data, you know, often, I think, because of concerns about information hazards. And they found the arguments in this paper are much more convincing because we are able to share data.&lt;/p&gt;



&lt;p&gt;So the process we underwent that I felt good about was trying to really clearly articulate, when we talk about an information hazard, what are we worried about being done with this data? And if we put this data in public, completely open source, does it shift the risk at all? You know, I think doing that kind of marginal contribution comparison is really important because it also let us make more things available publicly.&lt;/p&gt;



&lt;p&gt;But there were a few tiers of data that after a lot of discussion amongst the authors of the paper, we thought, OK, potentially someone who wanted to do harm, if they got access to this data, it might make it easier for them. Again, not necessarily saying it, you know, it opens the floodgates, but it might make it easier for them. And when we thought about that, we thought, OK, you know, giving all of those paraphrased protein sequences, maybe, maybe that, you know, compared to having to set up the whole pipeline with the open-source tools yourself, just giving you those protein sequences, maybe that makes your life a bit easier if you’re trying to do harm.&lt;/p&gt;



&lt;p&gt;And then we thought, OK, giving you those protein sequences plus whether or not they were successfully flagged, maybe that makes your life, you know, quite a bit easier. And then finally, we thought, OK, the code that we want to share with some people who might try to reproduce these results or might try to build new screening systems that are more robust, we want to share the code with them. But again, if you have that whole code pipeline just prepared for you, it might really help make your life easier if you’re trying to do harm.&lt;/p&gt;



&lt;p&gt;And so we, sort of, sorted the data into these three tiers and then went through a process actually very inspired by the existing customer screening processes in nucleic acid synthesis about how to determine, you know, we tried to take an approach not of what gets you in but what gets you out. You know, for the most part, we think it should be possible to access this data.&lt;/p&gt;



&lt;p&gt;You know, if you have an affiliation with a recognizable institution or some good explanation of why you don’t have one right now, you know, if you have a reason for accessing this data, it shouldn’t be too hard to meet those requirements, but we wanted to have some in place. And we wanted it to be possible to rule out some people from getting access to this data. And so we’ve tried to be extremely transparent about what those are. If you go through our data access process and for some reason you get rejected, you’ll get a list of, “Here’s the reasons we rejected you. If you don’t think that’s right, get back to us.”&lt;/p&gt;



&lt;p&gt;So I’m really excited to pilot this in part because I think, you know, we’re already in conversations with some other people handling potential bio-AI information hazard about doing a similar process for their data of, you know, tiering it, determining which gates to put in which tiers, but I really hope a number of people do get access through the process or if they try and they fail, they tell us why. Because I think as we move toward this world of potentially, you know, biology that is much easier to engineer, partly due to dual-use tools, you know, my dream is it’s, like, still hard to engineer harm with biology, even if it’s really easy to engineer biology. And I think these, kind of, new processes for managing access to things, this sort of like, you know, open but not completely public, I think those can be a big part of that layered defense.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Thanks, Tessa. So we’re getting close to closing, and I just thought I would ask each of you to just share some reflections on what we’ve learned, the process we’ve demonstrated, the tools, the policy work that we did, this idea of facing the dual-use dilemma with … even at the information hazard level, with sharing information versus withholding it. What do you think about how our whole end to end of the study, now reaching the two-year point, can help other fields facing dual-use dilemmas?&lt;/p&gt;



&lt;p&gt;Tessa, Bruce, James … James, have you ever thought about that? And we’ll go to Bruce and then Tessa.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS:&lt;/strong&gt;&amp;nbsp;Yeah, I think it was an excellent model. I would like to see a study like this repeated on a schedule, you know, every six months because from where I sit, you know, the tools that we used for this project are now two years old. And so capabilities have moved on. Is the picture the same in terms of defensive capability? And so using that model over time, I think, would be incredibly valuable. And then using the findings to chart, you know, how much should we be investing in alternative strategies for this kind of risk mitigation for AI tool … the &lt;em&gt;products&lt;/em&gt; of AI tools?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Bruce.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN:&lt;/strong&gt;&amp;nbsp;Yeah, I think I would extend on what James said. The anecdote I like to point out about this project is, kind of, our schedule. We found the vulnerability and it was patched within a week, two weeks, on all major synthesis screening platforms. We wrote the paper within a month. We expanded on the paper within two months, and then we spent a year and a half to nearly two years [LAUGHS] trying to figure out what goes into the paper; how do we release this information; you know, how do we do this responsibly?&lt;/p&gt;



&lt;p&gt;And my hope is similar to what James said. We’ve made it easier for others to do this type of work. Not this exact work; it doesn’t have to necessarily do with proteins. But to do this type of work where you are dealing with potential hazards but there is also value in sharing and that hopefully that year and a half we spent figuring out how to appropriately share and what to share will not be a year and a half for other teams because these systems are in place or at least there is an example to follow up from. So that’s my takeaway.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ:&lt;/strong&gt; Tessa, bring us home—&lt;em&gt;bring us home!&lt;/em&gt; [LAUGHS]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;&lt;em&gt;Bring us home!&lt;/em&gt; Let’s do it faster next time. [LAUGHTER] Come talk to any of us if you’re dealing with this kind of stuff. You know, I think IBBIS, especially, we want to be a partner for building those layers of defense and, you know, having ripped out our hair as a collective over the past year and a half about the right process to follow here, I think we all really hope it’ll be faster next time.&lt;/p&gt;



&lt;p&gt;And I think, you know, the other thing I would encourage is if you’re an AI developer, I would encourage you to think about how your tool can strengthen screening and strengthen recognition of threats.&lt;/p&gt;



&lt;p&gt;I know James and I have talked before about how, you know, our Google search alerts each week send us dozens of cool AI bio papers, and it’s more like once a year or maybe once every six months, if we’re lucky, that we get something that’s like applying AI bio to biosecurity. So, you know, if you’re interested in these threats, I think we’d love to see more work that’s directly applied to facing these threats using the most modern technology.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;HORVITZ: &lt;/strong&gt;Well said.&lt;/p&gt;



&lt;p&gt;Well, Bruce, James, Tessa, thank you so much for joining me today and for representing the many collaborators, both coauthors and beyond, who made this project possible.&lt;/p&gt;



&lt;p&gt;It’s been a true pleasure to work with you. I’m so excited about what we’ve accomplished, the processes and the models that we’re now sharing with the world. And I’m deeply grateful for the collective intelligence and dedication that really powered the effort from the very beginning. So thanks again.&lt;/p&gt;



&lt;p&gt;[MUSIC]&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;WITTMANN: &lt;/strong&gt;Thanks, Eric.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;DIGGANS: &lt;/strong&gt;Thank you.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;ALEXANIAN: &lt;/strong&gt;Thank you.&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;[1] The original organization was founded in 2009 and became the International Gene Synthesis Consortium in 2010.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/</guid><pubDate>Mon, 06 Oct 2025 14:04:34 +0000</pubDate></item><item><title>AMD wins massive AI chip deal from OpenAI with stock sweetener (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/amd-wins-massive-ai-chip-deal-from-openai-with-stock-sweetener/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT maker will be allowed to buy 10% of AMD for a penny per share.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-300x169.jpg" width="300" /&gt;
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, AMD announced it will supply AI chips to OpenAI in a multi-year deal worth tens of billions of dollars annually that gives the ChatGPT creator an option to acquire up to 10 percent of the chipmaker's stock for 1 cent per share, Reuters reports. The agreement covers hundreds of thousands of AMD's AI graphics processing units over several years starting in the second half of 2026.&lt;/p&gt;
&lt;p&gt;The deal marks a major endorsement of AMD's AI hardware and software capabilities as the company competes with Nvidia for dominance in the AI chip market. AMD executives project the agreement will generate more than $100 billion in new revenue over four years from OpenAI and other customers who follow OpenAI's lead.&lt;/p&gt;
&lt;p&gt;"We view this deal as certainly transformative, not just for AMD, but for the dynamics of the industry," AMD Executive Vice President Forrest Norrod told Reuters on Sunday. The chipmaker will start booking income from the deal next year when OpenAI starts building a 1 gigawatt facility based on AMD's forthcoming MI450 series chips.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;As part of the arrangement, AMD will allow OpenAI to purchase up to 160 million AMD shares at 1 cent each throughout the chips deal.&lt;/p&gt;
&lt;h2&gt;OpenAI diversifies its chip supply&lt;/h2&gt;
&lt;p&gt;With demand for AI compute growing rapidly, companies like OpenAI have been looking for secondary supply lines and sources of additional computing capacity, and the AMD partnership is part the company's wider effort to secure sufficient computing power for its AI operations. In September, Nvidia announced an investment of up to $100 billion in OpenAI that included supplying at least 10 gigawatts of Nvidia systems. OpenAI plans to deploy a gigawatt of Nvidia's next-generation Vera Rubin chips in late 2026.&lt;/p&gt;
&lt;p&gt;OpenAI has worked with AMD for years, according to Reuters, providing input on the design of older generations of AI chips such as the MI300X. The new agreement calls for deploying the equivalent of 6 gigawatts of computing power using AMD chips over multiple years.&lt;/p&gt;
&lt;p&gt;Beyond working with chip suppliers, OpenAI is widely reported to be developing its own silicon for AI applications and has partnered with Broadcom, as we reported in February. A person familiar with the matter told Reuters the AMD deal does not change OpenAI's ongoing compute plans, including its chip development effort or its partnership with Microsoft.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        ChatGPT maker will be allowed to buy 10% of AMD for a penny per share.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-300x169.jpg" width="300" /&gt;
                  &lt;img alt="A glowing OpenAI logo on a light blue background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2023/11/openai_iceblue_hero-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI / Benj Edwards

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, AMD announced it will supply AI chips to OpenAI in a multi-year deal worth tens of billions of dollars annually that gives the ChatGPT creator an option to acquire up to 10 percent of the chipmaker's stock for 1 cent per share, Reuters reports. The agreement covers hundreds of thousands of AMD's AI graphics processing units over several years starting in the second half of 2026.&lt;/p&gt;
&lt;p&gt;The deal marks a major endorsement of AMD's AI hardware and software capabilities as the company competes with Nvidia for dominance in the AI chip market. AMD executives project the agreement will generate more than $100 billion in new revenue over four years from OpenAI and other customers who follow OpenAI's lead.&lt;/p&gt;
&lt;p&gt;"We view this deal as certainly transformative, not just for AMD, but for the dynamics of the industry," AMD Executive Vice President Forrest Norrod told Reuters on Sunday. The chipmaker will start booking income from the deal next year when OpenAI starts building a 1 gigawatt facility based on AMD's forthcoming MI450 series chips.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;As part of the arrangement, AMD will allow OpenAI to purchase up to 160 million AMD shares at 1 cent each throughout the chips deal.&lt;/p&gt;
&lt;h2&gt;OpenAI diversifies its chip supply&lt;/h2&gt;
&lt;p&gt;With demand for AI compute growing rapidly, companies like OpenAI have been looking for secondary supply lines and sources of additional computing capacity, and the AMD partnership is part the company's wider effort to secure sufficient computing power for its AI operations. In September, Nvidia announced an investment of up to $100 billion in OpenAI that included supplying at least 10 gigawatts of Nvidia systems. OpenAI plans to deploy a gigawatt of Nvidia's next-generation Vera Rubin chips in late 2026.&lt;/p&gt;
&lt;p&gt;OpenAI has worked with AMD for years, according to Reuters, providing input on the design of older generations of AI chips such as the MI300X. The new agreement calls for deploying the equivalent of 6 gigawatts of computing power using AMD chips over multiple years.&lt;/p&gt;
&lt;p&gt;Beyond working with chip suppliers, OpenAI is widely reported to be developing its own silicon for AI applications and has partnered with Broadcom, as we reported in February. A person familiar with the matter told Reuters the AMD deal does not change OpenAI's ongoing compute plans, including its chip development effort or its partnership with Microsoft.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/amd-wins-massive-ai-chip-deal-from-openai-with-stock-sweetener/</guid><pubDate>Mon, 06 Oct 2025 14:45:28 +0000</pubDate></item><item><title>A 19-year-old nabs backing from Google execs for his AI memory startup, Supermemory (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/a-19-year-old-nabs-backing-from-google-execs-for-his-ai-memory-startup-supermemory/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Context windows of AI models, which indicate the ability of a model to “remember” information, have increased over time. However, researchers have suggested new ways to increase long-term memory of AI models, as they often can’t hold context over several sessions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nineteen-year-old founder Dhravya Shah is attempting to solve problems in this area by building a memory solution, called Supermemory, for AI apps.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Originally from Mumbai in India, Shah started building consumer-facing bots and apps a few years ago. He even sold his bot that formatted tweets into good-looking screenshots to the social media tool Hypefury.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The founder, who was preparing for an entrance exam to get into IIT (Indian Institute of Technology), earned good money from this sale and decided to move to the U.S. to attend Arizona State University instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After relocating, he challenged himself to build something new each week for 40 weeks. During one of those weeks, he built Supermemory (which was initially called Any Context) and put it on GitHub. At that time, the tool allowed you to chat with your Twitter bookmarks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The current version of the tool extracts “memories” or insights from unstructured data and helps the applications understand the context better.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3054464" height="395" src="https://techcrunch.com/wp-content/uploads/2025/10/Memory-map-Supermemory.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A memory map extracted by Supermemory&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Supermemory&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Shah secured an internship at Cloudflare in 2024, where he worked on AI and infrastructure. He later worked as a developer relations lead at the company. During this time, advisors, including Cloudflare CTO Dane Knecht, asked him to turn Supermemory into a product. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, he decided to build Supermemory full-time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supermemory, now described as a universal memory API for AI apps, builds a knowledge graph based on the data it processes, and personalizes the context for the users. For instance, it can support querying across month-old entries for a writing or a journaling app, or searching for an email app. As the solution allows for multimodal inputs, it could also allow a video editor to fetch relevant assets from a library for a particular prompt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup can ingest any type of data, the company says, including files, docs, chats, projects, emails, PDFs, and app data streams. Its chatbot and notetaker feature lets users add memories in text, add files or links, and connect to apps like Google Drive, OneDrive, or Notion. There’s also a Chrome extension that lets you easily add notes from a website.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Our core strength is to extract insights from any kind of unstructured data and give the apps more context about users. As we work across multimodal data, our solution is suitable for all kinds of AI apps ranging from email clients to video editors,” Shah said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supermemory has secured seed funding of $2.6 million led by Susa Ventures, Browder Capital, and SF1.vc. The round also includes individual investors like Cloudflare’s Knecht, Google AI chief Jeff Dean, DeepMind product manager Logan Kilpatrick, Sentry founder David Cramer, and executives from OpenAI, Meta, and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shah said at one point Y Combinator also approached him to join one of its batches, but he already had investors on board, so the timing didn’t work out.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3054468" height="410" src="https://techcrunch.com/wp-content/uploads/2025/10/50a44eb1-cf53-4256-a05f-e6d3b6919644-spark-clipboard.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Supermemory&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Joshua Browder, founder and CEO of “Robot lawyer” startup DoNotPay, who runs Browder Capital as a solo GP, was impressed by Shah’s tenacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I connected with Dhravya over X, and what struck me was how quickly he moves and builds things, and that prompted me to invest in him,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has multiple existing customers, including a16z-backed desktop assistant Cluely, AI video editor Montra, AI search Scira, Composio’s multi-MCP tool Rube, and real estate startup Rets. Plus, it’s working with a robotics company to retain visual memories captured by a robot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this is a slant toward consumers, the app feels more like a playground for developers to understand more about the tool and potentially use it in their workflows or their own apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supermemory has substantial competitors in the memory space. Startups like Felicis Ventures-backed Letta and Mem0 (where Shah worked for a short while) are building a memory layer for agents. Supermemory’s own backer, Susa Ventures, has invested in Memories.ai along with Samsung, which can tap into thousands of hours of footage to get insights. Shah says that while these startups might serve different industries and use cases, Supermemory will stand out because it offers lower latency.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“More and more AI companies will need a memory layer. Supermemory’s solution provides high performance while allowing you to surface relevant context quickly,” Browder said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Context windows of AI models, which indicate the ability of a model to “remember” information, have increased over time. However, researchers have suggested new ways to increase long-term memory of AI models, as they often can’t hold context over several sessions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nineteen-year-old founder Dhravya Shah is attempting to solve problems in this area by building a memory solution, called Supermemory, for AI apps.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Originally from Mumbai in India, Shah started building consumer-facing bots and apps a few years ago. He even sold his bot that formatted tweets into good-looking screenshots to the social media tool Hypefury.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The founder, who was preparing for an entrance exam to get into IIT (Indian Institute of Technology), earned good money from this sale and decided to move to the U.S. to attend Arizona State University instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After relocating, he challenged himself to build something new each week for 40 weeks. During one of those weeks, he built Supermemory (which was initially called Any Context) and put it on GitHub. At that time, the tool allowed you to chat with your Twitter bookmarks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The current version of the tool extracts “memories” or insights from unstructured data and helps the applications understand the context better.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3054464" height="395" src="https://techcrunch.com/wp-content/uploads/2025/10/Memory-map-Supermemory.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A memory map extracted by Supermemory&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Supermemory&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Shah secured an internship at Cloudflare in 2024, where he worked on AI and infrastructure. He later worked as a developer relations lead at the company. During this time, advisors, including Cloudflare CTO Dane Knecht, asked him to turn Supermemory into a product. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, he decided to build Supermemory full-time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supermemory, now described as a universal memory API for AI apps, builds a knowledge graph based on the data it processes, and personalizes the context for the users. For instance, it can support querying across month-old entries for a writing or a journaling app, or searching for an email app. As the solution allows for multimodal inputs, it could also allow a video editor to fetch relevant assets from a library for a particular prompt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup can ingest any type of data, the company says, including files, docs, chats, projects, emails, PDFs, and app data streams. Its chatbot and notetaker feature lets users add memories in text, add files or links, and connect to apps like Google Drive, OneDrive, or Notion. There’s also a Chrome extension that lets you easily add notes from a website.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Our core strength is to extract insights from any kind of unstructured data and give the apps more context about users. As we work across multimodal data, our solution is suitable for all kinds of AI apps ranging from email clients to video editors,” Shah said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supermemory has secured seed funding of $2.6 million led by Susa Ventures, Browder Capital, and SF1.vc. The round also includes individual investors like Cloudflare’s Knecht, Google AI chief Jeff Dean, DeepMind product manager Logan Kilpatrick, Sentry founder David Cramer, and executives from OpenAI, Meta, and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shah said at one point Y Combinator also approached him to join one of its batches, but he already had investors on board, so the timing didn’t work out.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3054468" height="410" src="https://techcrunch.com/wp-content/uploads/2025/10/50a44eb1-cf53-4256-a05f-e6d3b6919644-spark-clipboard.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Supermemory&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Joshua Browder, founder and CEO of “Robot lawyer” startup DoNotPay, who runs Browder Capital as a solo GP, was impressed by Shah’s tenacity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I connected with Dhravya over X, and what struck me was how quickly he moves and builds things, and that prompted me to invest in him,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has multiple existing customers, including a16z-backed desktop assistant Cluely, AI video editor Montra, AI search Scira, Composio’s multi-MCP tool Rube, and real estate startup Rets. Plus, it’s working with a robotics company to retain visual memories captured by a robot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this is a slant toward consumers, the app feels more like a playground for developers to understand more about the tool and potentially use it in their workflows or their own apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Supermemory has substantial competitors in the memory space. Startups like Felicis Ventures-backed Letta and Mem0 (where Shah worked for a short while) are building a memory layer for agents. Supermemory’s own backer, Susa Ventures, has invested in Memories.ai along with Samsung, which can tap into thousands of hours of footage to get insights. Shah says that while these startups might serve different industries and use cases, Supermemory will stand out because it offers lower latency.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“More and more AI companies will need a memory layer. Supermemory’s solution provides high performance while allowing you to surface relevant context quickly,” Browder said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/a-19-year-old-nabs-backing-from-google-execs-for-his-ai-memory-startup-supermemory/</guid><pubDate>Mon, 06 Oct 2025 15:45:13 +0000</pubDate></item><item><title>What to expect at OpenAI’s DevDay 2025, and how to watch it (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/what-to-expect-at-openais-devday-2025-and-how-to-watch-it/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778705142.jpg?resize=1200,758" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is gearing up to host its third annual developer conference, DevDay 2025, on Monday. The company says more than 1,500 people are scheduled to convene at Fort Mason in San Francisco for OpenAI’s “biggest event yet,” which features announcements, keynotes from OpenAI executives, and a fireside chat between CEO Sam Altman and longtime Apple designer Jony Ive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From the sound of it, DevDay 2025 is shaping up to be a grand display of OpenAI’s rising dominance in Silicon Valley against giants like Apple, Google, and Meta. OpenAI is currently building an AI device, a social media app, and an AI-powered browser to take on Chrome. In other words, OpenAI has a lot more going on than it did during its first DevDay in 2023, when it mostly just had ChatGPT and an API business for developers to access its models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the same time, OpenAI faces more competition than ever in the bid to win over developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last year, Anthropic’s and Google’s AI models have become increasingly capable for coding tasks and web design. OpenAI has been forced to release better AI models at lower prices to remain in the race. In the background, Meta has built up an impressive roster of AI talent in its new group, Meta Superintelligence Labs, which could become another threat to OpenAI in the near future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI unveiled at its first DevDay in 2023 a new AI model, GPT-4 Turbo, and Altman shared his vision for a marketplace of AI agents called the GPT Store. Altman was ousted as CEO days later — only to return after a dramatic weekend of negotiations. In 2024, OpenAI responded with a more subdued conference, announcing some meaningful developer upgrades, such as an API for AI voice applications, but not much else.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nothing is confirmed to launch at DevDay 2025, stoking plenty of rumors. Perhaps OpenAI will finally unveil the AI-powered browser it’s been working on, or maybe give an update on the AI device it’s building with Ive and former Apple executives. It’s also possible there could be some updates related to the GPT Store, or a new offering that lets developers build agentic workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch will be on the ground in San Francisco covering the event live, so you can check back here for all the news. Here’s what you need to know about OpenAI’s DevDay, and how to watch it.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;DevDay 2025 kicks off at 10 a.m. PT October 6 with an opening keynote from Altman, in which he’s scheduled to unveil “announcements, live demos, and a vision of how developers are reshaping the future with AI.” The keynote will last roughly one hour and will be livestreamed on OpenAI’s YouTube page.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s the only event that will be livestreamed for remote attendees.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For in-person attendees, there will be onstage presentations and talks from Cursor CEO Michael Truell, San Francisco mayor Daniel Lurie, and Andreessen Horowitz investing partner Kimberly Tan, among others. Several OpenAI employees will also give speeches about their work, including model behavior researcher Laurentia Romaniuk and Codex lead Alexander Embiricos.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s also supposed to be a series of AI-powered sideshows at DevDay 2025. One of them is “Sora Cinema,” which is described as a “cozy mini-theater with popcorn” featuring short films generated by OpenAI’s video model, Sora. There’s also supposed to be a phone booth with a “living portrait” of the famed computer scientist Alan Turing “that speaks back.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later in the afternoon, there will be two big events to cap off DevDay. These last two events won’t be livestreamed, but they will be posted on YouTube later that day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At 3:15 p.m. PT, there will be a “Developer State of the Union” with OpenAI president Greg Brockman and Olivier Godement, who heads up product for the OpenAI Platform. The two OpenAI executives are slated to “demo new capabilities” and share what’s ahead for developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, at 4:15 p.m. PT, Altman and Ive will give a “Closing Fireside Chat” to discuss the “craft of building in the age of AI.” That conversation will last about 45 minutes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Update 10/6/25: This article was originally published on 10/3/25. It was updated to reflect changes to DevDay’s guest list, as well as new rumors that surfaced over the weekend.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/GettyImages-1778705142.jpg?resize=1200,758" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is gearing up to host its third annual developer conference, DevDay 2025, on Monday. The company says more than 1,500 people are scheduled to convene at Fort Mason in San Francisco for OpenAI’s “biggest event yet,” which features announcements, keynotes from OpenAI executives, and a fireside chat between CEO Sam Altman and longtime Apple designer Jony Ive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From the sound of it, DevDay 2025 is shaping up to be a grand display of OpenAI’s rising dominance in Silicon Valley against giants like Apple, Google, and Meta. OpenAI is currently building an AI device, a social media app, and an AI-powered browser to take on Chrome. In other words, OpenAI has a lot more going on than it did during its first DevDay in 2023, when it mostly just had ChatGPT and an API business for developers to access its models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the same time, OpenAI faces more competition than ever in the bid to win over developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last year, Anthropic’s and Google’s AI models have become increasingly capable for coding tasks and web design. OpenAI has been forced to release better AI models at lower prices to remain in the race. In the background, Meta has built up an impressive roster of AI talent in its new group, Meta Superintelligence Labs, which could become another threat to OpenAI in the near future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI unveiled at its first DevDay in 2023 a new AI model, GPT-4 Turbo, and Altman shared his vision for a marketplace of AI agents called the GPT Store. Altman was ousted as CEO days later — only to return after a dramatic weekend of negotiations. In 2024, OpenAI responded with a more subdued conference, announcing some meaningful developer upgrades, such as an API for AI voice applications, but not much else.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nothing is confirmed to launch at DevDay 2025, stoking plenty of rumors. Perhaps OpenAI will finally unveil the AI-powered browser it’s been working on, or maybe give an update on the AI device it’s building with Ive and former Apple executives. It’s also possible there could be some updates related to the GPT Store, or a new offering that lets developers build agentic workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch will be on the ground in San Francisco covering the event live, so you can check back here for all the news. Here’s what you need to know about OpenAI’s DevDay, and how to watch it.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;DevDay 2025 kicks off at 10 a.m. PT October 6 with an opening keynote from Altman, in which he’s scheduled to unveil “announcements, live demos, and a vision of how developers are reshaping the future with AI.” The keynote will last roughly one hour and will be livestreamed on OpenAI’s YouTube page.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s the only event that will be livestreamed for remote attendees.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For in-person attendees, there will be onstage presentations and talks from Cursor CEO Michael Truell, San Francisco mayor Daniel Lurie, and Andreessen Horowitz investing partner Kimberly Tan, among others. Several OpenAI employees will also give speeches about their work, including model behavior researcher Laurentia Romaniuk and Codex lead Alexander Embiricos.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There’s also supposed to be a series of AI-powered sideshows at DevDay 2025. One of them is “Sora Cinema,” which is described as a “cozy mini-theater with popcorn” featuring short films generated by OpenAI’s video model, Sora. There’s also supposed to be a phone booth with a “living portrait” of the famed computer scientist Alan Turing “that speaks back.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later in the afternoon, there will be two big events to cap off DevDay. These last two events won’t be livestreamed, but they will be posted on YouTube later that day.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At 3:15 p.m. PT, there will be a “Developer State of the Union” with OpenAI president Greg Brockman and Olivier Godement, who heads up product for the OpenAI Platform. The two OpenAI executives are slated to “demo new capabilities” and share what’s ahead for developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, at 4:15 p.m. PT, Altman and Ive will give a “Closing Fireside Chat” to discuss the “craft of building in the age of AI.” That conversation will last about 45 minutes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Update 10/6/25: This article was originally published on 10/3/25. It was updated to reflect changes to DevDay’s guest list, as well as new rumors that surfaced over the weekend.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/what-to-expect-at-openais-devday-2025-and-how-to-watch-it/</guid><pubDate>Mon, 06 Oct 2025 15:53:22 +0000</pubDate></item><item><title>Deloitte will refund Australian government for AI hallucination-filled report (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/deloitte-will-refund-australian-government-for-ai-hallucination-filled-report/</link><description>&lt;article class="double-column h-entry post-2120888 post type-post status-publish format-standard has-post-thumbnail hentry category-ai tag-ai tag-australia tag-chatgpt tag-deloitte tag-government tag-openai"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Consulting firm quietly admitted to GPT-4o use after fake citations were found in August.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="553" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-758288217-640x553.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-758288217-1152x648-1759767307.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An actual photo of GTP-4o coming up with plausible-sounding sources for Deloitte's report.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Australian Financial Review reports that Deloitte Australia will offer the Australian government a partial refund for a report that was littered with AI-hallucinated quotes and references to nonexistent research.&lt;/p&gt;
&lt;p&gt;Deloitte's "Targeted Compliance Framework Assurance Review" was finalized in July and published by Australia's Department of Employment and Workplace Relations (DEWR) in August (Internet Archive version of the original). The report, which cost Australian taxpayers nearly $440,000 AUD (about $290,000 USD), focuses on the technical framework the government uses to automate penalties under the country's welfare system.&lt;/p&gt;
&lt;p&gt;Shortly after the report was published, though, Sydney University Deputy Director of Health Law Chris Rudge noticed citations to multiple papers and publications that did not exist. That included multiple references to nonexistent reports by Lisa Burton Crawford, a real professor at the University of Sydney law school.&lt;/p&gt;
&lt;p&gt;"It is concerning to see research attributed to me in this way," Crawford told the AFR in August. "I would like to see an explanation from Deloitte as to how the citations were generated."&lt;/p&gt;
&lt;h2&gt;“A small number of corrections”&lt;/h2&gt;
&lt;p&gt;Deloitte and the DEWR buried that explanation in an updated version of the original report published Friday "to address a small number of corrections to references and footnotes," according to the DEWR website. On page 58 of that 273-page updated report, Deloitte added a reference to "a generative AI large language model (Azure OpenAI GPT-4o) based tool chain" that was used as part of the technical workstream to help "[assess] whether system code state can be mapped to business requirements and compliance needs."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Of the 141 sources cited in an extensive "Reference List" in the original report, only 127 appear in the updated report. In addition to the now-deleted references to fake publications from Crawford and other academics, the updated report also removed a fabricated quote attributed to an actual ruling from federal justice Jennifer Davies (spelled as "Davis" in the original report).&lt;/p&gt;
&lt;p&gt;Deloitte Australia said it will repay the final installment of its contract with the government, though it's unclear which portion of the total contract that represents. A spokesperson for DEWR told the AFR that "the substance of the independent review is retained, and there are no changes to the recommendations."&lt;/p&gt;
&lt;p&gt;But Sydney University's Rudge told AFR that "you cannot trust the recommendations when the very foundation of the report is built on a flawed, originally undisclosed, and non-expert methodology... Deloitte has admitted to using generative AI for a core analytical task; but it failed to disclose this in the first place."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #f4ff81; background-color: #c0ca33;"&gt;&lt;span class="ars-avatar-letter"&gt;D&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Dputiger
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            Earlier this year, Deloitte declared it would start using generative AI for its reports as a way of enhancing the value provided to its clients. I don't remember if they said it in a specific report or not, but I recall seeing it. &lt;p&gt;The citation issue continues to trip people up across the spectrum, from lawyers to business analysts. It's striking how many supposedly smart people do not understand the limits of the tools they insist will deliver such amazing value.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-10-06T16:36:58+00:00"&gt;October 6, 2025 at 4:36 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2120888 post type-post status-publish format-standard has-post-thumbnail hentry category-ai tag-ai tag-australia tag-chatgpt tag-deloitte tag-government tag-openai"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Consulting firm quietly admitted to GPT-4o use after fake citations were found in August.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="553" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-758288217-640x553.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-758288217-1152x648-1759767307.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      An actual photo of GTP-4o coming up with plausible-sounding sources for Deloitte's report.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The Australian Financial Review reports that Deloitte Australia will offer the Australian government a partial refund for a report that was littered with AI-hallucinated quotes and references to nonexistent research.&lt;/p&gt;
&lt;p&gt;Deloitte's "Targeted Compliance Framework Assurance Review" was finalized in July and published by Australia's Department of Employment and Workplace Relations (DEWR) in August (Internet Archive version of the original). The report, which cost Australian taxpayers nearly $440,000 AUD (about $290,000 USD), focuses on the technical framework the government uses to automate penalties under the country's welfare system.&lt;/p&gt;
&lt;p&gt;Shortly after the report was published, though, Sydney University Deputy Director of Health Law Chris Rudge noticed citations to multiple papers and publications that did not exist. That included multiple references to nonexistent reports by Lisa Burton Crawford, a real professor at the University of Sydney law school.&lt;/p&gt;
&lt;p&gt;"It is concerning to see research attributed to me in this way," Crawford told the AFR in August. "I would like to see an explanation from Deloitte as to how the citations were generated."&lt;/p&gt;
&lt;h2&gt;“A small number of corrections”&lt;/h2&gt;
&lt;p&gt;Deloitte and the DEWR buried that explanation in an updated version of the original report published Friday "to address a small number of corrections to references and footnotes," according to the DEWR website. On page 58 of that 273-page updated report, Deloitte added a reference to "a generative AI large language model (Azure OpenAI GPT-4o) based tool chain" that was used as part of the technical workstream to help "[assess] whether system code state can be mapped to business requirements and compliance needs."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Of the 141 sources cited in an extensive "Reference List" in the original report, only 127 appear in the updated report. In addition to the now-deleted references to fake publications from Crawford and other academics, the updated report also removed a fabricated quote attributed to an actual ruling from federal justice Jennifer Davies (spelled as "Davis" in the original report).&lt;/p&gt;
&lt;p&gt;Deloitte Australia said it will repay the final installment of its contract with the government, though it's unclear which portion of the total contract that represents. A spokesperson for DEWR told the AFR that "the substance of the independent review is retained, and there are no changes to the recommendations."&lt;/p&gt;
&lt;p&gt;But Sydney University's Rudge told AFR that "you cannot trust the recommendations when the very foundation of the report is built on a flawed, originally undisclosed, and non-expert methodology... Deloitte has admitted to using generative AI for a core analytical task; but it failed to disclose this in the first place."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #f4ff81; background-color: #c0ca33;"&gt;&lt;span class="ars-avatar-letter"&gt;D&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Dputiger
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            Earlier this year, Deloitte declared it would start using generative AI for its reports as a way of enhancing the value provided to its clients. I don't remember if they said it in a specific report or not, but I recall seeing it. &lt;p&gt;The citation issue continues to trip people up across the spectrum, from lawyers to business analysts. It's striking how many supposedly smart people do not understand the limits of the tools they insist will deliver such amazing value.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-10-06T16:36:58+00:00"&gt;October 6, 2025 at 4:36 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/deloitte-will-refund-australian-government-for-ai-hallucination-filled-report/</guid><pubDate>Mon, 06 Oct 2025 16:30:29 +0000</pubDate></item><item><title>Meta Llama: Everything you need to know about the open generative AI model (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/meta-llama-everything-you-need-to-know-about-the-open-generative-ai-model/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Like every Big Tech company these days, Meta has its own flagship generative AI model, called Llama. Llama is&amp;nbsp;somewhat unique&amp;nbsp;among major models in that&amp;nbsp;it’s&amp;nbsp;“open,” meaning developers can download and use it however they please (with certain limitations).&amp;nbsp;That’s&amp;nbsp;in contrast to models like&amp;nbsp;Anthropic’s&amp;nbsp;Claude,&amp;nbsp;Google’s Gemini,&amp;nbsp;xAI’s&amp;nbsp;Grok, and most of OpenAI’s ChatGPT models,&amp;nbsp;which can only be accessed via APIs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the interest of giving developers choice, however, Meta has also partnered with vendors, including AWS, Google Cloud,&amp;nbsp;and Microsoft Azure, to make cloud-hosted versions of Llama available. In addition, the company&amp;nbsp;publishes tools, libraries, and recipes in its Llama cookbook to help developers fine-tune, evaluate, and adapt the models to their domain. With newer generations like&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Llama 3&amp;nbsp;and Llama 4, these capabilities have expanded to include native multimodal support and broader cloud rollouts.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Here’s&amp;nbsp;everything you need to know about&amp;nbsp;Meta’s&amp;nbsp;Llama, from its capabilities and editions to where you can use it.&amp;nbsp;We’ll&amp;nbsp;keep this post updated as Meta releases upgrades and introduces new dev tools to support the model’s use.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-llama"&gt;What is Llama? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Llama is a family of models — not just one. The latest version is&amp;nbsp;Llama 4;&amp;nbsp;it was&amp;nbsp;released in April 2025&amp;nbsp;and&amp;nbsp;includes three models:&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Scout:&lt;/strong&gt;&amp;nbsp;17 billion active parameters, 109 billion total parameters, and a context window of 10 million tokens.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Maverick:&amp;nbsp;&lt;/strong&gt;17 billion active parameters, 400 billion total parameters, and a context window of 1 million tokens.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Behemoth&lt;/strong&gt;:&amp;nbsp;Not yet released&amp;nbsp;but&amp;nbsp;will&amp;nbsp;have 288 billion active&amp;nbsp;parameters&amp;nbsp;and 2 trillion total parameters.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;(In data science, tokens are subdivided bits of raw data, like the syllables “fan,” “tas,” and “tic” in the word “fantastic.”)&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A model’s context, or context window, refers to input data (e.g., text) that the model considers before generating output (e.g.,&amp;nbsp;additional&amp;nbsp;text). Long context can prevent models from “forgetting” the content of recent docs and data, and from veering off topic and extrapolating wrongly. However, longer context windows can also&amp;nbsp;result in the model “forgetting” certain safety guardrails&amp;nbsp;and being more prone to produce content that is in line with the conversation, which has led&amp;nbsp;some users toward&amp;nbsp;delusional thinking.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For reference,&amp;nbsp;the&amp;nbsp;10 million context window&amp;nbsp;that Llama 4 Scout promises&amp;nbsp;roughly equals&amp;nbsp;the text of about 80 average novels.&amp;nbsp;Llama 4&amp;nbsp;Maverick’s&amp;nbsp;1 million context window equals about eight novels.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;All&amp;nbsp;of&amp;nbsp;the Llama 4 models&amp;nbsp;were trained on “large amounts of unlabeled text, image, and video data” to give them “broad visual understanding,”&amp;nbsp;as well as on 200 languages,&amp;nbsp;according to Meta.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 Scout and Maverick are Meta’s first open-weight natively multimodal models.&amp;nbsp;They’re&amp;nbsp;built using a “mixture-of-experts” (MoE) architecture, which reduces computational load&amp;nbsp;and improves efficiency in training and inference. Scout, for example, has 16 experts, and Maverick has 128 experts.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 Behemoth includes 16 experts, and Meta is referring to it as a teacher for the smaller models.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Llama 4 builds on the Llama 3 series, which included 3.1 and 3.2 models widely used for instruction-tuned applications and cloud deployment.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-can-llama-do"&gt;What can Llama do? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Like other generative AI models, Llama can perform a range of different assistive tasks, like coding and answering basic math questions, as well as summarizing documents in&amp;nbsp;at least 12&amp;nbsp;languages (Arabic,&amp;nbsp;English, German, French,&amp;nbsp;Hindi, Indonesian,&amp;nbsp;Italian, Portuguese, Hindi,&amp;nbsp;Spanish, Tagalog, Thai, and Vietnamese). Most text-based workloads — think analyzing&amp;nbsp;large&amp;nbsp;files like PDFs and spreadsheets — are within its purview, and all Llama 4&amp;nbsp;models&amp;nbsp;support text, image, and video input.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 Scout is designed for longer workflows and massive data analysis. Maverick is a generalist model that is better at balancing reasoning power and response speed and is suitable for coding, chatbots, and technical assistants. And Behemoth is designed for advanced research, model distillation, and STEM tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama models, including Llama 3.1, can be configured to&amp;nbsp;leverage&amp;nbsp;third-party applications, tools, and APIs to perform tasks. They are trained to use Brave Search for answering questions about recent events;&amp;nbsp;the Wolfram Alpha API for math- and science-related queries;&amp;nbsp;and a Python interpreter for validating code. However, these tools require&amp;nbsp;proper configuration and are not automatically enabled out of the box.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-where-can-i-use-llama"&gt;Where can I use Llama?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If&amp;nbsp;you’re&amp;nbsp;looking to simply chat with Llama,&amp;nbsp;it’s&amp;nbsp;powering the Meta AI chatbot experience&amp;nbsp;on Facebook Messenger, WhatsApp, Instagram, Oculus,&amp;nbsp;and Meta.ai&amp;nbsp;in 40 countries. Fine-tuned versions of Llama are used in Meta AI experiences in over 200 countries and territories.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 models Scout and Maverick are available on Llama.com and Meta’s partners, including the AI developer platform Hugging Face. Behemoth is still in training.&amp;nbsp;Developers&amp;nbsp;building with&amp;nbsp;Llama can download,&amp;nbsp;use,&amp;nbsp;or fine-tune the model across most of the popular cloud platforms.&amp;nbsp;Meta claims it has&amp;nbsp;more than&amp;nbsp;25 partners hosting Llama, including Nvidia, Databricks,&amp;nbsp;Groq, Dell,&amp;nbsp;and Snowflake.&amp;nbsp;And while “selling access” to Meta’s openly available models&amp;nbsp;isn’t&amp;nbsp;Meta’s business model, the company makes some money&amp;nbsp;through&amp;nbsp;revenue-sharing agreements&amp;nbsp;with model hosts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of these partners have built&amp;nbsp;additional&amp;nbsp;tools and services on top of Llama, including tools that let the models reference proprietary data and enable them to run at lower latencies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Importantly, the Llama license&amp;nbsp;constrains how developers can deploy the model: App developers with more than 700 million monthly users must request a special license from Meta that the company will grant on its discretion.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In May 2025, Meta launched a&amp;nbsp;new program&amp;nbsp;to incentivize startups to adopt its Llama models. Llama for Startups gives companies support from Meta’s Llama team and access to potential funding.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Alongside Llama, Meta provides tools intended to make the model “safer” to use:&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Llama Guard&lt;/strong&gt;, a moderation framework.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;CyberSecEval&lt;/strong&gt;, a cybersecurity risk-assessment suite.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Llama Firewall&lt;/strong&gt;, a security guardrail designed to enable building secure AI systems.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Code Shield&lt;/strong&gt;, which provides support for inference-time filtering of insecure code produced by LLMs.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Llama&amp;nbsp;Guard tries to detect potentially problematic content either fed into — or generated — by a Llama model, including content relating to criminal activity, child exploitation, copyright violations, hate, self-harm, and sexual abuse.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said,&amp;nbsp;it’s&amp;nbsp;clearly not a silver bullet since&amp;nbsp;Meta’s own previous guidelines&amp;nbsp;allowed the chatbot to engage in sensual and romantic chats with minors, and some reports show those&amp;nbsp;turned&amp;nbsp;into&amp;nbsp;sexual conversations.&amp;nbsp;Developers can&amp;nbsp;customize&amp;nbsp;the categories of blocked content and apply the blocks to all the languages Llama supports.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like Llama Guard, Prompt Guard can block text intended for Llama, but only text meant to “attack” the model and get it to behave in undesirable ways. Meta claims that&amp;nbsp;Llama&amp;nbsp;Guard can defend against explicitly malicious prompts (i.e., jailbreaks that&amp;nbsp;attempt&amp;nbsp;to get around Llama’s built-in safety filters) in addition to prompts that&amp;nbsp;contain&amp;nbsp;“injected inputs.”&amp;nbsp;The Llama Firewall works to detect and prevent risks like prompt injection, insecure code, and risky tool interactions. And Code Shield helps mitigate insecure code suggestions and offers secure command execution for seven programming languages.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for&amp;nbsp;CyberSecEval,&amp;nbsp;it’s&amp;nbsp;less a tool than a collection of benchmarks to measure model security.&amp;nbsp;CyberSecEval&amp;nbsp;can assess the risk a Llama model poses (at least according to Meta’s criteria) to app developers and end users in areas like “automated social engineering” and “scaling offensive cyber operations.”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-llama-s-limitations"&gt;Llama’s limitations&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3054573" height="328" src="https://techcrunch.com/wp-content/uploads/2024/09/Llama-4-coding-benchmark.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Artificial Analysis&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Llama comes with certain risks and limitations, like all generative AI models.&amp;nbsp;For example, while its most recent model has multimodal features, those are&amp;nbsp;mainly limited&amp;nbsp;to&amp;nbsp;the English&amp;nbsp;language&amp;nbsp;for now.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zooming out,&amp;nbsp;Meta used a dataset of pirated e-books&amp;nbsp;and articles to train its Llama models. A federal judge recently sided with Meta in a copyright lawsuit brought against the company by 13 book authors, ruling that&amp;nbsp;the use of copyrighted works for training fell under “fair use.” However, if Llama&amp;nbsp;regurgitates&amp;nbsp;a copyrighted snippet&amp;nbsp;and someone uses it in a product, they could potentially be infringing on copyright and be liable.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta also&amp;nbsp;controversially trains its AI on Instagram and Facebook posts,&amp;nbsp;photos,&amp;nbsp;and captions, and&amp;nbsp;makes it difficult for users to opt out.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Programming is another area where&amp;nbsp;it’s&amp;nbsp;wise to tread lightly when using Llama.&amp;nbsp;That’s&amp;nbsp;because Llama might —&amp;nbsp;perhaps&amp;nbsp;more&amp;nbsp;so than&amp;nbsp;its generative AI counterparts —&amp;nbsp;produce buggy or insecure code.&amp;nbsp;On&amp;nbsp;LiveCodeBench, a&amp;nbsp;benchmark&amp;nbsp;that tests AI models on competitive coding problems, Meta’s Llama 4 Maverick model achieved a score of 40%.&amp;nbsp;That’s&amp;nbsp;compared to&amp;nbsp;85% for OpenAI’s GPT-5 high&amp;nbsp;and&amp;nbsp;83% for&amp;nbsp;xAI’s&amp;nbsp;Grok 4 Fast.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As always,&amp;nbsp;it’s&amp;nbsp;best to have a human expert review any AI-generated code before incorporating it into a service or software.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, as with other AI models, Llama models are still guilty of generating&amp;nbsp;plausible-sounding&amp;nbsp;but false or misleading information, whether&amp;nbsp;that’s&amp;nbsp;in coding, legal guidance, or&amp;nbsp;emotional conversations with AI personas.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This was originally published on September 8, 2024, and is updated regularly with new information.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Like every Big Tech company these days, Meta has its own flagship generative AI model, called Llama. Llama is&amp;nbsp;somewhat unique&amp;nbsp;among major models in that&amp;nbsp;it’s&amp;nbsp;“open,” meaning developers can download and use it however they please (with certain limitations).&amp;nbsp;That’s&amp;nbsp;in contrast to models like&amp;nbsp;Anthropic’s&amp;nbsp;Claude,&amp;nbsp;Google’s Gemini,&amp;nbsp;xAI’s&amp;nbsp;Grok, and most of OpenAI’s ChatGPT models,&amp;nbsp;which can only be accessed via APIs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the interest of giving developers choice, however, Meta has also partnered with vendors, including AWS, Google Cloud,&amp;nbsp;and Microsoft Azure, to make cloud-hosted versions of Llama available. In addition, the company&amp;nbsp;publishes tools, libraries, and recipes in its Llama cookbook to help developers fine-tune, evaluate, and adapt the models to their domain. With newer generations like&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Llama 3&amp;nbsp;and Llama 4, these capabilities have expanded to include native multimodal support and broader cloud rollouts.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Here’s&amp;nbsp;everything you need to know about&amp;nbsp;Meta’s&amp;nbsp;Llama, from its capabilities and editions to where you can use it.&amp;nbsp;We’ll&amp;nbsp;keep this post updated as Meta releases upgrades and introduces new dev tools to support the model’s use.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-llama"&gt;What is Llama? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Llama is a family of models — not just one. The latest version is&amp;nbsp;Llama 4;&amp;nbsp;it was&amp;nbsp;released in April 2025&amp;nbsp;and&amp;nbsp;includes three models:&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Scout:&lt;/strong&gt;&amp;nbsp;17 billion active parameters, 109 billion total parameters, and a context window of 10 million tokens.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Maverick:&amp;nbsp;&lt;/strong&gt;17 billion active parameters, 400 billion total parameters, and a context window of 1 million tokens.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Behemoth&lt;/strong&gt;:&amp;nbsp;Not yet released&amp;nbsp;but&amp;nbsp;will&amp;nbsp;have 288 billion active&amp;nbsp;parameters&amp;nbsp;and 2 trillion total parameters.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;(In data science, tokens are subdivided bits of raw data, like the syllables “fan,” “tas,” and “tic” in the word “fantastic.”)&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A model’s context, or context window, refers to input data (e.g., text) that the model considers before generating output (e.g.,&amp;nbsp;additional&amp;nbsp;text). Long context can prevent models from “forgetting” the content of recent docs and data, and from veering off topic and extrapolating wrongly. However, longer context windows can also&amp;nbsp;result in the model “forgetting” certain safety guardrails&amp;nbsp;and being more prone to produce content that is in line with the conversation, which has led&amp;nbsp;some users toward&amp;nbsp;delusional thinking.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For reference,&amp;nbsp;the&amp;nbsp;10 million context window&amp;nbsp;that Llama 4 Scout promises&amp;nbsp;roughly equals&amp;nbsp;the text of about 80 average novels.&amp;nbsp;Llama 4&amp;nbsp;Maverick’s&amp;nbsp;1 million context window equals about eight novels.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;All&amp;nbsp;of&amp;nbsp;the Llama 4 models&amp;nbsp;were trained on “large amounts of unlabeled text, image, and video data” to give them “broad visual understanding,”&amp;nbsp;as well as on 200 languages,&amp;nbsp;according to Meta.&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 Scout and Maverick are Meta’s first open-weight natively multimodal models.&amp;nbsp;They’re&amp;nbsp;built using a “mixture-of-experts” (MoE) architecture, which reduces computational load&amp;nbsp;and improves efficiency in training and inference. Scout, for example, has 16 experts, and Maverick has 128 experts.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 Behemoth includes 16 experts, and Meta is referring to it as a teacher for the smaller models.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Llama 4 builds on the Llama 3 series, which included 3.1 and 3.2 models widely used for instruction-tuned applications and cloud deployment.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-can-llama-do"&gt;What can Llama do? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Like other generative AI models, Llama can perform a range of different assistive tasks, like coding and answering basic math questions, as well as summarizing documents in&amp;nbsp;at least 12&amp;nbsp;languages (Arabic,&amp;nbsp;English, German, French,&amp;nbsp;Hindi, Indonesian,&amp;nbsp;Italian, Portuguese, Hindi,&amp;nbsp;Spanish, Tagalog, Thai, and Vietnamese). Most text-based workloads — think analyzing&amp;nbsp;large&amp;nbsp;files like PDFs and spreadsheets — are within its purview, and all Llama 4&amp;nbsp;models&amp;nbsp;support text, image, and video input.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 Scout is designed for longer workflows and massive data analysis. Maverick is a generalist model that is better at balancing reasoning power and response speed and is suitable for coding, chatbots, and technical assistants. And Behemoth is designed for advanced research, model distillation, and STEM tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama models, including Llama 3.1, can be configured to&amp;nbsp;leverage&amp;nbsp;third-party applications, tools, and APIs to perform tasks. They are trained to use Brave Search for answering questions about recent events;&amp;nbsp;the Wolfram Alpha API for math- and science-related queries;&amp;nbsp;and a Python interpreter for validating code. However, these tools require&amp;nbsp;proper configuration and are not automatically enabled out of the box.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-where-can-i-use-llama"&gt;Where can I use Llama?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;If&amp;nbsp;you’re&amp;nbsp;looking to simply chat with Llama,&amp;nbsp;it’s&amp;nbsp;powering the Meta AI chatbot experience&amp;nbsp;on Facebook Messenger, WhatsApp, Instagram, Oculus,&amp;nbsp;and Meta.ai&amp;nbsp;in 40 countries. Fine-tuned versions of Llama are used in Meta AI experiences in over 200 countries and territories.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Llama 4 models Scout and Maverick are available on Llama.com and Meta’s partners, including the AI developer platform Hugging Face. Behemoth is still in training.&amp;nbsp;Developers&amp;nbsp;building with&amp;nbsp;Llama can download,&amp;nbsp;use,&amp;nbsp;or fine-tune the model across most of the popular cloud platforms.&amp;nbsp;Meta claims it has&amp;nbsp;more than&amp;nbsp;25 partners hosting Llama, including Nvidia, Databricks,&amp;nbsp;Groq, Dell,&amp;nbsp;and Snowflake.&amp;nbsp;And while “selling access” to Meta’s openly available models&amp;nbsp;isn’t&amp;nbsp;Meta’s business model, the company makes some money&amp;nbsp;through&amp;nbsp;revenue-sharing agreements&amp;nbsp;with model hosts.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of these partners have built&amp;nbsp;additional&amp;nbsp;tools and services on top of Llama, including tools that let the models reference proprietary data and enable them to run at lower latencies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Importantly, the Llama license&amp;nbsp;constrains how developers can deploy the model: App developers with more than 700 million monthly users must request a special license from Meta that the company will grant on its discretion.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In May 2025, Meta launched a&amp;nbsp;new program&amp;nbsp;to incentivize startups to adopt its Llama models. Llama for Startups gives companies support from Meta’s Llama team and access to potential funding.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Alongside Llama, Meta provides tools intended to make the model “safer” to use:&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Llama Guard&lt;/strong&gt;, a moderation framework.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;CyberSecEval&lt;/strong&gt;, a cybersecurity risk-assessment suite.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Llama Firewall&lt;/strong&gt;, a security guardrail designed to enable building secure AI systems.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Code Shield&lt;/strong&gt;, which provides support for inference-time filtering of insecure code produced by LLMs.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Llama&amp;nbsp;Guard tries to detect potentially problematic content either fed into — or generated — by a Llama model, including content relating to criminal activity, child exploitation, copyright violations, hate, self-harm, and sexual abuse.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said,&amp;nbsp;it’s&amp;nbsp;clearly not a silver bullet since&amp;nbsp;Meta’s own previous guidelines&amp;nbsp;allowed the chatbot to engage in sensual and romantic chats with minors, and some reports show those&amp;nbsp;turned&amp;nbsp;into&amp;nbsp;sexual conversations.&amp;nbsp;Developers can&amp;nbsp;customize&amp;nbsp;the categories of blocked content and apply the blocks to all the languages Llama supports.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like Llama Guard, Prompt Guard can block text intended for Llama, but only text meant to “attack” the model and get it to behave in undesirable ways. Meta claims that&amp;nbsp;Llama&amp;nbsp;Guard can defend against explicitly malicious prompts (i.e., jailbreaks that&amp;nbsp;attempt&amp;nbsp;to get around Llama’s built-in safety filters) in addition to prompts that&amp;nbsp;contain&amp;nbsp;“injected inputs.”&amp;nbsp;The Llama Firewall works to detect and prevent risks like prompt injection, insecure code, and risky tool interactions. And Code Shield helps mitigate insecure code suggestions and offers secure command execution for seven programming languages.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As for&amp;nbsp;CyberSecEval,&amp;nbsp;it’s&amp;nbsp;less a tool than a collection of benchmarks to measure model security.&amp;nbsp;CyberSecEval&amp;nbsp;can assess the risk a Llama model poses (at least according to Meta’s criteria) to app developers and end users in areas like “automated social engineering” and “scaling offensive cyber operations.”&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-llama-s-limitations"&gt;Llama’s limitations&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3054573" height="328" src="https://techcrunch.com/wp-content/uploads/2024/09/Llama-4-coding-benchmark.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Artificial Analysis&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Llama comes with certain risks and limitations, like all generative AI models.&amp;nbsp;For example, while its most recent model has multimodal features, those are&amp;nbsp;mainly limited&amp;nbsp;to&amp;nbsp;the English&amp;nbsp;language&amp;nbsp;for now.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zooming out,&amp;nbsp;Meta used a dataset of pirated e-books&amp;nbsp;and articles to train its Llama models. A federal judge recently sided with Meta in a copyright lawsuit brought against the company by 13 book authors, ruling that&amp;nbsp;the use of copyrighted works for training fell under “fair use.” However, if Llama&amp;nbsp;regurgitates&amp;nbsp;a copyrighted snippet&amp;nbsp;and someone uses it in a product, they could potentially be infringing on copyright and be liable.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta also&amp;nbsp;controversially trains its AI on Instagram and Facebook posts,&amp;nbsp;photos,&amp;nbsp;and captions, and&amp;nbsp;makes it difficult for users to opt out.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Programming is another area where&amp;nbsp;it’s&amp;nbsp;wise to tread lightly when using Llama.&amp;nbsp;That’s&amp;nbsp;because Llama might —&amp;nbsp;perhaps&amp;nbsp;more&amp;nbsp;so than&amp;nbsp;its generative AI counterparts —&amp;nbsp;produce buggy or insecure code.&amp;nbsp;On&amp;nbsp;LiveCodeBench, a&amp;nbsp;benchmark&amp;nbsp;that tests AI models on competitive coding problems, Meta’s Llama 4 Maverick model achieved a score of 40%.&amp;nbsp;That’s&amp;nbsp;compared to&amp;nbsp;85% for OpenAI’s GPT-5 high&amp;nbsp;and&amp;nbsp;83% for&amp;nbsp;xAI’s&amp;nbsp;Grok 4 Fast.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As always,&amp;nbsp;it’s&amp;nbsp;best to have a human expert review any AI-generated code before incorporating it into a service or software.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, as with other AI models, Llama models are still guilty of generating&amp;nbsp;plausible-sounding&amp;nbsp;but false or misleading information, whether&amp;nbsp;that’s&amp;nbsp;in coding, legal guidance, or&amp;nbsp;emotional conversations with AI personas.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This was originally published on September 8, 2024, and is updated regularly with new information.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/meta-llama-everything-you-need-to-know-about-the-open-generative-ai-model/</guid><pubDate>Mon, 06 Oct 2025 17:11:50 +0000</pubDate></item><item><title>Sam Altman says ChatGPT has hit 800M weekly active users (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/sam-altman-says-chatgpt-has-hit-800m-weekly-active-users/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-06-at-1.13.28PM.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI CEO Sam Altman said Monday that ChatGPT has reached 800 million weekly active users, marking an increase of adoption among consumers, developers, enterprises, and governments.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT’s impressive growth comes as OpenAI is on a race to&amp;nbsp;secure as many AI chips&amp;nbsp;and build as much&amp;nbsp;AI infrastructure&amp;nbsp;as possible. In August, OpenAI said it was on the cusp of reaching&amp;nbsp;700 million weekly active users, already an increase from 500 million weekly active users at the end of March.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Today, 4 million developers have built with&amp;nbsp;OpenAI,” Altman said. “More than 800 million people use&amp;nbsp;ChatGPT every week, and we process over 6 billion tokens per minute on the API.&amp;nbsp;Thanks to all of you,&amp;nbsp;AI has gone from something people play with to something people build with every day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman made the announcement during the keynote presentation for OpenAI’s Dev Day, which also included announcements for new tools for building apps inside of ChatGPT as well as constructing more complex agentic systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This will enable a new generation of apps that are interactive, adaptive, and personalized, that you can chat with,” Altman said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in November 2022, ChatGPT saw unprecedented user growth almost immediately, becoming both the preeminent consumer AI product and one of the fastest-growing online services ever. More recently the tool expanded into proactive services with OpenAI Pulse, sending customized morning briefs to participating users. The service has also been dogged by concerns about sycophancy and AI-induced delusions, particularly in the recent case of Allan Brooks, who was misled into believing he’d made a novel mathematical discovery with ChatGPT’s help.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still legally designated as a nonprofit, OpenAI became the most valuable privately held company in the world on Thursday, after a private stock sale of $6.6 billion in shares valued the company at $500 billion. The company has also been launching new products at a blistering pace, releasing a new version of its video-generation tool Sora last week, paired with an accompanying social media network. That same week, the company partnered with Stripe to launch a platform for agentic commerce.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-06-at-1.13.28PM.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI CEO Sam Altman said Monday that ChatGPT has reached 800 million weekly active users, marking an increase of adoption among consumers, developers, enterprises, and governments.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT’s impressive growth comes as OpenAI is on a race to&amp;nbsp;secure as many AI chips&amp;nbsp;and build as much&amp;nbsp;AI infrastructure&amp;nbsp;as possible. In August, OpenAI said it was on the cusp of reaching&amp;nbsp;700 million weekly active users, already an increase from 500 million weekly active users at the end of March.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Today, 4 million developers have built with&amp;nbsp;OpenAI,” Altman said. “More than 800 million people use&amp;nbsp;ChatGPT every week, and we process over 6 billion tokens per minute on the API.&amp;nbsp;Thanks to all of you,&amp;nbsp;AI has gone from something people play with to something people build with every day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman made the announcement during the keynote presentation for OpenAI’s Dev Day, which also included announcements for new tools for building apps inside of ChatGPT as well as constructing more complex agentic systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This will enable a new generation of apps that are interactive, adaptive, and personalized, that you can chat with,” Altman said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in November 2022, ChatGPT saw unprecedented user growth almost immediately, becoming both the preeminent consumer AI product and one of the fastest-growing online services ever. More recently the tool expanded into proactive services with OpenAI Pulse, sending customized morning briefs to participating users. The service has also been dogged by concerns about sycophancy and AI-induced delusions, particularly in the recent case of Allan Brooks, who was misled into believing he’d made a novel mathematical discovery with ChatGPT’s help.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still legally designated as a nonprofit, OpenAI became the most valuable privately held company in the world on Thursday, after a private stock sale of $6.6 billion in shares valued the company at $500 billion. The company has also been launching new products at a blistering pace, releasing a new version of its video-generation tool Sora last week, paired with an accompanying social media network. That same week, the company partnered with Stripe to launch a platform for agentic commerce.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/sam-altman-says-chatgpt-has-hit-800m-weekly-active-users/</guid><pubDate>Mon, 06 Oct 2025 17:30:27 +0000</pubDate></item><item><title>OpenAI launches apps inside of ChatGPT (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/openai-launches-apps-inside-of-chatgpt/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is launching a new way for developers to build applications inside of ChatGPT. Starting Monday, users in ChatGPT will be able to access interactive applications from companies like Booking.com, Expedia, Spotify, Figma, Coursera, Zillow, and Canva. OpenAI is also launching a preview of the Apps SDK, the developer-facing toolkit to build these apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI made the announcement at its annual developer conference, DevDay 2025. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We want ChatGPT to be a great way for people to make progress, to be more productive, more inventive, to learn faster, to do whatever they’re trying to do in their lives better,” said CEO Sam Altman. “[Apps inside of ChatGPT] will enable a new generation of apps that are interactive, adaptive, and personalized, that you can chat with.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new system is OpenAI’s latest attempt to build an ecosystem of apps around its flagship AI product, ChatGPT. The launch follows OpenAI’s previous attempts to let developers build interactive applications, such as through its GPT Store. Unlike that product, which was a separate app store, Monday’s launch puts apps directly in ChatGPT’s responses and lets users call up third-party tools in their everyday conversations. This gives developers better distribution for the apps they build and aims to make a richer experience for users in ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By typing the names of different apps in ChatGPT, users can draw in content from a variety of services. For example, users can say, “Figma, turn this sketch into a workable diagram” to call up the Figma app. Users can also call up the Coursera app by asking, “Coursera, can you teach me something about machine learning?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo of Zillow’s application, users could prompt ChatGPT in natural language to search for apartments in their area within a specific price range. ChatGPT then pulled up an interactive map showing options, and users could talk with ChatGPT to learn more about each one.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3054625" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/02-The-Zillow-App-in-ChatGPT-Product-Mockup-2-RENTING.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Zillow&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT will also surface relevant apps when they could be helpful to a user. If someone asks for a playlist for a party this weekend, ChatGPT may call up the Spotify app in the conversation. In the future, OpenAI says apps like DoorDash, Instacart, Uber, and AllTrails will be available in ChatGPT as well. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says the new system is built using the Model Context Protocol (MCP), which allows developers to connect their data sources to an AI system. ChatGPT apps can also trigger actions and render a fully interactive UI in the chatbot’s responses. Certain apps are able to display videos in ChatGPT, which will be pinned to the top of the web page and can be altered based on user requests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If users are already subscribed to a product, they’ll be able to log in to their account directly in ChatGPT to access certain features. Altman also says OpenAI will support ways to monetize apps inside of ChatGPT in the future, including through the company’s recently launched Instant Checkout feature in ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Key questions around apps in ChatGPT will be privacy, and how much data third-party developers will have access to. OpenAI says developers must “collect only the minimum data they need, and be transparent about permissions.” However, it’s unclear whether developers would have access to a user’s entire conversation with ChatGPT, the past few messages, or just the prompt that summoned up the app.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s also unclear how ChatGPT will choose a service between competing companies, such as DoorDash and Instacart. One could imagine how companies could pay to be surfaced in ChatGPT responses, but OpenAI says it plans to prioritize the user experience above all else.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is launching a new way for developers to build applications inside of ChatGPT. Starting Monday, users in ChatGPT will be able to access interactive applications from companies like Booking.com, Expedia, Spotify, Figma, Coursera, Zillow, and Canva. OpenAI is also launching a preview of the Apps SDK, the developer-facing toolkit to build these apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI made the announcement at its annual developer conference, DevDay 2025. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We want ChatGPT to be a great way for people to make progress, to be more productive, more inventive, to learn faster, to do whatever they’re trying to do in their lives better,” said CEO Sam Altman. “[Apps inside of ChatGPT] will enable a new generation of apps that are interactive, adaptive, and personalized, that you can chat with.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new system is OpenAI’s latest attempt to build an ecosystem of apps around its flagship AI product, ChatGPT. The launch follows OpenAI’s previous attempts to let developers build interactive applications, such as through its GPT Store. Unlike that product, which was a separate app store, Monday’s launch puts apps directly in ChatGPT’s responses and lets users call up third-party tools in their everyday conversations. This gives developers better distribution for the apps they build and aims to make a richer experience for users in ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By typing the names of different apps in ChatGPT, users can draw in content from a variety of services. For example, users can say, “Figma, turn this sketch into a workable diagram” to call up the Figma app. Users can also call up the Coursera app by asking, “Coursera, can you teach me something about machine learning?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo of Zillow’s application, users could prompt ChatGPT in natural language to search for apartments in their area within a specific price range. ChatGPT then pulled up an interactive map showing options, and users could talk with ChatGPT to learn more about each one.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3054625" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/02-The-Zillow-App-in-ChatGPT-Product-Mockup-2-RENTING.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Zillow&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT will also surface relevant apps when they could be helpful to a user. If someone asks for a playlist for a party this weekend, ChatGPT may call up the Spotify app in the conversation. In the future, OpenAI says apps like DoorDash, Instacart, Uber, and AllTrails will be available in ChatGPT as well. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says the new system is built using the Model Context Protocol (MCP), which allows developers to connect their data sources to an AI system. ChatGPT apps can also trigger actions and render a fully interactive UI in the chatbot’s responses. Certain apps are able to display videos in ChatGPT, which will be pinned to the top of the web page and can be altered based on user requests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If users are already subscribed to a product, they’ll be able to log in to their account directly in ChatGPT to access certain features. Altman also says OpenAI will support ways to monetize apps inside of ChatGPT in the future, including through the company’s recently launched Instant Checkout feature in ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Key questions around apps in ChatGPT will be privacy, and how much data third-party developers will have access to. OpenAI says developers must “collect only the minimum data they need, and be transparent about permissions.” However, it’s unclear whether developers would have access to a user’s entire conversation with ChatGPT, the past few messages, or just the prompt that summoned up the app.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s also unclear how ChatGPT will choose a service between competing companies, such as DoorDash and Instacart. One could imagine how companies could pay to be surfaced in ChatGPT responses, but OpenAI says it plans to prioritize the user experience above all else.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/openai-launches-apps-inside-of-chatgpt/</guid><pubDate>Mon, 06 Oct 2025 17:57:59 +0000</pubDate></item><item><title>OpenAI launches AgentKit to help developers build and ship AI agents (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/openai-launches-agentkit-to-help-developers-build-and-ship-ai-agents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-06-at-1.38.49PM.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI CEO Sam Altman on Monday announced the launch of&amp;nbsp;AgentKit, a toolkit for building and deploying&amp;nbsp;AI agents,&amp;nbsp;at the firm’s Dev Day event.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AgentKit&amp;nbsp;is a complete set of building blocks available in the open AI platform designed to help you take agents from prototype to production. It is everything you need to build, deploy, and optimize agent workflows with way less friction,” Altman said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch highlights OpenAI’s push to increase developer adoption by making agent building faster and easier. It also signals a competitive move against other AI platforms racing to offer integrated tools for building autonomous agents for enterprises that can perform complex tasks, not just respond to prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AgentKit&amp;nbsp;was one of several announcements at OpenAI’s Dev Day,&amp;nbsp;including the launch of the ability to build apps directly inside ChatGPT, which has hit 800 million weekly active users.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AgentKit&amp;nbsp;includes a few core capabilities. The first is Agent Builder, which Altman&amp;nbsp;described&amp;nbsp;as like Canva for building agents.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a fast, visual way to design the logic, steps, ideas,” Altman said. “It’s&amp;nbsp;built on top of the responses API that hundreds of thousands of developers already use.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second capability of&amp;nbsp;AgentKit&amp;nbsp;is&amp;nbsp;ChatKit, which provides a simple embeddable chat interface that developers can use to bring chat experiences into their own apps.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can bring your own brand, your own workflows, whatever makes your own product unique,” Altman said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Evals for Agents&amp;nbsp;introduces tools to measure AI agent performance, including step-by-step trace grading, datasets for assessing individual agent components, automated prompt optimization, and the ability to run evaluations on external models directly from the OpenAI platform.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally,&amp;nbsp;AgentKit&amp;nbsp;includes access to OpenAI’s connector registry, so developers can securely connect agents to internal tools and third-party systems through an “admin control panel” while&amp;nbsp;maintaining&amp;nbsp;security and control.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To prove how easy it is to use&amp;nbsp;AgentKit, Christina Huang, an OpenAI engineer, built an entire AI workflow and two AI agents live onstage in under eight minutes.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is all the stuff that we wished we had when we were trying to build our first agents,” Altman said, noting that OpenAI has already signed on several launch partners that have already scaled agents using&amp;nbsp;AgentKit.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-06-at-1.38.49PM.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI CEO Sam Altman on Monday announced the launch of&amp;nbsp;AgentKit, a toolkit for building and deploying&amp;nbsp;AI agents,&amp;nbsp;at the firm’s Dev Day event.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AgentKit&amp;nbsp;is a complete set of building blocks available in the open AI platform designed to help you take agents from prototype to production. It is everything you need to build, deploy, and optimize agent workflows with way less friction,” Altman said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch highlights OpenAI’s push to increase developer adoption by making agent building faster and easier. It also signals a competitive move against other AI platforms racing to offer integrated tools for building autonomous agents for enterprises that can perform complex tasks, not just respond to prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AgentKit&amp;nbsp;was one of several announcements at OpenAI’s Dev Day,&amp;nbsp;including the launch of the ability to build apps directly inside ChatGPT, which has hit 800 million weekly active users.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AgentKit&amp;nbsp;includes a few core capabilities. The first is Agent Builder, which Altman&amp;nbsp;described&amp;nbsp;as like Canva for building agents.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s a fast, visual way to design the logic, steps, ideas,” Altman said. “It’s&amp;nbsp;built on top of the responses API that hundreds of thousands of developers already use.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The second capability of&amp;nbsp;AgentKit&amp;nbsp;is&amp;nbsp;ChatKit, which provides a simple embeddable chat interface that developers can use to bring chat experiences into their own apps.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can bring your own brand, your own workflows, whatever makes your own product unique,” Altman said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Evals for Agents&amp;nbsp;introduces tools to measure AI agent performance, including step-by-step trace grading, datasets for assessing individual agent components, automated prompt optimization, and the ability to run evaluations on external models directly from the OpenAI platform.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally,&amp;nbsp;AgentKit&amp;nbsp;includes access to OpenAI’s connector registry, so developers can securely connect agents to internal tools and third-party systems through an “admin control panel” while&amp;nbsp;maintaining&amp;nbsp;security and control.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To prove how easy it is to use&amp;nbsp;AgentKit, Christina Huang, an OpenAI engineer, built an entire AI workflow and two AI agents live onstage in under eight minutes.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is all the stuff that we wished we had when we were trying to build our first agents,” Altman said, noting that OpenAI has already signed on several launch partners that have already scaled agents using&amp;nbsp;AgentKit.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/openai-launches-agentkit-to-help-developers-build-and-ship-ai-agents/</guid><pubDate>Mon, 06 Oct 2025 18:00:31 +0000</pubDate></item><item><title>[NEW] OpenAI announces Apps SDK allowing ChatGPT to launch and run third party apps like Zillow, Canva, Spotify (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party</link><description>[unable to retrieve full-text content]&lt;p&gt;OpenAI&amp;#x27;s annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and &lt;b&gt;CEO Sam Altman&lt;/b&gt;&lt;a href="https://openai.com/index/introducing-apps-in-chatgpt/"&gt;&lt;b&gt; announced a new &amp;quot;Apps SDK&amp;quot; &lt;/b&gt;&lt;/a&gt;&lt;b&gt;that makes it &amp;quot;possible to build apps inside of ChatGPT,&amp;quot;&lt;/b&gt; including paid apps, which companies can charge users for using OpenAI&amp;#x27;s &lt;a href="https://venturebeat.com/ai/openai-debuts-new-chatgpt-buy-button-and-open-source-agentic-commerce"&gt;recently unveiled Agentic Commerce Protocol (ACP)&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that &lt;i&gt;without &lt;/i&gt;ever leaving ChatGPT. &lt;/p&gt;&lt;p&gt;This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. &lt;/p&gt;&lt;p&gt;You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or ask Coursera about a specific lesson&amp;#x27;s content while dit plays on video, all from within ChatGPT — with many other apps also already offering their own connections (see below).&lt;/p&gt;&lt;p&gt;&amp;quot;This will enable a new generation of apps that are interactive, adaptive and personalized, that you can chat with,&amp;quot; Altman said.&lt;/p&gt;&lt;p&gt;While the Apps SDK is available today in preview, &lt;a href="https://developers.openai.com/apps-sdk"&gt;OpenAI said&lt;/a&gt; it would not begin accepting new apps within ChatGPT or allow them to charge users until &amp;quot;later this year.&amp;quot;&lt;/p&gt;&lt;p&gt;ChatGPT in-line app access is already rolling out to ChatGPT Free, Plus, Go and Pro users — outside of the European Union only for now — with Business, Enterprise, and Education tiers expected to receive access to the apps later this year.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built atop common MCP standard&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Built on &lt;a href="https://venturebeat.com/ai/mcp-and-the-innovation-paradox-why-open-standards-will-save-ai-from-itself"&gt;the open source standard Model Context Protocol (MCP)&lt;/a&gt; introduced by rival Anthropic nearly a year ago, the Apps SDK gives third-party developers working independently or on behalf of enterprises large and small to connect selected data, &amp;quot;trigger actions, and render a fully interactive UI [user interface]&amp;quot; Altman explained during his introductory keynote speech. &lt;/p&gt;&lt;p&gt;The Apps SDK includes a &amp;quot;talking to apps&amp;quot; feature that allows ChatGPT and the underlying GPT-5 or other &amp;quot;o-series&amp;quot; models piloting it underneath to obtain updated context from the third-party app or service, so the model &amp;quot;always knows about exactly what you&amp;#x27;re user is interacting with,&amp;quot; according to another presenter and OpenAI engineer, Alexi Christakis.&lt;/p&gt;&lt;p&gt;Developers can build apps that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;appear &lt;b&gt;inline&lt;/b&gt; in chat as lightweight cards or carousels&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;expand to &lt;b&gt;fullscreen&lt;/b&gt; for immersive tasks like maps, menus, or slides&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;use &lt;b&gt;picture-in-picture&lt;/b&gt; for live sessions such as video, games, or quizzes&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each mode is designed to preserve ChatGPT’s minimal, conversational flow while adding interactivity and brand presence.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Early integrations with Coursera, Canva, Zillow and more...&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Christakis showed off early integrations of&lt;b&gt; external apps built atop the Apps SDK&lt;/b&gt;, including ones from e-learning company &lt;b&gt;Coursera&lt;/b&gt;, cloud design software company &lt;b&gt;Canva&lt;/b&gt;, and real estate listings and agent connections search engine, &lt;b&gt;Zillow&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Altman also announced Apps SDK integrations with additional partners not demoed officially during the keynote including: &lt;b&gt;Booking.com&lt;/b&gt;, &lt;b&gt;Expedia&lt;/b&gt;, &lt;b&gt;Figma&lt;/b&gt; and &lt;b&gt;Spotify &lt;/b&gt;and in documentation, said more upcoming partners are on deck: &lt;b&gt;AllTrails, Peloton, OpenTable, Target, theFork, and Uber&lt;/b&gt;, representing lifestyle, commerce, and productivity categories.&lt;/p&gt;&lt;p&gt;The &lt;b&gt;Coursera demo&lt;/b&gt; included an example of how the user onboards to the external app, including a new login screen for the app (Coursera) that appears within the ChatGPT chat interface, activated simply by a text prompt from the user asking: &amp;quot;Coursera can you teach me something about machine learning&amp;quot;?&lt;/p&gt;&lt;p&gt;Once logged in, the app launched within the chat interface, &amp;quot;in line&amp;quot; and can render anything from the web, including interactive elements like video. &lt;/p&gt;&lt;p&gt;Christakis explained and showed the Apps SDK also supports &amp;quot;picture-in-picture&amp;quot; and &amp;quot;fullscreen&amp;quot; views, allowing the user to choose how to interact with it.&lt;/p&gt;&lt;p&gt;When playing a Coursera video that appeared, he showed that it automatically pinned the video to the top of the screen so the user could keep watching it even as they continued to have a back-and-forth dialog in text with ChatGPT in the typical input/output prompts and responses below. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Users can then ask ChatGPT about content appearing in the video without specifying exactly what was said&lt;/b&gt;, as the Agents SDK pipes the information on the backend, server-side, from the connected app to the underlying ChatGPT AI model. So &amp;quot;can you explain more about what they&amp;#x27;re saying right now&amp;quot; will automatically surface the relevant portion of the video and provide that to the underlying AI model for it to analyze and respond to through text.&lt;/p&gt;&lt;p&gt;In another example, Christakis opened an older, existing ChatGPT conversation he&amp;#x27;d had about his siblings&amp;#x27; dog walking business and resumed the conversation by asking another third-party app, &lt;b&gt;Canva&lt;/b&gt;, to generate a poster using one of ChatGPT&amp;#x27;s recommended business names, &amp;quot;Walk This Wag,&amp;quot; along with specific guidance about font choice (&amp;quot;sans serif&amp;quot;) and overall coloration and style (&amp;quot;bright and colorful.&amp;quot;)&lt;/p&gt;&lt;p&gt;Instead of the user manually having to go and add all those specific elements to a Canva template, ChatGPT went and issued the commands and performed the actions on behalf of the user in the background.&lt;/p&gt;&lt;p&gt;After a few minutes, ChatGPT responded with several poster designs generated directly within the Canva app, but displayed them all in the user&amp;#x27;s ChatGPT chat session where they could see, review, enlarge and provide feedback or ask for adjustments on all of them.&lt;/p&gt;&lt;p&gt;Christakis then asked for ChatGPT to turn one of the slides into an entire slide deck so the founders of the dog walking business could present it to investors, which did it in the background over several minutes while he presented a final integrated app, &lt;b&gt;Zillow&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;He started a new chat session and asked a simple question: &amp;quot;based on our conversations, what would be a good city to expand the dog walking business.&amp;quot;&lt;/p&gt;&lt;p&gt;Using ChatGPT&amp;#x27;s optional memory feature, it referenced the dog walk conversation and suggested Pittsburgh, which Christakis used as a chance to type in &amp;quot;Zillow&amp;quot; and &amp;quot;show me some homes for sale there,&amp;quot; which called up an interactive map from Zillow with homes for sale and prices listed and hover-over animations, all in-line within ChatGPT.&lt;/p&gt;&lt;p&gt;Clicking a specific home also opened a fullscreen view with &amp;quot;most of the Zillow experience,&amp;quot; entirely without leaving ChatGPT, including the ability to request home tours and contact agents and filtering by bedrooms and other qualities like outdoor space. ChatGPT pulls up the requested filtered Zillow search as well as provides a text-based response in-line explaining what it did and why.&lt;/p&gt;&lt;p&gt;The user can then ask follow-up questions about the specific property — such as &amp;quot;how close is it to a dog park?&amp;quot; — or compare it to other properties, all within ChatGPT.&lt;/p&gt;&lt;p&gt;It can also use apps in conjunction with its Search function, searching the web to compare the app information (in this case, Zillow) with other sources.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety, privacy, and developer standards&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;OpenAI emphasized that &lt;b&gt;apps must comply with strict privacy, safety, and content &lt;/b&gt;&lt;a href="https://developers.openai.com/apps-sdk/app-developer-guidelines"&gt;&lt;b&gt;standards&lt;/b&gt;&lt;/a&gt; to be listed in the ChatGPT directory. Apps must:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;serve a &lt;b&gt;clear and valuable purpose&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;be &lt;b&gt;predictable and reliable&lt;/b&gt; in behavior&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;be &lt;b&gt;safe for general audiences&lt;/b&gt;, including teens aged 13–17&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;respect user privacy&lt;/b&gt; and limit data collection to only what’s necessary&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Every app must also include a &lt;b&gt;clear, published privacy policy&lt;/b&gt;, obtain user consent before connecting, and identify any actions that modify external data (e.g., posting, sending, uploading).&lt;/p&gt;&lt;p&gt;Apps violating OpenAI’s usage policies, crashing frequently, or misrepresenting their capabilities may be removed at any time. Developers must submit from &lt;b&gt;verified accounts&lt;/b&gt;, provide &lt;b&gt;customer support contacts&lt;/b&gt;, and maintain their apps for stability and compliance.&lt;/p&gt;&lt;p&gt;OpenAI also published &lt;b&gt;developer &lt;/b&gt;&lt;a href="https://developers.openai.com/apps-sdk/concepts/design-guidelines"&gt;&lt;b&gt;design guidelines&lt;/b&gt;&lt;/a&gt;, outlining how apps should look, sound, and behave. They must follow ChatGPT’s visual system — including consistent color palettes, typography, spacing, and iconography — and maintain accessibility standards such as alt text and readable contrast ratios.&lt;/p&gt;&lt;p&gt;Partners can show brand logos and accent colors but not alter ChatGPT’s core interface or use promotional language. Apps should remain &lt;b&gt;“conversational, intelligent, simple, responsive, and accessible,”&lt;/b&gt; according to the documentation.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A new conversational app ecosystem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;By opening ChatGPT to third-party apps and payments, OpenAI is taking a major step toward transforming ChatGPT from a chatbot into a full-fledged &lt;b&gt;AI operating system&lt;/b&gt; — one that combines conversational intelligence, rich interfaces, and embedded commerce.&lt;/p&gt;&lt;p&gt;For developers, that means direct access to &lt;b&gt;over 800 million ChatGPT users&lt;/b&gt;, who can discover apps “at the right time” through natural conversation — whether planning trips, learning, or shopping.&lt;/p&gt;&lt;p&gt;For users, it means &lt;b&gt;a new generation of apps you can chat with&lt;/b&gt; — where a single interface helps you book a flight, design a slide deck, or learn a new skill without ever leaving ChatGPT.&lt;/p&gt;&lt;p&gt;As OpenAI put it: “This is just the start of apps in ChatGPT, bringing new utility to users and new opportunities for developers.”&lt;/p&gt;&lt;p&gt;There remain a few big questions, namely: 1. what happens to all the data from those third-party apps as they interface with ChatGPT and its users...does OpenAI get access to it and can it train upon it? 2. What happens to OpenAI&amp;#x27;s once much-hyped&lt;a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come"&gt; GPT Store,&lt;/a&gt; which had been in the past promoted as a way for third-party creators and developers to create custom, task-specific versions of ChatGPT and make money on them through a usage-based revenue share model?&lt;/p&gt;&lt;p&gt;We&amp;#x27;ve asked the company about both issues and will update when we hear back. &lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;OpenAI&amp;#x27;s annual conference for third-party developers, DevDay, kicked off with a bang today as co-founder and &lt;b&gt;CEO Sam Altman&lt;/b&gt;&lt;a href="https://openai.com/index/introducing-apps-in-chatgpt/"&gt;&lt;b&gt; announced a new &amp;quot;Apps SDK&amp;quot; &lt;/b&gt;&lt;/a&gt;&lt;b&gt;that makes it &amp;quot;possible to build apps inside of ChatGPT,&amp;quot;&lt;/b&gt; including paid apps, which companies can charge users for using OpenAI&amp;#x27;s &lt;a href="https://venturebeat.com/ai/openai-debuts-new-chatgpt-buy-button-and-open-source-agentic-commerce"&gt;recently unveiled Agentic Commerce Protocol (ACP)&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;In other words, instead of launching apps one-by-one on your phone, computer, or on the web — now you can do all that &lt;i&gt;without &lt;/i&gt;ever leaving ChatGPT. &lt;/p&gt;&lt;p&gt;This feature allows the user to log-into their accounts on those external apps and bring all their information back into ChatGPT, and use the apps very similarly to how they already do outside of the chatbot, but now with the ability to ask ChatGPT to perform certain actions, analyze content, or go beyond what each app could offer on its own. &lt;/p&gt;&lt;p&gt;You can direct Canva to make you slides based on a text description, ask Zillow for home listings in a certain area fitting certain requirements, or ask Coursera about a specific lesson&amp;#x27;s content while dit plays on video, all from within ChatGPT — with many other apps also already offering their own connections (see below).&lt;/p&gt;&lt;p&gt;&amp;quot;This will enable a new generation of apps that are interactive, adaptive and personalized, that you can chat with,&amp;quot; Altman said.&lt;/p&gt;&lt;p&gt;While the Apps SDK is available today in preview, &lt;a href="https://developers.openai.com/apps-sdk"&gt;OpenAI said&lt;/a&gt; it would not begin accepting new apps within ChatGPT or allow them to charge users until &amp;quot;later this year.&amp;quot;&lt;/p&gt;&lt;p&gt;ChatGPT in-line app access is already rolling out to ChatGPT Free, Plus, Go and Pro users — outside of the European Union only for now — with Business, Enterprise, and Education tiers expected to receive access to the apps later this year.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built atop common MCP standard&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Built on &lt;a href="https://venturebeat.com/ai/mcp-and-the-innovation-paradox-why-open-standards-will-save-ai-from-itself"&gt;the open source standard Model Context Protocol (MCP)&lt;/a&gt; introduced by rival Anthropic nearly a year ago, the Apps SDK gives third-party developers working independently or on behalf of enterprises large and small to connect selected data, &amp;quot;trigger actions, and render a fully interactive UI [user interface]&amp;quot; Altman explained during his introductory keynote speech. &lt;/p&gt;&lt;p&gt;The Apps SDK includes a &amp;quot;talking to apps&amp;quot; feature that allows ChatGPT and the underlying GPT-5 or other &amp;quot;o-series&amp;quot; models piloting it underneath to obtain updated context from the third-party app or service, so the model &amp;quot;always knows about exactly what you&amp;#x27;re user is interacting with,&amp;quot; according to another presenter and OpenAI engineer, Alexi Christakis.&lt;/p&gt;&lt;p&gt;Developers can build apps that:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;appear &lt;b&gt;inline&lt;/b&gt; in chat as lightweight cards or carousels&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;expand to &lt;b&gt;fullscreen&lt;/b&gt; for immersive tasks like maps, menus, or slides&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;use &lt;b&gt;picture-in-picture&lt;/b&gt; for live sessions such as video, games, or quizzes&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Each mode is designed to preserve ChatGPT’s minimal, conversational flow while adding interactivity and brand presence.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Early integrations with Coursera, Canva, Zillow and more...&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Christakis showed off early integrations of&lt;b&gt; external apps built atop the Apps SDK&lt;/b&gt;, including ones from e-learning company &lt;b&gt;Coursera&lt;/b&gt;, cloud design software company &lt;b&gt;Canva&lt;/b&gt;, and real estate listings and agent connections search engine, &lt;b&gt;Zillow&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;Altman also announced Apps SDK integrations with additional partners not demoed officially during the keynote including: &lt;b&gt;Booking.com&lt;/b&gt;, &lt;b&gt;Expedia&lt;/b&gt;, &lt;b&gt;Figma&lt;/b&gt; and &lt;b&gt;Spotify &lt;/b&gt;and in documentation, said more upcoming partners are on deck: &lt;b&gt;AllTrails, Peloton, OpenTable, Target, theFork, and Uber&lt;/b&gt;, representing lifestyle, commerce, and productivity categories.&lt;/p&gt;&lt;p&gt;The &lt;b&gt;Coursera demo&lt;/b&gt; included an example of how the user onboards to the external app, including a new login screen for the app (Coursera) that appears within the ChatGPT chat interface, activated simply by a text prompt from the user asking: &amp;quot;Coursera can you teach me something about machine learning&amp;quot;?&lt;/p&gt;&lt;p&gt;Once logged in, the app launched within the chat interface, &amp;quot;in line&amp;quot; and can render anything from the web, including interactive elements like video. &lt;/p&gt;&lt;p&gt;Christakis explained and showed the Apps SDK also supports &amp;quot;picture-in-picture&amp;quot; and &amp;quot;fullscreen&amp;quot; views, allowing the user to choose how to interact with it.&lt;/p&gt;&lt;p&gt;When playing a Coursera video that appeared, he showed that it automatically pinned the video to the top of the screen so the user could keep watching it even as they continued to have a back-and-forth dialog in text with ChatGPT in the typical input/output prompts and responses below. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Users can then ask ChatGPT about content appearing in the video without specifying exactly what was said&lt;/b&gt;, as the Agents SDK pipes the information on the backend, server-side, from the connected app to the underlying ChatGPT AI model. So &amp;quot;can you explain more about what they&amp;#x27;re saying right now&amp;quot; will automatically surface the relevant portion of the video and provide that to the underlying AI model for it to analyze and respond to through text.&lt;/p&gt;&lt;p&gt;In another example, Christakis opened an older, existing ChatGPT conversation he&amp;#x27;d had about his siblings&amp;#x27; dog walking business and resumed the conversation by asking another third-party app, &lt;b&gt;Canva&lt;/b&gt;, to generate a poster using one of ChatGPT&amp;#x27;s recommended business names, &amp;quot;Walk This Wag,&amp;quot; along with specific guidance about font choice (&amp;quot;sans serif&amp;quot;) and overall coloration and style (&amp;quot;bright and colorful.&amp;quot;)&lt;/p&gt;&lt;p&gt;Instead of the user manually having to go and add all those specific elements to a Canva template, ChatGPT went and issued the commands and performed the actions on behalf of the user in the background.&lt;/p&gt;&lt;p&gt;After a few minutes, ChatGPT responded with several poster designs generated directly within the Canva app, but displayed them all in the user&amp;#x27;s ChatGPT chat session where they could see, review, enlarge and provide feedback or ask for adjustments on all of them.&lt;/p&gt;&lt;p&gt;Christakis then asked for ChatGPT to turn one of the slides into an entire slide deck so the founders of the dog walking business could present it to investors, which did it in the background over several minutes while he presented a final integrated app, &lt;b&gt;Zillow&lt;/b&gt;.&lt;/p&gt;&lt;p&gt;He started a new chat session and asked a simple question: &amp;quot;based on our conversations, what would be a good city to expand the dog walking business.&amp;quot;&lt;/p&gt;&lt;p&gt;Using ChatGPT&amp;#x27;s optional memory feature, it referenced the dog walk conversation and suggested Pittsburgh, which Christakis used as a chance to type in &amp;quot;Zillow&amp;quot; and &amp;quot;show me some homes for sale there,&amp;quot; which called up an interactive map from Zillow with homes for sale and prices listed and hover-over animations, all in-line within ChatGPT.&lt;/p&gt;&lt;p&gt;Clicking a specific home also opened a fullscreen view with &amp;quot;most of the Zillow experience,&amp;quot; entirely without leaving ChatGPT, including the ability to request home tours and contact agents and filtering by bedrooms and other qualities like outdoor space. ChatGPT pulls up the requested filtered Zillow search as well as provides a text-based response in-line explaining what it did and why.&lt;/p&gt;&lt;p&gt;The user can then ask follow-up questions about the specific property — such as &amp;quot;how close is it to a dog park?&amp;quot; — or compare it to other properties, all within ChatGPT.&lt;/p&gt;&lt;p&gt;It can also use apps in conjunction with its Search function, searching the web to compare the app information (in this case, Zillow) with other sources.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety, privacy, and developer standards&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;OpenAI emphasized that &lt;b&gt;apps must comply with strict privacy, safety, and content &lt;/b&gt;&lt;a href="https://developers.openai.com/apps-sdk/app-developer-guidelines"&gt;&lt;b&gt;standards&lt;/b&gt;&lt;/a&gt; to be listed in the ChatGPT directory. Apps must:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;serve a &lt;b&gt;clear and valuable purpose&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;be &lt;b&gt;predictable and reliable&lt;/b&gt; in behavior&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;be &lt;b&gt;safe for general audiences&lt;/b&gt;, including teens aged 13–17&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;respect user privacy&lt;/b&gt; and limit data collection to only what’s necessary&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Every app must also include a &lt;b&gt;clear, published privacy policy&lt;/b&gt;, obtain user consent before connecting, and identify any actions that modify external data (e.g., posting, sending, uploading).&lt;/p&gt;&lt;p&gt;Apps violating OpenAI’s usage policies, crashing frequently, or misrepresenting their capabilities may be removed at any time. Developers must submit from &lt;b&gt;verified accounts&lt;/b&gt;, provide &lt;b&gt;customer support contacts&lt;/b&gt;, and maintain their apps for stability and compliance.&lt;/p&gt;&lt;p&gt;OpenAI also published &lt;b&gt;developer &lt;/b&gt;&lt;a href="https://developers.openai.com/apps-sdk/concepts/design-guidelines"&gt;&lt;b&gt;design guidelines&lt;/b&gt;&lt;/a&gt;, outlining how apps should look, sound, and behave. They must follow ChatGPT’s visual system — including consistent color palettes, typography, spacing, and iconography — and maintain accessibility standards such as alt text and readable contrast ratios.&lt;/p&gt;&lt;p&gt;Partners can show brand logos and accent colors but not alter ChatGPT’s core interface or use promotional language. Apps should remain &lt;b&gt;“conversational, intelligent, simple, responsive, and accessible,”&lt;/b&gt; according to the documentation.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;A new conversational app ecosystem&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;By opening ChatGPT to third-party apps and payments, OpenAI is taking a major step toward transforming ChatGPT from a chatbot into a full-fledged &lt;b&gt;AI operating system&lt;/b&gt; — one that combines conversational intelligence, rich interfaces, and embedded commerce.&lt;/p&gt;&lt;p&gt;For developers, that means direct access to &lt;b&gt;over 800 million ChatGPT users&lt;/b&gt;, who can discover apps “at the right time” through natural conversation — whether planning trips, learning, or shopping.&lt;/p&gt;&lt;p&gt;For users, it means &lt;b&gt;a new generation of apps you can chat with&lt;/b&gt; — where a single interface helps you book a flight, design a slide deck, or learn a new skill without ever leaving ChatGPT.&lt;/p&gt;&lt;p&gt;As OpenAI put it: “This is just the start of apps in ChatGPT, bringing new utility to users and new opportunities for developers.”&lt;/p&gt;&lt;p&gt;There remain a few big questions, namely: 1. what happens to all the data from those third-party apps as they interface with ChatGPT and its users...does OpenAI get access to it and can it train upon it? 2. What happens to OpenAI&amp;#x27;s once much-hyped&lt;a href="https://venturebeat.com/ai/openai-launches-gpt-store-but-revenue-sharing-is-still-to-come"&gt; GPT Store,&lt;/a&gt; which had been in the past promoted as a way for third-party creators and developers to create custom, task-specific versions of ChatGPT and make money on them through a usage-based revenue share model?&lt;/p&gt;&lt;p&gt;We&amp;#x27;ve asked the company about both issues and will update when we hear back. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-announces-apps-sdk-allowing-chatgpt-to-launch-and-run-third-party</guid><pubDate>Mon, 06 Oct 2025 18:24:00 +0000</pubDate></item><item><title>[NEW] MrBeast says AI could threaten creators’ livelihoods, calling it ‘scary times’ for the industry (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/mrbeast-says-ai-could-threaten-creators-livelihoods-calling-it-scary-times-for-the-industry/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Beast-Games-Suit-Arms-Crossed.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Top YouTube creator MrBeast is worried about AI’s impact on creators’ livelihoods, despite having dabbled with using the technology himself. On Monday, the creator posted his concerns on social media, where he openly wondered how AI-generated videos could affect the “millions of creators currently making content for a living.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Scary times,” he added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;MrBeast, whose real name is Jimmy Donaldson, is No. 1 on Forbes’ 2025 list of top creators, with $85 million in earnings and 634 million followers. What he says and does, as a result of his position, has an outsized influence across the industry. So if MrBeast is openly questioning whether AI is an existential threat to his business and others like it, then it’s fair to say that smaller creators are likely even more worried.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments follow the recent launch of OpenAI’s Sora 2, a new version of its audio and video generator, alongside a mobile app that lets users create AI, including videos of themselves, which are shared in a TikTok-style vertical feed. The app has been an early hit, quickly hitting No. 1 on the U.S. App Store after a surge of downloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube has also embraced AI, launching things like AI editing tools, including those that let creators generate AI videos using its video model Veo to animate still photos or apply different styles to their videos. The company has infused AI into its product as well, for things like making clips or highlights from Live videos or podcasts. An AI chatbot can answer creators’ questions inside YouTube’s channel management software, YouTube Studio.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MrBeast has also involved himself with AI, as commenters were quick to point out. The creator this summer faced a fair bit of backlash from fans and creators alike after releasing a tool that used AI to create video thumbnails. He quickly removed the tool from his analytics platform, Viewstats, and said he’d replace it with links to human artists available for commission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His company’s philanthropy arm has also made AI investments at times.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;There is still debate as to whether the novelty of AI video creation will turn everyone into a creator, or if the best videos will still need a human’s creative mind to think them up and then prompt the tool correctly. At the same time, there are those who view AI videos as low-quality content, often dubbed “slop,” and dislike seeing it in their feeds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even if the AI becomes undetectable at some point in the future, it’s possible that creators revealed to be using it without disclosure could lose their fans’ trust and harm their reputation. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Beast-Games-Suit-Arms-Crossed.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Top YouTube creator MrBeast is worried about AI’s impact on creators’ livelihoods, despite having dabbled with using the technology himself. On Monday, the creator posted his concerns on social media, where he openly wondered how AI-generated videos could affect the “millions of creators currently making content for a living.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Scary times,” he added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;MrBeast, whose real name is Jimmy Donaldson, is No. 1 on Forbes’ 2025 list of top creators, with $85 million in earnings and 634 million followers. What he says and does, as a result of his position, has an outsized influence across the industry. So if MrBeast is openly questioning whether AI is an existential threat to his business and others like it, then it’s fair to say that smaller creators are likely even more worried.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His comments follow the recent launch of OpenAI’s Sora 2, a new version of its audio and video generator, alongside a mobile app that lets users create AI, including videos of themselves, which are shared in a TikTok-style vertical feed. The app has been an early hit, quickly hitting No. 1 on the U.S. App Store after a surge of downloads.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube has also embraced AI, launching things like AI editing tools, including those that let creators generate AI videos using its video model Veo to animate still photos or apply different styles to their videos. The company has infused AI into its product as well, for things like making clips or highlights from Live videos or podcasts. An AI chatbot can answer creators’ questions inside YouTube’s channel management software, YouTube Studio.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;MrBeast has also involved himself with AI, as commenters were quick to point out. The creator this summer faced a fair bit of backlash from fans and creators alike after releasing a tool that used AI to create video thumbnails. He quickly removed the tool from his analytics platform, Viewstats, and said he’d replace it with links to human artists available for commission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His company’s philanthropy arm has also made AI investments at times.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;There is still debate as to whether the novelty of AI video creation will turn everyone into a creator, or if the best videos will still need a human’s creative mind to think them up and then prompt the tool correctly. At the same time, there are those who view AI videos as low-quality content, often dubbed “slop,” and dislike seeing it in their feeds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even if the AI becomes undetectable at some point in the future, it’s possible that creators revealed to be using it without disclosure could lose their fans’ trust and harm their reputation. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/mrbeast-says-ai-could-threaten-creators-livelihoods-calling-it-scary-times-for-the-industry/</guid><pubDate>Mon, 06 Oct 2025 19:05:20 +0000</pubDate></item><item><title>[NEW] OpenAI ramps up developer push with more powerful models in its API (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/openai-ramps-up-developer-push-with-more-powerful-models-in-its-api/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-06-at-3.05.51PM.png?resize=1200,527" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI&amp;nbsp;unveiled&amp;nbsp;new API updates&amp;nbsp;at its Dev Day on Monday,&amp;nbsp;introducing GPT-5 Pro,&amp;nbsp;its latest language model,&amp;nbsp;its new video generation model&amp;nbsp;Sora 2, and a smaller, cheaper voice model.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The updates were part of a series of&amp;nbsp;announcements&amp;nbsp;geared toward wooing developers to OpenAI’s ecosystem, including the launch of an&amp;nbsp;agent-building tool&amp;nbsp;and the&amp;nbsp;ability to build apps in ChatGPT.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The addition of GPT-5 Pro might appeal to developers building applications in finance, legal, and healthcare — industries that need “high accuracy and depth of reasoning,” per OpenAI CEO Sam Altman.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman also noted that voice capabilities will be essential in the future as it quickly becomes one of the primary ways people use to interact with AI.&amp;nbsp;To that end, OpenAI is&amp;nbsp;launching&amp;nbsp;“gpt-realtime&amp;nbsp;mini,”&amp;nbsp;a&amp;nbsp;smaller, cheaper&amp;nbsp;voice model&amp;nbsp;in API&amp;nbsp;that supports low-latency streaming interactions for audio and speech.&amp;nbsp;The new model is 70% cheaper than&amp;nbsp;OpenAI’s&amp;nbsp;previous&amp;nbsp;advanced voice model&amp;nbsp;but&amp;nbsp;promises the “same voice quality and expressiveness.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, creators involved in OpenAI’s developer ecosystem can&amp;nbsp;now&amp;nbsp;tap into Sora 2 in preview in the API. OpenAI released Sora 2, its latest audio and video generator, last week alongside the Sora app,&amp;nbsp;a TikTok competitor&amp;nbsp;filled with&amp;nbsp;short AI-generated videos. The Sora app allows users to generate videos of themselves, friends, or anything based on a&amp;nbsp;prompt, and share it via a TikTok-style algorithmic feed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Developers] now have access to the same model that powers Sora 2’s stunning video outputs right in your own app,” Altman said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sora 2 builds on its&amp;nbsp;previous&amp;nbsp;generation with more realistic, physically consistent scenes with synchronized sound and greater creative control — from detailed camera direction to stylized visuals.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“For example, you can take the iPhone view and prompt Sora to expand it into a sweeping, cinematic wide shot,” Altman said.&amp;nbsp;“But one of the most exciting things that&amp;nbsp;we’ve been working on is how well this new model pairs sound with visuals, not just speech, but rich soundscapes, ambient audio, synchronized effects that are grounded in what you’re seeing.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sora 2 is pitched as a tool for concept development, whether it’s a visual starting point for an ad based on the general vibe of a product, or a Mattel designer turning a sketch into a toy concept&amp;nbsp;— an example Altman provided at Dev Day that sheds light on&amp;nbsp;OpenAI’s deal with the Barbie-maker&amp;nbsp;to bring generative AI into the toy-making pipeline.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-06-at-3.05.51PM.png?resize=1200,527" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI&amp;nbsp;unveiled&amp;nbsp;new API updates&amp;nbsp;at its Dev Day on Monday,&amp;nbsp;introducing GPT-5 Pro,&amp;nbsp;its latest language model,&amp;nbsp;its new video generation model&amp;nbsp;Sora 2, and a smaller, cheaper voice model.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The updates were part of a series of&amp;nbsp;announcements&amp;nbsp;geared toward wooing developers to OpenAI’s ecosystem, including the launch of an&amp;nbsp;agent-building tool&amp;nbsp;and the&amp;nbsp;ability to build apps in ChatGPT.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The addition of GPT-5 Pro might appeal to developers building applications in finance, legal, and healthcare — industries that need “high accuracy and depth of reasoning,” per OpenAI CEO Sam Altman.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman also noted that voice capabilities will be essential in the future as it quickly becomes one of the primary ways people use to interact with AI.&amp;nbsp;To that end, OpenAI is&amp;nbsp;launching&amp;nbsp;“gpt-realtime&amp;nbsp;mini,”&amp;nbsp;a&amp;nbsp;smaller, cheaper&amp;nbsp;voice model&amp;nbsp;in API&amp;nbsp;that supports low-latency streaming interactions for audio and speech.&amp;nbsp;The new model is 70% cheaper than&amp;nbsp;OpenAI’s&amp;nbsp;previous&amp;nbsp;advanced voice model&amp;nbsp;but&amp;nbsp;promises the “same voice quality and expressiveness.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, creators involved in OpenAI’s developer ecosystem can&amp;nbsp;now&amp;nbsp;tap into Sora 2 in preview in the API. OpenAI released Sora 2, its latest audio and video generator, last week alongside the Sora app,&amp;nbsp;a TikTok competitor&amp;nbsp;filled with&amp;nbsp;short AI-generated videos. The Sora app allows users to generate videos of themselves, friends, or anything based on a&amp;nbsp;prompt, and share it via a TikTok-style algorithmic feed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Developers] now have access to the same model that powers Sora 2’s stunning video outputs right in your own app,” Altman said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sora 2 builds on its&amp;nbsp;previous&amp;nbsp;generation with more realistic, physically consistent scenes with synchronized sound and greater creative control — from detailed camera direction to stylized visuals.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“For example, you can take the iPhone view and prompt Sora to expand it into a sweeping, cinematic wide shot,” Altman said.&amp;nbsp;“But one of the most exciting things that&amp;nbsp;we’ve been working on is how well this new model pairs sound with visuals, not just speech, but rich soundscapes, ambient audio, synchronized effects that are grounded in what you’re seeing.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sora 2 is pitched as a tool for concept development, whether it’s a visual starting point for an ad based on the general vibe of a product, or a Mattel designer turning a sketch into a toy concept&amp;nbsp;— an example Altman provided at Dev Day that sheds light on&amp;nbsp;OpenAI’s deal with the Barbie-maker&amp;nbsp;to bring generative AI into the toy-making pipeline.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/openai-ramps-up-developer-push-with-more-powerful-models-in-its-api/</guid><pubDate>Mon, 06 Oct 2025 19:10:27 +0000</pubDate></item><item><title>[NEW] OpenAI wants to make ChatGPT into a universal app frontend (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/openai-wants-to-make-chatgpt-into-a-universal-app-frontend/</link><description>&lt;article class="double-column h-entry post-2120929 post type-post status-publish format-standard has-post-thumbnail hentry category-ai tag-apps tag-canva tag-chatgpt tag-dev-days tag-figma tag-openai tag-sdk tag-spotify tag-zillow"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify, Canva, Zillow among today's launch partners, more coming later this year.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="383" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/appgpt-640x383.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/appgpt-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI's Alexi Christakis shows Figma-generated posters being generated within a ChatGPT conversation window.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;At an OpenAI Dev Days keynote today, CEO Sam Altman announced that the company is launching an SDK preview that will allow developers the ability "to build real apps inside of ChatGPT." Altman said that, starting today, the new SDK will give developers "full stack" control over app data, action triggers, and even interactive user interfaces for apps that can appear inline as part of an existing ChatGPT conversation window.&lt;/p&gt;
&lt;p&gt;The SDK is built on the open source Model Context Protocol (MCP), Altman said. That means developers that already use MCP only need to add an HTML resource to enable ChatGPT integration, he added.&lt;/p&gt;
&lt;p&gt;The new integration means a ChatGPT user can directly ask Figma to turn a sketch into a diagram, for instance, and get results integrated into their ChatGPT conversation. It also means that ChatGPT can suggest apps that might be suited to a more general query, like recommending and creating a Spotify playlist when someone asks for song suggestions.&lt;/p&gt;
&lt;p&gt;In a live onstage demo, OpenAI software engineer Alexi Christakis showed how the new API can "expose context back to ChatGPT from your app," a process he likened to ChatGPT "talking to apps." For instance, the LLM can expand in real time on what's being said in an embedded Coursera video. "I don't need to explain what I'm seeing in the video, ChatGPT sees it right away," Christakis said on stage.&lt;/p&gt;
&lt;p&gt;Other onstage demos showed off ChatGPT using Canva to generate poster ideas in the background while the user consulted an inline Zillow map for information. Even when the Zillow window was expanded to full screen, the user could ask ChatGPT for additional context via an overlaid chat window.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While Altman mentioned an "agentic commerce protocol" that will allow app users to enjoy "instant checkout" from within ChatGPT, he later clarified that details on monetization will only be available "soon."&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120934 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="900" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/appgpt2.png" width="2523" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A full list of third-party apps that will be integrated into ChatGPT in the coming weeks.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition to the apps mentioned above, others like Expedia and Booking.com will be available in ChatGPT starting today. Apps from other launch partners including Peloton, Target, Uber, and Doordash will be available inside ChatGPT "in the weeks ahead."&lt;/p&gt;
&lt;p&gt;Other developers can start building with the SDK today before submitting them to OpenAI for review and publication within ChatGPT "later this year." Altman said that apps that meet a certain set of "developer guidelines" will be listed in a comprehensive directory, while those meeting "higher standards for design and functionality will be featured more prominently."&lt;/p&gt;
&lt;h2&gt;AgentKit and API updates&lt;/h2&gt;
&lt;p&gt;Elsewhere in the keynote, Altman announced AgentKit, a new tool designed to let OpenAI users create specialized interactive chatbots using a simplified building block GUI interface. The new software includes integrated tools for measuring performance and testing workflows from within the ChatKit interface.&lt;/p&gt;
&lt;p&gt;In a live demo, OpenAI platform experience specialist Christina Huang gave herself an eight-minute deadline to use AgentKit to create a live, customized question-answering "Ask Froge" chatbot for the Dev Day website. While that demo was done with time to spare, Huang did make use of a lot of pre-built "widgets" and documents full of prepopulated information about the event to streamline the chatbot's creation.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI's Dev Days keynote in full.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The keynote also announced minor updates for OpenAI's codex coding agent, including integration with Slack and a new SDK to allow for easier integration into existing coding workflows. Altman also announced some recent models would be newly available to users via API, including Sora 2, GPT5-Pro, and a new smaller, cheaper version of the company's real-time audio interface.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #8c9eff; background-color: #1a237e;"&gt;&lt;span class="ars-avatar-letter"&gt;t&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              timby
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            So ... this is OpenAI's attempt at creating, would you say, an "everything" app?&lt;p&gt;Man, if we thought Elon hated Sam Altman &lt;i&gt;already&lt;/i&gt;, we haven't seen &lt;i&gt;anything&lt;/i&gt; yet.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-10-06T19:29:18+00:00"&gt;October 6, 2025 at 7:29 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2120929 post type-post status-publish format-standard has-post-thumbnail hentry category-ai tag-apps tag-canva tag-chatgpt tag-dev-days tag-figma tag-openai tag-sdk tag-spotify tag-zillow"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Spotify, Canva, Zillow among today's launch partners, more coming later this year.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="383" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/appgpt-640x383.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/appgpt-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI's Alexi Christakis shows Figma-generated posters being generated within a ChatGPT conversation window.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;At an OpenAI Dev Days keynote today, CEO Sam Altman announced that the company is launching an SDK preview that will allow developers the ability "to build real apps inside of ChatGPT." Altman said that, starting today, the new SDK will give developers "full stack" control over app data, action triggers, and even interactive user interfaces for apps that can appear inline as part of an existing ChatGPT conversation window.&lt;/p&gt;
&lt;p&gt;The SDK is built on the open source Model Context Protocol (MCP), Altman said. That means developers that already use MCP only need to add an HTML resource to enable ChatGPT integration, he added.&lt;/p&gt;
&lt;p&gt;The new integration means a ChatGPT user can directly ask Figma to turn a sketch into a diagram, for instance, and get results integrated into their ChatGPT conversation. It also means that ChatGPT can suggest apps that might be suited to a more general query, like recommending and creating a Spotify playlist when someone asks for song suggestions.&lt;/p&gt;
&lt;p&gt;In a live onstage demo, OpenAI software engineer Alexi Christakis showed how the new API can "expose context back to ChatGPT from your app," a process he likened to ChatGPT "talking to apps." For instance, the LLM can expand in real time on what's being said in an embedded Coursera video. "I don't need to explain what I'm seeing in the video, ChatGPT sees it right away," Christakis said on stage.&lt;/p&gt;
&lt;p&gt;Other onstage demos showed off ChatGPT using Canva to generate poster ideas in the background while the user consulted an inline Zillow map for information. Even when the Zillow window was expanded to full screen, the user could ask ChatGPT for additional context via an overlaid chat window.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While Altman mentioned an "agentic commerce protocol" that will allow app users to enjoy "instant checkout" from within ChatGPT, he later clarified that details on monetization will only be available "soon."&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2120934 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="900" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/appgpt2.png" width="2523" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A full list of third-party apps that will be integrated into ChatGPT in the coming weeks.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          OpenAI

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;In addition to the apps mentioned above, others like Expedia and Booking.com will be available in ChatGPT starting today. Apps from other launch partners including Peloton, Target, Uber, and Doordash will be available inside ChatGPT "in the weeks ahead."&lt;/p&gt;
&lt;p&gt;Other developers can start building with the SDK today before submitting them to OpenAI for review and publication within ChatGPT "later this year." Altman said that apps that meet a certain set of "developer guidelines" will be listed in a comprehensive directory, while those meeting "higher standards for design and functionality will be featured more prominently."&lt;/p&gt;
&lt;h2&gt;AgentKit and API updates&lt;/h2&gt;
&lt;p&gt;Elsewhere in the keynote, Altman announced AgentKit, a new tool designed to let OpenAI users create specialized interactive chatbots using a simplified building block GUI interface. The new software includes integrated tools for measuring performance and testing workflows from within the ChatKit interface.&lt;/p&gt;
&lt;p&gt;In a live demo, OpenAI platform experience specialist Christina Huang gave herself an eight-minute deadline to use AgentKit to create a live, customized question-answering "Ask Froge" chatbot for the Dev Day website. While that demo was done with time to spare, Huang did make use of a lot of pre-built "widgets" and documents full of prepopulated information about the event to streamline the chatbot's creation.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI's Dev Days keynote in full.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The keynote also announced minor updates for OpenAI's codex coding agent, including integration with Slack and a new SDK to allow for easier integration into existing coding workflows. Altman also announced some recent models would be newly available to users via API, including Sora 2, GPT5-Pro, and a new smaller, cheaper version of the company's real-time audio interface.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #8c9eff; background-color: #1a237e;"&gt;&lt;span class="ars-avatar-letter"&gt;t&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              timby
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            So ... this is OpenAI's attempt at creating, would you say, an "everything" app?&lt;p&gt;Man, if we thought Elon hated Sam Altman &lt;i&gt;already&lt;/i&gt;, we haven't seen &lt;i&gt;anything&lt;/i&gt; yet.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2025-10-06T19:29:18+00:00"&gt;October 6, 2025 at 7:29 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/openai-wants-to-make-chatgpt-into-a-universal-app-frontend/</guid><pubDate>Mon, 06 Oct 2025 19:10:44 +0000</pubDate></item><item><title>[NEW] Taylor Swift fans accuse singer of using AI in her Google scavenger hunt videos (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/taylor-swift-fans-accuse-singer-of-using-ai-in-her-google-scavenger-hunt-videos/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/taylor-swift-2024.jpg?resize=1200,783" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For the release of her twelfth album, “The Life of a Showgirl,” Taylor Swift sent fans on an online scavenger hunt this weekend, which began by searching for “Taylor Swift” on Google. But as fans unveiled secret videos as part of the campaign, some fretted that the clips looked like they were AI-generated — and they were not pleased.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Google search for the singer’s name yields a cryptic message: “12 cities, 12 doors, 1 video to unlock.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fans had to figure out the location of the doors, then physically find them and scan a QR code, which surfaced 12 unique videos that contained the clues needed to solve the puzzle. When fans Googled the correct phrase, another orange door appeared, which fans had to collectively “knock” on by clicking 12 million times. Finally, the door “opened,” revealing a lyric video for “The Fate of Ophelia,” which has its own orange door progress bar on YouTube.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube had scored the video exclusive for the track, as well as the lyric videos from the remaining songs on the new album.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google initially announced the scavenger hunt in a video on Instagram. The video begins with an aerial view of Earth, then quickly zooms in on a hilly, bejeweled landscape, until we see an orange door, overlayed with a Google search bar.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-instagram wp-block-embed-instagram"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While Swifties love a puzzle, some were rubbed the wrong way by the 12 clue-containing videos, which looked to be AI-generated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of searching for clues to unveil Swift’s new lyric video, as Swift intended, some fans began to scour the video clips like detectives, looking for signs that the scenes were synthetic. However, while there are clips that look computer-generated, it’s unclear if they were made using AI, and if so, to what extent. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It would make sense if these videos were generated using Google’s AI products. As OpenAI shows off its new Sora 2 video generator, this Taylor Swift collaboration would be a serendipitous opportunity for Google to show millions of Swifties what its Veo 3 model can do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google did not respond to TechCrunch’s request for comment on how these videos were generated or if Swift and Google worked together on this activation by using Google’s own AI technology. But Swift’s team and Google have teamed up for similar promotional activities in the past, we should note. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The use of AI in creative works is a sensitive subject. Some artists think these tools can help them,  while others have protested the manner in which large language models are trained on their work without consent, effectively using artists’ own work to create the technology that could threaten their livelihoods.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even Swift herself spoke out about the dangers of AI after President Donald Trump shared an AI-generated image of her showing support for his campaign last year; the incident spurred her to post an endorsement for former Vice President Kamala Harris, who ran against Trump in 2024.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-reddit wp-block-embed-reddit"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Recently I was made aware that AI of ‘me’ falsely endorsing Donald Trump’s presidential run was posted to his site. It really conjured up my fears around AI, and the dangers of spreading misinformation. It brought me to the conclusion that I need to be very transparent about my actual plans for this election as a voter,” she wrote on Instagram at the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The controversy around Swift’s possible use of AI is amplified given her own stature in the music industry. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI may appeal to some artists as a way to cut costs, the billionaire musician has every possible resource at her disposal to bring the fantastical scenes from her promotional videos to life.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/taylor-swift-2024.jpg?resize=1200,783" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For the release of her twelfth album, “The Life of a Showgirl,” Taylor Swift sent fans on an online scavenger hunt this weekend, which began by searching for “Taylor Swift” on Google. But as fans unveiled secret videos as part of the campaign, some fretted that the clips looked like they were AI-generated — and they were not pleased.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Google search for the singer’s name yields a cryptic message: “12 cities, 12 doors, 1 video to unlock.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Fans had to figure out the location of the doors, then physically find them and scan a QR code, which surfaced 12 unique videos that contained the clues needed to solve the puzzle. When fans Googled the correct phrase, another orange door appeared, which fans had to collectively “knock” on by clicking 12 million times. Finally, the door “opened,” revealing a lyric video for “The Fate of Ophelia,” which has its own orange door progress bar on YouTube.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;YouTube had scored the video exclusive for the track, as well as the lyric videos from the remaining songs on the new album.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google initially announced the scavenger hunt in a video on Instagram. The video begins with an aerial view of Earth, then quickly zooms in on a hilly, bejeweled landscape, until we see an orange door, overlayed with a Google search bar.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-instagram wp-block-embed-instagram"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While Swifties love a puzzle, some were rubbed the wrong way by the 12 clue-containing videos, which looked to be AI-generated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead of searching for clues to unveil Swift’s new lyric video, as Swift intended, some fans began to scour the video clips like detectives, looking for signs that the scenes were synthetic. However, while there are clips that look computer-generated, it’s unclear if they were made using AI, and if so, to what extent. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It would make sense if these videos were generated using Google’s AI products. As OpenAI shows off its new Sora 2 video generator, this Taylor Swift collaboration would be a serendipitous opportunity for Google to show millions of Swifties what its Veo 3 model can do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google did not respond to TechCrunch’s request for comment on how these videos were generated or if Swift and Google worked together on this activation by using Google’s own AI technology. But Swift’s team and Google have teamed up for similar promotional activities in the past, we should note. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The use of AI in creative works is a sensitive subject. Some artists think these tools can help them,  while others have protested the manner in which large language models are trained on their work without consent, effectively using artists’ own work to create the technology that could threaten their livelihoods.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even Swift herself spoke out about the dangers of AI after President Donald Trump shared an AI-generated image of her showing support for his campaign last year; the incident spurred her to post an endorsement for former Vice President Kamala Harris, who ran against Trump in 2024.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-reddit wp-block-embed-reddit"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Recently I was made aware that AI of ‘me’ falsely endorsing Donald Trump’s presidential run was posted to his site. It really conjured up my fears around AI, and the dangers of spreading misinformation. It brought me to the conclusion that I need to be very transparent about my actual plans for this election as a voter,” she wrote on Instagram at the time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The controversy around Swift’s possible use of AI is amplified given her own stature in the music industry. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI may appeal to some artists as a way to cut costs, the billionaire musician has every possible resource at her disposal to bring the fantastical scenes from her promotional videos to life.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/taylor-swift-fans-accuse-singer-of-using-ai-in-her-google-scavenger-hunt-videos/</guid><pubDate>Mon, 06 Oct 2025 20:50:16 +0000</pubDate></item><item><title>[NEW] Deloitte goes all in on AI — despite having to issue a hefty refund for use of AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/06/deloitte-goes-all-in-on-ai-despite-having-to-issue-a-hefty-refund-for-use-of-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-2169550517.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;br /&gt;Professional services and consultant firm Deloitte&amp;nbsp;announced a landmark AI&amp;nbsp;enterprise&amp;nbsp;deal with Anthropic&amp;nbsp;the same day it was revealed the company would issue a refund for a government-contracted report that contained inaccurate&amp;nbsp;AI-produced&amp;nbsp;slop.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: Deloitte’s deal with Anthropic is a referendum on its commitment to AI, even as it grapples with the technology. And Deloitte is not alone in this challenge. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The timing of this announcement is interesting —&amp;nbsp;comical even. On the same day Deloitte touted&amp;nbsp;its increased use of AI,&amp;nbsp;the Australia Department of Employment and Workplace Relations said the&amp;nbsp;consulting company would have to issue a refund&amp;nbsp;for a report it did for the department that included AI hallucinations, the Financial Times reported.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The department had commissioned a A$439,000 “independent assurance review” from Deloitte, which was published earlier this year. The Australian Financial Review reported in August the review had a number of errors, including multiple citations to non-existent academic reports. A corrected version of the review was uploaded to the department’s website last week. Deloitte will repay the final installment of its government contract, the FT reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Deloitte for comment and will update the article if the company responds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deloitte announced&amp;nbsp;Monday plans roll out&amp;nbsp;Anthropic’s&amp;nbsp;chatbot Claude to&amp;nbsp;its&amp;nbsp;nearly 500,000&amp;nbsp;global employees on Monday. Deloitte&amp;nbsp;and Anthropic, which formed a partnership last year,&amp;nbsp;plan to create compliance&amp;nbsp;products and&amp;nbsp;features for regulated industries&amp;nbsp;including&amp;nbsp;financial services,&amp;nbsp;healthcare&amp;nbsp;and public services, according to an&amp;nbsp;Anthropic blog post.&amp;nbsp;Deloitte&amp;nbsp;also&amp;nbsp;plans to create different AI agent “personas” to&amp;nbsp;represent&amp;nbsp;the different departments within the company including accountants and software developers,&amp;nbsp;according to reporting from CNBC.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Deloitte is making this significant investment in&amp;nbsp;Anthropic’s&amp;nbsp;AI platform because our approach to responsible AI is very aligned, and together we can reshape how enterprises operate over the next decade. Claude continues to be a leading choice for many clients and our own AI transformation,” Ranjit Bawa,&amp;nbsp;global&amp;nbsp;technology and&amp;nbsp;ecosystems&amp;nbsp;and&amp;nbsp;alliances&amp;nbsp;leader,&amp;nbsp;at Deloitte wrote in the blog post.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The financial terms of the deal&amp;nbsp;—&amp;nbsp;which Anthropic referred to as an alliance&amp;nbsp;—&amp;nbsp;were not&amp;nbsp;disclosed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is not only Anthropic’s&amp;nbsp;largest enterprise&amp;nbsp;deployment&amp;nbsp;yet, it also illustrates how AI is embedding itself in every aspect of modern life from tools used at work to casual queries made at home. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deloitte is not the only company, or individual,&amp;nbsp;getting caught using inaccurate AI-produced information in recent months either.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In May, the Chicago Sun-Times newspaper&amp;nbsp;had to admit that it ran an AI-generated list of books&amp;nbsp;for its annual summer reading list after readers discovered some of the book&amp;nbsp;titles were hallucinated even if the authors were real.&amp;nbsp;An internal document viewed by Business Insider showed Amazon’s AI productivity tool, Q Business, struggled with accuracy in its first year. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic&amp;nbsp;itself&amp;nbsp;has&amp;nbsp;also&amp;nbsp;been knocked for&amp;nbsp;using&amp;nbsp;AI-hallucinated information&amp;nbsp;from its own chatbot&amp;nbsp;Claude.&amp;nbsp;The AI research lab’s lawyer&amp;nbsp;apologized&amp;nbsp;after&amp;nbsp;the company used an AI-generated citation&amp;nbsp;in a legal dispute with&amp;nbsp;music publishers&amp;nbsp;earlier this year.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-2169550517.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;br /&gt;Professional services and consultant firm Deloitte&amp;nbsp;announced a landmark AI&amp;nbsp;enterprise&amp;nbsp;deal with Anthropic&amp;nbsp;the same day it was revealed the company would issue a refund for a government-contracted report that contained inaccurate&amp;nbsp;AI-produced&amp;nbsp;slop.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The upshot: Deloitte’s deal with Anthropic is a referendum on its commitment to AI, even as it grapples with the technology. And Deloitte is not alone in this challenge. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The timing of this announcement is interesting —&amp;nbsp;comical even. On the same day Deloitte touted&amp;nbsp;its increased use of AI,&amp;nbsp;the Australia Department of Employment and Workplace Relations said the&amp;nbsp;consulting company would have to issue a refund&amp;nbsp;for a report it did for the department that included AI hallucinations, the Financial Times reported.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The department had commissioned a A$439,000 “independent assurance review” from Deloitte, which was published earlier this year. The Australian Financial Review reported in August the review had a number of errors, including multiple citations to non-existent academic reports. A corrected version of the review was uploaded to the department’s website last week. Deloitte will repay the final installment of its government contract, the FT reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Deloitte for comment and will update the article if the company responds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deloitte announced&amp;nbsp;Monday plans roll out&amp;nbsp;Anthropic’s&amp;nbsp;chatbot Claude to&amp;nbsp;its&amp;nbsp;nearly 500,000&amp;nbsp;global employees on Monday. Deloitte&amp;nbsp;and Anthropic, which formed a partnership last year,&amp;nbsp;plan to create compliance&amp;nbsp;products and&amp;nbsp;features for regulated industries&amp;nbsp;including&amp;nbsp;financial services,&amp;nbsp;healthcare&amp;nbsp;and public services, according to an&amp;nbsp;Anthropic blog post.&amp;nbsp;Deloitte&amp;nbsp;also&amp;nbsp;plans to create different AI agent “personas” to&amp;nbsp;represent&amp;nbsp;the different departments within the company including accountants and software developers,&amp;nbsp;according to reporting from CNBC.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Deloitte is making this significant investment in&amp;nbsp;Anthropic’s&amp;nbsp;AI platform because our approach to responsible AI is very aligned, and together we can reshape how enterprises operate over the next decade. Claude continues to be a leading choice for many clients and our own AI transformation,” Ranjit Bawa,&amp;nbsp;global&amp;nbsp;technology and&amp;nbsp;ecosystems&amp;nbsp;and&amp;nbsp;alliances&amp;nbsp;leader,&amp;nbsp;at Deloitte wrote in the blog post.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The financial terms of the deal&amp;nbsp;—&amp;nbsp;which Anthropic referred to as an alliance&amp;nbsp;—&amp;nbsp;were not&amp;nbsp;disclosed.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal is not only Anthropic’s&amp;nbsp;largest enterprise&amp;nbsp;deployment&amp;nbsp;yet, it also illustrates how AI is embedding itself in every aspect of modern life from tools used at work to casual queries made at home. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Deloitte is not the only company, or individual,&amp;nbsp;getting caught using inaccurate AI-produced information in recent months either.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In May, the Chicago Sun-Times newspaper&amp;nbsp;had to admit that it ran an AI-generated list of books&amp;nbsp;for its annual summer reading list after readers discovered some of the book&amp;nbsp;titles were hallucinated even if the authors were real.&amp;nbsp;An internal document viewed by Business Insider showed Amazon’s AI productivity tool, Q Business, struggled with accuracy in its first year. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic&amp;nbsp;itself&amp;nbsp;has&amp;nbsp;also&amp;nbsp;been knocked for&amp;nbsp;using&amp;nbsp;AI-hallucinated information&amp;nbsp;from its own chatbot&amp;nbsp;Claude.&amp;nbsp;The AI research lab’s lawyer&amp;nbsp;apologized&amp;nbsp;after&amp;nbsp;the company used an AI-generated citation&amp;nbsp;in a legal dispute with&amp;nbsp;music publishers&amp;nbsp;earlier this year.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/06/deloitte-goes-all-in-on-ai-despite-having-to-issue-a-hefty-refund-for-use-of-ai/</guid><pubDate>Mon, 06 Oct 2025 22:29:47 +0000</pubDate></item><item><title>[NEW] OpenAI unveils AgentKit that lets developers drag and drop to build AI agents (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openai-unveils-agentkit-that-lets-developers-drag-and-drop-to-build-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt; launched an agent builder that the company hopes will eliminate fragmented tools and make it easier for enterprises to utilize OpenAI’s system to create agents. &lt;/p&gt;&lt;p&gt;&lt;a href="https://openai.com/index/introducing-agentkit/"&gt;AgentKit&lt;/a&gt;, announced during OpenAI’s DevDay in San Francisco, enables developers and enterprises to build agents and add chat capabilities in one place, potentially competing with platforms like &lt;a href="https://zapier.com/"&gt;Zapier&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;By offering a more streamlined way to create agents, OpenAI advances further into becoming a full-stack application provider.&lt;/p&gt;&lt;p&gt;“Until now, building agents meant juggling fragmented tools—complex orchestration with no versioning, custom connectors, manual eval pipelines, prompt tuning, and weeks of frontend work before launch,” the company said in a blog post. &lt;/p&gt;&lt;p&gt;AgentKit includes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Agent Builder, which is a visual canvas where devs can see what they’ve created and versioning multi-agent workflows&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Connector Registry is a central area for admins to manage connections across OpenAI products. A Global Admin console will be a prerequisite to using this feature.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;ChatKit enables users to integrate chat-based agents into their user interfaces. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Eventually, OpenAI said it will build a standalone Workflows API and add agent deployment tabs to ChatGPT. &lt;/p&gt;&lt;p&gt;OpenAI also expanded evaluation for agents, adding capabilities such as datasets with automated graders and annotations, trace grading that runs end-to-end assessments of workflows, automated prompt optimization, and support for third-party agent measurement tools. &lt;/p&gt;&lt;p&gt;Developers can access some features of AgentKit, but OpenAI is gradually rolling out additional features, such as Agent Builder. Currently, Agent Builder is available in beta, while ChatKit and new evaluation capabilities are generally available. Connector Registry “is beginning its beta rollout to some API and ChatGPT Enterprise and Edu users. &lt;/p&gt;&lt;p&gt;OpenAI said pricing for AgentKit tools will be included in the standard API model pricing. &lt;/p&gt;&lt;h2&gt;Agent Builder&lt;/h2&gt;&lt;p&gt;To clarify, many agents are built using OpenAI’s models; however, enterprises often access GPT-5 through other platforms to create their own agents. However, AgentKit brings enterprises more into its ecosystem, ensuring they don’t need to tap other platforms as often. &lt;/p&gt;&lt;p&gt;Demonstrated during DevDay, the company stated that Agent Builder is ideal for rapid iteration. It also provides developers with visibility into how the agents are working. &lt;/p&gt;&lt;p&gt;During the demo, an OpenAI developer made an agent that reads the DevDay agenda and suggests panels to watch. It took her just under eight minutes. &lt;/p&gt;&lt;p&gt;Other model providers saw the importance of offering developer toolkits to build agents to entice enterprises to use more of their tools. &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt; came out with its &lt;a href="https://venturebeat.com/ai/googles-new-agent-development-kit-lets-enterprises-rapidly-prototype-and-deploy-ai-agents-without-recoding"&gt;Agent Development Kit in April&lt;/a&gt;, expanding multi-agent system building “in under 100 lines of code.” &lt;a href="https://www.microsoft.com/"&gt;Microsoft&lt;/a&gt;, which runs the popular agent framework AutoGen, announced it is bringing agent creation to one place with its new &lt;a href="https://venturebeat.com/ai/microsoft-retires-autogen-and-debuts-agent-framework-to-unify-and-govern"&gt;Agent Framework&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;OpenAI customers, such as the fintech company Ramp, stated in a blog post that its teams were able to build a procurement agent in a few hours instead of months. &lt;/p&gt;&lt;p&gt;“Agent Builder transformed what once took months of complex orchestration, custom code, and manual optimizations into just a couple of hours. The visual canvas keeps product, legal, and engineering on the same page, slashing iteration cycles by 70% and getting an agent live in two sprints rather than two quarters,” Ramp said. &lt;/p&gt;&lt;p&gt;AgentKit’s Connector Registry would also enable enterprises to manage and maintain data across workspaces, consolidating data sources into a single panel that spans both ChatGPT and the API. It will have pre-built connectors to Dropbox, Google Drive, SharePoint and Microsoft Teams. It also supports third-party MCP servers. &lt;/p&gt;&lt;p&gt;Another capability of Agent Builder is Guardrails, an open-source safety layer that protects against the leakage of personally identifiable information (PII), jailbreaks, and unintended or malicious behavior.&lt;/p&gt;&lt;h2&gt;Bringing more chat &lt;/h2&gt;&lt;p&gt;Since most agentic interactions involve chat, it makes sense to simplify the process for developers to set up chat interfaces and connect them with the agents they’ve just built. &lt;/p&gt;&lt;p&gt;“Deploying chat UIs for agents can be surprisingly complex—handling streaming responses, managing threads, showing the model thinking and designing engaging in-chat experiences,” OpenAI said. &lt;/p&gt;&lt;p&gt;The company said ChatKit makes it simple to embed chat agents on platforms and embed these into apps or websites. &lt;/p&gt;&lt;p&gt;However, some OpenAI competitors have begun thinking beyond the chatbot and want to offer agentic interactions that feel more seamless. Google’s asynchronous coding agent, Jules, has &lt;a href="https://venturebeat.com/ai/googles-jules-coding-agent-moves-beyond-chat-with-new-command-line-and-api"&gt;introduced a new feature&lt;/a&gt; that enables users to interact with the agent through the command-line interface, eliminating the need to open a chat window. &lt;/p&gt;&lt;h2&gt;Responses &lt;/h2&gt;&lt;p&gt;The response to AgentKit has mainly been positive, with some developers noting that while it simplifies agent building, it doesn’t mean that everyone can now build agents. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Several developers view Agent Kit not as a Zapier killer, but rather as a tool that complements the pipeline. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Zapier debuted a no-code tool for building AI agents and bots,&lt;a href="https://venturebeat.com/ai/zapier-central-debuts-as-no-code-tool-for-building-enterprise-ai-bots"&gt; called Zapier Central,&lt;/a&gt; in 2024. &lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://openai.com/"&gt;OpenAI&lt;/a&gt; launched an agent builder that the company hopes will eliminate fragmented tools and make it easier for enterprises to utilize OpenAI’s system to create agents. &lt;/p&gt;&lt;p&gt;&lt;a href="https://openai.com/index/introducing-agentkit/"&gt;AgentKit&lt;/a&gt;, announced during OpenAI’s DevDay in San Francisco, enables developers and enterprises to build agents and add chat capabilities in one place, potentially competing with platforms like &lt;a href="https://zapier.com/"&gt;Zapier&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;By offering a more streamlined way to create agents, OpenAI advances further into becoming a full-stack application provider.&lt;/p&gt;&lt;p&gt;“Until now, building agents meant juggling fragmented tools—complex orchestration with no versioning, custom connectors, manual eval pipelines, prompt tuning, and weeks of frontend work before launch,” the company said in a blog post. &lt;/p&gt;&lt;p&gt;AgentKit includes:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Agent Builder, which is a visual canvas where devs can see what they’ve created and versioning multi-agent workflows&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Connector Registry is a central area for admins to manage connections across OpenAI products. A Global Admin console will be a prerequisite to using this feature.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;ChatKit enables users to integrate chat-based agents into their user interfaces. &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Eventually, OpenAI said it will build a standalone Workflows API and add agent deployment tabs to ChatGPT. &lt;/p&gt;&lt;p&gt;OpenAI also expanded evaluation for agents, adding capabilities such as datasets with automated graders and annotations, trace grading that runs end-to-end assessments of workflows, automated prompt optimization, and support for third-party agent measurement tools. &lt;/p&gt;&lt;p&gt;Developers can access some features of AgentKit, but OpenAI is gradually rolling out additional features, such as Agent Builder. Currently, Agent Builder is available in beta, while ChatKit and new evaluation capabilities are generally available. Connector Registry “is beginning its beta rollout to some API and ChatGPT Enterprise and Edu users. &lt;/p&gt;&lt;p&gt;OpenAI said pricing for AgentKit tools will be included in the standard API model pricing. &lt;/p&gt;&lt;h2&gt;Agent Builder&lt;/h2&gt;&lt;p&gt;To clarify, many agents are built using OpenAI’s models; however, enterprises often access GPT-5 through other platforms to create their own agents. However, AgentKit brings enterprises more into its ecosystem, ensuring they don’t need to tap other platforms as often. &lt;/p&gt;&lt;p&gt;Demonstrated during DevDay, the company stated that Agent Builder is ideal for rapid iteration. It also provides developers with visibility into how the agents are working. &lt;/p&gt;&lt;p&gt;During the demo, an OpenAI developer made an agent that reads the DevDay agenda and suggests panels to watch. It took her just under eight minutes. &lt;/p&gt;&lt;p&gt;Other model providers saw the importance of offering developer toolkits to build agents to entice enterprises to use more of their tools. &lt;a href="https://www.google.com/"&gt;Google&lt;/a&gt; came out with its &lt;a href="https://venturebeat.com/ai/googles-new-agent-development-kit-lets-enterprises-rapidly-prototype-and-deploy-ai-agents-without-recoding"&gt;Agent Development Kit in April&lt;/a&gt;, expanding multi-agent system building “in under 100 lines of code.” &lt;a href="https://www.microsoft.com/"&gt;Microsoft&lt;/a&gt;, which runs the popular agent framework AutoGen, announced it is bringing agent creation to one place with its new &lt;a href="https://venturebeat.com/ai/microsoft-retires-autogen-and-debuts-agent-framework-to-unify-and-govern"&gt;Agent Framework&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;OpenAI customers, such as the fintech company Ramp, stated in a blog post that its teams were able to build a procurement agent in a few hours instead of months. &lt;/p&gt;&lt;p&gt;“Agent Builder transformed what once took months of complex orchestration, custom code, and manual optimizations into just a couple of hours. The visual canvas keeps product, legal, and engineering on the same page, slashing iteration cycles by 70% and getting an agent live in two sprints rather than two quarters,” Ramp said. &lt;/p&gt;&lt;p&gt;AgentKit’s Connector Registry would also enable enterprises to manage and maintain data across workspaces, consolidating data sources into a single panel that spans both ChatGPT and the API. It will have pre-built connectors to Dropbox, Google Drive, SharePoint and Microsoft Teams. It also supports third-party MCP servers. &lt;/p&gt;&lt;p&gt;Another capability of Agent Builder is Guardrails, an open-source safety layer that protects against the leakage of personally identifiable information (PII), jailbreaks, and unintended or malicious behavior.&lt;/p&gt;&lt;h2&gt;Bringing more chat &lt;/h2&gt;&lt;p&gt;Since most agentic interactions involve chat, it makes sense to simplify the process for developers to set up chat interfaces and connect them with the agents they’ve just built. &lt;/p&gt;&lt;p&gt;“Deploying chat UIs for agents can be surprisingly complex—handling streaming responses, managing threads, showing the model thinking and designing engaging in-chat experiences,” OpenAI said. &lt;/p&gt;&lt;p&gt;The company said ChatKit makes it simple to embed chat agents on platforms and embed these into apps or websites. &lt;/p&gt;&lt;p&gt;However, some OpenAI competitors have begun thinking beyond the chatbot and want to offer agentic interactions that feel more seamless. Google’s asynchronous coding agent, Jules, has &lt;a href="https://venturebeat.com/ai/googles-jules-coding-agent-moves-beyond-chat-with-new-command-line-and-api"&gt;introduced a new feature&lt;/a&gt; that enables users to interact with the agent through the command-line interface, eliminating the need to open a chat window. &lt;/p&gt;&lt;h2&gt;Responses &lt;/h2&gt;&lt;p&gt;The response to AgentKit has mainly been positive, with some developers noting that while it simplifies agent building, it doesn’t mean that everyone can now build agents. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Several developers view Agent Kit not as a Zapier killer, but rather as a tool that complements the pipeline. &lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Zapier debuted a no-code tool for building AI agents and bots,&lt;a href="https://venturebeat.com/ai/zapier-central-debuts-as-no-code-tool-for-building-enterprise-ai-bots"&gt; called Zapier Central,&lt;/a&gt; in 2024. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-unveils-agentkit-that-lets-developers-drag-and-drop-to-build-ai</guid><pubDate>Mon, 06 Oct 2025 22:53:00 +0000</pubDate></item></channel></rss>