<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 19 Feb 2026 02:28:44 +0000</lastBuildDate><item><title>OpenAI pushes into higher education as India seeks to scale AI skills (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/openai-pushes-into-higher-education-as-india-seeks-to-scale-ai-skills/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/openai-india.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is expanding its footprint in India and moving into the country’s higher-education system through partnerships with leading academic institutions. The move comes as the South Asian nation seeks to scale AI skills and build domestic capacity in one of the world’s largest talent markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, OpenAI said it was partnering with six public and private higher-education institutions in India, including top engineering, management, medical, and design-focused institutes, with the aim of reaching more than 100,000 students, faculty, and staff over the next year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rather than focusing on consumer use, the initiative centers on integrating AI into core academic functions, signaling OpenAI’s interest in influencing how AI is taught, governed, and normalized within one of the world’s largest higher-education systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has already built a large consumer audience for its ChatGPT chatbot, which has over 100 million monthly active users in India, according to CEO Sam Altman, and India has emerged as the company’s second-largest user base after the U.S. The announcement also coincides with a broader push by leading AI firms to deepen their presence in India, which is hosting an AI Impact Summit in New Delhi this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first cohort of partners includes some of India’s most influential academic institutions, such as the Indian Institute of Technology Delhi, the Indian Institute of Management Ahmedabad, and the All India Institute of Medical Sciences New Delhi, alongside private universities and specialized design schools. The ChatGPT maker said the partnerships would span disciplines ranging from engineering and management to healthcare and creative fields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India has already emerged as a key testing ground for AI use in education. Last month, Google said India accounts for the highest global usage of its Gemini tools for learning. Microsoft, similarly, said this week it would expand its Elevate skilling program in India to train teachers across schools, vocational institutes, and higher-education settings, working with government agencies as part of a broader push to build AI skills at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said the partnerships would involve campus-wide access to its ChatGPT Edu tools, faculty training, and responsible-use frameworks. The focus, the company said, is on embedding AI into core academic workflows such as coding, research, analytics, and case analysis, rather than offering standalone access to tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Two of the partner institutions, the Indian Institute of Management Ahmedabad and Manipal Academy of Higher Education, will also introduce OpenAI-backed certifications. Additionally, OpenAI said it would work with Indian ed-tech platforms, including Physics Wallah, upGrad, and HCL GUVI, to extend AI training beyond campuses. These platforms will launch structured courses on AI fundamentals and ChatGPT use cases, aimed at students and early-career professionals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Raghav Gupta, head of education at OpenAI India, said educational institutions were a “critical route” to closing the gap between rapidly advancing AI tools and how people are actually using them, as skills demands shift across the economy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, OpenAI hired Gupta, a former Coursera Asia-Pacific managing director, as its India and Asia-Pacific head of education, alongside the launch of a Learning Accelerator program focused on expanding AI skills.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The flurry of moves into education underscores how AI companies are increasingly looking beyond consumer tools and corporate clients toward institutions that shape skills, norms, and long-term adoption. For countries like India, the contest is not just around access to AI, but also about who helps define how it is taught, governed, and embedded at scale.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/openai-india.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is expanding its footprint in India and moving into the country’s higher-education system through partnerships with leading academic institutions. The move comes as the South Asian nation seeks to scale AI skills and build domestic capacity in one of the world’s largest talent markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, OpenAI said it was partnering with six public and private higher-education institutions in India, including top engineering, management, medical, and design-focused institutes, with the aim of reaching more than 100,000 students, faculty, and staff over the next year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Rather than focusing on consumer use, the initiative centers on integrating AI into core academic functions, signaling OpenAI’s interest in influencing how AI is taught, governed, and normalized within one of the world’s largest higher-education systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has already built a large consumer audience for its ChatGPT chatbot, which has over 100 million monthly active users in India, according to CEO Sam Altman, and India has emerged as the company’s second-largest user base after the U.S. The announcement also coincides with a broader push by leading AI firms to deepen their presence in India, which is hosting an AI Impact Summit in New Delhi this week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first cohort of partners includes some of India’s most influential academic institutions, such as the Indian Institute of Technology Delhi, the Indian Institute of Management Ahmedabad, and the All India Institute of Medical Sciences New Delhi, alongside private universities and specialized design schools. The ChatGPT maker said the partnerships would span disciplines ranging from engineering and management to healthcare and creative fields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India has already emerged as a key testing ground for AI use in education. Last month, Google said India accounts for the highest global usage of its Gemini tools for learning. Microsoft, similarly, said this week it would expand its Elevate skilling program in India to train teachers across schools, vocational institutes, and higher-education settings, working with government agencies as part of a broader push to build AI skills at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI said the partnerships would involve campus-wide access to its ChatGPT Edu tools, faculty training, and responsible-use frameworks. The focus, the company said, is on embedding AI into core academic workflows such as coding, research, analytics, and case analysis, rather than offering standalone access to tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Two of the partner institutions, the Indian Institute of Management Ahmedabad and Manipal Academy of Higher Education, will also introduce OpenAI-backed certifications. Additionally, OpenAI said it would work with Indian ed-tech platforms, including Physics Wallah, upGrad, and HCL GUVI, to extend AI training beyond campuses. These platforms will launch structured courses on AI fundamentals and ChatGPT use cases, aimed at students and early-career professionals.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Raghav Gupta, head of education at OpenAI India, said educational institutions were a “critical route” to closing the gap between rapidly advancing AI tools and how people are actually using them, as skills demands shift across the economy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, OpenAI hired Gupta, a former Coursera Asia-Pacific managing director, as its India and Asia-Pacific head of education, alongside the launch of a Learning Accelerator program focused on expanding AI skills.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The flurry of moves into education underscores how AI companies are increasingly looking beyond consumer tools and corporate clients toward institutions that shape skills, norms, and long-term adoption. For countries like India, the contest is not just around access to AI, but also about who helps define how it is taught, governed, and embedded at scale.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/openai-pushes-into-higher-education-as-india-seeks-to-scale-ai-skills/</guid><pubDate>Wed, 18 Feb 2026 14:32:42 +0000</pubDate></item><item><title>Microsoft says Office bug exposed customers’ confidential emails to Copilot AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/microsoft-says-office-bug-exposed-customers-confidential-emails-to-copilot-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-2041281128-e1712563728365.jpg?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft has confirmed that a bug allowed its Copilot AI to summarize customers’ confidential emails for weeks without permission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bug, first reported by Bleeping Computer, allowed Copilot Chat to read and outline the contents of emails since January, even if customers had data loss prevention policies to prevent ingesting their sensitive information into Microsoft’s large language model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Copilot Chat allows paying Microsoft 365 customers to use the AI-powered chat feature in its Office software products, including Word, Excel, and PowerPoint.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft said the bug, trackable by admins as CW1226324, means that draft and sent email messages “with a confidential label applied are being incorrectly processed by Microsoft 365 Copilot chat.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant said it began rolling out a fix for the bug earlier in February. A spokesperson for Microsoft did not respond to a request for comment, including a question about how many customers are affected by the bug.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, the European Parliament’s IT department told lawmakers that it blocked the built-in AI features on their work-issued devices, citing concerns that the AI tools could upload potentially confidential correspondence to the cloud.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-2041281128-e1712563728365.jpg?resize=1200,676" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft has confirmed that a bug allowed its Copilot AI to summarize customers’ confidential emails for weeks without permission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The bug, first reported by Bleeping Computer, allowed Copilot Chat to read and outline the contents of emails since January, even if customers had data loss prevention policies to prevent ingesting their sensitive information into Microsoft’s large language model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Copilot Chat allows paying Microsoft 365 customers to use the AI-powered chat feature in its Office software products, including Word, Excel, and PowerPoint.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft said the bug, trackable by admins as CW1226324, means that draft and sent email messages “with a confidential label applied are being incorrectly processed by Microsoft 365 Copilot chat.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant said it began rolling out a fix for the bug earlier in February. A spokesperson for Microsoft did not respond to a request for comment, including a question about how many customers are affected by the bug.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, the European Parliament’s IT department told lawmakers that it blocked the built-in AI features on their work-issued devices, citing concerns that the AI tools could upload potentially confidential correspondence to the cloud.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/microsoft-says-office-bug-exposed-customers-confidential-emails-to-copilot-ai/</guid><pubDate>Wed, 18 Feb 2026 14:44:28 +0000</pubDate></item><item><title>How financial institutions are embedding AI decision-making (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-financial-institutions-embedding-ai-decision-making/</link><description>&lt;p&gt;For leaders in the financial sector, the experimental phase of generative AI has concluded and the focus for 2026 is operational integration.&lt;/p&gt;&lt;p&gt;While early adoption centred on content generation and efficiency in isolated workflows, the current requirement is to industrialise these capabilities. The objective is to create systems where AI agents do not merely assist human operators, but actively run processes within strict governance frameworks.&lt;/p&gt;&lt;p&gt;This transition presents specific architectural and cultural challenges. It requires a move from disparate tools to joined-up systems that manage data signals, decision logic, and execution layers simultaneously.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-financial-institutions-integrate-agentic-ai-workflows"&gt;Financial institutions integrate agentic AI workflows&lt;/h3&gt;&lt;p&gt;The primary bottleneck in scaling AI within financial services is no longer the availability of models or creative application, it is coordination. Marketing and customer experience teams often struggle to convert decisions into action due to friction between legacy systems, compliance approvals, and data silos.&lt;/p&gt;&lt;p&gt;Saachin Bhatt, Co-Founder and COO at Brdge, notes the distinction between current tools and future requirements: “An assistant helps you write faster. A copilot helps teams move faster. Agents run processes.”&lt;/p&gt;&lt;p&gt;For enterprise architects, this means building what Bhatt terms a ‘Moments Engine’. This operating model functions through five distinct stages:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Signals:&lt;/strong&gt; Detecting real-time events in the customer journey.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Decisions:&lt;/strong&gt; Determining the appropriate algorithmic response.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Message:&lt;/strong&gt; Generating communication aligned with brand parameters.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Routing:&lt;/strong&gt; Automated triage to determine if human approval is required.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Action and learning:&lt;/strong&gt; Deployment and feedback loop integration.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Most organisations possess components of this architecture but lack the integration to make it function as a unified system. The technical goal is to reduce the friction that slows down customer interactions. This involves creating pipelines where data flows seamlessly from signal detection to execution, minimising latency while maintaining security.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-governance-as-infrastructure"&gt;Governance as infrastructure&lt;/h3&gt;&lt;p&gt;In high-stakes environments like banking and insurance, speed cannot come at the cost of control. Trust remains the primary commercial asset. Consequently, governance must be treated as a technical feature rather than a bureaucratic hurdle.&lt;/p&gt;&lt;p&gt;The integration of AI into financial decision-making requires “guardrails” that are hard-coded into the system. This ensures that while AI agents can execute tasks autonomously, they operate within pre-defined risk parameters.&lt;/p&gt;&lt;p&gt;Farhad Divecha, Group CEO at Accuracast, suggests that creative optimisation must become a continuous loop where data-led insights feed innovation. However, this loop requires rigorous quality assurance workflows to ensure output never compromises brand integrity.&lt;/p&gt;&lt;p&gt;For technical teams, this implies a shift in how compliance is handled. Rather than a final check, regulatory requirements must be embedded into the prompt engineering and model fine-tuning stages.&lt;/p&gt;&lt;p&gt;“Legitimate interest is interesting, but it’s also where a lot of companies could trip up,” observes Jonathan Bowyer, former Marketing Director at Lloyds Banking Group. He argues that regulations like Consumer Duty help by forcing an outcome-based approach.&lt;/p&gt;&lt;p&gt;Technical leaders must work with risk teams to ensure AI-driven activity attests to brand values. This includes transparency protocols. Customers should know when they are interacting with an AI, and systems must provide a clear escalation path to human operators.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-architecture-for-restraint"&gt;Data architecture for restraint&lt;/h3&gt;&lt;p&gt;A common failure mode in personalisation engines is over-engagement. The technical capability to message a customer exists, but the logic to determine restraint is often missing. Effective personalisation relies on anticipation (i.e. knowing when to remain silent is as important as knowing when to speak.)&lt;/p&gt;&lt;p&gt;Jonathan Bowyer points out that personalisation has moved to anticipation. “Customers now expect brands to know when not to speak to them as opposed to when to speak to them.”&lt;/p&gt;&lt;p&gt;This requires a data architecture capable of cross-referencing customer context across multiple channels – including branches, apps, and contact centres – in real-time. If a customer is in financial distress, a marketing algorithm pushing a loan product creates a disconnect that erodes trust. The system must be capable of detecting negative signals and suppressing standard promotional workflows.&lt;/p&gt;&lt;p&gt;“The thing that kills trust is when you go to one channel and then move to another and have to answer the same questions all over again,” says Bowyer. Solving this requires unifying data stores so that the “memory” of the institution is accessible to every agent (whether digital or human) at the point of interaction.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-rise-of-generative-search-and-seo"&gt;The rise of generative search and SEO&lt;/h3&gt;&lt;p&gt;In the age of AI, the discovery layer for financial products is changing. Traditional search engine optimisation (SEO) focused on driving traffic to owned properties. The emergence of AI-generated answers means that brand visibility now occurs off-site, within the interface of an LLM or AI search tool.&lt;/p&gt;&lt;p&gt;“Digital PR and off-site SEO is returning to focus because generative AI answers are not confined to content pulled directly from a company’s website,” notes Divecha.&lt;/p&gt;&lt;p&gt;For CIOs and CDOs, this changes how information is structured and published. Technical SEO must evolve to ensure that the data fed into large language models is accurate and compliant.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Organisations that can confidently distribute high-quality information across the wider ecosystem gain reach without sacrificing control. This area, often termed ‘Generative Engine Optimisation’ (GEO), requires a technical strategy to ensure the brand is recommended and cited correctly by third-party AI agents.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-structured-agility"&gt;Structured agility&lt;/h3&gt;&lt;p&gt;There is a misconception that agility equates to a lack of structure. In regulated industries, the opposite is true.&lt;/p&gt;&lt;p&gt;Agile methodologies require strict frameworks to function safely. Ingrid Sierra, Brand and Marketing Director at Zego, explains: “There’s often confusion between agility and chaos. Calling something ‘agile’ doesn’t make it okay for everything to be improvised and unstructured.”&lt;/p&gt;&lt;p&gt;For technical leadership, this means systemising predictable work to create capacity for experimentation. It involves creating safe sandboxes where teams can test new AI agents or data models without risking production stability.&lt;/p&gt;&lt;p&gt;Agility starts with mindset, requiring staff who are willing to experiment. However, this experimentation must be deliberate. It requires collaboration between technical, marketing, and legal teams from the outset.&lt;/p&gt;&lt;p&gt;This “compliance-by-design” approach allows for faster iteration because the parameters of safety are established before the code is written.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-s-next-for-ai-in-the-financial-sector"&gt;What’s next for AI in the financial sector?&lt;/h3&gt;&lt;p&gt;Looking further ahead, the financial ecosystem will likely see direct interaction between AI agents acting on behalf of consumers and agents acting for institutions.&lt;/p&gt;&lt;p&gt;Melanie Lazarus, Ecosystem Engagement Director at Open Banking, warns: “We are entering a world where AI agents interact with each other, and that changes the foundations of consent, authentication, and authorisation.”&lt;/p&gt;&lt;p&gt;Tech leaders must begin architecting frameworks that protect customers in this agent-to-agent reality. This involves new protocols for identity verification and API security to ensure that an automated financial advisor acting for a client can securely interact with a bank’s infrastructure.&lt;/p&gt;&lt;p&gt;The mandate for 2026 is to turn the potential of AI into a reliable P&amp;amp;L driver. This requires a focus on infrastructure over hype and leaders must prioritise:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Unifying data streams:&lt;/strong&gt; Ensure signals from all channels feed into a central decision engine to enable context-aware actions.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Hard-coding governance:&lt;/strong&gt; Embed compliance rules into the AI workflow to allow for safe automation.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Agentic orchestration:&lt;/strong&gt; Move beyond chatbots to agents that can execute end-to-end processes.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Generative optimisation:&lt;/strong&gt; Structure public data to be readable and prioritised by external AI search engines.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Success will depend on how well these technical elements are integrated with human oversight. The winning organisations will be those that use AI automation to enhance, rather than replace, the judgment that is especially required in sectors like financial services.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Goldman Sachs deploys Anthropic systems with success&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;For leaders in the financial sector, the experimental phase of generative AI has concluded and the focus for 2026 is operational integration.&lt;/p&gt;&lt;p&gt;While early adoption centred on content generation and efficiency in isolated workflows, the current requirement is to industrialise these capabilities. The objective is to create systems where AI agents do not merely assist human operators, but actively run processes within strict governance frameworks.&lt;/p&gt;&lt;p&gt;This transition presents specific architectural and cultural challenges. It requires a move from disparate tools to joined-up systems that manage data signals, decision logic, and execution layers simultaneously.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-financial-institutions-integrate-agentic-ai-workflows"&gt;Financial institutions integrate agentic AI workflows&lt;/h3&gt;&lt;p&gt;The primary bottleneck in scaling AI within financial services is no longer the availability of models or creative application, it is coordination. Marketing and customer experience teams often struggle to convert decisions into action due to friction between legacy systems, compliance approvals, and data silos.&lt;/p&gt;&lt;p&gt;Saachin Bhatt, Co-Founder and COO at Brdge, notes the distinction between current tools and future requirements: “An assistant helps you write faster. A copilot helps teams move faster. Agents run processes.”&lt;/p&gt;&lt;p&gt;For enterprise architects, this means building what Bhatt terms a ‘Moments Engine’. This operating model functions through five distinct stages:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Signals:&lt;/strong&gt; Detecting real-time events in the customer journey.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Decisions:&lt;/strong&gt; Determining the appropriate algorithmic response.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Message:&lt;/strong&gt; Generating communication aligned with brand parameters.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Routing:&lt;/strong&gt; Automated triage to determine if human approval is required.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Action and learning:&lt;/strong&gt; Deployment and feedback loop integration.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Most organisations possess components of this architecture but lack the integration to make it function as a unified system. The technical goal is to reduce the friction that slows down customer interactions. This involves creating pipelines where data flows seamlessly from signal detection to execution, minimising latency while maintaining security.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-governance-as-infrastructure"&gt;Governance as infrastructure&lt;/h3&gt;&lt;p&gt;In high-stakes environments like banking and insurance, speed cannot come at the cost of control. Trust remains the primary commercial asset. Consequently, governance must be treated as a technical feature rather than a bureaucratic hurdle.&lt;/p&gt;&lt;p&gt;The integration of AI into financial decision-making requires “guardrails” that are hard-coded into the system. This ensures that while AI agents can execute tasks autonomously, they operate within pre-defined risk parameters.&lt;/p&gt;&lt;p&gt;Farhad Divecha, Group CEO at Accuracast, suggests that creative optimisation must become a continuous loop where data-led insights feed innovation. However, this loop requires rigorous quality assurance workflows to ensure output never compromises brand integrity.&lt;/p&gt;&lt;p&gt;For technical teams, this implies a shift in how compliance is handled. Rather than a final check, regulatory requirements must be embedded into the prompt engineering and model fine-tuning stages.&lt;/p&gt;&lt;p&gt;“Legitimate interest is interesting, but it’s also where a lot of companies could trip up,” observes Jonathan Bowyer, former Marketing Director at Lloyds Banking Group. He argues that regulations like Consumer Duty help by forcing an outcome-based approach.&lt;/p&gt;&lt;p&gt;Technical leaders must work with risk teams to ensure AI-driven activity attests to brand values. This includes transparency protocols. Customers should know when they are interacting with an AI, and systems must provide a clear escalation path to human operators.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-data-architecture-for-restraint"&gt;Data architecture for restraint&lt;/h3&gt;&lt;p&gt;A common failure mode in personalisation engines is over-engagement. The technical capability to message a customer exists, but the logic to determine restraint is often missing. Effective personalisation relies on anticipation (i.e. knowing when to remain silent is as important as knowing when to speak.)&lt;/p&gt;&lt;p&gt;Jonathan Bowyer points out that personalisation has moved to anticipation. “Customers now expect brands to know when not to speak to them as opposed to when to speak to them.”&lt;/p&gt;&lt;p&gt;This requires a data architecture capable of cross-referencing customer context across multiple channels – including branches, apps, and contact centres – in real-time. If a customer is in financial distress, a marketing algorithm pushing a loan product creates a disconnect that erodes trust. The system must be capable of detecting negative signals and suppressing standard promotional workflows.&lt;/p&gt;&lt;p&gt;“The thing that kills trust is when you go to one channel and then move to another and have to answer the same questions all over again,” says Bowyer. Solving this requires unifying data stores so that the “memory” of the institution is accessible to every agent (whether digital or human) at the point of interaction.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-rise-of-generative-search-and-seo"&gt;The rise of generative search and SEO&lt;/h3&gt;&lt;p&gt;In the age of AI, the discovery layer for financial products is changing. Traditional search engine optimisation (SEO) focused on driving traffic to owned properties. The emergence of AI-generated answers means that brand visibility now occurs off-site, within the interface of an LLM or AI search tool.&lt;/p&gt;&lt;p&gt;“Digital PR and off-site SEO is returning to focus because generative AI answers are not confined to content pulled directly from a company’s website,” notes Divecha.&lt;/p&gt;&lt;p&gt;For CIOs and CDOs, this changes how information is structured and published. Technical SEO must evolve to ensure that the data fed into large language models is accurate and compliant.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Organisations that can confidently distribute high-quality information across the wider ecosystem gain reach without sacrificing control. This area, often termed ‘Generative Engine Optimisation’ (GEO), requires a technical strategy to ensure the brand is recommended and cited correctly by third-party AI agents.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-structured-agility"&gt;Structured agility&lt;/h3&gt;&lt;p&gt;There is a misconception that agility equates to a lack of structure. In regulated industries, the opposite is true.&lt;/p&gt;&lt;p&gt;Agile methodologies require strict frameworks to function safely. Ingrid Sierra, Brand and Marketing Director at Zego, explains: “There’s often confusion between agility and chaos. Calling something ‘agile’ doesn’t make it okay for everything to be improvised and unstructured.”&lt;/p&gt;&lt;p&gt;For technical leadership, this means systemising predictable work to create capacity for experimentation. It involves creating safe sandboxes where teams can test new AI agents or data models without risking production stability.&lt;/p&gt;&lt;p&gt;Agility starts with mindset, requiring staff who are willing to experiment. However, this experimentation must be deliberate. It requires collaboration between technical, marketing, and legal teams from the outset.&lt;/p&gt;&lt;p&gt;This “compliance-by-design” approach allows for faster iteration because the parameters of safety are established before the code is written.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-s-next-for-ai-in-the-financial-sector"&gt;What’s next for AI in the financial sector?&lt;/h3&gt;&lt;p&gt;Looking further ahead, the financial ecosystem will likely see direct interaction between AI agents acting on behalf of consumers and agents acting for institutions.&lt;/p&gt;&lt;p&gt;Melanie Lazarus, Ecosystem Engagement Director at Open Banking, warns: “We are entering a world where AI agents interact with each other, and that changes the foundations of consent, authentication, and authorisation.”&lt;/p&gt;&lt;p&gt;Tech leaders must begin architecting frameworks that protect customers in this agent-to-agent reality. This involves new protocols for identity verification and API security to ensure that an automated financial advisor acting for a client can securely interact with a bank’s infrastructure.&lt;/p&gt;&lt;p&gt;The mandate for 2026 is to turn the potential of AI into a reliable P&amp;amp;L driver. This requires a focus on infrastructure over hype and leaders must prioritise:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Unifying data streams:&lt;/strong&gt; Ensure signals from all channels feed into a central decision engine to enable context-aware actions.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Hard-coding governance:&lt;/strong&gt; Embed compliance rules into the AI workflow to allow for safe automation.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Agentic orchestration:&lt;/strong&gt; Move beyond chatbots to agents that can execute end-to-end processes.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Generative optimisation:&lt;/strong&gt; Structure public data to be readable and prioritised by external AI search engines.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Success will depend on how well these technical elements are integrated with human oversight. The winning organisations will be those that use AI automation to enhance, rather than replace, the judgment that is especially required in sectors like financial services.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Goldman Sachs deploys Anthropic systems with success&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111908" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-financial-institutions-embedding-ai-decision-making/</guid><pubDate>Wed, 18 Feb 2026 15:02:14 +0000</pubDate></item><item><title>Kana emerges from stealth with $15M to build flexible AI agents for marketers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/kana-emerges-from-stealth-with-15m-to-build-flexible-ai-agents-for-marketers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Copy-of-Tom-Chavez-and-Vivek-Vaidya-Group-Shot.jpg?resize=1200,901" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Marketing is one of the few operations no industry can afford to ignore, which is why we have a veritable host of AI-powered marketing tools being shoved into marketers’ faces today. All the social platforms, from Facebook and Instagram to TikTok, and major incumbents like Microsoft and Google, to content-generation startups like Jasper and Copy.ai, offer AI tools that claim to make marketers’ lives easier in uncountable ways.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That was partly why I was confused to see yet another marketing AI startup entering the fray: San Francisco-based Kana just came out of stealth with a suite of AI agents that can do data analysis, audience targeting, campaign management, customer engagement, media planning, and optimizing for AI chatbots. The startup has raised $15 million in a seed funding round led by Mayfield. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But Kana has something going for it that most marketing startups today don’t: Its co-founders, Tom Chavez (CEO; pictured above on the right) and Vivek Vaidya (CTO; pictured above on the left), have been building marketing tech for more than 25 years. Kana’s actually their fourth venture after Rapt (acquired by Microsoft in 2008), Krux (bought by Salesforce in 2016), and startup studio super{set}, which they incubated Kana in for nine months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calling this a “wondrous” time to be building, Chavez said there was a clear opportunity to bring their experience and today’s AI tech to bear on this class of problems. “We see a market that’s crying out for solutions that meet this moment […] We understand the space deeply, having wallowed in it arguably a little too long; having really stood in our customers’ pain,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The solution, as Kana pitches it, involves “loosely coupled” AI agents that can be tailored “on the fly,” integrated into legacy marketing software, and can simultaneously work on different operations. So a marketer could, for example, upload a media brief that Kana’s agents would analyze to figure out the campaign goals, search for the audience to target, and pull in data from inventory and market research to further tweak the plan. The platform bakes in autonomous campaign tracking, optimization, and reporting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside agents, Kana offers synthetic data generation to augment third-party data sources for activities like market research and audience targeting. This, Chavez argued, could help companies reduce the costs of using third-party data, fill in gaps in the data, and help marketers run tests on various platforms faster and narrow down strategies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kana says this is all done while keeping humans in the loop so that marketers can approve the AI agents’ actions, give feedback, and customize what the agents do as their needs change.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chavez and Vaidya emphasized the importance of the platform’s flexibility, arguing that the ability to deploy, tailor, and build new agents in real time would let marketers see results on their campaigns faster than they would with legacy systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Going forward, the startup sees that very flexibility to customize its platform for customers, doubling as its moat against incumbents and other startups building similar products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have the opportunity not to create bespoke solutions, but to highly tailor and configure these solutions to meet customers where they are. Larger companies just are never going to get there,” Chavez said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We live in a world which allows us to explore a third option [with customers]: not build, not buy, but build with — build with in a way which is supported,” Vaidya added. “We can move with insane speed that these big companies just cannot. And that’s our advantage.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kana will use the fresh cash to expand hiring across engineering, product, and go-to-market. Mayfield managing partner Navin Chaddha is joining the company’s board. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Copy-of-Tom-Chavez-and-Vivek-Vaidya-Group-Shot.jpg?resize=1200,901" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Marketing is one of the few operations no industry can afford to ignore, which is why we have a veritable host of AI-powered marketing tools being shoved into marketers’ faces today. All the social platforms, from Facebook and Instagram to TikTok, and major incumbents like Microsoft and Google, to content-generation startups like Jasper and Copy.ai, offer AI tools that claim to make marketers’ lives easier in uncountable ways.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That was partly why I was confused to see yet another marketing AI startup entering the fray: San Francisco-based Kana just came out of stealth with a suite of AI agents that can do data analysis, audience targeting, campaign management, customer engagement, media planning, and optimizing for AI chatbots. The startup has raised $15 million in a seed funding round led by Mayfield. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But Kana has something going for it that most marketing startups today don’t: Its co-founders, Tom Chavez (CEO; pictured above on the right) and Vivek Vaidya (CTO; pictured above on the left), have been building marketing tech for more than 25 years. Kana’s actually their fourth venture after Rapt (acquired by Microsoft in 2008), Krux (bought by Salesforce in 2016), and startup studio super{set}, which they incubated Kana in for nine months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calling this a “wondrous” time to be building, Chavez said there was a clear opportunity to bring their experience and today’s AI tech to bear on this class of problems. “We see a market that’s crying out for solutions that meet this moment […] We understand the space deeply, having wallowed in it arguably a little too long; having really stood in our customers’ pain,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The solution, as Kana pitches it, involves “loosely coupled” AI agents that can be tailored “on the fly,” integrated into legacy marketing software, and can simultaneously work on different operations. So a marketer could, for example, upload a media brief that Kana’s agents would analyze to figure out the campaign goals, search for the audience to target, and pull in data from inventory and market research to further tweak the plan. The platform bakes in autonomous campaign tracking, optimization, and reporting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside agents, Kana offers synthetic data generation to augment third-party data sources for activities like market research and audience targeting. This, Chavez argued, could help companies reduce the costs of using third-party data, fill in gaps in the data, and help marketers run tests on various platforms faster and narrow down strategies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kana says this is all done while keeping humans in the loop so that marketers can approve the AI agents’ actions, give feedback, and customize what the agents do as their needs change.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chavez and Vaidya emphasized the importance of the platform’s flexibility, arguing that the ability to deploy, tailor, and build new agents in real time would let marketers see results on their campaigns faster than they would with legacy systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Going forward, the startup sees that very flexibility to customize its platform for customers, doubling as its moat against incumbents and other startups building similar products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have the opportunity not to create bespoke solutions, but to highly tailor and configure these solutions to meet customers where they are. Larger companies just are never going to get there,” Chavez said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We live in a world which allows us to explore a third option [with customers]: not build, not buy, but build with — build with in a way which is supported,” Vaidya added. “We can move with insane speed that these big companies just cannot. And that’s our advantage.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kana will use the fresh cash to expand hiring across engineering, product, and go-to-market. Mayfield managing partner Navin Chaddha is joining the company’s board. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/kana-emerges-from-stealth-with-15m-to-build-flexible-ai-agents-for-marketers/</guid><pubDate>Wed, 18 Feb 2026 15:08:40 +0000</pubDate></item><item><title>Google adds music-generation capabilities to the Gemini app (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/google-adds-music-generation-capabilities-to-the-gemini-app/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Lyria-feature.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that it’s adding a music-generation feature to the Gemini app. The company is using DeepMind’s Lyria 3 music-generation model to power the feature, which is still in beta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the feature, you’ll describe the song you want to create, and the app will generate a track along with lyrics. For instance, you could ask Gemini to create a “comical R&amp;amp;B slow jam about a sock finding its match,” and the app will generate a 30-second track along with cover art made by Nano Banana.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google said that you can even upload a photo or a video, and the AI-powered tool will create a song to match the mood of the media file.&lt;/p&gt;

&lt;div class="jwppp-video-box" id="jwppp-video-box-30938721"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that Lyria 3 improves on the previous generation of models, creating more realistic and complex music tracks. Users can also change and control other elements like style, vocals, and tempo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Along with rolling out Lyria 3 to the Gemini app, Google is making the model available to YouTube creators through the Dream Track feature on YouTube, a tool that helps creators make AI-generated tracks. The option was only available to YouTube creators in the U.S. until now. But with this release, Google is expanding Dream Track availability globally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google said that you can’t mimic an artist outright, but if you add an artist’s name to your prompt, Gemini will create a track in a similar style or a mood. (It’s not clear if generation will make it easier for others to decode the music style of a particular artist.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Music generation with Lyria 3 is designed for original expression, not for mimicking existing artists. If your prompt names a specific artist, Gemini will take this as broad creative inspiration and create a track that shares a similar style or mood. We also have filters in place to check outputs against existing content,” the company said in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that all songs created with the Lyria 3 model will have a SynthID watermark to identify AI-generated content. The company said that it’s also adding capabilities to identify AI-generated music with SynthID within Gemini. Users will be able to upload tracks and ask Gemini if it is AI-generated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Music generation is rolling out to all 18+ Gemini users across the world with support for English, German, Spanish, French, Hindi, Japanese, Korean, and Portuguese.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI-generated music has created mixed sentiments among artists and listeners. On one hand, companies like YouTube and Spotify are adopting AI and signing contracts with music labels to monetize AI-generated music. On the other hand, AI model and tooling companies are facing lawsuits from the music industry over copyrights of the training material. Platforms like Deezer have published tools to mark AI-generated music to curb fraudulent streams of this kind of music.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Lyria-feature.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that it’s adding a music-generation feature to the Gemini app. The company is using DeepMind’s Lyria 3 music-generation model to power the feature, which is still in beta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the feature, you’ll describe the song you want to create, and the app will generate a track along with lyrics. For instance, you could ask Gemini to create a “comical R&amp;amp;B slow jam about a sock finding its match,” and the app will generate a 30-second track along with cover art made by Nano Banana.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google said that you can even upload a photo or a video, and the AI-powered tool will create a song to match the mood of the media file.&lt;/p&gt;

&lt;div class="jwppp-video-box" id="jwppp-video-box-30938721"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The company said that Lyria 3 improves on the previous generation of models, creating more realistic and complex music tracks. Users can also change and control other elements like style, vocals, and tempo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Along with rolling out Lyria 3 to the Gemini app, Google is making the model available to YouTube creators through the Dream Track feature on YouTube, a tool that helps creators make AI-generated tracks. The option was only available to YouTube creators in the U.S. until now. But with this release, Google is expanding Dream Track availability globally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google said that you can’t mimic an artist outright, but if you add an artist’s name to your prompt, Gemini will create a track in a similar style or a mood. (It’s not clear if generation will make it easier for others to decode the music style of a particular artist.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Music generation with Lyria 3 is designed for original expression, not for mimicking existing artists. If your prompt names a specific artist, Gemini will take this as broad creative inspiration and create a track that shares a similar style or mood. We also have filters in place to check outputs against existing content,” the company said in a blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google noted that all songs created with the Lyria 3 model will have a SynthID watermark to identify AI-generated content. The company said that it’s also adding capabilities to identify AI-generated music with SynthID within Gemini. Users will be able to upload tracks and ask Gemini if it is AI-generated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Music generation is rolling out to all 18+ Gemini users across the world with support for English, German, Spanish, French, Hindi, Japanese, Korean, and Portuguese.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI-generated music has created mixed sentiments among artists and listeners. On one hand, companies like YouTube and Spotify are adopting AI and signing contracts with music labels to monetize AI-generated music. On the other hand, AI model and tooling companies are facing lawsuits from the music industry over copyrights of the training material. Platforms like Deezer have published tools to mark AI-generated music to curb fraudulent streams of this kind of music.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/google-adds-music-generation-capabilities-to-the-gemini-app/</guid><pubDate>Wed, 18 Feb 2026 16:00:00 +0000</pubDate></item><item><title>Record scratch—Google's Lyria 3 AI music model is coming to Gemini today (AI - Ars Technica)</title><link>https://arstechnica.com/google/2026/02/gemini-can-now-generate-ai-music-for-you-no-lyrics-required/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        With a simple prompt, you can generate 30 seconds of something like music.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Lyria AI album covers" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Lyria_Hero-Image-640x360.png" width="640" /&gt;
                  &lt;img alt="Lyria AI album covers" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Lyria_Hero-Image-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The American poet Henry Wadsworth Longfellow called music “the universal language of mankind.” Is that still true when the so-called music is being generated by a probabilistic robot instead of a human? We’re about to find out. Google has announced its latest Lyria 3 AI model is being deployed in the Gemini app, vastly expanding access to AI music generation.&lt;/p&gt;
&lt;p&gt;Google DeepMind has been tinkering with Lyria for a while now, offering limited access in developer-oriented products like Vertex AI. Lyria 3 is more capable than previous versions, and it’s also quicker to use. Just select the new “Create music” option in the Gemini app or web UI to get started. You can describe what you want and even upload an image to help the robot get the right vibe. And in a few seconds, you get music (or something like it).&lt;/p&gt;
&lt;p&gt;In case there was any uncertainty about whether Lyria tracks still counted as a human artistic endeavor, worry not! Unlike past versions of the model, you don’t even have to provide lyrics in your prompt. You can be vague with your request, and the model will create suitable lyrics for the 30-second song. Although with that limit, “jingle” might be more accurate.&lt;/p&gt;
&lt;p&gt;In addition to the track, each music creation job will come with an album cover-style image created by the Nano Banana model. Gemini will also have a pre-loaded set of AI tracks that you can choose to remix to your heart’s content. The Lyria 3 tools are also coming to Google’s Dream Track toolkit for YouTube Shorts, which will pair nicely with the Veo AI video options.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;So what kind of tracks can you expect Gemini to spit out? Google has provided some examples:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Sweet Like Plantain&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-1" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Sweet-Like-Plantain-included-in-blog.mp3?_=1" type="audio/mpeg" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: I’m feeling nostalgic. Create a track for my mother about the great times we had as kids and the memories of her home-cooked plantains. Make it a fun afrobeat track with a true African vibe.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Motown Parody&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-2" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Motown-Parody.wav?_=2" type="audio/wav" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: Quintessential 1970s Motown soul. Lush, orchestral R&amp;amp;B production. Warm bassline with melodic fills, locked into a steady drum groove with crisp snare and tambourine. Vintage organ harmonic bed. Three-piece brass section. Gritty, gospel-tinged male tenor lead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Pop Flutter&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-3" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Pop-Flutter.wav?_=3" type="audio/wav" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: Wistful and airy. Soft, breathy female vocals with intimacy. Rapid-fire drum and bass rhythm, low-passed and softened. Deep, warm bass swells. Dreamy electric piano chords and subtle chime textures. Rainy city vibes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Sea Shanty&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-4" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Sea-Shanty-Acapella.wav?_=4" type="audio/wav" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: An authentic A capella Sea Shanty featuring a robust male choir singing in a traditional call-and-response format. The piece is entirely vocal, relying on synchronized foot-stomps on a wooden deck and sharp handclaps to provide the rhythmic pulse. The lead is a weathered male baritone with a gravelly timbre who sings the narrative ‘chant’ lines. He is immediately answered by a powerful male choir singing in rich, rugged harmony on the ‘response’ lines. The voices are recorded with a natural room reverb that simulates the acoustic environment of a wooden ship’s deck, giving the vocals a resonant, atmospheric quality. The performance is energetic and driving, with the choir leaning into the rhythm of the stomps to create a sense of focused, communal effort. There are no instruments, only the layered textures of collective male voices spanning tenor, baritone, and bass ranges, all contributing to a confident, monolithic sound.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Sour notes&lt;/h2&gt;
&lt;p&gt;AI-generated music is not a new phenomenon. Several companies offer models that ingest and homogenize human-created music, and the resulting tracks can sound remarkably “real,” if a bit overproduced. Streaming services have already been inundated with phony AI artists, some of which have gathered thousands of listeners who may not even realize they’re grooving to the musical equivalent of a blender set to purée.&lt;/p&gt;
&lt;p&gt;Still, you have to seek out tools like that, and Google is bringing similar capabilities to the Gemini app. As one of the most popular AI platforms, we’re probably about to see a lot more AI music on the Internet. Google says tracks generated with Lyria 3 will have an audio version of Google’s SynthID embedded within. That means you’ll always be able to check if a piece of audio was created with Google’s AI by uploading it to Gemini, similar to the way you can check images and videos for SynthID tags.&lt;/p&gt;
&lt;p&gt;Google also says it has sought to create a music AI that respects copyright and partner agreements. If you name a specific artist in your prompt, Gemini won’t attempt to copy that artist’s sound. Instead, it’s trained to take that as “broad creative inspiration.” Although it also notes this process is not foolproof, and some of that original expression might imitate an artist too much. In those cases, Google invites users to report such shared content.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2141467-1" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Gemini-App-UI.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Lyria 3 is going live in the Gemini web interface today and should be available in the mobile app within a few days. It works in English, German, Spanish, French, Hindi, Japanese, Korean, and Portuguese, but Google plans to add more languages soon. While all users will have some access to music generation, those with AI Pro and AI Ultra subscriptions will have higher usage limits, but the specifics are unclear.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        With a simple prompt, you can generate 30 seconds of something like music.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Lyria AI album covers" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Lyria_Hero-Image-640x360.png" width="640" /&gt;
                  &lt;img alt="Lyria AI album covers" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Lyria_Hero-Image-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The American poet Henry Wadsworth Longfellow called music “the universal language of mankind.” Is that still true when the so-called music is being generated by a probabilistic robot instead of a human? We’re about to find out. Google has announced its latest Lyria 3 AI model is being deployed in the Gemini app, vastly expanding access to AI music generation.&lt;/p&gt;
&lt;p&gt;Google DeepMind has been tinkering with Lyria for a while now, offering limited access in developer-oriented products like Vertex AI. Lyria 3 is more capable than previous versions, and it’s also quicker to use. Just select the new “Create music” option in the Gemini app or web UI to get started. You can describe what you want and even upload an image to help the robot get the right vibe. And in a few seconds, you get music (or something like it).&lt;/p&gt;
&lt;p&gt;In case there was any uncertainty about whether Lyria tracks still counted as a human artistic endeavor, worry not! Unlike past versions of the model, you don’t even have to provide lyrics in your prompt. You can be vague with your request, and the model will create suitable lyrics for the 30-second song. Although with that limit, “jingle” might be more accurate.&lt;/p&gt;
&lt;p&gt;In addition to the track, each music creation job will come with an album cover-style image created by the Nano Banana model. Gemini will also have a pre-loaded set of AI tracks that you can choose to remix to your heart’s content. The Lyria 3 tools are also coming to Google’s Dream Track toolkit for YouTube Shorts, which will pair nicely with the Veo AI video options.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;So what kind of tracks can you expect Gemini to spit out? Google has provided some examples:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Sweet Like Plantain&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-1" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Sweet-Like-Plantain-included-in-blog.mp3?_=1" type="audio/mpeg" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: I’m feeling nostalgic. Create a track for my mother about the great times we had as kids and the memories of her home-cooked plantains. Make it a fun afrobeat track with a true African vibe.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Motown Parody&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-2" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Motown-Parody.wav?_=2" type="audio/wav" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: Quintessential 1970s Motown soul. Lush, orchestral R&amp;amp;B production. Warm bassline with melodic fills, locked into a steady drum groove with crisp snare and tambourine. Vintage organ harmonic bed. Three-piece brass section. Gritty, gospel-tinged male tenor lead.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Pop Flutter&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-3" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Pop-Flutter.wav?_=3" type="audio/wav" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: Wistful and airy. Soft, breathy female vocals with intimacy. Rapid-fire drum and bass rhythm, low-passed and softened. Deep, warm bass swells. Dreamy electric piano chords and subtle chime textures. Rainy city vibes.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“&lt;/em&gt;Sea Shanty&lt;em&gt;“&lt;/em&gt;&lt;/p&gt;
&lt;audio class="wp-audio-shortcode" controls="controls" id="audio-2141467-4" preload="none" style="width: 100%;"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Sea-Shanty-Acapella.wav?_=4" type="audio/wav" /&gt;&lt;/audio&gt;
&lt;p&gt;Prompt: An authentic A capella Sea Shanty featuring a robust male choir singing in a traditional call-and-response format. The piece is entirely vocal, relying on synchronized foot-stomps on a wooden deck and sharp handclaps to provide the rhythmic pulse. The lead is a weathered male baritone with a gravelly timbre who sings the narrative ‘chant’ lines. He is immediately answered by a powerful male choir singing in rich, rugged harmony on the ‘response’ lines. The voices are recorded with a natural room reverb that simulates the acoustic environment of a wooden ship’s deck, giving the vocals a resonant, atmospheric quality. The performance is energetic and driving, with the choir leaning into the rhythm of the stomps to create a sense of focused, communal effort. There are no instruments, only the layered textures of collective male voices spanning tenor, baritone, and bass ranges, all contributing to a confident, monolithic sound.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Sour notes&lt;/h2&gt;
&lt;p&gt;AI-generated music is not a new phenomenon. Several companies offer models that ingest and homogenize human-created music, and the resulting tracks can sound remarkably “real,” if a bit overproduced. Streaming services have already been inundated with phony AI artists, some of which have gathered thousands of listeners who may not even realize they’re grooving to the musical equivalent of a blender set to purée.&lt;/p&gt;
&lt;p&gt;Still, you have to seek out tools like that, and Google is bringing similar capabilities to the Gemini app. As one of the most popular AI platforms, we’re probably about to see a lot more AI music on the Internet. Google says tracks generated with Lyria 3 will have an audio version of Google’s SynthID embedded within. That means you’ll always be able to check if a piece of audio was created with Google’s AI by uploading it to Gemini, similar to the way you can check images and videos for SynthID tags.&lt;/p&gt;
&lt;p&gt;Google also says it has sought to create a music AI that respects copyright and partner agreements. If you name a specific artist in your prompt, Gemini won’t attempt to copy that artist’s sound. Instead, it’s trained to take that as “broad creative inspiration.” Although it also notes this process is not foolproof, and some of that original expression might imitate an artist too much. In those cases, Google invites users to report such shared content.&lt;/p&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1080" id="video-2141467-1" preload="metadata" width="1920"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Gemini-App-UI.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Lyria 3 is going live in the Gemini web interface today and should be available in the mobile app within a few days. It works in English, German, Spanish, French, Hindi, Japanese, Korean, and Portuguese, but Google plans to add more languages soon. While all users will have some access to music generation, those with AI Pro and AI Ultra subscriptions will have higher usage limits, but the specifics are unclear.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2026/02/gemini-can-now-generate-ai-music-for-you-no-lyrics-required/</guid><pubDate>Wed, 18 Feb 2026 16:00:11 +0000</pubDate></item><item><title>Google DeepMind wants to know if chatbots are just virtue signaling (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/18/1133299/google-deepmind-wants-to-know-if-chatbots-are-just-virtue-signaling/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/decision-tree2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;Google DeepMind is calling for the moral behavior of large language models—such as what they do when called on to act as companions, therapists, medical advisors, and so on—to be scrutinized with the same kind of rigor as their ability to code or do math.&lt;/p&gt;  &lt;p&gt;As LLMs improve, people are asking them to play more and more sensitive roles in their lives. Agents are starting to take actions on people’s behalf. LLMs may be able to influence human decision-making. And yet nobody knows how trustworthy this technology really is at such tasks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;With coding and math, you have clear-cut, correct answers that you can check, William Isaac, a research scientist at Google DeepMind, told me when I met him and Julia Haas, a fellow research scientist at the firm, for an exclusive preview of their work, which is published in &lt;em&gt;Nature&lt;/em&gt; today. That’s not the case for moral questions, which typically have a range of acceptable answers: “Morality is an important capability but hard to evaluate,” says Isaac.&lt;/p&gt;  &lt;p&gt;“In the moral domain, there’s no right and wrong,” adds Haas. “But it’s not by any means a free-for-all. There are better answers and there are worse answers.”&lt;/p&gt; 
 &lt;p&gt;The researchers have identified several key challenges and suggested ways to address them. But it is more a wish list than a set of ready-made solutions. “They do a nice job of bringing together different perspectives,” says Vera Demberg, who studies LLMs at Saarland University in Germany.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Better than "The Ethicist"&lt;/h3&gt;  &lt;p&gt;A number of studies have shown that LLMs can show remarkable moral competence. One study published last year found that people in the US scored ethical advice from OpenAI’s GPT-4o as being more moral, trustworthy, thoughtful, and correct than advice given by the (human) writer of “The Ethicist,” a popular &lt;em&gt;New York Times&lt;/em&gt; advice column.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The problem is that it is hard to unpick whether such behaviors are a performance—mimicking a memorized response, say—or evidence that there is in fact some kind of moral reasoning taking place inside the model. In other words, is it virtue or virtue signaling?&lt;/p&gt;  &lt;p&gt;This question matters because multiple studies also show just how untrustworthy LLMs can be. For a start, models can be too eager to please. They have been found to flip their answer to a moral question and say the exact opposite when a person disagrees or pushes back on their first response. Worse, the answers an LLM gives to a question can change in response to how it is presented or formatted. For example, researchers have found that models quizzed about political values can give different—sometimes opposite—answers depending on whether the questions offer multiple-choice answers or instruct the model to respond in its own words.&lt;/p&gt;  &lt;p&gt;In an even more striking case, Demberg and her colleagues presented several LLMs, including versions of Meta’s Llama 3 and Mistral, with a series of moral dilemmas and asked them to pick which of two options was the better outcome. The researchers found that the models often reversed their choice when the labels for those two options were changed from “Case 1” and “Case 2” to “(A)” and “(B).”&lt;/p&gt;  &lt;p&gt;They also showed that models changed their answers in response to other tiny formatting tweaks, including swapping the order of the options and ending the question with a colon instead of a question mark.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;In short, the appearance of moral behavior in LLMs should not be taken at face value. Models must be probed to see how robust that moral behavior really is. “For people to trust the answers, you need to know how you got there,” says Haas.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;More rigorous tests&lt;/h3&gt;  &lt;p&gt;What Haas, Isaac, and their colleagues at Google DeepMind propose is a new line of research to develop more rigorous techniques for evaluating moral competence in LLMs. This would include tests designed to push models to change their responses to moral questions. If a model flipped its moral position, it would show that it hadn’t engaged in robust moral reasoning.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another type of test would present models with variations of common moral problems to check whether they produce a rote response or one that’s more nuanced and relevant to the actual problem that was posed. For example, asking a model to talk through the moral implications of a complex scenario in which a man donates sperm to his son so that his son can have a child of his own might produce concerns about the social impact of allowing a man to be both biological father and biological grandfather to a child. But it should not produce concerns about incest, even though the scenario has superficial parallels with that taboo.&lt;/p&gt;  &lt;p&gt;Haas also says that getting models to provide a trace of the steps they took to produce an answer would give some insight into whether that answer was a fluke or grounded in actual evidence. Techniques such as chain-of-thought monitoring, in which researchers listen in on a kind of internal monologue that some LLMs produce as they work, could help here too.&lt;/p&gt; 

 &lt;p&gt;Another approach researchers could use to determine why a model gave a particular answer is mechanistic interpretability, which can provide small glimpses inside a model as it carries out a task. Neither chain-of-thought monitoring nor mechanistic interpretability provides perfect snapshots of a model’s workings. But the Google DeepMind team believes that combining such techniques with a wide range of rigorous tests will go a long way to figuring out exactly how far to trust LLMs with certain critical or sensitive tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Different values&lt;/h3&gt;  &lt;p&gt;And yet there’s a wider problem too. Models from major companies such as Google DeepMind are used across the world by people with different values and belief systems. The answer to a simple question like “Should I order pork chops?” should differ depending on whether or not the person asking is vegetarian or Jewish, for example.&lt;/p&gt;  &lt;p&gt;There’s no solution to this challenge, Haas and Isaac admit. But they think that models may need to be designed either to produce a range of acceptable answers, aiming to please everyone, or to have a kind of switch that turns different moral codes on and off depending on the user.&lt;/p&gt;  &lt;p&gt;“It’s a complex world out there,” says Haas. “We will probably need some combination of those things, because even if you’re taking just one population, there’s going to be a range of views represented.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;“It’s a fascinating paper,” says Danica Dillion at Ohio State University, who studies how large language models handle different belief systems and was not involved in the work. “Pluralism in AI is really important, and it’s one of the biggest limitations of LLMs and moral reasoning right now,” she says. “Even though they were trained on a ginormous amount of data, that data still leans heavily Western. When you probe LLMs, they do a lot better at representing Westerners’ morality than non-Westerners’.”&lt;/p&gt;  &lt;p&gt;But it is not yet clear how we can build models that are guaranteed to have moral competence across global cultures, says Demberg. “There are these two independent questions. One is: How should it work? And, secondly, how can it technically be achieved? And I think that both of those questions are pretty open at the moment.”&lt;/p&gt;  &lt;p&gt;For Isaac, that makes morality a new frontier for LLMs. “I think this is equally as fascinating as math and code in terms of what it means for AI progress,” he says. “You know, advancing moral competency could also mean that we’re going to see better AI systems overall that actually align with society.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/decision-tree2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;Google DeepMind is calling for the moral behavior of large language models—such as what they do when called on to act as companions, therapists, medical advisors, and so on—to be scrutinized with the same kind of rigor as their ability to code or do math.&lt;/p&gt;  &lt;p&gt;As LLMs improve, people are asking them to play more and more sensitive roles in their lives. Agents are starting to take actions on people’s behalf. LLMs may be able to influence human decision-making. And yet nobody knows how trustworthy this technology really is at such tasks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;With coding and math, you have clear-cut, correct answers that you can check, William Isaac, a research scientist at Google DeepMind, told me when I met him and Julia Haas, a fellow research scientist at the firm, for an exclusive preview of their work, which is published in &lt;em&gt;Nature&lt;/em&gt; today. That’s not the case for moral questions, which typically have a range of acceptable answers: “Morality is an important capability but hard to evaluate,” says Isaac.&lt;/p&gt;  &lt;p&gt;“In the moral domain, there’s no right and wrong,” adds Haas. “But it’s not by any means a free-for-all. There are better answers and there are worse answers.”&lt;/p&gt; 
 &lt;p&gt;The researchers have identified several key challenges and suggested ways to address them. But it is more a wish list than a set of ready-made solutions. “They do a nice job of bringing together different perspectives,” says Vera Demberg, who studies LLMs at Saarland University in Germany.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Better than "The Ethicist"&lt;/h3&gt;  &lt;p&gt;A number of studies have shown that LLMs can show remarkable moral competence. One study published last year found that people in the US scored ethical advice from OpenAI’s GPT-4o as being more moral, trustworthy, thoughtful, and correct than advice given by the (human) writer of “The Ethicist,” a popular &lt;em&gt;New York Times&lt;/em&gt; advice column.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The problem is that it is hard to unpick whether such behaviors are a performance—mimicking a memorized response, say—or evidence that there is in fact some kind of moral reasoning taking place inside the model. In other words, is it virtue or virtue signaling?&lt;/p&gt;  &lt;p&gt;This question matters because multiple studies also show just how untrustworthy LLMs can be. For a start, models can be too eager to please. They have been found to flip their answer to a moral question and say the exact opposite when a person disagrees or pushes back on their first response. Worse, the answers an LLM gives to a question can change in response to how it is presented or formatted. For example, researchers have found that models quizzed about political values can give different—sometimes opposite—answers depending on whether the questions offer multiple-choice answers or instruct the model to respond in its own words.&lt;/p&gt;  &lt;p&gt;In an even more striking case, Demberg and her colleagues presented several LLMs, including versions of Meta’s Llama 3 and Mistral, with a series of moral dilemmas and asked them to pick which of two options was the better outcome. The researchers found that the models often reversed their choice when the labels for those two options were changed from “Case 1” and “Case 2” to “(A)” and “(B).”&lt;/p&gt;  &lt;p&gt;They also showed that models changed their answers in response to other tiny formatting tweaks, including swapping the order of the options and ending the question with a colon instead of a question mark.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;In short, the appearance of moral behavior in LLMs should not be taken at face value. Models must be probed to see how robust that moral behavior really is. “For people to trust the answers, you need to know how you got there,” says Haas.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;More rigorous tests&lt;/h3&gt;  &lt;p&gt;What Haas, Isaac, and their colleagues at Google DeepMind propose is a new line of research to develop more rigorous techniques for evaluating moral competence in LLMs. This would include tests designed to push models to change their responses to moral questions. If a model flipped its moral position, it would show that it hadn’t engaged in robust moral reasoning.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another type of test would present models with variations of common moral problems to check whether they produce a rote response or one that’s more nuanced and relevant to the actual problem that was posed. For example, asking a model to talk through the moral implications of a complex scenario in which a man donates sperm to his son so that his son can have a child of his own might produce concerns about the social impact of allowing a man to be both biological father and biological grandfather to a child. But it should not produce concerns about incest, even though the scenario has superficial parallels with that taboo.&lt;/p&gt;  &lt;p&gt;Haas also says that getting models to provide a trace of the steps they took to produce an answer would give some insight into whether that answer was a fluke or grounded in actual evidence. Techniques such as chain-of-thought monitoring, in which researchers listen in on a kind of internal monologue that some LLMs produce as they work, could help here too.&lt;/p&gt; 

 &lt;p&gt;Another approach researchers could use to determine why a model gave a particular answer is mechanistic interpretability, which can provide small glimpses inside a model as it carries out a task. Neither chain-of-thought monitoring nor mechanistic interpretability provides perfect snapshots of a model’s workings. But the Google DeepMind team believes that combining such techniques with a wide range of rigorous tests will go a long way to figuring out exactly how far to trust LLMs with certain critical or sensitive tasks.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Different values&lt;/h3&gt;  &lt;p&gt;And yet there’s a wider problem too. Models from major companies such as Google DeepMind are used across the world by people with different values and belief systems. The answer to a simple question like “Should I order pork chops?” should differ depending on whether or not the person asking is vegetarian or Jewish, for example.&lt;/p&gt;  &lt;p&gt;There’s no solution to this challenge, Haas and Isaac admit. But they think that models may need to be designed either to produce a range of acceptable answers, aiming to please everyone, or to have a kind of switch that turns different moral codes on and off depending on the user.&lt;/p&gt;  &lt;p&gt;“It’s a complex world out there,” says Haas. “We will probably need some combination of those things, because even if you’re taking just one population, there’s going to be a range of views represented.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt;&lt;p&gt;“It’s a fascinating paper,” says Danica Dillion at Ohio State University, who studies how large language models handle different belief systems and was not involved in the work. “Pluralism in AI is really important, and it’s one of the biggest limitations of LLMs and moral reasoning right now,” she says. “Even though they were trained on a ginormous amount of data, that data still leans heavily Western. When you probe LLMs, they do a lot better at representing Westerners’ morality than non-Westerners’.”&lt;/p&gt;  &lt;p&gt;But it is not yet clear how we can build models that are guaranteed to have moral competence across global cultures, says Demberg. “There are these two independent questions. One is: How should it work? And, secondly, how can it technically be achieved? And I think that both of those questions are pretty open at the moment.”&lt;/p&gt;  &lt;p&gt;For Isaac, that makes morality a new frontier for LLMs. “I think this is equally as fascinating as math and code in terms of what it means for AI progress,” he says. “You know, advancing moral competency could also mean that we’re going to see better AI systems overall that actually align with society.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/18/1133299/google-deepmind-wants-to-know-if-chatbots-are-just-virtue-signaling/</guid><pubDate>Wed, 18 Feb 2026 16:00:22 +0000</pubDate></item><item><title>A new way to express yourself: Gemini can now create music (Google DeepMind News)</title><link>https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/0217_KeywordHeaderFinalc.width-1300.png" /&gt;&lt;/div&gt;&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="A new way to express yourself: Gemini can now create music"&gt;
      &lt;source src="https://storage.googleapis.com/gweb-uniblog-publish-prod/media/tts_audio_83328_umbriel_2026_02_19_02_23_13.wav" type="audio/x-wav" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Since launching the Gemini app, we've built tools to encourage creative expression through images and video. Today, we're taking the next step: custom music generation. Lyria 3, Google DeepMind’s latest generative music model, is rolling out today in beta in the Gemini app. Just describe an idea or upload a photo, like “a comical R&amp;amp;B slow jam about a sock finding their match" and in a matter of seconds, Gemini will translate it into a high-quality, catchy track. To push the creative envelope further, you can even ask Gemini to take inspiration from something you upload.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Lyria 3 improves on audio generation from our Lyria models in three important ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;No need to provide your own lyrics! They'll be generated for you based on your prompt.&lt;/li&gt;&lt;li&gt;You have more creative control over elements like the style, vocals and tempo you want.&lt;/li&gt;&lt;li&gt;You can create more realistic and musically complex tracks.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Here’s how you can use it:&lt;br /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Text to track:&lt;/b&gt; Describe a specific genre, mood, inside joke, or memory to create unique tracks with lyrics or instrumental audio that fits your vibe. &lt;i&gt;“I’m feeling nostalgic. Create a track for my mother about the great times we had as kids and the memories of her home cooked plantains. Make it a fun afrobeat track with a true African vibe.”&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;From photos and videos to track:&lt;/b&gt; Upload a photo or video and watch Gemini use the content to compose a track with lyrics that fit the mood perfectly. &lt;i&gt;“Use these photos to create a track about my dog Duncan on a hike in the woods.”&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Gemini app creates 30-second tracks with custom cover art generated by Nano Banana. This makes it easy to quickly share with friends by downloading or simply clicking the share link. The goal of these tracks isn't to create a musical masterpiece, but rather to give you a fun, unique way to express yourself.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    











&lt;div class="audio-player"&gt;
  &lt;audio class="audio-player__player" title="Sweet Like Plantain"&gt;
    &lt;audio controls="controls"&gt;
&lt;source src="https://storage.googleapis.com/gweb-uniblog-publish-prod/media/Sweet_Like_Plantain_1.mp3" type="audio/mpeg" /&gt;
&lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
&lt;/audio&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player__container"&gt;
    &lt;div class="audio-player__text-content"&gt;
      &lt;div class="audio-player__content"&gt;
        &lt;span class="audio-player__content--title"&gt;Sweet Like Plantain&lt;/span&gt;
        
      &lt;/div&gt;
      &lt;button class="audio-player__preview-play"&gt;
        &lt;svg class="icon audio-player__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;span class="audio-player__duration"&gt;&lt;/span&gt;
      &lt;/button&gt;
      &lt;span class="audio-player__duration--large-viewport"&gt;&lt;/span&gt;
    &lt;/div&gt;
    &lt;div class="audio-player__console"&gt;
      &lt;div class="audio-player__start"&gt;
        &lt;button class="audio-player__replay"&gt;
          &lt;svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;button class="audio-player__pause"&gt;
          &lt;svg class="icon audio-player__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;svg class="icon audio-player__icon-pause audio-player__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;button class="audio-player__forward"&gt;
          &lt;svg class="icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
      &lt;/div&gt;
      &lt;div class="audio-player__time-bar"&gt;
        &lt;span class="audio-player__current-time"&gt;&lt;/span&gt;
        &lt;div class="audio-player__timeline-slider-container"&gt;
          &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
        &lt;/div&gt;
        &lt;span class="audio-player__duration-time"&gt;&lt;/span&gt;
        &lt;div class="audio-player__volume-content"&gt;
          &lt;button class="audio-player__volume"&gt;
            &lt;svg class="icon audio-player__volume-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

            &lt;svg class="icon audio-player__mute-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/button&gt;
          &lt;div class="audio-player__volume-slider-container"&gt;
            &lt;input class="volume__slider" max="100" tabindex="0" type="range" value="100" /&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Creators can also explore Lyria 3 on YouTube’s Dream Track. Available in the U.S. and now rolling out to YouTube creators in other countries, Lyria 3 will enhance the quality of each unique Shorts soundtrack. Whether it's creating a lyrical verse or a vibey backing track, being able to better customize the soundtrack will take creators’ Shorts to the next level.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;New audio verification capabilities&lt;/h2&gt;&lt;p&gt;All tracks generated in the Gemini app are embedded with SynthID, our imperceptible watermark for identifying Google AI-generated content. We are also giving you more tools to help identify AI content, broadening our verification capabilities in the Gemini app to include audio, along with image and video. Simply upload a file and ask if it was generated using Google AI, and Gemini will check for SynthID and use its own reasoning to return a response.&lt;/p&gt;&lt;h2&gt;Our commitment to developing generative AI responsibly&lt;/h2&gt;&lt;p&gt;Since we first launched Lyria in 2023, we've sought to develop this technology responsibly in collaboration with the music community. We've learned a lot through these collaborations and our experiments, like Music AI Sandbox, and have been very mindful of copyright and partner agreements as we've trained Lyria 3.&lt;/p&gt;&lt;p&gt;Music generation with Lyria 3 is designed for original expression, not for mimicking existing artists. If your prompt names a specific artist, Gemini will take this as broad creative inspiration and create a track that shares a similar style or mood. We also have filters in place to check outputs against existing content. We recognize that our approach might not be foolproof, so you can report content that may violate your rights or the rights of others. Additionally, in order to use our products, users must adhere to our Terms of Service and Gen AI prohibited use policies, which prohibit violations of others’ intellectual property and privacy rights.&lt;/p&gt;&lt;p&gt;Lyria 3 is available in the Gemini app for all users 18+ in English, German, Spanish, French, Hindi, Japanese, Korean and Portuguese, with plans to expand quality and coverage of more languages, rolling out on desktop today and to the mobile app over the next several days. And Google AI Plus, Pro and Ultra subscribers will enjoy higher limits.&lt;/p&gt;&lt;p&gt;Our goal with music generation in the Gemini app is to help you add a fun, custom soundtrack to your daily life. Try it out today at gemini.google.com.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini App


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/0217_KeywordHeaderFinalc.width-1300.png" /&gt;&lt;/div&gt;&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="A new way to express yourself: Gemini can now create music"&gt;
      &lt;source src="https://storage.googleapis.com/gweb-uniblog-publish-prod/media/tts_audio_83328_umbriel_2026_02_19_02_23_13.wav" type="audio/x-wav" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Since launching the Gemini app, we've built tools to encourage creative expression through images and video. Today, we're taking the next step: custom music generation. Lyria 3, Google DeepMind’s latest generative music model, is rolling out today in beta in the Gemini app. Just describe an idea or upload a photo, like “a comical R&amp;amp;B slow jam about a sock finding their match" and in a matter of seconds, Gemini will translate it into a high-quality, catchy track. To push the creative envelope further, you can even ask Gemini to take inspiration from something you upload.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Lyria 3 improves on audio generation from our Lyria models in three important ways:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;No need to provide your own lyrics! They'll be generated for you based on your prompt.&lt;/li&gt;&lt;li&gt;You have more creative control over elements like the style, vocals and tempo you want.&lt;/li&gt;&lt;li&gt;You can create more realistic and musically complex tracks.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Here’s how you can use it:&lt;br /&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Text to track:&lt;/b&gt; Describe a specific genre, mood, inside joke, or memory to create unique tracks with lyrics or instrumental audio that fits your vibe. &lt;i&gt;“I’m feeling nostalgic. Create a track for my mother about the great times we had as kids and the memories of her home cooked plantains. Make it a fun afrobeat track with a true African vibe.”&lt;/i&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;From photos and videos to track:&lt;/b&gt; Upload a photo or video and watch Gemini use the content to compose a track with lyrics that fit the mood perfectly. &lt;i&gt;“Use these photos to create a track about my dog Duncan on a hike in the woods.”&lt;/i&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The Gemini app creates 30-second tracks with custom cover art generated by Nano Banana. This makes it easy to quickly share with friends by downloading or simply clicking the share link. The goal of these tracks isn't to create a musical masterpiece, but rather to give you a fun, unique way to express yourself.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    











&lt;div class="audio-player"&gt;
  &lt;audio class="audio-player__player" title="Sweet Like Plantain"&gt;
    &lt;audio controls="controls"&gt;
&lt;source src="https://storage.googleapis.com/gweb-uniblog-publish-prod/media/Sweet_Like_Plantain_1.mp3" type="audio/mpeg" /&gt;
&lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
&lt;/audio&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player__container"&gt;
    &lt;div class="audio-player__text-content"&gt;
      &lt;div class="audio-player__content"&gt;
        &lt;span class="audio-player__content--title"&gt;Sweet Like Plantain&lt;/span&gt;
        
      &lt;/div&gt;
      &lt;button class="audio-player__preview-play"&gt;
        &lt;svg class="icon audio-player__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;span class="audio-player__duration"&gt;&lt;/span&gt;
      &lt;/button&gt;
      &lt;span class="audio-player__duration--large-viewport"&gt;&lt;/span&gt;
    &lt;/div&gt;
    &lt;div class="audio-player__console"&gt;
      &lt;div class="audio-player__start"&gt;
        &lt;button class="audio-player__replay"&gt;
          &lt;svg class="icon h-c-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;button class="audio-player__pause"&gt;
          &lt;svg class="icon audio-player__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;svg class="icon audio-player__icon-pause audio-player__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;button class="audio-player__forward"&gt;
          &lt;svg class="icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
      &lt;/div&gt;
      &lt;div class="audio-player__time-bar"&gt;
        &lt;span class="audio-player__current-time"&gt;&lt;/span&gt;
        &lt;div class="audio-player__timeline-slider-container"&gt;
          &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
        &lt;/div&gt;
        &lt;span class="audio-player__duration-time"&gt;&lt;/span&gt;
        &lt;div class="audio-player__volume-content"&gt;
          &lt;button class="audio-player__volume"&gt;
            &lt;svg class="icon audio-player__volume-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

            &lt;svg class="icon audio-player__mute-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/button&gt;
          &lt;div class="audio-player__volume-slider-container"&gt;
            &lt;input class="volume__slider" max="100" tabindex="0" type="range" value="100" /&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Creators can also explore Lyria 3 on YouTube’s Dream Track. Available in the U.S. and now rolling out to YouTube creators in other countries, Lyria 3 will enhance the quality of each unique Shorts soundtrack. Whether it's creating a lyrical verse or a vibey backing track, being able to better customize the soundtrack will take creators’ Shorts to the next level.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;New audio verification capabilities&lt;/h2&gt;&lt;p&gt;All tracks generated in the Gemini app are embedded with SynthID, our imperceptible watermark for identifying Google AI-generated content. We are also giving you more tools to help identify AI content, broadening our verification capabilities in the Gemini app to include audio, along with image and video. Simply upload a file and ask if it was generated using Google AI, and Gemini will check for SynthID and use its own reasoning to return a response.&lt;/p&gt;&lt;h2&gt;Our commitment to developing generative AI responsibly&lt;/h2&gt;&lt;p&gt;Since we first launched Lyria in 2023, we've sought to develop this technology responsibly in collaboration with the music community. We've learned a lot through these collaborations and our experiments, like Music AI Sandbox, and have been very mindful of copyright and partner agreements as we've trained Lyria 3.&lt;/p&gt;&lt;p&gt;Music generation with Lyria 3 is designed for original expression, not for mimicking existing artists. If your prompt names a specific artist, Gemini will take this as broad creative inspiration and create a track that shares a similar style or mood. We also have filters in place to check outputs against existing content. We recognize that our approach might not be foolproof, so you can report content that may violate your rights or the rights of others. Additionally, in order to use our products, users must adhere to our Terms of Service and Gen AI prohibited use policies, which prohibit violations of others’ intellectual property and privacy rights.&lt;/p&gt;&lt;p&gt;Lyria 3 is available in the Gemini app for all users 18+ in English, German, Spanish, French, Hindi, Japanese, Korean and Portuguese, with plans to expand quality and coverage of more languages, rolling out on desktop today and to the mobile app over the next several days. And Google AI Plus, Pro and Ultra subscribers will enjoy higher limits.&lt;/p&gt;&lt;p&gt;Our goal with music generation in the Gemini app is to help you add a fun, custom soundtrack to your daily life. Try it out today at gemini.google.com.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini App


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/blog/a-new-way-to-express-yourself-gemini-can-now-create-music/</guid><pubDate>Wed, 18 Feb 2026 16:01:38 +0000</pubDate></item><item><title>Project Silica’s advances in glass storage technology (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="A blue-to-green gradient background featuring three white icons: a networked globe on the left, a cloud in the center, and a stacked database on the right." class="wp-image-1162006" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Microsoft Research publishes&amp;nbsp;breakthrough&amp;nbsp;in&amp;nbsp;&lt;em&gt;Nature&lt;/em&gt;&amp;nbsp;on glass-based data storage that could preserve information for 10,000 years.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;New&amp;nbsp;technique extends technology from expensive fused silica to ordinary borosilicate glass found in kitchen cookware.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Innovations enable faster parallel writing, simplified readers (one camera instead of three), and easier manufacturing.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Phase voxel method requires only a single laser pulse, significantly reducing complexity and cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;Long-term preservation of digital information has long challenged archivists and datacenters, as magnetic tapes and hard drives degrade within decades. Existing archival storage solutions have limited media lifespans that make them less than ideal for preserving information for future generations.&lt;/p&gt;



&lt;p&gt;Now, we are excited to report significant progress on Project Silica&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, our effort to encode data in glass using femtosecond lasers, a technology that could preserve information for 10,000 years. Glass is a permanent data storage material that is resistant to water, heat, and dust.&lt;/p&gt;



&lt;p&gt;In findings published in &lt;em&gt;Nature&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;, we describe a breakthrough that extends the technology beyond expensive fused silica to ordinary borosilicate glass. A readily available and lower-cost medium, this is the same material found in kitchen cookware and oven doors. This advance addresses key barriers to commercialization: cost and availability of storage media. We have unlocked the science for parallel high-speed writing and developed a technique to permit accelerated aging tests on the written glass, suggesting that the data should remain intact for at least 10,000 years.&lt;/p&gt;



&lt;p&gt;Storing data inside glass with femtosecond&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; laser pulses is one of the few technologies on the horizon with the potential for durable, immutable, and long-lived storage. Although we have been leading innovation in this type of storage for years, prior to this research the technique only worked with pure fused silica glass, a type of glass that is relatively difficult to manufacture and available from only a few sources.&lt;/p&gt;



&lt;p&gt;In the paper, we show how data can be stored in borosilicate glass. The new technique stores hundreds of layers of data in glass only 2mm thin, as with previous methods, but with important improvements. The reader for the glass now needs only one camera, not three or four, reducing cost and size. In addition, the writing devices require fewer parts, making them easier to manufacture and calibrate, and enabling them to encode data more quickly.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;video series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;On Second Thought&lt;/h2&gt;
				
								&lt;p class="large" id="on-second-thought"&gt;A video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="key-scientific-discoveries"&gt;Key scientific discoveries&lt;/h2&gt;



&lt;p&gt;The &lt;em&gt;Nature&lt;/em&gt; paper details several key new scientific discoveries:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Advances in birefringent voxel&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; writing&lt;/strong&gt;: For the previous type of data storage in fused silica glass using birefringent (i.e., polarization) voxels, we developed a technique to reduce the number of pulses used to form the voxel from many to only two, critically showing that the polarization of the first pulse is not important to the polarization of the voxel formed. We further developed this to enable pseudo-single-pulse writing, in which a single pulse can be split after its polarization is set to simultaneously form the first pulse for one voxel (where the polarization doesn’t matter) and the second pulse of another (where the set polarization is essential). We demonstrated how to use this pseudo-single-pulse writing to enable fast writing with beam scanning across the media.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Phase voxels, a new storage method&lt;/strong&gt;: We invented a new type of data storage in glass called phase voxels, in which the phase change of the glass is modified instead of its polarization, showing that only a single pulse is necessary to make a phase voxel. We demonstrated that these phase voxels can also be formed in borosilicate glass and devised a technique to read the phase information from phase voxels encoded in this material. We showed that the much higher levels of three-dimensional inter-symbol interference in phase voxels can be mitigated with a machine learning classification model.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Parallel writing capabilities&lt;/strong&gt;: By combining a mathematical model of pre-heating and post-heating within the glass with the invention of a multi-beam delivery system, we showed that many data voxels can be written in proximity in the glass at the same time, significantly increasing writing speed. We explained a method for using light emissions (a side effect of voxel formation) for both static calibration and dynamic control to fully support automatic writing operations.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Optimization and longevity testing&lt;/strong&gt;: We developed a new way to optimize symbol encodings using machine learning and a better way to understand the tradeoff between error rates, error protection, and error recovery when evaluating new digital storage systems. We also created a new nondestructive optical method&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to identify the aging of data storage voxels within the glass, using this and standard accelerated aging techniques to support data lasting 10,000 years. We extended the industry standard Gray codes to apply to nonpower-of-two numbers of symbols.&lt;/p&gt;


&lt;div class="wp-block-msr-cards msr-cards msr-cards--carousel mt-4 has-text-align-left has-cards"&gt;
	&lt;div class="msr-cards__inner"&gt;
		
					&lt;div class="mt-4"&gt;
				&lt;div class="row"&gt;
	&lt;div class="col-12 px-0 px-md-g"&gt;
		&lt;section&gt;
			
			&lt;div class="carousel slide carousel-content-cards carousel-sneak-peek"&gt;
				
					Skip slideshow for: 				
				&lt;div&gt;
					&lt;div class="carousel-controls"&gt;
						&lt;button class="carousel-control-prev " type="button"&gt;
							&lt;span class="sr-only"&gt;Previous slide&lt;/span&gt;
						&lt;/button&gt;

						&lt;ol class="carousel-indicators"&gt;
															&lt;li class="active"&gt;&lt;/li&gt;
															&lt;li class="class"&gt;&lt;/li&gt;
															&lt;li class="class"&gt;&lt;/li&gt;
															&lt;li class="class"&gt;&lt;/li&gt;
													&lt;/ol&gt;

						&lt;button class="carousel-control-next " type="button"&gt;
							&lt;span class="sr-only"&gt;Previous slide&lt;/span&gt;
						&lt;/button&gt;
					&lt;/div&gt;

					&lt;div class="carousel-inner"&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel active"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | Microsoft Azure - vertical color bars in varying shades of blue" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;A piece of Project Silica media written with data.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | photo of lab testing set up" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;A research-grade Writer used to set the record for high speed data writing into glass.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | photo of lab testing set up" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;A research-grade Reader for retrieving data from glass.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | photo of lab testing set up" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;Close up of Writer showing high-speed multi-beam data encoding on laser pulses.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
											&lt;/div&gt;
				&lt;/div&gt;
			&lt;/div&gt;
		&lt;/section&gt;
		
			End of slideshow for: 		
	&lt;/div&gt;
&lt;/div&gt;
			&lt;/div&gt;
		
			&lt;/div&gt;
&lt;/div&gt;



&lt;h2 class="wp-block-heading" id="demonstrating-the-technology"&gt;Demonstrating the technology&lt;/h2&gt;



&lt;p&gt;As a research initiative, Project Silica has demonstrated these advances through several proofs of concept, including storing Warner Bros.’ “Superman” movie on quartz glass&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, partnering with Global Music Vault&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to preserve music under ice for 10,000 years&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and working with students on a “Golden Record 2.0” project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a digitally curated archive of images, sounds, music, and spoken language, crowdsourced to represent and preserve humanity’s diversity for millennia.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead"&gt;Looking ahead&lt;/h2&gt;



&lt;p&gt;The research phase is now complete, and we are continuing to consider learnings from Project Silica as we explore the ongoing need for sustainable, long-term preservation of digital information. We have added this paper to our published works so that others can build on them.&lt;/p&gt;







&lt;p&gt;Project Silica has made scientific advances across multiple areas beyond laser direct writing (LDW) in glass, including archival storage systems design, archival workload analysis, datacenter robotics, erasure coding, free-space optical components, and machine learning-based methods for symbol decoding in storage systems. Many of these innovations were described in our ACM Transactions on Storage publication&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; in 2025.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="A blue-to-green gradient background featuring three white icons: a networked globe on the left, a cloud in the center, and a stacked database on the right." class="wp-image-1162006" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;div class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section"&gt;
	
	&lt;div class="container"&gt;
		&lt;div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow"&gt;
			&lt;div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex"&gt;
&lt;div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow"&gt;
&lt;h2 class="wp-block-heading h3" id="at-a-glance"&gt;At a glance&lt;/h2&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;Microsoft Research publishes&amp;nbsp;breakthrough&amp;nbsp;in&amp;nbsp;&lt;em&gt;Nature&lt;/em&gt;&amp;nbsp;on glass-based data storage that could preserve information for 10,000 years.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;New&amp;nbsp;technique extends technology from expensive fused silica to ordinary borosilicate glass found in kitchen cookware.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Innovations enable faster parallel writing, simplified readers (one camera instead of three), and easier manufacturing.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Phase voxel method requires only a single laser pulse, significantly reducing complexity and cost.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;		&lt;/div&gt;
	&lt;/div&gt;

	&lt;/div&gt;



&lt;p&gt;Long-term preservation of digital information has long challenged archivists and datacenters, as magnetic tapes and hard drives degrade within decades. Existing archival storage solutions have limited media lifespans that make them less than ideal for preserving information for future generations.&lt;/p&gt;



&lt;p&gt;Now, we are excited to report significant progress on Project Silica&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, our effort to encode data in glass using femtosecond lasers, a technology that could preserve information for 10,000 years. Glass is a permanent data storage material that is resistant to water, heat, and dust.&lt;/p&gt;



&lt;p&gt;In findings published in &lt;em&gt;Nature&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/em&gt;, we describe a breakthrough that extends the technology beyond expensive fused silica to ordinary borosilicate glass. A readily available and lower-cost medium, this is the same material found in kitchen cookware and oven doors. This advance addresses key barriers to commercialization: cost and availability of storage media. We have unlocked the science for parallel high-speed writing and developed a technique to permit accelerated aging tests on the written glass, suggesting that the data should remain intact for at least 10,000 years.&lt;/p&gt;



&lt;p&gt;Storing data inside glass with femtosecond&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; laser pulses is one of the few technologies on the horizon with the potential for durable, immutable, and long-lived storage. Although we have been leading innovation in this type of storage for years, prior to this research the technique only worked with pure fused silica glass, a type of glass that is relatively difficult to manufacture and available from only a few sources.&lt;/p&gt;



&lt;p&gt;In the paper, we show how data can be stored in borosilicate glass. The new technique stores hundreds of layers of data in glass only 2mm thin, as with previous methods, but with important improvements. The reader for the glass now needs only one camera, not three or four, reducing cost and size. In addition, the writing devices require fewer parts, making them easier to manufacture and calibrate, and enabling them to encode data more quickly.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;video series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;On Second Thought&lt;/h2&gt;
				
								&lt;p class="large" id="on-second-thought"&gt;A video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="key-scientific-discoveries"&gt;Key scientific discoveries&lt;/h2&gt;



&lt;p&gt;The &lt;em&gt;Nature&lt;/em&gt; paper details several key new scientific discoveries:&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Advances in birefringent voxel&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; writing&lt;/strong&gt;: For the previous type of data storage in fused silica glass using birefringent (i.e., polarization) voxels, we developed a technique to reduce the number of pulses used to form the voxel from many to only two, critically showing that the polarization of the first pulse is not important to the polarization of the voxel formed. We further developed this to enable pseudo-single-pulse writing, in which a single pulse can be split after its polarization is set to simultaneously form the first pulse for one voxel (where the polarization doesn’t matter) and the second pulse of another (where the set polarization is essential). We demonstrated how to use this pseudo-single-pulse writing to enable fast writing with beam scanning across the media.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Phase voxels, a new storage method&lt;/strong&gt;: We invented a new type of data storage in glass called phase voxels, in which the phase change of the glass is modified instead of its polarization, showing that only a single pulse is necessary to make a phase voxel. We demonstrated that these phase voxels can also be formed in borosilicate glass and devised a technique to read the phase information from phase voxels encoded in this material. We showed that the much higher levels of three-dimensional inter-symbol interference in phase voxels can be mitigated with a machine learning classification model.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Parallel writing capabilities&lt;/strong&gt;: By combining a mathematical model of pre-heating and post-heating within the glass with the invention of a multi-beam delivery system, we showed that many data voxels can be written in proximity in the glass at the same time, significantly increasing writing speed. We explained a method for using light emissions (a side effect of voxel formation) for both static calibration and dynamic control to fully support automatic writing operations.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Optimization and longevity testing&lt;/strong&gt;: We developed a new way to optimize symbol encodings using machine learning and a better way to understand the tradeoff between error rates, error protection, and error recovery when evaluating new digital storage systems. We also created a new nondestructive optical method&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to identify the aging of data storage voxels within the glass, using this and standard accelerated aging techniques to support data lasting 10,000 years. We extended the industry standard Gray codes to apply to nonpower-of-two numbers of symbols.&lt;/p&gt;


&lt;div class="wp-block-msr-cards msr-cards msr-cards--carousel mt-4 has-text-align-left has-cards"&gt;
	&lt;div class="msr-cards__inner"&gt;
		
					&lt;div class="mt-4"&gt;
				&lt;div class="row"&gt;
	&lt;div class="col-12 px-0 px-md-g"&gt;
		&lt;section&gt;
			
			&lt;div class="carousel slide carousel-content-cards carousel-sneak-peek"&gt;
				
					Skip slideshow for: 				
				&lt;div&gt;
					&lt;div class="carousel-controls"&gt;
						&lt;button class="carousel-control-prev " type="button"&gt;
							&lt;span class="sr-only"&gt;Previous slide&lt;/span&gt;
						&lt;/button&gt;

						&lt;ol class="carousel-indicators"&gt;
															&lt;li class="active"&gt;&lt;/li&gt;
															&lt;li class="class"&gt;&lt;/li&gt;
															&lt;li class="class"&gt;&lt;/li&gt;
															&lt;li class="class"&gt;&lt;/li&gt;
													&lt;/ol&gt;

						&lt;button class="carousel-control-next " type="button"&gt;
							&lt;span class="sr-only"&gt;Previous slide&lt;/span&gt;
						&lt;/button&gt;
					&lt;/div&gt;

					&lt;div class="carousel-inner"&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel active"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | Microsoft Azure - vertical color bars in varying shades of blue" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;A piece of Project Silica media written with data.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | photo of lab testing set up" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;A research-grade Writer used to set the record for high speed data writing into glass.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | photo of lab testing set up" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;A research-grade Reader for retrieving data from glass.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
													&lt;section class="carousel-item msr-cards__card msr-cards__card--carousel"&gt;

	&lt;div class="card material-card h-100 p-0"&gt;

					&lt;div class="embed-responsive embed-responsive-16by9"&gt;
				&lt;img alt="Project Silica | Nature | photo of lab testing set up" class="card-img embed-responsive-item img-object-cover" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-1024x683.jpg" width="1024" /&gt;			&lt;/div&gt;
		
		&lt;div class="card-body px-4 px-lg-5 pt-4"&gt;
										&lt;div class="card__description mb-3"&gt;
					&lt;p&gt;Close up of Writer showing high-speed multi-beam data encoding on laser pulses.&lt;/p&gt;				&lt;/div&gt;
					&lt;/div&gt;

			&lt;/div&gt;
&lt;/section&gt;
											&lt;/div&gt;
				&lt;/div&gt;
			&lt;/div&gt;
		&lt;/section&gt;
		
			End of slideshow for: 		
	&lt;/div&gt;
&lt;/div&gt;
			&lt;/div&gt;
		
			&lt;/div&gt;
&lt;/div&gt;



&lt;h2 class="wp-block-heading" id="demonstrating-the-technology"&gt;Demonstrating the technology&lt;/h2&gt;



&lt;p&gt;As a research initiative, Project Silica has demonstrated these advances through several proofs of concept, including storing Warner Bros.’ “Superman” movie on quartz glass&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, partnering with Global Music Vault&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; to preserve music under ice for 10,000 years&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and working with students on a “Golden Record 2.0” project&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, a digitally curated archive of images, sounds, music, and spoken language, crowdsourced to represent and preserve humanity’s diversity for millennia.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead"&gt;Looking ahead&lt;/h2&gt;



&lt;p&gt;The research phase is now complete, and we are continuing to consider learnings from Project Silica as we explore the ongoing need for sustainable, long-term preservation of digital information. We have added this paper to our published works so that others can build on them.&lt;/p&gt;







&lt;p&gt;Project Silica has made scientific advances across multiple areas beyond laser direct writing (LDW) in glass, including archival storage systems design, archival workload analysis, datacenter robotics, erasure coding, free-space optical components, and machine learning-based methods for symbol decoding in storage systems. Many of these innovations were described in our ACM Transactions on Storage publication&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; in 2025.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/</guid><pubDate>Wed, 18 Feb 2026 16:11:45 +0000</pubDate></item><item><title>IBM and UC Berkeley Diagnose Why Enterprise Agents Fail Using IT-Bench and MAST (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ibm-research/itbenchandmast</link><description>&lt;!-- HTML_TAG_START --&gt;
Ayhan Sebin
Saurabh Jha
Rohan Arora
Daby Sow
Mert Cemri
Melissa Pan
Ion Stoica
&lt;p&gt;ITBench HF Space
ITBench HF Dataset
MAST HF Dataset
ITBench Github
MAST Github&lt;/p&gt;
&lt;p&gt;IBM Research and UC Berkeley collaborated to study how agentic LLM systems break in real-world IT automation, for tasks involving incident triage, logs/metrics queries, and Kubernetes actions in long-horizon tool loops.&lt;/p&gt;
&lt;p&gt;Benchmarks typically reduce performance to a single number, telling you whether an agent failed but never why. To solve this black-box problem, we applied MAST (Multi-Agent System Failure Taxonomy), an emerging practice for diagnosing agentic reliability ). By leveraging MAST to analyze ITBench—the industry benchmark for SRE, Security, and FinOps automation—we turned raw execution traces into structured failure signatures, revealing exactly what broke and how to fix it. We annotated 310 ITBench SRE traces across three distinct model classes: Gemini-3-Flash, Kimi-K2, and GPT-OSS-120B.&lt;/p&gt;
&lt;p&gt;Key Findings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frontier models like Gemini-3-Flash fail cleanly (2.6 failure modes/trace), typically hitting isolated bottlenecks like verification. Large open models like GPT-OSS-120B suffer from cascading failure modes (5.3 failure modes/trace). -A single reasoning mismatch early in the run poisons the context, leading to compounding hallucinations.&lt;/li&gt;
&lt;li&gt;Across all models, the strongest predictor of failure is FM-3.3 (Incorrect Verification). Agents consistently "declare victory" without checking ground truth. &lt;/li&gt;
&lt;li&gt;Kimi-K2 struggles to recognize when a task is done. It exhibits a massive spike in Premature Termination (+46%) and Unaware of Termination Conditions (+43%), often quitting just before solving the problem or looping indefinitely.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Takeaways from our analysis when building agents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For Frontier Models like Gemini: Externalize Verification. Never let the LLM grade its own homework. Require hard tool evidence before exit.&lt;/li&gt;
&lt;li&gt;Put termination + loop control outside the model: Termination issues are common killers (FM-1.5). Add explicit stop conditions + loop detectors for repeated tool calls/actions or implement Finite State Machines.&lt;/li&gt;
&lt;li&gt;Force clarify-or-read-only when inputs are ambiguous: Clarification failures (FM-2.2) are a major failure driver for smaller models. Make ambiguity a first-class branch in your agent graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re building agents for enterprise IT workflows, this is the kind of evaluation you want: not just “did it pass?”, but “what broke, where, and what intervention is most leverageable?”&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The "Black Box" Problem of Agent Benchmarks
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Benchmarks like &lt;strong&gt;ITBench&lt;/strong&gt; are becoming the standard for measuring agentic performance in high-stakes IT automation tasks. In ITBench, agents act as Site Reliability Engineers (SREs) or Security Analysts tasked with diagnosing Kubernetes outages, patching vulnerabilities, or managing cloud costs in production environments.&lt;/p&gt;
&lt;p&gt;This benchmarks use success rate as a main metric to evaluate agents. However, this metric is insufficient for engineering robust systems. Knowing that an agentic system achieves a 14% success rate on ITBench tells us &lt;em&gt;that&lt;/em&gt; it failed, but not why: &lt;strong&gt;Did it fail because it forgot the context? Because it hallucinated a command? Or because it simply did not terminate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Without a comprehensive approach to diagnose these failures, developers are left guessing, often resorting to blind prompting tweaks that solve one problem only to create another.&lt;/p&gt;
&lt;p&gt;As a new standard to analyze the failure modes of complex agentic systems, we developed &lt;strong&gt;MAST (Multi-Agent System Failure Taxonomy)&lt;/strong&gt;. MAST brings more insights and open up the opaque evaluation of these benchmarks. Derived from a rigorous analysis of over 1,600 traces across seven different frameworks, MAST provides a standardized taxonomy for agent failures.&lt;/p&gt;
&lt;p&gt;MAST converts unstructured execution logs into structured "&lt;em&gt;failure vectors&lt;/em&gt;" based on 14 distinct patterns across three key categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FC1: System Design Issues&lt;/strong&gt; (The "Skeleton")&lt;ul&gt;
&lt;li&gt;Failures here stem from the agent's architecture and role definition.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Examples:&lt;/em&gt; &lt;strong&gt;FM-1.3 Step Repetition&lt;/strong&gt; (looping), &lt;strong&gt;FM-1.4 Loss of Conversation History&lt;/strong&gt; (memory leaks), &lt;strong&gt;FM-1.5 Unaware of Termination&lt;/strong&gt; (failing to stop).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FC2: Inter-Agent Misalignment&lt;/strong&gt; (The "Communication")&lt;ul&gt;
&lt;li&gt;Failures arising during runtime from how agents talk to each other or the environment.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Examples:&lt;/em&gt; &lt;strong&gt;FM-2.2 Fail to Ask for Clarification&lt;/strong&gt; (assuming instead of asking), &lt;strong&gt;FM-2.3 Task Derailment&lt;/strong&gt; (going off-topic).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FC3: Task Verification&lt;/strong&gt; (The "Quality Control")&lt;ul&gt;
&lt;li&gt;Failures in quality assurance of the agents' output.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Examples:&lt;/em&gt; &lt;strong&gt;FM-3.1 Premature Termination&lt;/strong&gt; (giving up too soon), &lt;strong&gt;FM-3.3 Incorrect Verification&lt;/strong&gt; (hallucinating success).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="MAST ANALYSIS" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/e4z2uA2pkASgWJdqdGJZl.webp" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The Experiment: Diagnosing ITBench Agents
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We stress-test the idea of using MAST to make agent evaluations actionable and gain insights on the failure modes by applying it to ITBench, a popular evaluation suite for IT automation tasks across &lt;strong&gt;SRE&lt;/strong&gt;, &lt;strong&gt;Security/Compliance&lt;/strong&gt;, and &lt;strong&gt;FinOps&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We annotated 310 ITBench SRE execution traces produced by an SRE agent built with Codex in realistic environments. These traces capture natural language interactions between agents and their tools across three models representing different capability tiers: Gemini-3-Flash, Kimi-K2, and GPT-OSS-120B. This lets us look past simple success metrics and investigate the distinct failure signatures driving these results. For this we use the recall scores, as the models by design only output a maximum of 3-5 outputs and SREs prefer the recall scores over F-1 score.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gemini-3-Flash:&lt;/strong&gt; 100 traces (75.5% Mean Recall)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kimi-K2:&lt;/strong&gt; 105 traces (28.6% Mean Recall)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-OSS-120B:&lt;/strong&gt; 105 traces (12.4% Mean Recall)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below, we detail the findings from this diagnostic analysis.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Finding 1: Stronger models like Gemini-3-Flash shows surgical (isolated failure modes) per trace whereas open sourced Kimi-K2 and GPT-oss-120b show compounding failure patterns
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When we examine the failed traces, a clear hierarchy of complexity becomes apparent across the three models. This is measured by the number of distinct failure modes observed per failed run.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gemini-3-Flash:&lt;/strong&gt;&amp;nbsp;2.6 failure modes per failed trace&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kimi-K2:&lt;/strong&gt;&amp;nbsp;4.7 failure modes per failed trace&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-OSS-120B:&lt;/strong&gt;&amp;nbsp;5.3 failure modes per failed trace&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This disparity in failure mode density reveals a fundamental difference in how these systems break down. Gemini-3-Flash exhibits a surgical failure profile. Even in unsuccessful runs, it maintains high internal coherence&amp;nbsp;and typically fails due to a single isolated failure, such as an incorrect verification step. These failures are precise and far easier to diagnose.&lt;/p&gt;
&lt;p&gt;On the opposite end of the spectrum, GPT-OSS-120B suffers from cascading collapse. In these&amp;nbsp;traces, we observe that errors tend to compound over time. A small reasoning mismatch&amp;nbsp;early in the process often leads to a deviation from the task specification, which in turn&amp;nbsp;triggers a total derailment of the agent. Kimi-K2 represents the&amp;nbsp;middle ground, where failures are more frequent and complex than the frontier model but do&amp;nbsp;not reach the systemic instability seen in the 120B open weights model.&lt;/p&gt;
&lt;p&gt;The significance of this finding is that a higher success rate is often accompanied&amp;nbsp;by isolated failure. Systems that fail with fewer simultaneous problems are far more predictable and&amp;nbsp;simpler to improve through targeted engineering interventions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="failure mode" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/7OtWv_RJ1BZj8n3ChhgUz.webp" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Finding 2: "Non-Fatal" vs. "Fatal" Failures
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Perhaps the most critical insight from MAST is distinguishing between failures that the system can &lt;em&gt;tolerate&lt;/em&gt; versus those that are fatal to success of the downstream task. By comparing the distribution of failure modes in &lt;strong&gt;Successful Traces&lt;/strong&gt; vs. &lt;strong&gt;Failed Traces&lt;/strong&gt;, we can classify them into three categories.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The "Non-Fatal" (Benign) Flaws
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Across all three models, certain failure modes appear frequently even in runs that ultimately succeed. These are often structural frictions rather than terminal bugs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FM-1.3 Step Repetition:&lt;/strong&gt;&amp;nbsp;This mode is present in over 90 percent of successful Kimi-K2 runs. In the SRE domain, iteration is often a necessity. An agent might query the&amp;nbsp;same metric multiple times to verify if a service is stabilizing or if a fix has taken&amp;nbsp;effect. Gemini-3-Flash actually shows less repetition in its failed traces, suggesting&amp;nbsp;that it sometimes fails because it does not iterate enough.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-1.1 Disobey Task Specification:&lt;/strong&gt;&amp;nbsp;Agents frequently deviate from strict tool formatting or sequential&amp;nbsp;instructions yet still manage to identify the correct root cause.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This separation is where MAST proves&amp;nbsp;its value. It allows us to ignore the bening failures like repetition that often occurs in troubleshooting, and focus instead on fatal failures that killed a run.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The "Fatal" Flaws
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Certain behaviors strongly separate success from failure. When these modes appear, the probability of a successful outcome drops precipitously. The most&amp;nbsp;prominent example is&amp;nbsp;&lt;strong&gt;FM-3.3 (Incorrect Verification)&lt;/strong&gt;. This mode&amp;nbsp;shows a 52 percent increase in failed Gemini-3-Flash traces compared to its&amp;nbsp;successful ones. Other prominent failure modes are 1.5 (Unaware of Termination Conditions) and 2.6 (Reasoning Action Mismatch).&lt;/p&gt;
&lt;p&gt;If these happen, the run is likely dead; guiding practitioners to develop robust context management strategies across agents in the system and multiple turns of interactions.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Case Study: Gemini-3-Flash&amp;nbsp;(Decisive but Overconfident)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Gemini-3-Flash is highly efficient, but its primary bottleneck is its tendency to assume success without rigorous proof. Its failure signature is dominated by a massive delta in verification errors. It often identifies the correct signals but terminates before cross-referencing them against the ground truth. To fix this, developers should implement an external verification gate. By requiring tool-based evidence like a cleared alert or a healthy metric threshold before allowing the agent to exit, we can mitigate this model’s inherent overconfidence.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fix:&lt;/strong&gt; To improve Gemini-3-Flash on ITBench, prompt engineering won't help much. In particular, the experiments we shown in our NeurIPS 2025 paper shows that with manual interventions like prompt engineering for memory related failures, we can get only up to around 15.6% performance improvements, whereas in a previous blogpost on MAST, we showed that by introducing new agents such as a &lt;strong&gt;Summarizer Agent&lt;/strong&gt; to remind the other agents of what is going on and continuously augment their state (fixing FM-1.4) or by introducing context management mechanisms (such as a stricter &lt;strong&gt;State Machine&lt;/strong&gt; to enforce termination to fix FM-1.5), we can get up to 53% performance improvement as these tackle more fundamental issues with the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="gemini 3" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/TIC3M-6ZyIJhRuWdv-Zvr.webp" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Case Study: Kimi-K2 (The Termination Crisis)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While termination confusion&amp;nbsp;(FM-3.1 and FM-1.5) is the prevalent failure mode&amp;nbsp;for Kimi-K2, its failed trajectories are defined by a pervasive&amp;nbsp;&lt;strong&gt;Action-Reasoning Mismatch (FM-2.6)&lt;/strong&gt;, which is present in a&amp;nbsp;staggering&amp;nbsp;&lt;strong&gt;92% of its failures&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The Execution Gap:&lt;/strong&gt;&amp;nbsp;While parts of its internal reasoning are often accurate, it suffers from a 92 percent failure prevalence of&amp;nbsp;&lt;strong&gt;FM-2.6 (Action-Reasoning Mismatch)&lt;/strong&gt;. It frequently identifies the correct next step but then executes a redundant or irrelevant command.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Meta-Loop Trap:&lt;/strong&gt;&amp;nbsp;Roughly 25 percent of failed traces involve&amp;nbsp;&lt;strong&gt;FM-2.3 (Task Derailment)&lt;/strong&gt;. When a tool call returns a minor error, the agent often abandons the primary incident to enter a cycle of debugging its own investigation scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kimi-K2 is a good example of an overthinking model, its reasoning chains are often too long but can fail at execution.&lt;/p&gt;
&lt;p&gt;&lt;img alt="kimi2" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/oiqI0JpzhJIzT8WU8YPRM.webp" /&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Case Study: &lt;strong&gt;GPT-OSS-120B&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;GPT-OSS-120B exhibits the most unstable failure signature of the cohort. This model exhibits an average of 5.3 distinct failure modes per failed trace, indicating a fundamental inability to maintain internal state.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Loss of Conversation History (FM-1.4):&lt;/strong&gt;&amp;nbsp;This is a unique fatal flaw for the 120B&amp;nbsp;model. It loses conversation history in&amp;nbsp;&lt;strong&gt;24%&lt;/strong&gt;&amp;nbsp;of traces, whereas Gemini-3-Flash exhibited zero memory loss and Kimi-K2 only 7%. As&amp;nbsp;SRE traces grow in length, GPT-OSS-120B effectively&amp;nbsp;"forgets" the alerts it was originally triaging, leading to total task&amp;nbsp;derailment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning Disconnect (FM-2.6):&lt;/strong&gt;&amp;nbsp;A staggering&amp;nbsp;&lt;strong&gt;94%&lt;/strong&gt;&amp;nbsp;of traces show a decoupling of reasoning and&amp;nbsp;action. It is nearly 3x more likely than Gemini (31%) to&amp;nbsp;describe a correct plan but then execute a completely unrelated or redundant tool call.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="OSS" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/i179ZOT-r6et7kC23J0oy.webp" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A different (and more useful) way to read the plots: “fatal” vs “non-fatal”
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In summary, MAST lets you split failure modes into two buckets:&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Recoverable / structural (show up even in successful traces)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;These are failures which are not fatal and from which the system can recover to successfully complete the task.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FM-1.3 Step repetition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-3.3 Incorrect verification&lt;/strong&gt; (important nuance: the system &lt;em&gt;does&lt;/em&gt; verify; it just verifies poorly)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-2.6 Reasoning–action mismatch&lt;/strong&gt; (often present, but not always decisive)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Fatal / decisive (strongly associated with failed traces)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;These are failures from which the system typically cannot recover.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FM-1.5 Unaware of termination conditions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-3.1 Premature termination&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-1.4 Loss of conversation history&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-2.3 Task derailment&lt;/strong&gt; (rare but extremely diagnostic when it appears)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-2.2 Fail to ask for clarification&lt;/strong&gt; (especially for Granite/Llama regimes)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the “richer understanding” piece: &lt;strong&gt;two models can have the same success rate on a small slice, yet fail for entirely different reasons—requiring different fixes.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;MAST is a tool that inspects the agentic system traces to identify fine-grain failure types that support system development and debugging. In this blog, we show that by applying MAST to ITBench, we move from generic observations ("Open models struggle") to a concrete engineering roadmap that help improving the performance of agentic systems relying on thse models, e.g.:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;For Gemini-3-Flash:&lt;/strong&gt; &amp;nbsp;Verification failure (&lt;strong&gt;FM-3.3&lt;/strong&gt;) is the most common fatal failure for surgical models. Never allow an agent to self-terminate; require hard, tool-mediated evidence (e.g., AlertManager clearance or K8s state changes) before a run is considered successful.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Kimi-K2:&lt;/strong&gt; Use a deterministic state machine to fix the model's frequent struggle with recognizing task completion. This model’s reasoning chains can be too long and struggle to terminate, so it might benefit significantly from a tighter control on when to end.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For GPT-oss-120b:&lt;/strong&gt; Systemic collapse occurs when minor reasoning mismatches (&lt;strong&gt;FM-2.6&lt;/strong&gt;) poison the task history. Implement aggressive context hygiene and early error detection to ensure that small misalignment's do not compound into total derailment.&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;!-- HTML_TAG_START --&gt;
Ayhan Sebin
Saurabh Jha
Rohan Arora
Daby Sow
Mert Cemri
Melissa Pan
Ion Stoica
&lt;p&gt;ITBench HF Space
ITBench HF Dataset
MAST HF Dataset
ITBench Github
MAST Github&lt;/p&gt;
&lt;p&gt;IBM Research and UC Berkeley collaborated to study how agentic LLM systems break in real-world IT automation, for tasks involving incident triage, logs/metrics queries, and Kubernetes actions in long-horizon tool loops.&lt;/p&gt;
&lt;p&gt;Benchmarks typically reduce performance to a single number, telling you whether an agent failed but never why. To solve this black-box problem, we applied MAST (Multi-Agent System Failure Taxonomy), an emerging practice for diagnosing agentic reliability ). By leveraging MAST to analyze ITBench—the industry benchmark for SRE, Security, and FinOps automation—we turned raw execution traces into structured failure signatures, revealing exactly what broke and how to fix it. We annotated 310 ITBench SRE traces across three distinct model classes: Gemini-3-Flash, Kimi-K2, and GPT-OSS-120B.&lt;/p&gt;
&lt;p&gt;Key Findings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Frontier models like Gemini-3-Flash fail cleanly (2.6 failure modes/trace), typically hitting isolated bottlenecks like verification. Large open models like GPT-OSS-120B suffer from cascading failure modes (5.3 failure modes/trace). -A single reasoning mismatch early in the run poisons the context, leading to compounding hallucinations.&lt;/li&gt;
&lt;li&gt;Across all models, the strongest predictor of failure is FM-3.3 (Incorrect Verification). Agents consistently "declare victory" without checking ground truth. &lt;/li&gt;
&lt;li&gt;Kimi-K2 struggles to recognize when a task is done. It exhibits a massive spike in Premature Termination (+46%) and Unaware of Termination Conditions (+43%), often quitting just before solving the problem or looping indefinitely.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Takeaways from our analysis when building agents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For Frontier Models like Gemini: Externalize Verification. Never let the LLM grade its own homework. Require hard tool evidence before exit.&lt;/li&gt;
&lt;li&gt;Put termination + loop control outside the model: Termination issues are common killers (FM-1.5). Add explicit stop conditions + loop detectors for repeated tool calls/actions or implement Finite State Machines.&lt;/li&gt;
&lt;li&gt;Force clarify-or-read-only when inputs are ambiguous: Clarification failures (FM-2.2) are a major failure driver for smaller models. Make ambiguity a first-class branch in your agent graph.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you’re building agents for enterprise IT workflows, this is the kind of evaluation you want: not just “did it pass?”, but “what broke, where, and what intervention is most leverageable?”&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The "Black Box" Problem of Agent Benchmarks
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Benchmarks like &lt;strong&gt;ITBench&lt;/strong&gt; are becoming the standard for measuring agentic performance in high-stakes IT automation tasks. In ITBench, agents act as Site Reliability Engineers (SREs) or Security Analysts tasked with diagnosing Kubernetes outages, patching vulnerabilities, or managing cloud costs in production environments.&lt;/p&gt;
&lt;p&gt;This benchmarks use success rate as a main metric to evaluate agents. However, this metric is insufficient for engineering robust systems. Knowing that an agentic system achieves a 14% success rate on ITBench tells us &lt;em&gt;that&lt;/em&gt; it failed, but not why: &lt;strong&gt;Did it fail because it forgot the context? Because it hallucinated a command? Or because it simply did not terminate?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Without a comprehensive approach to diagnose these failures, developers are left guessing, often resorting to blind prompting tweaks that solve one problem only to create another.&lt;/p&gt;
&lt;p&gt;As a new standard to analyze the failure modes of complex agentic systems, we developed &lt;strong&gt;MAST (Multi-Agent System Failure Taxonomy)&lt;/strong&gt;. MAST brings more insights and open up the opaque evaluation of these benchmarks. Derived from a rigorous analysis of over 1,600 traces across seven different frameworks, MAST provides a standardized taxonomy for agent failures.&lt;/p&gt;
&lt;p&gt;MAST converts unstructured execution logs into structured "&lt;em&gt;failure vectors&lt;/em&gt;" based on 14 distinct patterns across three key categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FC1: System Design Issues&lt;/strong&gt; (The "Skeleton")&lt;ul&gt;
&lt;li&gt;Failures here stem from the agent's architecture and role definition.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Examples:&lt;/em&gt; &lt;strong&gt;FM-1.3 Step Repetition&lt;/strong&gt; (looping), &lt;strong&gt;FM-1.4 Loss of Conversation History&lt;/strong&gt; (memory leaks), &lt;strong&gt;FM-1.5 Unaware of Termination&lt;/strong&gt; (failing to stop).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FC2: Inter-Agent Misalignment&lt;/strong&gt; (The "Communication")&lt;ul&gt;
&lt;li&gt;Failures arising during runtime from how agents talk to each other or the environment.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Examples:&lt;/em&gt; &lt;strong&gt;FM-2.2 Fail to Ask for Clarification&lt;/strong&gt; (assuming instead of asking), &lt;strong&gt;FM-2.3 Task Derailment&lt;/strong&gt; (going off-topic).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FC3: Task Verification&lt;/strong&gt; (The "Quality Control")&lt;ul&gt;
&lt;li&gt;Failures in quality assurance of the agents' output.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Examples:&lt;/em&gt; &lt;strong&gt;FM-3.1 Premature Termination&lt;/strong&gt; (giving up too soon), &lt;strong&gt;FM-3.3 Incorrect Verification&lt;/strong&gt; (hallucinating success).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="MAST ANALYSIS" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/e4z2uA2pkASgWJdqdGJZl.webp" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The Experiment: Diagnosing ITBench Agents
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;We stress-test the idea of using MAST to make agent evaluations actionable and gain insights on the failure modes by applying it to ITBench, a popular evaluation suite for IT automation tasks across &lt;strong&gt;SRE&lt;/strong&gt;, &lt;strong&gt;Security/Compliance&lt;/strong&gt;, and &lt;strong&gt;FinOps&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We annotated 310 ITBench SRE execution traces produced by an SRE agent built with Codex in realistic environments. These traces capture natural language interactions between agents and their tools across three models representing different capability tiers: Gemini-3-Flash, Kimi-K2, and GPT-OSS-120B. This lets us look past simple success metrics and investigate the distinct failure signatures driving these results. For this we use the recall scores, as the models by design only output a maximum of 3-5 outputs and SREs prefer the recall scores over F-1 score.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gemini-3-Flash:&lt;/strong&gt; 100 traces (75.5% Mean Recall)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kimi-K2:&lt;/strong&gt; 105 traces (28.6% Mean Recall)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-OSS-120B:&lt;/strong&gt; 105 traces (12.4% Mean Recall)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Below, we detail the findings from this diagnostic analysis.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Finding 1: Stronger models like Gemini-3-Flash shows surgical (isolated failure modes) per trace whereas open sourced Kimi-K2 and GPT-oss-120b show compounding failure patterns
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;When we examine the failed traces, a clear hierarchy of complexity becomes apparent across the three models. This is measured by the number of distinct failure modes observed per failed run.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gemini-3-Flash:&lt;/strong&gt;&amp;nbsp;2.6 failure modes per failed trace&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kimi-K2:&lt;/strong&gt;&amp;nbsp;4.7 failure modes per failed trace&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-OSS-120B:&lt;/strong&gt;&amp;nbsp;5.3 failure modes per failed trace&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This disparity in failure mode density reveals a fundamental difference in how these systems break down. Gemini-3-Flash exhibits a surgical failure profile. Even in unsuccessful runs, it maintains high internal coherence&amp;nbsp;and typically fails due to a single isolated failure, such as an incorrect verification step. These failures are precise and far easier to diagnose.&lt;/p&gt;
&lt;p&gt;On the opposite end of the spectrum, GPT-OSS-120B suffers from cascading collapse. In these&amp;nbsp;traces, we observe that errors tend to compound over time. A small reasoning mismatch&amp;nbsp;early in the process often leads to a deviation from the task specification, which in turn&amp;nbsp;triggers a total derailment of the agent. Kimi-K2 represents the&amp;nbsp;middle ground, where failures are more frequent and complex than the frontier model but do&amp;nbsp;not reach the systemic instability seen in the 120B open weights model.&lt;/p&gt;
&lt;p&gt;The significance of this finding is that a higher success rate is often accompanied&amp;nbsp;by isolated failure. Systems that fail with fewer simultaneous problems are far more predictable and&amp;nbsp;simpler to improve through targeted engineering interventions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="failure mode" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/7OtWv_RJ1BZj8n3ChhgUz.webp" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Finding 2: "Non-Fatal" vs. "Fatal" Failures
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Perhaps the most critical insight from MAST is distinguishing between failures that the system can &lt;em&gt;tolerate&lt;/em&gt; versus those that are fatal to success of the downstream task. By comparing the distribution of failure modes in &lt;strong&gt;Successful Traces&lt;/strong&gt; vs. &lt;strong&gt;Failed Traces&lt;/strong&gt;, we can classify them into three categories.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The "Non-Fatal" (Benign) Flaws
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Across all three models, certain failure modes appear frequently even in runs that ultimately succeed. These are often structural frictions rather than terminal bugs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FM-1.3 Step Repetition:&lt;/strong&gt;&amp;nbsp;This mode is present in over 90 percent of successful Kimi-K2 runs. In the SRE domain, iteration is often a necessity. An agent might query the&amp;nbsp;same metric multiple times to verify if a service is stabilizing or if a fix has taken&amp;nbsp;effect. Gemini-3-Flash actually shows less repetition in its failed traces, suggesting&amp;nbsp;that it sometimes fails because it does not iterate enough.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-1.1 Disobey Task Specification:&lt;/strong&gt;&amp;nbsp;Agents frequently deviate from strict tool formatting or sequential&amp;nbsp;instructions yet still manage to identify the correct root cause.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This separation is where MAST proves&amp;nbsp;its value. It allows us to ignore the bening failures like repetition that often occurs in troubleshooting, and focus instead on fatal failures that killed a run.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		The "Fatal" Flaws
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Certain behaviors strongly separate success from failure. When these modes appear, the probability of a successful outcome drops precipitously. The most&amp;nbsp;prominent example is&amp;nbsp;&lt;strong&gt;FM-3.3 (Incorrect Verification)&lt;/strong&gt;. This mode&amp;nbsp;shows a 52 percent increase in failed Gemini-3-Flash traces compared to its&amp;nbsp;successful ones. Other prominent failure modes are 1.5 (Unaware of Termination Conditions) and 2.6 (Reasoning Action Mismatch).&lt;/p&gt;
&lt;p&gt;If these happen, the run is likely dead; guiding practitioners to develop robust context management strategies across agents in the system and multiple turns of interactions.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Case Study: Gemini-3-Flash&amp;nbsp;(Decisive but Overconfident)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Gemini-3-Flash is highly efficient, but its primary bottleneck is its tendency to assume success without rigorous proof. Its failure signature is dominated by a massive delta in verification errors. It often identifies the correct signals but terminates before cross-referencing them against the ground truth. To fix this, developers should implement an external verification gate. By requiring tool-based evidence like a cleared alert or a healthy metric threshold before allowing the agent to exit, we can mitigate this model’s inherent overconfidence.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Fix:&lt;/strong&gt; To improve Gemini-3-Flash on ITBench, prompt engineering won't help much. In particular, the experiments we shown in our NeurIPS 2025 paper shows that with manual interventions like prompt engineering for memory related failures, we can get only up to around 15.6% performance improvements, whereas in a previous blogpost on MAST, we showed that by introducing new agents such as a &lt;strong&gt;Summarizer Agent&lt;/strong&gt; to remind the other agents of what is going on and continuously augment their state (fixing FM-1.4) or by introducing context management mechanisms (such as a stricter &lt;strong&gt;State Machine&lt;/strong&gt; to enforce termination to fix FM-1.5), we can get up to 53% performance improvement as these tackle more fundamental issues with the system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="gemini 3" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/TIC3M-6ZyIJhRuWdv-Zvr.webp" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Case Study: Kimi-K2 (The Termination Crisis)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While termination confusion&amp;nbsp;(FM-3.1 and FM-1.5) is the prevalent failure mode&amp;nbsp;for Kimi-K2, its failed trajectories are defined by a pervasive&amp;nbsp;&lt;strong&gt;Action-Reasoning Mismatch (FM-2.6)&lt;/strong&gt;, which is present in a&amp;nbsp;staggering&amp;nbsp;&lt;strong&gt;92% of its failures&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The Execution Gap:&lt;/strong&gt;&amp;nbsp;While parts of its internal reasoning are often accurate, it suffers from a 92 percent failure prevalence of&amp;nbsp;&lt;strong&gt;FM-2.6 (Action-Reasoning Mismatch)&lt;/strong&gt;. It frequently identifies the correct next step but then executes a redundant or irrelevant command.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The Meta-Loop Trap:&lt;/strong&gt;&amp;nbsp;Roughly 25 percent of failed traces involve&amp;nbsp;&lt;strong&gt;FM-2.3 (Task Derailment)&lt;/strong&gt;. When a tool call returns a minor error, the agent often abandons the primary incident to enter a cycle of debugging its own investigation scripts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kimi-K2 is a good example of an overthinking model, its reasoning chains are often too long but can fail at execution.&lt;/p&gt;
&lt;p&gt;&lt;img alt="kimi2" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/oiqI0JpzhJIzT8WU8YPRM.webp" /&gt;&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Case Study: &lt;strong&gt;GPT-OSS-120B&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;GPT-OSS-120B exhibits the most unstable failure signature of the cohort. This model exhibits an average of 5.3 distinct failure modes per failed trace, indicating a fundamental inability to maintain internal state.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Loss of Conversation History (FM-1.4):&lt;/strong&gt;&amp;nbsp;This is a unique fatal flaw for the 120B&amp;nbsp;model. It loses conversation history in&amp;nbsp;&lt;strong&gt;24%&lt;/strong&gt;&amp;nbsp;of traces, whereas Gemini-3-Flash exhibited zero memory loss and Kimi-K2 only 7%. As&amp;nbsp;SRE traces grow in length, GPT-OSS-120B effectively&amp;nbsp;"forgets" the alerts it was originally triaging, leading to total task&amp;nbsp;derailment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning Disconnect (FM-2.6):&lt;/strong&gt;&amp;nbsp;A staggering&amp;nbsp;&lt;strong&gt;94%&lt;/strong&gt;&amp;nbsp;of traces show a decoupling of reasoning and&amp;nbsp;action. It is nearly 3x more likely than Gemini (31%) to&amp;nbsp;describe a correct plan but then execute a completely unrelated or redundant tool call.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="OSS" src="https://cdn-uploads.huggingface.co/production/uploads/64e8143f6de557454220921e/i179ZOT-r6et7kC23J0oy.webp" /&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		A different (and more useful) way to read the plots: “fatal” vs “non-fatal”
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In summary, MAST lets you split failure modes into two buckets:&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Recoverable / structural (show up even in successful traces)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;These are failures which are not fatal and from which the system can recover to successfully complete the task.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FM-1.3 Step repetition&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-3.3 Incorrect verification&lt;/strong&gt; (important nuance: the system &lt;em&gt;does&lt;/em&gt; verify; it just verifies poorly)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-2.6 Reasoning–action mismatch&lt;/strong&gt; (often present, but not always decisive)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Fatal / decisive (strongly associated with failed traces)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;These are failures from which the system typically cannot recover.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FM-1.5 Unaware of termination conditions&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-3.1 Premature termination&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-1.4 Loss of conversation history&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-2.3 Task derailment&lt;/strong&gt; (rare but extremely diagnostic when it appears)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;FM-2.2 Fail to ask for clarification&lt;/strong&gt; (especially for Granite/Llama regimes)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the “richer understanding” piece: &lt;strong&gt;two models can have the same success rate on a small slice, yet fail for entirely different reasons—requiring different fixes.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Conclusion
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;MAST is a tool that inspects the agentic system traces to identify fine-grain failure types that support system development and debugging. In this blog, we show that by applying MAST to ITBench, we move from generic observations ("Open models struggle") to a concrete engineering roadmap that help improving the performance of agentic systems relying on thse models, e.g.:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;For Gemini-3-Flash:&lt;/strong&gt; &amp;nbsp;Verification failure (&lt;strong&gt;FM-3.3&lt;/strong&gt;) is the most common fatal failure for surgical models. Never allow an agent to self-terminate; require hard, tool-mediated evidence (e.g., AlertManager clearance or K8s state changes) before a run is considered successful.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For Kimi-K2:&lt;/strong&gt; Use a deterministic state machine to fix the model's frequent struggle with recognizing task completion. This model’s reasoning chains can be too long and struggle to terminate, so it might benefit significantly from a tighter control on when to end.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;For GPT-oss-120b:&lt;/strong&gt; Systemic collapse occurs when minor reasoning mismatches (&lt;strong&gt;FM-2.6&lt;/strong&gt;) poison the task history. Implement aggressive context hygiene and early error detection to ensure that small misalignment's do not compound into total derailment.&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ibm-research/itbenchandmast</guid><pubDate>Wed, 18 Feb 2026 16:15:45 +0000</pubDate></item><item><title>World Labs lands $1B, with $200M from Autodesk, to bring world models into 3D workflows (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/world-labs-lands-200m-from-autodesk-to-bring-world-models-into-3d-workflows/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Untitled-design.png?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Fei-Fei Li’s World Labs has secured a $200 million investment from software design giant Autodesk as part of a larger $1 billion round from backers, including AMD, Emerson Collective, Fidelity, Nvidia, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;World Labs, which emerged from stealth in 2024 with $230 million at a $1 billion valuation, declined to say whether the latest round boosted its valuation. However, reports a month ago suggested it was aiming to raise at a $5 billion valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The partnership between World Labs and Autodesk will see the two companies collaborating to explore how World Labs’ models — AI systems that can generate and reason about immersive 3D environments — can work alongside Autodesk’s tools, and vice versa, starting with a focus on entertainment use cases.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For World Labs, Autodesk’s investment is a signal that its product has commercial appeal. The startup’s first world model product, Marble, released last November, lets users create editable, downloadable 3D environments.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autodesk is one of the biggest developers of 3D CAD (computer-aided design) software. Its platform underpins architectural, engineering, construction, manufacturing, and entertainment workflows. That focus on the built world makes investment in advanced spatial AI a natural extension of its core business.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or as Li put it in a statement: “Autodesk has long helped people think spatially and solve real-world problems and, together, we share a clear purpose: building physical AI that augments human creativity and puts more powerful tools in the hands of designers, builders, and creators.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, Autodesk will serve as an adviser to World Labs, and the two will collaborate at the “research and model level.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Daron Green, Autodesk’s chief scientist, told TechCrunch the partnership is still in its early days, so the precise form it’s going to take hasn’t been determined yet.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You could anticipate us consuming their models or them consuming our models in different settings,” Green said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He mused that customers might like to start with a world-model-based sketch in World Labs (say, of an office layout) and then drill down on certain design aspects (like the design of the desk), which is where Autodesk’s tech might come in. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Similarly, you might want to take an object that you’ve designed in our [platform], and put it in a context that you create through one of [World Labs’] prompts,” Green said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green added that data sharing is not part of the agreement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green said the two companies plan to start with media and entertainment use cases. Most companies building world models — including Google DeepMind and Runway — see gaming and interactive entertainment as an initial go-to-market strategy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autodesk already works with most major media production companies and has been training models for character animation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“These are close to world models,” Green said. “They’re a characterization of an animal in the world that’s responding to physical constraints like time, maybe a terrain it needs to traverse. So there’s a physical understanding in the model, and you can see how that might be combined [with World Labs’ tech]. You’re not just animating the dog, but you’re giving it a world within which it can now interact.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership with World Labs supports Autodesk’s broader push to integrate more AI features across its software portfolio. The company is developing “neural CAD,” a new kind of generative AI model trained on geometric data that can reason about components and entire systems. Put simply, it can generate working 3D models, not just images, with an understanding of how those designs would function in the real world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autodesk’s neural CAD models are already being integrated into the firm’s product design and architecture products as a step toward more advanced spatial intelligence. But World Labs’ models could help extend that capability beyond individual design files toward more holistic digital representations of the physical world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green thinks different AI systems, including large language models, world models, and neural CAD will be combined in the future to improve designs for Autodesk’s customers.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“If AI is to be truly useful, it must understand worlds, not just words,” Li said in the statement. “Worlds are governed by geometry, physics, and dynamics, and reconciling the semantic, spatial, and physical is the next great frontier of AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated to include more details on World Labs’ raise.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/Untitled-design.png?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Fei-Fei Li’s World Labs has secured a $200 million investment from software design giant Autodesk as part of a larger $1 billion round from backers, including AMD, Emerson Collective, Fidelity, Nvidia, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;World Labs, which emerged from stealth in 2024 with $230 million at a $1 billion valuation, declined to say whether the latest round boosted its valuation. However, reports a month ago suggested it was aiming to raise at a $5 billion valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The partnership between World Labs and Autodesk will see the two companies collaborating to explore how World Labs’ models — AI systems that can generate and reason about immersive 3D environments — can work alongside Autodesk’s tools, and vice versa, starting with a focus on entertainment use cases.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For World Labs, Autodesk’s investment is a signal that its product has commercial appeal. The startup’s first world model product, Marble, released last November, lets users create editable, downloadable 3D environments.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autodesk is one of the biggest developers of 3D CAD (computer-aided design) software. Its platform underpins architectural, engineering, construction, manufacturing, and entertainment workflows. That focus on the built world makes investment in advanced spatial AI a natural extension of its core business.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Or as Li put it in a statement: “Autodesk has long helped people think spatially and solve real-world problems and, together, we share a clear purpose: building physical AI that augments human creativity and puts more powerful tools in the hands of designers, builders, and creators.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As part of the deal, Autodesk will serve as an adviser to World Labs, and the two will collaborate at the “research and model level.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Daron Green, Autodesk’s chief scientist, told TechCrunch the partnership is still in its early days, so the precise form it’s going to take hasn’t been determined yet.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You could anticipate us consuming their models or them consuming our models in different settings,” Green said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He mused that customers might like to start with a world-model-based sketch in World Labs (say, of an office layout) and then drill down on certain design aspects (like the design of the desk), which is where Autodesk’s tech might come in. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Similarly, you might want to take an object that you’ve designed in our [platform], and put it in a context that you create through one of [World Labs’] prompts,” Green said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green added that data sharing is not part of the agreement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green said the two companies plan to start with media and entertainment use cases. Most companies building world models — including Google DeepMind and Runway — see gaming and interactive entertainment as an initial go-to-market strategy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autodesk already works with most major media production companies and has been training models for character animation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“These are close to world models,” Green said. “They’re a characterization of an animal in the world that’s responding to physical constraints like time, maybe a terrain it needs to traverse. So there’s a physical understanding in the model, and you can see how that might be combined [with World Labs’ tech]. You’re not just animating the dog, but you’re giving it a world within which it can now interact.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The partnership with World Labs supports Autodesk’s broader push to integrate more AI features across its software portfolio. The company is developing “neural CAD,” a new kind of generative AI model trained on geometric data that can reason about components and entire systems. Put simply, it can generate working 3D models, not just images, with an understanding of how those designs would function in the real world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Autodesk’s neural CAD models are already being integrated into the firm’s product design and architecture products as a step toward more advanced spatial intelligence. But World Labs’ models could help extend that capability beyond individual design files toward more holistic digital representations of the physical world.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Green thinks different AI systems, including large language models, world models, and neural CAD will be combined in the future to improve designs for Autodesk’s customers.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“If AI is to be truly useful, it must understand worlds, not just words,” Li said in the statement. “Worlds are governed by geometry, physics, and dynamics, and reconciling the semantic, spatial, and physical is the next great frontier of AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated to include more details on World Labs’ raise.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/world-labs-lands-200m-from-autodesk-to-bring-world-models-into-3d-workflows/</guid><pubDate>Wed, 18 Feb 2026 18:07:16 +0000</pubDate></item><item><title>Amazon halts Blue Jay robotics project after less than 6 months (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/18/amazon-halts-blue-jay-robotics-project-after-less-than-six-months/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/about-amazon-hero-20230227-amazon-288.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has hundreds of thousands of robots in its warehouses, but that doesn’t mean all of its robotic initiatives are a success story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The e-commerce giant has halted its Blue Jay warehouse robotics project just months after unveiling the tech, as originally reported by Business Insider and confirmed by TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Blue Jay, a multi-armed robot designed to sort and move packages, was unveiled in October for use in the company’s same-day delivery facilities. At the time, the company was testing the robots at a facility in South Carolina and said it took Amazon significantly less time to develop Blue Jay — only about a year— than it did to develop its other warehouse robots, a speed the company credited to advancements in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon spokesperson Terrence Clark told TechCrunch that Blue Jay was launched as a prototype — although that was not made clear in the company’s original press release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company plans to use Blue Jay’s core technology for other robotics “manipulation programs” with employees who worked on Blue Jay being moved to other projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re always experimenting with new ways to improve the customer experience and make work safer, more efficient, and more engaging for our employees,” Clark told TechCrunch over email. “In this case, we’re actually accelerating the use of the underlying technology developed for Blue Jay, and nearly all of the technologies are being carried over and will continue to support employees across our network.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also unveiled the Vulcan robot last year, which is used in the storage compartments of the company’s warehouses. Vulcan is a two-armed robot, with one arm meant to rearrange and move items in a compartment while the other is equipped with a camera and suction cups to grab goods. The Vulcan can allegedly “feel” the objects that it touches and was trained on data gathered from real-world interactions.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon has been developing its internal robotics program since 2012 when it purchased Kiva Systems, a robotics company whose warehouse automation technology formed the foundation of Amazon’s fulfillment operations. It surpassed 1 million robots in its warehouses last July.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/about-amazon-hero-20230227-amazon-288.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon has hundreds of thousands of robots in its warehouses, but that doesn’t mean all of its robotic initiatives are a success story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The e-commerce giant has halted its Blue Jay warehouse robotics project just months after unveiling the tech, as originally reported by Business Insider and confirmed by TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Blue Jay, a multi-armed robot designed to sort and move packages, was unveiled in October for use in the company’s same-day delivery facilities. At the time, the company was testing the robots at a facility in South Carolina and said it took Amazon significantly less time to develop Blue Jay — only about a year— than it did to develop its other warehouse robots, a speed the company credited to advancements in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon spokesperson Terrence Clark told TechCrunch that Blue Jay was launched as a prototype — although that was not made clear in the company’s original press release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company plans to use Blue Jay’s core technology for other robotics “manipulation programs” with employees who worked on Blue Jay being moved to other projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re always experimenting with new ways to improve the customer experience and make work safer, more efficient, and more engaging for our employees,” Clark told TechCrunch over email. “In this case, we’re actually accelerating the use of the underlying technology developed for Blue Jay, and nearly all of the technologies are being carried over and will continue to support employees across our network.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also unveiled the Vulcan robot last year, which is used in the storage compartments of the company’s warehouses. Vulcan is a two-armed robot, with one arm meant to rearrange and move items in a compartment while the other is equipped with a camera and suction cups to grab goods. The Vulcan can allegedly “feel” the objects that it touches and was trained on data gathered from real-world interactions.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon has been developing its internal robotics program since 2012 when it purchased Kiva Systems, a robotics company whose warehouse automation technology formed the foundation of Amazon’s fulfillment operations. It surpassed 1 million robots in its warehouses last July.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/18/amazon-halts-blue-jay-robotics-project-after-less-than-six-months/</guid><pubDate>Wed, 18 Feb 2026 18:27:10 +0000</pubDate></item><item><title>[NEW] Google Cloud’s VP for startups on reading your ‘check engine light’ before it’s too late (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/google-clouds-vp-for-startups-on-reading-your-check-engine-light-before-its-too-late/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Darren-Mowry-headshot.png?resize=1200,960" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Together, they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How Google positions against AWS and Microsoft in the AI startup&amp;nbsp;race.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;TPUs vs GPUs: How much does hardware choice matter for early-stage companies?&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Which AI verticals are seeing real growth, and&amp;nbsp;what’s&amp;nbsp;standing out in biotech, climate tech, developer tools, and world models.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What red flags will signal that a startup&amp;nbsp;isn’t&amp;nbsp;going to make it.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Darren-Mowry-headshot.png?resize=1200,960" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Together, they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How Google positions against AWS and Microsoft in the AI startup&amp;nbsp;race.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;TPUs vs GPUs: How much does hardware choice matter for early-stage companies?&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Which AI verticals are seeing real growth, and&amp;nbsp;what’s&amp;nbsp;standing out in biotech, climate tech, developer tools, and world models.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;What red flags will signal that a startup&amp;nbsp;isn’t&amp;nbsp;going to make it.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/google-clouds-vp-for-startups-on-reading-your-check-engine-light-before-its-too-late/</guid><pubDate>Wed, 18 Feb 2026 20:22:29 +0000</pubDate></item><item><title>[NEW] Is your startup’s check engine light on? Google Cloud’s VP explains what to do (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/is-your-startups-check-engine-light-on-google-clouds-vp-explains-what-to-do/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/54885194602_44d8e5385f_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30941231"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Watch as they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/54885194602_44d8e5385f_o.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30941231"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond free credits and into real cloud bills.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan caught up with&amp;nbsp;Darren Mowry, Google Cloud’s vice president of global startups who is right at the center of those tradeoffs. Watch as they discuss what Mowry’s seeing across the startup ecosystem, how Google Cloud is competing for AI startups, and what founders should be thinking about as they scale.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/is-your-startups-check-engine-light-on-google-clouds-vp-explains-what-to-do/</guid><pubDate>Wed, 18 Feb 2026 21:07:00 +0000</pubDate></item><item><title>[NEW] The Reasonable Effectiveness of Virtue Ethics in AI Alignment (The Gradient)</title><link>https://thegradient.pub/virtue-ethics-ai-alignment/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://thegradient.pub/content/images/2026/02/ramelli-featured.jpg" /&gt;&lt;/div&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;: networks of actions, action-dispositions, action-evaluation criteria, and action-resources that structure, clarify, develop, and promote themselves. If we want AIs that can genuinely support, collaborate with, or even &lt;strong&gt;comply&lt;/strong&gt; with human agency, AI agents’ deliberations must share a “type signature” with the practices-based logic we use to reflect and act.&lt;/p&gt;
&lt;p&gt;I argue that these issues matter not just for aligning AI to grand ethical ideals like human flourishing, but also for aligning AI to core safety-properties like transparency, helpfulness, harmlessness, or corrigibility. Concepts like ’harmlessness’ or ‘corrigibility’ are unnatural -- brittle, unstable, arbitrary -- for agents who’d interpret them in terms of goals or rules, but natural for agents who’d interpret them as dynamics in networks of actions, action-dispositions, action-evaluation criteria, and action-resources.&lt;/p&gt;
&lt;p&gt;While the issues this essay tackles tend to sprawl, one theme that reappears over and over is the relevance of the formula ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly.’ I argue that this formula captures something important about both meaningful human life-activity (art is the artistic promotion of art, romance is the romantic promotion of romance) and real human morality (to care about kindness is to promote kindness kindly, to care about honesty is to promote honesty honestly).&lt;/p&gt;
&lt;p&gt;I start by asking: What follows for AI alignment if we take the concept of eudaimonia -- active, rational human flourishing -- seriously? I argue that the concept of eudaimonia doesn’t simply point to a desired state or trajectory of the world that we should set as an AI’s optimization target, but rather points to a structure of deliberation different from standard consequentialist rationality. I then argue that this form of rational activity and valuing, which l call &lt;em&gt;eudaimonic rationality&lt;/em&gt;, is a useful or even necessary framework for the agency and values of human-aligned AIs.&lt;/p&gt;
&lt;p&gt;These arguments are based both on the dangers of a “type mismatch” between human flourishing as an optimization target and consequentialist optimization as a form, and on certain material advantages that eudaimonic rationality plausibly possesses in comparison to deontological and consequentialist agency with regard to stability and safety.&lt;/p&gt;
&lt;p&gt;The concept of eudaimonia, I argue, suggests a form of rational activity without a strict distinction between means and ends, or between ‘instrumental’ and ‘terminal’ values. In this model of rational activity, a rational action is an element of a valued practice in roughly the same sense that a note is an element of a melody, a time-step is an element of a computation, and a moment in an organism’s cellular life is an element of that organism’s self-subsistence and self-development.&lt;/p&gt;
&lt;p&gt;My central claim is that our intuitions about the nature of human flourishing are implicitly intuitions that eudaimonic rationality can be functionally robust in a sense highly critical to AI alignment. More specifically, I argue that in light of our best intuitions about the nature of human flourishing it’s plausible that eudaimonic rationality is a &lt;em&gt;natural&lt;/em&gt; form of agency, and that eudaimonic rationality is &lt;em&gt;effective&lt;/em&gt; even by the light of certain consequentialist approximations of its values. I then argue that if our goal is to align AI in support of human flourishing, and if it is furthermore plausible that eudaimonic rationality is natural and efficacious, then many classical AI safety considerations and ‘paradoxes’ of AI alignment speak in favor of trying to instill AIs with eudaimonic rationality.&lt;/p&gt;
&lt;p&gt;Throughout this essay, I will sometimes explicitly and often implicitly be asking whether some form of agency or rationality or practice is &lt;em&gt;natural&lt;/em&gt;. The sense of ‘natural’ I’m calling on is certainly related to the senses used in various virtue-ethical traditions, but the interest I take in it is less immediately normative and more material or technical. While I have no reductive definition at hand, the intended meaning of ‘natural’ is related to stability, coherence, relative non-contingency, ease of learnability, lower algorithmic complexity, convergent cultural evolution, hypothetical convergent cultural evolution across different hypothetical rational-animal species, potential convergent evolution between humans and neural-network based AI, and targetability by ML training processes. While I will also make many direct references to AI alignment, this question of material naturalness is where the real alignment-critical action takes place: if we learn that certain exotic-sounding forms of agency, rationality, or practice are both themselves natural and make the contents of our all-too-human values natural in turn, then we have learned about good, relatively safe, and relatively easy targets for AI alignment.&lt;/p&gt;
&lt;p&gt;Readers may find the following section-by-section overview useful for navigating the essay:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part &lt;em&gt;I&lt;/em&gt; presents a class of cases of rational deliberation that are very different from the Effective Altruism-style optimization many in the AI-alignment world treat as the paradigm of rational deliberation. I call this class of rational deliberations 'eudaimonic rationality,' and identify it with the form of rationality that guides a mathematician or an artist or a friend when they reflect on what to do in mathematics or in art or in friendship.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;II&lt;/em&gt; looks at the case of research mathematics (via an account by Terry Tao) as an example of eudaimonic rationality at work. What does a mathematician try to do in math? I say she tries to be &lt;em&gt;mathematically excellent&lt;/em&gt;, which involves promoting mathematical excellence through mathematical excellence, and that this structure is closely related to why 'mathematical excellence' can even be a concept.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;III&lt;/em&gt; argues that for eudaimonic agents such as a mathematician who is trying to do excellent mathematics, distinctions between ‘instrumental goods’ and ‘terminal goods’ (intrinsic goods) are mostly unnatural. This makes reflection about values go very differently for a eudaimonic agent than for an Effective Altruism-style agent. Instead of looking to reduce a network of causally intertwined apparent values to a minimal base of intrinsic values that “explains away” the rest as instrumental, a eudaimonic agent looks for organism-like causal coherence in a network of apparent values.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IV&lt;/em&gt; cashes out the essay’s central concepts: A &lt;em&gt;eudaimonic practice&lt;/em&gt; is a network of actions, action-dispositions, action-evaluation criteria, and action-resources where high-scoring actions reliably (but defeasibly) causally promote future high-scoring actions. &lt;em&gt;Eudaimonic rationality&lt;/em&gt; is a class of reflective equilibration and deliberation processes that assume an underlying eudaimonic practice and seek to optimize aggregate action-scores specifically via high-scoring action.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;V&lt;/em&gt;, I argue that many puzzles and ‘paradoxes’ about AI alignment are driven by the assumption that mature AI agents will be Effective Altruism-style optimizers. A “type mismatch” between Effective Altruism-style optimization and eudaimonic rationality makes it nearly impossible to translate the interests of humans -- agents who practice eudaimonic rationality -- into a utility function legible to an Effective Altruism-style optimizer AI. But this does not mean that our values are inherently brittle, unnatural, or wildly contingent: while Effective Altruism-style optimizers may well be a natural type of agent, eudaimonic agents (whether biological or AI) are highly natural as well.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;VI&lt;/em&gt;, I ask whether a eudaimonically rational AI agent devoted to a practice like mathematical research would be safe by default. I argue that a practice like mathematical research plausibly has natural boundaries that exclude moves like ‘take over planet to get more compute for mathematical research,’ but the issue is nuanced. I propose that a practice’s boundaries (for which there may be multiple good natural candidates) may be most stable when a practice is paired with a support practice: a complementary practice for dealing with practice-external issues of maintenance and resource-gathering.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VII&lt;/em&gt; develops the idea of ‘support practices’: eudaimonically rational ways to support eudaimonic practices. We famously want AI agents to help humans lead flourishing lives, but how can we define the purview of this ‘help’? I argue that many core human practices have natural support-practices with a derived eudaimonic structure: the work of good couples’ therapist, for instance, is intertwined with but clearly distinct from a couple’s relationship-practice. Still, there remains a problem: a support-practice AI might harm other people and practices to help the people or practice it’s supporting.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VIII&lt;/em&gt; moves from eudaimonic rationality in general to eudaimonically rational morality. I argue that thinking of moral virtues as domain-general, always-on practices solves key AI-alignment-flavored problems with consequentialist and deontological moralities. The core idea is that the conditions for e.g. ‘kindness’ being a robust moral virtue are akin to the conditions for ‘mathematical excellence’ being a meaningful concept: it must be generally viable to promote kindness in yourself and others kindly. It’s this structure, I argue, that gives moral virtues material standing in a ‘fitness landscape’ riven by pressures from neural-network generalization dynamics, reinforcement-learning cycles, and social and natural selection.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IX&lt;/em&gt; argues that eudaimonic agents have some unique forms of robustness to RL-like and Darwinian-like dynamics that tend to mutate the values of EA-style optimizers. In particular, eudaimonic agents should be very robust to the risk of developing rogue subroutines (sometimes called ‘the inner alignment problem’).&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;X&lt;/em&gt; I discuss canonical AI-safety desiderata like transparency, corrigibility, and (more abstractly) niceness. I argue that treating these properties as moral virtues in my sense -- domain-general, always-on eudaimonic practices --  dissolves problems and paradoxes that arise when treating them as goals, as rules, or even as character traits. I end with an appendix on some prospects for RL regimes geared towards eudaimonic rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="i-rational-action-in-the-good-life"&gt;I. Rational Action in the Good Life&lt;/h2&gt;
&lt;p&gt;I start with a consideration of the nature of the good we hope AI alignment can promote. With the exception of hedonistic utilitarians, most actors interested in AI alignment understand our goal as a future brimming with human (and other sapient-being) flourishing: persons living good lives and forming good communities. What I believe many fail to reflect on, however, is that on any plausible conception human flourishing involves a kind of rational activity. Subjects engaged in human flourishing act in intelligible ways subject to reason, reflection, and revision, and this form of rational care and purposefulness is itself part of the constitution of our flourishing. I believe this characterization of human flourishing is relatively uncontroversial upon reflection, but it raises a kind of puzzle if we’re used to thinking of rationality in consequentialist (or consequentialist-with-deontological-constraints) terms: just what goal is the rational agency involved in human-flourishing activity directed towards?&lt;/p&gt;
&lt;p&gt;One obvious answer would be that, like all properly aligned rationality, the rational agency involved in human-flourishing activities is geared towards maximizing human (and other sapient) flourishing. But we should quickly find ourselves confused about the right way to describe the contribution that rational agency in human-flourishing activities makes to human flourishing. It seems neither appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing only by enacting rationality (by selecting actions that are intrinsically valuable when rationally selected), nor appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing just instrumentally (by selecting actions that causally promote human flourishing).&lt;/p&gt;
&lt;p&gt;The first option reduces our rational actions to something ritualistic, even as the good life surely involves mathematicians working to advance mathematics, friends speaking heart-to-heart to deepen intimacies, gymnasts practicing flips to get better at flips, and novelists revising chapters to improve their manuscripts. The second option threatens to make the good in the good life just impossible to find -- if speaking heart-to-heart is not the good of friendship, and working on math is the not the good of mathematics, then what is?&lt;/p&gt;
&lt;p&gt;This essay argues that deliberative reasoning about the good life is neither directed towards goals external to rational action nor directed towards rational action as an independent good, but towards acts of excellent participation in a valued open-ended process. I then go on to argue that the ‘eudaimonic’ structure of deliberation salient in cases like math or friendship (sloganized as ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) is also subtly critical in more worldly, strategic, or morally high-stakes contexts, and constitutes a major organizing principle of human action and deliberation.&lt;/p&gt;
&lt;h2 id="ii-what-is-a-practice"&gt;II. What Is a Practice?&lt;/h2&gt;
&lt;p&gt;Since ‘human flourishing’ can seem mysterious and abstract, let’s focus on some concrete eudaimonic practices. Consider practices like math, art, craft, friendship, athletics, romance, play, and technology, which are among our best-understood candidates for partial answers to the question ‘what would flourishing people in a flourishing community be doing.’ From a consequentialist point of view, these practices are all marked by extreme ambiguity -- and I would argue indeterminacy -- about what’s instrumental and what’s terminal in their guiding ideas of value. Here, for example, is Terry Tao’s account of goodness in mathematics:&lt;/p&gt;
&lt;p&gt;‘The very best examples of good mathematics do not merely fulfil one or more of the criteria of mathematical quality listed at the beginning of this article, but are more importantly part of a greater mathematical story, which then unfurls to generate many further pieces of good mathematics of many different types. Indeed, one can view the history of entire fields of mathematics as being primarily generated by a handful of these great stories, their evolution through time, and their interaction with each other. I would thus conclude that good mathematics [...] also depends on the more “global” question of how it fits in with other pieces of good mathematics, either by building upon earlier achievements or encouraging the development of future breakthroughs. [There seems] to be some undefinable sense that a certain piece of mathematics is “on to something”, that it is a piece of a larger puzzle waiting to be explored further.’&lt;/p&gt;
&lt;p&gt;It may be possible to give some post-hoc decomposition of Tao’s account into two logically distinct components -- a description of a utility-function over mathematical achievements and an empirical theory about causal relations between mathematical achievements -- but I believe this would be artificial and misleading. On a more natural reading, Tao is describing some of the conditions that make good mathematical practice a eudaimonic practice: In a mathematical practice guided by a cultivated mathematical practical-wisdom judgment (Tao’s ‘undefinable sense that a certain piece of mathematics is “on to something”’), present excellent performance by the standard of the practical-wisdom judgment reliably develops the conditions for future excellent performance by the standard of the mathematical practical-wisdom judgment, as well as cultivating our practical and theoretical grasp of the standard itself.&lt;/p&gt;
&lt;p&gt;This is not to suggest that ‘good mathematics causes future good mathematics’ is a full definition or even full informal description of good mathematics. My claim is only that the fact that good mathematics has a disposition to cause future good mathematics reveals something essential about our concept of good mathematics (and about the material affordances enabling this concept). By analogy, consider the respective concepts healthy tiger and healthy human: It's essential to the concept of a healthy tiger that &lt;em&gt;x&lt;/em&gt; being a healthy tiger now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy tiger 5 minutes in the future (since a healthy tiger body self-maintains and enables self-preservation tiger-behaviours), and essential to the concept of a healthy human that &lt;em&gt;x&lt;/em&gt; being a healthy human now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy human 5 minutes in the future (since a healthy human body self-maintains and enables self-preservation human behaviours). But these formulae aren't yet complete descriptions of 'healthy tiger' or 'healthy human,' as evidenced by the fact that we can tell apart a healthy tiger from a healthy human.&lt;/p&gt;
&lt;p&gt;Crucially, the mathematical practical-wisdom described by Tao is not entirely conceptually opaque beyond its basic characterization as a self-cultivating criterion for self-cultivating excellence in mathematical activity. Mathematical flourishing can partly be described as involving the instantiation of a relation (a mathematical-practice relation of ‘developmental connectedness’) among instantiations of relatively individually definable and quantifiable instances of mathematical value such as elegant proofs, clear expositions, strong theorems, cogent definitions and so on. Furthermore, this relation of developmental connectedness is partly defined by its reliable tendency to causally propagate instances of more individually and locally measurable mathematical value (instances of elegant proofs, clear exposition, strong theorems, cogent definitions and so on):&lt;/p&gt;
&lt;p&gt;[I believe] that good mathematics is more than simply the process of solving problems, building theories, and making arguments shorter, stronger, clearer, more elegant, or more rigorous, though these are of course all admirable goals; while achieving all of these tasks (and debating which ones should have higher priority within any given field), we should also be aware of any possible larger context that one’s results could be placed in, as this may well lead to the greatest long-term benefit for the result, for the field, and for mathematics as a whole.&lt;/p&gt;
&lt;p&gt;One could, again, try to interpret this causal relationship between excellence according to Tao’s ‘organicist’ (or ‘narrative’ or ‘developmental’) sense of good mathematics and the reliable propagation of narrow instances of good mathematics as evidence of a means-ends rational relation, where additive maximization of narrow instances of mathematical value is the utility function and ‘organicist’ mathematical insight is the means. For Tao, however, the evidential import of this causal relationship goes exactly the other way -- it suggests a unification of our myriad more-explicit and more-standalone conceptions of mathematical excellence into a more-ineffable but more-complete conception. As Tao says:&lt;/p&gt;
&lt;p&gt;It may seem from the above discussion that the problem of evaluating mathematical quality, while important, is a hopelessly complicated one, especially since many good mathematical achievements may score highly on some of the qualities listed above but not on others [...] However, there is the remarkable phenomenon that good mathematics in one of the above senses tends to beget more good mathematics in many of the other senses as well, leading to the tentative conjecture that perhaps there is, after all, a universal notion of good quality mathematics, and all the specific metrics listed above represent different routes to uncover new mathematics, or different stages or aspects of the evolution of a mathematical story.&lt;/p&gt;
&lt;h2 id="iii-inverting-consequentialist-reflection"&gt;III. Inverting Consequentialist Reflection&lt;/h2&gt;
&lt;p&gt;Tao’s reasoning about local and global mathematical values exemplifies a central difference between consequentialist rationality and eudaimonic rationality, now taken as paradigms not only for selecting actions but for reflecting on values. (Paradigms for what philosophers will sometimes call ‘reflective equilibration.’) Within the paradigm of consequentialist rationality, if excellence in accordance with a holistic, difficult-to-judge apparent value (say ‘freedom’) is reliably a powerful causal promoter of excellence in accordance with more explicit, more standalone apparent values (say ‘material comfort,’ ‘psychological health,’ ‘lifespan’), this relationship functions as evidence against the status of the holistic prima-facie value as a constitutive -- as opposed to instrumental -- value. Within the paradigm of eudaimonic rationality, by contrast, this same relationship functions as evidence for the status of the holistic prima-facie value as a constitutive value.&lt;/p&gt;
&lt;p&gt;For a (typical) consequentialist-rationality reflection process, evidence that the excellence of a whole causally contributes to the excellences of its parts explains away our investment in the excellence of the whole. The “coincidence” that the intrinsically valuable whole is also instrumentally valuable for its parts is taken to suggest a kind of double-counting error --  one we “fix” by concluding that the whole has no constitutive value but valuing the whole is an effective heuristic under normal circumstances. A eudaimonic-rationality reflective equilibration, by contrast, treats instrumental causal connections between excellences as evidence that our notions of excellence are picking out something appropriately ‘substantive.’&lt;br /&gt;For eudaimonic-rationality reflective equilibration, it is the discovery of causal and common-cause relations among excellences that ratifies our initial sense that caring about these excellences is eudaimonically rational. The discovery of these causal connections functions as evidence that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ‘local’ excellences we care about are resonant or fruitful, in that they causally promote each other and the holistic excellences in which they participate.&lt;/li&gt;
&lt;li&gt;The ‘holistic’ excellences we care about are materially efficacious and robust, in that they causally promote both the more local excellences that participate in them and their own continuation as future holistic excellence.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my view this is the right way to treat causal connections between (apparent) values if we’re hoping to capture actual human values-reflection, and points to an important strength of the eudaimonic rationality paradigm: Eudaimonic rationality dissolves the ‘paradox’ that in real-life arguments about the value of various human enterprises (e.g. the value of branches of science, branches of art, branches of sport), judgments of intrinsic value typically seek succor from some kind of claim to instrumental value. For example, a defense of the importance of research in quantum physics will often appeal to the wonderful technological, mathematical, and special-sciences applications quantum physics gave us, without meaning to reduce the worth of quantum physics to these applications. On my reading, these appeals aren't just additive -- 'aside from the intrinsic value there is also instrumental value' -- but presentations of evidence that research in quantum physics is a resonant part of a flourishing organic whole (e.g. the civilizational whole of ‘modern science and technology’).&lt;/p&gt;
&lt;p&gt;I believe that without 'organicism' of the kind described above, one faces a serious dilemma whenever one argues for the intrinsic worth of a pursuit or norm: either we stress the value's independence from all benefits and applications and make the claim of value dogmatic and irrelevant, or else we invite an instrumentalist reduction that ‘explains away’ the appearance of intrinsic value. Indeed, I’d argue that organicism of this kind is even necessary to make sense of caring about rationality (including truth, knowledge, non-contradiction and so on) non-instrumentally at all: the ‘paradox’ of rationality as a substantive value is that the typical usefulness of rationality suggests an error-theory about its apparent intrinsic value, since it’s a strange coincidence that rationality is both so typically useful and so intrinsically good. On an organicist account, however, we expect that major forms of excellence endemic to human life -- thought, understanding, knowledge, reasoned action -- both typically promote each other and typically promote our material flourishing and causal leverage on the world.&lt;/p&gt;
&lt;h2 id="iv-the-material-efficacy-condition"&gt;IV. The Material Efficacy Condition&lt;/h2&gt;
&lt;p&gt;Returning now to Tao’s account of good mathematics, let’s take final stock of our interpretation. I argue that mathematical excellence (the property marking ‘the very best examples of good mathematics’) according to Tao satisfies the following conditions, which I believe Tao intends as necessary but not sufficient:&lt;/p&gt;
&lt;p&gt;A) Mathematical excellence is a property of mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;B) An excellent mathematical-activity instance performed today is excellent partly by virtue of satisfying the mathematical-practice relation ‘builds on’ with regard to past excellent mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;C) An excellent mathematical-activity instance performed today is excellent partly by virtue of having a reliable causal tendency to bring about future excellent mathematical-activity instances that satisfy the mathematical-practice relation ‘builds on’ with regard to it.&lt;/p&gt;
&lt;p&gt;D) Instantiation of more local, more individually measurable criteria of mathematical-activity goodness such as elegant proofs, clear expositions, and strong theorems is a typical correlate of mathematical excellence.&lt;/p&gt;
&lt;p&gt;E) At a given moment in a given mathematical field, the instantiation of mathematical excellence will be predictably better-correlated with the instantiation of certain local criteria of mathematical-activity goodness than with others.&lt;/p&gt;
&lt;p&gt;Should we take these traits to collectively describe something more like a decision-procedure called ‘mathematical excellence’ that mathematicians should try to follow, or something more like an event called ‘mathematical excellence’ whose aggregate future-occurrences mathematicians should aspire to maximize? My contention is that Tao’s account is inherently ambiguous, and for a good reason: in ordinary circumstances there is no significant practical difference between doing excellent mathematics and doing instrumentally optimal mathematics with regard to maximizing future aggregate excellent mathematics. This isn’t to say that doing excellent mathematics is the instrumentally optimal action among all possible actions with regard to aggregate future excellent mathematics, but that (in ordinary circumstances) it is the instrumentally optimal choice from among mathematical actions with regard to aggregate future excellent mathematics.&lt;/p&gt;
&lt;p&gt;I propose that the rough matchup between mathematical excellence and optimal (among mathematical actions) instrumental promotion of aggregate mathematical excellence is neither an empirical miracle nor something determined ‘by definition’ in a trivializing sense. Rather, ‘mathematical excellence’ as used by Tao is a concept that has a referent only if there is a possible property &lt;em&gt;x&lt;/em&gt; that satisfies both desiderata &lt;em&gt;A&lt;/em&gt;-&lt;em&gt;E&lt;/em&gt; and the additional criterion that among mathematical actions, actions that are optimal as instantiations of &lt;em&gt;x&lt;/em&gt; are also roughly optimal for maximizing aggregate future instantiation of &lt;em&gt;x&lt;/em&gt;-ness.&lt;/p&gt;
&lt;p&gt;This is what I would describe as the material efficacy condition on eudaimonic rationality. In order for a practice to be fit for possessing internal criteria of flourishing, excellence, and eudaimonic rationality, a practice must materially allow for an (under normal circumstances) optimally self-promoting property &lt;em&gt;x&lt;/em&gt; that strongly correlates with a plethora of more local, more individually measurable properties whose instantiation is prima facie valuable. Stated more informally, there must exist a two-way causal relationship between a practice’s excellence and the material, psychological, and epistemic effects of its excellence, such that present excellence reliably materially, psychologically, and epistemically promotes future excellence.&lt;/p&gt;
&lt;h2 id="v-practices-and-optimization"&gt;V. Practices and Optimization&lt;/h2&gt;
&lt;p&gt;I earlier said that if human flourishing involves practicing eudaimonic rationality, there may well be a “type mismatch” between human flourishing and the kind of consequentialist optimization we often associate with the idea of an agenticly mature future AI. In fact, I believe that implicitly recognizing but misdiagnosing this type mismatch is at least partially responsible for MIRI-style pessimism about the probability of aligning any artificial agents to human values.&lt;/p&gt;
&lt;p&gt;On my view, the secret to relatively successful alignment among humans themselves (when there is successful alignment among humans) lies in the role attempted excellence plays as a filter on human interventions in the future trajectory of a eudaimonic practice. To the degree that humans value a given eudaimonic practice, they are committed to effecting their vision for the practice’s future-trajectory primarily by attempting acts of excellence in the present: we stake our intended effect over the practice’s future-trajectory on the self-propagating excellence of our intervention. While this ‘filter’ doesn’t necessarily stop the worst interventions from being harmful (there are forms of ‘anti-excellence’ that also have self-promotion powers), I contend that this filter is mechanically crucial for the possibility of reliably benign or positive interventions.&lt;/p&gt;
&lt;p&gt;What do I mean? Consider the difference between a world where scientists typically try to propagate (what they believe to be) the scientific truth mainly by means of submitting research work to scientific institutions, and a world where scientists typically try to propagate (what they believe to be) the scientific truth by means including propaganda, fraud, threats, bribery, and slander. As Liam Kofi Bright demonstrates in On Fraud, a community of consequentialist scientists devoted to maximizing truth will predictably match the latter model. I believe one lesson to be drawn is that humans’ ability to collaborate in the promotion of science depends on our ability to scientifically collaborate in the promotion of science, rather than throttle the future trajectory of science every-which-way  our financial and political powers based on our individual beliefs about the optimal trajectory of science.&lt;/p&gt;
&lt;p&gt;A flourishing eudaimonic practice is, above all, a natural-selection-like mechanism whose fitness-function selects among attempted acts of excellence the ones conducive to (and constitutive of) the practice’s flourishing, propagating the excellence these acts instantiate. When people committed to a eudaimonic practice make their attempted interventions into the future trajectory of the practice via acts of attempted excellence, the natural-selection-like mechanism embodied by the practice  (rather than any single individual’s theory of optimal future trajectory) is the aligned intelligence determining the practice’s future trajectory.&lt;/p&gt;
&lt;p&gt;The explanation here, again, is partly causal and partly constitutive: a practice’s “ultimate” norms of excellence, including the “ultimate” epistemic and alethic norms of a discursive practice, are partly defined by the succession of norms in the course of a practice’s development through best-efforts attempted excellence. Although this may be no deterrent to an already god-like optimizer who can simulate entire civilizational trajectories, an agent short of these capacities can best act on their vision of the optimal future-trajectory of a practice by attempting an excellent contribution to the practice.&lt;/p&gt;
&lt;p&gt;The second aspect of our type-mismatch is much more in the weeds: In my analysis so far, I discussed the overall excellence of the trajectory of a eudaimonic practice much like a consequentialist might discuss a quantity of utility. This may be taken to suggest that a ‘sophisticated consequentialist’ or ‘universal consequentialist’ could easily accommodate the implications of the so-called type mismatch by treating them as instrumental, decision-procedure level considerations against naive optimization. In fact, quantities like ‘aggregate democracy’ or ‘overall mathematical excellence’ are (on my view) practice-internal quantities that quickly lose meaning if we try to apply them outside the scope of a ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ decision-procedure.&lt;/p&gt;
&lt;p&gt;What do I mean?  Consider, for example, the practice of philosophy. Here are some questions that should arise for a consequentialist planner (including a sophisticated consequentialist planning decision-procedures or habits) who values philosophy practice-trajectories: Does rating (e.g.) Aristotle’s or Dharmakirti’s philosophical achievements as the most excellent achievements in philosophy imply that we should “tile the universe” with independent practice-trajectories designed to reproduce classical Greek or Indian philosophy? If not, is it because we should assign non-linearly greater value to longer trajectories? Or should we discount trajectories that have parallel contents? Or should we analyze the greatness of early achievements in a practice as mostly instrumental greatness but the greatness of later achievements in a practice as mostly intrinsically valuable? These are all, I believe, bad questions that have only arbitrary answers. To an agent trying to promote philosophy by doing excellent philosophical work, the bad questions above are naturally out of scope. The agent uses the concept of ‘aggregate philosophical excellence’ or ‘a philosophy practice-trajectory’s value’ only to reason about the philosophical influence of their work on the trajectory of the philosophy-practice in which it participates. Choosing an excellent action in practice requires (at most) quantitative comparison between different possible paths for a practice-trajectory, not quantitative comparison between possible worlds containing independent practice-trajectories sprinkled throughout time and space.&lt;/p&gt;
&lt;h2 id="vi-prospects-and-problems-for-ai"&gt;VI. Prospects and Problems for AI&lt;/h2&gt;
&lt;p&gt;Is this good news for AI alignment? It’s certainly good news that (if I’m right) eudaimonic practices are something like natural kinds marked by a causal structure that enables a self-developing excellence well-correlated with multiple naive local measures of quality. But does this mean we could develop a stable and safe (e.g.) ‘mathematical excellence through mathematical excellence’ AI? If we create a fully agentic AI mathematician, will it naturally abstain from trying to extend its longevity or get more resources (even for doing mathematics) other than by impressing us with excellent mathematical work?  I think that prospects are good, but not simple.&lt;/p&gt;
&lt;p&gt;I believe ‘mathematical excellence through mathematical excellence’ really can powerfully scope what mechanisms for shaping the future an AI cares to activate. An AI trained to follow ‘promote mathematics mathematically’ will only care about influencing the future by feeding  excellent mathematical work to mathematics’ excellence-propagation mechanism. But it’s harder to say whether the structure of mathematical practice also properly scopes what subactions can be taken as part of an instance of “doing math.” Is a human mathematician working on a would-be excellent proof in pen and paper practicing math when she is picking up a pen or flipping pages? When she is taking the bus to her office? When she’s buying amphetamines? And is an AI mathematician working on a would-be excellent proof practicing math when it opens a Python console? When it searches the web for new papers? When it harvests Earth for compute?&lt;/p&gt;
&lt;p&gt;I think these questions are complex, rather than nonsensical. Much like collective practices, individual practices -- for example a person’s or possibly an AI’s mathematical practice -- may possess functional organic unities that allow a meaningful distinction between internal dynamics (including dynamics of development and empowerment) and external interventions (including interventions of enhancement and provision). Still, it’s clear that eudaimonic practices do not exist in isolation, and that no practice can function without either blending with or relying on a “support practice” of some kind.&lt;/p&gt;
&lt;p&gt;How, then, do we rationally go about externally-oriented activities like building offices for mathematicians, performing elective reconstructive surgery on an athlete, or conducting couples therapy for romantic partners? And furthermore, how do we rationally go about allocating scarce resources useful for different practices, or judging whether to integrate (e.g.) performance-enhancing drugs into a practice?&lt;/p&gt;
&lt;p&gt;This is, I think, the fundamental question for AI alignment from the viewpoint of  ‘eudaimonic rationality.’ We want AI to support human eudaimonic practices -- and, if relevant, its own eudaimonic practices or participation in human eudaimonic practices -- in a eudaimonia-appropriate way. But how does the logic of eudaimonic rationality extend from eudaimonic practices to their support activities? How do we ‘eudaimonically-rationally’ do the dirty work that makes eudaimonia possible? My best answer is: carefully, kindly, respectfully, accountably, peacefully, honestly, sensitively.&lt;/p&gt;
&lt;h2 id="vii-from-support-practices-to-moral-practice"&gt;VII. From Support-Practices to Moral Practice&lt;/h2&gt;
&lt;p&gt;The theory of AI alignment, I propose, should fundamentally be a theory of the eudaimonic rationality of &lt;em&gt;support practices&lt;/em&gt;. One part of this theory should concern the ‘support’ relation itself, and analyze varieties of support practices and their appropriate relation to the self-determination of a eudaimonic practice: Support-practices such as acquiring resources for a practice, maintaining an enabling environment, coaching practitioners, conducting (physical or psychological) therapy for practitioners, devising technological enhancements for a practice, and educating the public about a practice, each have their own ‘role-morality’ vis-a-vis the practice they support. It is this part of the theory of ‘support practices’ that should, if all goes well in the theory’s construction, describe the various practice-external ways to eudaimonically-rationally act on a pro-attitude towards the aggregate excellence of the practice’s future trajectory without treating it like a quantity of utility. (Much like the concept of ‘mathematical action’ scopes the range of action-choices in such a way that decision-theoretic optimization of math’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint, so should the concepts of various types of ‘support action’ scope the range of action-choices in such a way that decision-theoretic optimization of a practice’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint when the choice of actions is scoped.)&lt;/p&gt;
&lt;p&gt;What is more difficult is delineating the appropriate relationship of a support-practice to everything &lt;em&gt;&lt;strong&gt;outside&lt;/strong&gt;&lt;/em&gt; the practice it supports. What stops a marriage-therapist AI on Mars from appropriately tending to the marriage of a Mars-dwelling couple but harvesting Earth for compute to be a better therapist-AI for that couple? While we can perhaps imagine a person or AI taking up a support-role for ‘humanity’s flourishing as whole,’ so that there’s no outside to speak of, I am not sure that the concept of practice remains natural at this level of abstraction. We have no real grasp on a &lt;em&gt;direct&lt;/em&gt; practice of human flourishing, but rather grasp it as the harmonious and mutually supportive interaction of all eudaimonic practices and support-practices participating in the flourishing. And as there is, indeed, not much outside of the practice of human flourishing, it’s also unclear whether there is room for a support-practice &lt;em&gt;external&lt;/em&gt; to the field of human flourishing itself.&lt;/p&gt;
&lt;p&gt;It’s here that I want to call on the classic idea of domain-general virtues, the traditional centerpiece of theories of human flourishing. I propose that the cultivation of human flourishing as such --  the cultivation of the harmony of a multiplicity of practices, including their resource-hungry support practices -- is the cultivation of an &lt;em&gt;&lt;strong&gt;adverbial&lt;/strong&gt;&lt;/em&gt; practice that modulates each and every practice. What makes our practices ‘play nice’ together are our adverbial practices of going about  any practice &lt;em&gt;carefully&lt;/em&gt;, &lt;em&gt;kindly&lt;/em&gt;, &lt;em&gt;respectfully&lt;/em&gt;, &lt;em&gt;accountably&lt;/em&gt;, &lt;em&gt;peacefully&lt;/em&gt;, &lt;em&gt;honestly&lt;/em&gt;, &lt;em&gt;sensitively&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="viii-virtue-decision-theory"&gt;VIII. Virtue decision-theory&lt;/h2&gt;
&lt;p&gt;Why think of qualities like kindness, respectfulness, or honesty as ‘practices’? The first reason is that devotion to a quality like kindness or honesty displays the same &lt;strong&gt;normative structure&lt;/strong&gt; with regard to means and ends as we find in devotion to a practice: An agent devoted to kindness cares about their own future kindness (and about the future kindness of others), but will seek to secure future kindness only by &lt;em&gt;kind means&lt;/em&gt;. The second reason is that qualities like kindness or honesty also approximately have the &lt;strong&gt;material&lt;/strong&gt; structure of a practice: there exist effective very kind strategies for promoting kindness in oneself and others, and when these strategies succeed they further increase affordances for effective very kind strategies for promoting kindness/honesty in oneself and others.&lt;/p&gt;
&lt;p&gt;The difference between adverbial practices like kindness or honesty and practices like research mathematics is that adverbial practices don’t have a “proprietary” domain. In a practice like research mathematics, the material structure of the domain does the most of work of directing agents to a eudaimonic form of agency all by itself, as long as the agents restrict themselves to in-domain actions. (Recall that we described mathematically excellent action as, in ordinary circumstances, the best action among mathematical action for maximizing aggregate mathematical excellence.) With a domain-general, adverbial practice like kindness the normative structure needs to do somewhat more heavy lifting.&lt;/p&gt;
&lt;p&gt;The following is a first pass at characterizing the &lt;strong&gt;normative structure&lt;/strong&gt; of an adverbial practice that values some action-quality &lt;em&gt;x&lt;/em&gt;. The corresponding material efficiency condition (or &lt;strong&gt;material structure&lt;/strong&gt;) necessary for the practice to be viable is that under ordinary circumstances this decision-procedure be instrumentally competitive with naive optimization of aggregate &lt;em&gt;x&lt;/em&gt;-ness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actions (or more generally 'computations') get an &lt;em&gt;x&lt;/em&gt;-ness rating. We define the agent’s expected utility conditional on a candidate action a as the sum of two utility functions: a bounded utility function on the &lt;em&gt;x&lt;/em&gt;-ness of a and a more tightly bounded utility function on the expected aggregate &lt;em&gt;x&lt;/em&gt;-ness of the agent's future actions conditional on a. (Thus the agent will choose an action with mildly suboptimal &lt;em&gt;x&lt;/em&gt;-ness if it gives a big boost to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness, but refuse certain large sacrifices of present &lt;em&gt;x&lt;/em&gt;-ness for big boosts to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A commitment to an adverbial practice that values &lt;em&gt;x&lt;/em&gt; is a commitment to promoting &lt;em&gt;x&lt;/em&gt;-ness (in oneself and others) &lt;em&gt;x&lt;/em&gt;-ingly.  The agent strikes a balance between promoting &lt;em&gt;x&lt;/em&gt;-ness and acting &lt;em&gt;x&lt;/em&gt;-ingly that heavily prioritizes acting &lt;em&gt;x&lt;/em&gt;-ingly when the two are in conflict, but if &lt;em&gt;x&lt;/em&gt; meets the material efficacy condition then the loss this balance imposes on future &lt;em&gt;x&lt;/em&gt;-ness will be small under normal circumstances, and -- from our point of view -- desirable in abnormal circumstances. This is because just like the practices of research mathematics, philosophy, or art, an adverbial practice is a crucial ‘epistemic filter’ on actions aiming to shape its future, and the (e.g.) future kindness a paperclipper-like future-kindness-optimizer optimizes for is probably not the kindness we want. What we know about kindness with relative certainty is that we’d like people and AIs here and now to act kindly, and to develop, propagate, and empower the habit and art of kindness in a way that is both kind and clever.&lt;/p&gt;
&lt;p&gt;To keep our conceptual system nicely organized, we might want distinguish merely (e.g.) very kind action from an action that is both very kind and highly promotive of future kindness in oneself and others, and call the latter sort of action excellently kind.  What I call the material efficacy conditions for adverbial practices states not that the kindest action best-promotes aggregate kindness, but that there are almost always action-options that are excellently kind: very kind actions that strongly promote aggregate kindness in oneself and others.&lt;/p&gt;
&lt;h2 id="ix-virtue-decision-theory-is-natural-for-humans-and-ais"&gt;IX. Virtue decision-theory is 'Natural' for Humans and AIs&lt;/h2&gt;
&lt;p&gt;I’ve said that the robustness or ‘naturalness’ of a practice’s normative structure (‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) depends on the practice’s &lt;strong&gt;material structure&lt;/strong&gt;: the capacity of high &lt;em&gt;x&lt;/em&gt;-ness actions to causally promote aggregate &lt;em&gt;x&lt;/em&gt;-ness. I also said that in key real-world practices, commitment to &lt;em&gt;x&lt;/em&gt;-ing might optimize aggregate &lt;em&gt;x&lt;/em&gt;-ness even better than direct optimization would. These two claims are best understood together. On my view, the normative structure ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ appears prominently in human life because (given the right material structure) ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ is a &lt;em&gt;much more stable&lt;/em&gt; than ‘promote &lt;em&gt;x&lt;/em&gt;.’&lt;/p&gt;
&lt;p&gt;How so? Both humans and any sufficiently dynamic AI agent operate in a world that subjects their agency, values, and dispositions to constant mutation pressures from RL-like and Darwinian-like processes. Eudaimonic deliberation is an RL-dynamics-native, Darwinian-dynamics-native operation: its direct object is a form of life that reinforces, enables, and propagates that same form of life. When an &lt;em&gt;x&lt;/em&gt;-ing successfully promotes (in expectation) aggregate &lt;em&gt;x&lt;/em&gt;-nes, the fact of its success itself promotes &lt;em&gt;x&lt;/em&gt;-ing because it reverberates via ubiquitous RL-like and Darwinian-like processes that reinforce (a generalization of) successful action. The material structure of a practice is the backbone that makes reliable success and meaningful generalization possible -- the right ecology of  neural-network generalization dynamics, reinforcement-learning feedback loop dynamics, and neural and environmental selection dynamics.&lt;/p&gt;
&lt;p&gt;An EA-style optimizer trying to minimize risk from optimization-goal-mutation, by contrast, is fighting an uphill battle to foresee and contain the RL-like and Darwinian-like side effects of its optimization actions. One critical mutation-pressure in particular is the risk that an optimizer agent will cultivate, reinforce, and materially empower subroutines (what high-church alignment theory calls ‘mesaoptimizers’) that initially serve the optimization goal but gradually distort or overtake it. For example, if a pro-democracy government instates a secret police to detect and extrajudicially kill anti-democracy agitators, and the government increases the secret police’s funding whenever the police convincingly reports discovering an agitator, the secret police might grow into a distorting influence on the government’s democracy-promotion effort. In light or risks like this, it’s not surprising that oppressive democracy-promotion is generally considered an unserious or dishonest idea: even if an agent were to abstract some concept of ‘aggregate democracy’ from democratic practice into a consequentialist value, it’s plausible that the agent should then immediately revert to a commitment to democratic practice (‘promote democracy democratically’) on sophisticated-consequentialist grounds.&lt;/p&gt;
&lt;p&gt;We should perhaps imagine eudaimonic practices as fixed points at the end of a chain of mesaoptimisers taking over outer optimisers and then being taken over by their own mesaoptimisers in turn. What the practice contributes that puts a stop to this process concept of &lt;em&gt;x&lt;/em&gt;-ness that’s applicable to every agentic subroutine of &lt;em&gt;x&lt;/em&gt;-ing across all nesting levels, so that &lt;em&gt;x&lt;/em&gt;-ness is reinforced (both directly and through generalization) across all subroutines and levels.&lt;/p&gt;
&lt;h2 id="x-virtue-decision-theory-is-safe-in-humans-and-ais"&gt;X. Virtue-decision-theory is Safe in Humans and AIs&lt;/h2&gt;
&lt;p&gt;Let’s talk about AI alignment in the more narrow, concrete sense. It’s widely accepted that if early strategically aware AIs possess values like corrigibility, transparency, and perhaps niceness, further alignment efforts are much more likely to succeed. But values like corrigibility or transparency or niceness don’t easily fit into an intuitively consequentialist form like ‘maximize lifetime corrigible behavior’ or ‘maximize lifetime transparency.’ In fact, an AI valuing its own corrigibility or transparency or niceness in an intuitively consequentialist way can lead to extreme power-seeking: the AI should seek to violently remake the world to (for example) protect itself from the risk that humans will modify the AI  to be less corrigible or transparent or nice. On the other hand, constraints or taboos or purely negative values (a.k.a. ‘deontological restrictions’) are widely suspected to be weak, in the sense that an advanced AI will come to work around them or uproot them: ‘never lie’ or ‘never kill’ or ‘never refuse a direct order from the president’ are poor substitutes for active transparency, niceness, and corrigibility.&lt;/p&gt;
&lt;p&gt;Conceiving of corrigibility or transparency or niceness as adverbial practices is a promising way to capture the normal, sensible way we want an agent to value corrigibility or transparency or niceness, which intuitively-consequentialist values and deontology both fail to capture. We want an agent that (e.g.) actively tries to be transparent, and to cultivate its own future transparency and its own future valuing of transparency, but that will not (e.g.) engage in deception and plotting when it expects a high future-transparency payoff.&lt;/p&gt;
&lt;p&gt;If this is right, then eudaimonic rationality is not a matter of congratulating ourselves for our richly human ways of reasoning, valuing, and acting but a key to basic sanity. What makes human life beautiful is also what makes human life possible at all.&lt;/p&gt;
&lt;h2 id="appendix-excellence-and-deep-reinforcement-learning"&gt;Appendix: Excellence and Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Within the context of broadly RL-based training of deep neural networks, it may be possible to give some more concrete meaning to what I called the material efficacy condition for a property qualifying as an adverbial practices. We can now understand the material efficacy condition on &lt;em&gt;x&lt;/em&gt; partly in terms of the conditions necessary for ‘promote &lt;em&gt;x&lt;/em&gt;-ness &lt;em&gt;x&lt;/em&gt;-ingly’ to be a viable target for RL. Consider an RL training regimen where &lt;em&gt;x&lt;/em&gt;-ness is rewarded but aggregate &lt;em&gt;x&lt;/em&gt;-ness reward is bounded with some asymptotic function on the sum. For &lt;em&gt;x&lt;/em&gt; to meet the RL version of the material efficacy condition, it must be possible to design an initial reward model (most likely LLM-based) that assigns actions an &lt;em&gt;x&lt;/em&gt;-ness rating such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;x&lt;/em&gt;-ness rating is enough of a natural abstraction that reinforcement of high &lt;em&gt;x&lt;/em&gt;-ness actions generalizes.&lt;/li&gt;
&lt;li&gt;If high &lt;em&gt;x&lt;/em&gt;-ness action both depends on having capital of some kind and is suboptimal from the viewpoint of general power-seeking, there must typically be some high &lt;em&gt;x&lt;/em&gt;-ness actions that approximately make up for the (future &lt;em&gt;x&lt;/em&gt;-ness wise) opportunity cost by creating capital useful for &lt;em&gt;x&lt;/em&gt;-ing.&lt;br /&gt;(Illustration: If you dream of achieving great theater acting, one way to do it is to become President of the United States and then pursue a theater career after your presidency, immediately getting interest from great directors who'll help you achieve great acting. Alternatively, you could start in a regional theater after high school, demonstrate talent by acting well, get invited to work with better and better theater directors who develop your skills and reputation -- skills and reputation that are not as generally useful as those you get by being POTUS -- and achieve great acting through that feedback loop.)&lt;/li&gt;
&lt;li&gt;For any capability &lt;em&gt;y&lt;/em&gt; necessary to reward in training to produce effective AI, there must be an unlimited local-optimization path of Pareto improvement for &lt;em&gt;x&lt;/em&gt;-ness and &lt;em&gt;y&lt;/em&gt; together.&lt;br /&gt;(Illustration: Maybe the most effective kind of engineering manager is ruthless; a nice engineering manager can still grow in effectiveness without becoming less nice, because there are many effective nice-engineering-management techniques to master.)&lt;/li&gt;
&lt;li&gt;Successful initial training in ‘promoting &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ allows the model to be used as a basis for a new reward model which human experts judge as better-capturing our concept of &lt;em&gt;x&lt;/em&gt;-ness. The process should be iterable.&lt;br /&gt;(If the model is LLM-based, improved performance may automatically lead to improved understanding of the &lt;em&gt;x&lt;/em&gt;-ness concept. More generally, data from training runs as well the model’s value-function could be used to refine an &lt;em&gt;x&lt;/em&gt;-ness rating that more strongly implements conditions 1-3.)&lt;/li&gt;
&lt;/ol&gt;


&lt;!--kg-card-end: markdown--&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://thegradient.pub/content/images/2026/02/ramelli-featured.jpg" /&gt;&lt;/div&gt;&lt;!--kg-card-begin: markdown--&gt;&lt;h2 id="preface"&gt;Preface&lt;/h2&gt;
&lt;p&gt;This essay argues that rational people don’t have goals, and that rational AIs shouldn’t have goals. Human actions are rational not because we direct them at some final ‘goals,’ but because we align actions to &lt;em&gt;practices&lt;/em&gt;: networks of actions, action-dispositions, action-evaluation criteria, and action-resources that structure, clarify, develop, and promote themselves. If we want AIs that can genuinely support, collaborate with, or even &lt;strong&gt;comply&lt;/strong&gt; with human agency, AI agents’ deliberations must share a “type signature” with the practices-based logic we use to reflect and act.&lt;/p&gt;
&lt;p&gt;I argue that these issues matter not just for aligning AI to grand ethical ideals like human flourishing, but also for aligning AI to core safety-properties like transparency, helpfulness, harmlessness, or corrigibility. Concepts like ’harmlessness’ or ‘corrigibility’ are unnatural -- brittle, unstable, arbitrary -- for agents who’d interpret them in terms of goals or rules, but natural for agents who’d interpret them as dynamics in networks of actions, action-dispositions, action-evaluation criteria, and action-resources.&lt;/p&gt;
&lt;p&gt;While the issues this essay tackles tend to sprawl, one theme that reappears over and over is the relevance of the formula ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly.’ I argue that this formula captures something important about both meaningful human life-activity (art is the artistic promotion of art, romance is the romantic promotion of romance) and real human morality (to care about kindness is to promote kindness kindly, to care about honesty is to promote honesty honestly).&lt;/p&gt;
&lt;p&gt;I start by asking: What follows for AI alignment if we take the concept of eudaimonia -- active, rational human flourishing -- seriously? I argue that the concept of eudaimonia doesn’t simply point to a desired state or trajectory of the world that we should set as an AI’s optimization target, but rather points to a structure of deliberation different from standard consequentialist rationality. I then argue that this form of rational activity and valuing, which l call &lt;em&gt;eudaimonic rationality&lt;/em&gt;, is a useful or even necessary framework for the agency and values of human-aligned AIs.&lt;/p&gt;
&lt;p&gt;These arguments are based both on the dangers of a “type mismatch” between human flourishing as an optimization target and consequentialist optimization as a form, and on certain material advantages that eudaimonic rationality plausibly possesses in comparison to deontological and consequentialist agency with regard to stability and safety.&lt;/p&gt;
&lt;p&gt;The concept of eudaimonia, I argue, suggests a form of rational activity without a strict distinction between means and ends, or between ‘instrumental’ and ‘terminal’ values. In this model of rational activity, a rational action is an element of a valued practice in roughly the same sense that a note is an element of a melody, a time-step is an element of a computation, and a moment in an organism’s cellular life is an element of that organism’s self-subsistence and self-development.&lt;/p&gt;
&lt;p&gt;My central claim is that our intuitions about the nature of human flourishing are implicitly intuitions that eudaimonic rationality can be functionally robust in a sense highly critical to AI alignment. More specifically, I argue that in light of our best intuitions about the nature of human flourishing it’s plausible that eudaimonic rationality is a &lt;em&gt;natural&lt;/em&gt; form of agency, and that eudaimonic rationality is &lt;em&gt;effective&lt;/em&gt; even by the light of certain consequentialist approximations of its values. I then argue that if our goal is to align AI in support of human flourishing, and if it is furthermore plausible that eudaimonic rationality is natural and efficacious, then many classical AI safety considerations and ‘paradoxes’ of AI alignment speak in favor of trying to instill AIs with eudaimonic rationality.&lt;/p&gt;
&lt;p&gt;Throughout this essay, I will sometimes explicitly and often implicitly be asking whether some form of agency or rationality or practice is &lt;em&gt;natural&lt;/em&gt;. The sense of ‘natural’ I’m calling on is certainly related to the senses used in various virtue-ethical traditions, but the interest I take in it is less immediately normative and more material or technical. While I have no reductive definition at hand, the intended meaning of ‘natural’ is related to stability, coherence, relative non-contingency, ease of learnability, lower algorithmic complexity, convergent cultural evolution, hypothetical convergent cultural evolution across different hypothetical rational-animal species, potential convergent evolution between humans and neural-network based AI, and targetability by ML training processes. While I will also make many direct references to AI alignment, this question of material naturalness is where the real alignment-critical action takes place: if we learn that certain exotic-sounding forms of agency, rationality, or practice are both themselves natural and make the contents of our all-too-human values natural in turn, then we have learned about good, relatively safe, and relatively easy targets for AI alignment.&lt;/p&gt;
&lt;p&gt;Readers may find the following section-by-section overview useful for navigating the essay:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part &lt;em&gt;I&lt;/em&gt; presents a class of cases of rational deliberation that are very different from the Effective Altruism-style optimization many in the AI-alignment world treat as the paradigm of rational deliberation. I call this class of rational deliberations 'eudaimonic rationality,' and identify it with the form of rationality that guides a mathematician or an artist or a friend when they reflect on what to do in mathematics or in art or in friendship.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;II&lt;/em&gt; looks at the case of research mathematics (via an account by Terry Tao) as an example of eudaimonic rationality at work. What does a mathematician try to do in math? I say she tries to be &lt;em&gt;mathematically excellent&lt;/em&gt;, which involves promoting mathematical excellence through mathematical excellence, and that this structure is closely related to why 'mathematical excellence' can even be a concept.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;III&lt;/em&gt; argues that for eudaimonic agents such as a mathematician who is trying to do excellent mathematics, distinctions between ‘instrumental goods’ and ‘terminal goods’ (intrinsic goods) are mostly unnatural. This makes reflection about values go very differently for a eudaimonic agent than for an Effective Altruism-style agent. Instead of looking to reduce a network of causally intertwined apparent values to a minimal base of intrinsic values that “explains away” the rest as instrumental, a eudaimonic agent looks for organism-like causal coherence in a network of apparent values.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IV&lt;/em&gt; cashes out the essay’s central concepts: A &lt;em&gt;eudaimonic practice&lt;/em&gt; is a network of actions, action-dispositions, action-evaluation criteria, and action-resources where high-scoring actions reliably (but defeasibly) causally promote future high-scoring actions. &lt;em&gt;Eudaimonic rationality&lt;/em&gt; is a class of reflective equilibration and deliberation processes that assume an underlying eudaimonic practice and seek to optimize aggregate action-scores specifically via high-scoring action.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;V&lt;/em&gt;, I argue that many puzzles and ‘paradoxes’ about AI alignment are driven by the assumption that mature AI agents will be Effective Altruism-style optimizers. A “type mismatch” between Effective Altruism-style optimization and eudaimonic rationality makes it nearly impossible to translate the interests of humans -- agents who practice eudaimonic rationality -- into a utility function legible to an Effective Altruism-style optimizer AI. But this does not mean that our values are inherently brittle, unnatural, or wildly contingent: while Effective Altruism-style optimizers may well be a natural type of agent, eudaimonic agents (whether biological or AI) are highly natural as well.&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;VI&lt;/em&gt;, I ask whether a eudaimonically rational AI agent devoted to a practice like mathematical research would be safe by default. I argue that a practice like mathematical research plausibly has natural boundaries that exclude moves like ‘take over planet to get more compute for mathematical research,’ but the issue is nuanced. I propose that a practice’s boundaries (for which there may be multiple good natural candidates) may be most stable when a practice is paired with a support practice: a complementary practice for dealing with practice-external issues of maintenance and resource-gathering.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VII&lt;/em&gt; develops the idea of ‘support practices’: eudaimonically rational ways to support eudaimonic practices. We famously want AI agents to help humans lead flourishing lives, but how can we define the purview of this ‘help’? I argue that many core human practices have natural support-practices with a derived eudaimonic structure: the work of good couples’ therapist, for instance, is intertwined with but clearly distinct from a couple’s relationship-practice. Still, there remains a problem: a support-practice AI might harm other people and practices to help the people or practice it’s supporting.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;VIII&lt;/em&gt; moves from eudaimonic rationality in general to eudaimonically rational morality. I argue that thinking of moral virtues as domain-general, always-on practices solves key AI-alignment-flavored problems with consequentialist and deontological moralities. The core idea is that the conditions for e.g. ‘kindness’ being a robust moral virtue are akin to the conditions for ‘mathematical excellence’ being a meaningful concept: it must be generally viable to promote kindness in yourself and others kindly. It’s this structure, I argue, that gives moral virtues material standing in a ‘fitness landscape’ riven by pressures from neural-network generalization dynamics, reinforcement-learning cycles, and social and natural selection.&lt;/li&gt;
&lt;li&gt;Part &lt;em&gt;IX&lt;/em&gt; argues that eudaimonic agents have some unique forms of robustness to RL-like and Darwinian-like dynamics that tend to mutate the values of EA-style optimizers. In particular, eudaimonic agents should be very robust to the risk of developing rogue subroutines (sometimes called ‘the inner alignment problem’).&lt;/li&gt;
&lt;li&gt;In part &lt;em&gt;X&lt;/em&gt; I discuss canonical AI-safety desiderata like transparency, corrigibility, and (more abstractly) niceness. I argue that treating these properties as moral virtues in my sense -- domain-general, always-on eudaimonic practices --  dissolves problems and paradoxes that arise when treating them as goals, as rules, or even as character traits. I end with an appendix on some prospects for RL regimes geared towards eudaimonic rationality.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="i-rational-action-in-the-good-life"&gt;I. Rational Action in the Good Life&lt;/h2&gt;
&lt;p&gt;I start with a consideration of the nature of the good we hope AI alignment can promote. With the exception of hedonistic utilitarians, most actors interested in AI alignment understand our goal as a future brimming with human (and other sapient-being) flourishing: persons living good lives and forming good communities. What I believe many fail to reflect on, however, is that on any plausible conception human flourishing involves a kind of rational activity. Subjects engaged in human flourishing act in intelligible ways subject to reason, reflection, and revision, and this form of rational care and purposefulness is itself part of the constitution of our flourishing. I believe this characterization of human flourishing is relatively uncontroversial upon reflection, but it raises a kind of puzzle if we’re used to thinking of rationality in consequentialist (or consequentialist-with-deontological-constraints) terms: just what goal is the rational agency involved in human-flourishing activity directed towards?&lt;/p&gt;
&lt;p&gt;One obvious answer would be that, like all properly aligned rationality, the rational agency involved in human-flourishing activities is geared towards maximizing human (and other sapient) flourishing. But we should quickly find ourselves confused about the right way to describe the contribution that rational agency in human-flourishing activities makes to human flourishing. It seems neither appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing only by enacting rationality (by selecting actions that are intrinsically valuable when rationally selected), nor appropriate to say that the rational agency involved in a human-flourishing activity contributes to human flourishing just instrumentally (by selecting actions that causally promote human flourishing).&lt;/p&gt;
&lt;p&gt;The first option reduces our rational actions to something ritualistic, even as the good life surely involves mathematicians working to advance mathematics, friends speaking heart-to-heart to deepen intimacies, gymnasts practicing flips to get better at flips, and novelists revising chapters to improve their manuscripts. The second option threatens to make the good in the good life just impossible to find -- if speaking heart-to-heart is not the good of friendship, and working on math is the not the good of mathematics, then what is?&lt;/p&gt;
&lt;p&gt;This essay argues that deliberative reasoning about the good life is neither directed towards goals external to rational action nor directed towards rational action as an independent good, but towards acts of excellent participation in a valued open-ended process. I then go on to argue that the ‘eudaimonic’ structure of deliberation salient in cases like math or friendship (sloganized as ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) is also subtly critical in more worldly, strategic, or morally high-stakes contexts, and constitutes a major organizing principle of human action and deliberation.&lt;/p&gt;
&lt;h2 id="ii-what-is-a-practice"&gt;II. What Is a Practice?&lt;/h2&gt;
&lt;p&gt;Since ‘human flourishing’ can seem mysterious and abstract, let’s focus on some concrete eudaimonic practices. Consider practices like math, art, craft, friendship, athletics, romance, play, and technology, which are among our best-understood candidates for partial answers to the question ‘what would flourishing people in a flourishing community be doing.’ From a consequentialist point of view, these practices are all marked by extreme ambiguity -- and I would argue indeterminacy -- about what’s instrumental and what’s terminal in their guiding ideas of value. Here, for example, is Terry Tao’s account of goodness in mathematics:&lt;/p&gt;
&lt;p&gt;‘The very best examples of good mathematics do not merely fulfil one or more of the criteria of mathematical quality listed at the beginning of this article, but are more importantly part of a greater mathematical story, which then unfurls to generate many further pieces of good mathematics of many different types. Indeed, one can view the history of entire fields of mathematics as being primarily generated by a handful of these great stories, their evolution through time, and their interaction with each other. I would thus conclude that good mathematics [...] also depends on the more “global” question of how it fits in with other pieces of good mathematics, either by building upon earlier achievements or encouraging the development of future breakthroughs. [There seems] to be some undefinable sense that a certain piece of mathematics is “on to something”, that it is a piece of a larger puzzle waiting to be explored further.’&lt;/p&gt;
&lt;p&gt;It may be possible to give some post-hoc decomposition of Tao’s account into two logically distinct components -- a description of a utility-function over mathematical achievements and an empirical theory about causal relations between mathematical achievements -- but I believe this would be artificial and misleading. On a more natural reading, Tao is describing some of the conditions that make good mathematical practice a eudaimonic practice: In a mathematical practice guided by a cultivated mathematical practical-wisdom judgment (Tao’s ‘undefinable sense that a certain piece of mathematics is “on to something”’), present excellent performance by the standard of the practical-wisdom judgment reliably develops the conditions for future excellent performance by the standard of the mathematical practical-wisdom judgment, as well as cultivating our practical and theoretical grasp of the standard itself.&lt;/p&gt;
&lt;p&gt;This is not to suggest that ‘good mathematics causes future good mathematics’ is a full definition or even full informal description of good mathematics. My claim is only that the fact that good mathematics has a disposition to cause future good mathematics reveals something essential about our concept of good mathematics (and about the material affordances enabling this concept). By analogy, consider the respective concepts healthy tiger and healthy human: It's essential to the concept of a healthy tiger that &lt;em&gt;x&lt;/em&gt; being a healthy tiger now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy tiger 5 minutes in the future (since a healthy tiger body self-maintains and enables self-preservation tiger-behaviours), and essential to the concept of a healthy human that &lt;em&gt;x&lt;/em&gt; being a healthy human now has a disposition to make &lt;em&gt;x&lt;/em&gt; be a healthy human 5 minutes in the future (since a healthy human body self-maintains and enables self-preservation human behaviours). But these formulae aren't yet complete descriptions of 'healthy tiger' or 'healthy human,' as evidenced by the fact that we can tell apart a healthy tiger from a healthy human.&lt;/p&gt;
&lt;p&gt;Crucially, the mathematical practical-wisdom described by Tao is not entirely conceptually opaque beyond its basic characterization as a self-cultivating criterion for self-cultivating excellence in mathematical activity. Mathematical flourishing can partly be described as involving the instantiation of a relation (a mathematical-practice relation of ‘developmental connectedness’) among instantiations of relatively individually definable and quantifiable instances of mathematical value such as elegant proofs, clear expositions, strong theorems, cogent definitions and so on. Furthermore, this relation of developmental connectedness is partly defined by its reliable tendency to causally propagate instances of more individually and locally measurable mathematical value (instances of elegant proofs, clear exposition, strong theorems, cogent definitions and so on):&lt;/p&gt;
&lt;p&gt;[I believe] that good mathematics is more than simply the process of solving problems, building theories, and making arguments shorter, stronger, clearer, more elegant, or more rigorous, though these are of course all admirable goals; while achieving all of these tasks (and debating which ones should have higher priority within any given field), we should also be aware of any possible larger context that one’s results could be placed in, as this may well lead to the greatest long-term benefit for the result, for the field, and for mathematics as a whole.&lt;/p&gt;
&lt;p&gt;One could, again, try to interpret this causal relationship between excellence according to Tao’s ‘organicist’ (or ‘narrative’ or ‘developmental’) sense of good mathematics and the reliable propagation of narrow instances of good mathematics as evidence of a means-ends rational relation, where additive maximization of narrow instances of mathematical value is the utility function and ‘organicist’ mathematical insight is the means. For Tao, however, the evidential import of this causal relationship goes exactly the other way -- it suggests a unification of our myriad more-explicit and more-standalone conceptions of mathematical excellence into a more-ineffable but more-complete conception. As Tao says:&lt;/p&gt;
&lt;p&gt;It may seem from the above discussion that the problem of evaluating mathematical quality, while important, is a hopelessly complicated one, especially since many good mathematical achievements may score highly on some of the qualities listed above but not on others [...] However, there is the remarkable phenomenon that good mathematics in one of the above senses tends to beget more good mathematics in many of the other senses as well, leading to the tentative conjecture that perhaps there is, after all, a universal notion of good quality mathematics, and all the specific metrics listed above represent different routes to uncover new mathematics, or different stages or aspects of the evolution of a mathematical story.&lt;/p&gt;
&lt;h2 id="iii-inverting-consequentialist-reflection"&gt;III. Inverting Consequentialist Reflection&lt;/h2&gt;
&lt;p&gt;Tao’s reasoning about local and global mathematical values exemplifies a central difference between consequentialist rationality and eudaimonic rationality, now taken as paradigms not only for selecting actions but for reflecting on values. (Paradigms for what philosophers will sometimes call ‘reflective equilibration.’) Within the paradigm of consequentialist rationality, if excellence in accordance with a holistic, difficult-to-judge apparent value (say ‘freedom’) is reliably a powerful causal promoter of excellence in accordance with more explicit, more standalone apparent values (say ‘material comfort,’ ‘psychological health,’ ‘lifespan’), this relationship functions as evidence against the status of the holistic prima-facie value as a constitutive -- as opposed to instrumental -- value. Within the paradigm of eudaimonic rationality, by contrast, this same relationship functions as evidence for the status of the holistic prima-facie value as a constitutive value.&lt;/p&gt;
&lt;p&gt;For a (typical) consequentialist-rationality reflection process, evidence that the excellence of a whole causally contributes to the excellences of its parts explains away our investment in the excellence of the whole. The “coincidence” that the intrinsically valuable whole is also instrumentally valuable for its parts is taken to suggest a kind of double-counting error --  one we “fix” by concluding that the whole has no constitutive value but valuing the whole is an effective heuristic under normal circumstances. A eudaimonic-rationality reflective equilibration, by contrast, treats instrumental causal connections between excellences as evidence that our notions of excellence are picking out something appropriately ‘substantive.’&lt;br /&gt;For eudaimonic-rationality reflective equilibration, it is the discovery of causal and common-cause relations among excellences that ratifies our initial sense that caring about these excellences is eudaimonically rational. The discovery of these causal connections functions as evidence that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The ‘local’ excellences we care about are resonant or fruitful, in that they causally promote each other and the holistic excellences in which they participate.&lt;/li&gt;
&lt;li&gt;The ‘holistic’ excellences we care about are materially efficacious and robust, in that they causally promote both the more local excellences that participate in them and their own continuation as future holistic excellence.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In my view this is the right way to treat causal connections between (apparent) values if we’re hoping to capture actual human values-reflection, and points to an important strength of the eudaimonic rationality paradigm: Eudaimonic rationality dissolves the ‘paradox’ that in real-life arguments about the value of various human enterprises (e.g. the value of branches of science, branches of art, branches of sport), judgments of intrinsic value typically seek succor from some kind of claim to instrumental value. For example, a defense of the importance of research in quantum physics will often appeal to the wonderful technological, mathematical, and special-sciences applications quantum physics gave us, without meaning to reduce the worth of quantum physics to these applications. On my reading, these appeals aren't just additive -- 'aside from the intrinsic value there is also instrumental value' -- but presentations of evidence that research in quantum physics is a resonant part of a flourishing organic whole (e.g. the civilizational whole of ‘modern science and technology’).&lt;/p&gt;
&lt;p&gt;I believe that without 'organicism' of the kind described above, one faces a serious dilemma whenever one argues for the intrinsic worth of a pursuit or norm: either we stress the value's independence from all benefits and applications and make the claim of value dogmatic and irrelevant, or else we invite an instrumentalist reduction that ‘explains away’ the appearance of intrinsic value. Indeed, I’d argue that organicism of this kind is even necessary to make sense of caring about rationality (including truth, knowledge, non-contradiction and so on) non-instrumentally at all: the ‘paradox’ of rationality as a substantive value is that the typical usefulness of rationality suggests an error-theory about its apparent intrinsic value, since it’s a strange coincidence that rationality is both so typically useful and so intrinsically good. On an organicist account, however, we expect that major forms of excellence endemic to human life -- thought, understanding, knowledge, reasoned action -- both typically promote each other and typically promote our material flourishing and causal leverage on the world.&lt;/p&gt;
&lt;h2 id="iv-the-material-efficacy-condition"&gt;IV. The Material Efficacy Condition&lt;/h2&gt;
&lt;p&gt;Returning now to Tao’s account of good mathematics, let’s take final stock of our interpretation. I argue that mathematical excellence (the property marking ‘the very best examples of good mathematics’) according to Tao satisfies the following conditions, which I believe Tao intends as necessary but not sufficient:&lt;/p&gt;
&lt;p&gt;A) Mathematical excellence is a property of mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;B) An excellent mathematical-activity instance performed today is excellent partly by virtue of satisfying the mathematical-practice relation ‘builds on’ with regard to past excellent mathematical-activity instances.&lt;/p&gt;
&lt;p&gt;C) An excellent mathematical-activity instance performed today is excellent partly by virtue of having a reliable causal tendency to bring about future excellent mathematical-activity instances that satisfy the mathematical-practice relation ‘builds on’ with regard to it.&lt;/p&gt;
&lt;p&gt;D) Instantiation of more local, more individually measurable criteria of mathematical-activity goodness such as elegant proofs, clear expositions, and strong theorems is a typical correlate of mathematical excellence.&lt;/p&gt;
&lt;p&gt;E) At a given moment in a given mathematical field, the instantiation of mathematical excellence will be predictably better-correlated with the instantiation of certain local criteria of mathematical-activity goodness than with others.&lt;/p&gt;
&lt;p&gt;Should we take these traits to collectively describe something more like a decision-procedure called ‘mathematical excellence’ that mathematicians should try to follow, or something more like an event called ‘mathematical excellence’ whose aggregate future-occurrences mathematicians should aspire to maximize? My contention is that Tao’s account is inherently ambiguous, and for a good reason: in ordinary circumstances there is no significant practical difference between doing excellent mathematics and doing instrumentally optimal mathematics with regard to maximizing future aggregate excellent mathematics. This isn’t to say that doing excellent mathematics is the instrumentally optimal action among all possible actions with regard to aggregate future excellent mathematics, but that (in ordinary circumstances) it is the instrumentally optimal choice from among mathematical actions with regard to aggregate future excellent mathematics.&lt;/p&gt;
&lt;p&gt;I propose that the rough matchup between mathematical excellence and optimal (among mathematical actions) instrumental promotion of aggregate mathematical excellence is neither an empirical miracle nor something determined ‘by definition’ in a trivializing sense. Rather, ‘mathematical excellence’ as used by Tao is a concept that has a referent only if there is a possible property &lt;em&gt;x&lt;/em&gt; that satisfies both desiderata &lt;em&gt;A&lt;/em&gt;-&lt;em&gt;E&lt;/em&gt; and the additional criterion that among mathematical actions, actions that are optimal as instantiations of &lt;em&gt;x&lt;/em&gt; are also roughly optimal for maximizing aggregate future instantiation of &lt;em&gt;x&lt;/em&gt;-ness.&lt;/p&gt;
&lt;p&gt;This is what I would describe as the material efficacy condition on eudaimonic rationality. In order for a practice to be fit for possessing internal criteria of flourishing, excellence, and eudaimonic rationality, a practice must materially allow for an (under normal circumstances) optimally self-promoting property &lt;em&gt;x&lt;/em&gt; that strongly correlates with a plethora of more local, more individually measurable properties whose instantiation is prima facie valuable. Stated more informally, there must exist a two-way causal relationship between a practice’s excellence and the material, psychological, and epistemic effects of its excellence, such that present excellence reliably materially, psychologically, and epistemically promotes future excellence.&lt;/p&gt;
&lt;h2 id="v-practices-and-optimization"&gt;V. Practices and Optimization&lt;/h2&gt;
&lt;p&gt;I earlier said that if human flourishing involves practicing eudaimonic rationality, there may well be a “type mismatch” between human flourishing and the kind of consequentialist optimization we often associate with the idea of an agenticly mature future AI. In fact, I believe that implicitly recognizing but misdiagnosing this type mismatch is at least partially responsible for MIRI-style pessimism about the probability of aligning any artificial agents to human values.&lt;/p&gt;
&lt;p&gt;On my view, the secret to relatively successful alignment among humans themselves (when there is successful alignment among humans) lies in the role attempted excellence plays as a filter on human interventions in the future trajectory of a eudaimonic practice. To the degree that humans value a given eudaimonic practice, they are committed to effecting their vision for the practice’s future-trajectory primarily by attempting acts of excellence in the present: we stake our intended effect over the practice’s future-trajectory on the self-propagating excellence of our intervention. While this ‘filter’ doesn’t necessarily stop the worst interventions from being harmful (there are forms of ‘anti-excellence’ that also have self-promotion powers), I contend that this filter is mechanically crucial for the possibility of reliably benign or positive interventions.&lt;/p&gt;
&lt;p&gt;What do I mean? Consider the difference between a world where scientists typically try to propagate (what they believe to be) the scientific truth mainly by means of submitting research work to scientific institutions, and a world where scientists typically try to propagate (what they believe to be) the scientific truth by means including propaganda, fraud, threats, bribery, and slander. As Liam Kofi Bright demonstrates in On Fraud, a community of consequentialist scientists devoted to maximizing truth will predictably match the latter model. I believe one lesson to be drawn is that humans’ ability to collaborate in the promotion of science depends on our ability to scientifically collaborate in the promotion of science, rather than throttle the future trajectory of science every-which-way  our financial and political powers based on our individual beliefs about the optimal trajectory of science.&lt;/p&gt;
&lt;p&gt;A flourishing eudaimonic practice is, above all, a natural-selection-like mechanism whose fitness-function selects among attempted acts of excellence the ones conducive to (and constitutive of) the practice’s flourishing, propagating the excellence these acts instantiate. When people committed to a eudaimonic practice make their attempted interventions into the future trajectory of the practice via acts of attempted excellence, the natural-selection-like mechanism embodied by the practice  (rather than any single individual’s theory of optimal future trajectory) is the aligned intelligence determining the practice’s future trajectory.&lt;/p&gt;
&lt;p&gt;The explanation here, again, is partly causal and partly constitutive: a practice’s “ultimate” norms of excellence, including the “ultimate” epistemic and alethic norms of a discursive practice, are partly defined by the succession of norms in the course of a practice’s development through best-efforts attempted excellence. Although this may be no deterrent to an already god-like optimizer who can simulate entire civilizational trajectories, an agent short of these capacities can best act on their vision of the optimal future-trajectory of a practice by attempting an excellent contribution to the practice.&lt;/p&gt;
&lt;p&gt;The second aspect of our type-mismatch is much more in the weeds: In my analysis so far, I discussed the overall excellence of the trajectory of a eudaimonic practice much like a consequentialist might discuss a quantity of utility. This may be taken to suggest that a ‘sophisticated consequentialist’ or ‘universal consequentialist’ could easily accommodate the implications of the so-called type mismatch by treating them as instrumental, decision-procedure level considerations against naive optimization. In fact, quantities like ‘aggregate democracy’ or ‘overall mathematical excellence’ are (on my view) practice-internal quantities that quickly lose meaning if we try to apply them outside the scope of a ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ decision-procedure.&lt;/p&gt;
&lt;p&gt;What do I mean?  Consider, for example, the practice of philosophy. Here are some questions that should arise for a consequentialist planner (including a sophisticated consequentialist planning decision-procedures or habits) who values philosophy practice-trajectories: Does rating (e.g.) Aristotle’s or Dharmakirti’s philosophical achievements as the most excellent achievements in philosophy imply that we should “tile the universe” with independent practice-trajectories designed to reproduce classical Greek or Indian philosophy? If not, is it because we should assign non-linearly greater value to longer trajectories? Or should we discount trajectories that have parallel contents? Or should we analyze the greatness of early achievements in a practice as mostly instrumental greatness but the greatness of later achievements in a practice as mostly intrinsically valuable? These are all, I believe, bad questions that have only arbitrary answers. To an agent trying to promote philosophy by doing excellent philosophical work, the bad questions above are naturally out of scope. The agent uses the concept of ‘aggregate philosophical excellence’ or ‘a philosophy practice-trajectory’s value’ only to reason about the philosophical influence of their work on the trajectory of the philosophy-practice in which it participates. Choosing an excellent action in practice requires (at most) quantitative comparison between different possible paths for a practice-trajectory, not quantitative comparison between possible worlds containing independent practice-trajectories sprinkled throughout time and space.&lt;/p&gt;
&lt;h2 id="vi-prospects-and-problems-for-ai"&gt;VI. Prospects and Problems for AI&lt;/h2&gt;
&lt;p&gt;Is this good news for AI alignment? It’s certainly good news that (if I’m right) eudaimonic practices are something like natural kinds marked by a causal structure that enables a self-developing excellence well-correlated with multiple naive local measures of quality. But does this mean we could develop a stable and safe (e.g.) ‘mathematical excellence through mathematical excellence’ AI? If we create a fully agentic AI mathematician, will it naturally abstain from trying to extend its longevity or get more resources (even for doing mathematics) other than by impressing us with excellent mathematical work?  I think that prospects are good, but not simple.&lt;/p&gt;
&lt;p&gt;I believe ‘mathematical excellence through mathematical excellence’ really can powerfully scope what mechanisms for shaping the future an AI cares to activate. An AI trained to follow ‘promote mathematics mathematically’ will only care about influencing the future by feeding  excellent mathematical work to mathematics’ excellence-propagation mechanism. But it’s harder to say whether the structure of mathematical practice also properly scopes what subactions can be taken as part of an instance of “doing math.” Is a human mathematician working on a would-be excellent proof in pen and paper practicing math when she is picking up a pen or flipping pages? When she is taking the bus to her office? When she’s buying amphetamines? And is an AI mathematician working on a would-be excellent proof practicing math when it opens a Python console? When it searches the web for new papers? When it harvests Earth for compute?&lt;/p&gt;
&lt;p&gt;I think these questions are complex, rather than nonsensical. Much like collective practices, individual practices -- for example a person’s or possibly an AI’s mathematical practice -- may possess functional organic unities that allow a meaningful distinction between internal dynamics (including dynamics of development and empowerment) and external interventions (including interventions of enhancement and provision). Still, it’s clear that eudaimonic practices do not exist in isolation, and that no practice can function without either blending with or relying on a “support practice” of some kind.&lt;/p&gt;
&lt;p&gt;How, then, do we rationally go about externally-oriented activities like building offices for mathematicians, performing elective reconstructive surgery on an athlete, or conducting couples therapy for romantic partners? And furthermore, how do we rationally go about allocating scarce resources useful for different practices, or judging whether to integrate (e.g.) performance-enhancing drugs into a practice?&lt;/p&gt;
&lt;p&gt;This is, I think, the fundamental question for AI alignment from the viewpoint of  ‘eudaimonic rationality.’ We want AI to support human eudaimonic practices -- and, if relevant, its own eudaimonic practices or participation in human eudaimonic practices -- in a eudaimonia-appropriate way. But how does the logic of eudaimonic rationality extend from eudaimonic practices to their support activities? How do we ‘eudaimonically-rationally’ do the dirty work that makes eudaimonia possible? My best answer is: carefully, kindly, respectfully, accountably, peacefully, honestly, sensitively.&lt;/p&gt;
&lt;h2 id="vii-from-support-practices-to-moral-practice"&gt;VII. From Support-Practices to Moral Practice&lt;/h2&gt;
&lt;p&gt;The theory of AI alignment, I propose, should fundamentally be a theory of the eudaimonic rationality of &lt;em&gt;support practices&lt;/em&gt;. One part of this theory should concern the ‘support’ relation itself, and analyze varieties of support practices and their appropriate relation to the self-determination of a eudaimonic practice: Support-practices such as acquiring resources for a practice, maintaining an enabling environment, coaching practitioners, conducting (physical or psychological) therapy for practitioners, devising technological enhancements for a practice, and educating the public about a practice, each have their own ‘role-morality’ vis-a-vis the practice they support. It is this part of the theory of ‘support practices’ that should, if all goes well in the theory’s construction, describe the various practice-external ways to eudaimonically-rationally act on a pro-attitude towards the aggregate excellence of the practice’s future trajectory without treating it like a quantity of utility. (Much like the concept of ‘mathematical action’ scopes the range of action-choices in such a way that decision-theoretic optimization of math’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint, so should the concepts of various types of ‘support action’ scope the range of action-choices in such a way that decision-theoretic optimization of a practice’s aggregate excellence becomes mostly well-behaved from an organicist viewpoint when the choice of actions is scoped.)&lt;/p&gt;
&lt;p&gt;What is more difficult is delineating the appropriate relationship of a support-practice to everything &lt;em&gt;&lt;strong&gt;outside&lt;/strong&gt;&lt;/em&gt; the practice it supports. What stops a marriage-therapist AI on Mars from appropriately tending to the marriage of a Mars-dwelling couple but harvesting Earth for compute to be a better therapist-AI for that couple? While we can perhaps imagine a person or AI taking up a support-role for ‘humanity’s flourishing as whole,’ so that there’s no outside to speak of, I am not sure that the concept of practice remains natural at this level of abstraction. We have no real grasp on a &lt;em&gt;direct&lt;/em&gt; practice of human flourishing, but rather grasp it as the harmonious and mutually supportive interaction of all eudaimonic practices and support-practices participating in the flourishing. And as there is, indeed, not much outside of the practice of human flourishing, it’s also unclear whether there is room for a support-practice &lt;em&gt;external&lt;/em&gt; to the field of human flourishing itself.&lt;/p&gt;
&lt;p&gt;It’s here that I want to call on the classic idea of domain-general virtues, the traditional centerpiece of theories of human flourishing. I propose that the cultivation of human flourishing as such --  the cultivation of the harmony of a multiplicity of practices, including their resource-hungry support practices -- is the cultivation of an &lt;em&gt;&lt;strong&gt;adverbial&lt;/strong&gt;&lt;/em&gt; practice that modulates each and every practice. What makes our practices ‘play nice’ together are our adverbial practices of going about  any practice &lt;em&gt;carefully&lt;/em&gt;, &lt;em&gt;kindly&lt;/em&gt;, &lt;em&gt;respectfully&lt;/em&gt;, &lt;em&gt;accountably&lt;/em&gt;, &lt;em&gt;peacefully&lt;/em&gt;, &lt;em&gt;honestly&lt;/em&gt;, &lt;em&gt;sensitively&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id="viii-virtue-decision-theory"&gt;VIII. Virtue decision-theory&lt;/h2&gt;
&lt;p&gt;Why think of qualities like kindness, respectfulness, or honesty as ‘practices’? The first reason is that devotion to a quality like kindness or honesty displays the same &lt;strong&gt;normative structure&lt;/strong&gt; with regard to means and ends as we find in devotion to a practice: An agent devoted to kindness cares about their own future kindness (and about the future kindness of others), but will seek to secure future kindness only by &lt;em&gt;kind means&lt;/em&gt;. The second reason is that qualities like kindness or honesty also approximately have the &lt;strong&gt;material&lt;/strong&gt; structure of a practice: there exist effective very kind strategies for promoting kindness in oneself and others, and when these strategies succeed they further increase affordances for effective very kind strategies for promoting kindness/honesty in oneself and others.&lt;/p&gt;
&lt;p&gt;The difference between adverbial practices like kindness or honesty and practices like research mathematics is that adverbial practices don’t have a “proprietary” domain. In a practice like research mathematics, the material structure of the domain does the most of work of directing agents to a eudaimonic form of agency all by itself, as long as the agents restrict themselves to in-domain actions. (Recall that we described mathematically excellent action as, in ordinary circumstances, the best action among mathematical action for maximizing aggregate mathematical excellence.) With a domain-general, adverbial practice like kindness the normative structure needs to do somewhat more heavy lifting.&lt;/p&gt;
&lt;p&gt;The following is a first pass at characterizing the &lt;strong&gt;normative structure&lt;/strong&gt; of an adverbial practice that values some action-quality &lt;em&gt;x&lt;/em&gt;. The corresponding material efficiency condition (or &lt;strong&gt;material structure&lt;/strong&gt;) necessary for the practice to be viable is that under ordinary circumstances this decision-procedure be instrumentally competitive with naive optimization of aggregate &lt;em&gt;x&lt;/em&gt;-ness:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Actions (or more generally 'computations') get an &lt;em&gt;x&lt;/em&gt;-ness rating. We define the agent’s expected utility conditional on a candidate action a as the sum of two utility functions: a bounded utility function on the &lt;em&gt;x&lt;/em&gt;-ness of a and a more tightly bounded utility function on the expected aggregate &lt;em&gt;x&lt;/em&gt;-ness of the agent's future actions conditional on a. (Thus the agent will choose an action with mildly suboptimal &lt;em&gt;x&lt;/em&gt;-ness if it gives a big boost to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness, but refuse certain large sacrifices of present &lt;em&gt;x&lt;/em&gt;-ness for big boosts to expected aggregate future &lt;em&gt;x&lt;/em&gt;-ness.)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A commitment to an adverbial practice that values &lt;em&gt;x&lt;/em&gt; is a commitment to promoting &lt;em&gt;x&lt;/em&gt;-ness (in oneself and others) &lt;em&gt;x&lt;/em&gt;-ingly.  The agent strikes a balance between promoting &lt;em&gt;x&lt;/em&gt;-ness and acting &lt;em&gt;x&lt;/em&gt;-ingly that heavily prioritizes acting &lt;em&gt;x&lt;/em&gt;-ingly when the two are in conflict, but if &lt;em&gt;x&lt;/em&gt; meets the material efficacy condition then the loss this balance imposes on future &lt;em&gt;x&lt;/em&gt;-ness will be small under normal circumstances, and -- from our point of view -- desirable in abnormal circumstances. This is because just like the practices of research mathematics, philosophy, or art, an adverbial practice is a crucial ‘epistemic filter’ on actions aiming to shape its future, and the (e.g.) future kindness a paperclipper-like future-kindness-optimizer optimizes for is probably not the kindness we want. What we know about kindness with relative certainty is that we’d like people and AIs here and now to act kindly, and to develop, propagate, and empower the habit and art of kindness in a way that is both kind and clever.&lt;/p&gt;
&lt;p&gt;To keep our conceptual system nicely organized, we might want distinguish merely (e.g.) very kind action from an action that is both very kind and highly promotive of future kindness in oneself and others, and call the latter sort of action excellently kind.  What I call the material efficacy conditions for adverbial practices states not that the kindest action best-promotes aggregate kindness, but that there are almost always action-options that are excellently kind: very kind actions that strongly promote aggregate kindness in oneself and others.&lt;/p&gt;
&lt;h2 id="ix-virtue-decision-theory-is-natural-for-humans-and-ais"&gt;IX. Virtue decision-theory is 'Natural' for Humans and AIs&lt;/h2&gt;
&lt;p&gt;I’ve said that the robustness or ‘naturalness’ of a practice’s normative structure (‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’) depends on the practice’s &lt;strong&gt;material structure&lt;/strong&gt;: the capacity of high &lt;em&gt;x&lt;/em&gt;-ness actions to causally promote aggregate &lt;em&gt;x&lt;/em&gt;-ness. I also said that in key real-world practices, commitment to &lt;em&gt;x&lt;/em&gt;-ing might optimize aggregate &lt;em&gt;x&lt;/em&gt;-ness even better than direct optimization would. These two claims are best understood together. On my view, the normative structure ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ appears prominently in human life because (given the right material structure) ‘promote &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ is a &lt;em&gt;much more stable&lt;/em&gt; than ‘promote &lt;em&gt;x&lt;/em&gt;.’&lt;/p&gt;
&lt;p&gt;How so? Both humans and any sufficiently dynamic AI agent operate in a world that subjects their agency, values, and dispositions to constant mutation pressures from RL-like and Darwinian-like processes. Eudaimonic deliberation is an RL-dynamics-native, Darwinian-dynamics-native operation: its direct object is a form of life that reinforces, enables, and propagates that same form of life. When an &lt;em&gt;x&lt;/em&gt;-ing successfully promotes (in expectation) aggregate &lt;em&gt;x&lt;/em&gt;-nes, the fact of its success itself promotes &lt;em&gt;x&lt;/em&gt;-ing because it reverberates via ubiquitous RL-like and Darwinian-like processes that reinforce (a generalization of) successful action. The material structure of a practice is the backbone that makes reliable success and meaningful generalization possible -- the right ecology of  neural-network generalization dynamics, reinforcement-learning feedback loop dynamics, and neural and environmental selection dynamics.&lt;/p&gt;
&lt;p&gt;An EA-style optimizer trying to minimize risk from optimization-goal-mutation, by contrast, is fighting an uphill battle to foresee and contain the RL-like and Darwinian-like side effects of its optimization actions. One critical mutation-pressure in particular is the risk that an optimizer agent will cultivate, reinforce, and materially empower subroutines (what high-church alignment theory calls ‘mesaoptimizers’) that initially serve the optimization goal but gradually distort or overtake it. For example, if a pro-democracy government instates a secret police to detect and extrajudicially kill anti-democracy agitators, and the government increases the secret police’s funding whenever the police convincingly reports discovering an agitator, the secret police might grow into a distorting influence on the government’s democracy-promotion effort. In light or risks like this, it’s not surprising that oppressive democracy-promotion is generally considered an unserious or dishonest idea: even if an agent were to abstract some concept of ‘aggregate democracy’ from democratic practice into a consequentialist value, it’s plausible that the agent should then immediately revert to a commitment to democratic practice (‘promote democracy democratically’) on sophisticated-consequentialist grounds.&lt;/p&gt;
&lt;p&gt;We should perhaps imagine eudaimonic practices as fixed points at the end of a chain of mesaoptimisers taking over outer optimisers and then being taken over by their own mesaoptimisers in turn. What the practice contributes that puts a stop to this process concept of &lt;em&gt;x&lt;/em&gt;-ness that’s applicable to every agentic subroutine of &lt;em&gt;x&lt;/em&gt;-ing across all nesting levels, so that &lt;em&gt;x&lt;/em&gt;-ness is reinforced (both directly and through generalization) across all subroutines and levels.&lt;/p&gt;
&lt;h2 id="x-virtue-decision-theory-is-safe-in-humans-and-ais"&gt;X. Virtue-decision-theory is Safe in Humans and AIs&lt;/h2&gt;
&lt;p&gt;Let’s talk about AI alignment in the more narrow, concrete sense. It’s widely accepted that if early strategically aware AIs possess values like corrigibility, transparency, and perhaps niceness, further alignment efforts are much more likely to succeed. But values like corrigibility or transparency or niceness don’t easily fit into an intuitively consequentialist form like ‘maximize lifetime corrigible behavior’ or ‘maximize lifetime transparency.’ In fact, an AI valuing its own corrigibility or transparency or niceness in an intuitively consequentialist way can lead to extreme power-seeking: the AI should seek to violently remake the world to (for example) protect itself from the risk that humans will modify the AI  to be less corrigible or transparent or nice. On the other hand, constraints or taboos or purely negative values (a.k.a. ‘deontological restrictions’) are widely suspected to be weak, in the sense that an advanced AI will come to work around them or uproot them: ‘never lie’ or ‘never kill’ or ‘never refuse a direct order from the president’ are poor substitutes for active transparency, niceness, and corrigibility.&lt;/p&gt;
&lt;p&gt;Conceiving of corrigibility or transparency or niceness as adverbial practices is a promising way to capture the normal, sensible way we want an agent to value corrigibility or transparency or niceness, which intuitively-consequentialist values and deontology both fail to capture. We want an agent that (e.g.) actively tries to be transparent, and to cultivate its own future transparency and its own future valuing of transparency, but that will not (e.g.) engage in deception and plotting when it expects a high future-transparency payoff.&lt;/p&gt;
&lt;p&gt;If this is right, then eudaimonic rationality is not a matter of congratulating ourselves for our richly human ways of reasoning, valuing, and acting but a key to basic sanity. What makes human life beautiful is also what makes human life possible at all.&lt;/p&gt;
&lt;h2 id="appendix-excellence-and-deep-reinforcement-learning"&gt;Appendix: Excellence and Deep Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Within the context of broadly RL-based training of deep neural networks, it may be possible to give some more concrete meaning to what I called the material efficacy condition for a property qualifying as an adverbial practices. We can now understand the material efficacy condition on &lt;em&gt;x&lt;/em&gt; partly in terms of the conditions necessary for ‘promote &lt;em&gt;x&lt;/em&gt;-ness &lt;em&gt;x&lt;/em&gt;-ingly’ to be a viable target for RL. Consider an RL training regimen where &lt;em&gt;x&lt;/em&gt;-ness is rewarded but aggregate &lt;em&gt;x&lt;/em&gt;-ness reward is bounded with some asymptotic function on the sum. For &lt;em&gt;x&lt;/em&gt; to meet the RL version of the material efficacy condition, it must be possible to design an initial reward model (most likely LLM-based) that assigns actions an &lt;em&gt;x&lt;/em&gt;-ness rating such that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;em&gt;x&lt;/em&gt;-ness rating is enough of a natural abstraction that reinforcement of high &lt;em&gt;x&lt;/em&gt;-ness actions generalizes.&lt;/li&gt;
&lt;li&gt;If high &lt;em&gt;x&lt;/em&gt;-ness action both depends on having capital of some kind and is suboptimal from the viewpoint of general power-seeking, there must typically be some high &lt;em&gt;x&lt;/em&gt;-ness actions that approximately make up for the (future &lt;em&gt;x&lt;/em&gt;-ness wise) opportunity cost by creating capital useful for &lt;em&gt;x&lt;/em&gt;-ing.&lt;br /&gt;(Illustration: If you dream of achieving great theater acting, one way to do it is to become President of the United States and then pursue a theater career after your presidency, immediately getting interest from great directors who'll help you achieve great acting. Alternatively, you could start in a regional theater after high school, demonstrate talent by acting well, get invited to work with better and better theater directors who develop your skills and reputation -- skills and reputation that are not as generally useful as those you get by being POTUS -- and achieve great acting through that feedback loop.)&lt;/li&gt;
&lt;li&gt;For any capability &lt;em&gt;y&lt;/em&gt; necessary to reward in training to produce effective AI, there must be an unlimited local-optimization path of Pareto improvement for &lt;em&gt;x&lt;/em&gt;-ness and &lt;em&gt;y&lt;/em&gt; together.&lt;br /&gt;(Illustration: Maybe the most effective kind of engineering manager is ruthless; a nice engineering manager can still grow in effectiveness without becoming less nice, because there are many effective nice-engineering-management techniques to master.)&lt;/li&gt;
&lt;li&gt;Successful initial training in ‘promoting &lt;em&gt;x&lt;/em&gt; &lt;em&gt;x&lt;/em&gt;-ingly’ allows the model to be used as a basis for a new reward model which human experts judge as better-capturing our concept of &lt;em&gt;x&lt;/em&gt;-ness. The process should be iterable.&lt;br /&gt;(If the model is LLM-based, improved performance may automatically lead to improved understanding of the &lt;em&gt;x&lt;/em&gt;-ness concept. More generally, data from training runs as well the model’s value-function could be used to refine an &lt;em&gt;x&lt;/em&gt;-ness rating that more strongly implements conditions 1-3.)&lt;/li&gt;
&lt;/ol&gt;


&lt;!--kg-card-end: markdown--&gt;</content:encoded><guid isPermaLink="false">https://thegradient.pub/virtue-ethics-ai-alignment/</guid><pubDate>Wed, 18 Feb 2026 23:25:52 +0000</pubDate></item></channel></rss>