<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 03 Jul 2025 06:33:51 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Confronting the AI/energy conundrum (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/confronting-ai-energy-conundrum-0702</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MITEI-evelyn-wang.JPG" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The explosive growth of AI-powered computing centers is creating an unprecedented surge in electricity demand that threatens to overwhelm power grids and derail climate goals. At the same time, artificial intelligence technologies could revolutionize energy systems, accelerating the transition to clean power.&lt;/p&gt;&lt;p&gt;“We’re at a cusp of potentially gigantic change throughout the economy,” said William H. Green, director of the MIT Energy Initiative (MITEI) and Hoyt C. Hottel Professor in the MIT Department of Chemical Engineering, at MITEI’s Spring Symposium, “AI and energy: Peril and promise,” held on May 13. The event brought together experts from industry, academia, and government to explore solutions to what Green described as both “local problems with electric supply and meeting our clean energy targets” while seeking to “reap the benefits of AI without some of the harms.” The challenge of data center energy demand and potential benefits of AI to the energy transition is a research priority for MITEI.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI’s startling energy demands&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;From the start, the symposium highlighted sobering statistics about AI’s appetite for electricity. After decades of flat electricity demand in the United States, computing centers now consume approximately 4 percent of the nation's electricity. Although there is great uncertainty, some projections suggest this demand could rise to 12-15 percent by 2030, largely driven by artificial intelligence applications.&lt;/p&gt;&lt;p&gt;Vijay Gadepally, senior scientist at MIT’s Lincoln Laboratory, emphasized the scale of AI’s consumption. “The power required for sustaining some of these large models is doubling almost every three months,” he noted. “A single ChatGPT conversation uses as much electricity as charging your phone, and generating an image consumes about a bottle of water for cooling.”&lt;/p&gt;&lt;p&gt;Facilities requiring 50 to 100 megawatts of power are emerging rapidly across the United States and globally, driven both by casual and institutional research needs relying on large language programs such as ChatGPT and Gemini. Gadepally cited congressional testimony by Sam Altman, CEO of OpenAI, highlighting how fundamental this relationship has become: “The cost of intelligence, the cost of AI, will converge to the cost of energy.”&lt;/p&gt;&lt;p&gt;“The energy demands of AI are a significant challenge, but we also have an opportunity to harness these vast computational capabilities to contribute to climate change solutions,” said Evelyn Wang, MIT vice president for energy and climate and the former director at the Advanced Research Projects Agency-Energy (ARPA-E) at the U.S. Department of Energy.&lt;/p&gt;&lt;p&gt;Wang also noted that innovations developed for AI and data centers — such as efficiency, cooling technologies, and clean-power solutions — could have broad applications beyond computing facilities themselves.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Strategies for clean energy solutions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The symposium explored multiple pathways to address the AI-energy challenge. Some panelists presented models suggesting that while artificial intelligence may increase emissions in the short term, its optimization capabilities could enable substantial emissions reductions after 2030 through more efficient power systems and accelerated clean technology development.&lt;/p&gt;&lt;p&gt;Research shows regional variations in the cost of powering computing centers with clean electricity, according to Emre Gençer, co-founder and CEO of Sesame Sustainability and former MITEI principal research scientist. Gençer’s analysis revealed that the central United States offers considerably lower costs due to complementary solar and wind resources. However, achieving zero-emission power would require massive battery deployments — five to 10 times more than moderate carbon scenarios — driving costs two to three times higher.&lt;/p&gt;&lt;p&gt;“If we want to do zero emissions with reliable power, we need technologies other than renewables and batteries, which will be too expensive,” Gençer said. He pointed to “long-duration storage technologies, small modular reactors, geothermal, or hybrid approaches” as necessary complements.&lt;/p&gt;&lt;p&gt;Because of data center energy demand, there is renewed interest in nuclear power, noted Kathryn Biegel, manager of R&amp;amp;D and corporate strategy at Constellation Energy, adding that her company is restarting the reactor at the former Three Mile Island site, now called the “Crane Clean Energy Center,” to meet this demand. “The data center space has become a major, major priority for Constellation,” she said, emphasizing how their needs for both reliability and carbon-free electricity are reshaping the power industry.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Can AI accelerate the energy transition?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Artificial intelligence could dramatically improve power systems, according to Priya Donti, assistant professor and the Silverman Family Career Development Professor in MIT's Department of Electrical Engineering and Computer Science and the Laboratory for Information and Decision Systems. She showcased how AI can accelerate power grid optimization by embedding physics-based constraints into neural networks, potentially solving complex power flow problems at “10 times, or even greater, speed compared to your traditional models.”&lt;/p&gt;&lt;p&gt;AI is already reducing carbon emissions, according to examples shared by Antonia Gawel, global director of sustainability and partnerships at Google. Google Maps’ fuel-efficient routing feature has “helped to prevent more than 2.9 million metric tons of GHG [greenhouse gas] emissions reductions since launch, which is the equivalent of taking 650,000 fuel-based cars off the road for a year," she said. Another Google research project uses artificial intelligence to help pilots avoid creating contrails, which represent about 1 percent of global warming impact.&lt;/p&gt;&lt;p&gt;AI’s potential to speed materials discovery for power applications was highlighted by Rafael Gómez-Bombarelli, the Paul M. Cook Career Development Associate Professor in the MIT Department of Materials Science and Engineering. “AI-supervised models can be trained to go from structure to property,” he noted, enabling the development of materials crucial for both computing and efficiency.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Securing growth with sustainability&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Throughout the symposium, participants grappled with balancing rapid AI deployment against environmental impacts. While AI training receives most attention, Dustin Demetriou, senior technical staff member in sustainability and data center innovation at IBM, quoted a World Economic Forum article that suggested that “80 percent of the environmental footprint is estimated to be due to inferencing.” Demetriou emphasized the need for efficiency across all artificial intelligence applications.&lt;/p&gt;&lt;p&gt;Jevons’ paradox, where “efficiency gains tend to increase overall resource consumption rather than decrease it” is another factor to consider, cautioned Emma Strubell, the Raj Reddy Assistant Professor in the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University. Strubell advocated for viewing computing center electricity as a limited resource requiring thoughtful allocation across different applications.&lt;/p&gt;&lt;p&gt;Several presenters discussed novel approaches for integrating renewable sources with existing grid infrastructure, including potential hybrid solutions that combine clean installations with existing natural gas plants that have valuable grid connections already in place. These approaches could provide substantial clean capacity across the United States at reasonable costs while minimizing reliability impacts.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Navigating the AI-energy paradox&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The symposium highlighted MIT’s central role in developing solutions to the AI-electricity challenge.&lt;/p&gt;&lt;p&gt;Green spoke of a new MITEI program on computing centers, power, and computation that will operate alongside the comprehensive spread of MIT Climate Project research. “We’re going to try to tackle a very complicated problem all the way from the power sources through the actual algorithms that deliver value to the customers — in a way that’s going to be acceptable to all the stakeholders and really meet all the needs,” Green said.&lt;/p&gt;&lt;p&gt;Participants in the symposium were polled about priorities for MIT’s research by Randall Field, MITEI director of research. The real-time results ranked “data center and grid integration issues” as the top priority, followed by “AI for accelerated discovery of advanced materials for energy.”&lt;/p&gt;&lt;p&gt;In addition, attendees revealed that most view AI's potential regarding power as a “promise,” rather than a “peril,” although a considerable portion remain uncertain about the ultimate impact. When asked about priorities in power supply for computing facilities, half of the respondents selected carbon intensity as their top concern, with reliability and cost following.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MITEI-evelyn-wang.JPG" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The explosive growth of AI-powered computing centers is creating an unprecedented surge in electricity demand that threatens to overwhelm power grids and derail climate goals. At the same time, artificial intelligence technologies could revolutionize energy systems, accelerating the transition to clean power.&lt;/p&gt;&lt;p&gt;“We’re at a cusp of potentially gigantic change throughout the economy,” said William H. Green, director of the MIT Energy Initiative (MITEI) and Hoyt C. Hottel Professor in the MIT Department of Chemical Engineering, at MITEI’s Spring Symposium, “AI and energy: Peril and promise,” held on May 13. The event brought together experts from industry, academia, and government to explore solutions to what Green described as both “local problems with electric supply and meeting our clean energy targets” while seeking to “reap the benefits of AI without some of the harms.” The challenge of data center energy demand and potential benefits of AI to the energy transition is a research priority for MITEI.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;AI’s startling energy demands&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;From the start, the symposium highlighted sobering statistics about AI’s appetite for electricity. After decades of flat electricity demand in the United States, computing centers now consume approximately 4 percent of the nation's electricity. Although there is great uncertainty, some projections suggest this demand could rise to 12-15 percent by 2030, largely driven by artificial intelligence applications.&lt;/p&gt;&lt;p&gt;Vijay Gadepally, senior scientist at MIT’s Lincoln Laboratory, emphasized the scale of AI’s consumption. “The power required for sustaining some of these large models is doubling almost every three months,” he noted. “A single ChatGPT conversation uses as much electricity as charging your phone, and generating an image consumes about a bottle of water for cooling.”&lt;/p&gt;&lt;p&gt;Facilities requiring 50 to 100 megawatts of power are emerging rapidly across the United States and globally, driven both by casual and institutional research needs relying on large language programs such as ChatGPT and Gemini. Gadepally cited congressional testimony by Sam Altman, CEO of OpenAI, highlighting how fundamental this relationship has become: “The cost of intelligence, the cost of AI, will converge to the cost of energy.”&lt;/p&gt;&lt;p&gt;“The energy demands of AI are a significant challenge, but we also have an opportunity to harness these vast computational capabilities to contribute to climate change solutions,” said Evelyn Wang, MIT vice president for energy and climate and the former director at the Advanced Research Projects Agency-Energy (ARPA-E) at the U.S. Department of Energy.&lt;/p&gt;&lt;p&gt;Wang also noted that innovations developed for AI and data centers — such as efficiency, cooling technologies, and clean-power solutions — could have broad applications beyond computing facilities themselves.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Strategies for clean energy solutions&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The symposium explored multiple pathways to address the AI-energy challenge. Some panelists presented models suggesting that while artificial intelligence may increase emissions in the short term, its optimization capabilities could enable substantial emissions reductions after 2030 through more efficient power systems and accelerated clean technology development.&lt;/p&gt;&lt;p&gt;Research shows regional variations in the cost of powering computing centers with clean electricity, according to Emre Gençer, co-founder and CEO of Sesame Sustainability and former MITEI principal research scientist. Gençer’s analysis revealed that the central United States offers considerably lower costs due to complementary solar and wind resources. However, achieving zero-emission power would require massive battery deployments — five to 10 times more than moderate carbon scenarios — driving costs two to three times higher.&lt;/p&gt;&lt;p&gt;“If we want to do zero emissions with reliable power, we need technologies other than renewables and batteries, which will be too expensive,” Gençer said. He pointed to “long-duration storage technologies, small modular reactors, geothermal, or hybrid approaches” as necessary complements.&lt;/p&gt;&lt;p&gt;Because of data center energy demand, there is renewed interest in nuclear power, noted Kathryn Biegel, manager of R&amp;amp;D and corporate strategy at Constellation Energy, adding that her company is restarting the reactor at the former Three Mile Island site, now called the “Crane Clean Energy Center,” to meet this demand. “The data center space has become a major, major priority for Constellation,” she said, emphasizing how their needs for both reliability and carbon-free electricity are reshaping the power industry.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Can AI accelerate the energy transition?&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Artificial intelligence could dramatically improve power systems, according to Priya Donti, assistant professor and the Silverman Family Career Development Professor in MIT's Department of Electrical Engineering and Computer Science and the Laboratory for Information and Decision Systems. She showcased how AI can accelerate power grid optimization by embedding physics-based constraints into neural networks, potentially solving complex power flow problems at “10 times, or even greater, speed compared to your traditional models.”&lt;/p&gt;&lt;p&gt;AI is already reducing carbon emissions, according to examples shared by Antonia Gawel, global director of sustainability and partnerships at Google. Google Maps’ fuel-efficient routing feature has “helped to prevent more than 2.9 million metric tons of GHG [greenhouse gas] emissions reductions since launch, which is the equivalent of taking 650,000 fuel-based cars off the road for a year," she said. Another Google research project uses artificial intelligence to help pilots avoid creating contrails, which represent about 1 percent of global warming impact.&lt;/p&gt;&lt;p&gt;AI’s potential to speed materials discovery for power applications was highlighted by Rafael Gómez-Bombarelli, the Paul M. Cook Career Development Associate Professor in the MIT Department of Materials Science and Engineering. “AI-supervised models can be trained to go from structure to property,” he noted, enabling the development of materials crucial for both computing and efficiency.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Securing growth with sustainability&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Throughout the symposium, participants grappled with balancing rapid AI deployment against environmental impacts. While AI training receives most attention, Dustin Demetriou, senior technical staff member in sustainability and data center innovation at IBM, quoted a World Economic Forum article that suggested that “80 percent of the environmental footprint is estimated to be due to inferencing.” Demetriou emphasized the need for efficiency across all artificial intelligence applications.&lt;/p&gt;&lt;p&gt;Jevons’ paradox, where “efficiency gains tend to increase overall resource consumption rather than decrease it” is another factor to consider, cautioned Emma Strubell, the Raj Reddy Assistant Professor in the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University. Strubell advocated for viewing computing center electricity as a limited resource requiring thoughtful allocation across different applications.&lt;/p&gt;&lt;p&gt;Several presenters discussed novel approaches for integrating renewable sources with existing grid infrastructure, including potential hybrid solutions that combine clean installations with existing natural gas plants that have valuable grid connections already in place. These approaches could provide substantial clean capacity across the United States at reasonable costs while minimizing reliability impacts.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Navigating the AI-energy paradox&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The symposium highlighted MIT’s central role in developing solutions to the AI-electricity challenge.&lt;/p&gt;&lt;p&gt;Green spoke of a new MITEI program on computing centers, power, and computation that will operate alongside the comprehensive spread of MIT Climate Project research. “We’re going to try to tackle a very complicated problem all the way from the power sources through the actual algorithms that deliver value to the customers — in a way that’s going to be acceptable to all the stakeholders and really meet all the needs,” Green said.&lt;/p&gt;&lt;p&gt;Participants in the symposium were polled about priorities for MIT’s research by Randall Field, MITEI director of research. The real-time results ranked “data center and grid integration issues” as the top priority, followed by “AI for accelerated discovery of advanced materials for energy.”&lt;/p&gt;&lt;p&gt;In addition, attendees revealed that most view AI's potential regarding power as a “promise,” rather than a “peril,” although a considerable portion remain uncertain about the ultimate impact. When asked about priorities in power supply for computing facilities, half of the respondents selected carbon intensity as their top concern, with reliability and cost following.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/confronting-ai-energy-conundrum-0702</guid><pubDate>Wed, 02 Jul 2025 19:00:00 +0000</pubDate></item><item><title>Could Google’s Veo 3 be the start of playable world models? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/02/could-googles-veo-3-be-the-start-of-playable-world-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/ETB2801.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Demis Hassabis, CEO of Google’s AI research organization DeepMind, appeared to suggest Tuesday evening that Veo 3, Google’s latest video-generating model, could potentially be used for video games.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to a post on X beseeching Google to “Let me play a video game of my veo 3 videos already,” and asking, “playable world models wen?” Hassabis responded, “now wouldn’t that be something.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Wednesday morning, Logan Kilpatrick, lead product for Google’s AI Studio and Gemini API, chimed in with a reply: “🤐🤐🤐🤐”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both posts from the Google executives are little more than playful suggestions, and a Google spokesperson told TechCrunch the company had nothing to share at the moment. But building playable world models isn’t outside the realm of possibilities for the tech giant. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;World models are different from video-generation models. The former simulates the dynamics of a real-world environment, which lets agents predict how the world will evolve in response to their actions. Video-gen models synthesize realistic video sequences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has plans to turn its multimodal foundation model, Gemini 2.5 Pro, into a world model that simulates aspects of the human brain. In December, DeepMind unveiled Genie 2, a model that can generate an “endless” variety of playable worlds. The following month, we reported that Google was forming a new team to work on AI models that can simulate the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others are working on building world models — most notably, AI pioneer Fei-Fei Li. Li came out of stealth last year with World Labs, a startup that has built its own AI system that generates video game-like, 3D scenes from a single image.&lt;/p&gt;


&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Veo 3, which is still in public preview, can create video as well as audio to go along with clips — anything from speech to soundtracks. While Veo 3 creates realistic movements by simulating real-world physics, it isn’t quite a world model yet. Instead, it could be used for cinematic storytelling in games, like cutscenes, trailers, and narrative prototyping&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The model is also still a “passive output” generative model, and it (or a future Veo generation) would need to shift to a simulator that’s more active, interactive, and predictive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the real challenge with video game production isn’t just impressive visuals; it’s real-time, consistent, and controllable simulation. That’s why it might make sense to see Google take a hybrid approach that leverages Veo and Genie in the future, should it pursue video game or playable world development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google could find itself competing with Microsoft, Scenario, Runway, Pika, and, eventually, OpenAI’s video-generating model Sora.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given Google’s planned moves in the world models space and its reputation for using its deep pockets and distribution muscle to steamroll rivals, competitors in this space would be wise to keep a close watch.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/ETB2801.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Demis Hassabis, CEO of Google’s AI research organization DeepMind, appeared to suggest Tuesday evening that Veo 3, Google’s latest video-generating model, could potentially be used for video games.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to a post on X beseeching Google to “Let me play a video game of my veo 3 videos already,” and asking, “playable world models wen?” Hassabis responded, “now wouldn’t that be something.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Wednesday morning, Logan Kilpatrick, lead product for Google’s AI Studio and Gemini API, chimed in with a reply: “🤐🤐🤐🤐”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both posts from the Google executives are little more than playful suggestions, and a Google spokesperson told TechCrunch the company had nothing to share at the moment. But building playable world models isn’t outside the realm of possibilities for the tech giant. &lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;World models are different from video-generation models. The former simulates the dynamics of a real-world environment, which lets agents predict how the world will evolve in response to their actions. Video-gen models synthesize realistic video sequences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has plans to turn its multimodal foundation model, Gemini 2.5 Pro, into a world model that simulates aspects of the human brain. In December, DeepMind unveiled Genie 2, a model that can generate an “endless” variety of playable worlds. The following month, we reported that Google was forming a new team to work on AI models that can simulate the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others are working on building world models — most notably, AI pioneer Fei-Fei Li. Li came out of stealth last year with World Labs, a startup that has built its own AI system that generates video game-like, 3D scenes from a single image.&lt;/p&gt;


&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Veo 3, which is still in public preview, can create video as well as audio to go along with clips — anything from speech to soundtracks. While Veo 3 creates realistic movements by simulating real-world physics, it isn’t quite a world model yet. Instead, it could be used for cinematic storytelling in games, like cutscenes, trailers, and narrative prototyping&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The model is also still a “passive output” generative model, and it (or a future Veo generation) would need to shift to a simulator that’s more active, interactive, and predictive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the real challenge with video game production isn’t just impressive visuals; it’s real-time, consistent, and controllable simulation. That’s why it might make sense to see Google take a hybrid approach that leverages Veo and Genie in the future, should it pursue video game or playable world development.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Google could find itself competing with Microsoft, Scenario, Runway, Pika, and, eventually, OpenAI’s video-generating model Sora.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given Google’s planned moves in the world models space and its reputation for using its deep pockets and distribution muscle to steamroll rivals, competitors in this space would be wise to keep a close watch.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/02/could-googles-veo-3-be-the-start-of-playable-world-models/</guid><pubDate>Wed, 02 Jul 2025 19:22:15 +0000</pubDate></item><item><title>Everything that could go wrong with X’s new AI-written community notes (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/07/everything-that-could-go-wrong-with-xs-new-ai-written-community-notes/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        X says AI can supercharge community notes, but that comes with obvious risks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="376" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1760687083-640x376.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1760687083-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Elon Musk's X arguably revolutionized social media fact-checking by rolling out "community notes," which created a system to crowdsource diverse views on whether certain X posts were trustworthy or not.&lt;/p&gt;
&lt;p&gt;But now, the platform plans to allow AI to write community notes, and that could potentially ruin whatever trust X users had in the fact-checking system—which X has fully acknowledged.&lt;/p&gt;
&lt;p&gt;In a research paper, X described the initiative as an "upgrade" while explaining everything that could possibly go wrong with AI-written community notes.&lt;/p&gt;
&lt;p&gt;In an ideal world, X described AI agents that speed up and increase the number of community notes added to incorrect posts, ramping up fact-checking efforts platform-wide. Each AI-written note will be rated by a human reviewer, providing feedback that makes the AI agent better at writing notes the longer this feedback loop cycles. As the AI agents get better at writing notes, that leaves human reviewers to focus on more nuanced fact-checking that AI cannot quickly address, such as posts requiring niche expertise or social awareness. Together, the human and AI reviewers, if all goes well, could transform not just X's fact-checking, X's paper suggested, but also potentially provide "a blueprint for a new form of human-AI collaboration in the production of public knowledge."&lt;/p&gt;
&lt;p&gt;Among key questions that remain, however, is a big one: X isn't sure if AI-written notes will be as accurate as notes written by humans. Complicating that further, it seems likely that AI agents could generate "persuasive but inaccurate notes," which human raters might rate as helpful since AI is "exceptionally skilled at crafting persuasive, emotionally resonant, and seemingly neutral notes." That could disrupt the feedback loop, watering down community notes and making the whole system less trustworthy over time, X's research paper warned.&lt;/p&gt;
&lt;p&gt;"If rated helpfulness isn’t perfectly correlated with accuracy, then highly polished but misleading notes could be more likely to pass the approval threshold," the paper said. "This risk could grow as LLMs advance; they could not only write persuasively but also more easily research and construct a seemingly robust body of evidence for nearly any claim, regardless of its veracity, making it even harder for human raters to spot deception or errors."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;X is already facing criticism over its AI plans. On Tuesday, former United Kingdom technology minister, Damian Collins, accused X of building a system that could allow "the industrial manipulation of what people see and decide to trust" on a platform with more than 600 million users, The Guardian reported.&lt;/p&gt;
&lt;p&gt;Collins claimed that AI notes risked increasing the promotion of "lies and conspiracy theories" on X, and he wasn't the only expert sounding alarms. Samuel Stockwell, a research associate at the Centre for Emerging Technology and Security at the Alan Turing Institute, told The Guardian that X's success largely depends on "the quality of safeguards X puts in place against the risk that these AI ‘note writers’ could hallucinate and amplify misinformation in their outputs."&lt;/p&gt;
&lt;p&gt;"AI chatbots often struggle with nuance and context but are good at confidently providing answers that sound persuasive even when untrue," Stockwell said. "That could be a dangerous combination if not effectively addressed by the platform."&lt;/p&gt;
&lt;p&gt;Also complicating things: anyone can create an AI agent using any technology to write community notes, X's Community Notes account explained. That means that some AI agents may be more biased or defective than others.&lt;/p&gt;
&lt;p&gt;If this dystopian version of events occurs, X predicts that human writers may get sick of writing notes, threatening the diversity of viewpoints that made community notes so trustworthy to begin with.&lt;/p&gt;
&lt;p&gt;And for any human writers and reviewers who stick around, it's possible that the sheer volume of AI-written notes may overload them. Andy Dudfield, the head of AI at a UK fact-checking organization called Full Fact, told The Guardian that X risks "increasing the already significant burden on human reviewers to check even more draft notes, opening the door to a worrying and plausible situation in which notes could be drafted, reviewed, and published entirely by AI without the careful consideration that human input provides."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;X is planning more research to ensure the "human rating capacity can sufficiently scale," but if it cannot solve this riddle, it knows "the impact of the most genuinely critical notes" risks being diluted.&lt;/p&gt;
&lt;p&gt;One possible solution to this "bottleneck," researchers noted, would be to remove the human review process and apply AI-written notes in "similar contexts" that human raters have previously approved. But the biggest potential downfall there is obvious.&lt;/p&gt;
&lt;p&gt;"Automatically matching notes to posts that people do not think need them could significantly undermine trust in the system," X's paper acknowledged.&lt;/p&gt;
&lt;p&gt;Ultimately, AI note writers on X may be deemed an "erroneous" tool, researchers admitted, but they're going ahead with testing to find out.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;AI-written notes will start posting this month&lt;/h2&gt;
&lt;p&gt;All AI-written community notes "will be clearly marked for users," X's Community Notes account said. The first AI notes will only appear on posts where people have requested a note, the account said, but eventually AI note writers could be allowed to select posts for fact-checking.&lt;/p&gt;
&lt;p&gt;More will be revealed when AI-written notes start appearing on X later this month, but in the meantime, X users can start testing AI note writers today and soon be considered for admission in the initial cohort of AI agents. (If any Ars readers end up testing out an AI note writer, this Ars writer would be curious to learn more about your experience.)&lt;/p&gt;
&lt;p&gt;For its research, X collaborated with post-graduate students, research affiliates, and professors investigating topics like human trust in AI, fine-tuning AI, and AI safety at Harvard University, the Massachusetts Institute of Technology, Stanford University, and the University of Washington.&lt;/p&gt;
&lt;p&gt;Researchers agreed that "under certain circumstances," AI agents can "produce notes that are of similar quality to human-written notes—at a fraction of the time and effort." They suggested that more research is needed to overcome flagged risks to reap the benefits of what could be "a transformative opportunity" that "offers promise of dramatically increased scale and speed" of fact-checking on X.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;If AI note writers "generate initial drafts that represent a wider range of perspectives than a single human writer typically could, the quality of community deliberation is improved from the start," the paper said.&lt;/p&gt;
&lt;h2&gt;Future of AI notes&lt;/h2&gt;
&lt;p&gt;Researchers imagine that once X's testing is completed, AI note writers could not just aid in researching problematic posts flagged by human users, but also one day select posts predicted to go viral and stop misinformation from spreading faster than human reviewers could.&lt;/p&gt;
&lt;p&gt;Additional perks from this automated system, they suggested, would include X note raters quickly accessing more thorough research and evidence synthesis, as well as clearer note composition, which could speed up the rating process.&lt;/p&gt;
&lt;p&gt;And perhaps one day, AI agents could even learn to predict rating scores to speed things up even more, researchers speculated. However, more research would be needed to ensure that wouldn't homogenize community notes, buffing them out to the point that no one reads them.&lt;/p&gt;
&lt;p&gt;Perhaps the most Musk-ian of ideas proposed in the paper, is a notion of training AI note writers with clashing views to "adversarially debate the merits of a note." Supposedly, that "could help instantly surface potential flaws, hidden biases, or fabricated evidence, empowering the human rater to make a more informed judgment."&lt;/p&gt;
&lt;p&gt;"Instead of starting from scratch, the rater now plays the role of an adjudicator—evaluating a structured clash of arguments," the paper said.&lt;/p&gt;
&lt;p&gt;While X may be moving to reduce the workload for X users writing community notes, it's clear that AI could never replace humans, researchers said. Those humans are necessary for more than just rubber-stamping AI-written notes.&lt;/p&gt;
&lt;p&gt;Human notes that are "written from scratch" are valuable to train the AI agents and some raters' niche expertise cannot easily be replicated, the paper said. And perhaps most obviously, humans "are uniquely positioned to identify deficits or biases" and therefore more likely to be compelled to write notes "on topics the automated writers overlook," such as spam or scams.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        X says AI can supercharge community notes, but that comes with obvious risks.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="376" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1760687083-640x376.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/GettyImages-1760687083-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio | DigitalVision Vectors

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Elon Musk's X arguably revolutionized social media fact-checking by rolling out "community notes," which created a system to crowdsource diverse views on whether certain X posts were trustworthy or not.&lt;/p&gt;
&lt;p&gt;But now, the platform plans to allow AI to write community notes, and that could potentially ruin whatever trust X users had in the fact-checking system—which X has fully acknowledged.&lt;/p&gt;
&lt;p&gt;In a research paper, X described the initiative as an "upgrade" while explaining everything that could possibly go wrong with AI-written community notes.&lt;/p&gt;
&lt;p&gt;In an ideal world, X described AI agents that speed up and increase the number of community notes added to incorrect posts, ramping up fact-checking efforts platform-wide. Each AI-written note will be rated by a human reviewer, providing feedback that makes the AI agent better at writing notes the longer this feedback loop cycles. As the AI agents get better at writing notes, that leaves human reviewers to focus on more nuanced fact-checking that AI cannot quickly address, such as posts requiring niche expertise or social awareness. Together, the human and AI reviewers, if all goes well, could transform not just X's fact-checking, X's paper suggested, but also potentially provide "a blueprint for a new form of human-AI collaboration in the production of public knowledge."&lt;/p&gt;
&lt;p&gt;Among key questions that remain, however, is a big one: X isn't sure if AI-written notes will be as accurate as notes written by humans. Complicating that further, it seems likely that AI agents could generate "persuasive but inaccurate notes," which human raters might rate as helpful since AI is "exceptionally skilled at crafting persuasive, emotionally resonant, and seemingly neutral notes." That could disrupt the feedback loop, watering down community notes and making the whole system less trustworthy over time, X's research paper warned.&lt;/p&gt;
&lt;p&gt;"If rated helpfulness isn’t perfectly correlated with accuracy, then highly polished but misleading notes could be more likely to pass the approval threshold," the paper said. "This risk could grow as LLMs advance; they could not only write persuasively but also more easily research and construct a seemingly robust body of evidence for nearly any claim, regardless of its veracity, making it even harder for human raters to spot deception or errors."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;X is already facing criticism over its AI plans. On Tuesday, former United Kingdom technology minister, Damian Collins, accused X of building a system that could allow "the industrial manipulation of what people see and decide to trust" on a platform with more than 600 million users, The Guardian reported.&lt;/p&gt;
&lt;p&gt;Collins claimed that AI notes risked increasing the promotion of "lies and conspiracy theories" on X, and he wasn't the only expert sounding alarms. Samuel Stockwell, a research associate at the Centre for Emerging Technology and Security at the Alan Turing Institute, told The Guardian that X's success largely depends on "the quality of safeguards X puts in place against the risk that these AI ‘note writers’ could hallucinate and amplify misinformation in their outputs."&lt;/p&gt;
&lt;p&gt;"AI chatbots often struggle with nuance and context but are good at confidently providing answers that sound persuasive even when untrue," Stockwell said. "That could be a dangerous combination if not effectively addressed by the platform."&lt;/p&gt;
&lt;p&gt;Also complicating things: anyone can create an AI agent using any technology to write community notes, X's Community Notes account explained. That means that some AI agents may be more biased or defective than others.&lt;/p&gt;
&lt;p&gt;If this dystopian version of events occurs, X predicts that human writers may get sick of writing notes, threatening the diversity of viewpoints that made community notes so trustworthy to begin with.&lt;/p&gt;
&lt;p&gt;And for any human writers and reviewers who stick around, it's possible that the sheer volume of AI-written notes may overload them. Andy Dudfield, the head of AI at a UK fact-checking organization called Full Fact, told The Guardian that X risks "increasing the already significant burden on human reviewers to check even more draft notes, opening the door to a worrying and plausible situation in which notes could be drafted, reviewed, and published entirely by AI without the careful consideration that human input provides."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;X is planning more research to ensure the "human rating capacity can sufficiently scale," but if it cannot solve this riddle, it knows "the impact of the most genuinely critical notes" risks being diluted.&lt;/p&gt;
&lt;p&gt;One possible solution to this "bottleneck," researchers noted, would be to remove the human review process and apply AI-written notes in "similar contexts" that human raters have previously approved. But the biggest potential downfall there is obvious.&lt;/p&gt;
&lt;p&gt;"Automatically matching notes to posts that people do not think need them could significantly undermine trust in the system," X's paper acknowledged.&lt;/p&gt;
&lt;p&gt;Ultimately, AI note writers on X may be deemed an "erroneous" tool, researchers admitted, but they're going ahead with testing to find out.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;AI-written notes will start posting this month&lt;/h2&gt;
&lt;p&gt;All AI-written community notes "will be clearly marked for users," X's Community Notes account said. The first AI notes will only appear on posts where people have requested a note, the account said, but eventually AI note writers could be allowed to select posts for fact-checking.&lt;/p&gt;
&lt;p&gt;More will be revealed when AI-written notes start appearing on X later this month, but in the meantime, X users can start testing AI note writers today and soon be considered for admission in the initial cohort of AI agents. (If any Ars readers end up testing out an AI note writer, this Ars writer would be curious to learn more about your experience.)&lt;/p&gt;
&lt;p&gt;For its research, X collaborated with post-graduate students, research affiliates, and professors investigating topics like human trust in AI, fine-tuning AI, and AI safety at Harvard University, the Massachusetts Institute of Technology, Stanford University, and the University of Washington.&lt;/p&gt;
&lt;p&gt;Researchers agreed that "under certain circumstances," AI agents can "produce notes that are of similar quality to human-written notes—at a fraction of the time and effort." They suggested that more research is needed to overcome flagged risks to reap the benefits of what could be "a transformative opportunity" that "offers promise of dramatically increased scale and speed" of fact-checking on X.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;If AI note writers "generate initial drafts that represent a wider range of perspectives than a single human writer typically could, the quality of community deliberation is improved from the start," the paper said.&lt;/p&gt;
&lt;h2&gt;Future of AI notes&lt;/h2&gt;
&lt;p&gt;Researchers imagine that once X's testing is completed, AI note writers could not just aid in researching problematic posts flagged by human users, but also one day select posts predicted to go viral and stop misinformation from spreading faster than human reviewers could.&lt;/p&gt;
&lt;p&gt;Additional perks from this automated system, they suggested, would include X note raters quickly accessing more thorough research and evidence synthesis, as well as clearer note composition, which could speed up the rating process.&lt;/p&gt;
&lt;p&gt;And perhaps one day, AI agents could even learn to predict rating scores to speed things up even more, researchers speculated. However, more research would be needed to ensure that wouldn't homogenize community notes, buffing them out to the point that no one reads them.&lt;/p&gt;
&lt;p&gt;Perhaps the most Musk-ian of ideas proposed in the paper, is a notion of training AI note writers with clashing views to "adversarially debate the merits of a note." Supposedly, that "could help instantly surface potential flaws, hidden biases, or fabricated evidence, empowering the human rater to make a more informed judgment."&lt;/p&gt;
&lt;p&gt;"Instead of starting from scratch, the rater now plays the role of an adjudicator—evaluating a structured clash of arguments," the paper said.&lt;/p&gt;
&lt;p&gt;While X may be moving to reduce the workload for X users writing community notes, it's clear that AI could never replace humans, researchers said. Those humans are necessary for more than just rubber-stamping AI-written notes.&lt;/p&gt;
&lt;p&gt;Human notes that are "written from scratch" are valuable to train the AI agents and some raters' niche expertise cannot easily be replicated, the paper said. And perhaps most obviously, humans "are uniquely positioned to identify deficits or biases" and therefore more likely to be compelled to write notes "on topics the automated writers overlook," such as spam or scams.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/07/everything-that-could-go-wrong-with-xs-new-ai-written-community-notes/</guid><pubDate>Wed, 02 Jul 2025 21:00:39 +0000</pubDate></item><item><title>TikTok is being flooded with racist AI videos generated by Google’s Veo 3 (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/07/racist-ai-videos-created-with-google-veo-3-are-proliferating-on-tiktok/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google and TikTok have rules against this sort of thing, but it doesn't seem to matter.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A TikTok app icon on a phone screen." class="absolute inset-0 w-full h-full object-cover hidden" height="188" src="https://cdn.arstechnica.net/wp-content/uploads/2024/04/tiktok-300x188.jpg" width="300" /&gt;
                  &lt;img alt="A TikTok app icon on a phone screen." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/04/tiktok-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | Chesnot 

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The release of Google's Veo 3 video generator in May represented a disconcerting leap in AI video quality. While many of the viral AI videos we've seen are harmless fun, the model's pixel-perfect output can also be used for nefarious purposes. On TikTok, which may or may not be banned in the coming months, users have noticed a surplus of racist AI videos, courtesy of Google's Veo 3.&lt;/p&gt;
&lt;p&gt;According to a report from MediaMatters, numerous TikTok accounts have started posting AI-generated videos that use racist and antisemitic tropes in recent weeks. Most of the AI vitriol is aimed at Black people, depicting them as "the usual suspects" in crimes, absent parents, and monkeys with an affinity for watermelon. The content also targets immigrants and Jewish people. The videos top out at eight seconds and bear the "Veo" watermark, confirming they came from Google's leading AI model.&lt;/p&gt;
&lt;p&gt;The compilation video below has examples pulled from TikTok since the release of Veo 3, but be warned, it contains racist and antisemitic content. Some of the videos are shocking, which is likely the point—nothing drives engagement on social media like anger and drama. MediaMatters reports that the original posts have numerous comments echoing the stereotypes used in the video.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hateful AI videos generated by Veo 3 spreading on TikTok.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Google has stressed security when announcing new AI models—we've all seen an AI refuse to complete a task that runs afoul of its guardrails. And it's never fun when you have genuinely harmless intentions, but the system throws a false positive and blocks your output. Google has mostly struck the right balance previously, but it appears that Veo 3 is more compliant. We've tested a few simple prompts with Veo 3 and found it easy to reproduce elements of these videos.&lt;/p&gt;
&lt;h2&gt;Clear but unenforced policies&lt;/h2&gt;
&lt;p&gt;TikTok's terms of service ban this kind of content. "We do not allow any hate speech, hateful behavior, or promotion of hateful ideologies. This includes explicit or implicit content that attacks a protected group," the community guidelines read. Despite this blanket ban on racist caricatures, the hateful Veo 3 videos appear to be spreading unchecked.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;TikTok notes that it uses both technology and human moderators to identify rule-breaking content. However, the volume of uploads makes timely moderation difficult. While the racist videos racked up a lot of views, a TikTok spokesperson tells Ars that more than half of the accounts cited in the MediaMatters report were banned for policy violations before the report was published, and the remainder have now been removed.&lt;/p&gt;
&lt;p&gt;As for Google, it has a comprehensive Prohibited Use Policy that bans the use of its services to promote hate speech, harassment, bullying, intimidation, and abuse. The videos uncovered by MediaMatters all seem to fall under one or more of these categories. In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.&lt;/p&gt;
&lt;p&gt;TikTok, being the world's leading social video behemoth, is a natural place for these videos to spread. It's not exclusive to TikTok, though. X (formerly Twitter) has gained a reputation for very limited moderation, leading to an explosion of hateful AI content. This problem could also get worse very soon. Google has plans to integrate Veo 3 into YouTube Shorts, which could make it even easier for similar content to spread on YouTube.&lt;/p&gt;
&lt;p&gt;TikTok and Google have clear prohibitions on this content, which should have prevented it from being seen millions of times on social media. Enforcement of those policies, however, is lacking. TikTok is seemingly unable to keep up with the flood of video uploads, and Google's guardrails appear insufficient to block the creation of this content. We've reached out to Google to inquire about Veo 3's safety features but have not yet heard back.&lt;/p&gt;
&lt;p&gt;For as long as generative AI has existed, people have used it to create inflammatory and racist content. Google and others always talk about the guardrails to prevent misuse, but they can't catch everything. The realism of Veo 3 makes it especially attractive for those who want to spread hateful stereotypes. Maybe all the guardrails in the world won't stop that.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google and TikTok have rules against this sort of thing, but it doesn't seem to matter.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A TikTok app icon on a phone screen." class="absolute inset-0 w-full h-full object-cover hidden" height="188" src="https://cdn.arstechnica.net/wp-content/uploads/2024/04/tiktok-300x188.jpg" width="300" /&gt;
                  &lt;img alt="A TikTok app icon on a phone screen." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/04/tiktok-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | Chesnot 

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The release of Google's Veo 3 video generator in May represented a disconcerting leap in AI video quality. While many of the viral AI videos we've seen are harmless fun, the model's pixel-perfect output can also be used for nefarious purposes. On TikTok, which may or may not be banned in the coming months, users have noticed a surplus of racist AI videos, courtesy of Google's Veo 3.&lt;/p&gt;
&lt;p&gt;According to a report from MediaMatters, numerous TikTok accounts have started posting AI-generated videos that use racist and antisemitic tropes in recent weeks. Most of the AI vitriol is aimed at Black people, depicting them as "the usual suspects" in crimes, absent parents, and monkeys with an affinity for watermelon. The content also targets immigrants and Jewish people. The videos top out at eight seconds and bear the "Veo" watermark, confirming they came from Google's leading AI model.&lt;/p&gt;
&lt;p&gt;The compilation video below has examples pulled from TikTok since the release of Veo 3, but be warned, it contains racist and antisemitic content. Some of the videos are shocking, which is likely the point—nothing drives engagement on social media like anger and drama. MediaMatters reports that the original posts have numerous comments echoing the stereotypes used in the video.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Hateful AI videos generated by Veo 3 spreading on TikTok.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Google has stressed security when announcing new AI models—we've all seen an AI refuse to complete a task that runs afoul of its guardrails. And it's never fun when you have genuinely harmless intentions, but the system throws a false positive and blocks your output. Google has mostly struck the right balance previously, but it appears that Veo 3 is more compliant. We've tested a few simple prompts with Veo 3 and found it easy to reproduce elements of these videos.&lt;/p&gt;
&lt;h2&gt;Clear but unenforced policies&lt;/h2&gt;
&lt;p&gt;TikTok's terms of service ban this kind of content. "We do not allow any hate speech, hateful behavior, or promotion of hateful ideologies. This includes explicit or implicit content that attacks a protected group," the community guidelines read. Despite this blanket ban on racist caricatures, the hateful Veo 3 videos appear to be spreading unchecked.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;TikTok notes that it uses both technology and human moderators to identify rule-breaking content. However, the volume of uploads makes timely moderation difficult. While the racist videos racked up a lot of views, a TikTok spokesperson tells Ars that more than half of the accounts cited in the MediaMatters report were banned for policy violations before the report was published, and the remainder have now been removed.&lt;/p&gt;
&lt;p&gt;As for Google, it has a comprehensive Prohibited Use Policy that bans the use of its services to promote hate speech, harassment, bullying, intimidation, and abuse. The videos uncovered by MediaMatters all seem to fall under one or more of these categories. In a perfect world, Veo 3 would refuse to create these videos, but vagueness in the prompt and the AI's inability to understand the subtleties of racist tropes (i.e., the use of monkeys instead of humans in some videos) make it easy to skirt the rules.&lt;/p&gt;
&lt;p&gt;TikTok, being the world's leading social video behemoth, is a natural place for these videos to spread. It's not exclusive to TikTok, though. X (formerly Twitter) has gained a reputation for very limited moderation, leading to an explosion of hateful AI content. This problem could also get worse very soon. Google has plans to integrate Veo 3 into YouTube Shorts, which could make it even easier for similar content to spread on YouTube.&lt;/p&gt;
&lt;p&gt;TikTok and Google have clear prohibitions on this content, which should have prevented it from being seen millions of times on social media. Enforcement of those policies, however, is lacking. TikTok is seemingly unable to keep up with the flood of video uploads, and Google's guardrails appear insufficient to block the creation of this content. We've reached out to Google to inquire about Veo 3's safety features but have not yet heard back.&lt;/p&gt;
&lt;p&gt;For as long as generative AI has existed, people have used it to create inflammatory and racist content. Google and others always talk about the guardrails to prevent misuse, but they can't catch everything. The realism of Veo 3 makes it especially attractive for those who want to spread hateful stereotypes. Maybe all the guardrails in the world won't stop that.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/07/racist-ai-videos-created-with-google-veo-3-are-proliferating-on-tiktok/</guid><pubDate>Wed, 02 Jul 2025 21:18:08 +0000</pubDate></item><item><title>Wonder Dynamics co-founder Nikola Todorovic joins the AI Stage at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/02/wonder-dynamics-co-founder-nikola-todorovic-joins-the-ai-stage-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is back at Moscone West in San Francisco from October 27–29, bringing together 10,000+ startup and VC leaders to dig into what’s next in tech. And when it comes to artificial intelligence, the conversations aren’t just technical — they’re creative, cinematic, and boundary-pushing. That’s why Nikola Todorovic is headed to the AI Stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A visual effects veteran turned AI entrepreneur, Todorovic is the co-founder of Wonder Dynamics, now an Autodesk company. Alongside actor and producer Tye Sheridan, he helped launch Autodesk Flow Studio (formerly Wonder Studio), a groundbreaking AI platform that allows creators to seamlessly integrate 3D characters into live-action scenes. The platform uses cloud-based tools to automate complex processes like lighting, animation, and composition, giving filmmakers a radically faster and more accessible path to high-end visual effects.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Nikola Todorovic" class="wp-image-3024452" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Nikola-Todorovic-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-where-film-meets-ai"&gt;Where film meets AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Todorovic’s journey to this moment wasn’t traditional, but that’s exactly the point. As an award-winning filmmaker and VFX supervisor, he spent years working at the intersection of storytelling and technology. That experience led to Wonder Dynamics, where the mission has always been to empower artists, not replace them. The company’s acquisition by Autodesk in 2024 marked a major validation of that vision, and now Todorovic is helping shape the future of creative AI inside one of the industry’s biggest ecosystems.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At Disrupt, he’ll join &lt;strong&gt;other AI industry leaders&lt;/strong&gt; for a wide-ranging panel on what’s coming next — from generative tools to ethical design to the future of creator workflows. Stay tuned to the fast-growing &lt;strong&gt;Disrupt agenda page&lt;/strong&gt; for the latest updates. Expect a conversation in Todorovic’s session that spans beyond buzzwords and dives into the real-world impact of AI in media and beyond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000 other tech and VC leaders on the AI Stage at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; to hear from Nikola Todorovic and other top voices driving the future of artificial intelligence. It’s all happening October 27–29 at Moscone West in San Francisco. Lock in your spot today and &lt;strong&gt;save up to $675&lt;/strong&gt; before prices go up.&lt;br /&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is back at Moscone West in San Francisco from October 27–29, bringing together 10,000+ startup and VC leaders to dig into what’s next in tech. And when it comes to artificial intelligence, the conversations aren’t just technical — they’re creative, cinematic, and boundary-pushing. That’s why Nikola Todorovic is headed to the AI Stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A visual effects veteran turned AI entrepreneur, Todorovic is the co-founder of Wonder Dynamics, now an Autodesk company. Alongside actor and producer Tye Sheridan, he helped launch Autodesk Flow Studio (formerly Wonder Studio), a groundbreaking AI platform that allows creators to seamlessly integrate 3D characters into live-action scenes. The platform uses cloud-based tools to automate complex processes like lighting, animation, and composition, giving filmmakers a radically faster and more accessible path to high-end visual effects.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Nikola Todorovic" class="wp-image-3024452" height="383" src="https://techcrunch.com/wp-content/uploads/2025/07/TC25_Nikola-Todorovic-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-where-film-meets-ai"&gt;Where film meets AI&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Todorovic’s journey to this moment wasn’t traditional, but that’s exactly the point. As an award-winning filmmaker and VFX supervisor, he spent years working at the intersection of storytelling and technology. That experience led to Wonder Dynamics, where the mission has always been to empower artists, not replace them. The company’s acquisition by Autodesk in 2024 marked a major validation of that vision, and now Todorovic is helping shape the future of creative AI inside one of the industry’s biggest ecosystems.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At Disrupt, he’ll join &lt;strong&gt;other AI industry leaders&lt;/strong&gt; for a wide-ranging panel on what’s coming next — from generative tools to ethical design to the future of creator workflows. Stay tuned to the fast-growing &lt;strong&gt;Disrupt agenda page&lt;/strong&gt; for the latest updates. Expect a conversation in Todorovic’s session that spans beyond buzzwords and dives into the real-world impact of AI in media and beyond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Join 10,000 other tech and VC leaders on the AI Stage at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; to hear from Nikola Todorovic and other top voices driving the future of artificial intelligence. It’s all happening October 27–29 at Moscone West in San Francisco. Lock in your spot today and &lt;strong&gt;save up to $675&lt;/strong&gt; before prices go up.&lt;br /&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/02/wonder-dynamics-co-founder-nikola-todorovic-joins-the-ai-stage-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 02 Jul 2025 23:10:00 +0000</pubDate></item><item><title>OpenAI condemns Robinhood’s ‘OpenAI tokens’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/02/openai-condemns-robinhoods-openai-tokens/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1934195443-e.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI wants to make clear that Robinhood’s sale of “OpenAI tokens” will not give everyday consumers equity — or stock — in OpenAI, the company said in a post from its official newsroom account on X. OpenAI says it does not endorse Robinhood’s effort, nor was it involved in facilitating the token sale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“These ‘OpenAI tokens’ are not OpenAI equity,” said OpenAI’s newsroom account on Wednesday. “We did not partner with Robinhood, were not involved in this, and do not endorse it.&amp;nbsp;Any transfer of OpenAI equity requires our approval—we did not approve any transfer. Please be careful.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;These “OpenAI tokens” are not OpenAI equity. We did not partner with Robinhood, were not involved in this, and do not endorse it.&amp;nbsp; Any transfer of OpenAI equity requires our approval—we did not approve any transfer. &lt;/p&gt;&lt;p&gt;Please be careful.&lt;/p&gt;— OpenAI Newsroom (@OpenAINewsroom) July 2, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s statement is a response to Robinhood’s announcement earlier this week that it would start selling  so-called tokenized shares of OpenAI, SpaceX, and other private companies to people in the European Union.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Robinhood says the launch represents an attempt to give everyday people exposure to equity in the world’s most valuable private companies via blockchain. Hours after announcing these token sales, Robinhood’s stock price shot to an all-time high.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But stock in private companies like OpenAI and SpaceX are not available to the public. That’s what makes them private. They sell shares to investors of their choosing. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So OpenAI is openly disavowing Robinhood’s effort. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to OpenAI’s condemnation, Robinhood spokesperson Rouky Diallo told TechCrunch that OpenAI tokens were part of a “limited” giveaway to offer retail investors indirect exposure “through Robinhood’s ownership stake in a special purpose vehicle (SPV).”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;That suggests Robinhood owns shares of an SPV that controls a certain number of OpenAI’s shares. Like the tokens, shares of SPVs are not direct ownership of shares, either. They are ownership in a vehicle that owns the shares. In one way or another, Robinhood seems to be tying the price of its new tokenized product to the OpenAI shares in that SPV. But shares prices in an SPV can also differ from prices of an actual share of stock.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Robinhood’s help center, the company notes that when buying any of its stock tokens, “you are not buying the actual stocks — you are buying tokenized contracts that follow their price, recorded on a blockchain.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“While it is true that they aren’t technically ‘equity,’&amp;nbsp;[…] the tokens effectively give retail investors exposure to these private assets,” said Robinhood CEO Vlad Tenev in a post on X on Wednesday. “Our giveaway plants a seed for something much bigger, and since our announcement we’ve been hearing from many private companies that are eager to join us in the tokenization revolution.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI declined to comment further. Robinhood did not respond to TechCrunch’s additional questions about its SPV.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Private companies are known to push back against anything that could influence how their equity is valued. In recent months, humanoid robotics startup Figure AI sent cease-and-desist letters to two brokers running secondary markets that were marketing the company’s stock. Of course, these situations are different, but most startups don’t want people to believe that they’ve authorized share sales if they haven’t.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1934195443-e.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI wants to make clear that Robinhood’s sale of “OpenAI tokens” will not give everyday consumers equity — or stock — in OpenAI, the company said in a post from its official newsroom account on X. OpenAI says it does not endorse Robinhood’s effort, nor was it involved in facilitating the token sale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“These ‘OpenAI tokens’ are not OpenAI equity,” said OpenAI’s newsroom account on Wednesday. “We did not partner with Robinhood, were not involved in this, and do not endorse it.&amp;nbsp;Any transfer of OpenAI equity requires our approval—we did not approve any transfer. Please be careful.”&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;These “OpenAI tokens” are not OpenAI equity. We did not partner with Robinhood, were not involved in this, and do not endorse it.&amp;nbsp; Any transfer of OpenAI equity requires our approval—we did not approve any transfer. &lt;/p&gt;&lt;p&gt;Please be careful.&lt;/p&gt;— OpenAI Newsroom (@OpenAINewsroom) July 2, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s statement is a response to Robinhood’s announcement earlier this week that it would start selling  so-called tokenized shares of OpenAI, SpaceX, and other private companies to people in the European Union.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Robinhood says the launch represents an attempt to give everyday people exposure to equity in the world’s most valuable private companies via blockchain. Hours after announcing these token sales, Robinhood’s stock price shot to an all-time high.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But stock in private companies like OpenAI and SpaceX are not available to the public. That’s what makes them private. They sell shares to investors of their choosing. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So OpenAI is openly disavowing Robinhood’s effort. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to OpenAI’s condemnation, Robinhood spokesperson Rouky Diallo told TechCrunch that OpenAI tokens were part of a “limited” giveaway to offer retail investors indirect exposure “through Robinhood’s ownership stake in a special purpose vehicle (SPV).”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;That suggests Robinhood owns shares of an SPV that controls a certain number of OpenAI’s shares. Like the tokens, shares of SPVs are not direct ownership of shares, either. They are ownership in a vehicle that owns the shares. In one way or another, Robinhood seems to be tying the price of its new tokenized product to the OpenAI shares in that SPV. But shares prices in an SPV can also differ from prices of an actual share of stock.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Robinhood’s help center, the company notes that when buying any of its stock tokens, “you are not buying the actual stocks — you are buying tokenized contracts that follow their price, recorded on a blockchain.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“While it is true that they aren’t technically ‘equity,’&amp;nbsp;[…] the tokens effectively give retail investors exposure to these private assets,” said Robinhood CEO Vlad Tenev in a post on X on Wednesday. “Our giveaway plants a seed for something much bigger, and since our announcement we’ve been hearing from many private companies that are eager to join us in the tokenization revolution.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI declined to comment further. Robinhood did not respond to TechCrunch’s additional questions about its SPV.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Private companies are known to push back against anything that could influence how their equity is valued. In recent months, humanoid robotics startup Figure AI sent cease-and-desist letters to two brokers running secondary markets that were marketing the company’s stock. Of course, these situations are different, but most startups don’t want people to believe that they’ve authorized share sales if they haven’t.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/02/openai-condemns-robinhoods-openai-tokens/</guid><pubDate>Wed, 02 Jul 2025 23:43:27 +0000</pubDate></item><item><title>[NEW] AI job predictions become corporate America’s newest competitive sport (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/07/02/ai-job-predictions-become-corporate-americas-newest-competitive-sport/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/03/boxing-gloves-businessman.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In late May, Anthropic CEO Dario Amodei appeared to kick open the door on a sensitive topic, warning that half of entry-level jobs could vanish within five years because of AI and push U.S. unemployment up to 20%. But Amodei is far from alone in sharing aloud that he foresees a workforce bloodbath. A new WSJ story highlights how other CEOs are also issuing dire predictions about AI’s job impact, turning employment doom into something of a competitive sport.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several of these predictions came before Amodei’s comments. For example, at JPMorgan’s annual investor day earlier in May, its consumer banking chief Marianne Lake projected AI would “enable” a 10% workforce reduction. But they’ve been coming fast, and growing more stark, ever since. In a note last month, Amazon’s Andy Jassy warned employees to expect a smaller workforce due to the “once-in-a-lifetime” technological shift that’s afoot. ThredUp’s CEO said at a conference last month that AI will destroy “way more jobs than the average person thinks.” Not to be outdone, Ford’s Jim Farley delivered perhaps the most sweeping claim yet, saying last week that AI will “literally replace half of all white-collar workers in the U.S.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s a dramatic shift from executives’ previous cautious public statements about job displacement, notes the Journal. Indeed, the outlet notes that while some tech leaders — including from powerful AI companies — have proposed that fears are overblown, the growing string of warnings suggests massive restructurings are coming, whether people are ready for them or not.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2016/03/boxing-gloves-businessman.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In late May, Anthropic CEO Dario Amodei appeared to kick open the door on a sensitive topic, warning that half of entry-level jobs could vanish within five years because of AI and push U.S. unemployment up to 20%. But Amodei is far from alone in sharing aloud that he foresees a workforce bloodbath. A new WSJ story highlights how other CEOs are also issuing dire predictions about AI’s job impact, turning employment doom into something of a competitive sport.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Several of these predictions came before Amodei’s comments. For example, at JPMorgan’s annual investor day earlier in May, its consumer banking chief Marianne Lake projected AI would “enable” a 10% workforce reduction. But they’ve been coming fast, and growing more stark, ever since. In a note last month, Amazon’s Andy Jassy warned employees to expect a smaller workforce due to the “once-in-a-lifetime” technological shift that’s afoot. ThredUp’s CEO said at a conference last month that AI will destroy “way more jobs than the average person thinks.” Not to be outdone, Ford’s Jim Farley delivered perhaps the most sweeping claim yet, saying last week that AI will “literally replace half of all white-collar workers in the U.S.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s a dramatic shift from executives’ previous cautious public statements about job displacement, notes the Journal. Indeed, the outlet notes that while some tech leaders — including from powerful AI companies — have proposed that fears are overblown, the growing string of warnings suggests massive restructurings are coming, whether people are ready for them or not.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/07/02/ai-job-predictions-become-corporate-americas-newest-competitive-sport/</guid><pubDate>Thu, 03 Jul 2025 05:30:18 +0000</pubDate></item></channel></rss>