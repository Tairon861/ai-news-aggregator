<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 08 Oct 2025 18:31:39 +0000</lastBuildDate><item><title>AI Redaction That Puts Privacy First: CaseGuard Studio Leading The Way (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-redaction-that-puts-privacy-first-caseguard-studio-leading-the-way/</link><description>&lt;p&gt;Law enforcement, law firms, hospitals, and financial institutions are asked every day to release records, which can contain highly sensitive details – including addresses, social security numbers, medical diagnoses, evidence footage, and children’s identities.&lt;/p&gt;&lt;p&gt;To meet compliance and security requirements, staff spend hundreds of hours manually redacting sensitive information, yet when that process goes wrong, there can be costly consequences. Last year, healthcare company Advanced was fined £6 million for losing patient records that, among other details, contained information about how to gain entry to the homes of 890 care receivers. Even the smallest oversights can create unpleasant headlines and catastrophic fines.&lt;/p&gt;&lt;p&gt;This is the reality of modern data handling: leaks can be catastrophic, and compliance frameworks like GDPR, HIPAA, and FERPA, plus FOIA requests, require more vigilance than manual redaction can provide. What organizations need is not more staff to ensure proper redaction, but tools that achieve it quickly, reliably, and securely.&lt;/p&gt;&lt;p&gt;CaseGuard Studio, a US-based AI redaction &amp;amp; investigation platform, has built software that automates this manual work with 98% accuracy. It can process thousands of files in minutes, working on data that’s kept securely on-premises of any file type, including video, audio, documents, and images.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-manual-redaction-no-longer-works"&gt;Why Manual Redaction No Longer Works&lt;/h3&gt;&lt;p&gt;Redaction is not new, but the tools most people reach for were not built for the complexity of today’s compliance requirements. Adobe Acrobat, for example, offers text redaction but needs manual work on each document. Premiere’s video editing software requires frame-by-frame subject tracking for video redaction, which is slow and impractical. These solutions provide only limited capability and were never designed for departments that process a multitude of redactions on a weekly basis.&lt;/p&gt;&lt;p&gt;CaseGuard Studio, by contrast, was purpose-built for just this challenge. It can detect 12 categories of PII (personally-identifiable information) in video and images, such as faces, license plates, notepads, and more. It tracks and redacts all PII without needing manual frame-by-frame intervention.&lt;/p&gt;&lt;p&gt;For audio and documents, CaseGuard Studio supports over 30 PII types, like names, phone numbers, and addresses. Custom keywords, phrases, or sentences can be auto-detected and redacted directly from thousands of documents and transcripts, streamlining compliance in ways manual tools can’t match. It transcribes recordings with high accuracy and can translate to and from 100+ languages, so it can redact sensitive terms in multilingual content.&lt;/p&gt;&lt;p&gt;What once took days of human labor can now happen in minutes. CaseGuard Studio automates redaction work with 98% accuracy, up to 30 times faster than manual methods, and because it runs fully on-premise, data never leaves the device.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-to-ask-when-choosing-redaction-software"&gt;What to Ask When Choosing Redaction Software&lt;/h3&gt;&lt;p&gt;For organizations evaluating redaction software, the decision often comes down to a handful of critical questions that determine whether a platform can deliver on both compliance and efficiency. The following questions are central to making the right choice.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Can the software handle every file type we work with?&lt;/strong&gt; From scanned forms and handwritten notes to video, audio, and still images, organizations in sensitive sectors deal with more than PDFs.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Is the platform fully automated?&lt;/strong&gt; If redaction still means blacking out text with a Sharpie or scrubbing video frame by frame, the process is slow and prone to error. Full automation ensures accuracy and frees staff for higher-impact work.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Does the software ensure data never leaves your environment?&lt;/strong&gt; On-premise deployment means sensitive files are processed locally, so nothing is exposed to third-party servers or cloud risks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Does the pricing stay predictable as you scale?&lt;/strong&gt; Per-file or per-minute pricing quickly becomes unsustainable as workloads grow. Look for a flat subscription with unlimited redaction, so costs stay predictable no matter how much data you process.&lt;/li&gt;&lt;/ul&gt;&lt;h3 class="wp-block-heading" id="h-evaluating-caseguard-studio-against-the-four-redaction-essentials"&gt;Evaluating CaseGuard Studio Against the Four Redaction Essentials&lt;/h3&gt;&lt;p&gt;When assessed against these requirements, CaseGuard Studio was the only platform in our evaluation that consistently delivered across all five redaction essentials.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Auto-redact files from any source&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;From text documents and scanned forms to video, audio, images, and even handwriting, redaction has to cover every format where sensitive information might appear. Missing one identifiable feature, a face in a crowd or an un-redacted license plate, and a single oversight can be the difference between full compliance and a lawsuit. CaseGuard Studio automatically detects and redacts sensitive information across all these file types within a single platform with complete compliance.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Automated bulk redaction at speed and scale&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Thousands of files can be redacted in bulk, turning weeks of manual effort into minutes of processing. CaseGuard Studio handles workloads up to 32x faster than manual methods, with 98% accuracy, giving organizations the speed and scalability to meet growing compliance demands.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Your data, your control&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;CaseGuard Studio runs fully on-premise, within your secure environment, including air-gapped systems that are completely isolated from external networks. This ensures organizations retain full control of their data, with nothing exposed to third-party servers or cloud risks.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt; Unlimited redaction, no pay-per-file fees&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Pay-per-file pricing quickly adds up, making every additional redaction more expensive. CaseGuard Studio offers predictable pricing under a flat subscription with unlimited redaction, so costs remain the same no matter how heavy the redaction load is.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt; [embedded content]&lt;/p&gt;&lt;/figure&gt;&lt;h2 class="wp-block-heading" id="h-final-thoughts"&gt;Final Thoughts&lt;/h2&gt;&lt;p&gt;Over the course of our evaluation, we compared methods and platforms ranging from manual redaction and legacy PDF editors to newer AI-driven tools that have appeared in the last few years. Most delivered partial solutions, treating written documents well but failing on audio, while others blurred faces in video, but weren’t practical to use at scale. Cloud-only options raised sovereignty and compliance concerns that, for many users, would count them out of the running entirely.&lt;/p&gt;&lt;p&gt;CaseGuard Studio was the only platform that consistently met all five requirements detailed above. It supports the widest of file types, from body-cam video to scanned or handwritten forms.&lt;/p&gt;&lt;p&gt;Audio and video are probably the most difficult formats to redact, especially at scale. Here, CaseGuard wins our vote with its AI-powered smarts. It runs fully on-premise, keeps sensitive files under organizational control, and its local AI models are refined with each version release.&lt;/p&gt;&lt;p&gt;At a time when many cloud redaction software licensing models drive up costs as workloads grow, CaseGuard’s flat pricing offers a refreshing change — predictable, transparent, and sustainable.&lt;/p&gt;&lt;p&gt;For any organization facing rising compliance demands and ever-larger volumes of sensitive data, CaseGuard Studio is well worth a closer look. Click here to book a consultation.&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Law enforcement, law firms, hospitals, and financial institutions are asked every day to release records, which can contain highly sensitive details – including addresses, social security numbers, medical diagnoses, evidence footage, and children’s identities.&lt;/p&gt;&lt;p&gt;To meet compliance and security requirements, staff spend hundreds of hours manually redacting sensitive information, yet when that process goes wrong, there can be costly consequences. Last year, healthcare company Advanced was fined £6 million for losing patient records that, among other details, contained information about how to gain entry to the homes of 890 care receivers. Even the smallest oversights can create unpleasant headlines and catastrophic fines.&lt;/p&gt;&lt;p&gt;This is the reality of modern data handling: leaks can be catastrophic, and compliance frameworks like GDPR, HIPAA, and FERPA, plus FOIA requests, require more vigilance than manual redaction can provide. What organizations need is not more staff to ensure proper redaction, but tools that achieve it quickly, reliably, and securely.&lt;/p&gt;&lt;p&gt;CaseGuard Studio, a US-based AI redaction &amp;amp; investigation platform, has built software that automates this manual work with 98% accuracy. It can process thousands of files in minutes, working on data that’s kept securely on-premises of any file type, including video, audio, documents, and images.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-manual-redaction-no-longer-works"&gt;Why Manual Redaction No Longer Works&lt;/h3&gt;&lt;p&gt;Redaction is not new, but the tools most people reach for were not built for the complexity of today’s compliance requirements. Adobe Acrobat, for example, offers text redaction but needs manual work on each document. Premiere’s video editing software requires frame-by-frame subject tracking for video redaction, which is slow and impractical. These solutions provide only limited capability and were never designed for departments that process a multitude of redactions on a weekly basis.&lt;/p&gt;&lt;p&gt;CaseGuard Studio, by contrast, was purpose-built for just this challenge. It can detect 12 categories of PII (personally-identifiable information) in video and images, such as faces, license plates, notepads, and more. It tracks and redacts all PII without needing manual frame-by-frame intervention.&lt;/p&gt;&lt;p&gt;For audio and documents, CaseGuard Studio supports over 30 PII types, like names, phone numbers, and addresses. Custom keywords, phrases, or sentences can be auto-detected and redacted directly from thousands of documents and transcripts, streamlining compliance in ways manual tools can’t match. It transcribes recordings with high accuracy and can translate to and from 100+ languages, so it can redact sensitive terms in multilingual content.&lt;/p&gt;&lt;p&gt;What once took days of human labor can now happen in minutes. CaseGuard Studio automates redaction work with 98% accuracy, up to 30 times faster than manual methods, and because it runs fully on-premise, data never leaves the device.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-to-ask-when-choosing-redaction-software"&gt;What to Ask When Choosing Redaction Software&lt;/h3&gt;&lt;p&gt;For organizations evaluating redaction software, the decision often comes down to a handful of critical questions that determine whether a platform can deliver on both compliance and efficiency. The following questions are central to making the right choice.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Can the software handle every file type we work with?&lt;/strong&gt; From scanned forms and handwritten notes to video, audio, and still images, organizations in sensitive sectors deal with more than PDFs.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Is the platform fully automated?&lt;/strong&gt; If redaction still means blacking out text with a Sharpie or scrubbing video frame by frame, the process is slow and prone to error. Full automation ensures accuracy and frees staff for higher-impact work.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Does the software ensure data never leaves your environment?&lt;/strong&gt; On-premise deployment means sensitive files are processed locally, so nothing is exposed to third-party servers or cloud risks.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Does the pricing stay predictable as you scale?&lt;/strong&gt; Per-file or per-minute pricing quickly becomes unsustainable as workloads grow. Look for a flat subscription with unlimited redaction, so costs stay predictable no matter how much data you process.&lt;/li&gt;&lt;/ul&gt;&lt;h3 class="wp-block-heading" id="h-evaluating-caseguard-studio-against-the-four-redaction-essentials"&gt;Evaluating CaseGuard Studio Against the Four Redaction Essentials&lt;/h3&gt;&lt;p&gt;When assessed against these requirements, CaseGuard Studio was the only platform in our evaluation that consistently delivered across all five redaction essentials.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Auto-redact files from any source&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;From text documents and scanned forms to video, audio, images, and even handwriting, redaction has to cover every format where sensitive information might appear. Missing one identifiable feature, a face in a crowd or an un-redacted license plate, and a single oversight can be the difference between full compliance and a lawsuit. CaseGuard Studio automatically detects and redacts sensitive information across all these file types within a single platform with complete compliance.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Automated bulk redaction at speed and scale&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Thousands of files can be redacted in bulk, turning weeks of manual effort into minutes of processing. CaseGuard Studio handles workloads up to 32x faster than manual methods, with 98% accuracy, giving organizations the speed and scalability to meet growing compliance demands.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Your data, your control&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;CaseGuard Studio runs fully on-premise, within your secure environment, including air-gapped systems that are completely isolated from external networks. This ensures organizations retain full control of their data, with nothing exposed to third-party servers or cloud risks.&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt; Unlimited redaction, no pay-per-file fees&lt;/strong&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Pay-per-file pricing quickly adds up, making every additional redaction more expensive. CaseGuard Studio offers predictable pricing under a flat subscription with unlimited redaction, so costs remain the same no matter how heavy the redaction load is.&lt;/p&gt;&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt; [embedded content]&lt;/p&gt;&lt;/figure&gt;&lt;h2 class="wp-block-heading" id="h-final-thoughts"&gt;Final Thoughts&lt;/h2&gt;&lt;p&gt;Over the course of our evaluation, we compared methods and platforms ranging from manual redaction and legacy PDF editors to newer AI-driven tools that have appeared in the last few years. Most delivered partial solutions, treating written documents well but failing on audio, while others blurred faces in video, but weren’t practical to use at scale. Cloud-only options raised sovereignty and compliance concerns that, for many users, would count them out of the running entirely.&lt;/p&gt;&lt;p&gt;CaseGuard Studio was the only platform that consistently met all five requirements detailed above. It supports the widest of file types, from body-cam video to scanned or handwritten forms.&lt;/p&gt;&lt;p&gt;Audio and video are probably the most difficult formats to redact, especially at scale. Here, CaseGuard wins our vote with its AI-powered smarts. It runs fully on-premise, keeps sensitive files under organizational control, and its local AI models are refined with each version release.&lt;/p&gt;&lt;p&gt;At a time when many cloud redaction software licensing models drive up costs as workloads grow, CaseGuard’s flat pricing offers a refreshing change — predictable, transparent, and sustainable.&lt;/p&gt;&lt;p&gt;For any organization facing rising compliance demands and ever-larger volumes of sensitive data, CaseGuard Studio is well worth a closer look. Click here to book a consultation.&lt;/p&gt;&lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-redaction-that-puts-privacy-first-caseguard-studio-leading-the-way/</guid><pubDate>Wed, 08 Oct 2025 09:07:44 +0000</pubDate></item><item><title>Tuned Global strengthens its leadership in music technology with the acquisition of Figaro.ai (AI News)</title><link>https://www.artificialintelligence-news.com/news/tuned-global-strengthens-its-leadership-in-music-technology-with-the-acquisition-of-figaro-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/music-technology.png" /&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;The acquisition underscores Tuned Global’s commitment to shaping the future of the music industry by empowering clients with innovative technology and unmatched execution, while continuing to support existing Figaro.ai customers.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Wednesday, 8 October, 2025&lt;/strong&gt; — &lt;strong&gt;Tuned Global&lt;/strong&gt;, the leading music and media technology platform, has today announced the acquisition of &lt;strong&gt;Figaro.ai &lt;/strong&gt;&lt;strong&gt;(by FeedForward)&lt;/strong&gt;, a London-based audio-AI company known for making music catalogues smarter and more discoverable.&lt;/p&gt;&lt;p&gt;The acquisition advances Tuned Global’s strategy to be the cloud platform that clients build on to innovate. By bringing Figaro.ai into its partner-friendly platform, Tuned Global is enhancing its platform with AI innovation to deliver practical outcomes for customers: faster innovation, greater engagement and measurable business impact.&lt;/p&gt;&lt;p&gt;“With Figaro.ai, Tuned Global cements its position as the most comprehensive music platform, where innovation across AI, fraud detection, rights management, search and recommendations can be built,” &lt;strong&gt;said Tuned Global CEO Con Raso.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This move is about using technology to improve client outcomes and deliver value for the wider industry. It follows Tuned Global’s earlier acquisition of Pacemaker, known for pioneering AI-powered mixing technology, and reflects a clear strategy: acquiring companies whose innovation and IP help clients and the industry build the future of music technology and streaming. Integrating Figaro.ai strengthens Tuned Global’s ability to power premium, highly relevant music experiences at scale.&lt;/p&gt;&lt;p&gt;Tuned Global remains an open ecosystem, with Figaro.ai integrated as another component in a broader platform. It will operate as an integrated but distinct component within Tuned Global’s broader platform, complementing existing partners and expanding client options.&lt;/p&gt;&lt;p&gt;Current Figaro.ai clients will continue to be supported, backed by Tuned Global’s global reach, infrastructure and long-term commitment.&lt;/p&gt;&lt;p&gt;The Figaro.ai team, including founders &lt;strong&gt;Lydia Gregory&lt;/strong&gt; and &lt;strong&gt;Kevin Webster&lt;/strong&gt;, will be integrated within Tuned Global, ensuring continuity for existing customers and adding deep AI expertise to accelerate the company’s roadmap.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tuned Global CEO Con Raso&lt;/strong&gt; said he was thrilled to welcome the deeply skilled Figaro.ai team into the fold.&lt;/p&gt;&lt;p&gt;&amp;nbsp;“At Tuned Global, we see ourselves as the hub where innovation in music technology takes shape. We are building the largest open ecosystem of AI music intelligence, giving our clients maximum choice and real impact for the music industry,” he said. “With Figaro.ai joining the platform, we’re not only expanding that ecosystem with cutting-edge technology but also welcoming a highly skilled team whose expertise strengthens our ability to deliver music experiences that are powerful, flexible and future-ready.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Figaro.ai&lt;/strong&gt;&lt;strong&gt; CEO and Co-founder Lydia Gregory&lt;/strong&gt; said she was excited to join Tuned Global and amplify the impact of her company’s objectives.&lt;/p&gt;&lt;p&gt;&amp;nbsp;“Figaro.ai has always been about combining deep technical expertise with a passion for music discovery. I’m incredibly proud of the team that built this technology, and I’m thrilled that they are joining me as part of Tuned Global,” she said. “Being integrated into a platform of this scale means we can continue our mission with greater reach and impact, while ensuring continuity for the clients who already rely on us. Together, we’re ready to help the industry deliver music experiences that are more relevant, premium, and engaging than ever before.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;About Tuned Global&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Tuned Global is the leading data-driven cloud and software platform that empowers businesses to integrate commercial music into their apps or launch complete streaming experiences using advanced APIs, real-time analytics, licensing solutions, and customisable white-label apps.&lt;/p&gt;&lt;p&gt;Our turnkey solutions for music, audio, and video — coupled with a broad ecosystem of third-party music tech integrations — make us the most comprehensive platform for powering any digital music project. We streamline complexities in licensing, rights management, and content delivery, enabling rapid innovation and bringing new ideas to life.&lt;/p&gt;&lt;p&gt;Since 2011, we’ve supported 40+ companies in 70+ countries — across telecom, fitness, media, aviation, and more — to deliver innovative music experiences faster and more cost-effectively. For more information, visit www.tunedglobal.com.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;About Figaro&lt;/strong&gt;&lt;br /&gt;Figaro is the audio intelligence platform for music search, tagging and content detection – powering smarter discovery and content management across sync, DSPs, UGC, and distribution.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/music-technology.png" /&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;The acquisition underscores Tuned Global’s commitment to shaping the future of the music industry by empowering clients with innovative technology and unmatched execution, while continuing to support existing Figaro.ai customers.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Wednesday, 8 October, 2025&lt;/strong&gt; — &lt;strong&gt;Tuned Global&lt;/strong&gt;, the leading music and media technology platform, has today announced the acquisition of &lt;strong&gt;Figaro.ai &lt;/strong&gt;&lt;strong&gt;(by FeedForward)&lt;/strong&gt;, a London-based audio-AI company known for making music catalogues smarter and more discoverable.&lt;/p&gt;&lt;p&gt;The acquisition advances Tuned Global’s strategy to be the cloud platform that clients build on to innovate. By bringing Figaro.ai into its partner-friendly platform, Tuned Global is enhancing its platform with AI innovation to deliver practical outcomes for customers: faster innovation, greater engagement and measurable business impact.&lt;/p&gt;&lt;p&gt;“With Figaro.ai, Tuned Global cements its position as the most comprehensive music platform, where innovation across AI, fraud detection, rights management, search and recommendations can be built,” &lt;strong&gt;said Tuned Global CEO Con Raso.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This move is about using technology to improve client outcomes and deliver value for the wider industry. It follows Tuned Global’s earlier acquisition of Pacemaker, known for pioneering AI-powered mixing technology, and reflects a clear strategy: acquiring companies whose innovation and IP help clients and the industry build the future of music technology and streaming. Integrating Figaro.ai strengthens Tuned Global’s ability to power premium, highly relevant music experiences at scale.&lt;/p&gt;&lt;p&gt;Tuned Global remains an open ecosystem, with Figaro.ai integrated as another component in a broader platform. It will operate as an integrated but distinct component within Tuned Global’s broader platform, complementing existing partners and expanding client options.&lt;/p&gt;&lt;p&gt;Current Figaro.ai clients will continue to be supported, backed by Tuned Global’s global reach, infrastructure and long-term commitment.&lt;/p&gt;&lt;p&gt;The Figaro.ai team, including founders &lt;strong&gt;Lydia Gregory&lt;/strong&gt; and &lt;strong&gt;Kevin Webster&lt;/strong&gt;, will be integrated within Tuned Global, ensuring continuity for existing customers and adding deep AI expertise to accelerate the company’s roadmap.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Tuned Global CEO Con Raso&lt;/strong&gt; said he was thrilled to welcome the deeply skilled Figaro.ai team into the fold.&lt;/p&gt;&lt;p&gt;&amp;nbsp;“At Tuned Global, we see ourselves as the hub where innovation in music technology takes shape. We are building the largest open ecosystem of AI music intelligence, giving our clients maximum choice and real impact for the music industry,” he said. “With Figaro.ai joining the platform, we’re not only expanding that ecosystem with cutting-edge technology but also welcoming a highly skilled team whose expertise strengthens our ability to deliver music experiences that are powerful, flexible and future-ready.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Figaro.ai&lt;/strong&gt;&lt;strong&gt; CEO and Co-founder Lydia Gregory&lt;/strong&gt; said she was excited to join Tuned Global and amplify the impact of her company’s objectives.&lt;/p&gt;&lt;p&gt;&amp;nbsp;“Figaro.ai has always been about combining deep technical expertise with a passion for music discovery. I’m incredibly proud of the team that built this technology, and I’m thrilled that they are joining me as part of Tuned Global,” she said. “Being integrated into a platform of this scale means we can continue our mission with greater reach and impact, while ensuring continuity for the clients who already rely on us. Together, we’re ready to help the industry deliver music experiences that are more relevant, premium, and engaging than ever before.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;About Tuned Global&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Tuned Global is the leading data-driven cloud and software platform that empowers businesses to integrate commercial music into their apps or launch complete streaming experiences using advanced APIs, real-time analytics, licensing solutions, and customisable white-label apps.&lt;/p&gt;&lt;p&gt;Our turnkey solutions for music, audio, and video — coupled with a broad ecosystem of third-party music tech integrations — make us the most comprehensive platform for powering any digital music project. We streamline complexities in licensing, rights management, and content delivery, enabling rapid innovation and bringing new ideas to life.&lt;/p&gt;&lt;p&gt;Since 2011, we’ve supported 40+ companies in 70+ countries — across telecom, fitness, media, aviation, and more — to deliver innovative music experiences faster and more cost-effectively. For more information, visit www.tunedglobal.com.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;About Figaro&lt;/strong&gt;&lt;br /&gt;Figaro is the audio intelligence platform for music search, tagging and content detection – powering smarter discovery and content management across sync, DSPs, UGC, and distribution.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/tuned-global-strengthens-its-leadership-in-music-technology-with-the-acquisition-of-figaro-ai/</guid><pubDate>Wed, 08 Oct 2025 10:37:29 +0000</pubDate></item><item><title>Introducing the Gemini 2.5 Computer Use model (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/introducing-the-gemini-2-5-computer-use-model/</link><description>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Gemini Computer Use" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU_2096x1182_RD6-V01.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
              







            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Earlier this year, we mentioned that we're bringing computer use capabilities to developers via the Gemini API. Today, we are releasing the Gemini 2.5 Computer Use model, our new specialized model built on Gemini 2.5 Pro’s visual understanding and reasoning capabilities that powers agents capable of interacting with user interfaces (UIs). It outperforms leading alternatives on multiple web and mobile control benchmarks, all with lower latency. Developers can access these capabilities via the Gemini API in Google AI Studio and Vertex AI.&lt;/p&gt;&lt;p&gt;While AI models can interface with software through structured APIs, many digital tasks still require direct interaction with graphical user interfaces, for example, filling and submitting forms. To complete these tasks, agents must navigate web pages and applications just as humans do: by clicking, typing and scrolling. The ability to natively fill out forms, manipulate interactive elements like dropdowns and filters, and operate behind logins is a crucial next step in building powerful, general-purpose agents.&lt;/p&gt;&lt;h2&gt;How it works&lt;/h2&gt;&lt;p&gt;The model’s core capabilities are exposed through the new `computer_use` tool in the Gemini API and should be operated within a loop. Inputs to the tool are the user request, screenshot of the environment, and a history of recent actions. The input can also specify whether to exclude functions from the full list of supported UI actions or specify additional custom functions to include.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 2.5 Computer Use Model flow&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="Diagram of AI agent loop: Initial task leads to a screenshot/context, which is sent to the Model, which returns a response to the computer environment to execute an action." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;The model then analyzes these inputs and generates a response, typically a function call representing one of the UI actions such as clicking or typing. This response may also contain a request for an end user confirmation, which is required for certain actions such as making a purchase. The client-side code then executes the received action.&lt;/p&gt;&lt;p&gt;After the action is executed, a new screenshot of the GUI and the current URL are sent back to the Computer Use model as a function response restarting the loop. This iterative process continues until the task is complete, an error occurs or the interaction is terminated by a safety response or user decision.&lt;/p&gt;&lt;p&gt;The Gemini 2.5 Computer Use model is primarily optimized for web browsers, but also demonstrates strong promise for mobile UI control tasks. It is not yet optimized for desktop OS-level control.&lt;/p&gt;&lt;p&gt;Check out a few demos below to see the model in action (shown here at 3X speed).&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;&lt;b&gt;Prompt:&lt;/b&gt; “From https://tinyurl.com/pet-care-signup, get all details for any pet with a California residency and add them as a guest in my spa CRM at https://pet-luxe-spa.web.app/. Then, set up a follow up visit appointment with the specialist Anima Lavar for October 10th anytime after 8am. The reason for the visit is the same as their requested treatment.”&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;&lt;b&gt;Prompt: “&lt;/b&gt;My art club brainstormed tasks ahead of our fair. The board is chaotic and I need your help organizing the tasks into some categories I created. Go to sticky-note-jam.web.app and ensure notes are clearly in the right sections. Drag them there if not.”&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How it performs&lt;/h2&gt;&lt;p&gt;The Gemini 2.5 Computer Use model demonstrates strong performance on multiple web and mobile control benchmarks. The table below includes results from self-reported numbers, evaluations run by Browserbase and evaluations we ran ourselves. Evaluation details are available in the Gemini 2.5 Computer Use evaluation info and in Browserbase’s blog post. Unless otherwise indicated, scores shown are for computer use tools exposed via API.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 2.5 Computer Use outperforms leading alternatives on multiple benchmarks&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="Benchmark performance table: Gemini 2.5 Computer Use leads in Online-Mind2Web, WebVoyager, and AndroidWorld benchmarks." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;The model offers leading quality for browser control at the lowest latency, as measured by performance on the Browserbase harness for Online-Mind2Web.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 2.5 Computer Use delivers high accuracy while maintaining low latency&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="Latency vs. Quality scatterplot: Gemini 2.5 Computer Use is lowest in latency and highest in accuracy (70%+ accuracy, ∼225 sec latency)." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU-Scatterplot-RD7.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How we approached safety&lt;/h2&gt;&lt;p&gt;We believe that the only way to build agents that will benefit everyone is to be responsible from the start. AI agents that control computers introduce unique risks, including intentional misuse by users, unexpected model behavior, and prompt injections and scams in the web environment. Thus, it is critical to implement safety guardrails with care.&lt;/p&gt;&lt;p&gt;We have trained safety features directly into the model to address these three key risks (described in the Gemini 2.5 Computer Use System Card).&lt;/p&gt;&lt;p&gt;Further, we also provide developers with safety controls, which empower developers to prevent the model from auto-completing potentially high-risk or harmful actions. Examples of these actions include harming a system's integrity, compromising security, bypassing CAPTCHAs, or controlling medical devices. The controls:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Per-step safety service:&lt;/b&gt; An out-of-model, inference-time safety service that assesses each action the model proposes before it’s executed.&lt;/li&gt;&lt;li&gt;&lt;b&gt;System instructions:&lt;/b&gt; Developers can further specify that the agent either refuses or asks for user confirmation before it takes specific kinds of high-stakes actions. (Example in documentation).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Additional recommendations for developers on safety measures and best practices can be found in our documentation. While these safeguards are designed to reduce risk, we urge all developers to thoroughly test their systems before launch.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How early testers have used it&lt;/h2&gt;&lt;p&gt;Google teams have already deployed the model to production for use cases including UI testing, which can make software development signficantly faster. Versions of this model have also been powering Project Mariner, the Firebase Testing Agent, and some agentic capabilities in AI Mode in Search.&lt;/p&gt;&lt;p&gt;Users from our early access program have also been testing the model to power personal assistants, workflow automation, and UI testing, and have seen strong results. In their own words:&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    








  

  
    








  

  
    








  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How to get started&lt;/h2&gt;&lt;p&gt;Starting today, the model is available in public preview, accessible via the Gemini API on Google AI Studio and Vertex AI.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Try it now:&lt;/b&gt; In a demo environment hosted by Browserbase.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Start building&lt;/b&gt;: Dive into our reference and documentation (see Vertex AI docs for enterprise use) to learn how to build your own agent loop locally with Playwright or in a cloud VM with Browserbase.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Join the community:&lt;/b&gt; We’re excited to see what you build. Share feedback and help guide our roadmap in our Developer Forum.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


Gemini Models


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</description><content:encoded>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Gemini Computer Use" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU_2096x1182_RD6-V01.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
              







            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;Earlier this year, we mentioned that we're bringing computer use capabilities to developers via the Gemini API. Today, we are releasing the Gemini 2.5 Computer Use model, our new specialized model built on Gemini 2.5 Pro’s visual understanding and reasoning capabilities that powers agents capable of interacting with user interfaces (UIs). It outperforms leading alternatives on multiple web and mobile control benchmarks, all with lower latency. Developers can access these capabilities via the Gemini API in Google AI Studio and Vertex AI.&lt;/p&gt;&lt;p&gt;While AI models can interface with software through structured APIs, many digital tasks still require direct interaction with graphical user interfaces, for example, filling and submitting forms. To complete these tasks, agents must navigate web pages and applications just as humans do: by clicking, typing and scrolling. The ability to natively fill out forms, manipulate interactive elements like dropdowns and filters, and operate behind logins is a crucial next step in building powerful, general-purpose agents.&lt;/p&gt;&lt;h2&gt;How it works&lt;/h2&gt;&lt;p&gt;The model’s core capabilities are exposed through the new `computer_use` tool in the Gemini API and should be operated within a loop. Inputs to the tool are the user request, screenshot of the environment, and a history of recent actions. The input can also specify whether to exclude functions from the full list of supported UI actions or specify additional custom functions to include.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 2.5 Computer Use Model flow&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="Diagram of AI agent loop: Initial task leads to a screenshot/context, which is sent to the Model, which returns a response to the computer environment to execute an action." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;The model then analyzes these inputs and generates a response, typically a function call representing one of the UI actions such as clicking or typing. This response may also contain a request for an end user confirmation, which is required for certain actions such as making a purchase. The client-side code then executes the received action.&lt;/p&gt;&lt;p&gt;After the action is executed, a new screenshot of the GUI and the current URL are sent back to the Computer Use model as a function response restarting the loop. This iterative process continues until the task is complete, an error occurs or the interaction is terminated by a safety response or user decision.&lt;/p&gt;&lt;p&gt;The Gemini 2.5 Computer Use model is primarily optimized for web browsers, but also demonstrates strong promise for mobile UI control tasks. It is not yet optimized for desktop OS-level control.&lt;/p&gt;&lt;p&gt;Check out a few demos below to see the model in action (shown here at 3X speed).&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;&lt;b&gt;Prompt:&lt;/b&gt; “From https://tinyurl.com/pet-care-signup, get all details for any pet with a California residency and add them as a guest in my spa CRM at https://pet-luxe-spa.web.app/. Then, set up a follow up visit appointment with the specialist Anima Lavar for October 10th anytime after 8am. The reason for the visit is the same as their requested treatment.”&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;&lt;b&gt;Prompt: “&lt;/b&gt;My art club brainstormed tasks ahead of our fair. The board is chaotic and I need your help organizing the tasks into some categories I created. Go to sticky-note-jam.web.app and ensure notes are clearly in the right sections. Drag them there if not.”&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How it performs&lt;/h2&gt;&lt;p&gt;The Gemini 2.5 Computer Use model demonstrates strong performance on multiple web and mobile control benchmarks. The table below includes results from self-reported numbers, evaluations run by Browserbase and evaluations we ran ourselves. Evaluation details are available in the Gemini 2.5 Computer Use evaluation info and in Browserbase’s blog post. Unless otherwise indicated, scores shown are for computer use tools exposed via API.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 2.5 Computer Use outperforms leading alternatives on multiple benchmarks&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="Benchmark performance table: Gemini 2.5 Computer Use leads in Online-Mind2Web, WebVoyager, and AndroidWorld benchmarks." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;The model offers leading quality for browser control at the lowest latency, as measured by performance on the Browserbase harness for Online-Mind2Web.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    















  
    &lt;div&gt;
      &lt;div class="rich-text"&gt;&lt;p&gt;Gemini 2.5 Computer Use delivers high accuracy while maintaining low latency&lt;/p&gt;&lt;/div&gt;
    &lt;/div&gt;
  
  
    &lt;div&gt;
      &lt;img alt="Latency vs. Quality scatterplot: Gemini 2.5 Computer Use is lowest in latency and highest in accuracy (70%+ accuracy, ∼225 sec latency)." src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/CTU-Scatterplot-RD7.width-1000.format-webp.webp" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How we approached safety&lt;/h2&gt;&lt;p&gt;We believe that the only way to build agents that will benefit everyone is to be responsible from the start. AI agents that control computers introduce unique risks, including intentional misuse by users, unexpected model behavior, and prompt injections and scams in the web environment. Thus, it is critical to implement safety guardrails with care.&lt;/p&gt;&lt;p&gt;We have trained safety features directly into the model to address these three key risks (described in the Gemini 2.5 Computer Use System Card).&lt;/p&gt;&lt;p&gt;Further, we also provide developers with safety controls, which empower developers to prevent the model from auto-completing potentially high-risk or harmful actions. Examples of these actions include harming a system's integrity, compromising security, bypassing CAPTCHAs, or controlling medical devices. The controls:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Per-step safety service:&lt;/b&gt; An out-of-model, inference-time safety service that assesses each action the model proposes before it’s executed.&lt;/li&gt;&lt;li&gt;&lt;b&gt;System instructions:&lt;/b&gt; Developers can further specify that the agent either refuses or asks for user confirmation before it takes specific kinds of high-stakes actions. (Example in documentation).&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Additional recommendations for developers on safety measures and best practices can be found in our documentation. While these safeguards are designed to reduce risk, we urge all developers to thoroughly test their systems before launch.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How early testers have used it&lt;/h2&gt;&lt;p&gt;Google teams have already deployed the model to production for use cases including UI testing, which can make software development signficantly faster. Versions of this model have also been powering Project Mariner, the Firebase Testing Agent, and some agentic capabilities in AI Mode in Search.&lt;/p&gt;&lt;p&gt;Users from our early access program have also been testing the model to power personal assistants, workflow automation, and UI testing, and have seen strong results. In their own words:&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    








  

  
    








  

  
    








  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How to get started&lt;/h2&gt;&lt;p&gt;Starting today, the model is available in public preview, accessible via the Gemini API on Google AI Studio and Vertex AI.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;Try it now:&lt;/b&gt; In a demo environment hosted by Browserbase.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Start building&lt;/b&gt;: Dive into our reference and documentation (see Vertex AI docs for enterprise use) to learn how to build your own agent loop locally with Playwright or in a cloud VM with Browserbase.&lt;/li&gt;&lt;li&gt;&lt;b&gt;Join the community:&lt;/b&gt; We’re excited to see what you build. Share feedback and help guide our roadmap in our Developer Forum.&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


Gemini Models


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/introducing-the-gemini-2-5-computer-use-model/</guid><pubDate>Wed, 08 Oct 2025 10:55:26 +0000</pubDate></item><item><title>Samsung’s tiny AI model beats giant reasoning LLMs (AI News)</title><link>https://www.artificialintelligence-news.com/news/samsung-tiny-ai-model-beats-giant-reasoning-llms/</link><description>&lt;p&gt;A new paper from a Samsung AI researcher explains how a small network can beat massive Large Language Models (LLMs) in complex reasoning.&lt;/p&gt;&lt;p&gt;In the race for AI supremacy, the industry mantra has often been “bigger is better.” Tech giants have poured billions into creating ever-larger models, but according to Alexia Jolicoeur-Martineau of Samsung SAIL Montréal, a radically different and more efficient path forward is possible with the Tiny Recursive Model (TRM).&lt;/p&gt;&lt;p&gt;Using a model with just 7 million parameters, less than 0.01% of the size of leading LLMs, TRM achieves new state-of-the-art results on notoriously difficult benchmarks like the ARC-AGI intelligence test. Samsung’s work challenges the prevailing assumption that sheer scale is the only way to advance the capabilities of AI models, offering a more sustainable and parameter-efficient alternative.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-overcoming-the-limits-of-scale"&gt;Overcoming the limits of scale&lt;/h3&gt;&lt;p&gt;While LLMs have shown incredible prowess in generating human-like text, their ability to perform complex, multi-step reasoning can be brittle. Because they generate answers token-by-token, a single mistake early in the process can derail the entire solution, leading to an invalid final answer.&lt;/p&gt;&lt;p&gt;Techniques like Chain-of-Thought, where a model “thinks out loud” to break down a problem, have been developed to mitigate this. However, these methods are computationally expensive, often require vast amounts of high-quality reasoning data that may not be available, and can still produce flawed logic. Even with these augmentations, LLMs struggle with certain puzzles where perfect logical execution is necessary.&lt;/p&gt;&lt;p&gt;Samsung’s work builds upon a recent AI model known as the Hierarchical Reasoning Model (HRM). HRM introduced a novel method using two small neural networks that recursively work on a problem at different frequencies to refine an answer. It showed great promise but was complicated, relying on uncertain biological arguments and complex fixed-point theorems that were not guaranteed to apply.&lt;/p&gt;&lt;p&gt;Instead of HRM’s two networks, TRM uses a single, tiny network that recursively improves both its internal “reasoning” and its proposed “answer”.&lt;/p&gt;&lt;p&gt;The model is given the question, an initial guess at the answer, and a latent reasoning feature. It first cycles through several steps to refine its latent reasoning based on all three inputs. Then, using this improved reasoning, it updates its prediction for the final answer. This entire process can be repeated up to 16 times, allowing the model to progressively correct its own mistakes in a highly parameter-efficient manner.&lt;/p&gt;&lt;p&gt;Counterintuitively, the research discovered that a tiny network with only two layers achieved far better generalisation than a four-layer version. This reduction in size appears to prevent the model from overfitting; a common problem when training on smaller, specialised datasets.&lt;/p&gt;&lt;p&gt;TRM also dispenses with the complex mathematical justifications used by its predecessor. The original HRM model required the assumption that its functions converged to a fixed point to justify its training method. TRM bypasses this entirely by simply back-propagating through its full recursion process. This change alone provided a massive boost in performance, improving accuracy on the Sudoku-Extreme benchmark from 56.5% to 87.4% in an ablation study.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-samsung-s-model-smashes-ai-benchmarks-with-fewer-resources"&gt;Samsung’s model smashes AI benchmarks with fewer resources&lt;/h3&gt;&lt;p&gt;The results speak for themselves. On the Sudoku-Extreme dataset, which uses only 1,000 training examples, TRM achieves an 87.4% test accuracy, a huge leap from HRM’s 55%. On Maze-Hard, a task involving finding long paths through 30×30 mazes, TRM scores 85.3% compared to HRM’s 74.5%.&lt;/p&gt;&lt;p&gt;Most notably, TRM makes huge strides on the Abstraction and Reasoning Corpus (ARC-AGI), a benchmark designed to measure true fluid intelligence in AI. With just 7M parameters, TRM achieves 44.6% accuracy on ARC-AGI-1 and 7.8% on ARC-AGI-2. This outperforms HRM, which used a 27M parameter model, and even surpasses many of the world’s largest LLMs. For comparison, Gemini 2.5 Pro scores only 4.9% on ARC-AGI-2.&lt;/p&gt;&lt;p&gt;The training process for TRM has also been made more efficient. An adaptive mechanism called ACT – which decides when the model has improved an answer enough and can move to a new data sample – was simplified to remove the need for a second, costly forward pass through the network during each training step. This change was made with no major difference in final generalisation.&lt;/p&gt;&lt;p&gt;This research from Samsung presents a compelling argument against the current trajectory of ever-expanding AI models. It shows that by designing architectures that can iteratively reason and self-correct, it is possible to solve extremely difficult problems with a tiny fraction of the computational resources.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google’s new AI agent rewrites code to automate vulnerability fixes&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109805" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-1.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;A new paper from a Samsung AI researcher explains how a small network can beat massive Large Language Models (LLMs) in complex reasoning.&lt;/p&gt;&lt;p&gt;In the race for AI supremacy, the industry mantra has often been “bigger is better.” Tech giants have poured billions into creating ever-larger models, but according to Alexia Jolicoeur-Martineau of Samsung SAIL Montréal, a radically different and more efficient path forward is possible with the Tiny Recursive Model (TRM).&lt;/p&gt;&lt;p&gt;Using a model with just 7 million parameters, less than 0.01% of the size of leading LLMs, TRM achieves new state-of-the-art results on notoriously difficult benchmarks like the ARC-AGI intelligence test. Samsung’s work challenges the prevailing assumption that sheer scale is the only way to advance the capabilities of AI models, offering a more sustainable and parameter-efficient alternative.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-overcoming-the-limits-of-scale"&gt;Overcoming the limits of scale&lt;/h3&gt;&lt;p&gt;While LLMs have shown incredible prowess in generating human-like text, their ability to perform complex, multi-step reasoning can be brittle. Because they generate answers token-by-token, a single mistake early in the process can derail the entire solution, leading to an invalid final answer.&lt;/p&gt;&lt;p&gt;Techniques like Chain-of-Thought, where a model “thinks out loud” to break down a problem, have been developed to mitigate this. However, these methods are computationally expensive, often require vast amounts of high-quality reasoning data that may not be available, and can still produce flawed logic. Even with these augmentations, LLMs struggle with certain puzzles where perfect logical execution is necessary.&lt;/p&gt;&lt;p&gt;Samsung’s work builds upon a recent AI model known as the Hierarchical Reasoning Model (HRM). HRM introduced a novel method using two small neural networks that recursively work on a problem at different frequencies to refine an answer. It showed great promise but was complicated, relying on uncertain biological arguments and complex fixed-point theorems that were not guaranteed to apply.&lt;/p&gt;&lt;p&gt;Instead of HRM’s two networks, TRM uses a single, tiny network that recursively improves both its internal “reasoning” and its proposed “answer”.&lt;/p&gt;&lt;p&gt;The model is given the question, an initial guess at the answer, and a latent reasoning feature. It first cycles through several steps to refine its latent reasoning based on all three inputs. Then, using this improved reasoning, it updates its prediction for the final answer. This entire process can be repeated up to 16 times, allowing the model to progressively correct its own mistakes in a highly parameter-efficient manner.&lt;/p&gt;&lt;p&gt;Counterintuitively, the research discovered that a tiny network with only two layers achieved far better generalisation than a four-layer version. This reduction in size appears to prevent the model from overfitting; a common problem when training on smaller, specialised datasets.&lt;/p&gt;&lt;p&gt;TRM also dispenses with the complex mathematical justifications used by its predecessor. The original HRM model required the assumption that its functions converged to a fixed point to justify its training method. TRM bypasses this entirely by simply back-propagating through its full recursion process. This change alone provided a massive boost in performance, improving accuracy on the Sudoku-Extreme benchmark from 56.5% to 87.4% in an ablation study.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-samsung-s-model-smashes-ai-benchmarks-with-fewer-resources"&gt;Samsung’s model smashes AI benchmarks with fewer resources&lt;/h3&gt;&lt;p&gt;The results speak for themselves. On the Sudoku-Extreme dataset, which uses only 1,000 training examples, TRM achieves an 87.4% test accuracy, a huge leap from HRM’s 55%. On Maze-Hard, a task involving finding long paths through 30×30 mazes, TRM scores 85.3% compared to HRM’s 74.5%.&lt;/p&gt;&lt;p&gt;Most notably, TRM makes huge strides on the Abstraction and Reasoning Corpus (ARC-AGI), a benchmark designed to measure true fluid intelligence in AI. With just 7M parameters, TRM achieves 44.6% accuracy on ARC-AGI-1 and 7.8% on ARC-AGI-2. This outperforms HRM, which used a 27M parameter model, and even surpasses many of the world’s largest LLMs. For comparison, Gemini 2.5 Pro scores only 4.9% on ARC-AGI-2.&lt;/p&gt;&lt;p&gt;The training process for TRM has also been made more efficient. An adaptive mechanism called ACT – which decides when the model has improved an answer enough and can move to a new data sample – was simplified to remove the need for a second, costly forward pass through the network during each training step. This change was made with no major difference in final generalisation.&lt;/p&gt;&lt;p&gt;This research from Samsung presents a compelling argument against the current trajectory of ever-expanding AI models. It shows that by designing architectures that can iteratively reason and self-correct, it is possible to solve extremely difficult problems with a tiny fraction of the computational resources.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Google’s new AI agent rewrites code to automate vulnerability fixes&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109805" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-1.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/samsung-tiny-ai-model-beats-giant-reasoning-llms/</guid><pubDate>Wed, 08 Oct 2025 11:55:19 +0000</pubDate></item><item><title>AI21’s Jamba Reasoning 3B Redefines What “Small” Means in LLMs — 250K Context on a Laptop (AI | VentureBeat)</title><link>https://venturebeat.com/ai/ai21s-jamba-reasoning-3b-redefines-what-small-means-in-llms-250k-context-on</link><description>[unable to retrieve full-text content]&lt;p&gt;The latest addition to the small model wave for enterprises comes from &lt;a href="https://www.ai21.com/"&gt;&lt;u&gt;AI21 Labs&lt;/u&gt;&lt;/a&gt;, which is betting that bringing models to devices will free up traffic in data centers. &lt;/p&gt;&lt;p&gt;AI21’s Jamba Reasoning 3B, a “tiny” open-source model that can run extended reasoning, code generation and respond based on ground truth. Jamba Reasoning 3B handles more than 250,000 tokens and can run inference on edge devices. &lt;/p&gt;&lt;p&gt;The company said Jamba Reasoning 3B works on devices such as laptops and mobile phones. &lt;/p&gt;&lt;p&gt;Ori Goshen, co-CEO of AI21, told VentureBeat that the company sees more enterprise use cases for small models, mainly because moving most inference to devices frees up data centers.  &lt;/p&gt;&lt;p&gt;“What we&amp;#x27;re seeing right now in the industry is an economics issue where there are very expensive data center build-outs, and the revenue that is generated from the data centers versus the depreciation rate of all their chips shows the math doesn&amp;#x27;t add up,” Goshen said. &lt;/p&gt;&lt;p&gt;He added that in the future “the industry by and large would be hybrid in the sense that some of the computation will be on devices locally and other inference will move to GPUs.”&lt;/p&gt;&lt;h2&gt;Tested on a MacBook&lt;/h2&gt;&lt;p&gt;
Jamba Reasoning 3B combines the Mamba architecture and Transformers to allow it to run a 250K token window on devices. AI21 said it can do 2-4x faster inference speeds. Goshen said the Mamba architecture significantly contributed to the model’s speed. &lt;/p&gt;&lt;p&gt;Jamba Reasoning 3B’s hybrid architecture also allows it to reduce memory requirements, thereby reducing its computing needs. &lt;/p&gt;&lt;p&gt;AI21 tested the model on a standard MacBook Pro and found that it can process 35 tokens per second. &lt;/p&gt;&lt;p&gt;Goshen said the model works best for tasks involving function calling, policy-grounded generation and tool routing. He said that simple requests, such as asking for information about a forthcoming meeting and asking the model to create an agenda for it, could be done on devices. The more complex reasoning tasks can be saved for GPU clusters. &lt;/p&gt;&lt;h2&gt;Small models in enterprise&lt;/h2&gt;&lt;p&gt;Enterprises have been interested in using a mix of small models, some of which are specifically designed for their industry and some that are condensed versions of LLMs. &lt;/p&gt;&lt;p&gt;In September, &lt;a href="https://business.facebook.com/"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt; released &lt;a href="https://venturebeat.com/ai/metas-new-small-reasoning-model-shows-industry-shift-toward-tiny-ai-for"&gt;&lt;u&gt;MobileLLM-R1, a family of reasoning models&lt;/u&gt;&lt;/a&gt; ranging from 140M to 950M parameters. These models are designed for math, coding and scientific reasoning rather than chat applications. MobileLLM-R1 can run on compute-constrained devices. &lt;/p&gt;&lt;p&gt;&lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;’s &lt;a href="https://venturebeat.com/ai/google-unveils-open-source-gemma-3-model-with-128k-context-window"&gt;&lt;u&gt;Gemma&lt;/u&gt;&lt;/a&gt; was one of the first small models to come to the market, designed to run on portable devices like laptops and mobile phones. Gemma has since &lt;a href="https://venturebeat.com/ai/google-unveils-ultra-small-and-efficient-open-source-ai-model-gemma-3-270m-that-can-run-on-smartphones"&gt;&lt;u&gt;been expanded&lt;/u&gt;&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Companies like &lt;a href="https://www.myfico.com/"&gt;&lt;u&gt;FICO&lt;/u&gt;&lt;/a&gt; have also begun building their own models. &lt;a href="https://venturebeat.com/ai/ficos-answer-to-ai-risk-a-foundation-model-that-scores-every-output-for"&gt;&lt;u&gt;FICO launched&lt;/u&gt;&lt;/a&gt; its FICO Focused Language and FICO Focused Sequence small models that will only answer finance-specific questions. &lt;/p&gt;&lt;p&gt;Goshen said the big difference their model offers is that it’s even smaller than most models and yet it can run reasoning tasks without sacrificing speed. &lt;/p&gt;&lt;h2&gt;Benchmark testing &lt;/h2&gt;&lt;p&gt;In benchmark testing, Jamba Reasoning 3B demonstrated strong performance compared to other small models, including &lt;a href="https://chat.qwen.ai/"&gt;&lt;u&gt;Qwen&lt;/u&gt;&lt;/a&gt; 4B, &lt;a href="https://business.facebook.com/"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt;’s Llama 3.2B-3B, and Phi-4-Mini from &lt;a href="https://www.microsoft.com/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;It outperformed all models on the IFBench test and Humanity’s Last Exam, although it came in second to Qwen 4 on MMLU-Pro. &lt;/p&gt;&lt;p&gt;Goshen said another advantage of small models like Jamba Reasoning 3B is that they are highly steerable and provide better privacy options to enterprises because the inference is not sent to a server elsewhere. &lt;/p&gt;&lt;p&gt;“I do believe there’s a world where you can optimize for the needs and the experience of the customer, and the models that will be kept on devices are a large part of it,” he said. &lt;/p&gt;&lt;p&gt;




&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The latest addition to the small model wave for enterprises comes from &lt;a href="https://www.ai21.com/"&gt;&lt;u&gt;AI21 Labs&lt;/u&gt;&lt;/a&gt;, which is betting that bringing models to devices will free up traffic in data centers. &lt;/p&gt;&lt;p&gt;AI21’s Jamba Reasoning 3B, a “tiny” open-source model that can run extended reasoning, code generation and respond based on ground truth. Jamba Reasoning 3B handles more than 250,000 tokens and can run inference on edge devices. &lt;/p&gt;&lt;p&gt;The company said Jamba Reasoning 3B works on devices such as laptops and mobile phones. &lt;/p&gt;&lt;p&gt;Ori Goshen, co-CEO of AI21, told VentureBeat that the company sees more enterprise use cases for small models, mainly because moving most inference to devices frees up data centers.  &lt;/p&gt;&lt;p&gt;“What we&amp;#x27;re seeing right now in the industry is an economics issue where there are very expensive data center build-outs, and the revenue that is generated from the data centers versus the depreciation rate of all their chips shows the math doesn&amp;#x27;t add up,” Goshen said. &lt;/p&gt;&lt;p&gt;He added that in the future “the industry by and large would be hybrid in the sense that some of the computation will be on devices locally and other inference will move to GPUs.”&lt;/p&gt;&lt;h2&gt;Tested on a MacBook&lt;/h2&gt;&lt;p&gt;
Jamba Reasoning 3B combines the Mamba architecture and Transformers to allow it to run a 250K token window on devices. AI21 said it can do 2-4x faster inference speeds. Goshen said the Mamba architecture significantly contributed to the model’s speed. &lt;/p&gt;&lt;p&gt;Jamba Reasoning 3B’s hybrid architecture also allows it to reduce memory requirements, thereby reducing its computing needs. &lt;/p&gt;&lt;p&gt;AI21 tested the model on a standard MacBook Pro and found that it can process 35 tokens per second. &lt;/p&gt;&lt;p&gt;Goshen said the model works best for tasks involving function calling, policy-grounded generation and tool routing. He said that simple requests, such as asking for information about a forthcoming meeting and asking the model to create an agenda for it, could be done on devices. The more complex reasoning tasks can be saved for GPU clusters. &lt;/p&gt;&lt;h2&gt;Small models in enterprise&lt;/h2&gt;&lt;p&gt;Enterprises have been interested in using a mix of small models, some of which are specifically designed for their industry and some that are condensed versions of LLMs. &lt;/p&gt;&lt;p&gt;In September, &lt;a href="https://business.facebook.com/"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt; released &lt;a href="https://venturebeat.com/ai/metas-new-small-reasoning-model-shows-industry-shift-toward-tiny-ai-for"&gt;&lt;u&gt;MobileLLM-R1, a family of reasoning models&lt;/u&gt;&lt;/a&gt; ranging from 140M to 950M parameters. These models are designed for math, coding and scientific reasoning rather than chat applications. MobileLLM-R1 can run on compute-constrained devices. &lt;/p&gt;&lt;p&gt;&lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;’s &lt;a href="https://venturebeat.com/ai/google-unveils-open-source-gemma-3-model-with-128k-context-window"&gt;&lt;u&gt;Gemma&lt;/u&gt;&lt;/a&gt; was one of the first small models to come to the market, designed to run on portable devices like laptops and mobile phones. Gemma has since &lt;a href="https://venturebeat.com/ai/google-unveils-ultra-small-and-efficient-open-source-ai-model-gemma-3-270m-that-can-run-on-smartphones"&gt;&lt;u&gt;been expanded&lt;/u&gt;&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Companies like &lt;a href="https://www.myfico.com/"&gt;&lt;u&gt;FICO&lt;/u&gt;&lt;/a&gt; have also begun building their own models. &lt;a href="https://venturebeat.com/ai/ficos-answer-to-ai-risk-a-foundation-model-that-scores-every-output-for"&gt;&lt;u&gt;FICO launched&lt;/u&gt;&lt;/a&gt; its FICO Focused Language and FICO Focused Sequence small models that will only answer finance-specific questions. &lt;/p&gt;&lt;p&gt;Goshen said the big difference their model offers is that it’s even smaller than most models and yet it can run reasoning tasks without sacrificing speed. &lt;/p&gt;&lt;h2&gt;Benchmark testing &lt;/h2&gt;&lt;p&gt;In benchmark testing, Jamba Reasoning 3B demonstrated strong performance compared to other small models, including &lt;a href="https://chat.qwen.ai/"&gt;&lt;u&gt;Qwen&lt;/u&gt;&lt;/a&gt; 4B, &lt;a href="https://business.facebook.com/"&gt;&lt;u&gt;Meta&lt;/u&gt;&lt;/a&gt;’s Llama 3.2B-3B, and Phi-4-Mini from &lt;a href="https://www.microsoft.com/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;It outperformed all models on the IFBench test and Humanity’s Last Exam, although it came in second to Qwen 4 on MMLU-Pro. &lt;/p&gt;&lt;p&gt;Goshen said another advantage of small models like Jamba Reasoning 3B is that they are highly steerable and provide better privacy options to enterprises because the inference is not sent to a server elsewhere. &lt;/p&gt;&lt;p&gt;“I do believe there’s a world where you can optimize for the needs and the experience of the customer, and the models that will be kept on devices are a large part of it,” he said. &lt;/p&gt;&lt;p&gt;




&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ai21s-jamba-reasoning-3b-redefines-what-small-means-in-llms-250k-context-on</guid><pubDate>Wed, 08 Oct 2025 12:00:00 +0000</pubDate></item><item><title>The Download: carbon removal factories’ funding cuts, and AI toys (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/08/1125301/the-download-carbon-removal-factories-funding-cuts-and-ai-toys/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The Trump administration may cut funding for two major direct-air capture plants&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The US Department of Energy appears poised to terminate funding for a pair of large carbon-sucking factories that were originally set to receive more than $1 billion in government grants, according to a department-issued list of projects obtained by &lt;em&gt;MIT Technology Review&lt;/em&gt; and circulating among federal agencies.&lt;/p&gt;&lt;p&gt;One of the projects is the South Texas Direct Air Capture Hub, a facility that Occidental Petroleum’s 1PointFive subsidiary planned to develop in Kleberg County, Texas. The other is Project Cypress in Louisiana, a collaboration between Battelle, Climeworks, and Heirloom. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James Temple&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI toys are all the rage in China—and now they’re appearing on shelves in the US too&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Kids have always played with and talked to stuffed animals. But now their toys can talk back, thanks to a wave of companies that are fitting children’s playthings with chatbots and voice assistants.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;It’s a trend that has particularly taken off in China: A recent report by the Shenzhen Toy Industry Association and JD.com predicts that the sector will surpass ¥100 billion ($14 billion) by 2030, growing faster than almost any other branch of consumer AI. But Chinese AI toy companies have their sights set beyond the nation’s borders. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;2025 climate tech companies to watch: Pairwise and its climate-adapted crops&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Climate change will make it increasingly difficult to grow crops across many parts of the world. Startup Pairwise is using CRISPR gene editing to develop plants that can better withstand adverse conditions.&lt;/p&gt;  &lt;p&gt;The company uses cutting-edge gene editing to produce crops that can withstand increasingly harsh climate conditions, helping to feed a growing population even as the world warms. Last year, it delivered its first food to the US market: a less-bitter–tasting mustard green. It’s now working to produce crops with climate-resilient traits, through partnerships with two of the world’s largest plant biotech companies. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James Temple&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Pairwise is one of our 10 climate tech companies to watch—our annual list of some of the most promising climate tech firms on the planet. &lt;/strong&gt;&lt;strong&gt;Check out the rest of the list here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How to measure the returns on R&amp;amp;D spending&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Given the draconian cuts to US federal funding for science, it’s worth asking some hard-nosed money questions: How much should we be spending on R&amp;amp;D? How much value do we get out of such investments, anyway?&lt;/p&gt;  &lt;p&gt;To answer that, in several recent papers, economists have approached this issue in clever new ways.&amp;nbsp; And, though they ask slightly different questions, their conclusions share a bottom line: R&amp;amp;D is, in fact, one of the better long-term investments that the government can make.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;1 How OpenAI and Nvidia are fueling the AI bubble&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Experts fear their circular deals could be artificially inflating the market. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;OpenAI will pay for AMD’s chips using, err, AMD’s own stock. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;The Bank of England is concerned about AI inflating tech stocks. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;What comes next, that’s the big question. &lt;/em&gt;(NBC News)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Around 15% of the world’s working population is using AI&lt;/strong&gt;&lt;br /&gt;And countries in Europe are among the most enthusiastic adopters. (FT $)&lt;br /&gt;+ &lt;em&gt;The EU is keen to get even more of its citizens using it, too. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, America’s public opinion towards AI is souring. &lt;/em&gt;(WP $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;3 Three quantum mechanics scientists have won the Nobel Prize for Physics&lt;/strong&gt;&lt;br /&gt;Two of whom were instrumental in building Google’s working quantum machines. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Their work shone a light on behaviors of the subatomic realm. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;Quantum particles behave in notoriously strange ways. &lt;/em&gt;(New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 The CDC has finally signed off on covid vaccine recommendations&lt;/strong&gt;&lt;br /&gt;Despite the delay, access looks largely similar to last years’. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;The Supreme Court isn’t sold on medical expertise these days. &lt;/em&gt;(Vox)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;5 What makes TikTok so ‘sticky’&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Even its hardcore users can be persuaded to keep scrolling for hours. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 ICE bought fake cell towers to spy on nearby phones&lt;br /&gt;&lt;/strong&gt;It’s used cell-site simulators in the past to track down alleged criminals. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;Meet the volunteers tracking ICE officers in LA. &lt;/em&gt;(New Yorker $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Watermark removers for Sora 2 videos are already readily available&lt;br /&gt;&lt;/strong&gt;No permission? No problem. (404 Media)&lt;br /&gt;+ &lt;em&gt;What about copyright for AI-generated art? &lt;/em&gt;(The Information $)&lt;br /&gt;+ &lt;em&gt;And what comes next for AI copyright lawsuits? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 How diamonds can help to cool down chips&lt;br /&gt;&lt;/strong&gt;They’re remarkably good at transferring heat. (NYT $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Amazon Pharmacy is launching electronic prescription kiosks&lt;/strong&gt;&lt;br /&gt;For drugs including antibiotics, asthma inhalers and treatments for high blood pressure. (Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Should you limit your smartphone use to two hours a day?&lt;/strong&gt;&lt;br /&gt;Japan thinks so. (The Guardian)&lt;br /&gt;+ &lt;em&gt;How to log off. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“OpenAI is building the future of AI on infrastructure it doesn't own, power it doesn't control, and capital it doesn't have.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Andrey Sidorenko, head of research at data firm Mostly AI, critiques what he calls the consolidation of the AI ecosystem in a post on LinkedIn.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1125309" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image_d541b2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How AI can help make cities work better&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In recent decades, cities have become increasingly adept at amassing all sorts of data. But that data can have limited impact when government officials are unable to communicate, let alone analyze or put to use, all the information they have access to.&lt;/p&gt;&lt;p&gt;This dynamic has always bothered Sarah Williams, a professor of urban planning and technology at MIT. Shortly after joining MIT in 2012, Williams created the Civic Data Design Lab to bridge that divide. Over the years, she and her colleagues have made urban planning data more vivid and accessible through human stories and striking graphics. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Ben Schneider&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Life lessons from the one and only Ozzy Osbourne—what’s not to like?&lt;br /&gt;+ Did you know that most countries have their own camouflage? Check the patterns out here.&lt;br /&gt;+ These hamsters getting an MRI scan is the cutest thing you’ll see today.&lt;br /&gt;+ Pumpkin chili sounds like a fantastic way to warm up.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The Trump administration may cut funding for two major direct-air capture plants&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The US Department of Energy appears poised to terminate funding for a pair of large carbon-sucking factories that were originally set to receive more than $1 billion in government grants, according to a department-issued list of projects obtained by &lt;em&gt;MIT Technology Review&lt;/em&gt; and circulating among federal agencies.&lt;/p&gt;&lt;p&gt;One of the projects is the South Texas Direct Air Capture Hub, a facility that Occidental Petroleum’s 1PointFive subsidiary planned to develop in Kleberg County, Texas. The other is Project Cypress in Louisiana, a collaboration between Battelle, Climeworks, and Heirloom. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James Temple&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI toys are all the rage in China—and now they’re appearing on shelves in the US too&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Kids have always played with and talked to stuffed animals. But now their toys can talk back, thanks to a wave of companies that are fitting children’s playthings with chatbots and voice assistants.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;It’s a trend that has particularly taken off in China: A recent report by the Shenzhen Toy Industry Association and JD.com predicts that the sector will surpass ¥100 billion ($14 billion) by 2030, growing faster than almost any other branch of consumer AI. But Chinese AI toy companies have their sights set beyond the nation’s borders. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;2025 climate tech companies to watch: Pairwise and its climate-adapted crops&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Climate change will make it increasingly difficult to grow crops across many parts of the world. Startup Pairwise is using CRISPR gene editing to develop plants that can better withstand adverse conditions.&lt;/p&gt;  &lt;p&gt;The company uses cutting-edge gene editing to produce crops that can withstand increasingly harsh climate conditions, helping to feed a growing population even as the world warms. Last year, it delivered its first food to the US market: a less-bitter–tasting mustard green. It’s now working to produce crops with climate-resilient traits, through partnerships with two of the world’s largest plant biotech companies. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—James Temple&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Pairwise is one of our 10 climate tech companies to watch—our annual list of some of the most promising climate tech firms on the planet. &lt;/strong&gt;&lt;strong&gt;Check out the rest of the list here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 

   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;MIT Technology Review Narrated: How to measure the returns on R&amp;amp;D spending&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Given the draconian cuts to US federal funding for science, it’s worth asking some hard-nosed money questions: How much should we be spending on R&amp;amp;D? How much value do we get out of such investments, anyway?&lt;/p&gt;  &lt;p&gt;To answer that, in several recent papers, economists have approached this issue in clever new ways.&amp;nbsp; And, though they ask slightly different questions, their conclusions share a bottom line: R&amp;amp;D is, in fact, one of the better long-term investments that the government can make.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This is our latest story to be turned into a MIT Technology Review Narrated podcast, which we’re publishing each week on Spotify and Apple Podcasts. Just navigate to MIT Technology Review Narrated on either platform, and follow us to get all our new content as it’s released.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;1 How OpenAI and Nvidia are fueling the AI bubble&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Experts fear their circular deals could be artificially inflating the market. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;OpenAI will pay for AMD’s chips using, err, AMD’s own stock. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;The Bank of England is concerned about AI inflating tech stocks. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;What comes next, that’s the big question. &lt;/em&gt;(NBC News)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Around 15% of the world’s working population is using AI&lt;/strong&gt;&lt;br /&gt;And countries in Europe are among the most enthusiastic adopters. (FT $)&lt;br /&gt;+ &lt;em&gt;The EU is keen to get even more of its citizens using it, too. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;Meanwhile, America’s public opinion towards AI is souring. &lt;/em&gt;(WP $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;3 Three quantum mechanics scientists have won the Nobel Prize for Physics&lt;/strong&gt;&lt;br /&gt;Two of whom were instrumental in building Google’s working quantum machines. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Their work shone a light on behaviors of the subatomic realm. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;Quantum particles behave in notoriously strange ways. &lt;/em&gt;(New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 The CDC has finally signed off on covid vaccine recommendations&lt;/strong&gt;&lt;br /&gt;Despite the delay, access looks largely similar to last years’. (Ars Technica)&lt;br /&gt;+ &lt;em&gt;The Supreme Court isn’t sold on medical expertise these days. &lt;/em&gt;(Vox)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;5 What makes TikTok so ‘sticky’&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Even its hardcore users can be persuaded to keep scrolling for hours. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 ICE bought fake cell towers to spy on nearby phones&lt;br /&gt;&lt;/strong&gt;It’s used cell-site simulators in the past to track down alleged criminals. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;Meet the volunteers tracking ICE officers in LA. &lt;/em&gt;(New Yorker $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Watermark removers for Sora 2 videos are already readily available&lt;br /&gt;&lt;/strong&gt;No permission? No problem. (404 Media)&lt;br /&gt;+ &lt;em&gt;What about copyright for AI-generated art? &lt;/em&gt;(The Information $)&lt;br /&gt;+ &lt;em&gt;And what comes next for AI copyright lawsuits? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 How diamonds can help to cool down chips&lt;br /&gt;&lt;/strong&gt;They’re remarkably good at transferring heat. (NYT $)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Amazon Pharmacy is launching electronic prescription kiosks&lt;/strong&gt;&lt;br /&gt;For drugs including antibiotics, asthma inhalers and treatments for high blood pressure. (Reuters)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Should you limit your smartphone use to two hours a day?&lt;/strong&gt;&lt;br /&gt;Japan thinks so. (The Guardian)&lt;br /&gt;+ &lt;em&gt;How to log off. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“OpenAI is building the future of AI on infrastructure it doesn't own, power it doesn't control, and capital it doesn't have.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Andrey Sidorenko, head of research at data firm Mostly AI, critiques what he calls the consolidation of the AI ecosystem in a post on LinkedIn.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1125309" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image_d541b2.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How AI can help make cities work better&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In recent decades, cities have become increasingly adept at amassing all sorts of data. But that data can have limited impact when government officials are unable to communicate, let alone analyze or put to use, all the information they have access to.&lt;/p&gt;&lt;p&gt;This dynamic has always bothered Sarah Williams, a professor of urban planning and technology at MIT. Shortly after joining MIT in 2012, Williams created the Civic Data Design Lab to bridge that divide. Over the years, she and her colleagues have made urban planning data more vivid and accessible through human stories and striking graphics. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Ben Schneider&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Life lessons from the one and only Ozzy Osbourne—what’s not to like?&lt;br /&gt;+ Did you know that most countries have their own camouflage? Check the patterns out here.&lt;br /&gt;+ These hamsters getting an MRI scan is the cutest thing you’ll see today.&lt;br /&gt;+ Pumpkin chili sounds like a fantastic way to warm up.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/08/1125301/the-download-carbon-removal-factories-funding-cuts-and-ai-toys/</guid><pubDate>Wed, 08 Oct 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Google’s Search Live comes to India, AI Mode gets more languages (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/googles-search-live-comes-to-india-ai-mode-gets-more-languages/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/google-ai-mode-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is bringing its AI-powered conversational search feature, Search Live, to India — launching in English and Hindi — and expanding AI Mode to seven new Indian languages, as the company strengthens its presence in one of its fastest-growing markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in the U.S. in July, Search Live builds on Google’s Project Astra technology and is available through the company’s AI Mode. The feature lets users point their phone camera at objects to get real-time assistance, supporting back-and-forth conversations that draw on the visual context from the camera feed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With this launch, India becomes the second market after the U.S. to get Search Live — a logical move given the country’s vast base of early AI adopters, who have helped Google grow products like Gemini’s Nano Banana model. Google plans to leverage India’s early adoption to train its systems on a wider range of visual contexts, making Search Live more capable over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“People in India are power users of multimodal search, forming our largest user base for both voice and visual search globally,” said Hema Budaraju, vice president of product management for Search at Google, in a new blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Search Live is starting to roll out to users in India today and will reach more people over the coming weeks. Once available, users can access it by tapping the “Live” icon under the search bar in the Google app, or by opening Lens and selecting “Live” from the bottom of the screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Google revealed that Search Live is powered by a custom version of Gemini. The Gemini app separately includes a similarly named feature called Gemini Live, which was introduced in May and offers a comparable experience. This overlap could easily confuse some users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has also expanded its AI Mode to seven Indian languages: Bengali, Kannada, Malayalam, Marathi, Tamil, Telugu, and Urdu. This is part of a global AI Mode expansion, which will be available in more than 35 new languages and 40 new countries and territories, making the AI-powered search experience accessible in over 200 countries and territories worldwide.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s AI Mode, which debuted in the U.S. in March and rolled out to more U.S. users in May, lets users ask complex, multi-part questions through an AI-powered interface. It launched in India in June and expanded globally in August. Last month, Google added five new languages — including Hindi, Indonesian, and Japanese — to the feature.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The advanced reasoning and multimodal understanding of our custom Gemini model for Search allows AI Mode to truly grasp the subtleties of local languages, ensuring AI Mode is genuinely helpful and relevant in all the new languages we introduce,” Budaraju said in the post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s AI Mode and other AI features, including AI Overviews, have faced criticism for reducing search traffic to online publishers. The company, however, has denied that its AI-driven search tools are hurting website visits.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/google-ai-mode-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is bringing its AI-powered conversational search feature, Search Live, to India — launching in English and Hindi — and expanding AI Mode to seven new Indian languages, as the company strengthens its presence in one of its fastest-growing markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Introduced in the U.S. in July, Search Live builds on Google’s Project Astra technology and is available through the company’s AI Mode. The feature lets users point their phone camera at objects to get real-time assistance, supporting back-and-forth conversations that draw on the visual context from the camera feed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With this launch, India becomes the second market after the U.S. to get Search Live — a logical move given the country’s vast base of early AI adopters, who have helped Google grow products like Gemini’s Nano Banana model. Google plans to leverage India’s early adoption to train its systems on a wider range of visual contexts, making Search Live more capable over time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“People in India are power users of multimodal search, forming our largest user base for both voice and visual search globally,” said Hema Budaraju, vice president of product management for Search at Google, in a new blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Search Live is starting to roll out to users in India today and will reach more people over the coming weeks. Once available, users can access it by tapping the “Live” icon under the search bar in the Google app, or by opening Lens and selecting “Live” from the bottom of the screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, Google revealed that Search Live is powered by a custom version of Gemini. The Gemini app separately includes a similarly named feature called Gemini Live, which was introduced in May and offers a comparable experience. This overlap could easily confuse some users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has also expanded its AI Mode to seven Indian languages: Bengali, Kannada, Malayalam, Marathi, Tamil, Telugu, and Urdu. This is part of a global AI Mode expansion, which will be available in more than 35 new languages and 40 new countries and territories, making the AI-powered search experience accessible in over 200 countries and territories worldwide.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s AI Mode, which debuted in the U.S. in March and rolled out to more U.S. users in May, lets users ask complex, multi-part questions through an AI-powered interface. It launched in India in June and expanded globally in August. Last month, Google added five new languages — including Hindi, Indonesian, and Japanese — to the feature.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The advanced reasoning and multimodal understanding of our custom Gemini model for Search allows AI Mode to truly grasp the subtleties of local languages, ensuring AI Mode is genuinely helpful and relevant in all the new languages we introduce,” Budaraju said in the post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s AI Mode and other AI features, including AI Overviews, have faced criticism for reducing search traffic to online publishers. The company, however, has denied that its AI-driven search tools are hurting website visits.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/googles-search-live-comes-to-india-ai-mode-gets-more-languages/</guid><pubDate>Wed, 08 Oct 2025 12:50:57 +0000</pubDate></item><item><title>[NEW] Insurers balk at paying out huge settlements for claims against AI firms (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/insurers-balk-at-paying-out-huge-settlements-for-claims-against-ai-firms/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI, Anthropic consider using investor funds to settle potential lawsuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OpenAI logo with $100 bill" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/aimoney-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OpenAI logo with $100 bill" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/aimoney-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          FT montage/Dreamstime

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI and Anthropic are considering using investor funds to settle potential claims from multibillion-dollar lawsuits, as insurers balk at providing comprehensive coverage for the risks associated with artificial intelligence.&lt;/p&gt;
&lt;p&gt;The two US-based AI start-ups have traditional business insurance coverage in place, but insurance professionals said AI model providers will struggle to secure protection for the full scale of damages they may need to pay out in the future.&lt;/p&gt;
&lt;p&gt;OpenAI, which has tapped the world’s second-largest insurance broker Aon for help, has secured cover of up to $300 million for emerging AI risks, according to people familiar with the company’s policy.&lt;/p&gt;
&lt;p&gt;Another person familiar with the policy disputed that figure, saying it was much lower. But all agreed the amount fell far short of the coverage to insure against potential losses from a series of multibillion-dollar legal claims.&lt;/p&gt;
&lt;p&gt;Aon declined to comment on individual companies. But Kevin Kalinich, head of cyber risk at Aon, said of the insurance sector broadly, “we don’t yet have enough capacity for [model] providers.”&lt;/p&gt;
&lt;p&gt;He added of insurers, “what they can’t afford to pay is if an AI provider makes a mistake that ends up as... a systemic, correlated, aggregated risk.”&lt;/p&gt;
&lt;p&gt;The industry’s reticence to provide comprehensive cover for AI companies comes from the unprecedented scale of potential claims faced by relatively young tech companies. The risk is heightened as enormous damages known as “nuclear verdicts” against big US companies have also become more common.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;OpenAI is currently being sued for copyright infringement by The New York Times and authors who claim their content was used to train models without consent. It is also being sued for wrongful death by the parents of a 16-year-old who died by suicide after discussing methods with ChatGPT.&lt;/p&gt;
&lt;p&gt;Two people with knowledge of the matter said OpenAI has considered “self insurance,” or putting aside investor funding in order to expand its coverage. The company has raised nearly $60 billion to date, with a substantial amount of the funding contingent on a proposed corporate restructuring.&lt;/p&gt;
&lt;p&gt;One of those people said OpenAI had discussed setting up a “captive”—a ringfenced insurance vehicle often used by large companies to manage emerging risks. Big tech companies such as Microsoft, Meta, and Google have used captives to cover Internet-era liabilities such as cyber or social media.&lt;/p&gt;
&lt;p&gt;Captives can also carry risks, since a substantial claim can deplete an underfunded captive, leaving the parent company vulnerable.&lt;/p&gt;
&lt;p&gt;OpenAI said it has insurance in place and is evaluating different insurance structures as the company grows, but does not currently have a captive and declined to comment on future plans.&lt;/p&gt;
&lt;p&gt;Anthropic has agreed to pay $1.5 billion to settle a class-action lawsuit with authors over their alleged use of pirated books to train AI models.&lt;/p&gt;
&lt;p&gt;In court documents, Anthropic’s lawyers warned the suit carried the specter of “unprecedented and potentially business-threatening statutory damages against the smallest one of the many companies developing [AI] with the same books data.”&lt;/p&gt;
&lt;p&gt;Anthropic, which has raised more than $30 billion to date, is partly using its own funds for the settlement, according to one person with knowledge of the matter. Anthropic declined to comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI, Anthropic consider using investor funds to settle potential lawsuits.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OpenAI logo with $100 bill" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/aimoney-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OpenAI logo with $100 bill" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/aimoney-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          FT montage/Dreamstime

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI and Anthropic are considering using investor funds to settle potential claims from multibillion-dollar lawsuits, as insurers balk at providing comprehensive coverage for the risks associated with artificial intelligence.&lt;/p&gt;
&lt;p&gt;The two US-based AI start-ups have traditional business insurance coverage in place, but insurance professionals said AI model providers will struggle to secure protection for the full scale of damages they may need to pay out in the future.&lt;/p&gt;
&lt;p&gt;OpenAI, which has tapped the world’s second-largest insurance broker Aon for help, has secured cover of up to $300 million for emerging AI risks, according to people familiar with the company’s policy.&lt;/p&gt;
&lt;p&gt;Another person familiar with the policy disputed that figure, saying it was much lower. But all agreed the amount fell far short of the coverage to insure against potential losses from a series of multibillion-dollar legal claims.&lt;/p&gt;
&lt;p&gt;Aon declined to comment on individual companies. But Kevin Kalinich, head of cyber risk at Aon, said of the insurance sector broadly, “we don’t yet have enough capacity for [model] providers.”&lt;/p&gt;
&lt;p&gt;He added of insurers, “what they can’t afford to pay is if an AI provider makes a mistake that ends up as... a systemic, correlated, aggregated risk.”&lt;/p&gt;
&lt;p&gt;The industry’s reticence to provide comprehensive cover for AI companies comes from the unprecedented scale of potential claims faced by relatively young tech companies. The risk is heightened as enormous damages known as “nuclear verdicts” against big US companies have also become more common.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;OpenAI is currently being sued for copyright infringement by The New York Times and authors who claim their content was used to train models without consent. It is also being sued for wrongful death by the parents of a 16-year-old who died by suicide after discussing methods with ChatGPT.&lt;/p&gt;
&lt;p&gt;Two people with knowledge of the matter said OpenAI has considered “self insurance,” or putting aside investor funding in order to expand its coverage. The company has raised nearly $60 billion to date, with a substantial amount of the funding contingent on a proposed corporate restructuring.&lt;/p&gt;
&lt;p&gt;One of those people said OpenAI had discussed setting up a “captive”—a ringfenced insurance vehicle often used by large companies to manage emerging risks. Big tech companies such as Microsoft, Meta, and Google have used captives to cover Internet-era liabilities such as cyber or social media.&lt;/p&gt;
&lt;p&gt;Captives can also carry risks, since a substantial claim can deplete an underfunded captive, leaving the parent company vulnerable.&lt;/p&gt;
&lt;p&gt;OpenAI said it has insurance in place and is evaluating different insurance structures as the company grows, but does not currently have a captive and declined to comment on future plans.&lt;/p&gt;
&lt;p&gt;Anthropic has agreed to pay $1.5 billion to settle a class-action lawsuit with authors over their alleged use of pirated books to train AI models.&lt;/p&gt;
&lt;p&gt;In court documents, Anthropic’s lawyers warned the suit carried the specter of “unprecedented and potentially business-threatening statutory damages against the smallest one of the many companies developing [AI] with the same books data.”&lt;/p&gt;
&lt;p&gt;Anthropic, which has raised more than $30 billion to date, is partly using its own funds for the settlement, according to one person with knowledge of the matter. Anthropic declined to comment.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/insurers-balk-at-paying-out-huge-settlements-for-claims-against-ai-firms/</guid><pubDate>Wed, 08 Oct 2025 13:31:28 +0000</pubDate></item><item><title>[NEW] From courtside to code: Tristan Thompson on AI, sports, and startups at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/tristan-thompson-on-ai-sports-and-startups-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Former NBA champion, fintech entrepreneur, and AI founder Tristan Thompson is bringing a different kind of game to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, taking place October 27-29 at San Francisco’s Moscone West — one that blends athletic grit, entrepreneurial instinct, and cutting-edge AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After more than a decade lighting up the NBA, Thompson has shifted focus from the hardwood to the innovation arena. His latest venture, &lt;strong&gt;TracyAI&lt;/strong&gt;, applies advanced artificial intelligence to sports analytics — turning real-time data into predictive insights that empower athletes, teams, and fans alike.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this fireside chat where sports meets tech. &lt;strong&gt;Register now&lt;/strong&gt; to save up to $444 on your ticket or up to &lt;strong&gt;30% on group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Tristan Thompson" class="wp-image-3055324" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Tristan-Thompson-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-unlocking-the-next-era-of-sports-and-tech"&gt;Unlocking the next era of sports and tech&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At Disrupt, Thompson will unpack how he’s using AI to reimagine performance, strategy, and fan engagement, and why athletes are increasingly shaping the startup ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Thompson’s tech story doesn’t stop there. He currently holds four C-suite roles across the web3 and fintech landscape — including chief digital equity officer at World Mobile, driving affordable community-owned internet access, and chief advisory officer at AxonDAO, an AI-powered medical research platform inspired by his brother’s epilepsy journey. He’s also the co-founder of Basketball Fun, an immersive digital experience redefining how fans connect to the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an outspoken advocate for innovation and inclusion, Thompson is part of a new wave of athlete entrepreneurs using their platforms for impact. His projects have been featured by Forbes, CNBC Crypto World, and CoinDesk, and he recently took the stage at Bitcoin 2025 to discuss athlete-driven innovation in decentralized tech.&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;“Sports taught me discipline, but tech taught me scale. When you merge those worlds, the potential is unlimited.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 class="wp-block-heading" id="h-from-nba-courts-to-ai-innovation-catch-thompson-live-at-disrupt-2025"&gt;From NBA courts to AI innovation — catch Thompson live at Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Lean in on Tristan Thompson’s fireside chat at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, where he’ll explore how AI is transforming the future of sports, startups, and storytelling — and how athletes are becoming the next generation of founders. &lt;strong&gt;Grab your Disrupt 2025 pass&lt;/strong&gt; and join the conversation on the future of AI, venture, and innovation at one of the most anticipated tech events of the year. Bringing a group? &lt;strong&gt;Save up to 30% on bundle passes&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Former NBA champion, fintech entrepreneur, and AI founder Tristan Thompson is bringing a different kind of game to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, taking place October 27-29 at San Francisco’s Moscone West — one that blends athletic grit, entrepreneurial instinct, and cutting-edge AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After more than a decade lighting up the NBA, Thompson has shifted focus from the hardwood to the innovation arena. His latest venture, &lt;strong&gt;TracyAI&lt;/strong&gt;, applies advanced artificial intelligence to sports analytics — turning real-time data into predictive insights that empower athletes, teams, and fans alike.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this fireside chat where sports meets tech. &lt;strong&gt;Register now&lt;/strong&gt; to save up to $444 on your ticket or up to &lt;strong&gt;30% on group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Tristan Thompson" class="wp-image-3055324" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Tristan-Thompson-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-unlocking-the-next-era-of-sports-and-tech"&gt;Unlocking the next era of sports and tech&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At Disrupt, Thompson will unpack how he’s using AI to reimagine performance, strategy, and fan engagement, and why athletes are increasingly shaping the startup ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Thompson’s tech story doesn’t stop there. He currently holds four C-suite roles across the web3 and fintech landscape — including chief digital equity officer at World Mobile, driving affordable community-owned internet access, and chief advisory officer at AxonDAO, an AI-powered medical research platform inspired by his brother’s epilepsy journey. He’s also the co-founder of Basketball Fun, an immersive digital experience redefining how fans connect to the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an outspoken advocate for innovation and inclusion, Thompson is part of a new wave of athlete entrepreneurs using their platforms for impact. His projects have been featured by Forbes, CNBC Crypto World, and CoinDesk, and he recently took the stage at Bitcoin 2025 to discuss athlete-driven innovation in decentralized tech.&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;“Sports taught me discipline, but tech taught me scale. When you merge those worlds, the potential is unlimited.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 class="wp-block-heading" id="h-from-nba-courts-to-ai-innovation-catch-thompson-live-at-disrupt-2025"&gt;From NBA courts to AI innovation — catch Thompson live at Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Lean in on Tristan Thompson’s fireside chat at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, where he’ll explore how AI is transforming the future of sports, startups, and storytelling — and how athletes are becoming the next generation of founders. &lt;strong&gt;Grab your Disrupt 2025 pass&lt;/strong&gt; and join the conversation on the future of AI, venture, and innovation at one of the most anticipated tech events of the year. Bringing a group? &lt;strong&gt;Save up to 30% on bundle passes&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/tristan-thompson-on-ai-sports-and-startups-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 08 Oct 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Google launches extensions system for its command-line coding tool (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/google-launches-extensions-system-for-its-command-line-coding-tool/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-08-at-9.22.17-AM.jpg?resize=1200,578" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, Google officially launched a new feature for its command-line AI system, Gemini CLI, allowing outside companies to integrate directly into the AI product. Called Gemini CLI Extensions, the feature is launching with extensions from Figma, Stripe, and other companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes just two days after OpenAI’s launch of apps in ChatGPT, which also integrated third-party systems into an AI environment. But while app access to ChatGPT is tightly curated, Gemini CLI extensions can be published with no endorsement or participation from Google. Available extensions will be hosted in public repositories on GitHub and installed manually by developers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“That open ecosystem is vital to us,” Taylor Mullen, a senior staff engineer on the project, told TechCrunch. “Everything we’re doing is grounded in a fair ecosystem that anyone can participate in.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first available extension was to Google’s own Nanobanana image generator, which was posted to GitHub last week. Once installed, the extension allows users to generate images directly from the Gemini CLI terminal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Gemini CLI has grown to over one million users, Google says, with usage heavily skewed toward software developers. Notably, Gemini CLI is heavily used in the development and maintenance of its own codebase, closely overseen by product managers, as detailed in a recent TechCrunch interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview, Google’s senior director of product management for developer tools Ryan J. Salva told TechCrunch that the purpose of the new feature was to turn Gemini CLI into “an extensibility platform, a conduit to other tools and instructions that come from elsewhere in your tool chain.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-08-at-9.22.17-AM.jpg?resize=1200,578" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, Google officially launched a new feature for its command-line AI system, Gemini CLI, allowing outside companies to integrate directly into the AI product. Called Gemini CLI Extensions, the feature is launching with extensions from Figma, Stripe, and other companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes just two days after OpenAI’s launch of apps in ChatGPT, which also integrated third-party systems into an AI environment. But while app access to ChatGPT is tightly curated, Gemini CLI extensions can be published with no endorsement or participation from Google. Available extensions will be hosted in public repositories on GitHub and installed manually by developers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“That open ecosystem is vital to us,” Taylor Mullen, a senior staff engineer on the project, told TechCrunch. “Everything we’re doing is grounded in a fair ecosystem that anyone can participate in.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first available extension was to Google’s own Nanobanana image generator, which was posted to GitHub last week. Once installed, the extension allows users to generate images directly from the Gemini CLI terminal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Gemini CLI has grown to over one million users, Google says, with usage heavily skewed toward software developers. Notably, Gemini CLI is heavily used in the development and maintenance of its own codebase, closely overseen by product managers, as detailed in a recent TechCrunch interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview, Google’s senior director of product management for developer tools Ryan J. Salva told TechCrunch that the purpose of the new feature was to turn Gemini CLI into “an extensibility platform, a conduit to other tools and instructions that come from elsewhere in your tool chain.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/google-launches-extensions-system-for-its-command-line-coding-tool/</guid><pubDate>Wed, 08 Oct 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] SoftBank bulks up its robotics portfolio with ABB Group’s robotics unit (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/softbank-bulks-up-its-robotics-portfolio-with-abb-groups-robotics-unit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1980672274.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Japanese investing conglomerate SoftBank Group is buying a robotics company as the financial behemoth says physical AI is its next frontier.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank announced on Wednesday that it has acquired Zurich, Switzerland-based ABB Group’s robotics business unit for $5.375 billion. The deal is subject to regulatory approval; SoftBank predicts the deal will close in mid-to-late 2026 according to a press release. Sami Atiya, the head of the division, will;exit the company once the acquisition is complete, per ABB Group.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ABB’s robotics business&amp;nbsp;employs&amp;nbsp;roughly 7,000 people and&amp;nbsp;sells a variety of robots and equipment, designed for tasks like picking,&amp;nbsp;cleaning,&amp;nbsp;and painting.&amp;nbsp;The company&amp;nbsp;made&amp;nbsp;$2.3 billion&amp;nbsp;in revenue in 2024, representing 7% of ABB’s&amp;nbsp;overall revenue.&amp;nbsp;ABB&amp;nbsp;announced plans to&amp;nbsp;spin out its robotics group in&amp;nbsp;April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank said it hopes to be able to “reignite” sales at ABB’s robotics spinoff. The organization’s 2024 revenue was only $2.3 billion, down from $2.5 billion the previous year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank has been building up its investments and holdings in robotics over the last few years. The entity has invested in companies including more legacy players like AutoStore and startups including Skild AI and Agile Robots.&amp;nbsp; &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm also launched its own robotics platform, SoftBank Robotics Group, in 2014.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“SoftBank’s next frontier is physical AI,” Masayoshi Son, the chairman and CEO of SoftBank said in the company’s press release. “Together with ABB Robotics, we will unite world-class technology and talent under our shared vision to fuse Artificial Super Intelligence and robotics — driving a groundbreaking evolution that will propel humanity forward.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics is one of four focus areas for SoftBank, together with&amp;nbsp;AI chips, AI data&amp;nbsp;centers,&amp;nbsp;and energy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The center of the ‘Information Revolution’ has evolved from personal computers, the internet, and broadband to smartphones, and has now entered a new phase led by artificial intelligence,” SoftBank said in its press release. “In this context, [SoftBank Group] has declared its mission to realize artificial super intelligence (ASI) for the advancement of humanity.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to SoftBank for more information.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1980672274.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Japanese investing conglomerate SoftBank Group is buying a robotics company as the financial behemoth says physical AI is its next frontier.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank announced on Wednesday that it has acquired Zurich, Switzerland-based ABB Group’s robotics business unit for $5.375 billion. The deal is subject to regulatory approval; SoftBank predicts the deal will close in mid-to-late 2026 according to a press release. Sami Atiya, the head of the division, will;exit the company once the acquisition is complete, per ABB Group.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ABB’s robotics business&amp;nbsp;employs&amp;nbsp;roughly 7,000 people and&amp;nbsp;sells a variety of robots and equipment, designed for tasks like picking,&amp;nbsp;cleaning,&amp;nbsp;and painting.&amp;nbsp;The company&amp;nbsp;made&amp;nbsp;$2.3 billion&amp;nbsp;in revenue in 2024, representing 7% of ABB’s&amp;nbsp;overall revenue.&amp;nbsp;ABB&amp;nbsp;announced plans to&amp;nbsp;spin out its robotics group in&amp;nbsp;April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank said it hopes to be able to “reignite” sales at ABB’s robotics spinoff. The organization’s 2024 revenue was only $2.3 billion, down from $2.5 billion the previous year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank has been building up its investments and holdings in robotics over the last few years. The entity has invested in companies including more legacy players like AutoStore and startups including Skild AI and Agile Robots.&amp;nbsp; &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm also launched its own robotics platform, SoftBank Robotics Group, in 2014.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“SoftBank’s next frontier is physical AI,” Masayoshi Son, the chairman and CEO of SoftBank said in the company’s press release. “Together with ABB Robotics, we will unite world-class technology and talent under our shared vision to fuse Artificial Super Intelligence and robotics — driving a groundbreaking evolution that will propel humanity forward.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics is one of four focus areas for SoftBank, together with&amp;nbsp;AI chips, AI data&amp;nbsp;centers,&amp;nbsp;and energy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The center of the ‘Information Revolution’ has evolved from personal computers, the internet, and broadband to smartphones, and has now entered a new phase led by artificial intelligence,” SoftBank said in its press release. “In this context, [SoftBank Group] has declared its mission to realize artificial super intelligence (ASI) for the advancement of humanity.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to SoftBank for more information.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/softbank-bulks-up-its-robotics-portfolio-with-abb-groups-robotics-unit/</guid><pubDate>Wed, 08 Oct 2025 14:10:18 +0000</pubDate></item><item><title>[NEW] Unveiling the next wave of Startup Battlefield 200 VC judges at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/unveiling-the-next-wave-of-startup-battlefield-200-vc-judges-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;&amp;nbsp;at&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;&amp;nbsp;is&amp;nbsp;almost here, and the pressure has never been greater. With&amp;nbsp;&lt;strong&gt;$100,000&lt;/strong&gt;&amp;nbsp;on the line, the&amp;nbsp;top 20 early-stage startups&amp;nbsp;know they must deliver their&amp;nbsp;very best&amp;nbsp;on the&amp;nbsp;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;from&amp;nbsp;October 27-29 at San Francisco’s Moscone West. Meanwhile, our VC judges will bring sharp questions, deep experience, and a discerning eye to distinguish the promising from the truly exceptional.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founders&amp;nbsp;can gain invaluable insights into what makes a winning pitch and a sustainable startup from these seasoned VCs.&amp;nbsp;Investors&amp;nbsp;can discover pitch-ready, impactful startups to add to their pipeline. Everyone else can&amp;nbsp;witness&amp;nbsp;the intense startup showdown, where the top 20 TechCrunch-vetted startups pitch for the equity-free prize and the coveted Disrupt Cup.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to watch and save up to $444 on your&amp;nbsp;pass, or&amp;nbsp;bring your team or friends and&amp;nbsp;&lt;strong&gt;save up to 30% with bundle passes&lt;/strong&gt;.&amp;nbsp;Don’t&amp;nbsp;miss this nail-biting “World Series” of pitch competitions live.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-next-batch-of-nbsp-battlefield-nbsp-200-judges"&gt;Meet the next batch of&amp;nbsp;Battlefield&amp;nbsp;200 judges&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Today,&amp;nbsp;we’re&amp;nbsp;excited to introduce the&amp;nbsp;fourth wave of judges&amp;nbsp;joining the competition. Only one more reveal&amp;nbsp;remains. Stay tuned&amp;nbsp;to the&amp;nbsp;&lt;strong&gt;Disrupt agenda&lt;/strong&gt;&amp;nbsp;to see&amp;nbsp;who the last five judges will&amp;nbsp;be.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Allison Baum Gates, Sara Ittelson, Miloni Madan Presler, Katelin Holloway, Rinki Sethi" class="wp-image-3055320" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Presler-Gates-Holloway-Ittelson-Sethi_5-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-allison-baum-gates-general-partner-nbsp-sempervirens-nbsp-venture-capital-nbsp"&gt;Allison Baum Gates, General Partner,&amp;nbsp;SemperVirens&amp;nbsp;Venture Capital&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Allison Baum Gates&amp;nbsp;is a general partner at&amp;nbsp;SemperVirens, a venture fund investing in healthcare, fintech, and enterprise SaaS. To date, she has raised hundreds of millions of dollars from LPs in the U.S., Europe, and Asia, and has deployed capital into more than 70 companies, including nine unicorns and 24 exits, and she holds 10 board positions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gates has been featured in major publications such as the Financial Times, Wall Street Journal, Forbes, and TechCrunch, and has taught hundreds of business school students. She is a lecturer in management at Stanford GSB and the author of&amp;nbsp;“Breaking into Venture”&amp;nbsp;(McGraw-Hill, 2023), a comprehensive guide for navigating the VC industry. Her second book,&amp;nbsp;“Beyond the Pitch: The Psychology of Raising VC Funding,” will be released next year. She holds a B.A. in Economics with Honors from Harvard University and is proficient in three languages.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-katelin-holloway-founding-partner-sevensevensix"&gt;Katelin Holloway,&amp;nbsp;Founding Partner, SevenSevenSix&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Katelin Holloway&amp;nbsp;is a founding partner at Alexis Ohanian’s early-stage venture capital firm,&amp;nbsp;SevenSevenSix. A seasoned investor with more than 20 years of operational experience at companies like Pixar and Reddit, Holloway backs transformative startups across diverse sectors. She invests in solutions that expand human potential and resilience, advancing health, exploring new frontiers, driving sustainability, fostering creativity, and enriching the human experience.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At SevenSevenSix, Holloway combines her operational&amp;nbsp;expertise&amp;nbsp;with a deep commitment to supporting founders as they navigate early-stage challenges and scale their visions. She is dedicated to reshaping venture capital by empowering generational entrepreneurs to tackle humanity’s biggest challenges while delivering exceptional returns.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="(L-R) Amanda Silberling, Senior Writer, Digital Culture, TechCrunch, Josh Fabian, Co-founder &amp;amp; CEO, Metafy and Katelin Holloway, Founding Partner, Seven Seven Six speak onstage during TechCrunch Disrupt 2022 on October 20, 2022 in San Francisco, California." class="wp-image-2434494" height="453" src="https://techcrunch.com/wp-content/uploads/2022/11/Amanda-Silberling-Senior-Writer-Digital-Culture-TechCrunch-Josh-Fabian-Co-founder-CEO-Metafy-and-Katelin-Holloway-Founding-Partner-Seven-Seven-Six-speak-onstage-during-TechCrunch-Disrupt-2022-on-October-20-202.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kelly Sullivan / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-miloni-madan-presler-partner-nbsp-institutional-venture-partners-ivp-nbsp"&gt;Miloni Madan Presler, Partner,&amp;nbsp;Institutional Venture Partners (IVP)&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Miloni Madan Presler&amp;nbsp;is passionate about partnering with businesses that innovate with purpose — companies that solve real problems rather than innovate for innovation’s sake. As a general partner at&amp;nbsp;Institutional Venture Partners (IVP), she focuses on supporting founders who build products and solutions that people truly need, not just want. She seeks out companies that bridge gaps in fragmented ecosystems, automate outdated processes, and deliver data-driven platforms capable of transforming industries. Madan&amp;nbsp;is driven by the belief that these companies — and their impact — will endure long beyond any single investment, and&amp;nbsp;it’s&amp;nbsp;this pursuit of lasting change that fuels her commitment and intentionality in every partnership.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-sara-nbsp-ittelson-partner-accel-nbsp"&gt;Sara&amp;nbsp;Ittelson, Partner, Accel&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Sara&amp;nbsp;Ittelson&amp;nbsp;joined&amp;nbsp;Accel&amp;nbsp;in 2022 and focuses on early-stage consumer, enterprise, and AI companies.&amp;nbsp;Before&amp;nbsp;Accel, she was&amp;nbsp;head of&amp;nbsp;strategic&amp;nbsp;partnerships at Faire and previously worked in Global Business Development at Uber.&amp;nbsp;Ittelson&amp;nbsp;is from Chico, California, and graduated from Northwestern University and Stanford.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - SEPTEMBER 19: (L-R) Accel Partner Sara Ittelson, Kindred Ventures Founder &amp;amp; Managing Partner Steve Jang, and Breakthrough Energy Ventures Partner Libby Wayman speak onstage during TechCrunch Disrupt 2023 at Moscone Center on September 19, 2023 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2602892" height="383" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1691159208.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-rinki-sethi-founding-partner-lockstep-nbsp"&gt;Rinki Sethi, Founding Partner, Lockstep&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Rinki Sethi&amp;nbsp;is&amp;nbsp;chief&amp;nbsp;security&amp;nbsp;officer at&amp;nbsp;Upwind Security&amp;nbsp;and&amp;nbsp;founding&amp;nbsp;partner at&amp;nbsp;Lockstep. She previously served as&amp;nbsp;vice&amp;nbsp;president and&amp;nbsp;chief&amp;nbsp;information&amp;nbsp;security&amp;nbsp;officer at Twitter, BILL, and Rubrik and held senior security leadership roles&amp;nbsp;at&amp;nbsp;Palo Alto Networks, IBM,&amp;nbsp;eBay,&amp;nbsp;and Intuit. Sethi brings deep&amp;nbsp;expertise&amp;nbsp;to both the private sector and corporate boards, shaping security strategy at the highest levels.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-now-s-the-time-to-save-on-your-disrupt-pass-nbsp"&gt;Now’s the time to save on your Disrupt pass&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Don’t&amp;nbsp;miss the startup pitch competition of the year, live. With&amp;nbsp;electrifying&amp;nbsp;energy, high stakes, and sharp questions, Startup Battlefield 200 has long been the launchpad for early-stage startups. Brands like Discord, Trello, and Mint got their big break on the Disrupt Stage.&amp;nbsp;&lt;strong&gt;Register here&lt;/strong&gt;&amp;nbsp;to join before ticket&amp;nbsp;prices&amp;nbsp;increase.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Startup Battlefield 200 2023" class="wp-image-3013876" height="453" src="https://techcrunch.com/wp-content/uploads/2025/06/Startup-Battlefield-2023-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;&amp;nbsp;at&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;&amp;nbsp;is&amp;nbsp;almost here, and the pressure has never been greater. With&amp;nbsp;&lt;strong&gt;$100,000&lt;/strong&gt;&amp;nbsp;on the line, the&amp;nbsp;top 20 early-stage startups&amp;nbsp;know they must deliver their&amp;nbsp;very best&amp;nbsp;on the&amp;nbsp;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;from&amp;nbsp;October 27-29 at San Francisco’s Moscone West. Meanwhile, our VC judges will bring sharp questions, deep experience, and a discerning eye to distinguish the promising from the truly exceptional.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founders&amp;nbsp;can gain invaluable insights into what makes a winning pitch and a sustainable startup from these seasoned VCs.&amp;nbsp;Investors&amp;nbsp;can discover pitch-ready, impactful startups to add to their pipeline. Everyone else can&amp;nbsp;witness&amp;nbsp;the intense startup showdown, where the top 20 TechCrunch-vetted startups pitch for the equity-free prize and the coveted Disrupt Cup.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to watch and save up to $444 on your&amp;nbsp;pass, or&amp;nbsp;bring your team or friends and&amp;nbsp;&lt;strong&gt;save up to 30% with bundle passes&lt;/strong&gt;.&amp;nbsp;Don’t&amp;nbsp;miss this nail-biting “World Series” of pitch competitions live.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-next-batch-of-nbsp-battlefield-nbsp-200-judges"&gt;Meet the next batch of&amp;nbsp;Battlefield&amp;nbsp;200 judges&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Today,&amp;nbsp;we’re&amp;nbsp;excited to introduce the&amp;nbsp;fourth wave of judges&amp;nbsp;joining the competition. Only one more reveal&amp;nbsp;remains. Stay tuned&amp;nbsp;to the&amp;nbsp;&lt;strong&gt;Disrupt agenda&lt;/strong&gt;&amp;nbsp;to see&amp;nbsp;who the last five judges will&amp;nbsp;be.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Allison Baum Gates, Sara Ittelson, Miloni Madan Presler, Katelin Holloway, Rinki Sethi" class="wp-image-3055320" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Presler-Gates-Holloway-Ittelson-Sethi_5-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-allison-baum-gates-general-partner-nbsp-sempervirens-nbsp-venture-capital-nbsp"&gt;Allison Baum Gates, General Partner,&amp;nbsp;SemperVirens&amp;nbsp;Venture Capital&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Allison Baum Gates&amp;nbsp;is a general partner at&amp;nbsp;SemperVirens, a venture fund investing in healthcare, fintech, and enterprise SaaS. To date, she has raised hundreds of millions of dollars from LPs in the U.S., Europe, and Asia, and has deployed capital into more than 70 companies, including nine unicorns and 24 exits, and she holds 10 board positions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gates has been featured in major publications such as the Financial Times, Wall Street Journal, Forbes, and TechCrunch, and has taught hundreds of business school students. She is a lecturer in management at Stanford GSB and the author of&amp;nbsp;“Breaking into Venture”&amp;nbsp;(McGraw-Hill, 2023), a comprehensive guide for navigating the VC industry. Her second book,&amp;nbsp;“Beyond the Pitch: The Psychology of Raising VC Funding,” will be released next year. She holds a B.A. in Economics with Honors from Harvard University and is proficient in three languages.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-katelin-holloway-founding-partner-sevensevensix"&gt;Katelin Holloway,&amp;nbsp;Founding Partner, SevenSevenSix&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Katelin Holloway&amp;nbsp;is a founding partner at Alexis Ohanian’s early-stage venture capital firm,&amp;nbsp;SevenSevenSix. A seasoned investor with more than 20 years of operational experience at companies like Pixar and Reddit, Holloway backs transformative startups across diverse sectors. She invests in solutions that expand human potential and resilience, advancing health, exploring new frontiers, driving sustainability, fostering creativity, and enriching the human experience.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At SevenSevenSix, Holloway combines her operational&amp;nbsp;expertise&amp;nbsp;with a deep commitment to supporting founders as they navigate early-stage challenges and scale their visions. She is dedicated to reshaping venture capital by empowering generational entrepreneurs to tackle humanity’s biggest challenges while delivering exceptional returns.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="(L-R) Amanda Silberling, Senior Writer, Digital Culture, TechCrunch, Josh Fabian, Co-founder &amp;amp; CEO, Metafy and Katelin Holloway, Founding Partner, Seven Seven Six speak onstage during TechCrunch Disrupt 2022 on October 20, 2022 in San Francisco, California." class="wp-image-2434494" height="453" src="https://techcrunch.com/wp-content/uploads/2022/11/Amanda-Silberling-Senior-Writer-Digital-Culture-TechCrunch-Josh-Fabian-Co-founder-CEO-Metafy-and-Katelin-Holloway-Founding-Partner-Seven-Seven-Six-speak-onstage-during-TechCrunch-Disrupt-2022-on-October-20-202.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kelly Sullivan / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-miloni-madan-presler-partner-nbsp-institutional-venture-partners-ivp-nbsp"&gt;Miloni Madan Presler, Partner,&amp;nbsp;Institutional Venture Partners (IVP)&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Miloni Madan Presler&amp;nbsp;is passionate about partnering with businesses that innovate with purpose — companies that solve real problems rather than innovate for innovation’s sake. As a general partner at&amp;nbsp;Institutional Venture Partners (IVP), she focuses on supporting founders who build products and solutions that people truly need, not just want. She seeks out companies that bridge gaps in fragmented ecosystems, automate outdated processes, and deliver data-driven platforms capable of transforming industries. Madan&amp;nbsp;is driven by the belief that these companies — and their impact — will endure long beyond any single investment, and&amp;nbsp;it’s&amp;nbsp;this pursuit of lasting change that fuels her commitment and intentionality in every partnership.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-sara-nbsp-ittelson-partner-accel-nbsp"&gt;Sara&amp;nbsp;Ittelson, Partner, Accel&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Sara&amp;nbsp;Ittelson&amp;nbsp;joined&amp;nbsp;Accel&amp;nbsp;in 2022 and focuses on early-stage consumer, enterprise, and AI companies.&amp;nbsp;Before&amp;nbsp;Accel, she was&amp;nbsp;head of&amp;nbsp;strategic&amp;nbsp;partnerships at Faire and previously worked in Global Business Development at Uber.&amp;nbsp;Ittelson&amp;nbsp;is from Chico, California, and graduated from Northwestern University and Stanford.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - SEPTEMBER 19: (L-R) Accel Partner Sara Ittelson, Kindred Ventures Founder &amp;amp; Managing Partner Steve Jang, and Breakthrough Energy Ventures Partner Libby Wayman speak onstage during TechCrunch Disrupt 2023 at Moscone Center on September 19, 2023 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2602892" height="383" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1691159208.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-rinki-sethi-founding-partner-lockstep-nbsp"&gt;Rinki Sethi, Founding Partner, Lockstep&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Rinki Sethi&amp;nbsp;is&amp;nbsp;chief&amp;nbsp;security&amp;nbsp;officer at&amp;nbsp;Upwind Security&amp;nbsp;and&amp;nbsp;founding&amp;nbsp;partner at&amp;nbsp;Lockstep. She previously served as&amp;nbsp;vice&amp;nbsp;president and&amp;nbsp;chief&amp;nbsp;information&amp;nbsp;security&amp;nbsp;officer at Twitter, BILL, and Rubrik and held senior security leadership roles&amp;nbsp;at&amp;nbsp;Palo Alto Networks, IBM,&amp;nbsp;eBay,&amp;nbsp;and Intuit. Sethi brings deep&amp;nbsp;expertise&amp;nbsp;to both the private sector and corporate boards, shaping security strategy at the highest levels.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-now-s-the-time-to-save-on-your-disrupt-pass-nbsp"&gt;Now’s the time to save on your Disrupt pass&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Don’t&amp;nbsp;miss the startup pitch competition of the year, live. With&amp;nbsp;electrifying&amp;nbsp;energy, high stakes, and sharp questions, Startup Battlefield 200 has long been the launchpad for early-stage startups. Brands like Discord, Trello, and Mint got their big break on the Disrupt Stage.&amp;nbsp;&lt;strong&gt;Register here&lt;/strong&gt;&amp;nbsp;to join before ticket&amp;nbsp;prices&amp;nbsp;increase.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Startup Battlefield 200 2023" class="wp-image-3013876" height="453" src="https://techcrunch.com/wp-content/uploads/2025/06/Startup-Battlefield-2023-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/unveiling-the-next-wave-of-startup-battlefield-200-vc-judges-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 08 Oct 2025 14:30:00 +0000</pubDate></item><item><title>[NEW] Google’s virtual try-on shopping tool expands to more countries, now lets you try on shoes (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/googles-virtual-try-on-shopping-tool-expands-to-more-countries-now-lets-you-try-on-shoes/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that its AI feature that lets users virtually try on clothes is expanding to Australia, Canada, and Japan. The tech giant also announced the feature now lets users virtually try on shoes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works by asking users to upload a photo to see how real clothes might look on them. Now, users can visualize how different pairs of shoes would look on them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To virtually try on a pair of shoes, users need to tap on any product listing on Google, select the “Try It On” button, then add a full-length photo of themselves. After a few seconds, they will see the shoes from the listing on a digital version of themselves. Users have the option to save or share the image with others.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055414" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-08-at-10.39.44AM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch comes two months after Google introduced the ability for users to virtually try on clothes using AI. While Google had already offered&amp;nbsp;virtual try-on technology before, the earlier features focused on showing items on a diverse range of models’ bodies. With the new AI feature, the company started allowing users to try clothes on a virtual version of their own body.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been investing in the virtual try-on space in other ways as well. In June, the tech giant launched an&amp;nbsp;experimental app called Doppl&amp;nbsp;that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While both the virtual try-on feature, which now includes shoes, and Doppl are powered by the same generative AI technology, Doppl is designed to let shoppers dive even deeper into virtual try-on, helping them curate their personal style. Plus, Doppl can create AI-generated videos so users can get a better sense of how the outfit would look on them in real life.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google isn’t the only company to launch virtual try-on technology, as both Amazon and Walmart have introduced similar features.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that its AI feature that lets users virtually try on clothes is expanding to Australia, Canada, and Japan. The tech giant also announced the feature now lets users virtually try on shoes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works by asking users to upload a photo to see how real clothes might look on them. Now, users can visualize how different pairs of shoes would look on them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To virtually try on a pair of shoes, users need to tap on any product listing on Google, select the “Try It On” button, then add a full-length photo of themselves. After a few seconds, they will see the shoes from the listing on a digital version of themselves. Users have the option to save or share the image with others.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055414" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-08-at-10.39.44AM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch comes two months after Google introduced the ability for users to virtually try on clothes using AI. While Google had already offered&amp;nbsp;virtual try-on technology before, the earlier features focused on showing items on a diverse range of models’ bodies. With the new AI feature, the company started allowing users to try clothes on a virtual version of their own body.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been investing in the virtual try-on space in other ways as well. In June, the tech giant launched an&amp;nbsp;experimental app called Doppl&amp;nbsp;that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While both the virtual try-on feature, which now includes shoes, and Doppl are powered by the same generative AI technology, Doppl is designed to let shoppers dive even deeper into virtual try-on, helping them curate their personal style. Plus, Doppl can create AI-generated videos so users can get a better sense of how the outfit would look on them in real life.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google isn’t the only company to launch virtual try-on technology, as both Amazon and Walmart have introduced similar features.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/googles-virtual-try-on-shopping-tool-expands-to-more-countries-now-lets-you-try-on-shoes/</guid><pubDate>Wed, 08 Oct 2025 14:49:01 +0000</pubDate></item><item><title>[NEW] Ganiga will showcase its waste-sorting robots at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/ganiga-will-showcase-its-waste-sorting-robots-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Despite the well-known&amp;nbsp;environmental&amp;nbsp;benefits of recycling,&amp;nbsp;it’s&amp;nbsp;estimated that&amp;nbsp;less than 10% of the world’s plastic&amp;nbsp;gets recycled.&amp;nbsp;Ganiga Innovation&amp;nbsp;looks to bring that&amp;nbsp;percentage&amp;nbsp;up using&amp;nbsp;AI-enabled&amp;nbsp;robotic waste bins.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Italian startup&amp;nbsp;Ganiga&amp;nbsp;built&amp;nbsp;three products to help better manage waste and recycling. The first is&amp;nbsp;a fleet&amp;nbsp;of robotic waste bins,&amp;nbsp;called&amp;nbsp;Hoooly,&amp;nbsp;that&amp;nbsp;use generative AI to&amp;nbsp;determine&amp;nbsp;what is trash and what is&amp;nbsp;recycling&amp;nbsp;and sort the waste accordingly.&amp;nbsp;The second is a smart lid that can be fitted to existing waste bins&amp;nbsp;with the same functionality as&amp;nbsp;its&amp;nbsp;larger bin counterpart.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company&amp;nbsp;also has a&amp;nbsp;software product that allows companies to track the waste they produce; it offers&amp;nbsp;suggestions for how&amp;nbsp;a company can reduce waste production based on its waste data.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga&amp;nbsp;will be showing off its tech as part of this year’s&amp;nbsp;Startup Battlefield&amp;nbsp;competition at&amp;nbsp;TechCrunch Disrupt&amp;nbsp;2025,&amp;nbsp;which&amp;nbsp;runs from&amp;nbsp;October 27 through 30&amp;nbsp;in San Francisco.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nicolas&amp;nbsp;Zeoli,&amp;nbsp;the&amp;nbsp;founder and CEO of&amp;nbsp;Ganiga, told TechCrunch that&amp;nbsp;he’s&amp;nbsp;had dreams of building the next great&amp;nbsp;company, like Facebook or Apple, since he was younger.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He decided to focus on waste&amp;nbsp;because he said&amp;nbsp;issues surrounding waste management are very tangible in his native Italy — and it was clear there&amp;nbsp;wasn’t&amp;nbsp;much being done about it.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We all need to reclaim this problem,”&amp;nbsp;Zeoli&amp;nbsp;said. “I read 100 articles on this problem. For example, in one year, only in one year, in the&amp;nbsp;whole world&amp;nbsp;over 100 million tons of plastic is created and only 9% is recycled. This is a very&amp;nbsp;real&amp;nbsp;problem.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Zeoli&amp;nbsp;launched&amp;nbsp;Ganiga&amp;nbsp;in 2021&amp;nbsp;and&amp;nbsp;built its first prototype in 2022. Zeoli&amp;nbsp;said they decided to focus on building a bin to solve this problem because it not only gives&amp;nbsp;&amp;nbsp;people&amp;nbsp;a&amp;nbsp;physical&amp;nbsp;place to put waste that can ensure it gets properly recycled and sorted, but also because the bins spit out data that can be used for the future.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Waste management is also expensive for companies, Zeoli said. Many organizations, especially in Europe, have ESG mandates to adhere&amp;nbsp;to. Zeoli hopes&amp;nbsp;Hoooly&amp;nbsp;can help companies better track their waste production to&amp;nbsp;help them reduce waste and&amp;nbsp;waste-related&amp;nbsp;costs down the line.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga&amp;nbsp;started selling its&amp;nbsp;bins in 2024 and has since sold more than 120 robots to&amp;nbsp;customers&amp;nbsp;like&amp;nbsp;Google&amp;nbsp;and&amp;nbsp;multiple airports including the ones in&amp;nbsp;Bologna, Venice, and Madrid, among others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zeoli&amp;nbsp;said the company made $500,000 in revenue in 2024 and is already up to $750,000 in just the first nine months of 2025.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also raised $1.5 million in pre-seed funding from investors including clean tech VC firm&amp;nbsp;NextSTEP&amp;nbsp;and&amp;nbsp;Next Energy Capital, among others.&amp;nbsp;Ganiga&amp;nbsp;is looking to raise a $3 million seed round.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is gearing up to launch its latest product in November,&amp;nbsp;the&amp;nbsp;Hooolyfood,&amp;nbsp;which&amp;nbsp;is a software product that uses camera images to determine&amp;nbsp;the exact amount of food waste.&amp;nbsp;The company plans to delve into further software-focused products&amp;nbsp;in&amp;nbsp;the future, too, Zeoli said, based on the data their current bins and software is collecting.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga&amp;nbsp;has focused on the European market thus&amp;nbsp;far,&amp;nbsp;but Zeoli&amp;nbsp;said he’s hoping to expand into the U.S.; the company is even thinking of moving its headquarters stateside in 2026.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ganiga&amp;nbsp;is the first startup in all&amp;nbsp;the&amp;nbsp;world&amp;nbsp;to fill one airport&amp;nbsp;with&amp;nbsp;the smart bins,”&amp;nbsp;Zeoli said. “This is important because we don’t target the prototype,&amp;nbsp;we are a product, and we&amp;nbsp;are&amp;nbsp;open&amp;nbsp;to&amp;nbsp;the&amp;nbsp;market.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;If you want to learn from Ganiga firsthand, and see dozens of&amp;nbsp;additional&amp;nbsp;pitches, valuable workshops and make the connections that drive business results,&amp;nbsp;&lt;/em&gt;&lt;em&gt;head here to learn more about this year’s Disrupt&lt;/em&gt;&lt;em&gt;, held October 27 to 29 in San Francisco.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025" class="wp-image-3048094" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Disrupt_General_Article_No-Anniversary_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Despite the well-known&amp;nbsp;environmental&amp;nbsp;benefits of recycling,&amp;nbsp;it’s&amp;nbsp;estimated that&amp;nbsp;less than 10% of the world’s plastic&amp;nbsp;gets recycled.&amp;nbsp;Ganiga Innovation&amp;nbsp;looks to bring that&amp;nbsp;percentage&amp;nbsp;up using&amp;nbsp;AI-enabled&amp;nbsp;robotic waste bins.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Italian startup&amp;nbsp;Ganiga&amp;nbsp;built&amp;nbsp;three products to help better manage waste and recycling. The first is&amp;nbsp;a fleet&amp;nbsp;of robotic waste bins,&amp;nbsp;called&amp;nbsp;Hoooly,&amp;nbsp;that&amp;nbsp;use generative AI to&amp;nbsp;determine&amp;nbsp;what is trash and what is&amp;nbsp;recycling&amp;nbsp;and sort the waste accordingly.&amp;nbsp;The second is a smart lid that can be fitted to existing waste bins&amp;nbsp;with the same functionality as&amp;nbsp;its&amp;nbsp;larger bin counterpart.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company&amp;nbsp;also has a&amp;nbsp;software product that allows companies to track the waste they produce; it offers&amp;nbsp;suggestions for how&amp;nbsp;a company can reduce waste production based on its waste data.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga&amp;nbsp;will be showing off its tech as part of this year’s&amp;nbsp;Startup Battlefield&amp;nbsp;competition at&amp;nbsp;TechCrunch Disrupt&amp;nbsp;2025,&amp;nbsp;which&amp;nbsp;runs from&amp;nbsp;October 27 through 30&amp;nbsp;in San Francisco.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nicolas&amp;nbsp;Zeoli,&amp;nbsp;the&amp;nbsp;founder and CEO of&amp;nbsp;Ganiga, told TechCrunch that&amp;nbsp;he’s&amp;nbsp;had dreams of building the next great&amp;nbsp;company, like Facebook or Apple, since he was younger.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He decided to focus on waste&amp;nbsp;because he said&amp;nbsp;issues surrounding waste management are very tangible in his native Italy — and it was clear there&amp;nbsp;wasn’t&amp;nbsp;much being done about it.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We all need to reclaim this problem,”&amp;nbsp;Zeoli&amp;nbsp;said. “I read 100 articles on this problem. For example, in one year, only in one year, in the&amp;nbsp;whole world&amp;nbsp;over 100 million tons of plastic is created and only 9% is recycled. This is a very&amp;nbsp;real&amp;nbsp;problem.”&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Zeoli&amp;nbsp;launched&amp;nbsp;Ganiga&amp;nbsp;in 2021&amp;nbsp;and&amp;nbsp;built its first prototype in 2022. Zeoli&amp;nbsp;said they decided to focus on building a bin to solve this problem because it not only gives&amp;nbsp;&amp;nbsp;people&amp;nbsp;a&amp;nbsp;physical&amp;nbsp;place to put waste that can ensure it gets properly recycled and sorted, but also because the bins spit out data that can be used for the future.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Waste management is also expensive for companies, Zeoli said. Many organizations, especially in Europe, have ESG mandates to adhere&amp;nbsp;to. Zeoli hopes&amp;nbsp;Hoooly&amp;nbsp;can help companies better track their waste production to&amp;nbsp;help them reduce waste and&amp;nbsp;waste-related&amp;nbsp;costs down the line.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga&amp;nbsp;started selling its&amp;nbsp;bins in 2024 and has since sold more than 120 robots to&amp;nbsp;customers&amp;nbsp;like&amp;nbsp;Google&amp;nbsp;and&amp;nbsp;multiple airports including the ones in&amp;nbsp;Bologna, Venice, and Madrid, among others.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zeoli&amp;nbsp;said the company made $500,000 in revenue in 2024 and is already up to $750,000 in just the first nine months of 2025.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also raised $1.5 million in pre-seed funding from investors including clean tech VC firm&amp;nbsp;NextSTEP&amp;nbsp;and&amp;nbsp;Next Energy Capital, among others.&amp;nbsp;Ganiga&amp;nbsp;is looking to raise a $3 million seed round.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is gearing up to launch its latest product in November,&amp;nbsp;the&amp;nbsp;Hooolyfood,&amp;nbsp;which&amp;nbsp;is a software product that uses camera images to determine&amp;nbsp;the exact amount of food waste.&amp;nbsp;The company plans to delve into further software-focused products&amp;nbsp;in&amp;nbsp;the future, too, Zeoli said, based on the data their current bins and software is collecting.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga&amp;nbsp;has focused on the European market thus&amp;nbsp;far,&amp;nbsp;but Zeoli&amp;nbsp;said he’s hoping to expand into the U.S.; the company is even thinking of moving its headquarters stateside in 2026.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ganiga&amp;nbsp;is the first startup in all&amp;nbsp;the&amp;nbsp;world&amp;nbsp;to fill one airport&amp;nbsp;with&amp;nbsp;the smart bins,”&amp;nbsp;Zeoli said. “This is important because we don’t target the prototype,&amp;nbsp;we are a product, and we&amp;nbsp;are&amp;nbsp;open&amp;nbsp;to&amp;nbsp;the&amp;nbsp;market.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;If you want to learn from Ganiga firsthand, and see dozens of&amp;nbsp;additional&amp;nbsp;pitches, valuable workshops and make the connections that drive business results,&amp;nbsp;&lt;/em&gt;&lt;em&gt;head here to learn more about this year’s Disrupt&lt;/em&gt;&lt;em&gt;, held October 27 to 29 in San Francisco.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025" class="wp-image-3048094" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Disrupt_General_Article_No-Anniversary_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/ganiga-will-showcase-its-waste-sorting-robots-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 08 Oct 2025 15:59:45 +0000</pubDate></item><item><title>[NEW] Vandals deface ads for AI necklaces that listen to all your conversations (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/10/vandals-deface-ads-for-ai-necklaces-that-listen-to-all-your-conversations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Critics attacked subway ads to defend human friends and broadly criticize AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A New York City subway ad takeover promoting an AI friend necklace sparked widespread vandalism.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Gerald Zaffuts | iStock Editorial / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;"AI doesn't care," a vandal scrawled on a New York subway ad promoting a wearable AI pendant called Friend, which was designed to monitor a user's everyday conversations and serve as a companion "who listens, responds, and supports you."&lt;/p&gt;
&lt;p&gt;"Human connection is sacred," the vandal wrote, emphasizing, "AI is &lt;em&gt;not &lt;/em&gt;your friend."&lt;/p&gt;
&lt;p&gt;This act of vandalism is now part of a huge online archive collecting defaced ads that the Friend campaign inspired, as many New Yorkers responded with vitriol to marketing claims that the AI "friend" would never "bail on dinner" or abandon you to ride the subway alone.&lt;/p&gt;
&lt;p&gt;"Friends don't let friends sell their souls," another vandal wrote.&lt;/p&gt;
&lt;p&gt;Others criticizing the AI pendant—which sells for $129 and will be available in Walmart stores soon—used the ads to lob larger political complaints about AI. For example, on an ad suggesting an AI friend would "never leave dirty dishes in the sink," a vandal called out AI data centers like xAI's for "poisoning black communities." Another wrote that "freely giving your personal info to Big Tech won't heal your wounds," urging subway riders to join the backlash against Palantir, which The Guardian reported uses AI to surveil individuals and identify military targets.&lt;/p&gt;
&lt;p&gt;A review of the archive shows that much of the criticism focused on the potential for the pendant to spy on users. "AI surveillance slop," one vandal summed it up, while another defaced an ad promising "I'll binge the entire series with you" to say "I'll steal your info, steal your data, steal your identity."&lt;/p&gt;
&lt;p&gt;According&amp;nbsp;to The New York Times, over the past six weeks, the Friend ads have become "one of the most talked about subway marketing campaigns in recent memory," after 22-year-old Friend founder Avi Schiffmann paid less than $1 million to flood MTA subway cars with ads across New York.&amp;nbsp;Since its rollout in New York, the campaign has spread to Los Angeles, and Chicago will be next, but MTA subway cars were targeted first to drive as much hype as possible, Schiffman confirmed.&lt;/p&gt;
&lt;p&gt;"Only the MTA allows you to buy a full takeover like that," Schiffman told the NYT. "It almost feels illegal."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to backlash over feared surveillance capitalism, critics have accused Schiffman of taking advantage of the loneliness epidemic. Conducting a survey last year, researchers with Harvard Graduate School of Education’s Making Caring Common found that people between "30-44 years of age were the loneliest group." Overall, 73 percent of those surveyed "selected technology as contributing to loneliness in the country."&lt;/p&gt;
&lt;p&gt;But Schiffman rejects these criticisms, telling the NYT that his AI Friend pendant is intended to supplement human friends, not replace them, supposedly helping to raise the "average emotional intelligence" of users "significantly."&lt;/p&gt;
&lt;p&gt;"I don’t view this as dystopian," Schiffman said, suggesting that "the AI friend is a new category of companionship, one that will coexist alongside traditional friends rather than replace them," the NYT reported. "We have a cat and a dog and a child and an adult in the same room," the Friend founder said. "Why not an AI?"&lt;/p&gt;
&lt;p&gt;The MTA has not commented on the controversy, but Victoria Mottesheard—a vice president at Outfront Media, which manages MTA advertising—told the NYT that the Friend campaign blew up because AI "is the conversation of 2025."&lt;/p&gt;
&lt;h2&gt;Website lets anyone deface Friend ads&lt;/h2&gt;
&lt;p&gt;So far, the Friend ads have not yielded significant sales, Schiffman confirmed, telling the NYT that only 3,100 have sold. He expects that society isn't ready for AI companions to be promoted at such a large scale and that his ad campaign will help normalize AI friends.&lt;/p&gt;
&lt;p&gt;In the meantime, critics have rushed to attack Friend on social media, inspiring a website where anyone can vandalize a Friend ad and share it online. That website has received close to 6,000 submissions so far, its creator, Marc Mueller, told the NYT, and visitors can take a tour of these submissions by choosing to "ride train to see more" after creating their own vandalized version.&lt;/p&gt;
&lt;p&gt;For visitors to Mueller's site, riding the train displays a carousel documenting backlash to Friend, as well as "performance art" by visitors poking fun at the ads in less serious ways. One example showed a vandalized ad changing "Friend" to "Fries," with a crude illustration of McDonald's French fries, while another transformed the ad into a campaign for "fried chicken."&lt;/p&gt;
&lt;p&gt;Others were seemingly more serious about turning the ad into a warning. One vandal drew a bunch of arrows pointing to the "end" in Friend while turning the pendant into a cry-face emoji, seemingly drawing attention to research on the mental health risks of relying on AI companions—including the alleged suicide risks of products like Character.AI and ChatGPT, which have spawned lawsuits and prompted a Senate hearing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Critics attacked subway ads to defend human friends and broadly criticize AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A New York City subway ad takeover promoting an AI friend necklace sparked widespread vandalism.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Gerald Zaffuts | iStock Editorial / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;"AI doesn't care," a vandal scrawled on a New York subway ad promoting a wearable AI pendant called Friend, which was designed to monitor a user's everyday conversations and serve as a companion "who listens, responds, and supports you."&lt;/p&gt;
&lt;p&gt;"Human connection is sacred," the vandal wrote, emphasizing, "AI is &lt;em&gt;not &lt;/em&gt;your friend."&lt;/p&gt;
&lt;p&gt;This act of vandalism is now part of a huge online archive collecting defaced ads that the Friend campaign inspired, as many New Yorkers responded with vitriol to marketing claims that the AI "friend" would never "bail on dinner" or abandon you to ride the subway alone.&lt;/p&gt;
&lt;p&gt;"Friends don't let friends sell their souls," another vandal wrote.&lt;/p&gt;
&lt;p&gt;Others criticizing the AI pendant—which sells for $129 and will be available in Walmart stores soon—used the ads to lob larger political complaints about AI. For example, on an ad suggesting an AI friend would "never leave dirty dishes in the sink," a vandal called out AI data centers like xAI's for "poisoning black communities." Another wrote that "freely giving your personal info to Big Tech won't heal your wounds," urging subway riders to join the backlash against Palantir, which The Guardian reported uses AI to surveil individuals and identify military targets.&lt;/p&gt;
&lt;p&gt;A review of the archive shows that much of the criticism focused on the potential for the pendant to spy on users. "AI surveillance slop," one vandal summed it up, while another defaced an ad promising "I'll binge the entire series with you" to say "I'll steal your info, steal your data, steal your identity."&lt;/p&gt;
&lt;p&gt;According&amp;nbsp;to The New York Times, over the past six weeks, the Friend ads have become "one of the most talked about subway marketing campaigns in recent memory," after 22-year-old Friend founder Avi Schiffmann paid less than $1 million to flood MTA subway cars with ads across New York.&amp;nbsp;Since its rollout in New York, the campaign has spread to Los Angeles, and Chicago will be next, but MTA subway cars were targeted first to drive as much hype as possible, Schiffman confirmed.&lt;/p&gt;
&lt;p&gt;"Only the MTA allows you to buy a full takeover like that," Schiffman told the NYT. "It almost feels illegal."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to backlash over feared surveillance capitalism, critics have accused Schiffman of taking advantage of the loneliness epidemic. Conducting a survey last year, researchers with Harvard Graduate School of Education’s Making Caring Common found that people between "30-44 years of age were the loneliest group." Overall, 73 percent of those surveyed "selected technology as contributing to loneliness in the country."&lt;/p&gt;
&lt;p&gt;But Schiffman rejects these criticisms, telling the NYT that his AI Friend pendant is intended to supplement human friends, not replace them, supposedly helping to raise the "average emotional intelligence" of users "significantly."&lt;/p&gt;
&lt;p&gt;"I don’t view this as dystopian," Schiffman said, suggesting that "the AI friend is a new category of companionship, one that will coexist alongside traditional friends rather than replace them," the NYT reported. "We have a cat and a dog and a child and an adult in the same room," the Friend founder said. "Why not an AI?"&lt;/p&gt;
&lt;p&gt;The MTA has not commented on the controversy, but Victoria Mottesheard—a vice president at Outfront Media, which manages MTA advertising—told the NYT that the Friend campaign blew up because AI "is the conversation of 2025."&lt;/p&gt;
&lt;h2&gt;Website lets anyone deface Friend ads&lt;/h2&gt;
&lt;p&gt;So far, the Friend ads have not yielded significant sales, Schiffman confirmed, telling the NYT that only 3,100 have sold. He expects that society isn't ready for AI companions to be promoted at such a large scale and that his ad campaign will help normalize AI friends.&lt;/p&gt;
&lt;p&gt;In the meantime, critics have rushed to attack Friend on social media, inspiring a website where anyone can vandalize a Friend ad and share it online. That website has received close to 6,000 submissions so far, its creator, Marc Mueller, told the NYT, and visitors can take a tour of these submissions by choosing to "ride train to see more" after creating their own vandalized version.&lt;/p&gt;
&lt;p&gt;For visitors to Mueller's site, riding the train displays a carousel documenting backlash to Friend, as well as "performance art" by visitors poking fun at the ads in less serious ways. One example showed a vandalized ad changing "Friend" to "Fries," with a crude illustration of McDonald's French fries, while another transformed the ad into a campaign for "fried chicken."&lt;/p&gt;
&lt;p&gt;Others were seemingly more serious about turning the ad into a warning. One vandal drew a bunch of arrows pointing to the "end" in Friend while turning the pendant into a cry-face emoji, seemingly drawing attention to research on the mental health risks of relying on AI companions—including the alleged suicide risks of products like Character.AI and ChatGPT, which have spawned lawsuits and prompted a Senate hearing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/10/vandals-deface-ads-for-ai-necklaces-that-listen-to-all-your-conversations/</guid><pubDate>Wed, 08 Oct 2025 16:07:29 +0000</pubDate></item><item><title>[NEW] Zendesk says its new AI agent can solve 80% of support issues (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/zendesk-says-its-new-ai-agent-can-solve-80-of-support-issues/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/04/GettyImages-1082161742.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Zendesk announced Wednesday at its AI summit a string of LLM-driven products meant to reshape the company’s reliance on human technicians.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The center of the new features is an autonomous support agent that Zendesk believes will solve 80% of support issues without human intervention. That system will be supplemented by a co-pilot agent, which will assist human technicians with the remaining 20% of issues, as well as an admin-layer agent, a voice-based agent, and an analytics agent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Shashi Upadhyay, Zendesk’s President of Product, Engineering and AI, the new agents are part of a broader change in the support industry, as AI replaces much of the work that was previously done by humans. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The world’s going to shift from software that’s built for human users, to a system where AI actually does most of the work,” Upadhyay told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Independent benchmarks suggest that contemporary AI models are capable of taking on the work. TAU-bench, which was designed to measure a model’s&amp;nbsp;tool-calling ability,&amp;nbsp;includes a scenario in which models have to process a returned product — a close analogue to many support tasks. The current leader, Claude Sonnet 4.5, solves 85% of issues on the test.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After a chaotic investor fight in 2022, Zendesk has made a string of AI acquisitions that laid the groundwork for the current shift. The analytics agent launching today is built directly on the company’s Hyperarc acquisition, which was completed in July. Earlier AI acquisitions include the QA and agentic service system Klaus (acquired in February 2024) and the automation platform Ultimate (acquired the following March).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zendesk has previewed the new system with existing customers, and Upadhyay says the results have been promising.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“For customers that have been using it, consumer satisfaction has been up by five to ten points,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Large language models have often been deployed for customer support, although rarely at the Zendesk’s scale. Companies from Airbnb to Regal Theaters have already experimented with in-house chatbot support, often contracting directly with foundation model labs. But those systems typically deal with information retrieval rather than more complex troubleshooting or taking self-directed action.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If the new push for AI-based support is successful, the economic implications would be significant. Zendesk’s Resolution Platform already supports nearly 20,000 customers, resolving 4.6 billion tickets each year. Beyond Zendesk, the U.S. employs 2.4 million customer service representatives — with far larger workforces in other countries.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/04/GettyImages-1082161742.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Zendesk announced Wednesday at its AI summit a string of LLM-driven products meant to reshape the company’s reliance on human technicians.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The center of the new features is an autonomous support agent that Zendesk believes will solve 80% of support issues without human intervention. That system will be supplemented by a co-pilot agent, which will assist human technicians with the remaining 20% of issues, as well as an admin-layer agent, a voice-based agent, and an analytics agent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Shashi Upadhyay, Zendesk’s President of Product, Engineering and AI, the new agents are part of a broader change in the support industry, as AI replaces much of the work that was previously done by humans. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The world’s going to shift from software that’s built for human users, to a system where AI actually does most of the work,” Upadhyay told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Independent benchmarks suggest that contemporary AI models are capable of taking on the work. TAU-bench, which was designed to measure a model’s&amp;nbsp;tool-calling ability,&amp;nbsp;includes a scenario in which models have to process a returned product — a close analogue to many support tasks. The current leader, Claude Sonnet 4.5, solves 85% of issues on the test.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After a chaotic investor fight in 2022, Zendesk has made a string of AI acquisitions that laid the groundwork for the current shift. The analytics agent launching today is built directly on the company’s Hyperarc acquisition, which was completed in July. Earlier AI acquisitions include the QA and agentic service system Klaus (acquired in February 2024) and the automation platform Ultimate (acquired the following March).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zendesk has previewed the new system with existing customers, and Upadhyay says the results have been promising.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“For customers that have been using it, consumer satisfaction has been up by five to ten points,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Large language models have often been deployed for customer support, although rarely at the Zendesk’s scale. Companies from Airbnb to Regal Theaters have already experimented with in-house chatbot support, often contracting directly with foundation model labs. But those systems typically deal with information retrieval rather than more complex troubleshooting or taking self-directed action.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If the new push for AI-based support is successful, the economic implications would be significant. Zendesk’s Resolution Platform already supports nearly 20,000 customers, resolving 4.6 billion tickets each year. Beyond Zendesk, the U.S. employs 2.4 million customer service representatives — with far larger workforces in other countries.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/zendesk-says-its-new-ai-agent-can-solve-80-of-support-issues/</guid><pubDate>Wed, 08 Oct 2025 17:00:25 +0000</pubDate></item><item><title>[NEW] Using generative AI to diversify virtual training grounds for robots (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/using-generative-ai-diversify-virtual-training-grounds-robots-1008</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-csail-restaurant.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-19f1f3f3-7fff-ddef-2547-d3fc1f5d3464"&gt;Chatbots like ChatGPT and Claude have experienced a meteoric rise in usage over the past three years because they can help you with a wide range of tasks. Whether you’re writing Shakespearean sonnets, debugging code, or need an answer to an obscure trivia question, artificial intelligence systems seem to have you covered. The source of this versatility? Billions, or even trillions, of textual data points across the internet.&lt;/p&gt;&lt;p dir="ltr"&gt;Those data aren’t enough to teach a robot to be a helpful household or factory assistant, though. To understand how to handle, stack, and place various arrangements of objects across diverse environments, robots need demonstrations. You can think of robot training data as a collection of how-to videos that walk the systems through each motion of a task. Collecting these demonstrations on real robots is time-consuming and not perfectly repeatable, so engineers have created training data by generating simulations with AI (which don’t often reflect real-world physics), or tediously handcrafting each digital environment from scratch.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Toyota Research Institute may have found a way to create the diverse, realistic training grounds robots need. Their “steerable scene generation” approach creates digital scenes of things like kitchens, living rooms, and restaurants that engineers can use to simulate lots of real-world interactions and scenarios. Trained on over 44 million 3D rooms filled with models of objects such as tables and plates, the tool places existing assets in new scenes, then refines each one into a physically accurate, lifelike environment.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation creates these 3D worlds by “steering” a diffusion model — an AI system that generates a visual from random noise — toward a scene you’d find in everyday life. The researchers used this generative system to “in-paint” an environment, filling in particular elements throughout the scene. You can imagine a blank canvas suddenly turning into a kitchen scattered with 3D objects, which are gradually rearranged into a scene that imitates real-world physics. For example, the system ensures that a fork doesn’t pass through a bowl on a table — a common glitch in 3D graphics known as “clipping,” where models overlap or intersect.&lt;/p&gt;&lt;p&gt;How exactly steerable scene generation guides its creation toward realism, however, depends on the strategy you choose. Its main strategy is “Monte Carlo tree search” (MCTS), where the model creates a series of alternative scenes, filling them out in different ways toward a particular objective (like making a scene more physically realistic, or including as many edible items as possible). It’s used by the AI program AlphaGo to beat human opponents in Go (a game similar to chess), as the system considers potential sequences of moves before choosing the most advantageous one.&lt;/p&gt;&lt;p&gt;“We are the first to apply MCTS to scene generation by framing the scene generation task as a sequential decision-making process,” says MIT Department of Electrical Engineering and Computer Science (EECS) PhD student Nicholas Pfaff, who is a CSAIL researcher and a lead author on a&amp;nbsp;paper presenting the work. “We keep building on top of partial scenes to produce better or more desired scenes over time. As a result, MCTS creates scenes that are more complex than what the diffusion model was trained on.”&lt;/p&gt;&lt;p dir="ltr"&gt;In one particularly telling experiment, MCTS added the maximum number of objects to a simple restaurant scene. It featured as many as 34 items on a table, including massive stacks of dim sum dishes, after training on scenes with only 17 objects on average.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation also allows you to generate diverse training scenarios via reinforcement learning — essentially, teaching a diffusion model to fulfill an objective by trial-and-error. After you train on the initial data, your system undergoes a second training stage, where you outline a reward (basically, a desired outcome with a score indicating how close you are to that goal). The model automatically learns to create scenes with higher scores, often producing scenarios that are quite different from those it was trained on.&lt;/p&gt;&lt;p&gt;Users can also prompt the system directly by typing in specific visual descriptions (like “a kitchen with four apples and a bowl on the table”). Then, steerable scene generation can bring your requests to life with precision. For example, the tool accurately followed users’ prompts at rates of 98 percent when building scenes of pantry shelves, and 86 percent for messy breakfast tables. Both marks are at least a 10 percent improvement over comparable methods like&amp;nbsp;“MiDiffusion” and “DiffuScene.”&lt;/p&gt;&lt;p&gt;The system can also complete specific scenes via prompting or light directions (like “come up with a different scene arrangement using the same objects”). You could ask it to place apples on several plates on a kitchen table, for instance, or put board games and books on a shelf. It’s essentially “filling in the blank” by slotting items in empty spaces, but preserving the rest of a scene.&lt;/p&gt;&lt;p dir="ltr"&gt;According to the researchers, the strength of their project lies in its ability to create many scenes that roboticists can actually use. “A key insight from our findings is that it’s OK for the scenes we pre-trained on to not exactly resemble the scenes that we actually want,” says Pfaff. “Using our steering methods, we can move beyond that broad distribution and sample from a ‘better’ one. In other words, generating the diverse, realistic, and task-aligned scenes that we actually want to train our robots in.”&lt;/p&gt;&lt;p dir="ltr"&gt;Such vast scenes became the testing grounds where they could record a virtual robot interacting with different items. The machine carefully placed forks and knives into a cutlery holder, for instance, and rearranged bread onto plates in various 3D settings. Each simulation appeared fluid and realistic, resembling the real-world, adaptable robots steerable scene generation could help train, one day.&lt;/p&gt;&lt;p dir="ltr"&gt;While the system could be an encouraging path forward in generating lots of diverse training data for robots, the researchers say their work is more of a proof of concept. In the future, they’d like to use generative AI to create entirely new objects and scenes, instead of using a fixed library of assets. They also plan to incorporate articulated objects that the robot could open or twist (like cabinets or jars filled with food) to make the scenes even more interactive.&lt;/p&gt;&lt;p dir="ltr"&gt;To make their virtual environments even more realistic, Pfaff and his colleagues may incorporate real-world objects by using a library of objects and scenes pulled from images on the internet and using their previous work on “Scalable Real2Sim.” By expanding how diverse and lifelike AI-constructed robot testing grounds can be, the team hopes to build a community of users that’ll create lots of data, which could then be used as a massive dataset to teach dexterous robots different skills.&lt;/p&gt;&lt;p&gt;“Today, creating realistic scenes for simulation can be quite a challenging endeavor; procedural generation can readily produce a large number of scenes, but they likely won’t be representative of the environments the robot would encounter in the real world. Manually creating bespoke scenes is both time-consuming and expensive,” says Jeremy Binagia, an applied scientist at Amazon Robotics who wasn’t involved in the paper. “Steerable scene generation offers a better approach: train a generative model on a large collection of pre-existing scenes and adapt it (using a strategy such as reinforcement learning) to specific downstream applications. Compared to previous works that leverage an off-the-shelf vision-language model or focus just on arranging objects in a 2D grid, this approach guarantees physical feasibility and considers full 3D translation and rotation, enabling the generation of much more interesting scenes.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Steerable scene generation with post training and inference-time search provides a novel and efficient framework for automating scene generation at scale,” says Toyota Research Institute roboticist Rick Cory SM ’08, PhD ’10, who also wasn’t involved in the paper. “Moreover, it can generate ‘never-before-seen’ scenes that are deemed important for downstream tasks. In the future, combining this framework with vast internet data could unlock an important milestone towards efficient training of robots for deployment in the real world.”&lt;/p&gt;&lt;p&gt;Pfaff wrote the paper with senior author Russ Tedrake, the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT; a senior vice president of large behavior models at the Toyota Research Institute; and CSAIL principal investigator. Other authors were Toyota Research Institute robotics researcher Hongkai Dai SM ’12, PhD ’16; team lead and Senior Research Scientist Sergey Zakharov; and Carnegie Mellon University PhD student Shun Iwase. Their work was supported, in part, by Amazon and the Toyota Research Institute. The researchers presented their work at the Conference on Robot Learning (CoRL) in September.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-csail-restaurant.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-19f1f3f3-7fff-ddef-2547-d3fc1f5d3464"&gt;Chatbots like ChatGPT and Claude have experienced a meteoric rise in usage over the past three years because they can help you with a wide range of tasks. Whether you’re writing Shakespearean sonnets, debugging code, or need an answer to an obscure trivia question, artificial intelligence systems seem to have you covered. The source of this versatility? Billions, or even trillions, of textual data points across the internet.&lt;/p&gt;&lt;p dir="ltr"&gt;Those data aren’t enough to teach a robot to be a helpful household or factory assistant, though. To understand how to handle, stack, and place various arrangements of objects across diverse environments, robots need demonstrations. You can think of robot training data as a collection of how-to videos that walk the systems through each motion of a task. Collecting these demonstrations on real robots is time-consuming and not perfectly repeatable, so engineers have created training data by generating simulations with AI (which don’t often reflect real-world physics), or tediously handcrafting each digital environment from scratch.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Toyota Research Institute may have found a way to create the diverse, realistic training grounds robots need. Their “steerable scene generation” approach creates digital scenes of things like kitchens, living rooms, and restaurants that engineers can use to simulate lots of real-world interactions and scenarios. Trained on over 44 million 3D rooms filled with models of objects such as tables and plates, the tool places existing assets in new scenes, then refines each one into a physically accurate, lifelike environment.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation creates these 3D worlds by “steering” a diffusion model — an AI system that generates a visual from random noise — toward a scene you’d find in everyday life. The researchers used this generative system to “in-paint” an environment, filling in particular elements throughout the scene. You can imagine a blank canvas suddenly turning into a kitchen scattered with 3D objects, which are gradually rearranged into a scene that imitates real-world physics. For example, the system ensures that a fork doesn’t pass through a bowl on a table — a common glitch in 3D graphics known as “clipping,” where models overlap or intersect.&lt;/p&gt;&lt;p&gt;How exactly steerable scene generation guides its creation toward realism, however, depends on the strategy you choose. Its main strategy is “Monte Carlo tree search” (MCTS), where the model creates a series of alternative scenes, filling them out in different ways toward a particular objective (like making a scene more physically realistic, or including as many edible items as possible). It’s used by the AI program AlphaGo to beat human opponents in Go (a game similar to chess), as the system considers potential sequences of moves before choosing the most advantageous one.&lt;/p&gt;&lt;p&gt;“We are the first to apply MCTS to scene generation by framing the scene generation task as a sequential decision-making process,” says MIT Department of Electrical Engineering and Computer Science (EECS) PhD student Nicholas Pfaff, who is a CSAIL researcher and a lead author on a&amp;nbsp;paper presenting the work. “We keep building on top of partial scenes to produce better or more desired scenes over time. As a result, MCTS creates scenes that are more complex than what the diffusion model was trained on.”&lt;/p&gt;&lt;p dir="ltr"&gt;In one particularly telling experiment, MCTS added the maximum number of objects to a simple restaurant scene. It featured as many as 34 items on a table, including massive stacks of dim sum dishes, after training on scenes with only 17 objects on average.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation also allows you to generate diverse training scenarios via reinforcement learning — essentially, teaching a diffusion model to fulfill an objective by trial-and-error. After you train on the initial data, your system undergoes a second training stage, where you outline a reward (basically, a desired outcome with a score indicating how close you are to that goal). The model automatically learns to create scenes with higher scores, often producing scenarios that are quite different from those it was trained on.&lt;/p&gt;&lt;p&gt;Users can also prompt the system directly by typing in specific visual descriptions (like “a kitchen with four apples and a bowl on the table”). Then, steerable scene generation can bring your requests to life with precision. For example, the tool accurately followed users’ prompts at rates of 98 percent when building scenes of pantry shelves, and 86 percent for messy breakfast tables. Both marks are at least a 10 percent improvement over comparable methods like&amp;nbsp;“MiDiffusion” and “DiffuScene.”&lt;/p&gt;&lt;p&gt;The system can also complete specific scenes via prompting or light directions (like “come up with a different scene arrangement using the same objects”). You could ask it to place apples on several plates on a kitchen table, for instance, or put board games and books on a shelf. It’s essentially “filling in the blank” by slotting items in empty spaces, but preserving the rest of a scene.&lt;/p&gt;&lt;p dir="ltr"&gt;According to the researchers, the strength of their project lies in its ability to create many scenes that roboticists can actually use. “A key insight from our findings is that it’s OK for the scenes we pre-trained on to not exactly resemble the scenes that we actually want,” says Pfaff. “Using our steering methods, we can move beyond that broad distribution and sample from a ‘better’ one. In other words, generating the diverse, realistic, and task-aligned scenes that we actually want to train our robots in.”&lt;/p&gt;&lt;p dir="ltr"&gt;Such vast scenes became the testing grounds where they could record a virtual robot interacting with different items. The machine carefully placed forks and knives into a cutlery holder, for instance, and rearranged bread onto plates in various 3D settings. Each simulation appeared fluid and realistic, resembling the real-world, adaptable robots steerable scene generation could help train, one day.&lt;/p&gt;&lt;p dir="ltr"&gt;While the system could be an encouraging path forward in generating lots of diverse training data for robots, the researchers say their work is more of a proof of concept. In the future, they’d like to use generative AI to create entirely new objects and scenes, instead of using a fixed library of assets. They also plan to incorporate articulated objects that the robot could open or twist (like cabinets or jars filled with food) to make the scenes even more interactive.&lt;/p&gt;&lt;p dir="ltr"&gt;To make their virtual environments even more realistic, Pfaff and his colleagues may incorporate real-world objects by using a library of objects and scenes pulled from images on the internet and using their previous work on “Scalable Real2Sim.” By expanding how diverse and lifelike AI-constructed robot testing grounds can be, the team hopes to build a community of users that’ll create lots of data, which could then be used as a massive dataset to teach dexterous robots different skills.&lt;/p&gt;&lt;p&gt;“Today, creating realistic scenes for simulation can be quite a challenging endeavor; procedural generation can readily produce a large number of scenes, but they likely won’t be representative of the environments the robot would encounter in the real world. Manually creating bespoke scenes is both time-consuming and expensive,” says Jeremy Binagia, an applied scientist at Amazon Robotics who wasn’t involved in the paper. “Steerable scene generation offers a better approach: train a generative model on a large collection of pre-existing scenes and adapt it (using a strategy such as reinforcement learning) to specific downstream applications. Compared to previous works that leverage an off-the-shelf vision-language model or focus just on arranging objects in a 2D grid, this approach guarantees physical feasibility and considers full 3D translation and rotation, enabling the generation of much more interesting scenes.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Steerable scene generation with post training and inference-time search provides a novel and efficient framework for automating scene generation at scale,” says Toyota Research Institute roboticist Rick Cory SM ’08, PhD ’10, who also wasn’t involved in the paper. “Moreover, it can generate ‘never-before-seen’ scenes that are deemed important for downstream tasks. In the future, combining this framework with vast internet data could unlock an important milestone towards efficient training of robots for deployment in the real world.”&lt;/p&gt;&lt;p&gt;Pfaff wrote the paper with senior author Russ Tedrake, the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT; a senior vice president of large behavior models at the Toyota Research Institute; and CSAIL principal investigator. Other authors were Toyota Research Institute robotics researcher Hongkai Dai SM ’12, PhD ’16; team lead and Senior Research Scientist Sergey Zakharov; and Carnegie Mellon University PhD student Shun Iwase. Their work was supported, in part, by Amazon and the Toyota Research Institute. The researchers presented their work at the Conference on Robot Learning (CoRL) in September.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/using-generative-ai-diversify-virtual-training-grounds-robots-1008</guid><pubDate>Wed, 08 Oct 2025 17:45:00 +0000</pubDate></item><item><title>[NEW] Sora’s first week on iOS in the US was nearly as big as ChatGPT’s (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/soras-first-week-on-ios-in-the-us-was-nearly-as-big-as-chatgpts/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After OpenAI’s video-generating app Sora surged to the No. 1 position on the U.S. App Store, it has now, technically, experienced a bigger first week than ChatGPT on iOS, according to new data from app intelligence provider Appfigures. Its estimates show that Sora saw 627,000 iOS downloads in its first seven days of availability, compared with ChatGPT’s 606,000 iOS downloads during its first week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the fairest comparison, however, because ChatGPT was available only in the U.S. during its first week, while Sora is currently offered in the U.S. and Canada at launch. Still, Appfigures says that Canada contributed about 45K installs, so the Sora launch was about 96% of ChatGPT’s launch, if the data had been based on the U.S.&amp;nbsp;numbers only. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This level of consumer adoption is worth noting because Sora remains an invite-only app, while ChatGPT was more publicly available at launch. That makes Sora’s performance more impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its first day, Sora had seen 56,000 app installs in short order, bumping the app to become the No. 3 Top Overall app on the U.S. App Store. By Friday, October 3, it reached No. 1. That surge had already put Sora’s debut ahead of other major AI app launches, including Anthropic’s Claude and Microsoft’s Copilot, and put it on par with xAI’s Grok launch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A quick scan of social media provides plenty of anecdotes that support Appfigures’ data. Sora videos, which uses the new Sora 2 video model and gives users the ability to generate realistic deepfakes, seem to be everywhere. Users are even creating deepfakes of dead people, a use case that has prompted Zelda Williams, daughter of the late actor Robin Williams, to ask folks to stop sending her AI generated images of her father. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055495" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/thumbnail_sora-first-week-downloads-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Per Appfigures, the app has seen steady adoption since its first day on the market, September 30, 2025. Its data indicates that daily downloads on iOS hit a high mark of 107,800 downloads on October 1, 2025. It has since seen between lows of 84,400 daily installs (on Oct. 6) and 98,500 daily installs (on Oct. 4). &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that’s not quite as high as earlier in the week, it’s still decent numbers for an app that not everyone can yet use.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After OpenAI’s video-generating app Sora surged to the No. 1 position on the U.S. App Store, it has now, technically, experienced a bigger first week than ChatGPT on iOS, according to new data from app intelligence provider Appfigures. Its estimates show that Sora saw 627,000 iOS downloads in its first seven days of availability, compared with ChatGPT’s 606,000 iOS downloads during its first week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the fairest comparison, however, because ChatGPT was available only in the U.S. during its first week, while Sora is currently offered in the U.S. and Canada at launch. Still, Appfigures says that Canada contributed about 45K installs, so the Sora launch was about 96% of ChatGPT’s launch, if the data had been based on the U.S.&amp;nbsp;numbers only. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This level of consumer adoption is worth noting because Sora remains an invite-only app, while ChatGPT was more publicly available at launch. That makes Sora’s performance more impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its first day, Sora had seen 56,000 app installs in short order, bumping the app to become the No. 3 Top Overall app on the U.S. App Store. By Friday, October 3, it reached No. 1. That surge had already put Sora’s debut ahead of other major AI app launches, including Anthropic’s Claude and Microsoft’s Copilot, and put it on par with xAI’s Grok launch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A quick scan of social media provides plenty of anecdotes that support Appfigures’ data. Sora videos, which uses the new Sora 2 video model and gives users the ability to generate realistic deepfakes, seem to be everywhere. Users are even creating deepfakes of dead people, a use case that has prompted Zelda Williams, daughter of the late actor Robin Williams, to ask folks to stop sending her AI generated images of her father. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055495" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/thumbnail_sora-first-week-downloads-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Per Appfigures, the app has seen steady adoption since its first day on the market, September 30, 2025. Its data indicates that daily downloads on iOS hit a high mark of 107,800 downloads on October 1, 2025. It has since seen between lows of 84,400 daily installs (on Oct. 6) and 98,500 daily installs (on Oct. 4). &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that’s not quite as high as earlier in the week, it’s still decent numbers for an app that not everyone can yet use.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/soras-first-week-on-ios-in-the-us-was-nearly-as-big-as-chatgpts/</guid><pubDate>Wed, 08 Oct 2025 18:12:54 +0000</pubDate></item></channel></rss>