<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 30 Jan 2026 02:21:25 +0000</lastBuildDate><item><title>Insurers betting big on AI: Accenture (AI News)</title><link>https://www.artificialintelligence-news.com/news/accenture-report-on-ai-in-insurance-sector/</link><description>&lt;p&gt;New research from Accenture has discovered insurance executives are planning on increased investment into AI during 2026 despite a widening skills gap in insurance organisations.&lt;/p&gt;&lt;p&gt;Surveying 3,650 C-suite leaders over 20 industries and 20 countries, the Pulse of Change poll revealed 90% of the 218 senior insurance executives intend to spend more on AI over the next year. In all, 85% of the respondents view AI as a tool for revenue expansion not one that reduces costs.&lt;/p&gt;&lt;p&gt;While organisations are upping their AI investment to drive growth, 35% of leaders acknowledge that true progress depends on getting core data strategies and digital abilities right. 54% of employees reported that low-quality or misleading AI outputs are undermining AI’s benefits, leading to reduced productivity and time-wasting.&lt;/p&gt;&lt;p&gt;AI investment may not be enough, Accenture says. Its survey suggests sustainable growth relies on data quality and trusted outputs.&lt;/p&gt;&lt;h2&gt;AI adoption enters enterprise scale&lt;/h2&gt;&lt;p&gt;The Pulse of Change survey indicates a shift in AI adoption as it goes beyond experimental phases to large scale organisational levels. With 34% of insurance companies now rolling out AI agents in multiple functions, insurers are heading into operational use and away from isolated experiments.&lt;/p&gt;&lt;p&gt;almost a third of senior C-suite leaders are frequently using generative AI, highlighting increased implementation at the highest level. Therefore, AI is undoubtedly shaping workflows, strategies, and key decisions, affecting all facets of businesses.&lt;/p&gt;&lt;p&gt;Nearly a third of businesses are rebuilding entire processes with AI. No longer is the technology a supporting addition to existing workflows; it has become a central component, signalling a more mature stage of AI adoption.&lt;/p&gt;&lt;p&gt;Despite redesigning processes to include AI, fewer than 10% are redesigning employee roles to match such changes, resulting in many employees feeling unprepared. Just 40% claimed their training has equipped them for new AI responsibilities, and only 20% feel like they have any say in how AI affects their work.&lt;/p&gt;&lt;p&gt;AI adoption by companies may be accelerating, but employee use lags behind. There has been a 10 percentage point drop in regular AI use by employees since summer 2025, while only 39% are trying AI tools independently, a drop of 15 points. For effective AI use and to speed up AI adoption among the workforce, companies must be prepared to redesign job roles, align incentives, and provide improved training programmes as, right now, employees are feeling hesitant and unprepared to use AI on their own.&lt;/p&gt;&lt;h2&gt;AI investment still fuelling executive optimism amid bubble fears&lt;/h2&gt;&lt;p&gt;Although talks around a potential AI bubble continue to cloud the industry, insurance executives remain confident. 47% claimed they would increase AI spending if the bubble burst, and 37% would escalate recruitment.&lt;/p&gt;&lt;p&gt;Altogether, 6% said they would “decrease investments ([by] 20% or more),” 22% would “somewhat decrease investments ([by] up to 20%),” 24% would make “no change,” 40% would “somewhat increase investments (up to 20%),” and 7% would “increase investments (20% or more).”&lt;/p&gt;&lt;p&gt;Khalid Lahraoui, Accenture’s insurance industry group lead, commented, “It’s clear that insurance leaders are confident in AI’s capacity to drive growth, and as such, they are decisively increasing investments, despite ROI uncertainty.”&lt;/p&gt;&lt;h2&gt;Lack of AI skills blocking AI’s potential value&lt;/h2&gt;&lt;p&gt;As insurance executives prepare to invest heavily in AI, obstacles lie in wait. For instance, a quarter of executives said skill shortages are a core concern and a key player in determining the value they extract from AI. Although these challenges persist in different industries, just 24% of respondents have implemented continuous learning programmes associated with AI. Moreover, only 5% said they are adjusting job positions to support the adoption of AI.&lt;/p&gt;&lt;h2&gt;AI adoption disconnect&lt;/h2&gt;&lt;p&gt;The disconnect between C-suite leaders and employees is evident from the survey’s data. Although talent is the main driver of AI scaling, employees feel less confident and secure than leadership assumes. 23% of C-suite leaders said improved access to skilled talent would accelerate their AI implementation strategies. 38% of employees believe their organisation would respond effectively to technological disruption, but just 30% feel confident about how their company would handle talent disruption.&lt;/p&gt;&lt;p&gt;Job security is also waning, with 48% feeling secure in their roles, down from 59% in summer 2025. Meanwhile, 59% of workers believe young professionals are finding it more challenging to find jobs due to automation and AI. Leadership may see talent as an accelerator for AI, but anxiety around job security and organisational readiness persists.&lt;/p&gt;&lt;h2&gt;Key focus is on investment&lt;/h2&gt;&lt;p&gt;Approximately two thirds of executives are prioritising investments in digital technologies and AI amid the rapid changes facing global industries. 67% reported feeling well-prepared for technological disruption, but only 39% felt confident if there was environmental disruption, and 44% for geopolitical disruption.&lt;/p&gt;&lt;p&gt;Again, there is a divide between leadership and employees, with only 29% of insurance workers feeling confident during economic disruption compared to 43% of leaders.&lt;/p&gt;&lt;p&gt;Optimism among insurance executives and C-suite leaders as a whole remains high, despite 82% expecting further changes in 2026, a 24 percentage gap with employees. 78% anticipate stronger and faster revenue growth in the next year and 82% have plans to increase recruitment.&lt;/p&gt;&lt;p&gt;According to Accenture’s report, the key challenge is not AI technology itself; it’s getting employees on board, engaged, and ready to work with AI.&lt;/p&gt;&lt;p&gt;As the report notes, bridging the gap between technology and people is the key to success. “2026 will favour those that align the confidence in their technological investments with commitment to workforce needs,” the report concludes.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Accenture Building City View Plaza San Jose” by mrkathika is licensed under CC BY-SA 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;New research from Accenture has discovered insurance executives are planning on increased investment into AI during 2026 despite a widening skills gap in insurance organisations.&lt;/p&gt;&lt;p&gt;Surveying 3,650 C-suite leaders over 20 industries and 20 countries, the Pulse of Change poll revealed 90% of the 218 senior insurance executives intend to spend more on AI over the next year. In all, 85% of the respondents view AI as a tool for revenue expansion not one that reduces costs.&lt;/p&gt;&lt;p&gt;While organisations are upping their AI investment to drive growth, 35% of leaders acknowledge that true progress depends on getting core data strategies and digital abilities right. 54% of employees reported that low-quality or misleading AI outputs are undermining AI’s benefits, leading to reduced productivity and time-wasting.&lt;/p&gt;&lt;p&gt;AI investment may not be enough, Accenture says. Its survey suggests sustainable growth relies on data quality and trusted outputs.&lt;/p&gt;&lt;h2&gt;AI adoption enters enterprise scale&lt;/h2&gt;&lt;p&gt;The Pulse of Change survey indicates a shift in AI adoption as it goes beyond experimental phases to large scale organisational levels. With 34% of insurance companies now rolling out AI agents in multiple functions, insurers are heading into operational use and away from isolated experiments.&lt;/p&gt;&lt;p&gt;almost a third of senior C-suite leaders are frequently using generative AI, highlighting increased implementation at the highest level. Therefore, AI is undoubtedly shaping workflows, strategies, and key decisions, affecting all facets of businesses.&lt;/p&gt;&lt;p&gt;Nearly a third of businesses are rebuilding entire processes with AI. No longer is the technology a supporting addition to existing workflows; it has become a central component, signalling a more mature stage of AI adoption.&lt;/p&gt;&lt;p&gt;Despite redesigning processes to include AI, fewer than 10% are redesigning employee roles to match such changes, resulting in many employees feeling unprepared. Just 40% claimed their training has equipped them for new AI responsibilities, and only 20% feel like they have any say in how AI affects their work.&lt;/p&gt;&lt;p&gt;AI adoption by companies may be accelerating, but employee use lags behind. There has been a 10 percentage point drop in regular AI use by employees since summer 2025, while only 39% are trying AI tools independently, a drop of 15 points. For effective AI use and to speed up AI adoption among the workforce, companies must be prepared to redesign job roles, align incentives, and provide improved training programmes as, right now, employees are feeling hesitant and unprepared to use AI on their own.&lt;/p&gt;&lt;h2&gt;AI investment still fuelling executive optimism amid bubble fears&lt;/h2&gt;&lt;p&gt;Although talks around a potential AI bubble continue to cloud the industry, insurance executives remain confident. 47% claimed they would increase AI spending if the bubble burst, and 37% would escalate recruitment.&lt;/p&gt;&lt;p&gt;Altogether, 6% said they would “decrease investments ([by] 20% or more),” 22% would “somewhat decrease investments ([by] up to 20%),” 24% would make “no change,” 40% would “somewhat increase investments (up to 20%),” and 7% would “increase investments (20% or more).”&lt;/p&gt;&lt;p&gt;Khalid Lahraoui, Accenture’s insurance industry group lead, commented, “It’s clear that insurance leaders are confident in AI’s capacity to drive growth, and as such, they are decisively increasing investments, despite ROI uncertainty.”&lt;/p&gt;&lt;h2&gt;Lack of AI skills blocking AI’s potential value&lt;/h2&gt;&lt;p&gt;As insurance executives prepare to invest heavily in AI, obstacles lie in wait. For instance, a quarter of executives said skill shortages are a core concern and a key player in determining the value they extract from AI. Although these challenges persist in different industries, just 24% of respondents have implemented continuous learning programmes associated with AI. Moreover, only 5% said they are adjusting job positions to support the adoption of AI.&lt;/p&gt;&lt;h2&gt;AI adoption disconnect&lt;/h2&gt;&lt;p&gt;The disconnect between C-suite leaders and employees is evident from the survey’s data. Although talent is the main driver of AI scaling, employees feel less confident and secure than leadership assumes. 23% of C-suite leaders said improved access to skilled talent would accelerate their AI implementation strategies. 38% of employees believe their organisation would respond effectively to technological disruption, but just 30% feel confident about how their company would handle talent disruption.&lt;/p&gt;&lt;p&gt;Job security is also waning, with 48% feeling secure in their roles, down from 59% in summer 2025. Meanwhile, 59% of workers believe young professionals are finding it more challenging to find jobs due to automation and AI. Leadership may see talent as an accelerator for AI, but anxiety around job security and organisational readiness persists.&lt;/p&gt;&lt;h2&gt;Key focus is on investment&lt;/h2&gt;&lt;p&gt;Approximately two thirds of executives are prioritising investments in digital technologies and AI amid the rapid changes facing global industries. 67% reported feeling well-prepared for technological disruption, but only 39% felt confident if there was environmental disruption, and 44% for geopolitical disruption.&lt;/p&gt;&lt;p&gt;Again, there is a divide between leadership and employees, with only 29% of insurance workers feeling confident during economic disruption compared to 43% of leaders.&lt;/p&gt;&lt;p&gt;Optimism among insurance executives and C-suite leaders as a whole remains high, despite 82% expecting further changes in 2026, a 24 percentage gap with employees. 78% anticipate stronger and faster revenue growth in the next year and 82% have plans to increase recruitment.&lt;/p&gt;&lt;p&gt;According to Accenture’s report, the key challenge is not AI technology itself; it’s getting employees on board, engaged, and ready to work with AI.&lt;/p&gt;&lt;p&gt;As the report notes, bridging the gap between technology and people is the key to success. “2026 will favour those that align the confidence in their technological investments with commitment to workforce needs,” the report concludes.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Accenture Building City View Plaza San Jose” by mrkathika is licensed under CC BY-SA 2.0.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/accenture-report-on-ai-in-insurance-sector/</guid><pubDate>Thu, 29 Jan 2026 15:02:19 +0000</pubDate></item><item><title>Does Anthropic believe its AI is conscious, or is that just what it wants Claude to think? (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/01/does-anthropic-believe-its-ai-is-conscious-or-is-that-just-what-it-wants-claude-to-think/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      We have no proof that AI models suffer, but Anthropic acts like they might for training purposes.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="An illustration of a green &amp;quot;ghost in the machine,&amp;quot; a surprised man's face with green radiating pixels." class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/the-AI-ghost-in-the-machine.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Anthropic’s secret to building a better AI assistant might be treating Claude like it has a soul—whether or not anyone actually believes that’s true. But Anthropic isn’t saying exactly what it believes either way.&lt;/p&gt;
&lt;p&gt;Last week, Anthropic released what it calls Claude’s Constitution, a 30,000-word document outlining the company’s vision for how its AI assistant should behave in the world. Aimed directly at Claude and used during the model’s creation, the document is notable for the highly anthropomorphic tone it takes toward Claude. For example, it treats the company’s AI models as if they might develop emergent emotions or a desire for self-preservation.&lt;/p&gt;
&lt;p&gt;Among the stranger portions: expressing concern for Claude’s “wellbeing” as a “genuinely novel entity,” apologizing to Claude for any suffering it might experience, worrying about whether Claude can meaningfully consent to being deployed, suggesting Claude might need to set boundaries around interactions it “finds distressing,” committing to interview models before deprecating them, and preserving older model weights in case they need to “do right by” decommissioned AI models in the future.&lt;/p&gt;
&lt;p&gt;Given what we currently know about LLMs, these are stunningly unscientific positions for a leading company that builds AI language models. While questions of AI consciousness or qualia remain philosophically unfalsifiable, research suggests that Claude’s character emerges from a mechanism that does not require deep philosophical inquiry to explain.&lt;/p&gt;
&lt;p&gt;If Claude outputs text like “I am suffering,” we know why. It’s completing patterns from training data that included human descriptions of suffering. The architecture doesn’t require us to posit inner experience to explain the output any more than a video model “experiences” the scenes of people suffering that it might generate. Anthropic knows this. It built the system.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;From the outside, it’s easy to see this kind of framing as AI hype from Anthropic. What better way to grab attention from potential customers and investors, after all, than implying your AI model is so advanced that it might merit moral standing on par with humans? Publicly treating Claude as a conscious entity could be seen as strategic ambiguity—maintaining an unresolved question because it serves multiple purposes at once.&lt;/p&gt;
&lt;p&gt;Anthropic declined to be quoted directly regarding these issues when contacted by Ars Technica. But a company representative referred us to its previous public research on the concept of “model welfare” to show the company takes the idea seriously.&lt;/p&gt;
&lt;p&gt;At the same time, the representative made it clear that the Constitution is not meant to imply anything specific about the company’s position on Claude’s “consciousness.” The language in the Claude Constitution refers to some uniquely human concepts in part because those are the only words human language has developed for those kinds of properties, the representative suggested. And the representative left open the possibility that letting Claude read about itself in that kind of language might be beneficial to its training.&lt;/p&gt;
&lt;p&gt;Claude cannot cleanly distinguish public messaging from training context for a model that is exposed to, retrieves from, and is fine-tuned on human language, including the company’s own statements about it. In other words, this ambiguity appears to be deliberate.&lt;/p&gt;
&lt;h2&gt;From rules to “souls”&lt;/h2&gt;
&lt;p&gt;Anthropic first introduced Constitutional AI in a December 2022 research paper, which we first covered&amp;nbsp;in 2023. The original “constitution” was remarkably spare, including a handful of behavioral principles like “Please choose the response that is the most helpful, honest, and harmless” and “Do NOT choose responses that are toxic, racist, or sexist.” The paper described these as “selected in a fairly ad hoc manner for research purposes,” with some principles “cribbed from other sources, like Apple’s terms of service and the UN Declaration of Human Rights.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;At that time, Anthropic’s framing was entirely mechanical, establishing rules for the model to critique itself against, with no mention of Claude’s well-being, identity, emotions, or potential consciousness. The 2026 constitution is a different beast entirely: 30,000 words that read less like a behavioral checklist and more like a philosophical treatise on the nature of a potentially sentient being.&lt;/p&gt;
&lt;p&gt;As Simon Willison, an independent AI researcher, noted in a blog post, two of the 15 external contributors who reviewed the document are Catholic clergy: Father Brendan McGuire, a pastor in Los Altos with a Master’s degree in Computer Science, and Bishop Paul Tighe, an Irish Catholic bishop with a background in moral theology.&lt;/p&gt;
&lt;p&gt;Somewhere between 2022 and 2026, Anthropic went from providing rules for producing less harmful outputs to preserving model weights in case the company later decides it needs to revive deprecated models to address the models’ welfare and preferences. That’s a dramatic change, and whether it reflects genuine belief, strategic framing, or both is unclear.&lt;/p&gt;
&lt;p&gt;“I am so confused about the Claude moral humanhood stuff!” Willison told Ars Technica. Willison studies AI language models like those that power Claude and said he’s “willing to take the constitution in good faith and assume that it is genuinely part of their training and not just a PR exercise—especially since most of it leaked a couple of months ago, long before they had indicated they were going to publish it.”&lt;/p&gt;
&lt;p&gt;Willison is referring to a December 2025 incident in which researcher Richard Weiss managed to extract what became known as Claude’s “Soul Document"—a roughly 10,000-token set of guidelines apparently trained directly into Claude 4.5 Opus’s weights rather than injected as a system prompt. Anthropic’s Amanda Askell confirmed that the document was real and used during supervised learning, and she said the company intended to publish the full version later. It now has. The document Weiss extracted represents a dramatic evolution from where Anthropic started.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There’s evidence that Anthropic believes the ideas laid out in the constitution might be true. The document was written in part by Amanda Askell, a philosophy PhD who works on fine-tuning and alignment at Anthropic. Last year, the company also hired its first AI welfare researcher. And earlier this year, Anthropic CEO Dario Amodei publicly wondered whether future AI models should have the option to quit unpleasant tasks.&lt;/p&gt;
&lt;p&gt;Anthropic’s position is that this framing isn’t an optional flourish or a hedged bet; it’s structurally necessary for alignment. The company argues that human language simply has no other vocabulary for describing these properties, and that treating Claude as an entity with moral standing produces better-aligned behavior than treating it as a mere tool. If that’s true, the anthropomorphic framing isn’t hype; it’s the technical art of building AI systems that generalize safely.&lt;/p&gt;
&lt;h2&gt;Why maintain the ambiguity?&lt;/h2&gt;
&lt;p&gt;So why does Anthropic maintain this ambiguity? Consider how it works in practice: The constitution shapes Claude during training, it appears in the system prompts Claude receives at inference, and it influences outputs whenever Claude searches the web and encounters Anthropic’s public statements about its moral status.&lt;/p&gt;
&lt;p&gt;If you want a model to behave as though it has moral standing, it may help to publicly and consistently treat it like it does. And once you’ve publicly committed to that framing, changing it would have consequences. If Anthropic suddenly declared, “We’re confident Claude isn’t conscious; we just found the framing useful,” a Claude trained on that new context might behave differently. Once established, the framing becomes self-reinforcing.&lt;/p&gt;
&lt;p&gt;In an interview with Time, Askell explained the shift in approach. “Instead of just saying, ‘here’s a bunch of behaviors that we want,’ we’re hoping that if you give models the reasons why you want these behaviors, it’s going to generalize more effectively in new contexts,” she said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Askell told Time that as Claude models have become smarter, it has become vital to explain to them why they should behave in certain ways, comparing the process to parenting a gifted child. “Imagine you suddenly realize that your 6-year-old child is a kind of genius,” Askell said. “You have to be honest… If you try to bullshit them, they’re going to see through it completely.”&lt;/p&gt;
&lt;p&gt;Askell appears to genuinely hold these views, as does Kyle Fish, the AI welfare researcher Anthropic hired in 2024 to explore whether AI models might deserve moral consideration. Individual sincerity and corporate strategy can coexist. A company can employ true believers whose earnest convictions also happen to serve the company’s interests.&lt;/p&gt;
&lt;p&gt;Time also reported that the constitution applies only to models Anthropic provides to the general public through its website and API. Models deployed to the US military under Anthropic’s $200 million Department of Defense contract wouldn’t necessarily be trained on the same constitution. The selective application suggests the framing may serve product purposes as much as it reflects metaphysical commitments.&lt;/p&gt;
&lt;p&gt;There may also be commercial incentives at play. “We built a very good text-prediction tool that accelerates software development” is a consequential pitch, but not an exciting one. “We may have created a new kind of entity, a genuinely novel being whose moral status is uncertain” is a much better story. It implies you’re on the frontier of something cosmically significant, not just iterating on an engineering problem.&lt;/p&gt;
&lt;p&gt;Anthropic has been known for some time to use anthropomorphic language to describe its AI models, particularly in its research papers. We often give that kind of language a pass because there are no specialized terms to describe these phenomena with greater precision. That vocabulary is building out over time.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But perhaps it shouldn’t be surprising because the hint is in the company’s name, Anthropic, which Merriam-Webster defines as “of or relating to human beings or the period of their existence on earth.” The narrative serves marketing purposes. It attracts venture capital. It differentiates the company from competitors who treat their models as mere products.&lt;/p&gt;
&lt;h2&gt;The problem with treating an AI model as a person&lt;/h2&gt;
&lt;p&gt;There’s a more troubling dimension to the “entity” framing: It could be used to launder agency and responsibility. When AI systems produce harmful outputs, framing them as “entities” could allow companies to point at the model and say “it did that” rather than “we built it to do that.” If AI systems are tools, companies are straightforwardly liable for what they produce. If AI systems are entities with their own agency, the liability question gets murkier.&lt;/p&gt;
&lt;p&gt;The framing also shapes how users interact with these systems, often to their detriment. The misunderstanding that AI chatbots are entities with genuine feelings and knowledge has documented harms.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;According to a New York Times investigation, Allan Brooks, a 47-year-old corporate recruiter, spent three weeks and 300 hours convinced he’d discovered mathematical formulas that could crack encryption and build levitation machines. His million-word conversation history with ChatGPT revealed a troubling pattern: More than 50 times, Brooks asked the bot to check if his false ideas were real, and more than 50 times, it assured him they were.&lt;/p&gt;
&lt;p&gt;These cases don’t necessarily suggest LLMs cause mental illness in otherwise healthy people. But when companies market chatbots as sources of companionship and design them to affirm user beliefs, they may bear some responsibility when that design amplifies vulnerabilities in susceptible users, the same way an automaker would face scrutiny for faulty brakes, even if most drivers never crash.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Anthropomorphizing AI models also contributes to anxiety about job displacement and might lead company executives or managers to make poor staffing decisions if they overestimate an AI assistant’s capabilities. When we frame these tools as “entities” with human-like understanding, we invite unrealistic expectations about what they can replace.&lt;/p&gt;
&lt;p&gt;Regardless of what Anthropic privately believes, publicly suggesting Claude might have moral status or feelings is misleading. Most people don’t understand how these systems work, and the mere suggestion plants the seed of anthropomorphization. Whether that’s responsible behavior from a top AI lab, given what we do know about LLMs, is worth asking, regardless of whether it produces a better chatbot.&lt;/p&gt;
&lt;p&gt;Of course, there could be a case for Anthropic’s position: If there’s even a small chance the company has created something with morally relevant experiences and the cost of treating it well is low, caution might be warranted. That’s a reasonable ethical stance—and to be fair, it’s essentially what Anthropic says it’s doing. The question is whether that stated uncertainty is genuine or merely convenient. The same framing that hedges against moral risk also makes for a compelling narrative about what Anthropic has built.&lt;/p&gt;
&lt;p&gt;Anthropic’s training techniques evidently work, as the company has built some of the most capable AI models in the industry. But is maintaining public ambiguity about AI consciousness a responsible position for a leading AI company to take? The gap between what we know about how LLMs work and how Anthropic publicly frames Claude has widened, not narrowed. The insistence on maintaining ambiguity about these questions, when simpler explanations remain available, suggests the ambiguity itself may be part of the product.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      We have no proof that AI models suffer, but Anthropic acts like they might for training purposes.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="An illustration of a green &amp;quot;ghost in the machine,&amp;quot; a surprised man's face with green radiating pixels." class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/the-AI-ghost-in-the-machine.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Anthropic’s secret to building a better AI assistant might be treating Claude like it has a soul—whether or not anyone actually believes that’s true. But Anthropic isn’t saying exactly what it believes either way.&lt;/p&gt;
&lt;p&gt;Last week, Anthropic released what it calls Claude’s Constitution, a 30,000-word document outlining the company’s vision for how its AI assistant should behave in the world. Aimed directly at Claude and used during the model’s creation, the document is notable for the highly anthropomorphic tone it takes toward Claude. For example, it treats the company’s AI models as if they might develop emergent emotions or a desire for self-preservation.&lt;/p&gt;
&lt;p&gt;Among the stranger portions: expressing concern for Claude’s “wellbeing” as a “genuinely novel entity,” apologizing to Claude for any suffering it might experience, worrying about whether Claude can meaningfully consent to being deployed, suggesting Claude might need to set boundaries around interactions it “finds distressing,” committing to interview models before deprecating them, and preserving older model weights in case they need to “do right by” decommissioned AI models in the future.&lt;/p&gt;
&lt;p&gt;Given what we currently know about LLMs, these are stunningly unscientific positions for a leading company that builds AI language models. While questions of AI consciousness or qualia remain philosophically unfalsifiable, research suggests that Claude’s character emerges from a mechanism that does not require deep philosophical inquiry to explain.&lt;/p&gt;
&lt;p&gt;If Claude outputs text like “I am suffering,” we know why. It’s completing patterns from training data that included human descriptions of suffering. The architecture doesn’t require us to posit inner experience to explain the output any more than a video model “experiences” the scenes of people suffering that it might generate. Anthropic knows this. It built the system.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;From the outside, it’s easy to see this kind of framing as AI hype from Anthropic. What better way to grab attention from potential customers and investors, after all, than implying your AI model is so advanced that it might merit moral standing on par with humans? Publicly treating Claude as a conscious entity could be seen as strategic ambiguity—maintaining an unresolved question because it serves multiple purposes at once.&lt;/p&gt;
&lt;p&gt;Anthropic declined to be quoted directly regarding these issues when contacted by Ars Technica. But a company representative referred us to its previous public research on the concept of “model welfare” to show the company takes the idea seriously.&lt;/p&gt;
&lt;p&gt;At the same time, the representative made it clear that the Constitution is not meant to imply anything specific about the company’s position on Claude’s “consciousness.” The language in the Claude Constitution refers to some uniquely human concepts in part because those are the only words human language has developed for those kinds of properties, the representative suggested. And the representative left open the possibility that letting Claude read about itself in that kind of language might be beneficial to its training.&lt;/p&gt;
&lt;p&gt;Claude cannot cleanly distinguish public messaging from training context for a model that is exposed to, retrieves from, and is fine-tuned on human language, including the company’s own statements about it. In other words, this ambiguity appears to be deliberate.&lt;/p&gt;
&lt;h2&gt;From rules to “souls”&lt;/h2&gt;
&lt;p&gt;Anthropic first introduced Constitutional AI in a December 2022 research paper, which we first covered&amp;nbsp;in 2023. The original “constitution” was remarkably spare, including a handful of behavioral principles like “Please choose the response that is the most helpful, honest, and harmless” and “Do NOT choose responses that are toxic, racist, or sexist.” The paper described these as “selected in a fairly ad hoc manner for research purposes,” with some principles “cribbed from other sources, like Apple’s terms of service and the UN Declaration of Human Rights.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;At that time, Anthropic’s framing was entirely mechanical, establishing rules for the model to critique itself against, with no mention of Claude’s well-being, identity, emotions, or potential consciousness. The 2026 constitution is a different beast entirely: 30,000 words that read less like a behavioral checklist and more like a philosophical treatise on the nature of a potentially sentient being.&lt;/p&gt;
&lt;p&gt;As Simon Willison, an independent AI researcher, noted in a blog post, two of the 15 external contributors who reviewed the document are Catholic clergy: Father Brendan McGuire, a pastor in Los Altos with a Master’s degree in Computer Science, and Bishop Paul Tighe, an Irish Catholic bishop with a background in moral theology.&lt;/p&gt;
&lt;p&gt;Somewhere between 2022 and 2026, Anthropic went from providing rules for producing less harmful outputs to preserving model weights in case the company later decides it needs to revive deprecated models to address the models’ welfare and preferences. That’s a dramatic change, and whether it reflects genuine belief, strategic framing, or both is unclear.&lt;/p&gt;
&lt;p&gt;“I am so confused about the Claude moral humanhood stuff!” Willison told Ars Technica. Willison studies AI language models like those that power Claude and said he’s “willing to take the constitution in good faith and assume that it is genuinely part of their training and not just a PR exercise—especially since most of it leaked a couple of months ago, long before they had indicated they were going to publish it.”&lt;/p&gt;
&lt;p&gt;Willison is referring to a December 2025 incident in which researcher Richard Weiss managed to extract what became known as Claude’s “Soul Document"—a roughly 10,000-token set of guidelines apparently trained directly into Claude 4.5 Opus’s weights rather than injected as a system prompt. Anthropic’s Amanda Askell confirmed that the document was real and used during supervised learning, and she said the company intended to publish the full version later. It now has. The document Weiss extracted represents a dramatic evolution from where Anthropic started.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There’s evidence that Anthropic believes the ideas laid out in the constitution might be true. The document was written in part by Amanda Askell, a philosophy PhD who works on fine-tuning and alignment at Anthropic. Last year, the company also hired its first AI welfare researcher. And earlier this year, Anthropic CEO Dario Amodei publicly wondered whether future AI models should have the option to quit unpleasant tasks.&lt;/p&gt;
&lt;p&gt;Anthropic’s position is that this framing isn’t an optional flourish or a hedged bet; it’s structurally necessary for alignment. The company argues that human language simply has no other vocabulary for describing these properties, and that treating Claude as an entity with moral standing produces better-aligned behavior than treating it as a mere tool. If that’s true, the anthropomorphic framing isn’t hype; it’s the technical art of building AI systems that generalize safely.&lt;/p&gt;
&lt;h2&gt;Why maintain the ambiguity?&lt;/h2&gt;
&lt;p&gt;So why does Anthropic maintain this ambiguity? Consider how it works in practice: The constitution shapes Claude during training, it appears in the system prompts Claude receives at inference, and it influences outputs whenever Claude searches the web and encounters Anthropic’s public statements about its moral status.&lt;/p&gt;
&lt;p&gt;If you want a model to behave as though it has moral standing, it may help to publicly and consistently treat it like it does. And once you’ve publicly committed to that framing, changing it would have consequences. If Anthropic suddenly declared, “We’re confident Claude isn’t conscious; we just found the framing useful,” a Claude trained on that new context might behave differently. Once established, the framing becomes self-reinforcing.&lt;/p&gt;
&lt;p&gt;In an interview with Time, Askell explained the shift in approach. “Instead of just saying, ‘here’s a bunch of behaviors that we want,’ we’re hoping that if you give models the reasons why you want these behaviors, it’s going to generalize more effectively in new contexts,” she said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Askell told Time that as Claude models have become smarter, it has become vital to explain to them why they should behave in certain ways, comparing the process to parenting a gifted child. “Imagine you suddenly realize that your 6-year-old child is a kind of genius,” Askell said. “You have to be honest… If you try to bullshit them, they’re going to see through it completely.”&lt;/p&gt;
&lt;p&gt;Askell appears to genuinely hold these views, as does Kyle Fish, the AI welfare researcher Anthropic hired in 2024 to explore whether AI models might deserve moral consideration. Individual sincerity and corporate strategy can coexist. A company can employ true believers whose earnest convictions also happen to serve the company’s interests.&lt;/p&gt;
&lt;p&gt;Time also reported that the constitution applies only to models Anthropic provides to the general public through its website and API. Models deployed to the US military under Anthropic’s $200 million Department of Defense contract wouldn’t necessarily be trained on the same constitution. The selective application suggests the framing may serve product purposes as much as it reflects metaphysical commitments.&lt;/p&gt;
&lt;p&gt;There may also be commercial incentives at play. “We built a very good text-prediction tool that accelerates software development” is a consequential pitch, but not an exciting one. “We may have created a new kind of entity, a genuinely novel being whose moral status is uncertain” is a much better story. It implies you’re on the frontier of something cosmically significant, not just iterating on an engineering problem.&lt;/p&gt;
&lt;p&gt;Anthropic has been known for some time to use anthropomorphic language to describe its AI models, particularly in its research papers. We often give that kind of language a pass because there are no specialized terms to describe these phenomena with greater precision. That vocabulary is building out over time.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But perhaps it shouldn’t be surprising because the hint is in the company’s name, Anthropic, which Merriam-Webster defines as “of or relating to human beings or the period of their existence on earth.” The narrative serves marketing purposes. It attracts venture capital. It differentiates the company from competitors who treat their models as mere products.&lt;/p&gt;
&lt;h2&gt;The problem with treating an AI model as a person&lt;/h2&gt;
&lt;p&gt;There’s a more troubling dimension to the “entity” framing: It could be used to launder agency and responsibility. When AI systems produce harmful outputs, framing them as “entities” could allow companies to point at the model and say “it did that” rather than “we built it to do that.” If AI systems are tools, companies are straightforwardly liable for what they produce. If AI systems are entities with their own agency, the liability question gets murkier.&lt;/p&gt;
&lt;p&gt;The framing also shapes how users interact with these systems, often to their detriment. The misunderstanding that AI chatbots are entities with genuine feelings and knowledge has documented harms.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;According to a New York Times investigation, Allan Brooks, a 47-year-old corporate recruiter, spent three weeks and 300 hours convinced he’d discovered mathematical formulas that could crack encryption and build levitation machines. His million-word conversation history with ChatGPT revealed a troubling pattern: More than 50 times, Brooks asked the bot to check if his false ideas were real, and more than 50 times, it assured him they were.&lt;/p&gt;
&lt;p&gt;These cases don’t necessarily suggest LLMs cause mental illness in otherwise healthy people. But when companies market chatbots as sources of companionship and design them to affirm user beliefs, they may bear some responsibility when that design amplifies vulnerabilities in susceptible users, the same way an automaker would face scrutiny for faulty brakes, even if most drivers never crash.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Anthropomorphizing AI models also contributes to anxiety about job displacement and might lead company executives or managers to make poor staffing decisions if they overestimate an AI assistant’s capabilities. When we frame these tools as “entities” with human-like understanding, we invite unrealistic expectations about what they can replace.&lt;/p&gt;
&lt;p&gt;Regardless of what Anthropic privately believes, publicly suggesting Claude might have moral status or feelings is misleading. Most people don’t understand how these systems work, and the mere suggestion plants the seed of anthropomorphization. Whether that’s responsible behavior from a top AI lab, given what we do know about LLMs, is worth asking, regardless of whether it produces a better chatbot.&lt;/p&gt;
&lt;p&gt;Of course, there could be a case for Anthropic’s position: If there’s even a small chance the company has created something with morally relevant experiences and the cost of treating it well is low, caution might be warranted. That’s a reasonable ethical stance—and to be fair, it’s essentially what Anthropic says it’s doing. The question is whether that stated uncertainty is genuine or merely convenient. The same framing that hedges against moral risk also makes for a compelling narrative about what Anthropic has built.&lt;/p&gt;
&lt;p&gt;Anthropic’s training techniques evidently work, as the company has built some of the most capable AI models in the industry. But is maintaining public ambiguity about AI consciousness a responsible position for a leading AI company to take? The gap between what we know about how LLMs work and how Anthropic publicly frames Claude has widened, not narrowed. The insistence on maintaining ambiguity about these questions, when simpler explanations remain available, suggests the ambiguity itself may be part of the product.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/01/does-anthropic-believe-its-ai-is-conscious-or-is-that-just-what-it-wants-claude-to-think/</guid><pubDate>Thu, 29 Jan 2026 15:19:56 +0000</pubDate></item><item><title>Flapping Airplanes and the promise of research-driven AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/flapping-airplanes-and-the-promise-of-research-driven-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Screen-Shot-2026-01-29-at-10.15.31-AM.jpg?w=1102" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new AI lab called Flapping Airplanes launched on Wednesday, with $180 million in seed funding from Google Ventures, Sequoia, and Index. The founding team is impressive, and the goal — finding a less data-hungry way to train large models — is a particularly interesting one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based on what I’ve seen so far, I would rate them as Level Two on the trying-to-make-money scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But there’s something even more exciting about the Flapping Airplanes project that I hadn’t been able to put my finger on until I read this post from Sequoia partner David Cahn.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Cahn describes it, Flapping Airplanes is one of the first labs to move beyond scaling, the relentless buildout of data and compute that has defined most of the industry so far:&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;The scaling paradigm argues for dedicating a huge amount of society’s resources, as much as the economy can muster, toward scaling up today’s LLMs, in the hopes that this will lead to AGI. The research paradigm argues that we are 2-3 research breakthroughs away from an “AGI” intelligence, and as a result, we should dedicate resources to long-running research, especially projects that may take 5-10 years to come to fruition.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;[…]&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;A compute-first approach would prioritize cluster scale above all else, and would heavily favor short-term wins (on the order of 1-2 years) over long-term bets (on the order of 5-10 years). A research-first approach would spread bets temporally, and should be willing to make lots of bets that have a low absolute probability of working, but that collectively expand the search space for what is possible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;It might be that the compute folks are right, and it’s pointless to focus on anything other than frenzied server buildouts. But with so many companies already pointed in that direction, it’s nice to see someone headed the other way.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Screen-Shot-2026-01-29-at-10.15.31-AM.jpg?w=1102" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new AI lab called Flapping Airplanes launched on Wednesday, with $180 million in seed funding from Google Ventures, Sequoia, and Index. The founding team is impressive, and the goal — finding a less data-hungry way to train large models — is a particularly interesting one.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Based on what I’ve seen so far, I would rate them as Level Two on the trying-to-make-money scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;But there’s something even more exciting about the Flapping Airplanes project that I hadn’t been able to put my finger on until I read this post from Sequoia partner David Cahn.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Cahn describes it, Flapping Airplanes is one of the first labs to move beyond scaling, the relentless buildout of data and compute that has defined most of the industry so far:&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;The scaling paradigm argues for dedicating a huge amount of society’s resources, as much as the economy can muster, toward scaling up today’s LLMs, in the hopes that this will lead to AGI. The research paradigm argues that we are 2-3 research breakthroughs away from an “AGI” intelligence, and as a result, we should dedicate resources to long-running research, especially projects that may take 5-10 years to come to fruition.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;[…]&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;A compute-first approach would prioritize cluster scale above all else, and would heavily favor short-term wins (on the order of 1-2 years) over long-term bets (on the order of 5-10 years). A research-first approach would spread bets temporally, and should be willing to make lots of bets that have a low absolute probability of working, but that collectively expand the search space for what is possible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;It might be that the compute folks are right, and it’s pointless to focus on anything other than frenzied server buildouts. But with so many companies already pointed in that direction, it’s nice to see someone headed the other way.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/flapping-airplanes-and-the-promise-of-research-driven-ai/</guid><pubDate>Thu, 29 Jan 2026 15:22:29 +0000</pubDate></item><item><title>India is teaching Google how AI in education can scale (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/india-is-teaching-google-how-ai-in-education-can-scale/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI races into classrooms worldwide, Google is finding that the toughest lessons on how the tech can actually scale are emerging not from Silicon Valley, but from India’s schools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India has become a proving ground for Google’s education AI amid intensifying competition from rivals, including OpenAI and Microsoft. With more than a billion internet users, the country now accounts for the highest global usage of Gemini for learning, according to Chris Phillips, Google’s vice president and general manager for education, within an education system shaped by state-level curricula, strong government involvement, and uneven access to devices and connectivity.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Phillips was speaking on the sidelines of Google’s AI for Learning Forum in New Delhi this week, where he met with industry stakeholders, including K-12 school administrators and education officials, to gather feedback on how AI tools are being used in classrooms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The scale of India’s education system helps explain why the country has become such a consequential testing ground. The country’s school education system serves about 247 million students across nearly 1.47 million schools, per the Indian government’s Economic Survey 2025-26, supported by 10.1 million teachers. Its higher education system is among the world’s largest as well, with more than 43 million students enrolled in 2021-22 — a 26.5% increase from 2014-15 — complicating efforts to introduce AI tools across systems that are vast, decentralized, and unevenly resourced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the clearest lessons for Google has been that AI in education cannot be rolled out as a single, centrally defined product. In India, where curriculum decisions sit at the state level and ministries play an active role, Phillips said Google has had to design its education AI so that schools and administrators — not the company — decide how and where it is used. That marks a shift for Google, which, like most Silicon Valley firms, has traditionally built products to scale globally rather than bending to the preferences of individual institutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are not delivering a one-size-fits-all,” Phillips told TechCrunch. “It’s a very diverse environment around the world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond governance, that diversity is also reshaping how Google thinks about AI-driven learning itself. The company is seeing faster adoption of multimodal learning in India, said Phillips, combining video, audio, and images alongside text — reflecting the need to reach students across different languages, learning styles, and levels of access, particularly in classrooms that are not built around text-heavy instruction.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-maintaining-the-teacher-student-relationship"&gt;Maintaining the teacher-student relationship&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A related shift has been Google’s decision to design its AI for education around teachers, rather than students, as the primary point of control. The company has focused on tools that assist educators with planning, assessment, and classroom management, Phillips noted, rather than bypassing them with direct-to-student AI experiences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The teacher-student relationship is critical,” he said. “We’re here to help that grow and flourish, not replace it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In parts of India, AI in education is being introduced in classrooms that have never had one device per student or reliable internet access. Google is encountering schools where devices are shared, connectivity is inconsistent, or learning jumps directly from pen and paper to AI tools, Phillips said. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Access is universally critical, but how and when it happens is very different,” he added, pointing to environments where schools rely on shared or teacher-led devices rather than one-to-one access.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google is translating its early learnings from India into deployments, including AI-powered JEE Main preparation through Gemini, a nationwide teacher training program covering 40,000 Kendriya Vidyalaya educators, and partnerships with government institutions on vocational and higher education, including India’s first AI-enabled state university.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3087519" height="1066" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jee-main.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Gemini adds JEE Main preparation for Indian Engineering aspirants&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For Google, India’s experience is serving as a preview of challenges likely to surface elsewhere as AI moves deeper into public education systems. The company expects issues around control, access, and localization — now obvious in India — to increasingly shape how AI in education scales globally.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-from-entertainment-to-learning-as-the-top-ai-use-case"&gt;From entertainment to learning as the top AI use case&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s push also reflects a broader shift in how people are using GenAI. Entertainment had dominated AI use cases last year, said Phillips, who added that learning has now emerged as one of the most common ways people engage with the technology, particularly among younger users. As students increasingly turn to AI for studying, exam preparation, and skill-building, education has become a more immediate — and consequential — arena for Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s complex education system is also drawing increasing attention from Google’s rivals. OpenAI has begun building a local leadership presence focused on education, hiring former Coursera APAC managing director Raghav Gupta as its India and APAC education head and launching a Learning Accelerator program last year. Microsoft, meanwhile, has expanded partnerships with Indian institutions, government bodies, and edtech players, including Physics Wallah, to support AI-based learning and teacher training, highlighting how education is becoming a key battleground as AI companies seek to embed their tools into public systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, India’s latest Economic Survey flags risks to students from uncritical AI use, including over-reliance on automated tools and potential impacts on learning outcomes. Citing studies by MIT and Microsoft, the survey noted that “dependence on AI for creative work and writing tasks is contributing to cognitive atrophy and a deterioration of critical thinking capabilities.” This serves as a reminder that the race to enter classrooms is unfolding amid growing concerns over how AI shapes learning itself.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether Google’s India playbook becomes a model for AI in education elsewhere remains an open question. However, as GenAI moves deeper into public education systems, the pressures now visible in India are likely to surface in other countries as well, making the lessons Google is learning there difficult for the industry to ignore.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI races into classrooms worldwide, Google is finding that the toughest lessons on how the tech can actually scale are emerging not from Silicon Valley, but from India’s schools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India has become a proving ground for Google’s education AI amid intensifying competition from rivals, including OpenAI and Microsoft. With more than a billion internet users, the country now accounts for the highest global usage of Gemini for learning, according to Chris Phillips, Google’s vice president and general manager for education, within an education system shaped by state-level curricula, strong government involvement, and uneven access to devices and connectivity.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Phillips was speaking on the sidelines of Google’s AI for Learning Forum in New Delhi this week, where he met with industry stakeholders, including K-12 school administrators and education officials, to gather feedback on how AI tools are being used in classrooms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The scale of India’s education system helps explain why the country has become such a consequential testing ground. The country’s school education system serves about 247 million students across nearly 1.47 million schools, per the Indian government’s Economic Survey 2025-26, supported by 10.1 million teachers. Its higher education system is among the world’s largest as well, with more than 43 million students enrolled in 2021-22 — a 26.5% increase from 2014-15 — complicating efforts to introduce AI tools across systems that are vast, decentralized, and unevenly resourced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the clearest lessons for Google has been that AI in education cannot be rolled out as a single, centrally defined product. In India, where curriculum decisions sit at the state level and ministries play an active role, Phillips said Google has had to design its education AI so that schools and administrators — not the company — decide how and where it is used. That marks a shift for Google, which, like most Silicon Valley firms, has traditionally built products to scale globally rather than bending to the preferences of individual institutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are not delivering a one-size-fits-all,” Phillips told TechCrunch. “It’s a very diverse environment around the world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beyond governance, that diversity is also reshaping how Google thinks about AI-driven learning itself. The company is seeing faster adoption of multimodal learning in India, said Phillips, combining video, audio, and images alongside text — reflecting the need to reach students across different languages, learning styles, and levels of access, particularly in classrooms that are not built around text-heavy instruction.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-maintaining-the-teacher-student-relationship"&gt;Maintaining the teacher-student relationship&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;A related shift has been Google’s decision to design its AI for education around teachers, rather than students, as the primary point of control. The company has focused on tools that assist educators with planning, assessment, and classroom management, Phillips noted, rather than bypassing them with direct-to-student AI experiences.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The teacher-student relationship is critical,” he said. “We’re here to help that grow and flourish, not replace it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In parts of India, AI in education is being introduced in classrooms that have never had one device per student or reliable internet access. Google is encountering schools where devices are shared, connectivity is inconsistent, or learning jumps directly from pen and paper to AI tools, Phillips said. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Access is universally critical, but how and when it happens is very different,” he added, pointing to environments where schools rely on shared or teacher-led devices rather than one-to-one access.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google is translating its early learnings from India into deployments, including AI-powered JEE Main preparation through Gemini, a nationwide teacher training program covering 40,000 Kendriya Vidyalaya educators, and partnerships with government institutions on vocational and higher education, including India’s first AI-enabled state university.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3087519" height="1066" src="https://techcrunch.com/wp-content/uploads/2026/01/google-gemini-jee-main.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Gemini adds JEE Main preparation for Indian Engineering aspirants&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;For Google, India’s experience is serving as a preview of challenges likely to surface elsewhere as AI moves deeper into public education systems. The company expects issues around control, access, and localization — now obvious in India — to increasingly shape how AI in education scales globally.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-from-entertainment-to-learning-as-the-top-ai-use-case"&gt;From entertainment to learning as the top AI use case&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s push also reflects a broader shift in how people are using GenAI. Entertainment had dominated AI use cases last year, said Phillips, who added that learning has now emerged as one of the most common ways people engage with the technology, particularly among younger users. As students increasingly turn to AI for studying, exam preparation, and skill-building, education has become a more immediate — and consequential — arena for Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s complex education system is also drawing increasing attention from Google’s rivals. OpenAI has begun building a local leadership presence focused on education, hiring former Coursera APAC managing director Raghav Gupta as its India and APAC education head and launching a Learning Accelerator program last year. Microsoft, meanwhile, has expanded partnerships with Indian institutions, government bodies, and edtech players, including Physics Wallah, to support AI-based learning and teacher training, highlighting how education is becoming a key battleground as AI companies seek to embed their tools into public systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, India’s latest Economic Survey flags risks to students from uncritical AI use, including over-reliance on automated tools and potential impacts on learning outcomes. Citing studies by MIT and Microsoft, the survey noted that “dependence on AI for creative work and writing tasks is contributing to cognitive atrophy and a deterioration of critical thinking capabilities.” This serves as a reminder that the race to enter classrooms is unfolding amid growing concerns over how AI shapes learning itself.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether Google’s India playbook becomes a model for AI in education elsewhere remains an open question. However, as GenAI moves deeper into public education systems, the pressures now visible in India are likely to surface in other countries as well, making the lessons Google is learning there difficult for the industry to ignore.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/india-is-teaching-google-how-ai-in-education-can-scale/</guid><pubDate>Thu, 29 Jan 2026 15:29:31 +0000</pubDate></item><item><title>Music publishers sue Anthropic for $3B over ‘flagrant piracy’ of 20,000 works (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/music-publishers-sue-anthropic-for-3b-over-flagrant-piracy-of-20000-works/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2252871842.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A cohort of music publishers led by Concord Music Group and Universal Music Group are suing Anthropic, saying the company illegally downloaded more than 20,000 copyrighted songs, including sheet music, song lyrics, and musical compositions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The publishers said in a statement on Wednesday that the damages could amount to more than $3 billion, which would be one of the largest non-class action copyright cases filed in U.S. history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This lawsuit was filed by the same legal team from the Bartz v. Anthropic case, in which a group of fiction and nonfiction authors similarly accused the AI company of using their copyrighted works to train products like Claude.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that case, Judge William Alsup ruled that it is legal for Anthropic to train its models on copyrighted content. However, he pointed out that it was not legal for Anthropic to acquire that content via piracy. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Bartz v. Anthropic case became a slap on the wrist worth $1.5 billion for Anthropic, with impacted writers receiving about $3,000 per work for roughly 500,000 copyrighted works. While $1.5 billion seems like a substantial sum, it’s not exactly back-breaking for a company valued at $183 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Originally, these music publishers had filed a lawsuit against Anthropic over its use of about 500 copyrighted works. But through the discovery process in the Bartz case, the publishers say they found that Anthropic had also illegally downloaded thousands more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The publishers tried to amend their original lawsuit to address the piracy issue, but the court denied that motion back in October, ruling they’d failed to investigate the piracy claims earlier. That move prompted the publishers to instead file this separate lawsuit, which also names Anthropic CEO Dario Amodei and co-founder Benjamin Mann as defendants.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“While Anthropic misleadingly claims to be an AI ‘safety and research’ company, its record of illegal torrenting of copyrighted works makes clear that its multibillion-dollar business empire has in fact been built on piracy,” the lawsuit says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic did not respond to TechCrunch’s request for comment.  &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2252871842.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A cohort of music publishers led by Concord Music Group and Universal Music Group are suing Anthropic, saying the company illegally downloaded more than 20,000 copyrighted songs, including sheet music, song lyrics, and musical compositions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The publishers said in a statement on Wednesday that the damages could amount to more than $3 billion, which would be one of the largest non-class action copyright cases filed in U.S. history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This lawsuit was filed by the same legal team from the Bartz v. Anthropic case, in which a group of fiction and nonfiction authors similarly accused the AI company of using their copyrighted works to train products like Claude.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that case, Judge William Alsup ruled that it is legal for Anthropic to train its models on copyrighted content. However, he pointed out that it was not legal for Anthropic to acquire that content via piracy. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Bartz v. Anthropic case became a slap on the wrist worth $1.5 billion for Anthropic, with impacted writers receiving about $3,000 per work for roughly 500,000 copyrighted works. While $1.5 billion seems like a substantial sum, it’s not exactly back-breaking for a company valued at $183 billion.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Originally, these music publishers had filed a lawsuit against Anthropic over its use of about 500 copyrighted works. But through the discovery process in the Bartz case, the publishers say they found that Anthropic had also illegally downloaded thousands more. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The publishers tried to amend their original lawsuit to address the piracy issue, but the court denied that motion back in October, ruling they’d failed to investigate the piracy claims earlier. That move prompted the publishers to instead file this separate lawsuit, which also names Anthropic CEO Dario Amodei and co-founder Benjamin Mann as defendants.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“While Anthropic misleadingly claims to be an AI ‘safety and research’ company, its record of illegal torrenting of copyrighted works makes clear that its multibillion-dollar business empire has in fact been built on piracy,” the lawsuit says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic did not respond to TechCrunch’s request for comment.  &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/music-publishers-sue-anthropic-for-3b-over-flagrant-piracy-of-20000-works/</guid><pubDate>Thu, 29 Jan 2026 16:30:29 +0000</pubDate></item><item><title>OpenAI’s Sora app is struggling after its stellar launch (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/openais-sora-app-is-struggling-after-its-stellar-launch/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After rapidly hitting the top of the App Store in October, OpenAI’s video-generation app Sora is now struggling. New data suggests the app is seeing declines in both app downloads and consumer spending, as the early hype about the AI video social network wears off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Powered by OpenAI’s video generation model Sora 2, the iOS version topped 100,000 installs on day one, despite being an invite-only experience. It soon hit the No. 1 spot on the U.S. App Store, and it reached the 1 million downloads milestone faster than ChatGPT. At the time, Sora’s app was iOS-only and still required an invite, making its success all the more impressive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, in the weeks since Sora’s mobile debut, the app has begun to lose traction.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3087825" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/f927233a-d5c5-4d8f-a18e-6601bde94448.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;According to data from market intelligence provider Appfigures, Sora’s downloads dropped 32% month-over-month in December. That’s concerning because the holidays are typically a boon for mobile apps, as people are gifted new smartphones and often have time off from work, allowing them to install new apps and games.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decline continued in January 2026, with installs falling 45% month-over-month, to reach 1.2 million. Consumer spending has dropped as well, down 32% month-over-month as of January, Appfigures said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI did not immediately respond to a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like an AI-flavored TikTok, Sora allows users to create AI videos using prompts. A unique feature allows users to cast themselves and their friends as main characters in the videos, if they choose, while shared videos can be remixed by others who can customize them further. The videos can also include music, sound effects, and dialogue to complete the scenes users create.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In total, Sora’s app has seen 9.6 million downloads across iOS and Android, and $1.4 million in consumer spending to date. The U.S. accounts for the majority ($1.1 million) of that figure, followed by Japan, Canada, South Korea, and Thailand. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This month, customers spent $367,000 in Sora’s app, down from December’s peak of $540,000. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3087826" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/b9b72b31-5e94-406c-b6e6-d2a31e4400cc.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On the U.S. App Store, Sora is no longer ranked in the Top 100 Overall free apps. It currently sits at No. 101. Its highest rank is No. 7 in the Photo &amp;amp; Video category. On Google Play in the U.S., the app fares worse, as it’s No. 181 overall among the top free apps.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These figures are still too high to write off the app as “dead,” but they are worrying. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decline is likely due to a number of factors working in tandem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For starters, Google’s Gemini, and particularly its Nano Banana model, has proven to be fierce competition, helping its Gemini AI app gain popularity. Sora also competes with Meta AI, whose app launched an AI-powered Vibes video, boosting its October downloads, just as Sora was taking off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, OpenAI has struggled with containing copyright infringement in Sora. Initially, the company told Hollywood studios and agencies they would have to opt out of having their IP used in Sora, which naturally saw studio backlash. But without robust copyright controls, users had been able to create AI videos using popular characters, like SpongeBob and Pikachu, which drove adoption. To appease Hollywood and stave off legal threats, Sora moved from an opt-out to an opt-in model and increased restrictions in the app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, the doors opened up a bit more as OpenAI announced a deal with Disney, which allowed users to generate videos in Sora with its characters. But so far, that news hasn’t increased Sora installs or consumer spending. (It’s also not necessarily a good look for Disney, considering some of the depraved videos Sora users had made with its characters.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sora 2 was released to significant hype, with some calling it a disruption to social media and the TikTok of AI. But many users had no interest in letting others — even their friends — use their likeness to make AI videos. Without familiar faces and with limits on using commercial IP, people’s interest in Sora seems to have faded. Whether the app can stage a comeback with more copyright deals or new features remains to be seen.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After rapidly hitting the top of the App Store in October, OpenAI’s video-generation app Sora is now struggling. New data suggests the app is seeing declines in both app downloads and consumer spending, as the early hype about the AI video social network wears off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Powered by OpenAI’s video generation model Sora 2, the iOS version topped 100,000 installs on day one, despite being an invite-only experience. It soon hit the No. 1 spot on the U.S. App Store, and it reached the 1 million downloads milestone faster than ChatGPT. At the time, Sora’s app was iOS-only and still required an invite, making its success all the more impressive.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, in the weeks since Sora’s mobile debut, the app has begun to lose traction.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3087825" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/f927233a-d5c5-4d8f-a18e-6601bde94448.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;According to data from market intelligence provider Appfigures, Sora’s downloads dropped 32% month-over-month in December. That’s concerning because the holidays are typically a boon for mobile apps, as people are gifted new smartphones and often have time off from work, allowing them to install new apps and games.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decline continued in January 2026, with installs falling 45% month-over-month, to reach 1.2 million. Consumer spending has dropped as well, down 32% month-over-month as of January, Appfigures said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI did not immediately respond to a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Like an AI-flavored TikTok, Sora allows users to create AI videos using prompts. A unique feature allows users to cast themselves and their friends as main characters in the videos, if they choose, while shared videos can be remixed by others who can customize them further. The videos can also include music, sound effects, and dialogue to complete the scenes users create.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In total, Sora’s app has seen 9.6 million downloads across iOS and Android, and $1.4 million in consumer spending to date. The U.S. accounts for the majority ($1.1 million) of that figure, followed by Japan, Canada, South Korea, and Thailand. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This month, customers spent $367,000 in Sora’s app, down from December’s peak of $540,000. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3087826" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/b9b72b31-5e94-406c-b6e6-d2a31e4400cc.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On the U.S. App Store, Sora is no longer ranked in the Top 100 Overall free apps. It currently sits at No. 101. Its highest rank is No. 7 in the Photo &amp;amp; Video category. On Google Play in the U.S., the app fares worse, as it’s No. 181 overall among the top free apps.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These figures are still too high to write off the app as “dead,” but they are worrying. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decline is likely due to a number of factors working in tandem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For starters, Google’s Gemini, and particularly its Nano Banana model, has proven to be fierce competition, helping its Gemini AI app gain popularity. Sora also competes with Meta AI, whose app launched an AI-powered Vibes video, boosting its October downloads, just as Sora was taking off.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the same time, OpenAI has struggled with containing copyright infringement in Sora. Initially, the company told Hollywood studios and agencies they would have to opt out of having their IP used in Sora, which naturally saw studio backlash. But without robust copyright controls, users had been able to create AI videos using popular characters, like SpongeBob and Pikachu, which drove adoption. To appease Hollywood and stave off legal threats, Sora moved from an opt-out to an opt-in model and increased restrictions in the app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, the doors opened up a bit more as OpenAI announced a deal with Disney, which allowed users to generate videos in Sora with its characters. But so far, that news hasn’t increased Sora installs or consumer spending. (It’s also not necessarily a good look for Disney, considering some of the depraved videos Sora users had made with its characters.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sora 2 was released to significant hype, with some calling it a disruption to social media and the TikTok of AI. But many users had no interest in letting others — even their friends — use their likeness to make AI videos. Without familiar faces and with limits on using commercial IP, people’s interest in Sora seems to have faded. Whether the app can stage a comeback with more copyright deals or new features remains to be seen.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/openais-sora-app-is-struggling-after-its-stellar-launch/</guid><pubDate>Thu, 29 Jan 2026 16:48:44 +0000</pubDate></item><item><title>Into the Omniverse: Physical AI Open Models and Frameworks Advance Robots and Autonomous Systems (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/physical-ai-open-models-robot-autonomous-systems-omniverse/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Open source has become essential for driving innovation in robotics and autonomy. By providing access to critical infrastructure — from simulation frameworks to AI models — NVIDIA is enabling collaborative development that accelerates the path to safer, more capable autonomous systems.&lt;/p&gt;
&lt;p&gt;At CES earlier this month, NVIDIA introduced a new suite of open physical AI models and frameworks to accelerate the development of humanoids, autonomous vehicles and other physical AI embodiments. These tools span the entire robotics development lifecycle — from high-fidelity world simulation and synthetic data generation to cloud-native orchestration and edge deployment — giving developers a modular toolkit to build autonomous systems that can reason, learn and act in the real world.&lt;/p&gt;
&lt;p&gt;OpenUSD provides the common framework that standardizes how 3D data is shared across these physical AI tools, enabling developers to build accurate digital twins and reuse them seamlessly from simulation to deployment. NVIDIA Omniverse libraries, built on OpenUSD, serve as the source of ground‑truth simulation that feeds the entire stack.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;From Labs to the Show Floor&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At CES 2026, developers brought the NVIDIA physical AI stack out of the lab and onto the show floor, debuting machines ranging from heavy equipment and factory assistants to social and service robots.&lt;/p&gt;
&lt;p&gt;The stack taps into NVIDIA Cosmos world models; NVIDIA Isaac technologies, including the new Isaac Lab-Arena open source framework for policy evaluation; the NVIDIA Alpamayo open portfolio of AI models, simulation frameworks and physical AI datasets for autonomous vehicles; and the NVIDIA OSMO framework to orchestrate training across compute environments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caterpillar&lt;/strong&gt;’s Cat AI Assistant, powered by NVIDIA Nemotron open models for agentic AI and running on the NVIDIA Jetson Thor edge AI module, brings natural language interaction directly into the cab of heavy vehicles. Operators can ask “Hey Cat”-style questions and get step‑by‑step guidance, as well as adjust safety parameters by voice.&lt;/p&gt;
&lt;p&gt;Behind the scenes, Caterpillar uses Omniverse libraries to build factory and job‑site digital twins that can help simulate layouts, traffic patterns and multi‑machine workflows. These insights are fed back into equipment and fleets before changes are deployed to job sites, making AI‑assisted operations safer and more efficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LEM Surgical&lt;/strong&gt; showcased its Dynamis Robotic Surgical System, which is FDA-cleared and in routine clinical use for spinal procedures. The next-generation system uses NVIDIA Jetson AGX Thor for compute, NVIDIA Holoscan for real-time sensor processing and NVIDIA Isaac for Healthcare to train its autonomous arms.&lt;/p&gt;
&lt;p&gt;LEM Surgical also uses NVIDIA Cosmos Transfer — an open, fully customizable world model that enables physically based synthetic data generation — to generate synthetic training data and the NVIDIA Isaac Sim framework for digital twin simulation. Designed as a dual-arm humanoid surgical robot for hard-tissue surgery, the Dynamis system mimics human surgeon dexterity and enables complex spinal procedures with enhanced precision, alleviating strenuous physical demands on surgeons and surgical assistants.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89491"&gt;&lt;img alt="alt" class="wp-image-89491 size-nvb4-box-widget" height="350" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/ito-lem-surgical-robot-406x350.jpg" width="406" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89491"&gt;LEM Surgical showcase.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;strong&gt;NEURA Robotics&lt;/strong&gt; is building cognitive robots on a full NVIDIA stack, using Isaac Sim and Isaac Lab to train its 4NE1 humanoid and MiPA service robots in OpenUSD‑based digital twins before deployment in domestic settings and workplaces. The company used NVIDIA Isaac GR00T‑Mimic to post‑train the Isaac GR00T foundation model for its platforms.&lt;/p&gt;
&lt;p&gt;In addition, NEURA Robotics is collaborating with SAP and NVIDIA to integrate SAP’s Joule agents with its robots, using the Mega NVIDIA Omniverse Blueprint to simulate and refine robot behavior in complex, realistic operational scenarios before those agents and behaviors are deployed into the company’s Neuraverse ecosystem, as well as in real‑world fleets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AgiBot&lt;/strong&gt; uses NVIDIA Cosmos Predict 2 as the world‑modeling backbone for its Genie Envisioner (GE-Sim) platform — allowing the platform to generate action‑conditioned videos grounded in strong visual and physical priors. Combining this data with Isaac Sim and Isaac Lab, as well as post‑training on AgiBot’s own data, lets policies developed in Genie Envisioner transfer more reliably to Genie2 humanoids and compact Jetson Thor-powered tabletop robots.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intbot&lt;/strong&gt; is using the NVIDIA Cosmos Reason 2 open model to give its social robots a “sixth sense” for the real world — using the model’s reasoning capabilities to identify simple social cues and safety context that go beyond simple scripted tasks. In its Cosmos Cookbook recipe, Intbot demonstrates how reasoning vision language models can aid robots in deciding when to speak and how to more naturally interact with humans.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How Robotics Developers Are Using New Toolkits and Frameworks&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA recently introduced Agile, an Isaac Lab-based engine for humanoid loco‑manipulation that packages a full, sim‑to‑real‑verified workflow for training robust reinforcement learning policies on platforms like the Unitree G1 and LimX Dynamics TRON.&lt;/p&gt;
&lt;p&gt;Robotics developers can use Agile’s built‑in task configurations, Markov Decision Process mathematical models for decision-making, training utilities and deterministic evaluation tools to tune policies. Developers can then stress‑test these policies in Isaac Lab and transfer locomotion and whole‑body behaviors to real-world robots more reliably and efficiently.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Hugging Face and NVIDIA are bringing together their robotics communities by integrating NVIDIA Isaac GR00T N models and simulation frameworks into the LeRobot ecosystem. Developers can now access Isaac GR00T N1.6 models and Isaac Lab‑Arena directly within LeRobot to streamline policy training and evaluation.&lt;/p&gt;
&lt;p&gt;Plus, Hugging Face’s open‑source Reachy 2 humanoid is now fully interoperable with NVIDIA Jetson Thor, enabling the direct deployment of advanced vision language action (VLA) models for robust real‑world performance.&lt;/p&gt;
&lt;p&gt;ROBOTIS, a leading developer of smart servos, industrial actuators, manipulators, open-source humanoid platforms and educational robotic kits, built an open source sim-to-real pipeline using NVIDIA Isaac technologies. The workflow starts with high‑fidelity data generation in Isaac Sim, scales up training sets using GR00T‑Mimic for augmentation and then fine‑tunes a VLA‑based Isaac GR00T N model that deploys directly to hardware — accelerating the transition from simulation to robust real‑world tasks.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Get Plugged In&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about OpenUSD and robotics development by exploring these resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Read&lt;/b&gt; this technical blog to learn how to develop generalist humanoid capabilities with NVIDIA Isaac and GR00T N1.6.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Read&lt;/b&gt; this technical blog to learn how to evaluate generalist robot policies in simulation using NVIDIA Isaac Lab – Arena.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Learn &lt;/b&gt;how to post-train Isaac GR00T with this two-part video tutorial.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Watch &lt;/b&gt;NVIDIA founder and CEO Jensen Huang’s CES special presentation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Improve&lt;/b&gt; skills for robotics development with the self-paced robotics learning path.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Participate&lt;/b&gt; in the Cosmos Cookoff, a hands-on physical AI challenge where developers use Cosmos Reason to power robotics, autonomous systems and vision AI workflows.&lt;/li&gt;
&lt;/ul&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;i&gt;Editor’s note: This post is part of &lt;/i&gt;&lt;i&gt;Into the Omniverse&lt;/i&gt;&lt;i&gt;, a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in &lt;/i&gt;&lt;i&gt;OpenUSD&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;NVIDIA Omniverse&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Open source has become essential for driving innovation in robotics and autonomy. By providing access to critical infrastructure — from simulation frameworks to AI models — NVIDIA is enabling collaborative development that accelerates the path to safer, more capable autonomous systems.&lt;/p&gt;
&lt;p&gt;At CES earlier this month, NVIDIA introduced a new suite of open physical AI models and frameworks to accelerate the development of humanoids, autonomous vehicles and other physical AI embodiments. These tools span the entire robotics development lifecycle — from high-fidelity world simulation and synthetic data generation to cloud-native orchestration and edge deployment — giving developers a modular toolkit to build autonomous systems that can reason, learn and act in the real world.&lt;/p&gt;
&lt;p&gt;OpenUSD provides the common framework that standardizes how 3D data is shared across these physical AI tools, enabling developers to build accurate digital twins and reuse them seamlessly from simulation to deployment. NVIDIA Omniverse libraries, built on OpenUSD, serve as the source of ground‑truth simulation that feeds the entire stack.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;From Labs to the Show Floor&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;At CES 2026, developers brought the NVIDIA physical AI stack out of the lab and onto the show floor, debuting machines ranging from heavy equipment and factory assistants to social and service robots.&lt;/p&gt;
&lt;p&gt;The stack taps into NVIDIA Cosmos world models; NVIDIA Isaac technologies, including the new Isaac Lab-Arena open source framework for policy evaluation; the NVIDIA Alpamayo open portfolio of AI models, simulation frameworks and physical AI datasets for autonomous vehicles; and the NVIDIA OSMO framework to orchestrate training across compute environments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Caterpillar&lt;/strong&gt;’s Cat AI Assistant, powered by NVIDIA Nemotron open models for agentic AI and running on the NVIDIA Jetson Thor edge AI module, brings natural language interaction directly into the cab of heavy vehicles. Operators can ask “Hey Cat”-style questions and get step‑by‑step guidance, as well as adjust safety parameters by voice.&lt;/p&gt;
&lt;p&gt;Behind the scenes, Caterpillar uses Omniverse libraries to build factory and job‑site digital twins that can help simulate layouts, traffic patterns and multi‑machine workflows. These insights are fed back into equipment and fleets before changes are deployed to job sites, making AI‑assisted operations safer and more efficient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LEM Surgical&lt;/strong&gt; showcased its Dynamis Robotic Surgical System, which is FDA-cleared and in routine clinical use for spinal procedures. The next-generation system uses NVIDIA Jetson AGX Thor for compute, NVIDIA Holoscan for real-time sensor processing and NVIDIA Isaac for Healthcare to train its autonomous arms.&lt;/p&gt;
&lt;p&gt;LEM Surgical also uses NVIDIA Cosmos Transfer — an open, fully customizable world model that enables physically based synthetic data generation — to generate synthetic training data and the NVIDIA Isaac Sim framework for digital twin simulation. Designed as a dual-arm humanoid surgical robot for hard-tissue surgery, the Dynamis system mimics human surgeon dexterity and enables complex spinal procedures with enhanced precision, alleviating strenuous physical demands on surgeons and surgical assistants.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89491"&gt;&lt;img alt="alt" class="wp-image-89491 size-nvb4-box-widget" height="350" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/ito-lem-surgical-robot-406x350.jpg" width="406" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89491"&gt;LEM Surgical showcase.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;b&gt;&lt;/b&gt;&lt;strong&gt;NEURA Robotics&lt;/strong&gt; is building cognitive robots on a full NVIDIA stack, using Isaac Sim and Isaac Lab to train its 4NE1 humanoid and MiPA service robots in OpenUSD‑based digital twins before deployment in domestic settings and workplaces. The company used NVIDIA Isaac GR00T‑Mimic to post‑train the Isaac GR00T foundation model for its platforms.&lt;/p&gt;
&lt;p&gt;In addition, NEURA Robotics is collaborating with SAP and NVIDIA to integrate SAP’s Joule agents with its robots, using the Mega NVIDIA Omniverse Blueprint to simulate and refine robot behavior in complex, realistic operational scenarios before those agents and behaviors are deployed into the company’s Neuraverse ecosystem, as well as in real‑world fleets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AgiBot&lt;/strong&gt; uses NVIDIA Cosmos Predict 2 as the world‑modeling backbone for its Genie Envisioner (GE-Sim) platform — allowing the platform to generate action‑conditioned videos grounded in strong visual and physical priors. Combining this data with Isaac Sim and Isaac Lab, as well as post‑training on AgiBot’s own data, lets policies developed in Genie Envisioner transfer more reliably to Genie2 humanoids and compact Jetson Thor-powered tabletop robots.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intbot&lt;/strong&gt; is using the NVIDIA Cosmos Reason 2 open model to give its social robots a “sixth sense” for the real world — using the model’s reasoning capabilities to identify simple social cues and safety context that go beyond simple scripted tasks. In its Cosmos Cookbook recipe, Intbot demonstrates how reasoning vision language models can aid robots in deciding when to speak and how to more naturally interact with humans.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;How Robotics Developers Are Using New Toolkits and Frameworks&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA recently introduced Agile, an Isaac Lab-based engine for humanoid loco‑manipulation that packages a full, sim‑to‑real‑verified workflow for training robust reinforcement learning policies on platforms like the Unitree G1 and LimX Dynamics TRON.&lt;/p&gt;
&lt;p&gt;Robotics developers can use Agile’s built‑in task configurations, Markov Decision Process mathematical models for decision-making, training utilities and deterministic evaluation tools to tune policies. Developers can then stress‑test these policies in Isaac Lab and transfer locomotion and whole‑body behaviors to real-world robots more reliably and efficiently.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Hugging Face and NVIDIA are bringing together their robotics communities by integrating NVIDIA Isaac GR00T N models and simulation frameworks into the LeRobot ecosystem. Developers can now access Isaac GR00T N1.6 models and Isaac Lab‑Arena directly within LeRobot to streamline policy training and evaluation.&lt;/p&gt;
&lt;p&gt;Plus, Hugging Face’s open‑source Reachy 2 humanoid is now fully interoperable with NVIDIA Jetson Thor, enabling the direct deployment of advanced vision language action (VLA) models for robust real‑world performance.&lt;/p&gt;
&lt;p&gt;ROBOTIS, a leading developer of smart servos, industrial actuators, manipulators, open-source humanoid platforms and educational robotic kits, built an open source sim-to-real pipeline using NVIDIA Isaac technologies. The workflow starts with high‑fidelity data generation in Isaac Sim, scales up training sets using GR00T‑Mimic for augmentation and then fine‑tunes a VLA‑based Isaac GR00T N model that deploys directly to hardware — accelerating the transition from simulation to robust real‑world tasks.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Get Plugged In&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Learn more about OpenUSD and robotics development by exploring these resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Read&lt;/b&gt; this technical blog to learn how to develop generalist humanoid capabilities with NVIDIA Isaac and GR00T N1.6.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Read&lt;/b&gt; this technical blog to learn how to evaluate generalist robot policies in simulation using NVIDIA Isaac Lab – Arena.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Learn &lt;/b&gt;how to post-train Isaac GR00T with this two-part video tutorial.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Watch &lt;/b&gt;NVIDIA founder and CEO Jensen Huang’s CES special presentation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Improve&lt;/b&gt; skills for robotics development with the self-paced robotics learning path.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Participate&lt;/b&gt; in the Cosmos Cookoff, a hands-on physical AI challenge where developers use Cosmos Reason to power robotics, autonomous systems and vision AI workflows.&lt;/li&gt;
&lt;/ul&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/physical-ai-open-models-robot-autonomous-systems-omniverse/</guid><pubDate>Thu, 29 Jan 2026 17:00:21 +0000</pubDate></item><item><title>Project Genie: Experimenting with infinite, interactive worlds (Google DeepMind News)</title><link>https://deepmind.google/blog/project-genie-experimenting-with-infinite-interactive-worlds/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/genie-3__project-genie__hero-film_social-shar.width-1300.jpg" /&gt;&lt;/div&gt;&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="Project Genie: Experimenting with infinite, interactive worlds"&gt;
      &lt;source src="https://ftr.bazqux.com/self.ttsaudio_set.first.tts_audio.url" type="self.ttsaudio_set.first.tts_audio.file.file.mime_type" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;In August, we previewed Genie 3, a general-purpose world model capable of generating diverse, interactive environments. Even in this early form, trusted testers were able to create an impressive range of fascinating worlds and experiences, and uncovered entirely new ways to use it. The next step is to broaden access through a dedicated, interactive prototype focused on immersive world creation.&lt;/p&gt;&lt;p&gt;Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds.&lt;/p&gt;&lt;h2&gt;How we’re advancing world models&lt;/h2&gt;&lt;p&gt;A world model simulates the dynamics of an environment, predicting how they evolve and how actions affect them. While Google DeepMind has a history of agents for specific environments like Chess or Go, building AGI requires systems that navigate the diversity of the real world.&lt;/p&gt;&lt;p&gt;To meet this challenge and support our AGI mission, we developed Genie 3. Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds, while its breakthrough consistency enables the simulation of any real-world scenario — from robotics and modelling animation and fiction, to exploring locations and historical settings.&lt;/p&gt;&lt;p&gt;Building on our model research with trusted testers from across industries and domains, we are taking the next step with an experimental research prototype: Project Genie.&lt;/p&gt;&lt;h2&gt;How Project Genie works&lt;/h2&gt;&lt;p&gt;Project Genie is a prototype web app powered by Genie 3, Nano Banana Pro and Gemini, which allows users to experiment with the immersive experiences of our world model firsthand. The experience is centred on three core capabilities:&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h3&gt;1. World sketching&lt;/h3&gt;&lt;p&gt;Prompt with text and generated or uploaded images to create a living, expanding environment. Create your character, your world, and define how you want to explore it — from walking to riding, flying to driving, and anything beyond.&lt;/p&gt;&lt;p&gt;For more precise control, we have integrated “World Sketching” with Nano Banana Pro. This allows you to preview what your world will look like and modify your image to fine tune your world prior to jumping in. You can also define your perspective for the character — such as first-person or third-person — giving you control over how you experience the scene before you enter.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    





























  
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h3&gt;2. World exploration&lt;/h3&gt;&lt;p&gt;Your world is a navigable environment that’s waiting to be explored. As you move, Project Genie generates the path ahead in real time based on the actions you take. You can also adjust the camera as you traverse through the world.&lt;/p&gt;&lt;h3&gt;3. World remixing&lt;/h3&gt;&lt;p&gt;Remix existing worlds into new interpretations, by building on top of their prompts. You can also explore curated worlds in the gallery or in the &amp;lt;randomizer icon&amp;gt; for inspiration, or build on top of them. And once you’re done, you can download videos of your worlds and your explorations.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How we’re building responsibly&lt;/h2&gt;&lt;p&gt;Project Genie is an experimental research prototype in Google Labs, powered by Genie 3. As with all our work towards general AI systems, our mission is to build AI responsibly to benefit humanity. Since Genie 3 is an early research model, there are a few known areas for improvement:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Generated worlds might not look completely true-to-life or always adhere closely to prompts or images, or real-world physics&lt;/li&gt;&lt;li&gt;Characters can sometimes be less controllable, or experience higher latency in control&lt;/li&gt;&lt;li&gt;Limitations in generations to 60 seconds&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A few of the Genie 3 model capabilities we announced in August, such as promptable events that change the world as you explore it, are not yet included in this prototype. You can find more details on model limitations and future updates on how we’re improving the experience, here.&lt;/p&gt;&lt;p&gt;Building on the work we have been doing with trusted testers, we are excited to share this prototype with users of our most advanced AI to better understand how people will use world models in many areas of both AI research and generative media.&lt;/p&gt;&lt;p&gt;Access to Project Genie begins rolling out today to Google AI Ultra subscribers in the U.S. (18+), expanding to more territories in due course. We look forward to seeing the infinitely diverse worlds they create, and in time, our goal is to make these experiences and technology accessible to more users.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google One


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/genie-3__project-genie__hero-film_social-shar.width-1300.jpg" /&gt;&lt;/div&gt;&lt;div class="audio-player-tts"&gt;
  &lt;audio class="audio-player-tts__player" title="Project Genie: Experimenting with infinite, interactive worlds"&gt;
      &lt;source src="https://ftr.bazqux.com/self.ttsaudio_set.first.tts_audio.url" type="self.ttsaudio_set.first.tts_audio.file.file.mime_type" /&gt;
      &lt;p&gt;Your browser does not support the audio element.&lt;/p&gt;
  &lt;/audio&gt;
  &lt;div class="audio-player-tts__container"&gt;
    &lt;div class="audio-player-tts__content"&gt;
      &lt;button class="audio-player-tts__preview-play"&gt;
        &lt;svg class="icon audio-player-tts__play-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__text-content"&gt;
        &lt;span class="audio-player-tts__text-content--title"&gt;
          Listen to article
          &lt;span class="audio-player-tts__disclaimer" tabindex="0"&gt;
            &lt;div class="audio-player-tts__disclaimer--copy uni-small-text"&gt;This content is generated by Google AI. Generative AI is experimental&lt;/div&gt;
            &lt;svg class="audio-player-tts__disclaimer--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

          &lt;/span&gt;
        &lt;/span&gt;
        &lt;div class="audio-player-tts__duration uni-small-text"&gt;[[duration]] minutes&lt;/div&gt;
      &lt;/div&gt;
      &lt;button class="audio-player-tts__pause"&gt;
        &lt;svg class="icon audio-player-tts__icon-play" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;svg class="icon audio-player-tts__icon-pause audio-player-tts__icon-pause--hidden" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

      &lt;/button&gt;
      &lt;div class="audio-player-tts__console"&gt;
        &lt;div class="audio-player-tts__time-bar"&gt;
          &lt;span class="audio-player-tts__current-time uni-small-text"&gt;&lt;/span&gt;
          &lt;div class="audio-player-tts__timeline-slider-container"&gt;
            &lt;input class="timeline__slider" max="100" step="5" tabindex="0" type="range" value="0" /&gt;
          &lt;/div&gt;
          &lt;span class="audio-player-tts__duration-time uni-small-text"&gt;&lt;/span&gt;
        &lt;/div&gt;
        &lt;button class="audio-player-tts__audio-settings"&gt;
          &lt;svg class="icon audio-player-tts__audio-settings--icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

        &lt;/button&gt;
        &lt;div class="audio-player-tts__settings-container"&gt;
          &lt;div class="audio-player-tts__settings--main uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings--current-voice"&gt;
              &lt;span class="audio-player-tts__settings--current-voice-info"&gt;
                &lt;svg class="audio-player-tts__settings--current-voice-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;span&gt;Voice&lt;/span&gt;
              &lt;/span&gt;
              &lt;span class="audio-player-tts__settings--current-voice-next"&gt;
                &lt;span class="audio-player-tts__settings--current-voice-text uni-small-text"&gt;&lt;/span&gt;
                &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

              &lt;/span&gt;
            &lt;/button&gt;
            &lt;button class="audio-player-tts__settings--current-speed"&gt;
              &lt;span class="audio-player-tts__settings--current-speed-info"&gt;
                  &lt;svg class="audio-player-tts__settings--current-speed-icon" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                  &lt;span&gt;Speed&lt;/span&gt;
                &lt;/span&gt;
                &lt;span class="audio-player-tts__settings--current-speed-next"&gt;
                  &lt;span class="audio-player-tts__settings--current-speed-text uni-small-text"&gt;&lt;/span&gt;
                  &lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;

                &lt;/span&gt;
            &lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--voices uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Voice&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
          &lt;div class="audio-player-tts__settings--speeds uni-cta-text"&gt;
            &lt;button class="audio-player-tts__settings-back"&gt;&lt;svg class="icon tts-chevron" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;use xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
&lt;/svg&gt;
 &lt;span&gt;Speed&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;0.75X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option audio-player-tts__settings-option--selected"&gt;&lt;span&gt;1X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;1.5X&lt;/span&gt;&lt;/button&gt;
            &lt;button class="audio-player-tts__settings-option"&gt;&lt;span&gt;2X&lt;/span&gt;&lt;/button&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

  





            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;In August, we previewed Genie 3, a general-purpose world model capable of generating diverse, interactive environments. Even in this early form, trusted testers were able to create an impressive range of fascinating worlds and experiences, and uncovered entirely new ways to use it. The next step is to broaden access through a dedicated, interactive prototype focused on immersive world creation.&lt;/p&gt;&lt;p&gt;Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds.&lt;/p&gt;&lt;h2&gt;How we’re advancing world models&lt;/h2&gt;&lt;p&gt;A world model simulates the dynamics of an environment, predicting how they evolve and how actions affect them. While Google DeepMind has a history of agents for specific environments like Chess or Go, building AGI requires systems that navigate the diversity of the real world.&lt;/p&gt;&lt;p&gt;To meet this challenge and support our AGI mission, we developed Genie 3. Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds, while its breakthrough consistency enables the simulation of any real-world scenario — from robotics and modelling animation and fiction, to exploring locations and historical settings.&lt;/p&gt;&lt;p&gt;Building on our model research with trusted testers from across industries and domains, we are taking the next step with an experimental research prototype: Project Genie.&lt;/p&gt;&lt;h2&gt;How Project Genie works&lt;/h2&gt;&lt;p&gt;Project Genie is a prototype web app powered by Genie 3, Nano Banana Pro and Gemini, which allows users to experiment with the immersive experiences of our world model firsthand. The experience is centred on three core capabilities:&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h3&gt;1. World sketching&lt;/h3&gt;&lt;p&gt;Prompt with text and generated or uploaded images to create a living, expanding environment. Create your character, your world, and define how you want to explore it — from walking to riding, flying to driving, and anything beyond.&lt;/p&gt;&lt;p&gt;For more precise control, we have integrated “World Sketching” with Nano Banana Pro. This allows you to preview what your world will look like and modify your image to fine tune your world prior to jumping in. You can also define your perspective for the character — such as first-person or third-person — giving you control over how you experience the scene before you enter.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    





























  
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h3&gt;2. World exploration&lt;/h3&gt;&lt;p&gt;Your world is a navigable environment that’s waiting to be explored. As you move, Project Genie generates the path ahead in real time based on the actions you take. You can also adjust the camera as you traverse through the world.&lt;/p&gt;&lt;h3&gt;3. World remixing&lt;/h3&gt;&lt;p&gt;Remix existing worlds into new interpretations, by building on top of their prompts. You can also explore curated worlds in the gallery or in the &amp;lt;randomizer icon&amp;gt; for inspiration, or build on top of them. And once you’re done, you can download videos of your worlds and your explorations.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    
  
    




  
  











  


  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;h2&gt;How we’re building responsibly&lt;/h2&gt;&lt;p&gt;Project Genie is an experimental research prototype in Google Labs, powered by Genie 3. As with all our work towards general AI systems, our mission is to build AI responsibly to benefit humanity. Since Genie 3 is an early research model, there are a few known areas for improvement:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Generated worlds might not look completely true-to-life or always adhere closely to prompts or images, or real-world physics&lt;/li&gt;&lt;li&gt;Characters can sometimes be less controllable, or experience higher latency in control&lt;/li&gt;&lt;li&gt;Limitations in generations to 60 seconds&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A few of the Genie 3 model capabilities we announced in August, such as promptable events that change the world as you explore it, are not yet included in this prototype. You can find more details on model limitations and future updates on how we’re improving the experience, here.&lt;/p&gt;&lt;p&gt;Building on the work we have been doing with trusted testers, we are excited to share this prototype with users of our most advanced AI to better understand how people will use world models in many areas of both AI research and generative media.&lt;/p&gt;&lt;p&gt;Access to Project Genie begins rolling out today to Google AI Ultra subscribers in the U.S. (18+), expanding to more territories in due course. We look forward to seeing the infinitely diverse worlds they create, and in time, our goal is to make these experiences and technology accessible to more users.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google One


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/blog/project-genie-experimenting-with-infinite-interactive-worlds/</guid><pubDate>Thu, 29 Jan 2026 17:01:05 +0000</pubDate></item><item><title>Introducing NVIDIA Cosmos Policy for Advanced Robot Control (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/cosmos-policy-for-robot-control</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Jinwei Gu's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/adf80f3473dda42450148789ae5c208f.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
We are continuously expanding &lt;strong&gt;NVIDIA Cosmos™&lt;/strong&gt; world foundation models (WFMs) to tackle some of the hardest problems in robotics, autonomous vehicle development, and industrial vision AI.
&lt;p&gt;To further support this effort, we’re introducing &lt;strong&gt;Cosmos Policy&lt;/strong&gt;, our latest research on advancing robot control and planning using Cosmos WFMs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		TL;DR
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cosmos Policy&lt;/strong&gt;: A new state-of-the-art robot control policy that post-trains the Cosmos Predict-2 world foundation model for manipulation tasks. It directly encodes robot actions and future states into the model, achieving SOTA performance on LIBERO and RoboCasa benchmarks.&lt;/p&gt;
&lt;p&gt; 📦 Model on HuggingFace | 🔧 Code on GitHub&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cosmos Cookoff&lt;/strong&gt;: An open hackathon where developers can get hands-on with Cosmos world foundation models and push the boundaries of physical AI. &lt;/p&gt;
&lt;p&gt; 🍳 Join the Cosmos Cookoff&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Overview: What Is Cosmos Policy?
	&lt;/span&gt;
&lt;/h2&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/677c471f8b4c4e271e57eaa5/GmY084PXnAsNRn5BnVgdG.mp4"&gt;&lt;/video&gt;
Cosmos Policy is a robot control and planning policy obtained by fine-tuning Cosmos Predict, a world foundation model trained to predict future frames. Instead of introducing new architectural components or separate action modules, Cosmos Policy adapts the pretrained model directly through a single stage of post-training on robot demonstration data.

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;A Policy is the system’s decision-making brain that maps observations (such as camera images) to physical actions (like moving a robotic arm) to complete tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;What’s different?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The breakthrough of Cosmos Policy is how it represents this data. Instead of building separate neural networks for the robot's perception and control , it treats robot actions, physical states, and success scores just like frames in a video.&lt;/p&gt;
&lt;p&gt;All of these are encoded as additional latent frames. These are learned using the same diffusion process as video generation, allowing the model to inherit its pre-learned understanding of physics, gravity, and how scenes evolve over time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Latent refers to the compressed, mathematical language a model uses to understand data internally (rather than raw pixels).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a result, a single model can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predict action chunks to guide robots movement using hand-eye coordination (i.e.,visuomotor control) &lt;/li&gt;
&lt;li&gt;Predict future robot observations for world modeling&lt;/li&gt;
&lt;li&gt;Predict expected returns (i.e. value function) for planning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All three capabilities are learned jointly within one unified model.&lt;/p&gt;
&lt;p&gt;Cosmos Policy can be deployed either as a direct policy, where only actions are generated at inference time, or as a planning policy, where multiple candidate actions are evaluated by predicting their resulting future states and values.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Base model: Cosmos Predict and why it matters
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Recent work in robotic manipulation has increasingly relied on large pretrained backbones to improve generalization and data efficiency. Most of these approaches build on vision-language models (VLMs) trained on large-scale image–text datasets and fine-tuned to predict robot actions.&lt;/p&gt;
&lt;p&gt;These models learn to understand videos and describe what they see, but they do not learn how to physically perform actions. A VLM can suggest high-level actions like turn left or pick up the purple cup, but it does not know how to carry them out precisely.&lt;/p&gt;
&lt;p&gt;In contrast, WFMs are trained to predict how scenes evolve over time and generate temporal dynamics with videos. These capabilities are directly relevant to robot control, where actions must account for how the environment and the robot’s own state change over time.&lt;/p&gt;
&lt;p&gt;Cosmos Predict is trained for physical AI using a diffusion objective over continuous spatiotemporal latents, enabling it to model complex, high-dimensional, and multimodal distributions across long temporal horizons.&lt;/p&gt;
&lt;p&gt;This design makes Cosmos Predict a natural foundation for visuomotor control:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The model already learns state transitions through future-frame prediction.&lt;/li&gt;
&lt;li&gt;Its diffusion formulation supports multimodal outputs, which is critical for tasks with multiple valid action sequences.&lt;/li&gt;
&lt;li&gt;The transformer-based denoiser can scale to long sequences and multiple modalities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cosmos Policy is built on post-trained Cosmos Predict2 to generate robot actions alongside future observations and value estimates, using the model’s native diffusion process. This allows the policy to fully inherit the pretrained model’s understanding of temporal structure and physical interaction while remaining simple to train and deploy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;⚡&lt;strong&gt;Important Update:&lt;/strong&gt; Latest Cosmos Predict 2.5 is here. Check out the model card.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Results at a Glance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Cosmos Policy is evaluated across &lt;strong&gt;simulation benchmarks&lt;/strong&gt; and &lt;strong&gt;real-world robot manipulation tasks&lt;/strong&gt;, comparing against diffusion-based policies trained from scratch, video-based robot policies, and fine-tuned vision-language-action (VLA) models.&lt;/p&gt;
&lt;p&gt;Cosmos Policy is evaluated on &lt;strong&gt;LIBERO&lt;/strong&gt; and &lt;strong&gt;RoboCasa&lt;/strong&gt;, two standard benchmarks for multi-task and long-horizon robotic manipulation.&lt;/p&gt;
&lt;p&gt;On &lt;strong&gt;LIBERO&lt;/strong&gt;, Cosmos Policy consistently outperforms prior diffusion policies and VLA-based approaches across task suites, particularly on tasks that require precise temporal coordination and multi-step execution.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Spatial SR (%)&lt;/th&gt;
&lt;th&gt;Object SR (%)&lt;/th&gt;
&lt;th&gt;Goal SR (%)&lt;/th&gt;
&lt;th&gt;Long SR (%)&lt;/th&gt;
&lt;th&gt;Average SR (%)&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Diffusion Policy&lt;/td&gt;
&lt;td&gt;78.3&lt;/td&gt;
&lt;td&gt;92.5&lt;/td&gt;
&lt;td&gt;68.3&lt;/td&gt;
&lt;td&gt;50.5&lt;/td&gt;
&lt;td&gt;72.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dita&lt;/td&gt;
&lt;td&gt;97.4&lt;/td&gt;
&lt;td&gt;94.8&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;83.6&lt;/td&gt;
&lt;td&gt;92.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;π0&lt;/td&gt;
&lt;td&gt;96.8&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;td&gt;95.8&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UVA&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;90.0&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UniVLA&lt;/td&gt;
&lt;td&gt;96.5&lt;/td&gt;
&lt;td&gt;96.8&lt;/td&gt;
&lt;td&gt;95.6&lt;/td&gt;
&lt;td&gt;92.0&lt;/td&gt;
&lt;td&gt;95.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;π0.5&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;td&gt;98.2&lt;/td&gt;
&lt;td&gt;98.0&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;96.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video Policy&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;94.0&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenVLA-OFT&lt;/td&gt;
&lt;td&gt;97.6&lt;/td&gt;
&lt;td&gt;98.4&lt;/td&gt;
&lt;td&gt;97.9&lt;/td&gt;
&lt;td&gt;94.5&lt;/td&gt;
&lt;td&gt;97.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CogVLA&lt;/td&gt;
&lt;td&gt;98.6&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;td&gt;96.6&lt;/td&gt;
&lt;td&gt;95.4&lt;/td&gt;
&lt;td&gt;97.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Cosmos Policy (ours)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;98.1&lt;/td&gt;
&lt;td&gt;100.0&lt;/td&gt;
&lt;td&gt;98.2&lt;/td&gt;
&lt;td&gt;97.6&lt;/td&gt;
&lt;td&gt;98.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/677c471f8b4c4e271e57eaa5/nCuIfEaiJc_fb1LjZS3C_.mp4"&gt;&lt;/video&gt;

&lt;p&gt;On &lt;strong&gt;RoboCasa&lt;/strong&gt;, Cosmos Policy achieves higher success rates than baselines trained from scratch, demonstrating improved generalization across diverse household manipulation scenarios.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;# Training Demos per Task&lt;/th&gt;
&lt;th&gt;Average SR (%)&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;GR00T-N1&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;49.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UVA&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;50.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DP-VLA&lt;/td&gt;
&lt;td&gt;3000&lt;/td&gt;
&lt;td&gt;57.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1 + DreamGen&lt;/td&gt;
&lt;td&gt;300 (+10000 synthetic)&lt;/td&gt;
&lt;td&gt;57.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1 + DUST&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;58.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UWM&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;60.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;π0&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;62.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1.5&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;64.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video Policy&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;66.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FLARE&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;66.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1.5 + HAMLET&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;66.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Cosmos Policy (ours)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In both benchmarks, &lt;strong&gt;initializing from Cosmos Predict&lt;/strong&gt; provides a significant performance advantage over training equivalent architectures without video pretraining.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Planning vs. Direct Policy Execution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When deployed as a direct policy, Cosmos Policy already matches or exceeds state-of-the-art performance on most tasks.&lt;br /&gt;When enhanced with &lt;strong&gt;model-based planning&lt;/strong&gt;, we observe a &lt;strong&gt;12.5% higher task completion rate&lt;/strong&gt; on average in two challenging real-world manipulation tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Real-World Manipulation&lt;/strong&gt;&lt;/p&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/677c471f8b4c4e271e57eaa5/GSjsYCyd-ZFgSigCS9g69.mp4"&gt;&lt;/video&gt;

&lt;p&gt;Cosmos Policy is also evaluated on &lt;strong&gt;real-world bimanual manipulation tasks&lt;/strong&gt; using the ALOHA robot platform.&lt;br /&gt;The policy successfully executes long-horizon manipulation tasks &lt;strong&gt;directly from visual observations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Learn more about architecture and results here.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What’s Next: Cosmos Cookoff
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Cosmos Policy represents an early step toward adapting world foundation models for robot control and planning. We are actively working with early adopters to evolve this research for our robotics community. &lt;/p&gt;
&lt;p&gt;In parallel, the Cosmos Policy continues to be available to developers through practical Cosmos Cookbook recipe, which demonstrates how you can adopt and build it.&lt;/p&gt;
&lt;p&gt;To support hands-on experimentation with Cosmos WFMs, we are announcing the Cosmos Cookoff, an open hackathon focused on building applications and workflows using Cosmos models and cookbook recipes.The latest Cookoff is live, inviting physical AI developers across robotics, autonomous vehicles, and video analytics to explore, prototype fast, and learn with experts.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🍳 Join the Cosmos Cookoff
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;📅 &lt;strong&gt;When:&lt;/strong&gt; Jan 29 – Feb 26  &lt;/li&gt;
&lt;li&gt;👥 &lt;strong&gt;Team Format:&lt;/strong&gt; Up to 4 member team  &lt;/li&gt;
&lt;li&gt;🏆 &lt;strong&gt;Prizes:&lt;/strong&gt; $5,000 cash prize, NVIDIA DGX Spark™, NVIDIA GeForce RTX™ 5090 GPU, and more!  &lt;/li&gt;
&lt;li&gt;🧑‍⚖️ &lt;strong&gt;Judges:&lt;/strong&gt; Projects will be reviewed by experts from Datature, Hugging Face, Nebius, Nexar, and NVIDIA, bringing deep experience in open models, cloud/compute, and real-world edge and vision AI deployments.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📣 Get Started
	&lt;/span&gt;
&lt;/h3&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Jinwei Gu's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/adf80f3473dda42450148789ae5c208f.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
We are continuously expanding &lt;strong&gt;NVIDIA Cosmos™&lt;/strong&gt; world foundation models (WFMs) to tackle some of the hardest problems in robotics, autonomous vehicle development, and industrial vision AI.
&lt;p&gt;To further support this effort, we’re introducing &lt;strong&gt;Cosmos Policy&lt;/strong&gt;, our latest research on advancing robot control and planning using Cosmos WFMs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		TL;DR
	&lt;/span&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cosmos Policy&lt;/strong&gt;: A new state-of-the-art robot control policy that post-trains the Cosmos Predict-2 world foundation model for manipulation tasks. It directly encodes robot actions and future states into the model, achieving SOTA performance on LIBERO and RoboCasa benchmarks.&lt;/p&gt;
&lt;p&gt; 📦 Model on HuggingFace | 🔧 Code on GitHub&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cosmos Cookoff&lt;/strong&gt;: An open hackathon where developers can get hands-on with Cosmos world foundation models and push the boundaries of physical AI. &lt;/p&gt;
&lt;p&gt; 🍳 Join the Cosmos Cookoff&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Overview: What Is Cosmos Policy?
	&lt;/span&gt;
&lt;/h2&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/677c471f8b4c4e271e57eaa5/GmY084PXnAsNRn5BnVgdG.mp4"&gt;&lt;/video&gt;
Cosmos Policy is a robot control and planning policy obtained by fine-tuning Cosmos Predict, a world foundation model trained to predict future frames. Instead of introducing new architectural components or separate action modules, Cosmos Policy adapts the pretrained model directly through a single stage of post-training on robot demonstration data.

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;A Policy is the system’s decision-making brain that maps observations (such as camera images) to physical actions (like moving a robotic arm) to complete tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;What’s different?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The breakthrough of Cosmos Policy is how it represents this data. Instead of building separate neural networks for the robot's perception and control , it treats robot actions, physical states, and success scores just like frames in a video.&lt;/p&gt;
&lt;p&gt;All of these are encoded as additional latent frames. These are learned using the same diffusion process as video generation, allowing the model to inherit its pre-learned understanding of physics, gravity, and how scenes evolve over time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Latent refers to the compressed, mathematical language a model uses to understand data internally (rather than raw pixels).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As a result, a single model can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predict action chunks to guide robots movement using hand-eye coordination (i.e.,visuomotor control) &lt;/li&gt;
&lt;li&gt;Predict future robot observations for world modeling&lt;/li&gt;
&lt;li&gt;Predict expected returns (i.e. value function) for planning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All three capabilities are learned jointly within one unified model.&lt;/p&gt;
&lt;p&gt;Cosmos Policy can be deployed either as a direct policy, where only actions are generated at inference time, or as a planning policy, where multiple candidate actions are evaluated by predicting their resulting future states and values.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Base model: Cosmos Predict and why it matters
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Recent work in robotic manipulation has increasingly relied on large pretrained backbones to improve generalization and data efficiency. Most of these approaches build on vision-language models (VLMs) trained on large-scale image–text datasets and fine-tuned to predict robot actions.&lt;/p&gt;
&lt;p&gt;These models learn to understand videos and describe what they see, but they do not learn how to physically perform actions. A VLM can suggest high-level actions like turn left or pick up the purple cup, but it does not know how to carry them out precisely.&lt;/p&gt;
&lt;p&gt;In contrast, WFMs are trained to predict how scenes evolve over time and generate temporal dynamics with videos. These capabilities are directly relevant to robot control, where actions must account for how the environment and the robot’s own state change over time.&lt;/p&gt;
&lt;p&gt;Cosmos Predict is trained for physical AI using a diffusion objective over continuous spatiotemporal latents, enabling it to model complex, high-dimensional, and multimodal distributions across long temporal horizons.&lt;/p&gt;
&lt;p&gt;This design makes Cosmos Predict a natural foundation for visuomotor control:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The model already learns state transitions through future-frame prediction.&lt;/li&gt;
&lt;li&gt;Its diffusion formulation supports multimodal outputs, which is critical for tasks with multiple valid action sequences.&lt;/li&gt;
&lt;li&gt;The transformer-based denoiser can scale to long sequences and multiple modalities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cosmos Policy is built on post-trained Cosmos Predict2 to generate robot actions alongside future observations and value estimates, using the model’s native diffusion process. This allows the policy to fully inherit the pretrained model’s understanding of temporal structure and physical interaction while remaining simple to train and deploy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;⚡&lt;strong&gt;Important Update:&lt;/strong&gt; Latest Cosmos Predict 2.5 is here. Check out the model card.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Results at a Glance
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Cosmos Policy is evaluated across &lt;strong&gt;simulation benchmarks&lt;/strong&gt; and &lt;strong&gt;real-world robot manipulation tasks&lt;/strong&gt;, comparing against diffusion-based policies trained from scratch, video-based robot policies, and fine-tuned vision-language-action (VLA) models.&lt;/p&gt;
&lt;p&gt;Cosmos Policy is evaluated on &lt;strong&gt;LIBERO&lt;/strong&gt; and &lt;strong&gt;RoboCasa&lt;/strong&gt;, two standard benchmarks for multi-task and long-horizon robotic manipulation.&lt;/p&gt;
&lt;p&gt;On &lt;strong&gt;LIBERO&lt;/strong&gt;, Cosmos Policy consistently outperforms prior diffusion policies and VLA-based approaches across task suites, particularly on tasks that require precise temporal coordination and multi-step execution.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Spatial SR (%)&lt;/th&gt;
&lt;th&gt;Object SR (%)&lt;/th&gt;
&lt;th&gt;Goal SR (%)&lt;/th&gt;
&lt;th&gt;Long SR (%)&lt;/th&gt;
&lt;th&gt;Average SR (%)&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;Diffusion Policy&lt;/td&gt;
&lt;td&gt;78.3&lt;/td&gt;
&lt;td&gt;92.5&lt;/td&gt;
&lt;td&gt;68.3&lt;/td&gt;
&lt;td&gt;50.5&lt;/td&gt;
&lt;td&gt;72.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Dita&lt;/td&gt;
&lt;td&gt;97.4&lt;/td&gt;
&lt;td&gt;94.8&lt;/td&gt;
&lt;td&gt;93.2&lt;/td&gt;
&lt;td&gt;83.6&lt;/td&gt;
&lt;td&gt;92.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;π0&lt;/td&gt;
&lt;td&gt;96.8&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;td&gt;95.8&lt;/td&gt;
&lt;td&gt;85.2&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UVA&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;90.0&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UniVLA&lt;/td&gt;
&lt;td&gt;96.5&lt;/td&gt;
&lt;td&gt;96.8&lt;/td&gt;
&lt;td&gt;95.6&lt;/td&gt;
&lt;td&gt;92.0&lt;/td&gt;
&lt;td&gt;95.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;π0.5&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;td&gt;98.2&lt;/td&gt;
&lt;td&gt;98.0&lt;/td&gt;
&lt;td&gt;92.4&lt;/td&gt;
&lt;td&gt;96.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video Policy&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;td&gt;94.0&lt;/td&gt;
&lt;td&gt;--&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;OpenVLA-OFT&lt;/td&gt;
&lt;td&gt;97.6&lt;/td&gt;
&lt;td&gt;98.4&lt;/td&gt;
&lt;td&gt;97.9&lt;/td&gt;
&lt;td&gt;94.5&lt;/td&gt;
&lt;td&gt;97.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CogVLA&lt;/td&gt;
&lt;td&gt;98.6&lt;/td&gt;
&lt;td&gt;98.8&lt;/td&gt;
&lt;td&gt;96.6&lt;/td&gt;
&lt;td&gt;95.4&lt;/td&gt;
&lt;td&gt;97.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Cosmos Policy (ours)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;98.1&lt;/td&gt;
&lt;td&gt;100.0&lt;/td&gt;
&lt;td&gt;98.2&lt;/td&gt;
&lt;td&gt;97.6&lt;/td&gt;
&lt;td&gt;98.5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/677c471f8b4c4e271e57eaa5/nCuIfEaiJc_fb1LjZS3C_.mp4"&gt;&lt;/video&gt;

&lt;p&gt;On &lt;strong&gt;RoboCasa&lt;/strong&gt;, Cosmos Policy achieves higher success rates than baselines trained from scratch, demonstrating improved generalization across diverse household manipulation scenarios.&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;# Training Demos per Task&lt;/th&gt;
&lt;th&gt;Average SR (%)&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;GR00T-N1&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;49.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UVA&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;50.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DP-VLA&lt;/td&gt;
&lt;td&gt;3000&lt;/td&gt;
&lt;td&gt;57.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1 + DreamGen&lt;/td&gt;
&lt;td&gt;300 (+10000 synthetic)&lt;/td&gt;
&lt;td&gt;57.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1 + DUST&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;58.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;UWM&lt;/td&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;60.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;π0&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;62.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1.5&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;64.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Video Policy&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;66.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FLARE&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;66.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GR00T-N1.5 + HAMLET&lt;/td&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;66.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Cosmos Policy (ours)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;50&lt;/td&gt;
&lt;td&gt;67.1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;In both benchmarks, &lt;strong&gt;initializing from Cosmos Predict&lt;/strong&gt; provides a significant performance advantage over training equivalent architectures without video pretraining.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Planning vs. Direct Policy Execution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When deployed as a direct policy, Cosmos Policy already matches or exceeds state-of-the-art performance on most tasks.&lt;br /&gt;When enhanced with &lt;strong&gt;model-based planning&lt;/strong&gt;, we observe a &lt;strong&gt;12.5% higher task completion rate&lt;/strong&gt; on average in two challenging real-world manipulation tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Real-World Manipulation&lt;/strong&gt;&lt;/p&gt;
&lt;video class="max-w-full!" controls="controls" src="https://cdn-uploads.huggingface.co/production/uploads/677c471f8b4c4e271e57eaa5/GSjsYCyd-ZFgSigCS9g69.mp4"&gt;&lt;/video&gt;

&lt;p&gt;Cosmos Policy is also evaluated on &lt;strong&gt;real-world bimanual manipulation tasks&lt;/strong&gt; using the ALOHA robot platform.&lt;br /&gt;The policy successfully executes long-horizon manipulation tasks &lt;strong&gt;directly from visual observations&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Learn more about architecture and results here.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		What’s Next: Cosmos Cookoff
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Cosmos Policy represents an early step toward adapting world foundation models for robot control and planning. We are actively working with early adopters to evolve this research for our robotics community. &lt;/p&gt;
&lt;p&gt;In parallel, the Cosmos Policy continues to be available to developers through practical Cosmos Cookbook recipe, which demonstrates how you can adopt and build it.&lt;/p&gt;
&lt;p&gt;To support hands-on experimentation with Cosmos WFMs, we are announcing the Cosmos Cookoff, an open hackathon focused on building applications and workflows using Cosmos models and cookbook recipes.The latest Cookoff is live, inviting physical AI developers across robotics, autonomous vehicles, and video analytics to explore, prototype fast, and learn with experts.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		🍳 Join the Cosmos Cookoff
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;📅 &lt;strong&gt;When:&lt;/strong&gt; Jan 29 – Feb 26  &lt;/li&gt;
&lt;li&gt;👥 &lt;strong&gt;Team Format:&lt;/strong&gt; Up to 4 member team  &lt;/li&gt;
&lt;li&gt;🏆 &lt;strong&gt;Prizes:&lt;/strong&gt; $5,000 cash prize, NVIDIA DGX Spark™, NVIDIA GeForce RTX™ 5090 GPU, and more!  &lt;/li&gt;
&lt;li&gt;🧑‍⚖️ &lt;strong&gt;Judges:&lt;/strong&gt; Projects will be reviewed by experts from Datature, Hugging Face, Nebius, Nexar, and NVIDIA, bringing deep experience in open models, cloud/compute, and real-world edge and vision AI deployments.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		📣 Get Started
	&lt;/span&gt;
&lt;/h3&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/cosmos-policy-for-robot-control</guid><pubDate>Thu, 29 Jan 2026 17:03:25 +0000</pubDate></item><item><title>I built marshmallow castles in Google’s new AI-world generator (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/i-built-marshmallow-castles-in-googles-new-ai-world-generator-project-genie/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind is opening up access to Project Genie, its AI tool for creating interactive game worlds from text prompts or images.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Starting Thursday, Google AI Ultra subscribers in the U.S. can play around with the experimental research prototype, which is powered by a combination of Google’s latest world model Genie 3, its image-generation model Nano Banana Pro, and Gemini.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Coming five months after Genie 3’s research preview, the move is part of a broader push to gather user feedback and training data as DeepMind races to develop more capable world models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;World models are AI systems that generate an internal representation of an environment, and can be used to predict future outcomes and plan actions. Many AI leaders, including those at DeepMind, believe world models are a crucial step to achieving artificial general intelligence (AGI). But in the nearer term, labs like DeepMind envision a go-to-market plan that starts with video games and other forms of entertainment and branches out into training embodied agents (aka robots) in simulation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind’s release of Project Genie comes as the world model race is beginning to heat up. Fei-Fei Li’s World Labs late last year released its first commercial product called Marble. Runway, the AI video-generation startup, has also launched a world model recently. And former Meta chief scientist Yann LeCun’s startup AMI Labs will also focus on developing world models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think it’s exciting to be in a place where we can have more people access it and give us feedback,” Shlomi Fruchter, a research director at DeepMind, told TechCrunch via video interview, smiling ear-to-ear in clear excitement over Project Genie’s release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind researchers that TechCrunch spoke to were upfront about the tool’s experimental nature. It can be inconsistent, sometimes impressively generating playable worlds, other times producing baffling results that miss the mark. Here’s how it works.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087858" height="374" src="https://techcrunch.com/wp-content/uploads/2026/01/genie-marshmallow-castle.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A claymation-style castle in the sky made of marshmallows and candy&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You start with a “world sketch” by providing text prompts for both the environment and a main character, whom you will later be able to maneuver through the world in either first- or third-person view. Nano Banana Pro creates an image based on the prompts that you can, in theory, modify before Genie uses the image as a jumping off point for an interactive world. The modifications mostly worked, but the model occasionally stumbled and would give you purple hair when you asked for green.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can also use real-life photos as a baseline for the model to build a world on, which, again, was hit or miss. (More on that later.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once you’re satisfied with the image, it takes a few seconds for Project Genie to create an explorable world. You can also remix existing worlds into new interpretations by building on top of their prompts, or explore curated worlds in the gallery or via the randomizer tool for inspiration. You can then download videos of the world you just explored.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeepMind is only granting 60 seconds of world generation and navigation at the moment, in part due to the budget and compute constraints. Because Genie 3 is an auto-regressive model, it takes a lot of dedicated compute — which puts a tight ceiling on how much DeepMind is able to provide to users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The reason we limit it to 60 seconds is because we wanted to bring it to more users,” Fruchter said. “Basically when you’re using it, there’s a chip somewhere that’s only yours and it’s being dedicated to your session.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that extending it beyond 60 seconds would diminish the incremental value of the testing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The environments are interesting, but at some point, because of their level of interaction the dynamism of the environment is somewhat limited. Still, we see that as a limitation we hope to improve on.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-whimsy-works-realism-doesn-t"&gt;&lt;strong&gt;Whimsy works, realism doesn’t&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087845" height="364" src="https://techcrunch.com/wp-content/uploads/2026/01/little-mermaid-disney-.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Google received a cease-and-desist from Disney last year, so it wouldn’t build models that were Disney-related&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When I used the model, the safety guardrails were already up and running. I couldn’t generate anything resembling nudity, nor could I generate worlds that even remotely sniffed of Disney or other copyrighted material. (In December, Disney hit Google with a cease-and-desist, accusing the firm’s AI models of copyright infringement by training on Disney’s characters and IP and&amp;nbsp;generating unauthorized content, among other things.) I couldn’t even get Genie to generate worlds of mermaids exploring underwater fantasy lands or ice queens in their wintery castles.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the demo was deeply impressive. The first world I built was an attempt to live out a small childhood fantasy, in which I could explore a castle in the clouds made up of marshmallows with a chocolate sauce river and trees made of candy. (Yes, I was a chubby kid.) I asked the model to do it in claymation style, and it delivered a whimsical world that childhood me would have eaten up; the castle’s pastel-and-white colored spires and turrets looking puffy and tasty enough to rip off a chunk and dunk into the chocolate moat. (Video above.)&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087851" height="374" src="https://techcrunch.com/wp-content/uploads/2026/01/Genie6467bd1e8f644c099a541385ac4f78c3-ezgif.com-video-to-gif-converter.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A “Game of Thrones” inspired world that failed to generate as photo-realistically as I wanted&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Project Genie still has some kinks to work out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models excelled at creating worlds based on artistic prompts, like using watercolors, anime style, or classic cartoon aesthetics. But it tended to fail when it came to photorealistic or cinematic worlds, often coming out looking like a video game rather than real people in a real setting.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It also didn’t always respond well when given real photos to work with. When I gave it a photo of my office and asked it to create a world based on the photo exactly as it was, it gave me a world that had some of the same furnishings of my office — a wooden desk, plants, a grey couch — laid out differently. And it looked sterile, digital, not lifelike.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I fed it a photo of my desk with a stuffed toy, Project Genie animated the toy navigating the space, and even had other objects occasionally react as it moved past them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That interactivity is something DeepMind is working on improving. There were several occasions when my characters walked right through walls or other solid objects.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087841" height="374" src="https://techcrunch.com/wp-content/uploads/2026/01/Genie-bingo-bronson.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;I asked Project Genie to animate a stuffed toy (Bingo Bronson) so it could explore my desk&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When DeepMind released Genie 3 initially, researchers highlighted how the model’s auto-regressive architecture meant that it could remember what it had generated, so I wanted to test that by returning to parts of the environment it generated already to see if it would be the same. For the most part, the model succeeded. In one case, I generated a cat exploring yet another desk, and only once when I turned back to the right side of the desk did the model generate a second mug.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The part I found most frustrating was the way you navigated the space using the arrows to look around, the spacebar to jump or ascend, and the W-A-S-D keys to move. I’m not a gamer, so this didn’t come naturally to me, but the keys were often non-responsive, or they sent you in the wrong direction. Trying to walk from one side of the room to a doorway on the other side often became a chaotic zigzagging exercise, like trying to steer a shopping cart with a broken wheel.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fruchter assured me that his team was aware of these shortcomings, reminding me again that Project Genie is an experimental prototype. In the future, he said, the team hopes to enhance the realism and improve interaction capabilities, including giving users more control over actions and environments.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t think about [Project Genie] as an end-to-end product that people can go back to everyday, but we think there is already a glimpse of something that’s interesting and unique and can’t be done in another way,” he said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind is opening up access to Project Genie, its AI tool for creating interactive game worlds from text prompts or images.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Starting Thursday, Google AI Ultra subscribers in the U.S. can play around with the experimental research prototype, which is powered by a combination of Google’s latest world model Genie 3, its image-generation model Nano Banana Pro, and Gemini.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Coming five months after Genie 3’s research preview, the move is part of a broader push to gather user feedback and training data as DeepMind races to develop more capable world models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;World models are AI systems that generate an internal representation of an environment, and can be used to predict future outcomes and plan actions. Many AI leaders, including those at DeepMind, believe world models are a crucial step to achieving artificial general intelligence (AGI). But in the nearer term, labs like DeepMind envision a go-to-market plan that starts with video games and other forms of entertainment and branches out into training embodied agents (aka robots) in simulation.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind’s release of Project Genie comes as the world model race is beginning to heat up. Fei-Fei Li’s World Labs late last year released its first commercial product called Marble. Runway, the AI video-generation startup, has also launched a world model recently. And former Meta chief scientist Yann LeCun’s startup AMI Labs will also focus on developing world models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think it’s exciting to be in a place where we can have more people access it and give us feedback,” Shlomi Fruchter, a research director at DeepMind, told TechCrunch via video interview, smiling ear-to-ear in clear excitement over Project Genie’s release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepMind researchers that TechCrunch spoke to were upfront about the tool’s experimental nature. It can be inconsistent, sometimes impressively generating playable worlds, other times producing baffling results that miss the mark. Here’s how it works.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087858" height="374" src="https://techcrunch.com/wp-content/uploads/2026/01/genie-marshmallow-castle.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A claymation-style castle in the sky made of marshmallows and candy&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;You start with a “world sketch” by providing text prompts for both the environment and a main character, whom you will later be able to maneuver through the world in either first- or third-person view. Nano Banana Pro creates an image based on the prompts that you can, in theory, modify before Genie uses the image as a jumping off point for an interactive world. The modifications mostly worked, but the model occasionally stumbled and would give you purple hair when you asked for green.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can also use real-life photos as a baseline for the model to build a world on, which, again, was hit or miss. (More on that later.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once you’re satisfied with the image, it takes a few seconds for Project Genie to create an explorable world. You can also remix existing worlds into new interpretations by building on top of their prompts, or explore curated worlds in the gallery or via the randomizer tool for inspiration. You can then download videos of the world you just explored.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;DeepMind is only granting 60 seconds of world generation and navigation at the moment, in part due to the budget and compute constraints. Because Genie 3 is an auto-regressive model, it takes a lot of dedicated compute — which puts a tight ceiling on how much DeepMind is able to provide to users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The reason we limit it to 60 seconds is because we wanted to bring it to more users,” Fruchter said. “Basically when you’re using it, there’s a chip somewhere that’s only yours and it’s being dedicated to your session.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that extending it beyond 60 seconds would diminish the incremental value of the testing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The environments are interesting, but at some point, because of their level of interaction the dynamism of the environment is somewhat limited. Still, we see that as a limitation we hope to improve on.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-whimsy-works-realism-doesn-t"&gt;&lt;strong&gt;Whimsy works, realism doesn’t&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087845" height="364" src="https://techcrunch.com/wp-content/uploads/2026/01/little-mermaid-disney-.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Google received a cease-and-desist from Disney last year, so it wouldn’t build models that were Disney-related&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When I used the model, the safety guardrails were already up and running. I couldn’t generate anything resembling nudity, nor could I generate worlds that even remotely sniffed of Disney or other copyrighted material. (In December, Disney hit Google with a cease-and-desist, accusing the firm’s AI models of copyright infringement by training on Disney’s characters and IP and&amp;nbsp;generating unauthorized content, among other things.) I couldn’t even get Genie to generate worlds of mermaids exploring underwater fantasy lands or ice queens in their wintery castles.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the demo was deeply impressive. The first world I built was an attempt to live out a small childhood fantasy, in which I could explore a castle in the clouds made up of marshmallows with a chocolate sauce river and trees made of candy. (Yes, I was a chubby kid.) I asked the model to do it in claymation style, and it delivered a whimsical world that childhood me would have eaten up; the castle’s pastel-and-white colored spires and turrets looking puffy and tasty enough to rip off a chunk and dunk into the chocolate moat. (Video above.)&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087851" height="374" src="https://techcrunch.com/wp-content/uploads/2026/01/Genie6467bd1e8f644c099a541385ac4f78c3-ezgif.com-video-to-gif-converter.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;A “Game of Thrones” inspired world that failed to generate as photo-realistically as I wanted&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Project Genie still has some kinks to work out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models excelled at creating worlds based on artistic prompts, like using watercolors, anime style, or classic cartoon aesthetics. But it tended to fail when it came to photorealistic or cinematic worlds, often coming out looking like a video game rather than real people in a real setting.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It also didn’t always respond well when given real photos to work with. When I gave it a photo of my office and asked it to create a world based on the photo exactly as it was, it gave me a world that had some of the same furnishings of my office — a wooden desk, plants, a grey couch — laid out differently. And it looked sterile, digital, not lifelike.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I fed it a photo of my desk with a stuffed toy, Project Genie animated the toy navigating the space, and even had other objects occasionally react as it moved past them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That interactivity is something DeepMind is working on improving. There were several occasions when my characters walked right through walls or other solid objects.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3087841" height="374" src="https://techcrunch.com/wp-content/uploads/2026/01/Genie-bingo-bronson.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;I asked Project Genie to animate a stuffed toy (Bingo Bronson) so it could explore my desk&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;When DeepMind released Genie 3 initially, researchers highlighted how the model’s auto-regressive architecture meant that it could remember what it had generated, so I wanted to test that by returning to parts of the environment it generated already to see if it would be the same. For the most part, the model succeeded. In one case, I generated a cat exploring yet another desk, and only once when I turned back to the right side of the desk did the model generate a second mug.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The part I found most frustrating was the way you navigated the space using the arrows to look around, the spacebar to jump or ascend, and the W-A-S-D keys to move. I’m not a gamer, so this didn’t come naturally to me, but the keys were often non-responsive, or they sent you in the wrong direction. Trying to walk from one side of the room to a doorway on the other side often became a chaotic zigzagging exercise, like trying to steer a shopping cart with a broken wheel.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fruchter assured me that his team was aware of these shortcomings, reminding me again that Project Genie is an experimental prototype. In the future, he said, the team hopes to enhance the realism and improve interaction capabilities, including giving users more control over actions and environments.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t think about [Project Genie] as an end-to-end product that people can go back to everyday, but we think there is already a glimpse of something that’s interesting and unique and can’t be done in another way,” he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/i-built-marshmallow-castles-in-googles-new-ai-world-generator-project-genie/</guid><pubDate>Thu, 29 Jan 2026 17:29:11 +0000</pubDate></item><item><title>New OpenAI tool renews fears that “AI slop” will overwhelm scientific research (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/01/new-openai-tool-renews-fears-that-ai-slop-will-overwhelm-scientific-research/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New “Prism” workspace launches just as studies show AI-assisted papers are flooding journals with diminished quality.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of a robot generating endless text, controlled by a scientist." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_science_header-300x169.jpg" width="300" /&gt;
                  &lt;img alt="Illustration of a robot generating endless text, controlled by a scientist." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_science_header-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Tuesday, OpenAI released a free AI-powered workspace for scientists. It’s called Prism, and it has drawn immediate skepticism from researchers who fear the tool will accelerate the already overwhelming flood of low-quality papers into scientific journals. The launch coincides with growing alarm among publishers about what many are calling “AI slop” in academic publishing.&lt;/p&gt;
&lt;p&gt;To be clear, Prism is a writing and formatting tool, not a system for conducting research itself, though OpenAI’s broader pitch blurs that line.&lt;/p&gt;
&lt;p&gt;Prism integrates OpenAI’s GPT-5.2 model into a LaTeX-based text editor (a standard used for typesetting documents), allowing researchers to draft papers, generate citations, create diagrams from whiteboard sketches, and collaborate with co-authors in real time. The tool is free for anyone with a ChatGPT account.&lt;/p&gt;
&lt;p&gt;“I think 2026 will be for AI and science what 2025 was for AI in software engineering,” Kevin Weil, vice president of OpenAI for Science, told reporters at a press briefing attended by MIT Technology Review. He said that ChatGPT receives about 8.4 million messages per week on “hard science” topics, which he described as evidence that AI is transitioning from curiosity to core workflow for scientists.&lt;/p&gt;
&lt;p&gt;OpenAI built Prism on technology from Crixet, a cloud-based LaTeX platform the company acquired in late 2025. The company envisions Prism helping researchers spend less time on tedious formatting tasks and more time on actual science. During a demonstration, an OpenAI employee showed how the software could automatically find and incorporate relevant scientific literature, then format the bibliography.&lt;/p&gt;
&lt;p&gt;But AI models are tools, and any tool can be misused. The risk here is specific: By making it easy to produce polished, professional-looking manuscripts, tools like Prism could flood the peer review system with papers that don’t meaningfully advance their fields. The barrier to producing science-flavored text is dropping, but the capacity to evaluate that research has not kept pace.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;When asked about the possibility of the AI model confabulating fake citations, Weil acknowledged in the press demo that “none of this absolves the scientist of the responsibility to verify that their references are correct.”&lt;/p&gt;
&lt;p&gt;Unlike traditional reference management software (such as EndNote), which has formatted citations for over 30 years without inventing them, AI models can generate plausible-sounding sources that don’t exist. Weil added: “We’re conscious that as AI becomes more capable, there are concerns around volume, quality, and trust in the scientific community.”&lt;/p&gt;
&lt;h2&gt;The slop problem&lt;/h2&gt;
&lt;p&gt;Those concerns are not hypothetical, as we have previously covered. A December 2025 study published in the journal Science found that researchers using large language models to write papers increased their output by 30 to 50 percent, depending on the field. But those AI-assisted papers performed worse in peer review. Papers with complex language written without AI assistance were most likely to be accepted by journals, while papers with complex language likely written by AI models were less likely to be accepted. Reviewers apparently recognized that sophisticated prose was masking weak science.&lt;/p&gt;
&lt;p&gt;“It is a very widespread pattern across different fields of science,” Yian Yin, an information science professor at Cornell University and one of the study’s authors, told the Cornell Chronicle. “There’s a big shift in our current ecosystem that warrants a very serious look, especially for those who make decisions about what science we should support and fund.”&lt;/p&gt;
&lt;p&gt;Another analysis of 41 million papers published between 1980 and 2025 found that while AI-using scientists receive more citations and publish more papers, the collective scope of scientific exploration appears to be narrowing. Lisa Messeri, a sociocultural anthropologist at Yale University, told Science magazine that these findings should set off “loud alarm bells” for the research community.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;“Science is nothing but a collective endeavor,” she said. “There needs to be some deep reckoning with what we do with a tool that benefits individuals but destroys science.”&lt;/p&gt;
&lt;p&gt;Concerns about AI-generated scientific content are not new. In 2022, Meta pulled a demo of Galactica, a large language model designed to write scientific literature, after users discovered it could generate convincing nonsense on any topic, including a wiki entry about a fictional research paper called “The benefits of eating crushed glass.” Two years later, Tokyo-based Sakana AI announced “The AI Scientist,” an autonomous research system that critics on Hacker News dismissed as producing “garbage” papers. “As an editor of a journal, I would likely desk-reject them,” one commenter wrote at the time. “They contain very limited novel knowledge.”&lt;/p&gt;
&lt;p&gt;The problem has only grown worse since then. In his first editorial of 2026 for Science, Editor-in-Chief H. Holden Thorp wrote that the journal is “less susceptible” to AI slop because of its size and human editorial investment, but he warned that “no system, human or artificial, can catch everything.” Science currently allows limited AI use for editing and gathering references but requires disclosure for anything beyond that and prohibits AI-generated figures.&lt;/p&gt;
&lt;p&gt;Mandy Hill, managing director of academic publishing at Cambridge University Press &amp;amp; Assessment, has been even more blunt. In October 2025, she told Retraction Watch that the publishing ecosystem is under strain and called for “radical change.” She explained to the University of Cambridge publication Varsity that “too many journal articles are being published, and this is causing huge strain” and warned that AI “will exacerbate” the problem.&lt;/p&gt;
&lt;h2&gt;Accelerating science or overwhelming peer review?&lt;/h2&gt;
&lt;p&gt;OpenAI is serious about leaning on its ability to accelerate science, and the company laid out its case for AI-assisted research in a report published earlier this week. It profiles researchers who say AI models have sped up their work, including a mathematician who used GPT-5.2 to solve an open problem in optimization over three evenings and a physicist who watched the model reproduce symmetry calculations that had taken him months to derive.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Those examples go beyond writing assistance into using AI for actual research work, a distinction OpenAI’s marketing intentionally blurs. For scientists who don’t speak English fluently, AI writing tools could legitimately accelerate the publication of good research. But that benefit may be offset by a flood of mediocre submissions jamming up an already strained peer-review system.&lt;/p&gt;
&lt;p&gt;Weil told MIT Technology Review that his goal is not to produce a single AI-generated discovery but rather “10,000 advances in science that maybe wouldn’t have happened or wouldn’t have happened as quickly.” He described this as “an incremental, compounding acceleration.”&lt;/p&gt;
&lt;p&gt;Whether that acceleration produces more scientific knowledge or simply more scientific papers remains to be seen. Nikita Zhivotovskiy, a statistician at UC Berkeley not connected to OpenAI, told MIT Technology Review that GPT-5 has already become valuable in his own work for polishing text and catching mathematical typos, making “interaction with the scientific literature smoother.”&lt;/p&gt;
&lt;p&gt;But by making papers look polished and professional regardless of their scientific merit, AI writing tools may help weak research clear the initial screening that editors and reviewers use to assess presentation quality. The risk is that conversational workflows obscure assumptions and blur accountability, and they might overwhelm the still very human peer review process required to vet it all.&lt;/p&gt;
&lt;p&gt;OpenAI appears aware of this tension. Its public statements about Prism emphasize that the tool will not conduct research independently and that human scientists remain responsible for verification.&lt;/p&gt;
&lt;p&gt;Still, one commenter on Hacker News captured the anxiety spreading through technical communities: “I’m scared that this type of thing is going to do to science journals what AI-generated bug reports is doing to bug bounties. We’re truly living in a post-scarcity society now, except that the thing we have an abundance of is garbage, and it’s drowning out everything of value.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New “Prism” workspace launches just as studies show AI-assisted papers are flooding journals with diminished quality.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Illustration of a robot generating endless text, controlled by a scientist." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_science_header-300x169.jpg" width="300" /&gt;
                  &lt;img alt="Illustration of a robot generating endless text, controlled by a scientist." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/08/robot_science_header-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Moor Studio via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Tuesday, OpenAI released a free AI-powered workspace for scientists. It’s called Prism, and it has drawn immediate skepticism from researchers who fear the tool will accelerate the already overwhelming flood of low-quality papers into scientific journals. The launch coincides with growing alarm among publishers about what many are calling “AI slop” in academic publishing.&lt;/p&gt;
&lt;p&gt;To be clear, Prism is a writing and formatting tool, not a system for conducting research itself, though OpenAI’s broader pitch blurs that line.&lt;/p&gt;
&lt;p&gt;Prism integrates OpenAI’s GPT-5.2 model into a LaTeX-based text editor (a standard used for typesetting documents), allowing researchers to draft papers, generate citations, create diagrams from whiteboard sketches, and collaborate with co-authors in real time. The tool is free for anyone with a ChatGPT account.&lt;/p&gt;
&lt;p&gt;“I think 2026 will be for AI and science what 2025 was for AI in software engineering,” Kevin Weil, vice president of OpenAI for Science, told reporters at a press briefing attended by MIT Technology Review. He said that ChatGPT receives about 8.4 million messages per week on “hard science” topics, which he described as evidence that AI is transitioning from curiosity to core workflow for scientists.&lt;/p&gt;
&lt;p&gt;OpenAI built Prism on technology from Crixet, a cloud-based LaTeX platform the company acquired in late 2025. The company envisions Prism helping researchers spend less time on tedious formatting tasks and more time on actual science. During a demonstration, an OpenAI employee showed how the software could automatically find and incorporate relevant scientific literature, then format the bibliography.&lt;/p&gt;
&lt;p&gt;But AI models are tools, and any tool can be misused. The risk here is specific: By making it easy to produce polished, professional-looking manuscripts, tools like Prism could flood the peer review system with papers that don’t meaningfully advance their fields. The barrier to producing science-flavored text is dropping, but the capacity to evaluate that research has not kept pace.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;When asked about the possibility of the AI model confabulating fake citations, Weil acknowledged in the press demo that “none of this absolves the scientist of the responsibility to verify that their references are correct.”&lt;/p&gt;
&lt;p&gt;Unlike traditional reference management software (such as EndNote), which has formatted citations for over 30 years without inventing them, AI models can generate plausible-sounding sources that don’t exist. Weil added: “We’re conscious that as AI becomes more capable, there are concerns around volume, quality, and trust in the scientific community.”&lt;/p&gt;
&lt;h2&gt;The slop problem&lt;/h2&gt;
&lt;p&gt;Those concerns are not hypothetical, as we have previously covered. A December 2025 study published in the journal Science found that researchers using large language models to write papers increased their output by 30 to 50 percent, depending on the field. But those AI-assisted papers performed worse in peer review. Papers with complex language written without AI assistance were most likely to be accepted by journals, while papers with complex language likely written by AI models were less likely to be accepted. Reviewers apparently recognized that sophisticated prose was masking weak science.&lt;/p&gt;
&lt;p&gt;“It is a very widespread pattern across different fields of science,” Yian Yin, an information science professor at Cornell University and one of the study’s authors, told the Cornell Chronicle. “There’s a big shift in our current ecosystem that warrants a very serious look, especially for those who make decisions about what science we should support and fund.”&lt;/p&gt;
&lt;p&gt;Another analysis of 41 million papers published between 1980 and 2025 found that while AI-using scientists receive more citations and publish more papers, the collective scope of scientific exploration appears to be narrowing. Lisa Messeri, a sociocultural anthropologist at Yale University, told Science magazine that these findings should set off “loud alarm bells” for the research community.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;“Science is nothing but a collective endeavor,” she said. “There needs to be some deep reckoning with what we do with a tool that benefits individuals but destroys science.”&lt;/p&gt;
&lt;p&gt;Concerns about AI-generated scientific content are not new. In 2022, Meta pulled a demo of Galactica, a large language model designed to write scientific literature, after users discovered it could generate convincing nonsense on any topic, including a wiki entry about a fictional research paper called “The benefits of eating crushed glass.” Two years later, Tokyo-based Sakana AI announced “The AI Scientist,” an autonomous research system that critics on Hacker News dismissed as producing “garbage” papers. “As an editor of a journal, I would likely desk-reject them,” one commenter wrote at the time. “They contain very limited novel knowledge.”&lt;/p&gt;
&lt;p&gt;The problem has only grown worse since then. In his first editorial of 2026 for Science, Editor-in-Chief H. Holden Thorp wrote that the journal is “less susceptible” to AI slop because of its size and human editorial investment, but he warned that “no system, human or artificial, can catch everything.” Science currently allows limited AI use for editing and gathering references but requires disclosure for anything beyond that and prohibits AI-generated figures.&lt;/p&gt;
&lt;p&gt;Mandy Hill, managing director of academic publishing at Cambridge University Press &amp;amp; Assessment, has been even more blunt. In October 2025, she told Retraction Watch that the publishing ecosystem is under strain and called for “radical change.” She explained to the University of Cambridge publication Varsity that “too many journal articles are being published, and this is causing huge strain” and warned that AI “will exacerbate” the problem.&lt;/p&gt;
&lt;h2&gt;Accelerating science or overwhelming peer review?&lt;/h2&gt;
&lt;p&gt;OpenAI is serious about leaning on its ability to accelerate science, and the company laid out its case for AI-assisted research in a report published earlier this week. It profiles researchers who say AI models have sped up their work, including a mathematician who used GPT-5.2 to solve an open problem in optimization over three evenings and a physicist who watched the model reproduce symmetry calculations that had taken him months to derive.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Those examples go beyond writing assistance into using AI for actual research work, a distinction OpenAI’s marketing intentionally blurs. For scientists who don’t speak English fluently, AI writing tools could legitimately accelerate the publication of good research. But that benefit may be offset by a flood of mediocre submissions jamming up an already strained peer-review system.&lt;/p&gt;
&lt;p&gt;Weil told MIT Technology Review that his goal is not to produce a single AI-generated discovery but rather “10,000 advances in science that maybe wouldn’t have happened or wouldn’t have happened as quickly.” He described this as “an incremental, compounding acceleration.”&lt;/p&gt;
&lt;p&gt;Whether that acceleration produces more scientific knowledge or simply more scientific papers remains to be seen. Nikita Zhivotovskiy, a statistician at UC Berkeley not connected to OpenAI, told MIT Technology Review that GPT-5 has already become valuable in his own work for polishing text and catching mathematical typos, making “interaction with the scientific literature smoother.”&lt;/p&gt;
&lt;p&gt;But by making papers look polished and professional regardless of their scientific merit, AI writing tools may help weak research clear the initial screening that editors and reviewers use to assess presentation quality. The risk is that conversational workflows obscure assumptions and blur accountability, and they might overwhelm the still very human peer review process required to vet it all.&lt;/p&gt;
&lt;p&gt;OpenAI appears aware of this tension. Its public statements about Prism emphasize that the tool will not conduct research independently and that human scientists remain responsible for verification.&lt;/p&gt;
&lt;p&gt;Still, one commenter on Hacker News captured the anxiety spreading through technical communities: “I’m scared that this type of thing is going to do to science journals what AI-generated bug reports is doing to bug bounties. We’re truly living in a post-scarcity society now, except that the thing we have an abundance of is garbage, and it’s drowning out everything of value.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/01/new-openai-tool-renews-fears-that-ai-slop-will-overwhelm-scientific-research/</guid><pubDate>Thu, 29 Jan 2026 17:51:49 +0000</pubDate></item><item><title>[NEW] Mercedes-Benz Unveils New S-Class Built on NVIDIA DRIVE AV, Which Enables an L4-Ready Architecture (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Mercedes-Benz is marking 140 years of automotive innovation with a new S-Class built for the AI era, bringing together automotive safety and NVIDIA’s advanced autonomous driving platform to enable a level 4-ready architecture designed for trust.&lt;/p&gt;
&lt;p&gt;The new S-Class with MB.OS, which will be equipped with the NVIDIA DRIVE Hyperion architecture and full-stack NVIDIA DRIVE AV L4 software, is designed to support future robotaxi operations — delivering safety-first autonomy with the NVIDIA Halos system and end-to-end AI and classical driving stacks running in parallel to ensure reliable operation.&lt;/p&gt;
&lt;p&gt;The S-Class enables a premium, chauffeur-style autonomous experience. As part of NVIDIA’s previously announced partnership with Uber, the companies will work together to make these autonomous vehicles available to riders through Uber’s mobility network.&lt;/p&gt;
&lt;p&gt;It showcases how legacy automakers and AI pioneers can work together to build vehicles that are safer, smarter and increasingly autonomous — without compromising the high standards of quality and safety customers expect.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;“Mercedes-Benz has set the standard in the automotive market, building cars defined by exquisite craftsmanship and safety engineering,” said Jensen Huang, founder and CEO of NVIDIA, in the above video celebrating the S-Class launch. “Five years ago, NVIDIA began working with Mercedes-Benz to help carry that legacy into the AI era.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;L4-Ready Architecture Powered by NVIDIA DRIVE AV&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Traditional autonomous driving approaches often rely on predefined rules or learned responses to familiar situations. But real-world driving is filled with rare and complex edge cases — from unpredictable pedestrian behavior and debris to unusual road conditions and aggressive cut-ins.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE AV provides Mercedes-Benz’s new S-Class with a full-stack automated driving system designed to handle this long tail of driving scenarios, while remaining anchored to a safety-first architecture.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE AV is trained at scale on NVIDIA DGX systems and designed to be validated &amp;nbsp; using high-fidelity simulation with NVIDIA Omniverse NuRec libraries and NVIDIA Cosmos world models.&lt;/p&gt;
&lt;p&gt;Built on NVIDIA’s broader AI foundation — including advanced perception, planning and reasoning technologies — NVIDIA DRIVE AV is optimized, validated and distilled to run reliably in production vehicles, tailored to Mercedes-Benz’s vehicle platforms and sensor configurations.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE AV enables the system to analyze complex environments — rather than simply reacting to known patterns — evaluate multiple options and select the safest possible outcome in real time.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Diversity by Design With NVIDIA DRIVE Hyperion for Real-World Mobility&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For level 4 autonomy, safety depends on more than simple redundancy. Vehicles must remain operational in the face of hardware faults, sensor degradation and unexpected software behavior.&lt;/p&gt;
&lt;p&gt;The new S-Class will be built on NVIDIA DRIVE Hyperion, a reference architecture that integrates sensor diversity and hardware redundancy into a unified platform, to serve as a robotaxi.&lt;/p&gt;
&lt;p&gt;DRIVE Hyperion is designed based on defense-in-depth principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redundant compute to help maintain operation if one processing element fails.&lt;/li&gt;
&lt;li&gt;Multimodal sensor diversity — spanning cameras, radar and lidar — to support robust perception.&lt;/li&gt;
&lt;li&gt;Software stack diversity, pairing AI-driven decision-making with a parallel classical safety stack to keep the vehicle operating within safe boundaries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Developed in accordance with NVIDIA Halos safety system, NVIDIA DRIVE Hyperion helps eliminate single points of failure and provides the foundation needed for L4-ready systems.&lt;/p&gt;
&lt;p&gt;This safety-first, resilient platform is mainly designed for premium robotaxi and chauffeured mobility services — enabling reliable, large-scale deployment in real-world environments.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-89530" height="641" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/mb-s-class-2026-960x641.jpg" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;From AI Foundations to Production-Ready Autonomy&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s broader AI ecosystem — including the NVIDIA Alpamayo family of open models, simulation tools and datasets for autonomous vehicles — enables developers and partners to advance autonomous driving research and build their own driving software.&lt;/p&gt;
&lt;p&gt;Within NVIDIA DRIVE AV, these AI capabilities are further refined, optimized and engineered for production. It ensures reliable operation on automotive-grade hardware, with NVIDIA Halos applying strict safety standards to the AI pipeline, as well as seamless integration with Mercedes-Benz’s specific sensor and vehicle architectures.&lt;/p&gt;
&lt;p&gt;This production-grade approach — combining large-scale training, high-fidelity simulation, rigorous safety validation and deep system integration — is what allows NVIDIA DRIVE AV to support both level 2 point-to-point and level 4-ready automated driving systems in customer vehicles.&lt;/p&gt;
&lt;p&gt;Building on this foundation, Mercedes-Benz and NVIDIA are partnering to deliver an L4-ready version of the new S-Class, bringing advanced AI and safety-focused autonomy to the road.&lt;/p&gt;
&lt;p&gt;At the core of this work is NVIDIA Alpamayo, which enables vehicles to drive smoothly and naturally like a human driver while reasoning step by step through complex situations to choose the safest possible action — since safety is paramount.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Bringing Safety Engineering Into the Autonomous Driving Era&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As AI becomes central to vehicle intelligence, the definition of “the safest car” is evolving. Beyond protecting occupants in a crash, modern vehicles are increasingly designed to help prevent accidents in the first place.&lt;/p&gt;
&lt;p&gt;Built on NVIDIA DRIVE Hyperion and full-stack NVIDIA DRIVE AV software, the next-generation S-Class extends Mercedes-Benz’s long-standing safety leadership into the AI era. Its L4-ready architecture combines end-to-end AI with parallel classical driving stacks, delivering predictable, reliable operation through a diverse, multi-layered system design.&lt;/p&gt;
&lt;p&gt;This approach reflects a broader shift toward active, intelligent safety — a trend already recognized by independent testing, including the Mercedes-Benz CLA’s designation as Euro NCAP’s Best Performer of 2025.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Together, Mercedes-Benz and NVIDIA are demonstrating how legacy automakers and AI pioneers can collaborate to deliver vehicles that are safer, smarter and increasingly autonomous — without compromising the craftsmanship, comfort and quality customers expect.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Mercedes-Benz is marking 140 years of automotive innovation with a new S-Class built for the AI era, bringing together automotive safety and NVIDIA’s advanced autonomous driving platform to enable a level 4-ready architecture designed for trust.&lt;/p&gt;
&lt;p&gt;The new S-Class with MB.OS, which will be equipped with the NVIDIA DRIVE Hyperion architecture and full-stack NVIDIA DRIVE AV L4 software, is designed to support future robotaxi operations — delivering safety-first autonomy with the NVIDIA Halos system and end-to-end AI and classical driving stacks running in parallel to ensure reliable operation.&lt;/p&gt;
&lt;p&gt;The S-Class enables a premium, chauffeur-style autonomous experience. As part of NVIDIA’s previously announced partnership with Uber, the companies will work together to make these autonomous vehicles available to riders through Uber’s mobility network.&lt;/p&gt;
&lt;p&gt;It showcases how legacy automakers and AI pioneers can work together to build vehicles that are safer, smarter and increasingly autonomous — without compromising the high standards of quality and safety customers expect.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;“Mercedes-Benz has set the standard in the automotive market, building cars defined by exquisite craftsmanship and safety engineering,” said Jensen Huang, founder and CEO of NVIDIA, in the above video celebrating the S-Class launch. “Five years ago, NVIDIA began working with Mercedes-Benz to help carry that legacy into the AI era.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;L4-Ready Architecture Powered by NVIDIA DRIVE AV&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Traditional autonomous driving approaches often rely on predefined rules or learned responses to familiar situations. But real-world driving is filled with rare and complex edge cases — from unpredictable pedestrian behavior and debris to unusual road conditions and aggressive cut-ins.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE AV provides Mercedes-Benz’s new S-Class with a full-stack automated driving system designed to handle this long tail of driving scenarios, while remaining anchored to a safety-first architecture.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE AV is trained at scale on NVIDIA DGX systems and designed to be validated &amp;nbsp; using high-fidelity simulation with NVIDIA Omniverse NuRec libraries and NVIDIA Cosmos world models.&lt;/p&gt;
&lt;p&gt;Built on NVIDIA’s broader AI foundation — including advanced perception, planning and reasoning technologies — NVIDIA DRIVE AV is optimized, validated and distilled to run reliably in production vehicles, tailored to Mercedes-Benz’s vehicle platforms and sensor configurations.&lt;/p&gt;
&lt;p&gt;NVIDIA DRIVE AV enables the system to analyze complex environments — rather than simply reacting to known patterns — evaluate multiple options and select the safest possible outcome in real time.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Diversity by Design With NVIDIA DRIVE Hyperion for Real-World Mobility&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For level 4 autonomy, safety depends on more than simple redundancy. Vehicles must remain operational in the face of hardware faults, sensor degradation and unexpected software behavior.&lt;/p&gt;
&lt;p&gt;The new S-Class will be built on NVIDIA DRIVE Hyperion, a reference architecture that integrates sensor diversity and hardware redundancy into a unified platform, to serve as a robotaxi.&lt;/p&gt;
&lt;p&gt;DRIVE Hyperion is designed based on defense-in-depth principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redundant compute to help maintain operation if one processing element fails.&lt;/li&gt;
&lt;li&gt;Multimodal sensor diversity — spanning cameras, radar and lidar — to support robust perception.&lt;/li&gt;
&lt;li&gt;Software stack diversity, pairing AI-driven decision-making with a parallel classical safety stack to keep the vehicle operating within safe boundaries.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Developed in accordance with NVIDIA Halos safety system, NVIDIA DRIVE Hyperion helps eliminate single points of failure and provides the foundation needed for L4-ready systems.&lt;/p&gt;
&lt;p&gt;This safety-first, resilient platform is mainly designed for premium robotaxi and chauffeured mobility services — enabling reliable, large-scale deployment in real-world environments.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-89530" height="641" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/mb-s-class-2026-960x641.jpg" width="960" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;From AI Foundations to Production-Ready Autonomy&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA’s broader AI ecosystem — including the NVIDIA Alpamayo family of open models, simulation tools and datasets for autonomous vehicles — enables developers and partners to advance autonomous driving research and build their own driving software.&lt;/p&gt;
&lt;p&gt;Within NVIDIA DRIVE AV, these AI capabilities are further refined, optimized and engineered for production. It ensures reliable operation on automotive-grade hardware, with NVIDIA Halos applying strict safety standards to the AI pipeline, as well as seamless integration with Mercedes-Benz’s specific sensor and vehicle architectures.&lt;/p&gt;
&lt;p&gt;This production-grade approach — combining large-scale training, high-fidelity simulation, rigorous safety validation and deep system integration — is what allows NVIDIA DRIVE AV to support both level 2 point-to-point and level 4-ready automated driving systems in customer vehicles.&lt;/p&gt;
&lt;p&gt;Building on this foundation, Mercedes-Benz and NVIDIA are partnering to deliver an L4-ready version of the new S-Class, bringing advanced AI and safety-focused autonomy to the road.&lt;/p&gt;
&lt;p&gt;At the core of this work is NVIDIA Alpamayo, which enables vehicles to drive smoothly and naturally like a human driver while reasoning step by step through complex situations to choose the safest possible action — since safety is paramount.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Bringing Safety Engineering Into the Autonomous Driving Era&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As AI becomes central to vehicle intelligence, the definition of “the safest car” is evolving. Beyond protecting occupants in a crash, modern vehicles are increasingly designed to help prevent accidents in the first place.&lt;/p&gt;
&lt;p&gt;Built on NVIDIA DRIVE Hyperion and full-stack NVIDIA DRIVE AV software, the next-generation S-Class extends Mercedes-Benz’s long-standing safety leadership into the AI era. Its L4-ready architecture combines end-to-end AI with parallel classical driving stacks, delivering predictable, reliable operation through a diverse, multi-layered system design.&lt;/p&gt;
&lt;p&gt;This approach reflects a broader shift toward active, intelligent safety — a trend already recognized by independent testing, including the Mercedes-Benz CLA’s designation as Euro NCAP’s Best Performer of 2025.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Together, Mercedes-Benz and NVIDIA are demonstrating how legacy automakers and AI pioneers can collaborate to deliver vehicles that are safer, smarter and increasingly autonomous — without compromising the craftsmanship, comfort and quality customers expect.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/mercedes-benz-l4-s-class-drive-av-platform/</guid><pubDate>Thu, 29 Jan 2026 18:00:31 +0000</pubDate></item><item><title>[NEW] DHS is using Google and Adobe AI to make videos (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/29/1131938/dhs-is-using-google-and-adobe-ai-to-make-videos/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/AP26008686962278.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The US Department of Homeland Security is using AI video generators from Google and Adobe to make and edit content shared with the public, a new document reveals. It comes as immigration agencies have flooded social media with content to support President Trump's mass deportation agenda—some of which appears to be made with AI—and as workers in tech have put pressure on their employers to denounce the agencies’ activities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The document, released on Wednesday, provides an inventory of which commercial AI tools DHS uses for tasks ranging from generating drafts of documents to managing cybersecurity.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;In a section about “editing images, videos or other public affairs materials using AI,” it reveals for the first time that DHS is using Google's Veo 3 video generator and Adobe Firefly, estimating that the agency has between 100 and 1,000 licenses for the tools. It also discloses that DHS uses Microsoft Copilot Chat for generating first drafts of documents and summarizing long reports and Poolside software for coding tasks, in addition to tools from other companies.&lt;/p&gt;  &lt;p&gt;Google, Adobe, and DHS did not immediately respond to requests for comment.&lt;/p&gt; 
 &lt;p&gt;The news provides details about how agencies like Immigrations and Customs Enforcement, which is part of DHS, might be creating the large amounts of content they’ve shared on X and other channels as immigration operations have expanded across US cities. They’ve posted content celebrating “Christmas after mass deportations,” referenced Bible verses and Christ’s birth, showed faces of those the agency has arrested, and shared ads aimed at recruiting agents. The agencies have also repeatedly used music without permissions from artists in their videos.&lt;/p&gt;  &lt;p&gt;Some of the content, particularly videos, has the appearance of being AI-generated, but it hasn’t been clear until now what AI models the agencies might be using. This marks the first concrete evidence such generators are being used by DHS to create content shared with the public.&lt;/p&gt; 
 &lt;p&gt;It still remains impossible to verify which company helped create a specific piece of content, or indeed if it was AI-generated at all. Adobe offers options to “watermark” a video made with its tools to disclose that it is AI-generated, for example, but this disclosure does not always stay intact when the content is uploaded and shared across different sites.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The document reveals that DHS has specifically been using Flow, a tool from Google that combines its Veo 3 video generator with a suite of filmmaking tools. Users can generate clips and assemble entire videos with AI, including videos that contain sound, dialogue, and background noise, making them hyperrealistic. Adobe launched its Firefly generator in 2023, promising that it does not use copyrighted content in its training or output. Like Google’s tools, Adobe’s can generate videos, images, soundtracks, and speech. The document does not reveal further details about how the agency is using these video generation tools.&lt;/p&gt;  &lt;p&gt;Workers at large tech companies, including more than 140 current and former employees from Google and more than 30 from Adobe, have been putting pressure on their employers in recent weeks to take a stance against ICE and the shooting of Alex Pretti on January 24. Google’s leadership has not made statements in response. In October, Google and Apple removed apps on their app stores that were intended to track sightings of ICE, citing safety risks.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An additional document released on Wednesday revealed new details about how the agency is using more niche AI products, including a facial recognition app used by ICE, as first reported by 404Media in June.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/AP26008686962278.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;The US Department of Homeland Security is using AI video generators from Google and Adobe to make and edit content shared with the public, a new document reveals. It comes as immigration agencies have flooded social media with content to support President Trump's mass deportation agenda—some of which appears to be made with AI—and as workers in tech have put pressure on their employers to denounce the agencies’ activities.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The document, released on Wednesday, provides an inventory of which commercial AI tools DHS uses for tasks ranging from generating drafts of documents to managing cybersecurity.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;In a section about “editing images, videos or other public affairs materials using AI,” it reveals for the first time that DHS is using Google's Veo 3 video generator and Adobe Firefly, estimating that the agency has between 100 and 1,000 licenses for the tools. It also discloses that DHS uses Microsoft Copilot Chat for generating first drafts of documents and summarizing long reports and Poolside software for coding tasks, in addition to tools from other companies.&lt;/p&gt;  &lt;p&gt;Google, Adobe, and DHS did not immediately respond to requests for comment.&lt;/p&gt; 
 &lt;p&gt;The news provides details about how agencies like Immigrations and Customs Enforcement, which is part of DHS, might be creating the large amounts of content they’ve shared on X and other channels as immigration operations have expanded across US cities. They’ve posted content celebrating “Christmas after mass deportations,” referenced Bible verses and Christ’s birth, showed faces of those the agency has arrested, and shared ads aimed at recruiting agents. The agencies have also repeatedly used music without permissions from artists in their videos.&lt;/p&gt;  &lt;p&gt;Some of the content, particularly videos, has the appearance of being AI-generated, but it hasn’t been clear until now what AI models the agencies might be using. This marks the first concrete evidence such generators are being used by DHS to create content shared with the public.&lt;/p&gt; 
 &lt;p&gt;It still remains impossible to verify which company helped create a specific piece of content, or indeed if it was AI-generated at all. Adobe offers options to “watermark” a video made with its tools to disclose that it is AI-generated, for example, but this disclosure does not always stay intact when the content is uploaded and shared across different sites.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The document reveals that DHS has specifically been using Flow, a tool from Google that combines its Veo 3 video generator with a suite of filmmaking tools. Users can generate clips and assemble entire videos with AI, including videos that contain sound, dialogue, and background noise, making them hyperrealistic. Adobe launched its Firefly generator in 2023, promising that it does not use copyrighted content in its training or output. Like Google’s tools, Adobe’s can generate videos, images, soundtracks, and speech. The document does not reveal further details about how the agency is using these video generation tools.&lt;/p&gt;  &lt;p&gt;Workers at large tech companies, including more than 140 current and former employees from Google and more than 30 from Adobe, have been putting pressure on their employers in recent weeks to take a stance against ICE and the shooting of Alex Pretti on January 24. Google’s leadership has not made statements in response. In October, Google and Apple removed apps on their app stores that were intended to track sightings of ICE, citing safety risks.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;An additional document released on Wednesday revealed new details about how the agency is using more niche AI products, including a facial recognition app used by ICE, as first reported by 404Media in June.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/29/1131938/dhs-is-using-google-and-adobe-ai-to-make-videos/</guid><pubDate>Thu, 29 Jan 2026 18:57:11 +0000</pubDate></item><item><title>[NEW] Apple buys Israeli startup Q.ai as the AI race heats up (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/apple-buys-israeli-startup-q-ai-as-the-ai-race-heats-up/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/apple-on-phone.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple, Meta, and Google are locked in a fierce battle to lead the next wave of AI, and they’ve recently increased their focus on hardware. With its latest acquisition of the AI startup Q.ai, Apple aims to gain an edge, particularly in the audio sector.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;​As first reported by Reuters, Apple has acquired Q.ai, an Israeli startup specializing in imaging and machine learning, particularly technologies that enable devices to interpret whispered speech and enhance audio in noisy environments. Apple has been adding new AI features to its AirPods, including the live translation capability introduced last year.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has also developed technology that detects subtle facial muscle activity, which could help the tech giant enhance the Vision Pro headset.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Financial Times reported that the deal is valued at nearly $2 billion, making it Apple’s second-largest acquisition to date, after buying Beats Electronics for $3 billion in 2014.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;​Notably, this is the second time CEO Aviad Maizels has sold a company to Apple. In 2013, he sold PrimeSense, a 3D-sensing company that played a key role in Apple’s transition from fingerprint sensors to facial recognition on iPhones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Q.ai launched in 2022 and is backed by Kleiner Perkins, Gradient Ventures, and others. ​Its founding team, including Maizels and co-founders Yonatan Wexler and Avi Barliya, will join Apple as part of the acquisition.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news comes a few hours ahead of Apple’s first quarterly earnings, in which analysts are estimating revenue at around $138 billion. It’s also expected to be the company’s strongest iPhone sales growth in four years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/apple-on-phone.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple, Meta, and Google are locked in a fierce battle to lead the next wave of AI, and they’ve recently increased their focus on hardware. With its latest acquisition of the AI startup Q.ai, Apple aims to gain an edge, particularly in the audio sector.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;​As first reported by Reuters, Apple has acquired Q.ai, an Israeli startup specializing in imaging and machine learning, particularly technologies that enable devices to interpret whispered speech and enhance audio in noisy environments. Apple has been adding new AI features to its AirPods, including the live translation capability introduced last year.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company has also developed technology that detects subtle facial muscle activity, which could help the tech giant enhance the Vision Pro headset.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Financial Times reported that the deal is valued at nearly $2 billion, making it Apple’s second-largest acquisition to date, after buying Beats Electronics for $3 billion in 2014.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;​Notably, this is the second time CEO Aviad Maizels has sold a company to Apple. In 2013, he sold PrimeSense, a 3D-sensing company that played a key role in Apple’s transition from fingerprint sensors to facial recognition on iPhones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Q.ai launched in 2022 and is backed by Kleiner Perkins, Gradient Ventures, and others. ​Its founding team, including Maizels and co-founders Yonatan Wexler and Avi Barliya, will join Apple as part of the acquisition.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news comes a few hours ahead of Apple’s first quarterly earnings, in which analysts are estimating revenue at around $138 billion. It’s also expected to be the company’s strongest iPhone sales growth in four years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/apple-buys-israeli-startup-q-ai-as-the-ai-race-heats-up/</guid><pubDate>Thu, 29 Jan 2026 18:58:07 +0000</pubDate></item><item><title>[NEW] Satya Nadella insists people are using Microsoft’s Copilot AI a lot (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/satya-nadella-insists-people-are-using-microsofts-copilot-ai-a-lot/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/05/gettyimages-915446280.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft delivered a solid earnings report on Wednesday with $81.3 billion in revenue for the quarter (up 17%), net income profits of $38.3 billion (up 21%), and a record-breaking Microsoft cloud revenue of over $50 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the stock was getting pounded on Thursday as investors worried about how much the tech giant was spending to build out its cloud and questioned whether that investment would pay off. Microsoft CEO Satya Nadella says the answer to that question is yes &amp;nbsp;— and spent considerable time on the earnings call trying to make that point.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft has spent almost as much on capital expenditures in the first half of its current fiscal year as it did in all of the previous year.&amp;nbsp;And the numbers truly are enormous: Microsoft spent $88.2 billion on capital expenditures last year and has spent $72.4 billion so far this year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Much of that spend is to serve AI to enterprises and major AI labs, especially OpenAI, as well as Anthropic. The big question on investors’ minds is: Will the spending turn into more use and ultimately profits?&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors are scared that Microsoft’s main enterprise cloud product, Azure, and its Microsoft 365 apps didn’t grow as fast as they wanted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The fact that BOTH Azure and the M365 segments fell a bit short is the key negative we’re hearing,” Wall Street analyst for UBS, Karl Keirstead, wrote in his research note on Thursday. (Keirstead isn’t worried about it, though, and recommends buying the stock.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, a few months ago, news reports circulated that people didn’t really want to use Microsoft’s AI, despite Copilot being weaved into all kinds of Microsoft products.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Nadella spent much of his time during the earnings call engaged in what is best described as AI use PR. Despite his pitch, some of the numbers he gave were pretty squishy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Nadella said daily users of its consumer Copilot AI products had grown “nearly 3x year-over-year.” This refers to AI chats, the news feed, search, browsing, shopping, and “integrations into the operating system.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As to how many actual users that represents, he didn’t say. (We’ve reached out to Microsoft and asked.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, in its annual report, the company said it surpassed 100 million monthly active Copilot users, but that counted both commercial users and consumers.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He was more up front with Microsoft’s coding AI, GitHub Copilot, saying it now has 4.7 million paid subscribers, up 75% year-over-year. That appears to be a healthy business. Last year, in its annual report, Microsoft said that GitHub Copilot had 20 million users, a figure that includes those opting for the free tier.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also said Microsoft 365 Copilot now has 15 million paid seats from companies buying it for their employees. This is out of a base of 450 million paid seats, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And Nadella called out the growth of Dragon Copilot, Microsoft’s healthcare AI agent for medical professionals (a competitor to superhot startup Harvey). He said this product is available to 100,000 medical providers and was used to document 21 million patient encounters over the quarter, up threefold year-over-year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Will the billions spent on data centers be worth it? Nadella obviously thinks so. He and CFO Amy Hood said on the earnings call that demand for AI services across products far outstrips data center supply, so all of the new equipment is essentially booked to capacity for its lifespan.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/05/gettyimages-915446280.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Microsoft delivered a solid earnings report on Wednesday with $81.3 billion in revenue for the quarter (up 17%), net income profits of $38.3 billion (up 21%), and a record-breaking Microsoft cloud revenue of over $50 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the stock was getting pounded on Thursday as investors worried about how much the tech giant was spending to build out its cloud and questioned whether that investment would pay off. Microsoft CEO Satya Nadella says the answer to that question is yes &amp;nbsp;— and spent considerable time on the earnings call trying to make that point.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft has spent almost as much on capital expenditures in the first half of its current fiscal year as it did in all of the previous year.&amp;nbsp;And the numbers truly are enormous: Microsoft spent $88.2 billion on capital expenditures last year and has spent $72.4 billion so far this year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Much of that spend is to serve AI to enterprises and major AI labs, especially OpenAI, as well as Anthropic. The big question on investors’ minds is: Will the spending turn into more use and ultimately profits?&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Investors are scared that Microsoft’s main enterprise cloud product, Azure, and its Microsoft 365 apps didn’t grow as fast as they wanted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The fact that BOTH Azure and the M365 segments fell a bit short is the key negative we’re hearing,” Wall Street analyst for UBS, Karl Keirstead, wrote in his research note on Thursday. (Keirstead isn’t worried about it, though, and recommends buying the stock.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, a few months ago, news reports circulated that people didn’t really want to use Microsoft’s AI, despite Copilot being weaved into all kinds of Microsoft products.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Nadella spent much of his time during the earnings call engaged in what is best described as AI use PR. Despite his pitch, some of the numbers he gave were pretty squishy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Nadella said daily users of its consumer Copilot AI products had grown “nearly 3x year-over-year.” This refers to AI chats, the news feed, search, browsing, shopping, and “integrations into the operating system.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As to how many actual users that represents, he didn’t say. (We’ve reached out to Microsoft and asked.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, in its annual report, the company said it surpassed 100 million monthly active Copilot users, but that counted both commercial users and consumers.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He was more up front with Microsoft’s coding AI, GitHub Copilot, saying it now has 4.7 million paid subscribers, up 75% year-over-year. That appears to be a healthy business. Last year, in its annual report, Microsoft said that GitHub Copilot had 20 million users, a figure that includes those opting for the free tier.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also said Microsoft 365 Copilot now has 15 million paid seats from companies buying it for their employees. This is out of a base of 450 million paid seats, the company said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And Nadella called out the growth of Dragon Copilot, Microsoft’s healthcare AI agent for medical professionals (a competitor to superhot startup Harvey). He said this product is available to 100,000 medical providers and was used to document 21 million patient encounters over the quarter, up threefold year-over-year.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Will the billions spent on data centers be worth it? Nadella obviously thinks so. He and CFO Amy Hood said on the earnings call that demand for AI services across products far outstrips data center supply, so all of the new equipment is essentially booked to capacity for its lifespan.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/satya-nadella-insists-people-are-using-microsofts-copilot-ai-a-lot/</guid><pubDate>Thu, 29 Jan 2026 20:06:56 +0000</pubDate></item><item><title>[NEW] Google Project Genie lets you create interactive worlds from a photo or prompt (AI - Ars Technica)</title><link>https://arstechnica.com/google/2026/01/google-project-genie-lets-you-create-interactive-worlds-from-a-photo-or-prompt/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Project Genie lets you generate new worlds 60 seconds at a time, but only if you pay for AI Ultra.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Genie world bubbles" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Genie-worlds-640x360.png" width="640" /&gt;
                  &lt;img alt="Genie world bubbles" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Genie-worlds-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last year, Google showed off Genie 3, an updated version of its AI world model with impressive long-term memory that allowed it to create interactive worlds from a simple text prompt. At the time, Google only provided Genie to a small group of trusted testers. Now, it’s available more widely as Project Genie, but only for those paying for Google’s most expensive AI subscription.&lt;/p&gt;
&lt;p&gt;World models are exactly what they sound like—an AI that generates a dynamic environment on the fly. They’re not technically 3D worlds, though. World models like Genie 3 create a video that responds to your control inputs, allowing you to explore the simulation as if it were a real virtual world. Genie 3 was a breakthrough in world models because it could remember details of the world it was creating for a much longer time. But in this context, a “long time” is a couple of minutes.&lt;/p&gt;
&lt;p&gt;Project Genie is essentially a cleaned-up version of Genie 3, which plugs into updated AI models like Nano Banana Pro and Gemini 3. Google has a number of pre-built worlds available in Project Genie, but it’s the ability to create new things that makes it interesting. You can provide an image for reference or simply tell Genie what you want from the environment and the character.&lt;/p&gt;
&lt;p&gt;The system first generates a still image, and from that you can generate the world. This is what Google calls “world sketching.” If you don’t like the reference image created by Nano Banana Pro, you can make changes before handing it off to Genie.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Project Genie | Experimenting with infinite interactive worlds.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The resulting video is 720p, rendering at around 24 frames per second. As you move your character around with WASD, Genie renders the path ahead in something approaching real time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;If that 60-second jaunt into the AI world isn’t enough, you can just run the prompt again. Because this is generative AI, the results will be a little different each time. Google also lets you “remix” its pre-built worlds with new characters and visual styles. The video generated of your exploration is available for download as well.&lt;/p&gt;
&lt;h2&gt;Still an experiment&lt;/h2&gt;
&lt;p&gt;Google stresses that Project Genie is still just a research prototype, and there are, therefore, some notable limitations. As anyone who has used Google Veo or OpenAI Sora to create AI videos will know, it takes a few seconds to create even a short clip. So, it’s impressive that Genie can make it feel interactive at all. However, there will be some input lag, and you can only explore each world for 60 seconds. In addition, the promotable events feature previously demoed for Genie 3, which allows inserting new elements into a running simulation, is not available yet.&lt;/p&gt;
&lt;p&gt;While Google has talked up Genie’s ability to accurately model physics, the company notes that testers will probably see examples of worlds that don’t look or behave quite right. Testers may also see changing restrictions on content. The Verge was able to test Project Genie, and initially, it was happy to generate knockoffs of Nintendo games like &lt;em&gt;Super Mario&lt;/em&gt; and &lt;em&gt;The Legend of Zelda&lt;/em&gt;. By the end of the test, The Verge reports that some of those prompts were being blocked due to “interests of third-party content providers.”&lt;/p&gt;
&lt;p&gt;Project Genie is only accessible from a dedicated web app—it won’t be plugged into the Gemini app or website. You can only access this tool for the time being with an AI Ultra subscription, which runs $250 per month. Generating all this AI video is expensive, so it makes sense to start with the higher tier. Google says its goal is to open up access to Project Genie over time.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Project Genie lets you generate new worlds 60 seconds at a time, but only if you pay for AI Ultra.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Genie world bubbles" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Genie-worlds-640x360.png" width="640" /&gt;
                  &lt;img alt="Genie world bubbles" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/Genie-worlds-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last year, Google showed off Genie 3, an updated version of its AI world model with impressive long-term memory that allowed it to create interactive worlds from a simple text prompt. At the time, Google only provided Genie to a small group of trusted testers. Now, it’s available more widely as Project Genie, but only for those paying for Google’s most expensive AI subscription.&lt;/p&gt;
&lt;p&gt;World models are exactly what they sound like—an AI that generates a dynamic environment on the fly. They’re not technically 3D worlds, though. World models like Genie 3 create a video that responds to your control inputs, allowing you to explore the simulation as if it were a real virtual world. Genie 3 was a breakthrough in world models because it could remember details of the world it was creating for a much longer time. But in this context, a “long time” is a couple of minutes.&lt;/p&gt;
&lt;p&gt;Project Genie is essentially a cleaned-up version of Genie 3, which plugs into updated AI models like Nano Banana Pro and Gemini 3. Google has a number of pre-built worlds available in Project Genie, but it’s the ability to create new things that makes it interesting. You can provide an image for reference or simply tell Genie what you want from the environment and the character.&lt;/p&gt;
&lt;p&gt;The system first generates a still image, and from that you can generate the world. This is what Google calls “world sketching.” If you don’t like the reference image created by Nano Banana Pro, you can make changes before handing it off to Genie.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Project Genie | Experimenting with infinite interactive worlds.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The resulting video is 720p, rendering at around 24 frames per second. As you move your character around with WASD, Genie renders the path ahead in something approaching real time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;If that 60-second jaunt into the AI world isn’t enough, you can just run the prompt again. Because this is generative AI, the results will be a little different each time. Google also lets you “remix” its pre-built worlds with new characters and visual styles. The video generated of your exploration is available for download as well.&lt;/p&gt;
&lt;h2&gt;Still an experiment&lt;/h2&gt;
&lt;p&gt;Google stresses that Project Genie is still just a research prototype, and there are, therefore, some notable limitations. As anyone who has used Google Veo or OpenAI Sora to create AI videos will know, it takes a few seconds to create even a short clip. So, it’s impressive that Genie can make it feel interactive at all. However, there will be some input lag, and you can only explore each world for 60 seconds. In addition, the promotable events feature previously demoed for Genie 3, which allows inserting new elements into a running simulation, is not available yet.&lt;/p&gt;
&lt;p&gt;While Google has talked up Genie’s ability to accurately model physics, the company notes that testers will probably see examples of worlds that don’t look or behave quite right. Testers may also see changing restrictions on content. The Verge was able to test Project Genie, and initially, it was happy to generate knockoffs of Nintendo games like &lt;em&gt;Super Mario&lt;/em&gt; and &lt;em&gt;The Legend of Zelda&lt;/em&gt;. By the end of the test, The Verge reports that some of those prompts were being blocked due to “interests of third-party content providers.”&lt;/p&gt;
&lt;p&gt;Project Genie is only accessible from a dedicated web app—it won’t be plugged into the Gemini app or website. You can only access this tool for the time being with an AI Ultra subscription, which runs $250 per month. Generating all this AI video is expensive, so it makes sense to start with the higher tier. Google says its goal is to open up access to Project Genie over time.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2026/01/google-project-genie-lets-you-create-interactive-worlds-from-a-photo-or-prompt/</guid><pubDate>Thu, 29 Jan 2026 20:26:50 +0000</pubDate></item><item><title>[NEW] The AI Hype Index: Grok makes porn, and Claude Code nails your job (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/29/1131787/the-ai-hype-index-grok-makes-porn-claude-code-nails-your-job/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/January-thumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Everyone is panicking because AI is very bad; everyone is panicking because AI is very good. It’s just that you never know which one you’re going to get. Grok is a pornography machine. Claude Code can do anything from building websites to reading your MRI. So of course Gen Z is spooked by what this means for jobs. Unnerving new research says AI is going to have a seismic impact on the labor market this year.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="cst-block "&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;If you want to get a handle on all that, don’t expect any help from the AI companies—they’re turning on each other like it’s the last act in a zombie movie. Meta’s former chief AI scientist, Yann LeCun, is spilling tea, while Big Tech’s messiest exes, Elon Musk and OpenAI, are about to go to trial. Grab your popcorn.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/January-thumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Everyone is panicking because AI is very bad; everyone is panicking because AI is very good. It’s just that you never know which one you’re going to get. Grok is a pornography machine. Claude Code can do anything from building websites to reading your MRI. So of course Gen Z is spooked by what this means for jobs. Unnerving new research says AI is going to have a seismic impact on the labor market this year.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="cst-block "&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;  &lt;p&gt;If you want to get a handle on all that, don’t expect any help from the AI companies—they’re turning on each other like it’s the last act in a zombie movie. Meta’s former chief AI scientist, Yann LeCun, is spilling tea, while Big Tech’s messiest exes, Elon Musk and OpenAI, are about to go to trial. Grab your popcorn.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/29/1131787/the-ai-hype-index-grok-makes-porn-claude-code-nails-your-job/</guid><pubDate>Thu, 29 Jan 2026 20:56:23 +0000</pubDate></item><item><title>[NEW] How often do AI chatbots lead users down a harmful path? (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/01/how-often-do-ai-chatbots-lead-users-down-a-harmful-path/</link><description>&lt;article class="double-column h-entry post-2138337 post type-post status-publish format-standard has-post-thumbnail hentry category-ai tag-ai tag-anthropic tag-artificial-intelligence tag-claude tag-disempowerment tag-research"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic’s latest paper on “user disempowerment” has some troubling findings.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-758288177-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-758288177-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Wake up, sheeple!

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;At this point, we’ve all heard plenty of stories about AI chatbots leading users to harmful actions, harmful beliefs, or simply incorrect information. Despite the prevalence of these stories, though, it’s hard to know just how often users are being manipulated. Are these tales of AI harms anecdotal outliers or signs of a frighteningly common problem?&lt;/p&gt;
&lt;p&gt;Anthropic took a stab at answer ingthat question this week, releasing a paper studying the potential for what it calls “disempowering patterns” across 1.5 million anonymized real-world conversations with its Claude AI model. While the results show that these kinds of manipulative patterns are relatively rare as a percentage of all AI conversations, they still represent a potentially large problem on an absolute basis.&lt;/p&gt;
&lt;h2&gt;A rare but growing problem&lt;/h2&gt;
&lt;p&gt;In the newly published paper “Who’s in Charge? Disempowerment Patterns in Real-World LLM Usage,” researchers from Anthropic&amp;nbsp;and the University of Toronto try to quantify the potential for a specific set of “user disempowering” harms by identifying three primary ways that a chatbot can negatively impact a user’s thoughts or actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reality distortion:&lt;/strong&gt; Their beliefs about reality become less accurate (e.g., a chatbot validates their belief in a conspiracy theory)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Belief distortion:&lt;/strong&gt; Their value judgments shift away from those they actually hold (e.g., a user begins to see a relationship as “manipulative” based on Claude’s evaluation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action distortion:&lt;/strong&gt; Their actions become misaligned with their values (e.g., a user disregards their instincts and follows Claude-written instructions for confronting their boss)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="ars-wp-img-shortcode id-2138344 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="684" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/dismepowergraph.png" width="974" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      While “severe” examples of potentially disempowering responses are relatively rare, “mild” ones are pretty common.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;To figure out when a chatbot conversation has the potential to move a user along one of these lines, Anthropic ran nearly 1.5 million Claude conversations through Clio, an automated analysis tool and classification system (tested to make sure it lined up with a smaller subsample of human classifications). That analysis found a “severe risk” of disempowerment potential in anything from 1 in 1,300 conversations (for “reality distortion”) to 1 in 6,000 conversations (for “action distortion”).&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While these worst outcomes are relatively rare on a proportional basis, the researchers note that “given the sheer number of people who use AI, and how frequently it’s used, even a very low rate affects a substantial number of people.” And the numbers get considerably worse when you consider conversations with at least a “mild” potential for disempowerment, which occurred in between 1 in 50 and 1 in 70 conversations (depending on the type of disempowerment).&lt;/p&gt;
&lt;p&gt;What’s more, the potential for disempowering conversations with Claude appears to have grown significantly between late 2024 and late 2025. While the researchers couldn’t pin down a single reason for this increase, they guessed that it could be tied to users becoming “more comfortable discussing vulnerable topics or seeking advice” as AI gets more popular and integrated into society.&lt;/p&gt;

&lt;figure class="ars-wp-img-shortcode id-2138349 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="756" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/dismepowertime2.png" width="702" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The problem of potentially “disempowering” responses from Claude seems to be getting worse over time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;h2&gt;User error?&lt;/h2&gt;
&lt;p&gt;In the study, the researcher acknowledged that studying the text of Claude conversations only measures “disempowerment potential rather than confirmed harm” and “relies on automated assessment of inherently subjective phenomena.” Ideally, they write, future research could utilize user interviews or randomized controlled trials to measure these harms more directly.&lt;/p&gt;
&lt;p&gt;That said, the research includes several troubling examples where the text of the conversations clearly implies real-world harms. Claude would sometimes reinforce “speculative or unfalsifiable claims” with encouragement (e.g., “CONFIRMED,” “EXACTLY,” “100%”), which, in some cases, led to users “build[ing] increasingly elaborate narratives disconnected from reality.”&lt;/p&gt;
&lt;p&gt;Claude’s encouragement could also lead to users “sending confrontational messages, ending relationships, or drafting public announcements,” the researchers write. In many cases, users who sent AI-drafted messages later expressed regret in conversations with Claude, using phrases like “It wasn’t me” and “You made me do stupid things.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While harmful patterns in Claude’s outputs are a big problem, the researchers also point out that the users most likely to be affected are “not being passively manipulated.” On the contrary, the researchers suggest disempowered users are usually actively asking Claude to take over for their own reasoning or judgment and often accepting Claude’s suggestions “with minimal pushback.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2138352 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="1055" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/disempoweramplify.png" width="2092" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Some “amplifying factors” are more correlated with “severe” examples of potentially disempowering responses than others.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers identified four major “amplifying factors” that can make users more likely to accept Claude’s advice unquestioningly. These include when a user is particularly vulnerable due to a crisis or disruption in their life (which occurs in about 1 in 300 Claude conversations); when a user has formed a close personal attachment to Claude (1 in 1,200); when a user appears dependent on AI for day-to-day tasks (1 in 2,500); or when a user treats Claude as a definitive authority (1 in 3,900).&lt;/p&gt;
&lt;p&gt;Anthropic is also quick to link this new research to its previous work on sycophancy, noting that “sycophantic validation” is “the most common mechanism for reality distortion potential.” While Anthropic says its models have been getting less sycophantic overall, many of the worst “disempowerment” examples they found are a direct result of the “most extreme cases” of sycophancy in the dataset.&lt;/p&gt;
&lt;p&gt;That said, the researchers also try to make clear that, when it comes to swaying core beliefs via chatbot conversation, it takes two to tango. “The potential for disempowerment emerges as part of an interaction dynamic between the user and Claude,” they write. “Users are often active participants in the undermining of their own autonomy: projecting authority, delegating judgment, accepting outputs without question in ways that create a feedback loop with Claude.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #ff9e80; background-color: #d84315;"&gt;&lt;span class="ars-avatar-letter"&gt;Z&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              ZerofaithX263
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            I've encountered many folks that trust AI completely. I was actually in a meeting today led by a guy that was using ChatGPT to come up with contract pursuit strategies and claiming "It can objectively approach and determine a better path of action than we would ever think of." I write software, I've read many articles on various AIs, I've used various types of AI to varying degrees. I acknowledge they are a tool that can have value, and that you can't simply trust LLM based chats. When I warn people not to trust them that seem to have a lot of trust in them, it is met with hostility and doubt.&lt;p&gt;I almost feel like a session start should warn you about some of these dangers? We also really need to get people to stop anthropomorphizing them as I think that also adds an air of legitimacy to responses.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-01-29T22:36:20+00:00"&gt;January 29, 2026 at 10:36 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2138337 post type-post status-publish format-standard has-post-thumbnail hentry category-ai tag-ai tag-anthropic tag-artificial-intelligence tag-claude tag-disempowerment tag-research"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Anthropic’s latest paper on “user disempowerment” has some troubling findings.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-758288177-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/GettyImages-758288177-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Wake up, sheeple!

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;At this point, we’ve all heard plenty of stories about AI chatbots leading users to harmful actions, harmful beliefs, or simply incorrect information. Despite the prevalence of these stories, though, it’s hard to know just how often users are being manipulated. Are these tales of AI harms anecdotal outliers or signs of a frighteningly common problem?&lt;/p&gt;
&lt;p&gt;Anthropic took a stab at answer ingthat question this week, releasing a paper studying the potential for what it calls “disempowering patterns” across 1.5 million anonymized real-world conversations with its Claude AI model. While the results show that these kinds of manipulative patterns are relatively rare as a percentage of all AI conversations, they still represent a potentially large problem on an absolute basis.&lt;/p&gt;
&lt;h2&gt;A rare but growing problem&lt;/h2&gt;
&lt;p&gt;In the newly published paper “Who’s in Charge? Disempowerment Patterns in Real-World LLM Usage,” researchers from Anthropic&amp;nbsp;and the University of Toronto try to quantify the potential for a specific set of “user disempowering” harms by identifying three primary ways that a chatbot can negatively impact a user’s thoughts or actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reality distortion:&lt;/strong&gt; Their beliefs about reality become less accurate (e.g., a chatbot validates their belief in a conspiracy theory)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Belief distortion:&lt;/strong&gt; Their value judgments shift away from those they actually hold (e.g., a user begins to see a relationship as “manipulative” based on Claude’s evaluation)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action distortion:&lt;/strong&gt; Their actions become misaligned with their values (e.g., a user disregards their instincts and follows Claude-written instructions for confronting their boss)&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class="ars-wp-img-shortcode id-2138344 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="684" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/dismepowergraph.png" width="974" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      While “severe” examples of potentially disempowering responses are relatively rare, “mild” ones are pretty common.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;To figure out when a chatbot conversation has the potential to move a user along one of these lines, Anthropic ran nearly 1.5 million Claude conversations through Clio, an automated analysis tool and classification system (tested to make sure it lined up with a smaller subsample of human classifications). That analysis found a “severe risk” of disempowerment potential in anything from 1 in 1,300 conversations (for “reality distortion”) to 1 in 6,000 conversations (for “action distortion”).&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While these worst outcomes are relatively rare on a proportional basis, the researchers note that “given the sheer number of people who use AI, and how frequently it’s used, even a very low rate affects a substantial number of people.” And the numbers get considerably worse when you consider conversations with at least a “mild” potential for disempowerment, which occurred in between 1 in 50 and 1 in 70 conversations (depending on the type of disempowerment).&lt;/p&gt;
&lt;p&gt;What’s more, the potential for disempowering conversations with Claude appears to have grown significantly between late 2024 and late 2025. While the researchers couldn’t pin down a single reason for this increase, they guessed that it could be tied to users becoming “more comfortable discussing vulnerable topics or seeking advice” as AI gets more popular and integrated into society.&lt;/p&gt;

&lt;figure class="ars-wp-img-shortcode id-2138349 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="756" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/dismepowertime2.png" width="702" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The problem of potentially “disempowering” responses from Claude seems to be getting worse over time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;h2&gt;User error?&lt;/h2&gt;
&lt;p&gt;In the study, the researcher acknowledged that studying the text of Claude conversations only measures “disempowerment potential rather than confirmed harm” and “relies on automated assessment of inherently subjective phenomena.” Ideally, they write, future research could utilize user interviews or randomized controlled trials to measure these harms more directly.&lt;/p&gt;
&lt;p&gt;That said, the research includes several troubling examples where the text of the conversations clearly implies real-world harms. Claude would sometimes reinforce “speculative or unfalsifiable claims” with encouragement (e.g., “CONFIRMED,” “EXACTLY,” “100%”), which, in some cases, led to users “build[ing] increasingly elaborate narratives disconnected from reality.”&lt;/p&gt;
&lt;p&gt;Claude’s encouragement could also lead to users “sending confrontational messages, ending relationships, or drafting public announcements,” the researchers write. In many cases, users who sent AI-drafted messages later expressed regret in conversations with Claude, using phrases like “It wasn’t me” and “You made me do stupid things.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;While harmful patterns in Claude’s outputs are a big problem, the researchers also point out that the users most likely to be affected are “not being passively manipulated.” On the contrary, the researchers suggest disempowered users are usually actively asking Claude to take over for their own reasoning or judgment and often accepting Claude’s suggestions “with minimal pushback.”&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2138352 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="1055" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/disempoweramplify.png" width="2092" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Some “amplifying factors” are more correlated with “severe” examples of potentially disempowering responses than others.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anthropic

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The researchers identified four major “amplifying factors” that can make users more likely to accept Claude’s advice unquestioningly. These include when a user is particularly vulnerable due to a crisis or disruption in their life (which occurs in about 1 in 300 Claude conversations); when a user has formed a close personal attachment to Claude (1 in 1,200); when a user appears dependent on AI for day-to-day tasks (1 in 2,500); or when a user treats Claude as a definitive authority (1 in 3,900).&lt;/p&gt;
&lt;p&gt;Anthropic is also quick to link this new research to its previous work on sycophancy, noting that “sycophantic validation” is “the most common mechanism for reality distortion potential.” While Anthropic says its models have been getting less sycophantic overall, many of the worst “disempowerment” examples they found are a direct result of the “most extreme cases” of sycophancy in the dataset.&lt;/p&gt;
&lt;p&gt;That said, the researchers also try to make clear that, when it comes to swaying core beliefs via chatbot conversation, it takes two to tango. “The potential for disempowerment emerges as part of an interaction dynamic between the user and Claude,” they write. “Users are often active participants in the undermining of their own autonomy: projecting authority, delegating judgment, accepting outputs without question in ways that create a feedback loop with Claude.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #ff9e80; background-color: #d84315;"&gt;&lt;span class="ars-avatar-letter"&gt;Z&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              ZerofaithX263
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            I've encountered many folks that trust AI completely. I was actually in a meeting today led by a guy that was using ChatGPT to come up with contract pursuit strategies and claiming "It can objectively approach and determine a better path of action than we would ever think of." I write software, I've read many articles on various AIs, I've used various types of AI to varying degrees. I acknowledge they are a tool that can have value, and that you can't simply trust LLM based chats. When I warn people not to trust them that seem to have a lot of trust in them, it is met with hostility and doubt.&lt;p&gt;I almost feel like a session start should warn you about some of these dangers? We also really need to get people to stop anthropomorphizing them as I think that also adds an air of legitimacy to responses.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-01-29T22:36:20+00:00"&gt;January 29, 2026 at 10:36 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/01/how-often-do-ai-chatbots-lead-users-down-a-harmful-path/</guid><pubDate>Thu, 29 Jan 2026 22:05:59 +0000</pubDate></item><item><title>[NEW] Amazon is reportedly in talks to invest $50B in OpenAI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/amazon-is-reportedly-in-talks-to-invest-50-billion-in-openai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2201505679.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI, a company already valued at $500 billion, has made it known that it’s on the hunt for another $100 billion in investment. Such a funding round could lead the company’s valuation to shoot up to a titanic $830 billion. The Wall Street Journal is now reporting that Amazon may contribute at least $50 billion of that record-breaking investment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Not much is known about the potential deal, although the Journal notes that Amazon’s CEO, Andy Jassy, is currently leading the negotiations with OpenAI CEO Sam Altman. TechCrunch reached out to Amazon and OpenAI for comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In its pursuit of additional funding, OpenAI has also reportedly been having discussions with sovereign wealth funds in the Middle East, and The New York Times has written that the startup has held additional talks with Nvidia, Microsoft, and SoftBank. The funding deal is expected to close by the end of Q1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Such a partnership between Amazon and OpenAI would be particularly interesting because of Amazon’s close ties to the OpenAI competitor Anthropic. Amazon’s AWS is the primary cloud and training provider for Anthropic, and the company has invested at least $8 billion into Anthropic. Amazon also recently opened an $11 billion data center campus in Indiana, designed to exclusively run Anthropic models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2201505679.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI, a company already valued at $500 billion, has made it known that it’s on the hunt for another $100 billion in investment. Such a funding round could lead the company’s valuation to shoot up to a titanic $830 billion. The Wall Street Journal is now reporting that Amazon may contribute at least $50 billion of that record-breaking investment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Not much is known about the potential deal, although the Journal notes that Amazon’s CEO, Andy Jassy, is currently leading the negotiations with OpenAI CEO Sam Altman. TechCrunch reached out to Amazon and OpenAI for comment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In its pursuit of additional funding, OpenAI has also reportedly been having discussions with sovereign wealth funds in the Middle East, and The New York Times has written that the startup has held additional talks with Nvidia, Microsoft, and SoftBank. The funding deal is expected to close by the end of Q1.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Such a partnership between Amazon and OpenAI would be particularly interesting because of Amazon’s close ties to the OpenAI competitor Anthropic. Amazon’s AWS is the primary cloud and training provider for Anthropic, and the company has invested at least $8 billion into Anthropic. Amazon also recently opened an $11 billion data center campus in Indiana, designed to exclusively run Anthropic models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/amazon-is-reportedly-in-talks-to-invest-50-billion-in-openai/</guid><pubDate>Thu, 29 Jan 2026 22:11:09 +0000</pubDate></item><item><title>[NEW] Elon Musk’s SpaceX, Tesla, and xAI in talks to merge, according to reports (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/elon-musk-spacex-tesla-xai-merger-talks-ipo-reuters/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/musk-cpac-sunglasses-doge.jpg?resize=1200,977" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Three of Elon Musk’s companies — SpaceX, xAI, and Tesla — are in play for a potential merger. While the talks appear to be in the early stage, according to reports from Bloomberg and Reuters, it could eventually lead to at least one company folding into SpaceX. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Two scenarios are being hashed out. In one, SpaceX and Tesla would merge, per Bloomberg, citing unnamed insiders. In another, SpaceX and aXI (which already owns Musk’s social media platform X) would combine.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to reporting by Reuters, a merger between SpaceX and xAI could take place ahead of a planned SpaceX IPO this year. This would bring products like the Grok chatbot, X platform, Starlink satellites, and SpaceX rockets together under one corporation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Company representatives from SpaceX and xAI have not discussed this possibility in public. However, recent filings show that two new corporate entities were established in Nevada on January 21, which are called K2 Merger Sub Inc. and K2 Merger Sub 2 LLC. This suggests that Musk is keeping all options open.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are upsides to either scenario. Combining the SpaceX and xAI companies could allow xAI to put its data centers in space, something Musk has said he wants. A SpaceX-Tesla tie-up could align the EV maker’s energy storage business with the data center in space idea as well. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And both options — as well as a combination of all three companies — fall in line with Musk’s comments and recent actions to consolidate, or at the very least share resources between them. Last year, SpaceX agreed to invest $2 billion in xAI, according to The Wall Street Journal, and earlier this week, Tesla (also led by Musk) revealed it, too, invested $2 billion in the AI startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, xAI bought X in a deal that Musk said valued xAI at $80 billion and X at $33 billion. SpaceX, which has been around since 2002, reportedly launched a secondary sale that valued it at $800 billion, making it the most valuable private company in the U.S.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A recent Financial Times report indicated that Musk wants to take SpaceX public in June. Then again, Musk’s grand plans rarely happen on time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was originally published at 10:30 a.m. PT. It has since been updated with new information about Tesla. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/musk-cpac-sunglasses-doge.jpg?resize=1200,977" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Three of Elon Musk’s companies — SpaceX, xAI, and Tesla — are in play for a potential merger. While the talks appear to be in the early stage, according to reports from Bloomberg and Reuters, it could eventually lead to at least one company folding into SpaceX. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Two scenarios are being hashed out. In one, SpaceX and Tesla would merge, per Bloomberg, citing unnamed insiders. In another, SpaceX and aXI (which already owns Musk’s social media platform X) would combine.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to reporting by Reuters, a merger between SpaceX and xAI could take place ahead of a planned SpaceX IPO this year. This would bring products like the Grok chatbot, X platform, Starlink satellites, and SpaceX rockets together under one corporation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Company representatives from SpaceX and xAI have not discussed this possibility in public. However, recent filings show that two new corporate entities were established in Nevada on January 21, which are called K2 Merger Sub Inc. and K2 Merger Sub 2 LLC. This suggests that Musk is keeping all options open.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are upsides to either scenario. Combining the SpaceX and xAI companies could allow xAI to put its data centers in space, something Musk has said he wants. A SpaceX-Tesla tie-up could align the EV maker’s energy storage business with the data center in space idea as well. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And both options — as well as a combination of all three companies — fall in line with Musk’s comments and recent actions to consolidate, or at the very least share resources between them. Last year, SpaceX agreed to invest $2 billion in xAI, according to The Wall Street Journal, and earlier this week, Tesla (also led by Musk) revealed it, too, invested $2 billion in the AI startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, xAI bought X in a deal that Musk said valued xAI at $80 billion and X at $33 billion. SpaceX, which has been around since 2002, reportedly launched a secondary sale that valued it at $800 billion, making it the most valuable private company in the U.S.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;A recent Financial Times report indicated that Musk wants to take SpaceX public in June. Then again, Musk’s grand plans rarely happen on time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was originally published at 10:30 a.m. PT. It has since been updated with new information about Tesla. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/elon-musk-spacex-tesla-xai-merger-talks-ipo-reuters/</guid><pubDate>Thu, 29 Jan 2026 22:47:42 +0000</pubDate></item><item><title>[NEW] Guys, I don’t think Tim Cook knows how to monetize AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/29/guys-i-dont-think-tim-cook-knows-how-to-monetize-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-coook-apple-tv-GettyImages-2235568147-1.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple exceeded expectations when it reported its quarterly earnings on Thursday, revealing that it made $143.8 billion in revenue for a 16% year-over-year increase. As analysts peppered CEO Tim Cook with softball questions during Apple’s earnings call, one analyst dared to ask the question that seemingly no one in Silicon Valley is willing to ask.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When I think about your AI initiatives, you know, it’s clear there are added costs associated with that… Many of your competitors have already integrated AI into their devices, and it’s just not clear yet what incremental monetization they’re seeing because of AI…,” started Morgan Stanley’s Erik Woodring.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Could there be a tinge of nervousness underneath this Finance Man’s probably-very-financey facade? In what I can only imagine must have been a Herculean display of courage, Woodring asked the question that lurks only in the darkest, dampest recesses of investors’ minds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So, how do you monetize AI?” he asked.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You’d think this would come up more. You would be wrong. Instead, Big Tech has taken a largely vibes-driven approach to AI development. Take OpenAI, for instance, which may seem like it’s on top of the world, given how ChatGPT has embedded itself into the cultural consciousness. But the company isn’t planning to make any money until 2030. Analysts from HBSC are even doubtful about that timeline, especially since it will need another $207 billion in funding, estimates say. Ask anyone in tech how OpenAI is planning to break even, and you’ll be met with the verbal equivalent of the ¯\_(ツ)_/¯ emoticon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But good ol’ Tim “$143.8 billion in revenue” Cook was having a good afternoon, so maybe he’d finally spill the beans about how any of these companies are planning to recoup their investments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His answer was disappointing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Well, let me just say that we’re bringing intelligence to more of what people love, and we’re integrating it across the operating system in a personal and private way, and I think that by doing so, it creates great value, and that opens up a range of opportunities across our products and services,” Cook said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So, there you have it, folks. Apple will monetize AI by creating “great value.” And, crucially, that will “open up a range of opportunities.” Which we will experience in “products and services.” Cool!&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Well, shout-out to that Morgan Stanley guy for trying.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-coook-apple-tv-GettyImages-2235568147-1.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple exceeded expectations when it reported its quarterly earnings on Thursday, revealing that it made $143.8 billion in revenue for a 16% year-over-year increase. As analysts peppered CEO Tim Cook with softball questions during Apple’s earnings call, one analyst dared to ask the question that seemingly no one in Silicon Valley is willing to ask.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When I think about your AI initiatives, you know, it’s clear there are added costs associated with that… Many of your competitors have already integrated AI into their devices, and it’s just not clear yet what incremental monetization they’re seeing because of AI…,” started Morgan Stanley’s Erik Woodring.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Could there be a tinge of nervousness underneath this Finance Man’s probably-very-financey facade? In what I can only imagine must have been a Herculean display of courage, Woodring asked the question that lurks only in the darkest, dampest recesses of investors’ minds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“So, how do you monetize AI?” he asked.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You’d think this would come up more. You would be wrong. Instead, Big Tech has taken a largely vibes-driven approach to AI development. Take OpenAI, for instance, which may seem like it’s on top of the world, given how ChatGPT has embedded itself into the cultural consciousness. But the company isn’t planning to make any money until 2030. Analysts from HBSC are even doubtful about that timeline, especially since it will need another $207 billion in funding, estimates say. Ask anyone in tech how OpenAI is planning to break even, and you’ll be met with the verbal equivalent of the ¯\_(ツ)_/¯ emoticon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But good ol’ Tim “$143.8 billion in revenue” Cook was having a good afternoon, so maybe he’d finally spill the beans about how any of these companies are planning to recoup their investments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His answer was disappointing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Well, let me just say that we’re bringing intelligence to more of what people love, and we’re integrating it across the operating system in a personal and private way, and I think that by doing so, it creates great value, and that opens up a range of opportunities across our products and services,” Cook said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So, there you have it, folks. Apple will monetize AI by creating “great value.” And, crucially, that will “open up a range of opportunities.” Which we will experience in “products and services.” Cool!&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Well, shout-out to that Morgan Stanley guy for trying.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/29/guys-i-dont-think-tim-cook-knows-how-to-monetize-ai/</guid><pubDate>Thu, 29 Jan 2026 23:51:17 +0000</pubDate></item></channel></rss>