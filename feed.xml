<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 25 Sep 2025 06:32:13 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>AfriMed-QA: Benchmarking large language models for global health (The latest research from Google)</title><link>https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., USMLE MedQA), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level.&lt;/p&gt;&lt;p&gt;Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets.&lt;/p&gt;&lt;p&gt;To address this gap, we present AfriMed-QA, a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including Intron health, Sisonkebiotik, University of Cape Coast, the Federation of African Medical Students Association, and BioRAMP, which collectively form the AfriMed-QA consortium, and with support from PATH/The Gates Foundation. We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., USMLE MedQA), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level.&lt;/p&gt;&lt;p&gt;Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets.&lt;/p&gt;&lt;p&gt;To address this gap, we present AfriMed-QA, a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including Intron health, Sisonkebiotik, University of Cape Coast, the Federation of African Medical Students Association, and BioRAMP, which collectively form the AfriMed-QA consortium, and with support from PATH/The Gates Foundation. We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/</guid><pubDate>Wed, 24 Sep 2025 19:11:00 +0000</pubDate></item><item><title>Neon, the No. 2 social app on the Apple App Store, pays users to record their phone calls and sells data to AI firms (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/24/neon-the-no-2-social-app-on-the-apple-app-store-pays-users-to-record-their-phone-calls-and-sells-data-to-ai-firms/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new app offering to record your phone calls and pay you for the audio so it can sell the data to AI companies is, unbelievably, the No. 2 app in Apple’s U.S. App Store’s Social Networking section. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app, Neon Mobile, pitches itself as a moneymaking tool offering “hundreds or even thousands of dollars per year” for access to your audio conversations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Neon’s website says the company pays 30¢ per minute when you call other Neon users and up to $30 per day maximum for making calls to anyone else. The app also pays for referrals. The app first ranked No. 476 in the Social Networking category of the U.S. App Store on September 18 but jumped to No. 10 at the end of yesterday, according to data from app intelligence firm Appfigures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, Neon was spotted in the No. 2 position on the iPhone’s top free charts for social apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon also became the No. 7 top overall app or game earlier on Wednesday morning and became the No. 6 top app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Neon’s terms of service, the company’s mobile app can capture users’ inbound and outbound phone calls. However, Neon’s marketing claims to only record your side of the call unless it’s with another Neon user.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That data is being sold to “AI companies,” Neon’s terms of service state, “for the purpose of developing, training, testing, and improving machine learning models, artificial intelligence tools and systems, and related technologies.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="a screenshot showing Neon Mobile's website" class="wp-image-3049880" height="1756" src="https://techcrunch.com/wp-content/uploads/2025/09/neon-mobile-2025-09-24-at-3.11.50PM.jpg" width="3182" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Neon Mobile&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The fact that such an app exists and is permitted on the app stores is an indication of how far AI has encroached into users’ lives and areas once thought of as private. Its high ranking within the Apple App Store, meanwhile, is proof that there is now some subsection of the market seemingly willing to exchange their privacy for pennies, regardless of the larger cost to themselves or society.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite what Neon’s privacy policy says, its terms include a very broad license to its user data, where Neon grants itself a:&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;…worldwide, exclusive, irrevocable, transferable, royalty-free, fully paid right and license (with the right to sublicense through multiple tiers) to sell, use, host, store, transfer, publicly display, publicly perform (including by means of a digital audio transmission), communicate to the public, reproduce, modify for the purpose of formatting for display, create derivative works as authorized in these Terms, and distribute your Recordings, in whole or in part, in any media formats and through any media channels, in each instance whether now known or hereafter developed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;That leaves plenty of wiggle room for Neon to do more with users’ data than it claims. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The terms also include an extensive section on beta features, which have no warranty and may have all sorts of issues and bugs.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="a screenshot from Neon's privacy policy, which reads:

&amp;quot;Recordings Generally. Certain features of the Service may permit users to send, submit, upload, or otherwise authorize the capture of (&amp;quot;Submit&amp;quot; Recordings and other information to the Service. You retain any copyright and other proprietary rights that you may hold in the Recordings that you Submit to the Service, subject to these Terms including Neon Mobile's rights and licenses granted to Neon Mobile under these Terms. For avoidance of doubt, your rights in Recordings are limited to playback and viewing of your own Recordings through our mobile application, which features we may offer in our sole discretion. 2. License Grant to Neon Mobile. By Submitting Recordings or other information to the Service, you grant Neon Mobile a worldwide, exclusive, irrevocable, transferable royalty-free, fully paid right and license (with the right to sublicense through multiple tiers) to sell, use, host, store, transfer, publicly display, publicly perform (including by means of a digital audio transmission), communicate to the public, reproduce, modify for the purpose of formatting for display, create derivative works as authorized in these Terms, and distribute your Recordings, in whole or in part, in any media formats and through any media channels, in each instance whether now known or hereafter developed.&amp;quot;" class="wp-image-3049882" height="520" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-3.17.31PM.jpg" width="2686" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Neon (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Though Neon’s app raises many red flags, it may be technically legal. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Recording only one side of the phone call is aimed at avoiding wiretap laws,” Jennifer Daniels, a partner with the law firm Blank Rome‘s Privacy, Security &amp;amp; Data Protection Group, tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Under [the] laws of many states, you have to have consent from both parties to a conversation in order to record it&amp;nbsp;… It’s an interesting approach,” says Daniels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Peter Jackson, cybersecurity and privacy attorney at Greenberg Glusker, agreed&amp;nbsp;— and tells TechCrunch that the language around “one-sided transcripts” sounds like it could be a backdoor way of saying that Neon records users’ calls in their entirety but may just remove what the other party said from the final transcript.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, the legal experts pointed to concerns about how anonymized the data may really be. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon claims it removes users’ names, emails, and phone numbers before selling data to AI companies. But the company doesn’t say how AI partners or others it sells to could use that data. Voice data could be used to make fake calls that sound like they’re coming from you, or AI companies could use your voice to make their own AI voices.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once your voice is over there, it can be used for fraud,” says Jackson. “Now this company has your phone number and essentially enough information — they have recordings of your voice, which could be used to create an impersonation of you and do all sorts of fraud.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if the company itself is trustworthy, Neon doesn’t disclose who its trusted partners are or what those entities are allowed to do with users’ data further down the road. Neon is also subject to potential data breaches, as any company with valuable data may be.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Neon mobile website screenshot showing founder &amp;quot;Alex&amp;quot;" class="wp-image-3049890" height="1140" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-3.26.32PM.jpg" width="3006" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Neon Mobile&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In a brief test by TechCrunch, Neon did not offer any indication that it was recording the user’s call, nor did it warn the call recipient. The app worked like any other voice-over-IP app, and the caller ID displayed the inbound phone number, as usual. (We’ll leave it to security researchers to attempt to verify the app’s other claims.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon founder Alex Kiam didn’t return a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kiam, who is identified only as “Alex” on the company website, operates Neon from a New York apartment, a business filing shows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A LinkedIn post indicates Kiam raised money from Upfront Ventures a few months ago for his startup, but the investor didn’t respond to an inquiry from TechCrunch as of the time of writing.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-has-ai-desensitized-users-to-privacy-concerns"&gt;Has AI desensitized users to privacy concerns? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;There was a time when companies looking to profit from data collection through mobile apps handled this type of thing on the sly. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When it was revealed in 2019 that Facebook was paying teens to install an app that spies on them, it was a scandal. The following year, headlines buzzed again when it was discovered that app store analytics providers operated dozens of seemingly innocuous apps to collect usage data about the mobile app ecosystem. There are regular warnings to be wary of VPN apps, which often aren’t as private as they claim. There are even government reports detailing how agencies regularly purchase personal data that’s “commercially available” on the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now AI agents regularly join meetings to take notes, and always-on AI devices are on the market. But at least in those cases, everyone is consenting to a recording, Daniels tells TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In light of this widespread usage and sale of personal data, there are likely now those cynical enough to think that if their data is being sold anyway, they may as well profit from it. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unfortunately, they may be sharing more information than they realize and putting others’ privacy at risk when they do. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There is a tremendous desire on the part of, certainly, knowledge workers — and frankly, everybody — to make it as easy as possible to do your job,” says Jackson. “And some of these productivity tools do that at the expense of, obviously, your privacy, but also, increasingly, the privacy of those with whom you are interacting on a day-to-day basis.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new app offering to record your phone calls and pay you for the audio so it can sell the data to AI companies is, unbelievably, the No. 2 app in Apple’s U.S. App Store’s Social Networking section. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app, Neon Mobile, pitches itself as a moneymaking tool offering “hundreds or even thousands of dollars per year” for access to your audio conversations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Neon’s website says the company pays 30¢ per minute when you call other Neon users and up to $30 per day maximum for making calls to anyone else. The app also pays for referrals. The app first ranked No. 476 in the Social Networking category of the U.S. App Store on September 18 but jumped to No. 10 at the end of yesterday, according to data from app intelligence firm Appfigures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, Neon was spotted in the No. 2 position on the iPhone’s top free charts for social apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon also became the No. 7 top overall app or game earlier on Wednesday morning and became the No. 6 top app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Neon’s terms of service, the company’s mobile app can capture users’ inbound and outbound phone calls. However, Neon’s marketing claims to only record your side of the call unless it’s with another Neon user.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That data is being sold to “AI companies,” Neon’s terms of service state, “for the purpose of developing, training, testing, and improving machine learning models, artificial intelligence tools and systems, and related technologies.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="a screenshot showing Neon Mobile's website" class="wp-image-3049880" height="1756" src="https://techcrunch.com/wp-content/uploads/2025/09/neon-mobile-2025-09-24-at-3.11.50PM.jpg" width="3182" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Neon Mobile&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The fact that such an app exists and is permitted on the app stores is an indication of how far AI has encroached into users’ lives and areas once thought of as private. Its high ranking within the Apple App Store, meanwhile, is proof that there is now some subsection of the market seemingly willing to exchange their privacy for pennies, regardless of the larger cost to themselves or society.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite what Neon’s privacy policy says, its terms include a very broad license to its user data, where Neon grants itself a:&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;…worldwide, exclusive, irrevocable, transferable, royalty-free, fully paid right and license (with the right to sublicense through multiple tiers) to sell, use, host, store, transfer, publicly display, publicly perform (including by means of a digital audio transmission), communicate to the public, reproduce, modify for the purpose of formatting for display, create derivative works as authorized in these Terms, and distribute your Recordings, in whole or in part, in any media formats and through any media channels, in each instance whether now known or hereafter developed.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;That leaves plenty of wiggle room for Neon to do more with users’ data than it claims. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The terms also include an extensive section on beta features, which have no warranty and may have all sorts of issues and bugs.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="a screenshot from Neon's privacy policy, which reads:

&amp;quot;Recordings Generally. Certain features of the Service may permit users to send, submit, upload, or otherwise authorize the capture of (&amp;quot;Submit&amp;quot; Recordings and other information to the Service. You retain any copyright and other proprietary rights that you may hold in the Recordings that you Submit to the Service, subject to these Terms including Neon Mobile's rights and licenses granted to Neon Mobile under these Terms. For avoidance of doubt, your rights in Recordings are limited to playback and viewing of your own Recordings through our mobile application, which features we may offer in our sole discretion. 2. License Grant to Neon Mobile. By Submitting Recordings or other information to the Service, you grant Neon Mobile a worldwide, exclusive, irrevocable, transferable royalty-free, fully paid right and license (with the right to sublicense through multiple tiers) to sell, use, host, store, transfer, publicly display, publicly perform (including by means of a digital audio transmission), communicate to the public, reproduce, modify for the purpose of formatting for display, create derivative works as authorized in these Terms, and distribute your Recordings, in whole or in part, in any media formats and through any media channels, in each instance whether now known or hereafter developed.&amp;quot;" class="wp-image-3049882" height="520" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-3.17.31PM.jpg" width="2686" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Neon (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Though Neon’s app raises many red flags, it may be technically legal. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Recording only one side of the phone call is aimed at avoiding wiretap laws,” Jennifer Daniels, a partner with the law firm Blank Rome‘s Privacy, Security &amp;amp; Data Protection Group, tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Under [the] laws of many states, you have to have consent from both parties to a conversation in order to record it&amp;nbsp;… It’s an interesting approach,” says Daniels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Peter Jackson, cybersecurity and privacy attorney at Greenberg Glusker, agreed&amp;nbsp;— and tells TechCrunch that the language around “one-sided transcripts” sounds like it could be a backdoor way of saying that Neon records users’ calls in their entirety but may just remove what the other party said from the final transcript.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, the legal experts pointed to concerns about how anonymized the data may really be. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon claims it removes users’ names, emails, and phone numbers before selling data to AI companies. But the company doesn’t say how AI partners or others it sells to could use that data. Voice data could be used to make fake calls that sound like they’re coming from you, or AI companies could use your voice to make their own AI voices.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once your voice is over there, it can be used for fraud,” says Jackson. “Now this company has your phone number and essentially enough information — they have recordings of your voice, which could be used to create an impersonation of you and do all sorts of fraud.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if the company itself is trustworthy, Neon doesn’t disclose who its trusted partners are or what those entities are allowed to do with users’ data further down the road. Neon is also subject to potential data breaches, as any company with valuable data may be.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Neon mobile website screenshot showing founder &amp;quot;Alex&amp;quot;" class="wp-image-3049890" height="1140" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-24-at-3.26.32PM.jpg" width="3006" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Neon Mobile&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In a brief test by TechCrunch, Neon did not offer any indication that it was recording the user’s call, nor did it warn the call recipient. The app worked like any other voice-over-IP app, and the caller ID displayed the inbound phone number, as usual. (We’ll leave it to security researchers to attempt to verify the app’s other claims.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neon founder Alex Kiam didn’t return a request for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kiam, who is identified only as “Alex” on the company website, operates Neon from a New York apartment, a business filing shows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A LinkedIn post indicates Kiam raised money from Upfront Ventures a few months ago for his startup, but the investor didn’t respond to an inquiry from TechCrunch as of the time of writing.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-has-ai-desensitized-users-to-privacy-concerns"&gt;Has AI desensitized users to privacy concerns? &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;There was a time when companies looking to profit from data collection through mobile apps handled this type of thing on the sly. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When it was revealed in 2019 that Facebook was paying teens to install an app that spies on them, it was a scandal. The following year, headlines buzzed again when it was discovered that app store analytics providers operated dozens of seemingly innocuous apps to collect usage data about the mobile app ecosystem. There are regular warnings to be wary of VPN apps, which often aren’t as private as they claim. There are even government reports detailing how agencies regularly purchase personal data that’s “commercially available” on the market.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now AI agents regularly join meetings to take notes, and always-on AI devices are on the market. But at least in those cases, everyone is consenting to a recording, Daniels tells TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In light of this widespread usage and sale of personal data, there are likely now those cynical enough to think that if their data is being sold anyway, they may as well profit from it. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unfortunately, they may be sharing more information than they realize and putting others’ privacy at risk when they do. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There is a tremendous desire on the part of, certainly, knowledge workers — and frankly, everybody — to make it as easy as possible to do your job,” says Jackson. “And some of these productivity tools do that at the expense of, obviously, your privacy, but also, increasingly, the privacy of those with whom you are interacting on a day-to-day basis.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/24/neon-the-no-2-social-app-on-the-apple-app-store-pays-users-to-record-their-phone-calls-and-sells-data-to-ai-firms/</guid><pubDate>Wed, 24 Sep 2025 19:50:58 +0000</pubDate></item><item><title>Roundtables: The Future of Birth Control (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/09/24/1124058/roundtables-the-future-of-birth-control/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MITTR-Roundtables-2-Zoom-Opening-Overlay-1.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;
&lt;p&gt;&lt;em&gt;Available only for MIT Alumni and subscribers.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 core-embed_1"&gt;
&lt;figure class="wp-block-embed is-type-video is-provider-vimeo wp-block-embed-vimeo wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
[embedded content]

&lt;/div&gt;&lt;/figure&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Recorded on September 24, 2025&lt;br /&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Related Coverage: &lt;/strong&gt;&lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/09/MITTR-Roundtables-2-Zoom-Opening-Overlay-1.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;&lt;div class="contentBody__summaryBullets--81327c9379272772d1e74a64b6d4868a"&gt;
&lt;p&gt;&lt;em&gt;Available only for MIT Alumni and subscribers.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 core-embed_1"&gt;
&lt;figure class="wp-block-embed is-type-video is-provider-vimeo wp-block-embed-vimeo wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
[embedded content]

&lt;/div&gt;&lt;/figure&gt;
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Recorded on September 24, 2025&lt;br /&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Related Coverage: &lt;/strong&gt;&lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/09/24/1124058/roundtables-the-future-of-birth-control/</guid><pubDate>Wed, 24 Sep 2025 20:09:56 +0000</pubDate></item><item><title>Open Secret: How NVIDIA Nemotron Models, Datasets and Techniques Fuel AI Development (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/nemotron-open-source-ai/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Open technologies — made available to developers and businesses to adopt, modify and innovate with — have been part of every major technology shift, from the birth of the internet to the early days of cloud computing. AI should follow the same path.&lt;/p&gt;
&lt;p&gt;That’s why the NVIDIA Nemotron family of multimodal AI models, datasets and techniques is openly available. Accessible for research and commercial use, from local PCs to enterprise-scale systems, Nemotron provides an open foundation for building AI applications. It’s available for developers to get started on GitHub, Hugging Face and OpenRouter.&lt;/p&gt;
&lt;p&gt;Nemotron enables developers, startups and enterprises of any size to use models trained with transparent, open-source training data. It offers tools to accelerate every phase of development, from customization to deployment.&lt;/p&gt;
&lt;p&gt;The technology’s transparency means that its adopters can understand how their models work and trust the results they provide.&lt;/p&gt;
&lt;p&gt;Nemotron’s capabilities for generalized intelligence and agentic AI reasoning — and its adaptability to specialized AI use cases — have led to its widespread use today by AI innovators and leaders across industries such as manufacturing, healthcare, education and retail.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What’s NVIDIA Nemotron?&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA Nemotron is a collection of open-source AI technologies designed for efficient AI development at every stage. It includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Multimodal models: &lt;/b&gt;State-of-the-art AI models, delivered as open checkpoints, that excel at graduate-level scientific reasoning, advanced math, coding, instruction following, tool calling and visual reasoning.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Pretraining&lt;/b&gt;&lt;b&gt;, post-training and multimodal datasets:&lt;/b&gt; Collections of carefully chosen text, image and video data that teach AI models skills including language, math and problem-solving.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Numerical precision algorithms and recipes: &lt;/b&gt;Advanced precision techniques that make AI faster and cheaper to run while keeping answers accurate.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;System software for scaling training efficiently on GPU clusters: &lt;/b&gt;Optimized software and frameworks that unlock accelerating training and inference on NVIDIA GPUs at massive scale for the largest models.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Post-training methodologies and software: &lt;/b&gt;Fine-tuning steps that make AI smarter, safer and better at specific jobs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nemotron is part of NVIDIA’s wider efforts to provide open, transparent and adaptable AI platforms for developers, industry leaders and AI infrastructure builders across the private and public sectors.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-85292" height="512" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-nemotron-v5.jpg" width="1280" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What’s the Difference Between Generalized Intelligence and Specialized Intelligence?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA built Nemotron to raise the bar for generalized intelligence capabilities — including AI reasoning — while also accelerating specialization, helping businesses worldwide adopt AI for industry-specific challenges.&lt;/p&gt;
&lt;p&gt;Generalized intelligence refers to models trained on vast public datasets to perform a wide range of tasks. It serves as the engine needed for broad problem-solving and reasoning tasks. Specialized intelligence learns the unique language, processes and priorities of an industry or organization, giving AI models the ability to adapt to specific real-world applications.&lt;/p&gt;
&lt;p&gt;To deliver AI at scale across every industry, both are essential.&lt;/p&gt;
&lt;p&gt;That’s why Nemotron provides pretrained foundation models optimized for a range of computing platforms, as well as tools like NVIDIA NeMo and NVIDIA Dynamo to transform generalized AI models into custom models tailored for specialized intelligence.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Are Developers and Enterprises Using Nemotron?&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA is building Nemotron to accelerate the work of developers everywhere — and to inform the design of future AI systems.&lt;/p&gt;
&lt;p&gt;From researchers to startups and global enterprises, developers need flexible, trustworthy AI. Nemotron offers the tools to build, customize and integrate AI for virtually any field.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;CrowdStrike&lt;/b&gt; is integrating its Charlotte AI AgentWorks no-code platform for security teams with Nemotron, helping to power and secure the agentic ecosystem. This collaboration redefines security operations by enabling analysts to build and deploy specialized AI agents at scale, leveraging trusted, enterprise-grade security with Nemotron models.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;DataRobot&lt;/b&gt; is using Nemotron as the open foundation for training, customizing and managing AI agents at scale in the Agent Workforce Platform co-developed with NVIDIA— a solution for building, operating and governing a fully functional AI agent workforce, in on-premises, hybrid and multi-cloud environments.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ServiceNow&lt;/b&gt;&amp;nbsp;introduced the Apriel Nemotron 15B model earlier this year in partnership with NVIDIA. Post-trained with data from both companies, the model is purpose-built for real-time workflow execution and delivers advanced reasoning in a smaller size, making it faster, more efficient, and cost-effective.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;UK-LLM&lt;/b&gt;, a sovereign AI initiative led by University College London, used Nemotron open-source techniques and datasets to develop an AI reasoning model for English and Welsh.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVIDIA also uses the insights gained from developing Nemotron to inform the design of its next-generation systems, including Grace Blackwell, Vera Rubin and Feynman. The latest innovations in AI models, including reduced precision, sparse arithmetic, new attention mechanisms and optimization algorithms, all shape GPU architectures.&lt;/p&gt;
&lt;p&gt;For example, NVFP4, a new data format that uses just four bits per parameter during large language model (LLM) training, was discovered with Nemotron. This advancement — which dramatically reduces energy use — is influencing the design of future NVIDIA systems.&lt;/p&gt;
&lt;p&gt;NVIDIA also improves Nemotron with open technologies built by the broader AI community.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Alibaba&lt;/b&gt;’s Qwen open model has provided data augmentation that has improved Nemotron’s pretraining and post-training datasets. The latest Qwen3-Next architecture pushed the frontier of long-context AI, the model leverages Gated Delta Networks from NVIDIA research and MIT.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;DeepSeek R1&lt;/b&gt;, a pioneer in AI reasoning, led to the development of Nemotron math, code and reasoning open datasets that can be used to teach models how to think.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;OpenAI&lt;/b&gt;’s gpt-oss open-weight models demonstrate incredible reasoning, math and tool calling capabilities, including adjustable reasoning settings, that can be used to strengthen Nemotron post-training datasets.&lt;/li&gt;
&lt;li&gt;The Llama collection of open models by &lt;b&gt;Meta&lt;/b&gt; is the foundation for Llama-Nemotron, an open family of models that used Nemotron datasets and recipes to add advanced reasoning capabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Start training and customizing AI models and agents with NVIDIA Nemotron models and data on Hugging Face, or try models for free on OpenRouter. Developers using NVIDIA RTX PCs can access Nemotron via the llama.cpp framework.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Join NVIDIA for &lt;/i&gt;&lt;i&gt;Agentic AI Day&lt;/i&gt;&lt;i&gt; at NVIDIA GTC Washington, D.C. on Wednesday, Oct. 29. The event will bring together developers, researchers and technology leaders to highlight how NVIDIA technologies are accelerating national AI priorities and powering the next generation of AI agents.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA developer news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the developer community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp; &lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Open technologies — made available to developers and businesses to adopt, modify and innovate with — have been part of every major technology shift, from the birth of the internet to the early days of cloud computing. AI should follow the same path.&lt;/p&gt;
&lt;p&gt;That’s why the NVIDIA Nemotron family of multimodal AI models, datasets and techniques is openly available. Accessible for research and commercial use, from local PCs to enterprise-scale systems, Nemotron provides an open foundation for building AI applications. It’s available for developers to get started on GitHub, Hugging Face and OpenRouter.&lt;/p&gt;
&lt;p&gt;Nemotron enables developers, startups and enterprises of any size to use models trained with transparent, open-source training data. It offers tools to accelerate every phase of development, from customization to deployment.&lt;/p&gt;
&lt;p&gt;The technology’s transparency means that its adopters can understand how their models work and trust the results they provide.&lt;/p&gt;
&lt;p&gt;Nemotron’s capabilities for generalized intelligence and agentic AI reasoning — and its adaptability to specialized AI use cases — have led to its widespread use today by AI innovators and leaders across industries such as manufacturing, healthcare, education and retail.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What’s NVIDIA Nemotron?&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA Nemotron is a collection of open-source AI technologies designed for efficient AI development at every stage. It includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Multimodal models: &lt;/b&gt;State-of-the-art AI models, delivered as open checkpoints, that excel at graduate-level scientific reasoning, advanced math, coding, instruction following, tool calling and visual reasoning.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Pretraining&lt;/b&gt;&lt;b&gt;, post-training and multimodal datasets:&lt;/b&gt; Collections of carefully chosen text, image and video data that teach AI models skills including language, math and problem-solving.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Numerical precision algorithms and recipes: &lt;/b&gt;Advanced precision techniques that make AI faster and cheaper to run while keeping answers accurate.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;System software for scaling training efficiently on GPU clusters: &lt;/b&gt;Optimized software and frameworks that unlock accelerating training and inference on NVIDIA GPUs at massive scale for the largest models.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Post-training methodologies and software: &lt;/b&gt;Fine-tuning steps that make AI smarter, safer and better at specific jobs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nemotron is part of NVIDIA’s wider efforts to provide open, transparent and adaptable AI platforms for developers, industry leaders and AI infrastructure builders across the private and public sectors.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="aligncenter size-full wp-image-85292" height="512" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-nemotron-v5.jpg" width="1280" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What’s the Difference Between Generalized Intelligence and Specialized Intelligence?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA built Nemotron to raise the bar for generalized intelligence capabilities — including AI reasoning — while also accelerating specialization, helping businesses worldwide adopt AI for industry-specific challenges.&lt;/p&gt;
&lt;p&gt;Generalized intelligence refers to models trained on vast public datasets to perform a wide range of tasks. It serves as the engine needed for broad problem-solving and reasoning tasks. Specialized intelligence learns the unique language, processes and priorities of an industry or organization, giving AI models the ability to adapt to specific real-world applications.&lt;/p&gt;
&lt;p&gt;To deliver AI at scale across every industry, both are essential.&lt;/p&gt;
&lt;p&gt;That’s why Nemotron provides pretrained foundation models optimized for a range of computing platforms, as well as tools like NVIDIA NeMo and NVIDIA Dynamo to transform generalized AI models into custom models tailored for specialized intelligence.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How Are Developers and Enterprises Using Nemotron?&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA is building Nemotron to accelerate the work of developers everywhere — and to inform the design of future AI systems.&lt;/p&gt;
&lt;p&gt;From researchers to startups and global enterprises, developers need flexible, trustworthy AI. Nemotron offers the tools to build, customize and integrate AI for virtually any field.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;CrowdStrike&lt;/b&gt; is integrating its Charlotte AI AgentWorks no-code platform for security teams with Nemotron, helping to power and secure the agentic ecosystem. This collaboration redefines security operations by enabling analysts to build and deploy specialized AI agents at scale, leveraging trusted, enterprise-grade security with Nemotron models.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;DataRobot&lt;/b&gt; is using Nemotron as the open foundation for training, customizing and managing AI agents at scale in the Agent Workforce Platform co-developed with NVIDIA— a solution for building, operating and governing a fully functional AI agent workforce, in on-premises, hybrid and multi-cloud environments.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;ServiceNow&lt;/b&gt;&amp;nbsp;introduced the Apriel Nemotron 15B model earlier this year in partnership with NVIDIA. Post-trained with data from both companies, the model is purpose-built for real-time workflow execution and delivers advanced reasoning in a smaller size, making it faster, more efficient, and cost-effective.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;UK-LLM&lt;/b&gt;, a sovereign AI initiative led by University College London, used Nemotron open-source techniques and datasets to develop an AI reasoning model for English and Welsh.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVIDIA also uses the insights gained from developing Nemotron to inform the design of its next-generation systems, including Grace Blackwell, Vera Rubin and Feynman. The latest innovations in AI models, including reduced precision, sparse arithmetic, new attention mechanisms and optimization algorithms, all shape GPU architectures.&lt;/p&gt;
&lt;p&gt;For example, NVFP4, a new data format that uses just four bits per parameter during large language model (LLM) training, was discovered with Nemotron. This advancement — which dramatically reduces energy use — is influencing the design of future NVIDIA systems.&lt;/p&gt;
&lt;p&gt;NVIDIA also improves Nemotron with open technologies built by the broader AI community.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Alibaba&lt;/b&gt;’s Qwen open model has provided data augmentation that has improved Nemotron’s pretraining and post-training datasets. The latest Qwen3-Next architecture pushed the frontier of long-context AI, the model leverages Gated Delta Networks from NVIDIA research and MIT.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;DeepSeek R1&lt;/b&gt;, a pioneer in AI reasoning, led to the development of Nemotron math, code and reasoning open datasets that can be used to teach models how to think.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;OpenAI&lt;/b&gt;’s gpt-oss open-weight models demonstrate incredible reasoning, math and tool calling capabilities, including adjustable reasoning settings, that can be used to strengthen Nemotron post-training datasets.&lt;/li&gt;
&lt;li&gt;The Llama collection of open models by &lt;b&gt;Meta&lt;/b&gt; is the foundation for Llama-Nemotron, an open family of models that used Nemotron datasets and recipes to add advanced reasoning capabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Start training and customizing AI models and agents with NVIDIA Nemotron models and data on Hugging Face, or try models for free on OpenRouter. Developers using NVIDIA RTX PCs can access Nemotron via the llama.cpp framework.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Join NVIDIA for &lt;/i&gt;&lt;i&gt;Agentic AI Day&lt;/i&gt;&lt;i&gt; at NVIDIA GTC Washington, D.C. on Wednesday, Oct. 29. The event will bring together developers, researchers and technology leaders to highlight how NVIDIA technologies are accelerating national AI priorities and powering the next generation of AI agents.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA developer news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the developer community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp; &lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/nemotron-open-source-ai/</guid><pubDate>Wed, 24 Sep 2025 21:45:24 +0000</pubDate></item><item><title>[NEW] New AI system could accelerate clinical research (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-ai-system-could-accelerate-clinical-research-0925</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/MIT-Scalable-Segmentation-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Annotating regions of interest in medical images, a process known as segmentation, is often one of the first steps clinical researchers take when running a new study involving biomedical images.&lt;/p&gt;&lt;p&gt;For instance, to determine how the size of the brain’s hippocampus changes as patients age, the scientist first outlines each hippocampus in a series of brain scans. For many structures and image types, this is often a manual process that can be extremely time-consuming, especially if the regions being studied are challenging to delineate.&lt;/p&gt;&lt;p&gt;To streamline the process, MIT researchers developed an artificial intelligence-based system that enables a researcher to rapidly segment new biomedical imaging datasets by clicking, scribbling, and drawing boxes on the images. This new AI model uses these interactions to predict the segmentation.&lt;/p&gt;&lt;p&gt;As the user marks additional images, the number of interactions they need to perform decreases, eventually dropping to zero. The model can then segment each new image accurately without user input.&lt;/p&gt;&lt;p&gt;It can do this because the model’s architecture has been specially designed to use information from images it has already segmented to make new predictions.&lt;/p&gt;&lt;p&gt;Unlike other medical image segmentation models, this system allows the user to segment an entire dataset without repeating their work for each image.&lt;/p&gt;&lt;p&gt;In addition, the interactive tool does not require a presegmented image dataset for training, so users don’t need machine-learning expertise or extensive computational resources. They can use the system for a new segmentation task without retraining the model.&lt;/p&gt;&lt;p&gt;In the long run, this tool could accelerate studies of new treatment methods and reduce the cost of clinical trials and medical research. It could also be used by physicians to improve the efficiency of clinical applications, such as radiation treatment planning.&lt;/p&gt;&lt;p&gt;“Many scientists might only have time to segment a few images per day for their research because manual image segmentation is so time-consuming. Our hope is that this system will enable new science by allowing clinical researchers to conduct studies they were prohibited from doing before because of the lack of an efficient tool,” says Hallee Wong, an electrical engineering and computer science graduate student and lead author of a paper on this new tool.&lt;/p&gt;&lt;p&gt;She is joined on the paper by Jose Javier Gonzalez Ortiz PhD ’24; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering; and senior author Adrian Dalca, an assistant professor at Harvard Medical School and MGH, and a research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference on Computer Vision.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Streamlining segmentation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are primarily two methods researchers use to segment new sets of medical images. With interactive segmentation, they input an image into an AI system and use an interface to mark areas of interest. The model predicts the segmentation based on those interactions.&lt;/p&gt;&lt;p&gt;A tool previously developed by the MIT researchers,&amp;nbsp;ScribblePrompt, allows users to do this, but they must repeat the process for each new image.&lt;/p&gt;&lt;p&gt;Another approach is to develop a task-specific AI model to automatically segment the images. This approach requires the user to manually segment hundreds of images to create a dataset, and then train a machine-learning model. That model predicts the segmentation for a new image. But the user must start the complex, machine-learning-based process from scratch for each new task, and there is no way to correct the model if it makes a mistake.&lt;/p&gt;&lt;p&gt;This new system,&amp;nbsp;MultiverSeg, combines the best of each approach. It predicts a segmentation for a new image based on user interactions, like scribbles, but also keeps each segmented image in a context set that it refers to later.&lt;/p&gt;&lt;p&gt;When the user uploads a new image and marks areas of interest, the model draws on the examples in its context set to make a more accurate prediction, with less user input.&lt;/p&gt;&lt;p&gt;The researchers designed the model’s architecture to use a context set of any size, so&amp;nbsp;the user doesn’t need to have a certain number of images. This gives MultiverSeg the flexibility to be used in a range of applications.&lt;/p&gt;&lt;p&gt;“At some point, for many tasks, you shouldn’t need to provide any interactions. If you have enough examples in the context set, the model can accurately predict the segmentation on its own,” Wong says.&lt;/p&gt;&lt;p&gt;The researchers carefully engineered and trained the model on a diverse collection of biomedical imaging data to ensure it had the ability to incrementally improve its predictions based on user input.&lt;/p&gt;&lt;p&gt;The user doesn’t need to retrain or customize the model for their data. To use MultiverSeg for a new task, one can upload a new medical image and start marking it.&lt;/p&gt;&lt;p&gt;When the researchers compared MultiverSeg to state-of-the-art tools for in-context and interactive image segmentation, it outperformed each baseline.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Fewer clicks, better results&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Unlike these other tools, MultiverSeg requires less user input with each image. By the ninth new image, it needed only two clicks from the user to generate a segmentation more accurate than a model designed specifically for the task.&lt;/p&gt;&lt;p&gt;For some image types, like X-rays, the user might only need to segment one or two images manually before the model becomes accurate enough to make predictions on its own.&lt;/p&gt;&lt;p&gt;The tool’s interactivity also enables the user to make corrections to the model’s prediction, iterating until it reaches the desired level of accuracy. Compared to the researchers’ previous system, MultiverSeg reached 90 percent accuracy with roughly 2/3 the number of scribbles and 3/4 the number of clicks.&lt;/p&gt;&lt;p&gt;“With MultiverSeg, users can always provide more interactions to refine the AI predictions. This still dramatically accelerates the process because it is usually faster to correct something that exists than to start from scratch,” Wong says.&lt;/p&gt;&lt;p&gt;Moving forward, the researchers want to test this tool in real-world situations with clinical collaborators and improve it based on user feedback. They also want to enable MultiverSeg to segment 3D biomedical images.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by Quanta Computer, Inc. and the National Institutes of Health, with hardware support from the Massachusetts Life Sciences Center.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/MIT-Scalable-Segmentation-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Annotating regions of interest in medical images, a process known as segmentation, is often one of the first steps clinical researchers take when running a new study involving biomedical images.&lt;/p&gt;&lt;p&gt;For instance, to determine how the size of the brain’s hippocampus changes as patients age, the scientist first outlines each hippocampus in a series of brain scans. For many structures and image types, this is often a manual process that can be extremely time-consuming, especially if the regions being studied are challenging to delineate.&lt;/p&gt;&lt;p&gt;To streamline the process, MIT researchers developed an artificial intelligence-based system that enables a researcher to rapidly segment new biomedical imaging datasets by clicking, scribbling, and drawing boxes on the images. This new AI model uses these interactions to predict the segmentation.&lt;/p&gt;&lt;p&gt;As the user marks additional images, the number of interactions they need to perform decreases, eventually dropping to zero. The model can then segment each new image accurately without user input.&lt;/p&gt;&lt;p&gt;It can do this because the model’s architecture has been specially designed to use information from images it has already segmented to make new predictions.&lt;/p&gt;&lt;p&gt;Unlike other medical image segmentation models, this system allows the user to segment an entire dataset without repeating their work for each image.&lt;/p&gt;&lt;p&gt;In addition, the interactive tool does not require a presegmented image dataset for training, so users don’t need machine-learning expertise or extensive computational resources. They can use the system for a new segmentation task without retraining the model.&lt;/p&gt;&lt;p&gt;In the long run, this tool could accelerate studies of new treatment methods and reduce the cost of clinical trials and medical research. It could also be used by physicians to improve the efficiency of clinical applications, such as radiation treatment planning.&lt;/p&gt;&lt;p&gt;“Many scientists might only have time to segment a few images per day for their research because manual image segmentation is so time-consuming. Our hope is that this system will enable new science by allowing clinical researchers to conduct studies they were prohibited from doing before because of the lack of an efficient tool,” says Hallee Wong, an electrical engineering and computer science graduate student and lead author of a paper on this new tool.&lt;/p&gt;&lt;p&gt;She is joined on the paper by Jose Javier Gonzalez Ortiz PhD ’24; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering; and senior author Adrian Dalca, an assistant professor at Harvard Medical School and MGH, and a research scientist in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL). The research will be presented at the International Conference on Computer Vision.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Streamlining segmentation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;There are primarily two methods researchers use to segment new sets of medical images. With interactive segmentation, they input an image into an AI system and use an interface to mark areas of interest. The model predicts the segmentation based on those interactions.&lt;/p&gt;&lt;p&gt;A tool previously developed by the MIT researchers,&amp;nbsp;ScribblePrompt, allows users to do this, but they must repeat the process for each new image.&lt;/p&gt;&lt;p&gt;Another approach is to develop a task-specific AI model to automatically segment the images. This approach requires the user to manually segment hundreds of images to create a dataset, and then train a machine-learning model. That model predicts the segmentation for a new image. But the user must start the complex, machine-learning-based process from scratch for each new task, and there is no way to correct the model if it makes a mistake.&lt;/p&gt;&lt;p&gt;This new system,&amp;nbsp;MultiverSeg, combines the best of each approach. It predicts a segmentation for a new image based on user interactions, like scribbles, but also keeps each segmented image in a context set that it refers to later.&lt;/p&gt;&lt;p&gt;When the user uploads a new image and marks areas of interest, the model draws on the examples in its context set to make a more accurate prediction, with less user input.&lt;/p&gt;&lt;p&gt;The researchers designed the model’s architecture to use a context set of any size, so&amp;nbsp;the user doesn’t need to have a certain number of images. This gives MultiverSeg the flexibility to be used in a range of applications.&lt;/p&gt;&lt;p&gt;“At some point, for many tasks, you shouldn’t need to provide any interactions. If you have enough examples in the context set, the model can accurately predict the segmentation on its own,” Wong says.&lt;/p&gt;&lt;p&gt;The researchers carefully engineered and trained the model on a diverse collection of biomedical imaging data to ensure it had the ability to incrementally improve its predictions based on user input.&lt;/p&gt;&lt;p&gt;The user doesn’t need to retrain or customize the model for their data. To use MultiverSeg for a new task, one can upload a new medical image and start marking it.&lt;/p&gt;&lt;p&gt;When the researchers compared MultiverSeg to state-of-the-art tools for in-context and interactive image segmentation, it outperformed each baseline.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Fewer clicks, better results&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Unlike these other tools, MultiverSeg requires less user input with each image. By the ninth new image, it needed only two clicks from the user to generate a segmentation more accurate than a model designed specifically for the task.&lt;/p&gt;&lt;p&gt;For some image types, like X-rays, the user might only need to segment one or two images manually before the model becomes accurate enough to make predictions on its own.&lt;/p&gt;&lt;p&gt;The tool’s interactivity also enables the user to make corrections to the model’s prediction, iterating until it reaches the desired level of accuracy. Compared to the researchers’ previous system, MultiverSeg reached 90 percent accuracy with roughly 2/3 the number of scribbles and 3/4 the number of clicks.&lt;/p&gt;&lt;p&gt;“With MultiverSeg, users can always provide more interactions to refine the AI predictions. This still dramatically accelerates the process because it is usually faster to correct something that exists than to start from scratch,” Wong says.&lt;/p&gt;&lt;p&gt;Moving forward, the researchers want to test this tool in real-world situations with clinical collaborators and improve it based on user feedback. They also want to enable MultiverSeg to segment 3D biomedical images.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by Quanta Computer, Inc. and the National Institutes of Health, with hardware support from the Massachusetts Life Sciences Center.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-ai-system-could-accelerate-clinical-research-0925</guid><pubDate>Thu, 25 Sep 2025 04:00:00 +0000</pubDate></item><item><title>[NEW] It isn’t your imagination: Google Cloud is flooding the zone (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/24/it-isnt-your-imagination-google-cloud-is-flooding-the-zone/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The $100 billion partnership between Nvidia and OpenAI, announced Monday, represents – for now – the latest mega-deal reshaping the AI infrastructure landscape. The agreement involves non-voting shares tied to massive chip purchases and enough computing power for more than 5 million U.S. households, deepening the relationship between two of AI’s most powerful players.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google Cloud is placing a different bet entirely. While the industry’s biggest players cement ever-tighter partnerships, Google Cloud is hellbent on capturing the next generation of AI companies before they become too big to court.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Francis deSouza, its COO, has seen the AI revolution from multiple vantage points. As the former CEO of genomics giant Illumina, he watched machine learning transform drug discovery. As co-founder of a two-year-old AI alignment startup, Synth Labs, he has grappled with the safety challenges of increasingly powerful models. Now, having joined the C-suite at Google Cloud in January, he’s orchestrating a massive wager on AI’s second wave.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a story deSouza likes to tell in numbers. In a conversation with this editor, he notes several times that nine out of the top 10 AI labs use Google’s infrastructure. He also says that nearly all generative AI unicorns run on Google Cloud, that 60% of all gen AI startups worldwide have chosen Google as their cloud provider, and that the company has lined up $58 billion in new revenue commitments over the next two years, which represents more than double its current annual run rate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked what percentage of Google Cloud’s revenue comes from AI companies, he offers instead that “AI is resetting the cloud market, and Google Cloud is leading the way, especially with startups.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Nvidia-OpenAI deal exemplifies the scale of consolidation sweeping AI infrastructure. Microsoft’s original $1 billion OpenAI investment has grown to nearly $14 billion. Amazon followed with $8 billion in Anthropic investments,&amp;nbsp;securing deep hardware customizations that essentially tailor AI training to work better with Amazon’s infrastructure. Oracle has emerged as a surprise winner, too, landing a $30 billion cloud deal with OpenAI and then securing a jaw-dropping $300 billion five-year commitment starting in 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even Meta, despite building its own infrastructure, signed a $10 billion deal with Google Cloud while planning $600 billion in U.S. infrastructure spending through 2028. The Trump administration’s $500 billion “Stargate” project, involving SoftBank, OpenAI and Oracle, adds another layer to these interlocking partnerships.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These gigantic deals might seem threatening for Google, given the partnerships that companies like OpenAI and Nvidia appear to be cementing elsewhere. In fact, it looks a lot like Google is being cut out of some frenzied dealmaking.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image alignfull size-large"&gt;&lt;img alt="3D Google logo" class="wp-image-3041112" height="454" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;The Google logo appears during a meeting between Alphabet and Google CEO Sundar Pichai and Polish Prime Minister Donald Tusk at Google for Startups in Warsaw, Poland, on February 13, 2025. (Photo by Klaudia Radecka/NurPhoto via Getty Images)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Klaudia Radecka/NurPhoto / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But the corporate behemoth isn’t exactly sitting on its hands. Instead, Google Cloud is signing smaller companies like Loveable and Windsurf — what deSouza calls the “next generation of companies coming up”– as “primary computing partners” without major upfront investments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The approach reflects both opportunity and necessity. In a market where companies can go “from being a startup to being a multi-billion dollar company in a very short period of time,” as deSouza puts it, capturing future unicorns before they mature could prove more valuable than fighting over today’s giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The strategy extends beyond simple customer acquisition. Google offers AI startups $350,000 in cloud credits, access to its technical teams, and go-to-market support through its marketplace. Google Cloud also provides what deSouza describes as a “no compromise” AI stack – from chips to models to applications – with an “open ethos” that gives customers choice at every layer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Companies love the fact that they can get access to our AI stack, they can get access to our teams to understand where our technologies are going,” deSouza says during our interview. “They also love that they’re getting access to enterprise grade Google class infrastructure.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s infrastructure play got even more ambitious recently, with reporting revealing the company’s behind-the-scenes maneuvering to expand its custom AI chip business. According to The Information, Google has struck deals to place its tensor processing units (TPUs) in other cloud providers’ data centers for the first time, including an agreement with London-based Fluidstack that includes up to $3.2 billion in financial backing for a New York facility.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Competing directly with AI companies while simultaneously providing them infrastructure requires — let’s call it — finesse. Google Cloud provides TPU chips to OpenAI and hosts Anthropic’s Claude model through its Vertex AI platform, even as its own Gemini models compete head-to-head with both. (Google Cloud’s parent company, Alphabet, also owns a 14% stake in Anthropic, per New York Times court documents obtained earlier this year, though when asked directly about Google’s financial relationship with Anthropic, deSouza calls the relationship a “multi-layered partnership” then quickly redirects me to Google Cloud’s model marketplace – noting that customers can access various foundation models.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if Google is trying to be Switzerland while advancing its own agenda, it has had plenty of practice. The approach has roots in Google’s open-source contributions, from Kubernetes to the foundational “Attention is All You Need” paper that enabled the transformer architecture underlying most modern AI. More recently, Google published an open-source protocol called Agent-to-Agent (A2A) for inter-agent communication in an attempt to demonstrate its continued commitment to openness even in competitive areas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have made the explicit choice over the years to be open at every layer of the stack, and we know that this means companies can absolutely take our technology and use it to build a competitor at the next layer,” deSouza acknowledges. “That’s been happening for decades. That’s something we are okay with.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google Cloud’s courtship of startups comes at a particularly interesting moment. Just this month, federal judge Amit Mehta delivered a nuanced ruling in the government’s five-year-old search monopoly case, attempting to curb Google’s dominance without hampering its AI ambitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Google avoided the Justice Department’s most severe proposed penalties, including the forced divestment of its Chrome browser, the ruling underscored regulatory concerns about the company leveraging its search monopoly to dominate AI. Critics are worried, understandably, that Google’s vast trove of search data provides an unfair advantage in developing AI systems, and that the company could deploy the same monopolistic tactics that secured its search dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In conversation, deSouza is focused on far more positive outcomes. “I think we have an opportunity to fundamentally understand some of the major diseases that today we just don’t have a good understanding of,” deSouza says, for example, outlining a vision where Google Cloud helps power research into Alzheimer’s, Parkinson’s, and climate technologies. “We want to work very hard to make sure that we are pioneering the technologies that will enable that work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Critics may not easily be assuaged. By positioning itself as an open platform that empowers rather than controls the next generation of AI companies, Google Cloud may be showing regulators that it fosters competition rather than stifles it, all while forging relationships with startups that might help Google’s case if regulators ramp up pressure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For our full chat with deSouza, check out this week’s StrictlyVC Download podcast; a new episode comes out every Tuesday.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The $100 billion partnership between Nvidia and OpenAI, announced Monday, represents – for now – the latest mega-deal reshaping the AI infrastructure landscape. The agreement involves non-voting shares tied to massive chip purchases and enough computing power for more than 5 million U.S. households, deepening the relationship between two of AI’s most powerful players.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Google Cloud is placing a different bet entirely. While the industry’s biggest players cement ever-tighter partnerships, Google Cloud is hellbent on capturing the next generation of AI companies before they become too big to court.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Francis deSouza, its COO, has seen the AI revolution from multiple vantage points. As the former CEO of genomics giant Illumina, he watched machine learning transform drug discovery. As co-founder of a two-year-old AI alignment startup, Synth Labs, he has grappled with the safety challenges of increasingly powerful models. Now, having joined the C-suite at Google Cloud in January, he’s orchestrating a massive wager on AI’s second wave.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a story deSouza likes to tell in numbers. In a conversation with this editor, he notes several times that nine out of the top 10 AI labs use Google’s infrastructure. He also says that nearly all generative AI unicorns run on Google Cloud, that 60% of all gen AI startups worldwide have chosen Google as their cloud provider, and that the company has lined up $58 billion in new revenue commitments over the next two years, which represents more than double its current annual run rate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Asked what percentage of Google Cloud’s revenue comes from AI companies, he offers instead that “AI is resetting the cloud market, and Google Cloud is leading the way, especially with startups.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Nvidia-OpenAI deal exemplifies the scale of consolidation sweeping AI infrastructure. Microsoft’s original $1 billion OpenAI investment has grown to nearly $14 billion. Amazon followed with $8 billion in Anthropic investments,&amp;nbsp;securing deep hardware customizations that essentially tailor AI training to work better with Amazon’s infrastructure. Oracle has emerged as a surprise winner, too, landing a $30 billion cloud deal with OpenAI and then securing a jaw-dropping $300 billion five-year commitment starting in 2027.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Even Meta, despite building its own infrastructure, signed a $10 billion deal with Google Cloud while planning $600 billion in U.S. infrastructure spending through 2028. The Trump administration’s $500 billion “Stargate” project, involving SoftBank, OpenAI and Oracle, adds another layer to these interlocking partnerships.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;These gigantic deals might seem threatening for Google, given the partnerships that companies like OpenAI and Nvidia appear to be cementing elsewhere. In fact, it looks a lot like Google is being cut out of some frenzied dealmaking.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image alignfull size-large"&gt;&lt;img alt="3D Google logo" class="wp-image-3041112" height="454" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2198713751.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;The Google logo appears during a meeting between Alphabet and Google CEO Sundar Pichai and Polish Prime Minister Donald Tusk at Google for Startups in Warsaw, Poland, on February 13, 2025. (Photo by Klaudia Radecka/NurPhoto via Getty Images)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Klaudia Radecka/NurPhoto / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;But the corporate behemoth isn’t exactly sitting on its hands. Instead, Google Cloud is signing smaller companies like Loveable and Windsurf — what deSouza calls the “next generation of companies coming up”– as “primary computing partners” without major upfront investments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The approach reflects both opportunity and necessity. In a market where companies can go “from being a startup to being a multi-billion dollar company in a very short period of time,” as deSouza puts it, capturing future unicorns before they mature could prove more valuable than fighting over today’s giants.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The strategy extends beyond simple customer acquisition. Google offers AI startups $350,000 in cloud credits, access to its technical teams, and go-to-market support through its marketplace. Google Cloud also provides what deSouza describes as a “no compromise” AI stack – from chips to models to applications – with an “open ethos” that gives customers choice at every layer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Companies love the fact that they can get access to our AI stack, they can get access to our teams to understand where our technologies are going,” deSouza says during our interview. “They also love that they’re getting access to enterprise grade Google class infrastructure.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google’s infrastructure play got even more ambitious recently, with reporting revealing the company’s behind-the-scenes maneuvering to expand its custom AI chip business. According to The Information, Google has struck deals to place its tensor processing units (TPUs) in other cloud providers’ data centers for the first time, including an agreement with London-based Fluidstack that includes up to $3.2 billion in financial backing for a New York facility.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Competing directly with AI companies while simultaneously providing them infrastructure requires — let’s call it — finesse. Google Cloud provides TPU chips to OpenAI and hosts Anthropic’s Claude model through its Vertex AI platform, even as its own Gemini models compete head-to-head with both. (Google Cloud’s parent company, Alphabet, also owns a 14% stake in Anthropic, per New York Times court documents obtained earlier this year, though when asked directly about Google’s financial relationship with Anthropic, deSouza calls the relationship a “multi-layered partnership” then quickly redirects me to Google Cloud’s model marketplace – noting that customers can access various foundation models.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if Google is trying to be Switzerland while advancing its own agenda, it has had plenty of practice. The approach has roots in Google’s open-source contributions, from Kubernetes to the foundational “Attention is All You Need” paper that enabled the transformer architecture underlying most modern AI. More recently, Google published an open-source protocol called Agent-to-Agent (A2A) for inter-agent communication in an attempt to demonstrate its continued commitment to openness even in competitive areas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have made the explicit choice over the years to be open at every layer of the stack, and we know that this means companies can absolutely take our technology and use it to build a competitor at the next layer,” deSouza acknowledges. “That’s been happening for decades. That’s something we are okay with.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google Cloud’s courtship of startups comes at a particularly interesting moment. Just this month, federal judge Amit Mehta delivered a nuanced ruling in the government’s five-year-old search monopoly case, attempting to curb Google’s dominance without hampering its AI ambitions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Google avoided the Justice Department’s most severe proposed penalties, including the forced divestment of its Chrome browser, the ruling underscored regulatory concerns about the company leveraging its search monopoly to dominate AI. Critics are worried, understandably, that Google’s vast trove of search data provides an unfair advantage in developing AI systems, and that the company could deploy the same monopolistic tactics that secured its search dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In conversation, deSouza is focused on far more positive outcomes. “I think we have an opportunity to fundamentally understand some of the major diseases that today we just don’t have a good understanding of,” deSouza says, for example, outlining a vision where Google Cloud helps power research into Alzheimer’s, Parkinson’s, and climate technologies. “We want to work very hard to make sure that we are pioneering the technologies that will enable that work.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Critics may not easily be assuaged. By positioning itself as an open platform that empowers rather than controls the next generation of AI companies, Google Cloud may be showing regulators that it fosters competition rather than stifles it, all while forging relationships with startups that might help Google’s case if regulators ramp up pressure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For our full chat with deSouza, check out this week’s StrictlyVC Download podcast; a new episode comes out every Tuesday.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/24/it-isnt-your-imagination-google-cloud-is-flooding-the-zone/</guid><pubDate>Thu, 25 Sep 2025 04:41:36 +0000</pubDate></item></channel></rss>