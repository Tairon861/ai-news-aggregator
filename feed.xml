<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 09 Dec 2025 01:49:30 +0000</lastBuildDate><item><title> ()</title><link>https://www.wired.com/feed/category/artificial-intelligence/rss</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.wired.com/feed/category/artificial-intelligence/rss</guid></item><item><title>Google, Sony Innovation Fund, and Okta back Resemble AI’s push into deepfake detection (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-sony-and-okta-back-resemble-ai-push-into-deepfake-detection/</link><description>&lt;p&gt;Resemble AI has raised US$13 million in a new strategic investment round for AI deepfake detection. The funding brings its total venture investment to US$25 million, with participation from Berkeley CalFund, Berkeley Frontier Fund, Comcast Ventures, Craft Ventures, Gentree, Google’s AI Futures Fund, IAG Capital Partners, and others.&lt;/p&gt;&lt;p&gt;The funding comes as organisations are under pressure to verify the authenticity of digital content. Generative AI has made it easier for criminals to produce convincing deepfakes, contributing to more than US$1.56 billion in fraud losses in 2025. Analysts estimate that generative AI could enable US$40 billion in fraud losses in the US by 2027.&lt;/p&gt;&lt;p&gt;Recent incidents highlight how quickly threats evolve. In Singapore, 13 individuals collectively lost more than SGD 360,000 after scammers impersonated a telecommunications provider and the Monetary Authority of Singapore. The attackers used caller ID spoofing, voice deepfakes, and social engineering techniques that created urgency and used the public’s trust in government and telecom brands.&lt;/p&gt;&lt;h3&gt;Deepfake detection tools and new AI capabilities&lt;/h3&gt;&lt;p&gt;Resemble AI develops real-time verification tools that help enterprises detect AI-generated audio, video, images, and text. The company plans to use its new funding to expand global access to its AI deepfake detection platform, which includes two recent releases:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;DETECT-3B Omni&lt;/strong&gt;, a deepfake detection model designed for enterprise environments. The company reports 98% detection accuracy in more than 38 languages.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Resemble Intelligence&lt;/strong&gt;, a platform that provides explainability for multimodal and AI-generated content, using Google’s Gemini 3 models.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Resemble AI positions these tools as part of a broader effort to support real-time verification for human users and AI agents interacting with digital content.&lt;/p&gt;&lt;p&gt;According to the company, DETECT-3B Omni is already used in sectors like entertainment, telecommunications, and government. Public benchmark results on Hugging Face show the model ranking among the strongest performers on image and speech deepfake detection, with a lower average error rate than competing models.&lt;/p&gt;&lt;p&gt;Industry stakeholders say the rapid improvement of generative AI is reshaping how enterprises think about content trust and identity systems. Representatives from Google’s AI Futures Fund, Sony Ventures, and Okta noted organisations are moving toward verification layers that can help maintain trust in authentication processes.&lt;/p&gt;&lt;p&gt;Alongside the investment announcement, Resemble AI released its outlook on how deepfake-related risks may evolve in 2026. The company expects several shifts that could shape enterprise planning:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Deepfake verification could become standard for official communications&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Following incidents involving government officials, it anticipates real-time deepfake detection may eventually be required for official video conferencing. Such a move would likely create new procurement activity and increase adoption in the public sector.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Organisational readiness may determine competitive positioning&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As more jurisdictions introduce AI regulations, enterprises that integrate training, governance, and compliance processes early may find themselves better prepared for operational and regulatory demands.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Identity emerges as a central focus in AI security&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With many AI-related attacks relying on impersonation, organisations may place greater emphasis on identity-centric security models, including zero-trust approaches for human and machine identities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cyber insurance costs may rise&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The growing number of corporate deepfake incidents could lead insurers to reassess their policies on offer. Companies without detection tools could face higher premiums or limited coverage.&lt;/p&gt;&lt;p&gt;The investment underscores the growing need for enterprises to understand how generative AI changes their risk exposure. Organisations in all sectors are evaluating how verification, identity safeguards, and incident readiness can fit into their broader security and compliance strategies.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Pau Casals)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: AWS re:Invent 2025: Frontier AI agents replace chatbots&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-111182" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Resemble AI has raised US$13 million in a new strategic investment round for AI deepfake detection. The funding brings its total venture investment to US$25 million, with participation from Berkeley CalFund, Berkeley Frontier Fund, Comcast Ventures, Craft Ventures, Gentree, Google’s AI Futures Fund, IAG Capital Partners, and others.&lt;/p&gt;&lt;p&gt;The funding comes as organisations are under pressure to verify the authenticity of digital content. Generative AI has made it easier for criminals to produce convincing deepfakes, contributing to more than US$1.56 billion in fraud losses in 2025. Analysts estimate that generative AI could enable US$40 billion in fraud losses in the US by 2027.&lt;/p&gt;&lt;p&gt;Recent incidents highlight how quickly threats evolve. In Singapore, 13 individuals collectively lost more than SGD 360,000 after scammers impersonated a telecommunications provider and the Monetary Authority of Singapore. The attackers used caller ID spoofing, voice deepfakes, and social engineering techniques that created urgency and used the public’s trust in government and telecom brands.&lt;/p&gt;&lt;h3&gt;Deepfake detection tools and new AI capabilities&lt;/h3&gt;&lt;p&gt;Resemble AI develops real-time verification tools that help enterprises detect AI-generated audio, video, images, and text. The company plans to use its new funding to expand global access to its AI deepfake detection platform, which includes two recent releases:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;DETECT-3B Omni&lt;/strong&gt;, a deepfake detection model designed for enterprise environments. The company reports 98% detection accuracy in more than 38 languages.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Resemble Intelligence&lt;/strong&gt;, a platform that provides explainability for multimodal and AI-generated content, using Google’s Gemini 3 models.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Resemble AI positions these tools as part of a broader effort to support real-time verification for human users and AI agents interacting with digital content.&lt;/p&gt;&lt;p&gt;According to the company, DETECT-3B Omni is already used in sectors like entertainment, telecommunications, and government. Public benchmark results on Hugging Face show the model ranking among the strongest performers on image and speech deepfake detection, with a lower average error rate than competing models.&lt;/p&gt;&lt;p&gt;Industry stakeholders say the rapid improvement of generative AI is reshaping how enterprises think about content trust and identity systems. Representatives from Google’s AI Futures Fund, Sony Ventures, and Okta noted organisations are moving toward verification layers that can help maintain trust in authentication processes.&lt;/p&gt;&lt;p&gt;Alongside the investment announcement, Resemble AI released its outlook on how deepfake-related risks may evolve in 2026. The company expects several shifts that could shape enterprise planning:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Deepfake verification could become standard for official communications&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Following incidents involving government officials, it anticipates real-time deepfake detection may eventually be required for official video conferencing. Such a move would likely create new procurement activity and increase adoption in the public sector.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Organisational readiness may determine competitive positioning&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As more jurisdictions introduce AI regulations, enterprises that integrate training, governance, and compliance processes early may find themselves better prepared for operational and regulatory demands.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Identity emerges as a central focus in AI security&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;With many AI-related attacks relying on impersonation, organisations may place greater emphasis on identity-centric security models, including zero-trust approaches for human and machine identities.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cyber insurance costs may rise&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The growing number of corporate deepfake incidents could lead insurers to reassess their policies on offer. Companies without detection tools could face higher premiums or limited coverage.&lt;/p&gt;&lt;p&gt;The investment underscores the growing need for enterprises to understand how generative AI changes their risk exposure. Organisations in all sectors are evaluating how verification, identity safeguards, and incident readiness can fit into their broader security and compliance strategies.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Pau Casals)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: AWS re:Invent 2025: Frontier AI agents replace chatbots&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-111182" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-sony-and-okta-back-resemble-ai-push-into-deepfake-detection/</guid><pubDate>Mon, 08 Dec 2025 14:00:00 +0000</pubDate></item><item><title>Battling algorithmic bias in digital payments leads to competition win (AI News)</title><link>https://www.artificialintelligence-news.com/news/ai-algorithmic-bias-in-digital-payments-leads-to-competition-win/</link><description>&lt;p&gt;Digital payments and fintech company Ant International, has won the NeurIPS Competition of Fairness in AI Face Detection. The company says it’s committed to developing secure and inclusive financial services, particularly as deepfake technologies are becoming more common.&lt;/p&gt;&lt;p&gt;The growing use of facial recognition in many sectors has highlighted the issue of algorithmic bias in AI. Research conducted by NIST (National Institute of Standards and Technology) shows many widely used facial recognition algorithms exhibit considerably higher error rates when analysing the faces of women and people of colour, a disparity that stems from a lack of diversity in the training data and the demographics of those building and controlling many mainstream AI platforms. The consequences of biased algorithms can lead to the denial of financial services to large sections of the population, and is seen as a vulnerability in security protocols.&lt;/p&gt;&lt;p&gt;The NeurIPS Competition was held alongside the Conference on Neural Information Processing Systems, the well-respected AI conference, and challenged participants to create AI models capable of high performance and fairness covering a range of demographic factors: Gender, age, and skin tone. Ant International’s team beat over 2,100 submissions from 162 teams coming from all over the world. The given task was to accurately detect 1.2 million AI-generated face images which were chosen as properly representative of demographic groups.&lt;/p&gt;&lt;p&gt;The approach taken by Ant’s winning AI model combines a Mixture of Experts (MoE) architecture with a bias-detection mechanism. The system trains two competing neural networks: one focused on identifying deepfakes, and the other designed to challenge the first, forcing it to disregard demographic characteristics. This dynamic process helps ensure the system learns to detect genuine signs of manipulation rather than inadvertently relying on demographic patterns. The model’s training incorporated a globally representative dataset and incorporated real-world payment fraud scenarios to ensure its performance at scale.&lt;/p&gt;&lt;p&gt;“A biased AI system is inherently an insecure one,” explained Dr.Tianyi Zhang, general manager of risk management and cybersecurity at Ant International. “Our model’s fairness isn’t just a matter of ethics; it’s fundamental to preventing exploitation from deepfakes and ensuring reliable identity verification for every user”.&lt;/p&gt;&lt;p&gt;The technology behind the winning entry is now being integrated into Ant’s payment and financial services to help counter the threat of deepfakes, and the companies says it achieves a detection rate of in excess of 99.8% in all demographics and in the 200 markets Ant operates in.&lt;/p&gt;&lt;p&gt;Ant’s technology helps its customers meet global Electronic Know Your Customer (eKYC) standards, particularly during customer onboarding, without algorithmic bias. That’s held to be particularly important in emerging markets where greater financial inclusion can be hampered.&lt;/p&gt;&lt;p&gt;Ant International serves over 150 million merchants and 1.8 billion user accounts, known for services like Alipay+, Antom, Bettr and WorldFirst. The company has stated AI security is a pillar of its operations. Its AI SHIELD, a framework for risk management as built on AI Security Docker to help mitigate the risk of vulnerabilities in AI services like unauthorised access and data leakage.&lt;/p&gt;&lt;p&gt;AI SHIELD underpins a suite of risk-management solutions that provide broader protection of financial transactions, including measures against deepfake attacks and fraud. Alipay+ EasySafePay 360 has reduced incidents of account takeover in digital wallet payments by 90%, the company says.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “abstract art of a beautiful portrait, solid shapes, geometric shapes, neotokyo colors, muted colors, pixar, artstation, greg rutkowski, samdoesart, ge” – public domain)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Digital payments and fintech company Ant International, has won the NeurIPS Competition of Fairness in AI Face Detection. The company says it’s committed to developing secure and inclusive financial services, particularly as deepfake technologies are becoming more common.&lt;/p&gt;&lt;p&gt;The growing use of facial recognition in many sectors has highlighted the issue of algorithmic bias in AI. Research conducted by NIST (National Institute of Standards and Technology) shows many widely used facial recognition algorithms exhibit considerably higher error rates when analysing the faces of women and people of colour, a disparity that stems from a lack of diversity in the training data and the demographics of those building and controlling many mainstream AI platforms. The consequences of biased algorithms can lead to the denial of financial services to large sections of the population, and is seen as a vulnerability in security protocols.&lt;/p&gt;&lt;p&gt;The NeurIPS Competition was held alongside the Conference on Neural Information Processing Systems, the well-respected AI conference, and challenged participants to create AI models capable of high performance and fairness covering a range of demographic factors: Gender, age, and skin tone. Ant International’s team beat over 2,100 submissions from 162 teams coming from all over the world. The given task was to accurately detect 1.2 million AI-generated face images which were chosen as properly representative of demographic groups.&lt;/p&gt;&lt;p&gt;The approach taken by Ant’s winning AI model combines a Mixture of Experts (MoE) architecture with a bias-detection mechanism. The system trains two competing neural networks: one focused on identifying deepfakes, and the other designed to challenge the first, forcing it to disregard demographic characteristics. This dynamic process helps ensure the system learns to detect genuine signs of manipulation rather than inadvertently relying on demographic patterns. The model’s training incorporated a globally representative dataset and incorporated real-world payment fraud scenarios to ensure its performance at scale.&lt;/p&gt;&lt;p&gt;“A biased AI system is inherently an insecure one,” explained Dr.Tianyi Zhang, general manager of risk management and cybersecurity at Ant International. “Our model’s fairness isn’t just a matter of ethics; it’s fundamental to preventing exploitation from deepfakes and ensuring reliable identity verification for every user”.&lt;/p&gt;&lt;p&gt;The technology behind the winning entry is now being integrated into Ant’s payment and financial services to help counter the threat of deepfakes, and the companies says it achieves a detection rate of in excess of 99.8% in all demographics and in the 200 markets Ant operates in.&lt;/p&gt;&lt;p&gt;Ant’s technology helps its customers meet global Electronic Know Your Customer (eKYC) standards, particularly during customer onboarding, without algorithmic bias. That’s held to be particularly important in emerging markets where greater financial inclusion can be hampered.&lt;/p&gt;&lt;p&gt;Ant International serves over 150 million merchants and 1.8 billion user accounts, known for services like Alipay+, Antom, Bettr and WorldFirst. The company has stated AI security is a pillar of its operations. Its AI SHIELD, a framework for risk management as built on AI Security Docker to help mitigate the risk of vulnerabilities in AI services like unauthorised access and data leakage.&lt;/p&gt;&lt;p&gt;AI SHIELD underpins a suite of risk-management solutions that provide broader protection of financial transactions, including measures against deepfake attacks and fraud. Alipay+ EasySafePay 360 has reduced incidents of account takeover in digital wallet payments by 90%, the company says.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “abstract art of a beautiful portrait, solid shapes, geometric shapes, neotokyo colors, muted colors, pixar, artstation, greg rutkowski, samdoesart, ge” – public domain)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ai-algorithmic-bias-in-digital-payments-leads-to-competition-win/</guid><pubDate>Mon, 08 Dec 2025 14:19:28 +0000</pubDate></item><item><title>OpenAI: Enterprise users swap AI pilots for deep integrations (AI News)</title><link>https://www.artificialintelligence-news.com/news/openai-enterprise-users-swap-ai-pilots-for-deep-integrations/</link><description>&lt;p&gt;According to OpenAI, enterprise AI has graduated from the sandbox and is now being used for daily operations with deep workflow integrations.&lt;/p&gt;&lt;p&gt;New data from the company shows that firms are now assigning complex and multi-step workflows to models rather than simply asking for text summaries. The figures illustrate a hard change in how organisations deploy generative models.&lt;/p&gt;&lt;p&gt;With OpenAI’s platform now serving over 800 million users weekly, a “flywheel” effect is driving consumer familiarity into professional environments. The company’s latest report notes that over a million business customers now use these tools, and the goal is now even deeper integration.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This evolution presents two realities for decision-makers: productivity gains are concrete, but a growing divide between “frontier” adopters and the median enterprise suggests that value depends heavily on usage intensity.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-chatbots-to-deep-reasoning"&gt;From chatbots to deep reasoning&lt;/h3&gt;&lt;p&gt;The best metric for corporate deployment maturity is not seat count, but task complexity&lt;/p&gt;&lt;p&gt;OpenAI reports that ChatGPT message volume has grown eightfold year-over-year, but a better indicator for enterprise architects is the consumption of API reasoning tokens which suggests deeper integrations are taking place. This figure has increased by nearly 320 times per organisation—evidence that companies are systematically wiring more intelligent models into their products to handle logic rather than basic queries.&lt;/p&gt;&lt;p&gt;The rise of configurable interfaces supports this view. Weekly users of Custom GPTs and Projects (tools that allow workers to instruct models with specific institutional knowledge) have increased approximately 19x this year. Roughly 20 percent of all enterprise messages are now processed via these customised environments, indicating that standardisation is now a prerequisite for professional use.&lt;/p&gt;&lt;p&gt;For enterprise leaders auditing the ROI of AI seats, the data offers evidence on time savings. On average, users attribute between 40-60 minutes of time saved per active day to the technology. The impact varies by function: data science, engineering, and communication professionals report higher savings (averaging 60-80 minutes daily.)&lt;/p&gt;&lt;p&gt;Beyond efficiency, the software is altering role boundaries. There is a specific effect on technical capability, particularly regarding code generation.&lt;/p&gt;&lt;p&gt;Among enterprise users, OpenAI says that coding-related messages have risen across all business functions. Outside of engineering, IT, and research roles, coding queries have grown by an average of 36 percent over the past six months. Non-technical teams are using the tools to perform analysis that previously required specialised developers.&lt;/p&gt;&lt;p&gt;Operational improvements extend across departments. Survey data shows 87 percent of IT workers report faster issue resolution, while 75 percent of HR professionals see improved employee engagement.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-widening-enterprise-ai-competence-gap"&gt;Widening enterprise AI competence gap&lt;/h3&gt;&lt;p&gt;OpenAI’s data suggests that a split is forming between organisations that simply provide access to tools and those in which integrations are being deeply embedded into their operating models. The report identifies a “frontier” class of workers – those in the 95th percentile of adoption intensity – who generate six times more messages than the median worker.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This disparity is stark at the organisational level. Frontier firms generate approximately twice as many messages per seat as the median enterprise and seven times more messages to custom GPTs. Leading firms are not just using the tools more frequently; they are investing in the infrastructure and standardisation required to make AI a persistent part of operations.&lt;/p&gt;&lt;p&gt;Users who engage across a wider variety of tasks (roughly seven distinct types) report saving five times more time than those who limit their usage to three or four basic functions. Benefits correlate directly with the depth of use, implying that a “light touch” deployment plan may fail to deliver the anticipated ROI.&lt;/p&gt;&lt;p&gt;While the professional services, finance, and technology sectors were early adopters and maintain the largest scale of usage, other industries are sprinting to catch up. The technology sector leads with 11x year-over-year growth, but healthcare and manufacturing follow closely with 8x and 7x growth respectively.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Global adoption patterns also challenge the notion that this is solely a US-centric phenomenon. International usage is surging, with markets such as Australia, Brazil, the Netherlands, and France showing business customer growth rates exceeding 140 percent year-over-year. Japan has also surfaced as a key market, holding the largest number of corporate API customers outside of the US.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-openai-deep-ai-integrations-accelerate-enterprise-workflows"&gt;OpenAI: Deep AI integrations accelerate enterprise workflows&lt;/h3&gt;&lt;p&gt;Examples of deployment highlight how these tools influence key business metrics. Retailer Lowe’s deployed an associate-facing tool to over 1,700 stores, resulting in a customer satisfaction score increase of 200 basis points when associates used the system. Furthermore, when online customers engaged with the retailer’s AI tool, conversion rates more than doubled.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In the pharmaceutical sector, Moderna used enterprise AI to speed up the drafting of Target Product Profiles (TPPs), a process that typically involves weeks of cross-functional effort. By automating the extraction of key facts from massive evidence packs, the company reduced core analytical steps from weeks to hours.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Financial services firm BBVA leveraged the technology to fix a bottleneck in legal validation for corporate signatory authority. By building a generative AI solution to handle standard legal queries, the bank automated over 9,000 queries annually, effectively freeing up the equivalent of three full-time employees for higher-value tasks.&lt;/p&gt;&lt;p&gt;However, the transition to production-grade AI requires more than software procurement; it necessitates organisational readiness. The primary blockers for many organisations are no longer model capabilities, but implementation and internal structures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Leading firms consistently enable deep system integration by “turning on” connectors that give models secure access to company data. Yet, roughly one in four enterprises has not taken this step, limiting their models to generic knowledge rather than specific organisational context.&lt;/p&gt;&lt;p&gt;Successful deployment relies on executive sponsorship that sets explicit mandates and encourages the codification of institutional knowledge into reusable assets.&amp;nbsp;&lt;/p&gt;&lt;p&gt;As the technology continues to evolve, organisations must adjust their approach. OpenAI’s data suggests that success now depends on delegating complex workflows with deep integrations rather than just asking for outputs, treating AI as a primary engine for enterprise revenue growth.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;AWS re:Invent 2025: Frontier AI agents replace chatbots&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;According to OpenAI, enterprise AI has graduated from the sandbox and is now being used for daily operations with deep workflow integrations.&lt;/p&gt;&lt;p&gt;New data from the company shows that firms are now assigning complex and multi-step workflows to models rather than simply asking for text summaries. The figures illustrate a hard change in how organisations deploy generative models.&lt;/p&gt;&lt;p&gt;With OpenAI’s platform now serving over 800 million users weekly, a “flywheel” effect is driving consumer familiarity into professional environments. The company’s latest report notes that over a million business customers now use these tools, and the goal is now even deeper integration.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This evolution presents two realities for decision-makers: productivity gains are concrete, but a growing divide between “frontier” adopters and the median enterprise suggests that value depends heavily on usage intensity.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-chatbots-to-deep-reasoning"&gt;From chatbots to deep reasoning&lt;/h3&gt;&lt;p&gt;The best metric for corporate deployment maturity is not seat count, but task complexity&lt;/p&gt;&lt;p&gt;OpenAI reports that ChatGPT message volume has grown eightfold year-over-year, but a better indicator for enterprise architects is the consumption of API reasoning tokens which suggests deeper integrations are taking place. This figure has increased by nearly 320 times per organisation—evidence that companies are systematically wiring more intelligent models into their products to handle logic rather than basic queries.&lt;/p&gt;&lt;p&gt;The rise of configurable interfaces supports this view. Weekly users of Custom GPTs and Projects (tools that allow workers to instruct models with specific institutional knowledge) have increased approximately 19x this year. Roughly 20 percent of all enterprise messages are now processed via these customised environments, indicating that standardisation is now a prerequisite for professional use.&lt;/p&gt;&lt;p&gt;For enterprise leaders auditing the ROI of AI seats, the data offers evidence on time savings. On average, users attribute between 40-60 minutes of time saved per active day to the technology. The impact varies by function: data science, engineering, and communication professionals report higher savings (averaging 60-80 minutes daily.)&lt;/p&gt;&lt;p&gt;Beyond efficiency, the software is altering role boundaries. There is a specific effect on technical capability, particularly regarding code generation.&lt;/p&gt;&lt;p&gt;Among enterprise users, OpenAI says that coding-related messages have risen across all business functions. Outside of engineering, IT, and research roles, coding queries have grown by an average of 36 percent over the past six months. Non-technical teams are using the tools to perform analysis that previously required specialised developers.&lt;/p&gt;&lt;p&gt;Operational improvements extend across departments. Survey data shows 87 percent of IT workers report faster issue resolution, while 75 percent of HR professionals see improved employee engagement.&amp;nbsp;&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-widening-enterprise-ai-competence-gap"&gt;Widening enterprise AI competence gap&lt;/h3&gt;&lt;p&gt;OpenAI’s data suggests that a split is forming between organisations that simply provide access to tools and those in which integrations are being deeply embedded into their operating models. The report identifies a “frontier” class of workers – those in the 95th percentile of adoption intensity – who generate six times more messages than the median worker.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This disparity is stark at the organisational level. Frontier firms generate approximately twice as many messages per seat as the median enterprise and seven times more messages to custom GPTs. Leading firms are not just using the tools more frequently; they are investing in the infrastructure and standardisation required to make AI a persistent part of operations.&lt;/p&gt;&lt;p&gt;Users who engage across a wider variety of tasks (roughly seven distinct types) report saving five times more time than those who limit their usage to three or four basic functions. Benefits correlate directly with the depth of use, implying that a “light touch” deployment plan may fail to deliver the anticipated ROI.&lt;/p&gt;&lt;p&gt;While the professional services, finance, and technology sectors were early adopters and maintain the largest scale of usage, other industries are sprinting to catch up. The technology sector leads with 11x year-over-year growth, but healthcare and manufacturing follow closely with 8x and 7x growth respectively.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Global adoption patterns also challenge the notion that this is solely a US-centric phenomenon. International usage is surging, with markets such as Australia, Brazil, the Netherlands, and France showing business customer growth rates exceeding 140 percent year-over-year. Japan has also surfaced as a key market, holding the largest number of corporate API customers outside of the US.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-openai-deep-ai-integrations-accelerate-enterprise-workflows"&gt;OpenAI: Deep AI integrations accelerate enterprise workflows&lt;/h3&gt;&lt;p&gt;Examples of deployment highlight how these tools influence key business metrics. Retailer Lowe’s deployed an associate-facing tool to over 1,700 stores, resulting in a customer satisfaction score increase of 200 basis points when associates used the system. Furthermore, when online customers engaged with the retailer’s AI tool, conversion rates more than doubled.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In the pharmaceutical sector, Moderna used enterprise AI to speed up the drafting of Target Product Profiles (TPPs), a process that typically involves weeks of cross-functional effort. By automating the extraction of key facts from massive evidence packs, the company reduced core analytical steps from weeks to hours.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Financial services firm BBVA leveraged the technology to fix a bottleneck in legal validation for corporate signatory authority. By building a generative AI solution to handle standard legal queries, the bank automated over 9,000 queries annually, effectively freeing up the equivalent of three full-time employees for higher-value tasks.&lt;/p&gt;&lt;p&gt;However, the transition to production-grade AI requires more than software procurement; it necessitates organisational readiness. The primary blockers for many organisations are no longer model capabilities, but implementation and internal structures.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Leading firms consistently enable deep system integration by “turning on” connectors that give models secure access to company data. Yet, roughly one in four enterprises has not taken this step, limiting their models to generic knowledge rather than specific organisational context.&lt;/p&gt;&lt;p&gt;Successful deployment relies on executive sponsorship that sets explicit mandates and encourages the codification of institutional knowledge into reusable assets.&amp;nbsp;&lt;/p&gt;&lt;p&gt;As the technology continues to evolve, organisations must adjust their approach. OpenAI’s data suggests that success now depends on delegating complex workflows with deep integrations rather than just asking for outputs, treating AI as a primary engine for enterprise revenue growth.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;AWS re:Invent 2025: Frontier AI agents replace chatbots&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/openai-enterprise-users-swap-ai-pilots-for-deep-integrations/</guid><pubDate>Mon, 08 Dec 2025 14:41:25 +0000</pubDate></item><item><title>Booking.com’s agent strategy: Disciplined, modular and already delivering 2× accuracy (AI | VentureBeat)</title><link>https://venturebeat.com/ai/booking-coms-agent-strategy-disciplined-modular-and-already-delivering-2</link><description>[unable to retrieve full-text content]&lt;p&gt;When many enterprises weren’t even thinking about agentic behaviors or infrastructures, &lt;a href="http://Booking.com"&gt;Booking.com&lt;/a&gt; had already “stumbled” into them with its homegrown conversational recommendation system. 

This early experimentation has allowed the company to take a step back and avoid getting swept up in the frantic AI agent hype. Instead, it is taking a disciplined, layered, modular approach to model development: small, travel-specific models for cheap, fast inference; larger large language models (LLMs) for reasoning and understanding; and domain-tuned evaluations built in-house when precision is critical. 

With this hybrid strategy — combined with selective collaboration with OpenAI — Booking.com has seen accuracy double across key retrieval, ranking and customer-interaction tasks.

As Pranav Pathak, Booking.com’s AI product development lead, posed to VentureBeat in a new podcast: “Do you build it very, very specialized and bespoke and then have an army of a hundred agents? Or do you keep it general enough and have five agents that are good at generalized tasks, but then you have to orchestrate a lot around them? That&amp;#x27;s a balance that I think we&amp;#x27;re still trying to figure out, as is the rest of the industry.”

Check out the new &lt;i&gt;Beyond the Pilot&lt;/i&gt; &lt;a href="https://beyondthepilot.ubpages.com/"&gt;podcast here&lt;/a&gt;, and continue reading for highlights. &lt;/p&gt;&lt;h3&gt;Moving from guessing to deep personalization without being ‘creepy’&lt;/h3&gt;&lt;p&gt;Recommendation systems are core to Booking.com’s customer-facing platforms; however, traditional recommendation tools have been less about recommendation and more about guessing, Pathak conceded. So, from the start, he and his team vowed to avoid generic tools: As he put it, the price and recommendation should be based on customer context. 

Booking.com’s initial pre-gen AI tooling for intent and topic detection was a small language model, what Pathak described as “the scale and size of BERT.” The model ingested the customer’s inputs around their problem to determine whether it could be solved through self-service or bumped to a human agent. 

“We started with an architecture of ‘you have to call a tool if this is the intent you detect and this is how you&amp;#x27;ve parsed the structure,” Pathak explained. “That was very, very similar to the first few agentic architectures that came out in terms of reason and defining a tool call.” 

His team has since built out that architecture to include an LLM orchestrator that classifies queries, triggers retrieval-augmented generation (RAG) and calls APIs or smaller, specialized language models. “We&amp;#x27;ve been able to scale that system quite well because it was so close in architecture that, with a few tweaks, we now have a full agentic stack,” said Pathak. 

As a result, Booking.com is seeing a 2X increase in topic detection, which in turn is freeing up human agents’ bandwidth by 1.5 to 1.7X. More topics, even complicated ones previously identified as ‘other’ and requiring escalation, are being automated. 

Ultimately, this supports more self-service, freeing human agents to focus on customers with uniquely-specific problems that the platform doesn’t have a dedicated tool flow for — say, a family that is unable to access its hotel room at 2 a.m. when the front desk is closed. 

That not only “really starts to compound,” but has a direct, long-term impact on customer retention, Pathak noted. “One of the things we&amp;#x27;ve seen is, the better we are at customer service, the more loyal our customers are.”

Another recent rollout is personalized filtering. Booking.com has between 200 and 250 search filters on its website — an unrealistic amount for any human to sift through, Pathak pointed out. So, his team introduced a free text box that users can type into to immediately receive tailored filters. 

“That becomes such an important cue for personalization in terms of what you&amp;#x27;re looking for in your own words rather than a clickstream,” said Pathak. 

In turn, it cues Booking.com into what customers actually want. For instance, hot tubs — when filter personalization first rolled out, jacuzzi’s were one of the most popular requests. That wasn’t even a consideration previously; there wasn’t even a filter. Now that filter is live. 

“I had no idea,” Pathak noted. “I had never searched for a hot tub in my room honestly.”

When it comes to personalization, though, there is a fine line; memory remains complicated, Pathak emphasized. While it’s important to have long-term memories and evolving threads with customers — retaining information like their typical budgets, preferred hotel star ratings or whether they need disability access — it must be on their terms and protective of their privacy. 

Booking.com is extremely mindful with memory, seeking consent so as to not be “creepy” when collecting customer information. 

“Managing memory is much harder than actually building memory,” said Pathak. “The tech is out there, we have the technical chops to build it. We want to make sure we don&amp;#x27;t launch a memory object that doesn&amp;#x27;t respect customer consent, that doesn&amp;#x27;t feel very natural.”&lt;/p&gt;&lt;h3&gt;Finding a balance of build versus buy&lt;/h3&gt;&lt;p&gt;
As agents mature, Booking.com is navigating a central question facing the entire industry: How narrow should agents become? 

Instead of committing to either a swarm of highly specialized agents or a few generalized ones, the company aims for reversible decisions and avoids “one-way doors” that lock its architecture into long-term, costly paths. Pathak’s strategy is: Generalize where possible, specialize where necessary and keep agent design flexible to help ensure resiliency. 

Pathak and his team are “very mindful” of use cases, evaluating where to build more generalized, reusable agents or more task-specific ones. They strive to use the smallest model possible, with the highest level of accuracy and output quality, for each use case. Whatever can be generalized is. 

Latency is another important consideration. When factual accuracy and avoiding hallucinations is paramount, his team will use a larger, much slower model; but with search and recommendations, user expectations set speed. (Pathak noted: “No one’s patient.”)

“We would, for example, never use something as heavy as GPT-5 for just topic detection or for entity extraction,” he said. 

Booking.com takes a similarly elastic tack when it comes to monitoring and evaluations: If it&amp;#x27;s general-purpose monitoring that someone else is better at building and has horizontal capability, they’ll buy it. But if it’s instances where brand guidelines must be enforced, they’ll build their own evals. 

Ultimately, Booking.com has leaned into being “super anticipatory,” agile and flexible. “At this point with everything that&amp;#x27;s happening with AI, we are a little bit averse to walking through one way doors,” said Pathak. “We want as many of our decisions to be reversible as possible. We don&amp;#x27;t want to get locked into a decision that we cannot reverse two years from now.”&lt;/p&gt;&lt;h3&gt;What other builders can learn from Booking.com’s AI journey&lt;/h3&gt;&lt;p&gt;Booking.com’s AI journey can serve as an important blueprint for other enterprises. 

Looking back, Pathak acknowledged that they started out with a “pretty complicated” tech stack. They’re now in a good place with that, “but we probably could have started something much simpler and seen how customers interacted with it.”

Given that, he offered this valuable advice: If you’re just starting out with LLMs or agents, out-of-the-box APIs will do just fine. “There&amp;#x27;s enough customization with APIs that you can already get a lot of leverage before you decide you want to go do more.” 

On the other hand, if a use case requires customization not available through a standard API call, that makes a case for in-house tools. 

Still, he emphasized: Don&amp;#x27;t start with the complicated stuff. Tackle the “simplest, most painful problem you can find and the simplest, most obvious solution to that.” 

Identify the product market fit, then investigate the ecosystems, he advised — but don’t just rip out old infrastructures because a new use case demands something specific (like moving an entire cloud strategy from AWS to Azure just to use the OpenAI endpoint). 

Ultimately: “Don&amp;#x27;t lock yourself in too early,” Pathak noted. “Don&amp;#x27;t make decisions that are one-way doors until you are very confident that that&amp;#x27;s the solution that you want to go with.”&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;When many enterprises weren’t even thinking about agentic behaviors or infrastructures, &lt;a href="http://Booking.com"&gt;Booking.com&lt;/a&gt; had already “stumbled” into them with its homegrown conversational recommendation system. 

This early experimentation has allowed the company to take a step back and avoid getting swept up in the frantic AI agent hype. Instead, it is taking a disciplined, layered, modular approach to model development: small, travel-specific models for cheap, fast inference; larger large language models (LLMs) for reasoning and understanding; and domain-tuned evaluations built in-house when precision is critical. 

With this hybrid strategy — combined with selective collaboration with OpenAI — Booking.com has seen accuracy double across key retrieval, ranking and customer-interaction tasks.

As Pranav Pathak, Booking.com’s AI product development lead, posed to VentureBeat in a new podcast: “Do you build it very, very specialized and bespoke and then have an army of a hundred agents? Or do you keep it general enough and have five agents that are good at generalized tasks, but then you have to orchestrate a lot around them? That&amp;#x27;s a balance that I think we&amp;#x27;re still trying to figure out, as is the rest of the industry.”

Check out the new &lt;i&gt;Beyond the Pilot&lt;/i&gt; &lt;a href="https://beyondthepilot.ubpages.com/"&gt;podcast here&lt;/a&gt;, and continue reading for highlights. &lt;/p&gt;&lt;h3&gt;Moving from guessing to deep personalization without being ‘creepy’&lt;/h3&gt;&lt;p&gt;Recommendation systems are core to Booking.com’s customer-facing platforms; however, traditional recommendation tools have been less about recommendation and more about guessing, Pathak conceded. So, from the start, he and his team vowed to avoid generic tools: As he put it, the price and recommendation should be based on customer context. 

Booking.com’s initial pre-gen AI tooling for intent and topic detection was a small language model, what Pathak described as “the scale and size of BERT.” The model ingested the customer’s inputs around their problem to determine whether it could be solved through self-service or bumped to a human agent. 

“We started with an architecture of ‘you have to call a tool if this is the intent you detect and this is how you&amp;#x27;ve parsed the structure,” Pathak explained. “That was very, very similar to the first few agentic architectures that came out in terms of reason and defining a tool call.” 

His team has since built out that architecture to include an LLM orchestrator that classifies queries, triggers retrieval-augmented generation (RAG) and calls APIs or smaller, specialized language models. “We&amp;#x27;ve been able to scale that system quite well because it was so close in architecture that, with a few tweaks, we now have a full agentic stack,” said Pathak. 

As a result, Booking.com is seeing a 2X increase in topic detection, which in turn is freeing up human agents’ bandwidth by 1.5 to 1.7X. More topics, even complicated ones previously identified as ‘other’ and requiring escalation, are being automated. 

Ultimately, this supports more self-service, freeing human agents to focus on customers with uniquely-specific problems that the platform doesn’t have a dedicated tool flow for — say, a family that is unable to access its hotel room at 2 a.m. when the front desk is closed. 

That not only “really starts to compound,” but has a direct, long-term impact on customer retention, Pathak noted. “One of the things we&amp;#x27;ve seen is, the better we are at customer service, the more loyal our customers are.”

Another recent rollout is personalized filtering. Booking.com has between 200 and 250 search filters on its website — an unrealistic amount for any human to sift through, Pathak pointed out. So, his team introduced a free text box that users can type into to immediately receive tailored filters. 

“That becomes such an important cue for personalization in terms of what you&amp;#x27;re looking for in your own words rather than a clickstream,” said Pathak. 

In turn, it cues Booking.com into what customers actually want. For instance, hot tubs — when filter personalization first rolled out, jacuzzi’s were one of the most popular requests. That wasn’t even a consideration previously; there wasn’t even a filter. Now that filter is live. 

“I had no idea,” Pathak noted. “I had never searched for a hot tub in my room honestly.”

When it comes to personalization, though, there is a fine line; memory remains complicated, Pathak emphasized. While it’s important to have long-term memories and evolving threads with customers — retaining information like their typical budgets, preferred hotel star ratings or whether they need disability access — it must be on their terms and protective of their privacy. 

Booking.com is extremely mindful with memory, seeking consent so as to not be “creepy” when collecting customer information. 

“Managing memory is much harder than actually building memory,” said Pathak. “The tech is out there, we have the technical chops to build it. We want to make sure we don&amp;#x27;t launch a memory object that doesn&amp;#x27;t respect customer consent, that doesn&amp;#x27;t feel very natural.”&lt;/p&gt;&lt;h3&gt;Finding a balance of build versus buy&lt;/h3&gt;&lt;p&gt;
As agents mature, Booking.com is navigating a central question facing the entire industry: How narrow should agents become? 

Instead of committing to either a swarm of highly specialized agents or a few generalized ones, the company aims for reversible decisions and avoids “one-way doors” that lock its architecture into long-term, costly paths. Pathak’s strategy is: Generalize where possible, specialize where necessary and keep agent design flexible to help ensure resiliency. 

Pathak and his team are “very mindful” of use cases, evaluating where to build more generalized, reusable agents or more task-specific ones. They strive to use the smallest model possible, with the highest level of accuracy and output quality, for each use case. Whatever can be generalized is. 

Latency is another important consideration. When factual accuracy and avoiding hallucinations is paramount, his team will use a larger, much slower model; but with search and recommendations, user expectations set speed. (Pathak noted: “No one’s patient.”)

“We would, for example, never use something as heavy as GPT-5 for just topic detection or for entity extraction,” he said. 

Booking.com takes a similarly elastic tack when it comes to monitoring and evaluations: If it&amp;#x27;s general-purpose monitoring that someone else is better at building and has horizontal capability, they’ll buy it. But if it’s instances where brand guidelines must be enforced, they’ll build their own evals. 

Ultimately, Booking.com has leaned into being “super anticipatory,” agile and flexible. “At this point with everything that&amp;#x27;s happening with AI, we are a little bit averse to walking through one way doors,” said Pathak. “We want as many of our decisions to be reversible as possible. We don&amp;#x27;t want to get locked into a decision that we cannot reverse two years from now.”&lt;/p&gt;&lt;h3&gt;What other builders can learn from Booking.com’s AI journey&lt;/h3&gt;&lt;p&gt;Booking.com’s AI journey can serve as an important blueprint for other enterprises. 

Looking back, Pathak acknowledged that they started out with a “pretty complicated” tech stack. They’re now in a good place with that, “but we probably could have started something much simpler and seen how customers interacted with it.”

Given that, he offered this valuable advice: If you’re just starting out with LLMs or agents, out-of-the-box APIs will do just fine. “There&amp;#x27;s enough customization with APIs that you can already get a lot of leverage before you decide you want to go do more.” 

On the other hand, if a use case requires customization not available through a standard API call, that makes a case for in-house tools. 

Still, he emphasized: Don&amp;#x27;t start with the complicated stuff. Tackle the “simplest, most painful problem you can find and the simplest, most obvious solution to that.” 

Identify the product market fit, then investigate the ecosystems, he advised — but don’t just rip out old infrastructures because a new use case demands something specific (like moving an entire cloud strategy from AWS to Azure just to use the OpenAI endpoint). 

Ultimately: “Don&amp;#x27;t lock yourself in too early,” Pathak noted. “Don&amp;#x27;t make decisions that are one-way doors until you are very confident that that&amp;#x27;s the solution that you want to go with.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/booking-coms-agent-strategy-disciplined-modular-and-already-delivering-2</guid><pubDate>Mon, 08 Dec 2025 15:00:00 +0000</pubDate></item><item><title>‘ONE RULE’: Trump says he’ll sign an executive order blocking state AI laws despite bipartisan pushback (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/one-rule-trump-says-hell-sign-an-executive-order-blocking-state-ai-laws-despite-bipartisan-pushback/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/rnc-2024-report-v3.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;President Donald Trump said on Monday he plans to ink an executive order this week that would limit states from enacting their own regulation of AI technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I will be doing a ONE RULE Executive Order this week,” Trump posted on social media. “You can’t expect a company to get 50 Approvals every time they want to do something.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There must be only One Rulebook if we are going to continue to lead in AI,” Trump said. “We are beating ALL COUNTRIES at this point in the race, but that won’t last long if we are going to have 50 States, many of them bad actors, involved in RULES and the APPROVAL PROCESS…AI WILL BE DESTROYED IN ITS INFANCY!”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s statement comes days after an effort to preempt states from regulating AI was quashed in the Senate, as Congress couldn’t agree to insert the deeply unpopular proposal into a must-pass defense budget bill.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fast pace of AI development and the lack of general consumer protections from the federal government has led many states to enact their own rules around the technology. California, for example, has the AI safety and transparency bill SB 53, while Tennessee’s ELVIS Act protects musicians and performers from unauthorized AI-generated deepfakes of their voices and likenesses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Silicon Valley figures, including OpenAI President Greg Brockman and VC-turned-White House “AI czar” David Sacks, have argued that such laws by states would create an unworkable patchwork of laws that would stifle innovation and threaten the U.S.’s lead against China in the race to develop AI technology.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Silicon Valley has a mighty lobbying arm that has blocked meaningful technology regulation for years, and proponents of states’ regulatory rights say there’s no reason to believe state AI laws could “destroy AI progress,” as VCs and tech companies claim. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s executive order, a draft of which was leaked a couple of weeks ago, would create an “AI Litigation Task Force” to challenge state AI laws in court, direct agencies to evaluate state laws deemed “onerous,” and push the Federal Communications Commission and Federal Trade Commission toward national standards that override state rules.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Order would also give Sacks direct influence over AI policy, superseding the usual role of the White House Office of Science and Technology Policy, currently headed by Michael Kratsios.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Christmas comes early for AI billionaires who keep getting exactly what they want from The White House: a massive handout that makes it that much easier for them to make massive profits for themselves with exactly zero consideration for the risks to our kids, to our safety, and to our jobs,” New York Assembly member Alex Bores, who sponsored New York’s RAISE Act, said in a statement. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Attempts to block states’ power to regulate AI have been deeply unpopular on both sides of Congress. Earlier this year, Senator Ted Cruz (R-TX) introduced a proposal that would place a 10-year moratorium on AI legislation into the federal budget bill, but it was rejected 99-1, in a rare moment of bipartisan agreement that tech companies shouldn’t operate without oversight.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And when Trump’s draft was leaked last month, several Republican politicians spoke out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rep. Marjorie Taylor Greene (R-GA) posted on X: “States must retain the right to regulate and make laws on AI and anything else for the benefit of their state. Federalism must be preserved.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gov. Ron DeSantis (R-FL) posted late last week: “I oppose stripping Florida of our ability to legislate in the best interest of the people. A ten year AI moratorium bans state regulation of AI, which would prevent FL from enacting important protections for individuals, children and families.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeSantis has also called data centers as drains on power and water resources, as well as potential job killers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The rise of AI is the most significant economic and cultural shift occurring at the moment; denying the people the ability to channel these technologies in a productive way via self-government constitutes federal government overreach and lets technology companies run wild,” he said in a November X post.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Late last week, Sen. Marco Rubio (R-FL) warned Trump against the EO, advising him to “leave AI to the states” to preserve federalism and allow local protections.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The desire to protect people from potential harms of AI technology is not unfounded. There have been several deaths by suicide following prolonged conversations with AI chatbots, and psychologists have recorded an uptick in cases of a condition they’re calling “AI psychosis.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A bipartisan coalition of over 35 state attorneys general warned Congress last month that overriding state AI laws could have “disastrous consequences,” and more than 200 state lawmakers have issued an open letter opposing federal preemption, citing setbacks to progress on AI safety.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with comment from Alex Bores (D-NY).&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/07/rnc-2024-report-v3.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;President Donald Trump said on Monday he plans to ink an executive order this week that would limit states from enacting their own regulation of AI technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I will be doing a ONE RULE Executive Order this week,” Trump posted on social media. “You can’t expect a company to get 50 Approvals every time they want to do something.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There must be only One Rulebook if we are going to continue to lead in AI,” Trump said. “We are beating ALL COUNTRIES at this point in the race, but that won’t last long if we are going to have 50 States, many of them bad actors, involved in RULES and the APPROVAL PROCESS…AI WILL BE DESTROYED IN ITS INFANCY!”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s statement comes days after an effort to preempt states from regulating AI was quashed in the Senate, as Congress couldn’t agree to insert the deeply unpopular proposal into a must-pass defense budget bill.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fast pace of AI development and the lack of general consumer protections from the federal government has led many states to enact their own rules around the technology. California, for example, has the AI safety and transparency bill SB 53, while Tennessee’s ELVIS Act protects musicians and performers from unauthorized AI-generated deepfakes of their voices and likenesses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Silicon Valley figures, including OpenAI President Greg Brockman and VC-turned-White House “AI czar” David Sacks, have argued that such laws by states would create an unworkable patchwork of laws that would stifle innovation and threaten the U.S.’s lead against China in the race to develop AI technology.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Silicon Valley has a mighty lobbying arm that has blocked meaningful technology regulation for years, and proponents of states’ regulatory rights say there’s no reason to believe state AI laws could “destroy AI progress,” as VCs and tech companies claim. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Trump’s executive order, a draft of which was leaked a couple of weeks ago, would create an “AI Litigation Task Force” to challenge state AI laws in court, direct agencies to evaluate state laws deemed “onerous,” and push the Federal Communications Commission and Federal Trade Commission toward national standards that override state rules.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Order would also give Sacks direct influence over AI policy, superseding the usual role of the White House Office of Science and Technology Policy, currently headed by Michael Kratsios.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Christmas comes early for AI billionaires who keep getting exactly what they want from The White House: a massive handout that makes it that much easier for them to make massive profits for themselves with exactly zero consideration for the risks to our kids, to our safety, and to our jobs,” New York Assembly member Alex Bores, who sponsored New York’s RAISE Act, said in a statement. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Attempts to block states’ power to regulate AI have been deeply unpopular on both sides of Congress. Earlier this year, Senator Ted Cruz (R-TX) introduced a proposal that would place a 10-year moratorium on AI legislation into the federal budget bill, but it was rejected 99-1, in a rare moment of bipartisan agreement that tech companies shouldn’t operate without oversight.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And when Trump’s draft was leaked last month, several Republican politicians spoke out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rep. Marjorie Taylor Greene (R-GA) posted on X: “States must retain the right to regulate and make laws on AI and anything else for the benefit of their state. Federalism must be preserved.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gov. Ron DeSantis (R-FL) posted late last week: “I oppose stripping Florida of our ability to legislate in the best interest of the people. A ten year AI moratorium bans state regulation of AI, which would prevent FL from enacting important protections for individuals, children and families.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeSantis has also called data centers as drains on power and water resources, as well as potential job killers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The rise of AI is the most significant economic and cultural shift occurring at the moment; denying the people the ability to channel these technologies in a productive way via self-government constitutes federal government overreach and lets technology companies run wild,” he said in a November X post.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Late last week, Sen. Marco Rubio (R-FL) warned Trump against the EO, advising him to “leave AI to the states” to preserve federalism and allow local protections.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The desire to protect people from potential harms of AI technology is not unfounded. There have been several deaths by suicide following prolonged conversations with AI chatbots, and psychologists have recorded an uptick in cases of a condition they’re calling “AI psychosis.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A bipartisan coalition of over 35 state attorneys general warned Congress last month that overriding state AI laws could have “disastrous consequences,” and more than 200 state lawmakers have issued an open letter opposing federal preemption, citing setbacks to progress on AI safety.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article has been updated with comment from Alex Bores (D-NY).&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/one-rule-trump-says-hell-sign-an-executive-order-blocking-state-ai-laws-despite-bipartisan-pushback/</guid><pubDate>Mon, 08 Dec 2025 16:07:39 +0000</pubDate></item><item><title>Instacart pilots agentic commerce by embedding in ChatGPT (AI News)</title><link>https://www.artificialintelligence-news.com/news/instacart-pilots-agentic-commerce-by-embedding-in-chatgpt/</link><description>&lt;p&gt;Instacart has deployed an embedded checkout experience within ChatGPT through the emerging Agentic Commerce Protocol.&lt;/p&gt;&lt;p&gt;With the deployment, the company is the first partner to launch an app on ChatGPT that offers a complete shopping cycle – from query to payment – without requiring the user to leave the conversation interface.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operationalising-agentic-commerce"&gt;Operationalising agentic commerce&lt;/h3&gt;&lt;p&gt;The integration fixes a broken link in conversational commerce: the “handoff”. Historically, AI models could suggest products or generate meal plans, but the execution phase required deep-linking out to a separate application or website, often resulting in cart abandonment.&lt;/p&gt;&lt;p&gt;Under this new deployment, users can interact with the AI for meal planning and have the system build a cart based on local retailer inventory. The differentiator here is the checkout process. By leveraging the Agentic Commerce Protocol, the transaction is processed directly within the chat interface using a credit card flow powered by Stripe.&lt;/p&gt;&lt;p&gt;According to Nick Turley, VP and Head of ChatGPT, the objective is to connect AI suggestions directly to real-world services.&lt;/p&gt;&lt;p&gt;“With the Instacart app directly in ChatGPT, users can go from meal planning to checkout in a single, seamless conversation,” Turley said. “It’s another step toward bringing our vision to life—where AI delivers helpful suggestions and connects directly to real-world services, saving people time and effort in their everyday lives.”&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Instacart checkout agentic commerce experience embedded in OpenAI's ChatGPT." class="wp-image-111211" height="442" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image.jpeg" width="828" /&gt;&lt;/figure&gt;&lt;p&gt;This integration goes deeper than standard API consumption. Instacart served as an early contributor to the OpenAI Operator research preview, providing feedback to ensure the technology could navigate real-world constraints while adhering to established norms.&lt;/p&gt;&lt;p&gt;This “preview” involvement suggests that Instacart’s complex data environment – involving tens of thousands of SKUs and dynamic stock levels – served as a testing ground for OpenAI’s agentic capabilities. Rather than simply adopting the tool, Instacart helped define the parameters of how an AI agent interacts with external fulfilment logistics.&lt;/p&gt;&lt;p&gt;The Instacart deployment underscores why structured, real-time data matters when integrating with large language models (LLMs). An AI agent is only as effective as the data it can access; hallucinations in a commercial context – such as selling out-of-stock items – carry financial and reputational risk.&lt;/p&gt;&lt;p&gt;Anirban Kundu, CTO at Instacart, notes that powering shopping inside an AI agent requires technology capable of interpreting highly local and constantly fluctuating inventory. Instacart attempts to mitigate the “hallucination” risk by grounding the AI’s responses in its massive dataset, which covers more than 1.8 billion product instances across 100,000 stores.&lt;/p&gt;&lt;p&gt;“Instacart and ChatGPT are redefining what’s possible in AI-powered shopping,” said Kundu. “Built on Agentic Commerce Protocol, this experience brings intelligent, real-time support to one of the most essential parts of daily life: getting groceries to feed your family.&lt;/p&gt;&lt;p&gt;“Together, we’re creating a seamless and secure way for people to turn simple conversations into real-world action—helping customers go from inspiration to a full cart delivered from the store to their door with ease.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-dual-adoption-customer-facing-and-internal-efficiency"&gt;Dual adoption: Customer-facing and internal efficiency&lt;/h3&gt;&lt;p&gt;While the embedded checkout grabs headlines, Instacart’s broader plan involves extensive internal deployment. The company utilises ChatGPT Enterprise to streamline internal workflows, aimed at accelerating the development of customer experiences. Furthermore, they have deployed OpenAI’s Codex to power an internal coding agent.&lt;/p&gt;&lt;p&gt;This dual approach – using AI to sell (Agentic Commerce) and AI to build (Codex) – offers a model for operations. It moves beyond isolated pilots into a holistic stance where generative models drive both revenue and R&amp;amp;D efficiency.&lt;/p&gt;&lt;p&gt;The deployment points to a change in how brands view digital storefronts. Instacart’s approach appears to accept that consumer entry points are fragmenting. Rather than forcing all traffic through a proprietary app, the company is positioning its infrastructure as the backend fulfilment layer for third-party AI platforms.&lt;/p&gt;&lt;p&gt;The company has explicitly stated its intention to bridge AI inspiration with real-world fulfilment, acting as a primary partner for major AI players including OpenAI, Google, and Microsoft. By embedding its service into these broad-reach platforms, Instacart aims to capture incremental demand that originates outside its native ecosystem.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-implementation-and-availability-of-instacart-in-chatgpt"&gt;Implementation and availability of Instacart in ChatGPT&lt;/h3&gt;&lt;p&gt;The experience is currently active for users on desktop and mobile web platforms, while native mobile availability for iOS and Android applications is rolling out shortly.&lt;/p&gt;&lt;p&gt;To access the feature, users must invoke the specific Instacart application within the ChatGPT interface (for example, by prompting “Instacart, can you help me shop for apple pie ingredients?”) and link their accounts. This opt-in mechanism ensures that data sharing is consensual, a requisite governance step for enterprises deploying consumer-facing AI agents.&lt;/p&gt;&lt;p&gt;This integration serves as a case study of agentic AI for commerce. For retail and technology execs, the Instacart model demonstrates that the next phase of digital adoption involves preparing API structures and data pipelines to serve “non-human” customers (AI agents) as reliably as human ones.&lt;/p&gt;&lt;p&gt;The focus must remain on data accuracy and real-time availability; without these foundations, agentic workflows will fail to deliver return on investment.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI: Enterprise users swap AI pilots for deep integrations&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Instacart has deployed an embedded checkout experience within ChatGPT through the emerging Agentic Commerce Protocol.&lt;/p&gt;&lt;p&gt;With the deployment, the company is the first partner to launch an app on ChatGPT that offers a complete shopping cycle – from query to payment – without requiring the user to leave the conversation interface.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-operationalising-agentic-commerce"&gt;Operationalising agentic commerce&lt;/h3&gt;&lt;p&gt;The integration fixes a broken link in conversational commerce: the “handoff”. Historically, AI models could suggest products or generate meal plans, but the execution phase required deep-linking out to a separate application or website, often resulting in cart abandonment.&lt;/p&gt;&lt;p&gt;Under this new deployment, users can interact with the AI for meal planning and have the system build a cart based on local retailer inventory. The differentiator here is the checkout process. By leveraging the Agentic Commerce Protocol, the transaction is processed directly within the chat interface using a credit card flow powered by Stripe.&lt;/p&gt;&lt;p&gt;According to Nick Turley, VP and Head of ChatGPT, the objective is to connect AI suggestions directly to real-world services.&lt;/p&gt;&lt;p&gt;“With the Instacart app directly in ChatGPT, users can go from meal planning to checkout in a single, seamless conversation,” Turley said. “It’s another step toward bringing our vision to life—where AI delivers helpful suggestions and connects directly to real-world services, saving people time and effort in their everyday lives.”&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Instacart checkout agentic commerce experience embedded in OpenAI's ChatGPT." class="wp-image-111211" height="442" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/image.jpeg" width="828" /&gt;&lt;/figure&gt;&lt;p&gt;This integration goes deeper than standard API consumption. Instacart served as an early contributor to the OpenAI Operator research preview, providing feedback to ensure the technology could navigate real-world constraints while adhering to established norms.&lt;/p&gt;&lt;p&gt;This “preview” involvement suggests that Instacart’s complex data environment – involving tens of thousands of SKUs and dynamic stock levels – served as a testing ground for OpenAI’s agentic capabilities. Rather than simply adopting the tool, Instacart helped define the parameters of how an AI agent interacts with external fulfilment logistics.&lt;/p&gt;&lt;p&gt;The Instacart deployment underscores why structured, real-time data matters when integrating with large language models (LLMs). An AI agent is only as effective as the data it can access; hallucinations in a commercial context – such as selling out-of-stock items – carry financial and reputational risk.&lt;/p&gt;&lt;p&gt;Anirban Kundu, CTO at Instacart, notes that powering shopping inside an AI agent requires technology capable of interpreting highly local and constantly fluctuating inventory. Instacart attempts to mitigate the “hallucination” risk by grounding the AI’s responses in its massive dataset, which covers more than 1.8 billion product instances across 100,000 stores.&lt;/p&gt;&lt;p&gt;“Instacart and ChatGPT are redefining what’s possible in AI-powered shopping,” said Kundu. “Built on Agentic Commerce Protocol, this experience brings intelligent, real-time support to one of the most essential parts of daily life: getting groceries to feed your family.&lt;/p&gt;&lt;p&gt;“Together, we’re creating a seamless and secure way for people to turn simple conversations into real-world action—helping customers go from inspiration to a full cart delivered from the store to their door with ease.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-dual-adoption-customer-facing-and-internal-efficiency"&gt;Dual adoption: Customer-facing and internal efficiency&lt;/h3&gt;&lt;p&gt;While the embedded checkout grabs headlines, Instacart’s broader plan involves extensive internal deployment. The company utilises ChatGPT Enterprise to streamline internal workflows, aimed at accelerating the development of customer experiences. Furthermore, they have deployed OpenAI’s Codex to power an internal coding agent.&lt;/p&gt;&lt;p&gt;This dual approach – using AI to sell (Agentic Commerce) and AI to build (Codex) – offers a model for operations. It moves beyond isolated pilots into a holistic stance where generative models drive both revenue and R&amp;amp;D efficiency.&lt;/p&gt;&lt;p&gt;The deployment points to a change in how brands view digital storefronts. Instacart’s approach appears to accept that consumer entry points are fragmenting. Rather than forcing all traffic through a proprietary app, the company is positioning its infrastructure as the backend fulfilment layer for third-party AI platforms.&lt;/p&gt;&lt;p&gt;The company has explicitly stated its intention to bridge AI inspiration with real-world fulfilment, acting as a primary partner for major AI players including OpenAI, Google, and Microsoft. By embedding its service into these broad-reach platforms, Instacart aims to capture incremental demand that originates outside its native ecosystem.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-implementation-and-availability-of-instacart-in-chatgpt"&gt;Implementation and availability of Instacart in ChatGPT&lt;/h3&gt;&lt;p&gt;The experience is currently active for users on desktop and mobile web platforms, while native mobile availability for iOS and Android applications is rolling out shortly.&lt;/p&gt;&lt;p&gt;To access the feature, users must invoke the specific Instacart application within the ChatGPT interface (for example, by prompting “Instacart, can you help me shop for apple pie ingredients?”) and link their accounts. This opt-in mechanism ensures that data sharing is consensual, a requisite governance step for enterprises deploying consumer-facing AI agents.&lt;/p&gt;&lt;p&gt;This integration serves as a case study of agentic AI for commerce. For retail and technology execs, the Instacart model demonstrates that the next phase of digital adoption involves preparing API structures and data pipelines to serve “non-human” customers (AI agents) as reliably as human ones.&lt;/p&gt;&lt;p&gt;The focus must remain on data accuracy and real-time availability; without these foundations, agentic workflows will fail to deliver return on investment.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;OpenAI: Enterprise users swap AI pilots for deep integrations&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/instacart-pilots-agentic-commerce-by-embedding-in-chatgpt/</guid><pubDate>Mon, 08 Dec 2025 16:09:44 +0000</pubDate></item><item><title>The State of AI: A vision of the world in 2030 (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/08/1128922/the-state-of-ai-a-vision-of-the-world-in-2030/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;strong&gt;Welcome back to The State of AI, a new collaboration between the &lt;em&gt;Financial Times&lt;/em&gt; and &lt;em&gt;MIT Technology Review&lt;/em&gt;. Every Monday, writers from both publications debate one aspect of the generative AI revolution reshaping global power.&amp;nbsp;&lt;/strong&gt;You can read the rest of the series here.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;In this final edition, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;’s senior AI editor Will Douglas Heaven talks with Tim Bradshaw, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;FT&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; global tech correspondent, about where AI will go next, and what our world will look like in the next five years.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;(As part of this series, join &lt;em&gt;MIT Technology Review&lt;/em&gt;’s editor in chief, Mat Honan, and editor at large, David Rotman, for an exclusive conversation with &lt;em&gt;Financial Times&lt;/em&gt; columnist Richard Waters on how AI is reshaping the global economy. Live on Tuesday, December 9 at 1:00 p.m. ET. This is a subscriber-only event and you can sign up here.)&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="state of AI" class="wp-image-1128924" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Will Douglas Heaven writes:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Every time I’m asked what’s coming next, I get a Luke Haines song stuck in my head: “Please don’t ask me about the future / I am not a fortune teller.” But here goes. What will things be like in 2030? My answer: same but different.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are huge gulfs of opinion when it comes to predicting the near-future impacts of generative AI. In one camp we have the AI Futures Project, a small donation-funded research outfit led by former OpenAI researcher Daniel Kokotajlo. The nonprofit made a big splash back in April with AI 2027, a speculative account of what the world will look like two years from now.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The story follows the runaway advances of an AI firm called OpenBrain (any similarities are coincidental, etc.) all the way to a choose-your-own-adventure-style boom or doom ending. Kokotajlo and his coauthors make no bones about their expectation that in the next decade the impact of AI will exceed that of the Industrial Revolution—a 150-year period of economic and social upheaval so great that we still live in the world it wrought.&lt;/p&gt;  &lt;p&gt;At the other end of the scale we have team Normal Technology: Arvind Narayanan and Sayash Kapoor, a pair of Princeton University researchers and coauthors of the book &lt;em&gt;AI Snake Oil&lt;/em&gt;, who push back not only on most of AI 2027’s predictions but, more important, on its foundational worldview. That’s not how technology works, they argue.&lt;/p&gt;  &lt;p&gt;Advances at the cutting edge may come thick and fast, but change across the wider economy, and society as a whole, moves at human speed. Widespread adoption of new technologies can be slow; acceptance slower. AI will be no different.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What should we make of these extremes? ChatGPT came out three years ago last month, but it’s still not clear just how good the latest versions of this tech are at replacing lawyers or software developers or (gulp) journalists. And new updates no longer bring the step changes in capability that they once did.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;And yet this radical technology is so new it would be foolish to write it off so soon. Just think: Nobody even knows exactly how this technology works—let alone what it’s really for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As the rate of advance in the core technology slows down, applications of that tech will become the main differentiator between AI firms. (Witness the new browser wars and the chatbot pick-and-mix already on the market.) At the same time, high-end models are becoming cheaper to run and more accessible. Expect this to be where most of the action is: New ways to use existing models will keep them fresh and distract people waiting in line for what comes next.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, progress continues beyond LLMs. (Don’t forget—there was AI before ChatGPT, and there will be AI after it too.) Technologies such as reinforcement learning—the powerhouse behind AlphaGo, DeepMind’s board-game-playing AI that beat a Go grand master in 2016—is set to make a comeback. There’s also a lot of buzz around world models, a type of generative AI with a stronger grip on how the physical world fits together than LLMs display.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, I agree with team Normal Technology that rapid technological advances do not translate to economic or societal ones straight away. There’s just too much messy human stuff in the middle.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But Tim, over to you. I’m curious to hear what your tea leaves are saying.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Tim Bradshaw and Will Douglas Heaven" class="wp-image-1126832" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/FT_TR_Newsletter-Episode-06.jpg?w=2731" /&gt;&lt;div class="image-credit"&gt;FT/MIT TECHNOLOGY REVIEW | ADOBE STOCK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;Tim Bradshaw responds&lt;/strong&gt;"&lt;/p&gt;  &lt;p&gt;Will, I am more confident than you that the world will look quite different in 2030. In five years’ time, I expect the AI revolution to have proceeded apace. But who gets to benefit from those gains will create a world of AI haves and have-nots.&lt;/p&gt;  &lt;p&gt;It seems inevitable that the AI bubble will burst sometime before the end of the decade. Whether a venture capital funding shakeout comes in six months or two years (I feel the current frenzy still has some way to run), swathes of AI app developers will disappear overnight. Some will see their work absorbed by the models upon which they depend. Others will learn the hard way that you can’t sell services that cost $1 for 50 cents without a firehose of VC funding.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;How many of the foundation model companies survive is harder to call, but it already seems clear that OpenAI’s chain of interdependencies within Silicon Valley make it too big to fail. Still, a funding reckoning will force it to ratchet up pricing for its services.&lt;/p&gt;  &lt;p&gt;When OpenAI was created in 2015, it pledged to “advance digital intelligence in the way that is most likely to benefit humanity as a whole.” That seems increasingly untenable. Sooner or later, the investors who bought in at a $500 billion price tag will push for returns. Those data centers won’t pay for themselves. By that point, many companies and individuals will have come to depend on ChatGPT or other AI services for their everyday workflows. Those able to pay will reap the productivity benefits, scooping up the excess computing power as others are priced out of the market.&lt;/p&gt;  &lt;p&gt;Being able to layer several AI services on top of each other will provide a compounding effect. One example I heard on a recent trip to San Francisco: Ironing out the kinks in vibe coding is simply a matter of taking several passes at the same problem and then running a few more AI agents to look for bugs and security issues. That sounds incredibly GPU-intensive, implying that making AI really deliver on the current productivity promise will require customers to pay far more than most do today.&lt;/p&gt;  &lt;p&gt;The same holds true in physical AI. I fully expect robotaxis to be commonplace in every major city by the end of the decade, and I even expect to see humanoid robots in many homes. But while Waymo’s Uber-like prices in San Francisco and the kinds of low-cost robots produced by China’s Unitree give the impression today that these will soon be affordable for all, the compute cost involved in making them useful and ubiquitous seems destined to turn them into luxuries for the well-off, at least in the near term.&lt;/p&gt; 
 &lt;p&gt;The rest of us, meanwhile, will be left with an internet full of slop and unable to afford AI tools that actually work.&lt;/p&gt;  &lt;p&gt;Perhaps some breakthrough in computational efficiency will avert this fate. But the current AI boom means Silicon Valley’s AI companies lack the incentives to make leaner models or experiment with radically different kinds of chips. That only raises the likelihood that the next wave of AI innovation will come from outside the US, be that China, India, or somewhere even farther afield.&lt;/p&gt; 
 &lt;p&gt;Silicon Valley’s AI boom will surely end before 2030, but the race for global influence over the technology’s development—and the political arguments about how its benefits are distributed—seem set to continue well into the next decade.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Will replies:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;I am with you that the cost of this technology is going to lead to a world of haves and have-nots. Even today, $200+ a month buys power users of ChatGPT or Gemini a very different experience from that of people on the free tier. That capability gap is certain to increase as model makers seek to recoup costs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We’re going to see massive global disparities too. In the Global North, adoption has been off the charts. A recent report from Microsoft’s AI Economy Institute notes that AI is the fastest-spreading technology in human history: “In less than three years, more than 1.2 billion people have used AI tools, a rate of adoption faster than the internet, the personal computer, or even the smartphone.” And yet AI is useless without ready access to electricity and the internet; swathes of the world still have neither.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I still remain skeptical that we will see anything like the revolution that many insiders promise (and investors pray for) by 2030. When Microsoft talks about adoption here, it’s counting casual users rather than measuring long-term technological diffusion, which takes time. Meanwhile, casual users get bored and move on.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;How about this: If I live with a domestic robot in five years’ time, you can send your laundry to my house in a robotaxi any day of the week.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;JK! As if I could afford one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What is AI? It sounds like a stupid question, but it’s one that’s never been more urgent. In this deep dive, Will unpacks decades of spin and speculation to get to the heart of our collective technodream.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AGI—the idea that machines will be as smart as humans—has hijacked an entire industry (and possibly the US economy). For &lt;em&gt;MIT Technology Review’s&lt;/em&gt; recent New Conspiracy Age package, Will takes a provocative look at how AGI is like a conspiracy.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The FT examined the economics of self-driving cars this summer, asking who will foot the multi-billion-dollar bill to buy enough robotaxis to serve a big city like London or New York.&lt;br /&gt;A plausible counter-argument to Tim’s thesis on AI inequalities is that freely available open-source (or more accurately, “open weight”) models will keep pulling down prices. The US may want frontier models to be built on US chips but it is already losing the global south to Chinese software.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;strong&gt;Welcome back to The State of AI, a new collaboration between the &lt;em&gt;Financial Times&lt;/em&gt; and &lt;em&gt;MIT Technology Review&lt;/em&gt;. Every Monday, writers from both publications debate one aspect of the generative AI revolution reshaping global power.&amp;nbsp;&lt;/strong&gt;You can read the rest of the series here.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;In this final edition, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;’s senior AI editor Will Douglas Heaven talks with Tim Bradshaw, &lt;/strong&gt;&lt;strong&gt;&lt;em&gt;FT&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt; global tech correspondent, about where AI will go next, and what our world will look like in the next five years.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;(As part of this series, join &lt;em&gt;MIT Technology Review&lt;/em&gt;’s editor in chief, Mat Honan, and editor at large, David Rotman, for an exclusive conversation with &lt;em&gt;Financial Times&lt;/em&gt; columnist Richard Waters on how AI is reshaping the global economy. Live on Tuesday, December 9 at 1:00 p.m. ET. This is a subscriber-only event and you can sign up here.)&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="state of AI" class="wp-image-1128924" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/4c1e48f3-a8d9-fed3-bfeb-c686add0bb5d.png?w=1200" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Will Douglas Heaven writes:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Every time I’m asked what’s coming next, I get a Luke Haines song stuck in my head: “Please don’t ask me about the future / I am not a fortune teller.” But here goes. What will things be like in 2030? My answer: same but different.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are huge gulfs of opinion when it comes to predicting the near-future impacts of generative AI. In one camp we have the AI Futures Project, a small donation-funded research outfit led by former OpenAI researcher Daniel Kokotajlo. The nonprofit made a big splash back in April with AI 2027, a speculative account of what the world will look like two years from now.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The story follows the runaway advances of an AI firm called OpenBrain (any similarities are coincidental, etc.) all the way to a choose-your-own-adventure-style boom or doom ending. Kokotajlo and his coauthors make no bones about their expectation that in the next decade the impact of AI will exceed that of the Industrial Revolution—a 150-year period of economic and social upheaval so great that we still live in the world it wrought.&lt;/p&gt;  &lt;p&gt;At the other end of the scale we have team Normal Technology: Arvind Narayanan and Sayash Kapoor, a pair of Princeton University researchers and coauthors of the book &lt;em&gt;AI Snake Oil&lt;/em&gt;, who push back not only on most of AI 2027’s predictions but, more important, on its foundational worldview. That’s not how technology works, they argue.&lt;/p&gt;  &lt;p&gt;Advances at the cutting edge may come thick and fast, but change across the wider economy, and society as a whole, moves at human speed. Widespread adoption of new technologies can be slow; acceptance slower. AI will be no different.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What should we make of these extremes? ChatGPT came out three years ago last month, but it’s still not clear just how good the latest versions of this tech are at replacing lawyers or software developers or (gulp) journalists. And new updates no longer bring the step changes in capability that they once did.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;And yet this radical technology is so new it would be foolish to write it off so soon. Just think: Nobody even knows exactly how this technology works—let alone what it’s really for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As the rate of advance in the core technology slows down, applications of that tech will become the main differentiator between AI firms. (Witness the new browser wars and the chatbot pick-and-mix already on the market.) At the same time, high-end models are becoming cheaper to run and more accessible. Expect this to be where most of the action is: New ways to use existing models will keep them fresh and distract people waiting in line for what comes next.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, progress continues beyond LLMs. (Don’t forget—there was AI before ChatGPT, and there will be AI after it too.) Technologies such as reinforcement learning—the powerhouse behind AlphaGo, DeepMind’s board-game-playing AI that beat a Go grand master in 2016—is set to make a comeback. There’s also a lot of buzz around world models, a type of generative AI with a stronger grip on how the physical world fits together than LLMs display.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Ultimately, I agree with team Normal Technology that rapid technological advances do not translate to economic or societal ones straight away. There’s just too much messy human stuff in the middle.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But Tim, over to you. I’m curious to hear what your tea leaves are saying.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Tim Bradshaw and Will Douglas Heaven" class="wp-image-1126832" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/FT_TR_Newsletter-Episode-06.jpg?w=2731" /&gt;&lt;div class="image-credit"&gt;FT/MIT TECHNOLOGY REVIEW | ADOBE STOCK&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;Tim Bradshaw responds&lt;/strong&gt;"&lt;/p&gt;  &lt;p&gt;Will, I am more confident than you that the world will look quite different in 2030. In five years’ time, I expect the AI revolution to have proceeded apace. But who gets to benefit from those gains will create a world of AI haves and have-nots.&lt;/p&gt;  &lt;p&gt;It seems inevitable that the AI bubble will burst sometime before the end of the decade. Whether a venture capital funding shakeout comes in six months or two years (I feel the current frenzy still has some way to run), swathes of AI app developers will disappear overnight. Some will see their work absorbed by the models upon which they depend. Others will learn the hard way that you can’t sell services that cost $1 for 50 cents without a firehose of VC funding.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;How many of the foundation model companies survive is harder to call, but it already seems clear that OpenAI’s chain of interdependencies within Silicon Valley make it too big to fail. Still, a funding reckoning will force it to ratchet up pricing for its services.&lt;/p&gt;  &lt;p&gt;When OpenAI was created in 2015, it pledged to “advance digital intelligence in the way that is most likely to benefit humanity as a whole.” That seems increasingly untenable. Sooner or later, the investors who bought in at a $500 billion price tag will push for returns. Those data centers won’t pay for themselves. By that point, many companies and individuals will have come to depend on ChatGPT or other AI services for their everyday workflows. Those able to pay will reap the productivity benefits, scooping up the excess computing power as others are priced out of the market.&lt;/p&gt;  &lt;p&gt;Being able to layer several AI services on top of each other will provide a compounding effect. One example I heard on a recent trip to San Francisco: Ironing out the kinks in vibe coding is simply a matter of taking several passes at the same problem and then running a few more AI agents to look for bugs and security issues. That sounds incredibly GPU-intensive, implying that making AI really deliver on the current productivity promise will require customers to pay far more than most do today.&lt;/p&gt;  &lt;p&gt;The same holds true in physical AI. I fully expect robotaxis to be commonplace in every major city by the end of the decade, and I even expect to see humanoid robots in many homes. But while Waymo’s Uber-like prices in San Francisco and the kinds of low-cost robots produced by China’s Unitree give the impression today that these will soon be affordable for all, the compute cost involved in making them useful and ubiquitous seems destined to turn them into luxuries for the well-off, at least in the near term.&lt;/p&gt; 
 &lt;p&gt;The rest of us, meanwhile, will be left with an internet full of slop and unable to afford AI tools that actually work.&lt;/p&gt;  &lt;p&gt;Perhaps some breakthrough in computational efficiency will avert this fate. But the current AI boom means Silicon Valley’s AI companies lack the incentives to make leaner models or experiment with radically different kinds of chips. That only raises the likelihood that the next wave of AI innovation will come from outside the US, be that China, India, or somewhere even farther afield.&lt;/p&gt; 
 &lt;p&gt;Silicon Valley’s AI boom will surely end before 2030, but the race for global influence over the technology’s development—and the political arguments about how its benefits are distributed—seem set to continue well into the next decade.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Will replies:&amp;nbsp;&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;I am with you that the cost of this technology is going to lead to a world of haves and have-nots. Even today, $200+ a month buys power users of ChatGPT or Gemini a very different experience from that of people on the free tier. That capability gap is certain to increase as model makers seek to recoup costs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We’re going to see massive global disparities too. In the Global North, adoption has been off the charts. A recent report from Microsoft’s AI Economy Institute notes that AI is the fastest-spreading technology in human history: “In less than three years, more than 1.2 billion people have used AI tools, a rate of adoption faster than the internet, the personal computer, or even the smartphone.” And yet AI is useless without ready access to electricity and the internet; swathes of the world still have neither.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I still remain skeptical that we will see anything like the revolution that many insiders promise (and investors pray for) by 2030. When Microsoft talks about adoption here, it’s counting casual users rather than measuring long-term technological diffusion, which takes time. Meanwhile, casual users get bored and move on.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;How about this: If I live with a domestic robot in five years’ time, you can send your laundry to my house in a robotaxi any day of the week.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;JK! As if I could afford one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;What is AI? It sounds like a stupid question, but it’s one that’s never been more urgent. In this deep dive, Will unpacks decades of spin and speculation to get to the heart of our collective technodream.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AGI—the idea that machines will be as smart as humans—has hijacked an entire industry (and possibly the US economy). For &lt;em&gt;MIT Technology Review’s&lt;/em&gt; recent New Conspiracy Age package, Will takes a provocative look at how AGI is like a conspiracy.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The FT examined the economics of self-driving cars this summer, asking who will foot the multi-billion-dollar bill to buy enough robotaxis to serve a big city like London or New York.&lt;br /&gt;A plausible counter-argument to Tim’s thesis on AI inequalities is that freely available open-source (or more accurately, “open weight”) models will keep pulling down prices. The US may want frontier models to be built on US chips but it is already losing the global south to Chinese software.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/08/1128922/the-state-of-ai-a-vision-of-the-world-in-2030/</guid><pubDate>Mon, 08 Dec 2025 16:30:00 +0000</pubDate></item><item><title>You can buy your Instacart groceries without leaving ChatGPT (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/you-can-buy-your-instacart-groceries-without-leaving-chatgpt/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/OpenAI_Instacart_Partnership_Blog_16-9.webp?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI and Instacart are launching a grocery shopping experience inside of ChatGPT, allowing customers to brainstorm meal ideas, make a grocery list, and check out, all without leaving the chat interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This builds upon an existing partnership between OpenAI and Instacart; more than two years ago, Instacart launched an in-app AI search tool powered by ChatGPT, which helps shoppers ask questions about what to make for dinner or how to accommodate dietary restrictions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The relationship between OpenAI and Instacart seems to have only deepened after former Instacart CEO Fidji Simo — who was already an OpenAI board member — joined the company as the CEO of Applications in May.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agentic commerce — the use of AI tools to do shopping research and make purchases on a user’s behalf — is a current priority for OpenAI. Its most recent dev day focused on its plan to build apps into ChatGPT. In an early preview for developers, ChatGPT launched integrations with apps like Booking.com, Canva, Coursera, Expedia, Figma, Spotify, and Zillow; since then, OpenAI has announced further partnerships with Target, Intuit, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Leading up to this year’s holiday shopping season, both OpenAI and Perplexity announced in-app features that help users make decisions about what products to buy — so, you could ask ChatGPT to help you find the best deal on a gaming laptop that matches your specific criteria. Adobe predicted that&amp;nbsp;AI-assisted online shopping&amp;nbsp;will grow by 520% this holiday season.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite ChatGPT’s immense popularity, OpenAI is not making a profit, and it may not for another several years — if it ever does. Its products are so resource-intensive that even subscription costs don’t account for how much compute power the company uses to make its product work. These agentic commerce tools could give OpenAI another way of making money, since it’ll take an undisclosed “small fee” when it helps merchants make a sale. But it would take a whole lot of ChatGPT-based shopping for these fees to make a dent in OpenAI’s debt.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/OpenAI_Instacart_Partnership_Blog_16-9.webp?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI and Instacart are launching a grocery shopping experience inside of ChatGPT, allowing customers to brainstorm meal ideas, make a grocery list, and check out, all without leaving the chat interface. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This builds upon an existing partnership between OpenAI and Instacart; more than two years ago, Instacart launched an in-app AI search tool powered by ChatGPT, which helps shoppers ask questions about what to make for dinner or how to accommodate dietary restrictions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The relationship between OpenAI and Instacart seems to have only deepened after former Instacart CEO Fidji Simo — who was already an OpenAI board member — joined the company as the CEO of Applications in May.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agentic commerce — the use of AI tools to do shopping research and make purchases on a user’s behalf — is a current priority for OpenAI. Its most recent dev day focused on its plan to build apps into ChatGPT. In an early preview for developers, ChatGPT launched integrations with apps like Booking.com, Canva, Coursera, Expedia, Figma, Spotify, and Zillow; since then, OpenAI has announced further partnerships with Target, Intuit, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Leading up to this year’s holiday shopping season, both OpenAI and Perplexity announced in-app features that help users make decisions about what products to buy — so, you could ask ChatGPT to help you find the best deal on a gaming laptop that matches your specific criteria. Adobe predicted that&amp;nbsp;AI-assisted online shopping&amp;nbsp;will grow by 520% this holiday season.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite ChatGPT’s immense popularity, OpenAI is not making a profit, and it may not for another several years — if it ever does. Its products are so resource-intensive that even subscription costs don’t account for how much compute power the company uses to make its product work. These agentic commerce tools could give OpenAI another way of making money, since it’ll take an undisclosed “small fee” when it helps merchants make a sale. But it would take a whole lot of ChatGPT-based shopping for these fees to make a dent in OpenAI’s debt.&lt;/p&gt;


&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/you-can-buy-your-instacart-groceries-without-leaving-chatgpt/</guid><pubDate>Mon, 08 Dec 2025 17:59:53 +0000</pubDate></item><item><title>Google details security measures for Chrome’s agentic features (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/google-details-security-measures-for-chromes-agentic-features/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An increasing number of browsers are experimenting with agentic features that will take actions on your behalf, such as booking tickets or shopping for different items. However, these agentic capabilities also come with security risks that could lead to loss of data or money. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google detailed its approach to handling user security on Chrome using observer models and consent for user action. The company previewed agentic capabilities on Chrome in September and said these features will roll out in the coming months.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company said it is using the help of a few models to keep agentic actions in check. Google said it built a User Alignment Critic using Gemini to scrutinize the action items built by the planner model for a particular task. If the critic model thinks that the planned tasks don’t serve the user’s goal, it asks the planner model to rethink the strategy. Google noted that the critic model only sees the metadata of the proposed action and not the actual web content.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A pair of screenshots showing what Chrome's agent model can see from a website." class="wp-image-3073681" height="367" src="https://techcrunch.com/wp-content/uploads/2025/12/image-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;What’s more, to prevent agents from accessing disallowed or untrustworthy sites, Google is using Agent Origin Sets, which restrict the model to access read-only origins and read-writeable origins. Read-only origin is data that Gemini is permitted to consume content from. For instance, on a shopping site, the listings are relevant to the task, but banner ads aren’t. Similarly, Google said the agent is only allowed to click or type on certain iframes of a page.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This delineation enforces that only data from a limited set of origins is available to the agent, and this data can only be passed on to the writable origins. This bounds the threat vector of cross-origin data leaks. This also gives the browser the ability to enforce some of that separation, such as by not even sending to the model data that is outside the readable set,” the company said in a blog post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also keeping a check on page navigation by investigating URLs through another observer model. This can prevent navigation to harmful model-generated URLs, the company said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="A screenshot showing Chrome's agent model asking user permission before paying for an item while shopping." class="wp-image-3073680" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/image-2.jpeg?w=657" width="657" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The search giant said that it is also handing over the reins to users for sensitive tasks. For instance, when an agent tries to navigate to a sensitive site with information like banking or your medical data, it first asks the user. For sites that require sign-in, it’ll ask the user for permission to let Chrome use the password manager. Google said that the agent’s model doesn’t have exposure to password data. The company added that it will ask users before taking actions like making a purchase or sending a message.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google said that, in addition to this, it also has a prompt-injection classifier to prevent unwanted actions and is also testing agentic capabilities against attacks created by researchers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI browser makers are also paying attention to security. Earlier this month, Perplexity released a new open source content detection model to prevent prompt injection attacks against agents.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An increasing number of browsers are experimenting with agentic features that will take actions on your behalf, such as booking tickets or shopping for different items. However, these agentic capabilities also come with security risks that could lead to loss of data or money. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google detailed its approach to handling user security on Chrome using observer models and consent for user action. The company previewed agentic capabilities on Chrome in September and said these features will roll out in the coming months.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company said it is using the help of a few models to keep agentic actions in check. Google said it built a User Alignment Critic using Gemini to scrutinize the action items built by the planner model for a particular task. If the critic model thinks that the planned tasks don’t serve the user’s goal, it asks the planner model to rethink the strategy. Google noted that the critic model only sees the metadata of the proposed action and not the actual web content.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A pair of screenshots showing what Chrome's agent model can see from a website." class="wp-image-3073681" height="367" src="https://techcrunch.com/wp-content/uploads/2025/12/image-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;What’s more, to prevent agents from accessing disallowed or untrustworthy sites, Google is using Agent Origin Sets, which restrict the model to access read-only origins and read-writeable origins. Read-only origin is data that Gemini is permitted to consume content from. For instance, on a shopping site, the listings are relevant to the task, but banner ads aren’t. Similarly, Google said the agent is only allowed to click or type on certain iframes of a page.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This delineation enforces that only data from a limited set of origins is available to the agent, and this data can only be passed on to the writable origins. This bounds the threat vector of cross-origin data leaks. This also gives the browser the ability to enforce some of that separation, such as by not even sending to the model data that is outside the readable set,” the company said in a blog post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also keeping a check on page navigation by investigating URLs through another observer model. This can prevent navigation to harmful model-generated URLs, the company said.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="A screenshot showing Chrome's agent model asking user permission before paying for an item while shopping." class="wp-image-3073680" height="680" src="https://techcrunch.com/wp-content/uploads/2025/12/image-2.jpeg?w=657" width="657" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The search giant said that it is also handing over the reins to users for sensitive tasks. For instance, when an agent tries to navigate to a sensitive site with information like banking or your medical data, it first asks the user. For sites that require sign-in, it’ll ask the user for permission to let Chrome use the password manager. Google said that the agent’s model doesn’t have exposure to password data. The company added that it will ask users before taking actions like making a purchase or sending a message.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Google said that, in addition to this, it also has a prompt-injection classifier to prevent unwanted actions and is also testing agentic capabilities against attacks created by researchers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI browser makers are also paying attention to security. Earlier this month, Perplexity released a new open source content detection model to prevent prompt injection attacks against agents.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/google-details-security-measures-for-chromes-agentic-features/</guid><pubDate>Mon, 08 Dec 2025 18:00:00 +0000</pubDate></item><item><title>[NEW] Anthropic's Claude Code can now read your Slack messages and write code for you (AI | VentureBeat)</title><link>https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; on Monday launched a beta integration that connects its fast-growing &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; programming agent &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;directly to Slack&lt;/u&gt;&lt;/a&gt;, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.&lt;/p&gt;&lt;p&gt;The release, which Anthropic describes as a &amp;quot;&lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;research preview&lt;/u&gt;&lt;/a&gt;,&amp;quot; is the AI safety company&amp;#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating over $1 billion in annualized revenue just six months after its public debut in May.&lt;/p&gt;&lt;p&gt;&amp;quot;The critical context around engineering work often lives in Slack, including bug reports, feature requests, and engineering discussion,&amp;quot; the company wrote in its &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;announcement blog post&lt;/u&gt;&lt;/a&gt;. &amp;quot;When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From bug report to pull request: how the new Slack integration actually works&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The mechanics are deceptively simple but address a persistent friction point in software development: the gap between where problems get discussed and where they get fixed.&lt;/p&gt;&lt;p&gt;When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; session. Users can also explicitly instruct Claude to treat requests as coding tasks.&lt;/p&gt;&lt;p&gt;Claude gathers context from recent channel and thread messages in Slack to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories you&amp;#x27;ve authenticated to Claude Code on the web.&lt;/p&gt;&lt;p&gt;As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.&lt;/p&gt;&lt;p&gt;The feature builds on Anthropic&amp;#x27;s existing &lt;a href="https://www.claude.com/claude-and-slack"&gt;&lt;u&gt;Claude for Slack&lt;/u&gt;&lt;/a&gt; integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix, and post a pull request—all while updating the original Slack thread with its progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Anthropic is betting big on enterprise workflow integrations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit &lt;a href="https://www.linkedin.com/posts/mikekrieger_anthropic-acquires-bun-as-claude-code-reaches-activity-7401686833233076224-n3TI/"&gt;&lt;u&gt;$1 billion in revenue six months&lt;/u&gt;&lt;/a&gt; since its public debut in May, according to a LinkedIn post from Anthropic&amp;#x27;s chief product officer, Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify, and Salesforce.&lt;/p&gt;&lt;p&gt;The velocity of that growth helps explain why Anthropic made its &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;first-ever acquisition&lt;/u&gt;&lt;/a&gt; earlier this month. Anthropic declined to comment on financial details. The Information earlier reported on &lt;a href="https://www.theinformation.com/articles/anthropic-advanced-talks-buy-developer-tool-startup-first-acquisition"&gt;&lt;u&gt;Anthropic&amp;#x27;s bid to acquire Bun&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; is a breakthrough JavaScript runtime that is dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler, and test runner — it&amp;#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.&lt;/p&gt;&lt;p&gt;Since becoming generally available in May 2025, &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; has grown from its origins as an internal engineering experiment into a critical tool for many of the world&amp;#x27;s category-leading enterprises, including &lt;a href="https://www.netflix.com/"&gt;&lt;u&gt;Netflix&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://open.spotify.com/"&gt;&lt;u&gt;Spotify&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://kpmg.com/us/en.html"&gt;&lt;u&gt;KPMG&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.loreal.com/en/"&gt;&lt;u&gt;L&amp;#x27;Oreal&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.salesforce.com/"&gt;&lt;u&gt;Salesforce&lt;/u&gt;&lt;/a&gt; — and &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; has been key in helping scale its infrastructure throughout that evolution.&lt;/p&gt;&lt;p&gt;The acquisition signals that Anthropic views &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.&lt;/p&gt;&lt;p&gt;According to an Anthropic spokesperson, companies including &lt;a href="https://www.rakuten.com/"&gt;&lt;u&gt;Rakuten&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.novonordisk.com/"&gt;&lt;u&gt;Novo Nordisk&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.uber.com/"&gt;&lt;u&gt;Uber&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snowflake.com/en/"&gt;&lt;u&gt;Snowflake&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://ramp.com/business-cards?utm_campaign_id=15309675834&amp;amp;utm_ad_group_id=134786044053&amp;amp;utm_ad_id=737002215776&amp;amp;utm_matchtype=e&amp;amp;utm_term=ramp&amp;amp;utm_campaign=google-search-branded&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;hsa_acc=5740001625&amp;amp;hsa_cam=15309675834&amp;amp;hsa_grp=134786044053&amp;amp;hsa_ad=737002215776&amp;amp;hsa_src=g&amp;amp;hsa_tgt=aud-2197767270940:kwd-27557331&amp;amp;hsa_kw=ramp&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;cq_cmp=15309675834&amp;amp;cq_con=134786044053&amp;amp;cq_plac=&amp;amp;cq_net=g&amp;amp;cq_plt=gp&amp;amp;gad_source=1&amp;amp;gad_campaignid=15309675834&amp;amp;gbraid=0AAAAACjAjKPV4x9hvhWDm_aa7C0JiphLP&amp;amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDts_kz5iHuPDKNlcI4ZU2sHwiscemYdv3EbJXX7uKXC2wnh2yc5C1QAaAi2WEALw_wcB"&gt;&lt;u&gt;Ramp&lt;/u&gt;&lt;/a&gt; now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 days to just 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Claude Code&amp;#x27;s rapid rise from internal experiment to billion-dollar product&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack launch is the latest in a rapid series of Claude Code expansions. In late November, &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Claude Code was added to Anthropic&amp;#x27;s desktop apps&lt;/u&gt;&lt;/a&gt; including the Mac version. Claude Code was previously limited to mobile apps and the web. It allows software engineers to code, research, and update work with multiple local and remote sessions running at the same time.&lt;/p&gt;&lt;p&gt;That release accompanied Anthropic&amp;#x27;s unveiling of Claude &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Opus 4.5&lt;/u&gt;&lt;/a&gt;, its newest and most capable model. Claude Opus 4.5 is available today on the company&amp;#x27;s apps, API, and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, &lt;a href="https://www.anthropic.com/engineering/advanced-tool-use"&gt;&lt;u&gt;Anthropic released three new beta features for tool use&lt;/u&gt;&lt;/a&gt;: Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model&amp;#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;&lt;u&gt;Model Context Protocol (MCP)&lt;/u&gt;&lt;/a&gt; is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent and it unlocks an entire ecosystem of integrations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s own AI transformation: what happens when engineers use Claude all day&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic&lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt; surveyed 132 engineers and researchers&lt;/u&gt;&lt;/a&gt;, conducted 53 in-depth qualitative interviews, and studied internal Claude Code usage data to understand how AI use is changing work at the company.&lt;/p&gt;&lt;p&gt;Employees self-reported &lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt;using Claude in 60% of their work&lt;/u&gt;&lt;/a&gt; and achieving a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.&lt;/p&gt;&lt;p&gt;Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&amp;#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&amp;#x27;t be cost-effective if done manually.&lt;/p&gt;&lt;p&gt;The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without need for human intervention versus 9.8 tool calls from six months ago.&lt;/p&gt;&lt;p&gt;The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now compared to six months ago.&lt;/p&gt;&lt;p&gt;But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. &amp;quot;It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial and I go and talk to them,&amp;quot; one engineer explained. Several engineers said they &amp;quot;bounce ideas off&amp;quot; Claude, similar to interactions with human collaborators.&lt;/p&gt;&lt;p&gt;Others described experiencing less interaction with colleagues. Some appreciate the reduced social friction, but others resist the change or miss the older way of working: &amp;quot;I like working with people and it is sad that I &amp;#x27;need&amp;#x27; them less now.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Anthropic stacks up against OpenAI, Google, and Microsoft in the enterprise AI race&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic is not alone in racing to capture the enterprise coding market. &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; (through &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt;) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely-used enterprise communication platforms — Slack claims over 750,000 organizations use its software.&lt;/p&gt;&lt;p&gt;The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show &lt;a href="https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?gaa_at=eafs&amp;amp;gaa_n=AWEtsqf_yQrKzPR_c7Gjwi4FvVkM-Uc0I6k_V6Kev-0xg4RiUud_8rUKLIBVacKJN5E%3D&amp;amp;gaa_ts=693721bf&amp;amp;gaa_sig=m9SvmJAAB8gRVI39wbvEYOLFa9vd_lQbtYoQE1ROCKxaf6CXTOxXgKn4HmpIwymS0BUt58auzZusgCjq74SM2w%3D%3D"&gt;&lt;u&gt;Anthropic expects to break even by 2028&lt;/u&gt;&lt;/a&gt; — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware, and consumer products.&lt;/p&gt;&lt;p&gt;The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&amp;#x27;s biggest titans. Microsoft and Nvidia pledged up to &lt;a href="https://www.cnbc.com/2025/12/03/anthropic-claude-reportedly-preparing-ipo-race-openai-chatgpt-ft-wilson-sonsini-goodrich-rosati.html"&gt;&lt;u&gt;$15 billion in fresh investment&lt;/u&gt;&lt;/a&gt; in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&amp;#x27;s cloud. This is in addition to the &lt;a href="https://www.anthropic.com/news/anthropic-amazon-trainium"&gt;&lt;u&gt;$8 billion invested from Amazon &lt;/u&gt;&lt;/a&gt;and &lt;a href="https://www.cnbc.com/2025/12/04/google-replit-ai-vibe-coding-anthropic-cursor.html"&gt;&lt;u&gt;$3 billion from Google&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The cross-investment from both &lt;a href="https://microsoft.com/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; — fierce competitors in the cloud and AI spaces — highlights how valuable Anthropic&amp;#x27;s enterprise positioning has become. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What the Slack integration means for developers — and whether they can trust it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For engineering teams, the Slack integration promises to collapse the distance between problem identification and problem resolution. A bug report in a Slack channel can immediately trigger investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.&lt;/p&gt;&lt;p&gt;But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can &amp;quot;fully delegate&amp;quot; only 0-20% of their work to it. Claude is a constant collaborator but using it generally involves active supervision and validation, especially in high-stakes work — versus handing off tasks requiring no verification at all.&lt;/p&gt;&lt;p&gt;Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — &amp;quot;When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.&amp;quot;&lt;/p&gt;&lt;p&gt;The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&amp;#x27;s own research has documented.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The future of coding may be conversational—and Anthropic is racing to prove it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.&lt;/p&gt;&lt;p&gt;For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — that the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.&lt;/p&gt;&lt;p&gt;Whether that vision becomes reality will depend on whether &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: a billion dollars in revenue, a roster of Fortune 500 customers, and a growing ecosystem of integrations suggest Anthropic is onto something real.&lt;/p&gt;&lt;p&gt;But in one of Anthropic&amp;#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: &amp;quot;Nobody knows what&amp;#x27;s going to happen… the important thing is to just be really adaptable.&amp;quot;&lt;/p&gt;&lt;p&gt;In the age of AI coding agents, that may be the only career advice that holds up.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; on Monday launched a beta integration that connects its fast-growing &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; programming agent &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;directly to Slack&lt;/u&gt;&lt;/a&gt;, allowing software engineers to delegate coding tasks without leaving the workplace messaging platform where much of their daily communication already happens.&lt;/p&gt;&lt;p&gt;The release, which Anthropic describes as a &amp;quot;&lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;research preview&lt;/u&gt;&lt;/a&gt;,&amp;quot; is the AI safety company&amp;#x27;s latest move to embed its technology deeper into enterprise workflows — and comes as Claude Code has emerged as a surprise revenue engine, generating over $1 billion in annualized revenue just six months after its public debut in May.&lt;/p&gt;&lt;p&gt;&amp;quot;The critical context around engineering work often lives in Slack, including bug reports, feature requests, and engineering discussion,&amp;quot; the company wrote in its &lt;a href="https://www.anthropic.com/news/claude-code-and-slack"&gt;&lt;u&gt;announcement blog post&lt;/u&gt;&lt;/a&gt;. &amp;quot;When a bug report appears or a teammate needs a code fix, you can now tag Claude in Slack to automatically spin up a Claude Code session using the surrounding context.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From bug report to pull request: how the new Slack integration actually works&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The mechanics are deceptively simple but address a persistent friction point in software development: the gap between where problems get discussed and where they get fixed.&lt;/p&gt;&lt;p&gt;When a user mentions @Claude in a Slack channel or thread, Claude analyzes the message to determine whether it constitutes a coding task. If it does, the system automatically creates a new &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; session. Users can also explicitly instruct Claude to treat requests as coding tasks.&lt;/p&gt;&lt;p&gt;Claude gathers context from recent channel and thread messages in Slack to feed into the Claude Code session. It will use this context to automatically choose which repository to run the task on based on the repositories you&amp;#x27;ve authenticated to Claude Code on the web.&lt;/p&gt;&lt;p&gt;As the Claude Code session progresses, Claude posts status updates back to the Slack thread. Once complete, users receive a link to the full session where they can review changes, along with a direct link to open a pull request.&lt;/p&gt;&lt;p&gt;The feature builds on Anthropic&amp;#x27;s existing &lt;a href="https://www.claude.com/claude-and-slack"&gt;&lt;u&gt;Claude for Slack&lt;/u&gt;&lt;/a&gt; integration and requires users to have access to Claude Code on the web. In practical terms, a product manager reporting a bug in Slack could tag Claude, which would then analyze the conversation context, identify the relevant code repository, investigate the issue, propose a fix, and post a pull request—all while updating the original Slack thread with its progress.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why Anthropic is betting big on enterprise workflow integrations&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack integration arrives at a pivotal moment for Anthropic. Claude Code has already hit &lt;a href="https://www.linkedin.com/posts/mikekrieger_anthropic-acquires-bun-as-claude-code-reaches-activity-7401686833233076224-n3TI/"&gt;&lt;u&gt;$1 billion in revenue six months&lt;/u&gt;&lt;/a&gt; since its public debut in May, according to a LinkedIn post from Anthropic&amp;#x27;s chief product officer, Mike Krieger. The coding agent continues to barrel toward scale with customers like Netflix, Spotify, and Salesforce.&lt;/p&gt;&lt;p&gt;The velocity of that growth helps explain why Anthropic made its &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;first-ever acquisition&lt;/u&gt;&lt;/a&gt; earlier this month. Anthropic declined to comment on financial details. The Information earlier reported on &lt;a href="https://www.theinformation.com/articles/anthropic-advanced-talks-buy-developer-tool-startup-first-acquisition"&gt;&lt;u&gt;Anthropic&amp;#x27;s bid to acquire Bun&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; is a breakthrough JavaScript runtime that is dramatically faster than the leading competition. As an all-in-one toolkit — combining runtime, package manager, bundler, and test runner — it&amp;#x27;s become essential infrastructure for AI-led software engineering, helping developers build and test applications at unprecedented velocity.&lt;/p&gt;&lt;p&gt;Since becoming generally available in May 2025, &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; has grown from its origins as an internal engineering experiment into a critical tool for many of the world&amp;#x27;s category-leading enterprises, including &lt;a href="https://www.netflix.com/"&gt;&lt;u&gt;Netflix&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://open.spotify.com/"&gt;&lt;u&gt;Spotify&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://kpmg.com/us/en.html"&gt;&lt;u&gt;KPMG&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.loreal.com/en/"&gt;&lt;u&gt;L&amp;#x27;Oreal&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.salesforce.com/"&gt;&lt;u&gt;Salesforce&lt;/u&gt;&lt;/a&gt; — and &lt;a href="https://bun.com/blog/bun-joins-anthropic"&gt;&lt;u&gt;Bun&lt;/u&gt;&lt;/a&gt; has been key in helping scale its infrastructure throughout that evolution.&lt;/p&gt;&lt;p&gt;The acquisition signals that Anthropic views &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; not as a peripheral feature but as a core business line worth substantial investment. The Slack integration extends that bet, positioning Claude Code as an ambient presence in the workspaces where engineering decisions actually get made.&lt;/p&gt;&lt;p&gt;According to an Anthropic spokesperson, companies including &lt;a href="https://www.rakuten.com/"&gt;&lt;u&gt;Rakuten&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.novonordisk.com/"&gt;&lt;u&gt;Novo Nordisk&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.uber.com/"&gt;&lt;u&gt;Uber&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.snowflake.com/en/"&gt;&lt;u&gt;Snowflake&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://ramp.com/business-cards?utm_campaign_id=15309675834&amp;amp;utm_ad_group_id=134786044053&amp;amp;utm_ad_id=737002215776&amp;amp;utm_matchtype=e&amp;amp;utm_term=ramp&amp;amp;utm_campaign=google-search-branded&amp;amp;utm_source=google&amp;amp;utm_medium=cpc&amp;amp;hsa_acc=5740001625&amp;amp;hsa_cam=15309675834&amp;amp;hsa_grp=134786044053&amp;amp;hsa_ad=737002215776&amp;amp;hsa_src=g&amp;amp;hsa_tgt=aud-2197767270940:kwd-27557331&amp;amp;hsa_kw=ramp&amp;amp;hsa_mt=e&amp;amp;hsa_net=adwords&amp;amp;hsa_ver=3&amp;amp;cq_cmp=15309675834&amp;amp;cq_con=134786044053&amp;amp;cq_plac=&amp;amp;cq_net=g&amp;amp;cq_plt=gp&amp;amp;gad_source=1&amp;amp;gad_campaignid=15309675834&amp;amp;gbraid=0AAAAACjAjKPV4x9hvhWDm_aa7C0JiphLP&amp;amp;gclid=Cj0KCQiAi9rJBhCYARIsALyPDts_kz5iHuPDKNlcI4ZU2sHwiscemYdv3EbJXX7uKXC2wnh2yc5C1QAaAi2WEALw_wcB"&gt;&lt;u&gt;Ramp&lt;/u&gt;&lt;/a&gt; now use Claude Code for both professional and novice developers. Rakuten, the Japanese e-commerce giant, has reportedly reduced software development timelines from 24 days to just 5 days using the tool — a 79% reduction that illustrates the productivity claims Anthropic has been making.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Claude Code&amp;#x27;s rapid rise from internal experiment to billion-dollar product&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The Slack launch is the latest in a rapid series of Claude Code expansions. In late November, &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Claude Code was added to Anthropic&amp;#x27;s desktop apps&lt;/u&gt;&lt;/a&gt; including the Mac version. Claude Code was previously limited to mobile apps and the web. It allows software engineers to code, research, and update work with multiple local and remote sessions running at the same time.&lt;/p&gt;&lt;p&gt;That release accompanied Anthropic&amp;#x27;s unveiling of Claude &lt;a href="https://www.anthropic.com/news/claude-opus-4-5"&gt;&lt;u&gt;Opus 4.5&lt;/u&gt;&lt;/a&gt;, its newest and most capable model. Claude Opus 4.5 is available today on the company&amp;#x27;s apps, API, and on all three major cloud platforms. Pricing is $5/$25 per million tokens — making Opus-level capabilities accessible to even more users, teams, and enterprises.&lt;/p&gt;&lt;p&gt;The company has also invested heavily in the developer infrastructure that powers Claude Code. In late November, &lt;a href="https://www.anthropic.com/engineering/advanced-tool-use"&gt;&lt;u&gt;Anthropic released three new beta features for tool use&lt;/u&gt;&lt;/a&gt;: Tool Search Tool, which allows Claude to use search tools to access thousands of tools without consuming its context window; Programmatic Tool Calling, which allows Claude to invoke tools in a code execution environment reducing the impact on the model&amp;#x27;s context window; and Tool Use Examples, which provides a universal standard for demonstrating how to effectively use a given tool.&lt;/p&gt;&lt;p&gt;The &lt;a href="https://modelcontextprotocol.io/docs/getting-started/intro"&gt;&lt;u&gt;Model Context Protocol (MCP)&lt;/u&gt;&lt;/a&gt; is an open standard for connecting AI agents to external systems. Connecting agents to tools and data traditionally requires a custom integration for each pairing, creating fragmentation and duplicated effort that makes it difficult to scale truly connected systems. MCP provides a universal protocol — developers implement MCP once in their agent and it unlocks an entire ecosystem of integrations.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s own AI transformation: what happens when engineers use Claude all day&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic has been unusually transparent about how its own engineers use Claude Code — and the findings offer a preview of broader workforce implications. In August 2025, Anthropic&lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt; surveyed 132 engineers and researchers&lt;/u&gt;&lt;/a&gt;, conducted 53 in-depth qualitative interviews, and studied internal Claude Code usage data to understand how AI use is changing work at the company.&lt;/p&gt;&lt;p&gt;Employees self-reported &lt;a href="https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic"&gt;&lt;u&gt;using Claude in 60% of their work&lt;/u&gt;&lt;/a&gt; and achieving a 50% productivity boost, a 2-3x increase from this time last year. This productivity looks like slightly less time per task category, but considerably more output volume.&lt;/p&gt;&lt;p&gt;Perhaps most notably, 27% of Claude-assisted work consists of tasks that wouldn&amp;#x27;t have been done otherwise, such as scaling projects, making nice-to-have tools like interactive data dashboards, and exploratory work that wouldn&amp;#x27;t be cost-effective if done manually.&lt;/p&gt;&lt;p&gt;The internal research also revealed how Claude is changing the nature of engineering collaboration. The maximum number of consecutive tool calls Claude Code makes per transcript increased by 116%. Claude now chains together 21.2 independent tool calls without need for human intervention versus 9.8 tool calls from six months ago.&lt;/p&gt;&lt;p&gt;The number of human turns decreased by 33%. The average number of human turns decreased from 6.2 to 4.1 per transcript, suggesting that less human input is necessary to accomplish a given task now compared to six months ago.&lt;/p&gt;&lt;p&gt;But the research also surfaced tensions. One prominent theme was that Claude has become the first stop for questions that once went to colleagues. &amp;quot;It has reduced my dependence on [my team] by 80%, [but] the last 20% is crucial and I go and talk to them,&amp;quot; one engineer explained. Several engineers said they &amp;quot;bounce ideas off&amp;quot; Claude, similar to interactions with human collaborators.&lt;/p&gt;&lt;p&gt;Others described experiencing less interaction with colleagues. Some appreciate the reduced social friction, but others resist the change or miss the older way of working: &amp;quot;I like working with people and it is sad that I &amp;#x27;need&amp;#x27; them less now.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Anthropic stacks up against OpenAI, Google, and Microsoft in the enterprise AI race&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic is not alone in racing to capture the enterprise coding market. &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.microsoft.com/en-us/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; (through &lt;a href="https://github.com/features/copilot"&gt;&lt;u&gt;GitHub Copilot&lt;/u&gt;&lt;/a&gt;) are all pursuing similar integrations. The Slack launch gives Anthropic a presence in one of the most widely-used enterprise communication platforms — Slack claims over 750,000 organizations use its software.&lt;/p&gt;&lt;p&gt;The deal comes as Anthropic pursues a more disciplined growth path than rival OpenAI, focusing on enterprise customers and coding workloads. Internal financials reported by The Wall Street Journal show &lt;a href="https://www.wsj.com/tech/ai/openai-anthropic-profitability-e9f5bcd6?gaa_at=eafs&amp;amp;gaa_n=AWEtsqf_yQrKzPR_c7Gjwi4FvVkM-Uc0I6k_V6Kev-0xg4RiUud_8rUKLIBVacKJN5E%3D&amp;amp;gaa_ts=693721bf&amp;amp;gaa_sig=m9SvmJAAB8gRVI39wbvEYOLFa9vd_lQbtYoQE1ROCKxaf6CXTOxXgKn4HmpIwymS0BUt58auzZusgCjq74SM2w%3D%3D"&gt;&lt;u&gt;Anthropic expects to break even by 2028&lt;/u&gt;&lt;/a&gt; — two years earlier than OpenAI, which continues to invest heavily in infrastructure as it expands into video, hardware, and consumer products.&lt;/p&gt;&lt;p&gt;The move also marks an increased push into developer tooling. Anthropic has recently seen backing from some of tech&amp;#x27;s biggest titans. Microsoft and Nvidia pledged up to &lt;a href="https://www.cnbc.com/2025/12/03/anthropic-claude-reportedly-preparing-ipo-race-openai-chatgpt-ft-wilson-sonsini-goodrich-rosati.html"&gt;&lt;u&gt;$15 billion in fresh investment&lt;/u&gt;&lt;/a&gt; in Anthropic last month, alongside a $30 billion commitment from Anthropic to run Claude Code on Microsoft&amp;#x27;s cloud. This is in addition to the &lt;a href="https://www.anthropic.com/news/anthropic-amazon-trainium"&gt;&lt;u&gt;$8 billion invested from Amazon &lt;/u&gt;&lt;/a&gt;and &lt;a href="https://www.cnbc.com/2025/12/04/google-replit-ai-vibe-coding-anthropic-cursor.html"&gt;&lt;u&gt;$3 billion from Google&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The cross-investment from both &lt;a href="https://microsoft.com/"&gt;&lt;u&gt;Microsoft&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://www.google.com/"&gt;&lt;u&gt;Google&lt;/u&gt;&lt;/a&gt; — fierce competitors in the cloud and AI spaces — highlights how valuable Anthropic&amp;#x27;s enterprise positioning has become. By integrating with Slack (which is owned by Salesforce), Anthropic further embeds itself in the enterprise software ecosystem while remaining platform-agnostic.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What the Slack integration means for developers — and whether they can trust it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For engineering teams, the Slack integration promises to collapse the distance between problem identification and problem resolution. A bug report in a Slack channel can immediately trigger investigation. A feature request can spawn a prototype. A code review comment can generate a refactor.&lt;/p&gt;&lt;p&gt;But the integration also raises questions about oversight and code quality. Most Anthropic employees use Claude frequently while reporting they can &amp;quot;fully delegate&amp;quot; only 0-20% of their work to it. Claude is a constant collaborator but using it generally involves active supervision and validation, especially in high-stakes work — versus handing off tasks requiring no verification at all.&lt;/p&gt;&lt;p&gt;Some employees are concerned about the atrophy of deeper skillsets required for both writing and critiquing code — &amp;quot;When producing output is so easy and fast, it gets harder and harder to actually take the time to learn something.&amp;quot;&lt;/p&gt;&lt;p&gt;The Slack integration, by making Claude Code invocation as simple as an @mention, may accelerate both the productivity benefits and the skill-atrophy concerns that Anthropic&amp;#x27;s own research has documented.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The future of coding may be conversational—and Anthropic is racing to prove it&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The beta launch marks the beginning of what Anthropic expects will be a broader rollout, with documentation forthcoming for teams looking to deploy the integration and refinements planned based on user feedback during the research preview phase.&lt;/p&gt;&lt;p&gt;For Anthropic, the Slack integration is a calculated bet on a fundamental shift in how software gets written. The company is wagering that the future of coding will be conversational — that the walls between where developers talk about problems and where they solve them will dissolve entirely. The companies that win enterprise AI, in this view, will be the ones that meet developers not in specialized tools but in the chat windows they already have open all day.&lt;/p&gt;&lt;p&gt;Whether that vision becomes reality will depend on whether &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt; can deliver enterprise-grade reliability while maintaining the security that organizations demand. The early returns are promising: a billion dollars in revenue, a roster of Fortune 500 customers, and a growing ecosystem of integrations suggest Anthropic is onto something real.&lt;/p&gt;&lt;p&gt;But in one of Anthropic&amp;#x27;s own internal interviews, an engineer offered a more cautious assessment of the transformation underway: &amp;quot;Nobody knows what&amp;#x27;s going to happen… the important thing is to just be really adaptable.&amp;quot;&lt;/p&gt;&lt;p&gt;In the age of AI coding agents, that may be the only career advice that holds up.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropics-claude-code-can-now-read-your-slack-messages-and-write-code-for</guid><pubDate>Mon, 08 Dec 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] Claude Code is coming to Slack, and that’s a bigger deal than it sounds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/claude-code-is-coming-to-slack-and-thats-a-bigger-deal-than-it-sounds/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/claude-code-slack.png?resize=1200,685" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is launching Claude Code in Slack, allowing developers to delegate coding tasks directly from chat threads. The beta feature, available Monday as a research preview, builds on Anthropic’s existing Slack integration by adding full workflow automation. The rollout signals that the next frontier in coding assistants isn’t the model; it’s the workflow.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, developers could only get lightweight coding help via Claude in Slack — like writing snippets, debugging, and explanations. Now they can tag @Claude to spin up a complete coding session using Slack context like bug reports or feature requests. Claude analyzes recent messages to determine the right repository, posts progress updates in threads, and shares links to review work and open pull requests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move reflects a broader industry shift: AI coding assistants are migrating from IDEs (integrated development environment, where software development happens) into collaboration tools where teams already work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor offers Slack integration for drafting and debugging code in threads, while GitHub Copilot recently added features to generate pull requests from chat. OpenAI’s Codex is accessible via custom Slack bots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Slack, positioning itself as an “agentic hub” where AI meets workplace context creates a strategic advantage: Whichever AI tool dominates Slack — the center of engineering communication — could shape how software teams work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By letting developers move seamlessly from conversation to code without switching apps, Claude Code and similar tools represent a shift toward AI-embedded collaboration that could fundamentally change developer workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Anthropic has not yet confirmed when it would make a broader rollout available, the timing is strategic. The AI coding market is getting more competitive, and differentiation is starting to depend more on integration depth and distribution than model capability alone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the integration raises questions about code security and IP protection, as it adds another platform through which sensitive repository access must be managed and audited — while also introducing new dependencies where outages or rate limits in either Slack or Claude’s API could disrupt development workflows that teams previously controlled locally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Anthropic and Slack for more information.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/claude-code-slack.png?resize=1200,685" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is launching Claude Code in Slack, allowing developers to delegate coding tasks directly from chat threads. The beta feature, available Monday as a research preview, builds on Anthropic’s existing Slack integration by adding full workflow automation. The rollout signals that the next frontier in coding assistants isn’t the model; it’s the workflow.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Previously, developers could only get lightweight coding help via Claude in Slack — like writing snippets, debugging, and explanations. Now they can tag @Claude to spin up a complete coding session using Slack context like bug reports or feature requests. Claude analyzes recent messages to determine the right repository, posts progress updates in threads, and shares links to review work and open pull requests.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The move reflects a broader industry shift: AI coding assistants are migrating from IDEs (integrated development environment, where software development happens) into collaboration tools where teams already work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cursor offers Slack integration for drafting and debugging code in threads, while GitHub Copilot recently added features to generate pull requests from chat. OpenAI’s Codex is accessible via custom Slack bots.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For Slack, positioning itself as an “agentic hub” where AI meets workplace context creates a strategic advantage: Whichever AI tool dominates Slack — the center of engineering communication — could shape how software teams work.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By letting developers move seamlessly from conversation to code without switching apps, Claude Code and similar tools represent a shift toward AI-embedded collaboration that could fundamentally change developer workflows.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Anthropic has not yet confirmed when it would make a broader rollout available, the timing is strategic. The AI coding market is getting more competitive, and differentiation is starting to depend more on integration depth and distribution than model capability alone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;That said, the integration raises questions about code security and IP protection, as it adds another platform through which sensitive repository access must be managed and audited — while also introducing new dependencies where outages or rate limits in either Slack or Claude’s API could disrupt development workflows that teams previously controlled locally.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Anthropic and Slack for more information.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/claude-code-is-coming-to-slack-and-thats-a-bigger-deal-than-it-sounds/</guid><pubDate>Mon, 08 Dec 2025 19:18:05 +0000</pubDate></item><item><title>[NEW] MIT affiliates named 2025 Schmidt Sciences AI2050 Fellows (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-affiliates-named-schmidt-sciences-ai2050-fellows-1208</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/ai2050-fellows-2025_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Two current MIT affiliates and seven additional alumni are among those named to the 2025 cohort of&amp;nbsp;AI2050 Fellows. &amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Zongyi Li, a postdoc in the MIT Computer Science and Artificial Intelligence Lab, and Tess Smidt ’12, an associate professor of electrical engineering and computer science (EECS), were both named as AI2050 Early Career Fellows.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Seven additional MIT alumni were also honored. AI2050 Early Career Fellows include Brian Hie SM '19, PhD '21; Natasha Mary Jaques PhD '20; Martin Anton Schrimpf PhD '22; Lindsey Raymond SM '19, PhD '24, who will join the MIT faculty in EECS, the Department of Economics, and the MIT Schwarzman College of Computing in 2026; and Ellen Dee Zhong PhD ’22. AI2050 Senior Fellows include Surya Ganguli ’98, MNG ’98; and Luke Zettlemoyer SM ’03, PhD ’09.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;AI2050 Fellows are announced annually by Schmidt Sciences, a nonprofit organization founded in 2024 by Eric and Wendy Schmidt that works to accelerate scientific knowledge and breakthroughs with the most promising, advanced tools to support a thriving planet. The organization prioritizes research in areas poised for impact including AI and advanced computing, astrophysics, biosciences, climate, and space — as well as supporting researchers in a variety of disciplines through its science systems program.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li is postdoc in CSAIL working with associate professor of EECS Kaiming He. Li's research focuses on developing neural operator methods to accelerate scientific computing. He received his PhD in computing and mathematical sciences from Caltech, where he was advised by Anima Anandkumar and Andrew Stuart. He holds undergraduate degrees in computer science and mathematics from Washington University in St. Louis.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li's work has been supported by a Kortschak Scholarship, PIMCO Fellowship, Amazon AI4Science Fellowship, Nvidia Fellowship, and MIT-Novo Nordisk AI Fellowship. He has also completed three summer internships at Nvidia. Li will join the NYU Courant Institute of Mathematical Sciences as an assistant professor of mathematics and data science in fall 2026.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt, associate professor of electrical engineering and computer science (EECS), is the principal investigator of the Atomic Architects group at the Research Laboratory of Electronics (RLE), where she works at the intersection of physics, geometry, and machine learning to design algorithms that aid in the understanding of physical systems under physical and geometric constraints, with applications to the design both of new materials and new molecules. She has a particular focus on symmetries present in 3D physical systems, such as rotation, translation, and reflection.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt earned her BS in physics from MIT in 2012 and her PhD in physics from the University of California at Berkeley in 2018. Prior to joining the MIT EECS faculty in 2021, she was the 2018 Alvarez Postdoctoral Fellow in Computing Sciences at Lawrence Berkeley National Laboratory, and a software engineering intern on the Google Accelerated Sciences team, where she developed Euclidean symmetry equivariant neural networks that naturally handle 3D geometry and geometric tensor data. Besides the AI2050 fellowship, she has received an Air Force Office of Scientific Research Young Investigator Program award, the EECS Outstanding Educator Award, and a Transformative Research Fund award.&lt;/p&gt;&lt;p&gt;Conceived and co-chaired by Eric Schmidt and James Manyika, AI2050 is a philanthropic initiative aimed at helping to solve&amp;nbsp;hard problems in AI. Within their research, each fellow will contend with the central motivating question of AI2050: “It’s 2050. AI has turned out to be hugely beneficial to society. What happened? What are the most important problems we solved and the opportunities and possibilities we realized to ensure this outcome?”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/ai2050-fellows-2025_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr"&gt;Two current MIT affiliates and seven additional alumni are among those named to the 2025 cohort of&amp;nbsp;AI2050 Fellows. &amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Zongyi Li, a postdoc in the MIT Computer Science and Artificial Intelligence Lab, and Tess Smidt ’12, an associate professor of electrical engineering and computer science (EECS), were both named as AI2050 Early Career Fellows.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Seven additional MIT alumni were also honored. AI2050 Early Career Fellows include Brian Hie SM '19, PhD '21; Natasha Mary Jaques PhD '20; Martin Anton Schrimpf PhD '22; Lindsey Raymond SM '19, PhD '24, who will join the MIT faculty in EECS, the Department of Economics, and the MIT Schwarzman College of Computing in 2026; and Ellen Dee Zhong PhD ’22. AI2050 Senior Fellows include Surya Ganguli ’98, MNG ’98; and Luke Zettlemoyer SM ’03, PhD ’09.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;AI2050 Fellows are announced annually by Schmidt Sciences, a nonprofit organization founded in 2024 by Eric and Wendy Schmidt that works to accelerate scientific knowledge and breakthroughs with the most promising, advanced tools to support a thriving planet. The organization prioritizes research in areas poised for impact including AI and advanced computing, astrophysics, biosciences, climate, and space — as well as supporting researchers in a variety of disciplines through its science systems program.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li is postdoc in CSAIL working with associate professor of EECS Kaiming He. Li's research focuses on developing neural operator methods to accelerate scientific computing. He received his PhD in computing and mathematical sciences from Caltech, where he was advised by Anima Anandkumar and Andrew Stuart. He holds undergraduate degrees in computer science and mathematics from Washington University in St. Louis.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Li's work has been supported by a Kortschak Scholarship, PIMCO Fellowship, Amazon AI4Science Fellowship, Nvidia Fellowship, and MIT-Novo Nordisk AI Fellowship. He has also completed three summer internships at Nvidia. Li will join the NYU Courant Institute of Mathematical Sciences as an assistant professor of mathematics and data science in fall 2026.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt, associate professor of electrical engineering and computer science (EECS), is the principal investigator of the Atomic Architects group at the Research Laboratory of Electronics (RLE), where she works at the intersection of physics, geometry, and machine learning to design algorithms that aid in the understanding of physical systems under physical and geometric constraints, with applications to the design both of new materials and new molecules. She has a particular focus on symmetries present in 3D physical systems, such as rotation, translation, and reflection.&lt;/p&gt;&lt;p dir="ltr"&gt;Smidt earned her BS in physics from MIT in 2012 and her PhD in physics from the University of California at Berkeley in 2018. Prior to joining the MIT EECS faculty in 2021, she was the 2018 Alvarez Postdoctoral Fellow in Computing Sciences at Lawrence Berkeley National Laboratory, and a software engineering intern on the Google Accelerated Sciences team, where she developed Euclidean symmetry equivariant neural networks that naturally handle 3D geometry and geometric tensor data. Besides the AI2050 fellowship, she has received an Air Force Office of Scientific Research Young Investigator Program award, the EECS Outstanding Educator Award, and a Transformative Research Fund award.&lt;/p&gt;&lt;p&gt;Conceived and co-chaired by Eric Schmidt and James Manyika, AI2050 is a philanthropic initiative aimed at helping to solve&amp;nbsp;hard problems in AI. Within their research, each fellow will contend with the central motivating question of AI2050: “It’s 2050. AI has turned out to be hugely beneficial to society. What happened? What are the most important problems we solved and the opportunities and possibilities we realized to ensure this outcome?”&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-affiliates-named-schmidt-sciences-ai2050-fellows-1208</guid><pubDate>Mon, 08 Dec 2025 20:15:00 +0000</pubDate></item><item><title>[NEW] Google’s AI try-on app Doppl adds a shoppable discovery feed (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/googles-ai-try-on-app-doppl-adds-a-shoppable-discovery-feed/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/doppl.png?resize=1200,665" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s introducing a shoppable discovery feed in Doppl, its experimental app that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant says the idea behind the new feed is to display recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. Google determines your style by analyzing the preferences you share with Doppl and the items you interact with.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as short-form video feeds, particularly on TikTok and Instagram, have conditioned users to scroll visual feeds and buy what they see. However, unlike on TikTok and Instagram, where real influencers showcase products, Google’s new feed only consists of AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While some may not be fond of an AI-generated feed, Google likely sees it as a way to surface products in a format that people are already used to. Plus, it makes sense for the company to try a new e-commerce strategy, especially as it continues to lose ground to companies like Amazon and social media platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that AI-generated videos aren’t new to Doppl. While the app creates images of a virtual version of yourself wearing different outfits, it can turn these static images and convert them into AI-generated videos. The purpose of this is to give you a better sense of how the outfit would look on you in real life. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new discovery feed is rolling out to Doppl on iOS and Android in the U.S. for users 18 and above. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Although a feed consisting solely of AI-generated content would have seemed strange a year ago, the idea is now gaining traction. For example, OpenAI in September launched Sora, a social media platform of just AI videos. Meta also has a short-form video feed of AI-generated videos called “Vibes” in the Meta AI app.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/doppl.png?resize=1200,665" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Monday that it’s introducing a shoppable discovery feed in Doppl, its experimental app that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant says the idea behind the new feed is to display recommendations so users can discover and virtually try on new items. Nearly everything in the feed is shoppable, with direct links to merchants. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The discovery feed features AI-generated videos of real products and suggests outfits based on your personalized style. Google determines your style by analyzing the preferences you share with Doppl and the items you interact with.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move comes as short-form video feeds, particularly on TikTok and Instagram, have conditioned users to scroll visual feeds and buy what they see. However, unlike on TikTok and Instagram, where real influencers showcase products, Google’s new feed only consists of AI-generated content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While some may not be fond of an AI-generated feed, Google likely sees it as a way to surface products in a format that people are already used to. Plus, it makes sense for the company to try a new e-commerce strategy, especially as it continues to lose ground to companies like Amazon and social media platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting that AI-generated videos aren’t new to Doppl. While the app creates images of a virtual version of yourself wearing different outfits, it can turn these static images and convert them into AI-generated videos. The purpose of this is to give you a better sense of how the outfit would look on you in real life. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new discovery feed is rolling out to Doppl on iOS and Android in the U.S. for users 18 and above. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Although a feed consisting solely of AI-generated content would have seemed strange a year ago, the idea is now gaining traction. For example, OpenAI in September launched Sora, a social media platform of just AI videos. Meta also has a short-form video feed of AI-generated videos called “Vibes” in the Meta AI app.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/googles-ai-try-on-app-doppl-adds-a-shoppable-discovery-feed/</guid><pubDate>Mon, 08 Dec 2025 21:29:03 +0000</pubDate></item><item><title>[NEW] Department of Commerce approves Nvidia H200 chip exports to China (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/08/department-of-commerce-may-approve-nvidia-h200-chip-exports-to-china/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Advanced Nvidia AI chips can head back to China after all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Department of Commerce will allow Nvidia to ship H200 chips to China, as originally reported by Semafor, to approved customers in the country. The U.S. will take a 25% cut of these sales, CNBC reported. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;H200 chips are much more advanced than the H20 chips Nvidia developed specifically for the Chinese market, but the company would only be able to send H200s that are roughly 18 months old, Semafor reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Nvidia spokesperson told TechCrunch of the development: “We applaud President Trump’s decision to allow America’s chip industry to compete to support high paying jobs and manufacturing in America. Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news report comes a week after U.S. Commerce Secretary Howard Lutnick said the decision on exporting these H200 chips to China was in President Donald Trump’s hands.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decision to send the chips to China conflicts with Congressional concerns about national security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pete Ricketts, a Republican senator from Nebraska, and Chris Coons, a Democratic senator from Delaware, introduced a bill on December 4 that would block the export of advanced AI chips to China for more than two years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Secure and Feasible Exports Act (SAFE) Chips Act would require the Department of Commerce to deny any export license on advanced AI chips to China for 30 months. It’s unclear when legislators will vote on the proposed bill especially now that the Trump administration has given the green light to sell the H200 chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Congress has long been clear about sending advanced AI chips to China — on both sides of the aisle — President Trump has waffled on whether or not to allow the exports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration hit chip companies like Nvidia with licensing requirements to send their chips to China in April before it formally rescinded a Biden administration diffusion rule that would have regulated AI chip exports in May. Over the summer, the U.S. government signaled that companies would be able to start sending chips to China as long as the government got a 15% cut of all revenue, as chips became a bargaining tool in trade talks with China.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, by that point, the market for U.S.-developed chips in China was strained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September, China’s internet regulator, the Cyberspace Administration of China, banned domestic companies from buying Nvidia’s chips, leaving companies in the country to rely on less advanced domestic chips from Alibaba and Huawei.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, Trump said that Chinese president Xi Jinping “responded positively” to the latest H200 news in a Truth Social post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated on December 8 when the proposed decision was confirmed. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2219673294.jpg?resize=1200,750" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Advanced Nvidia AI chips can head back to China after all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Department of Commerce will allow Nvidia to ship H200 chips to China, as originally reported by Semafor, to approved customers in the country. The U.S. will take a 25% cut of these sales, CNBC reported. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;H200 chips are much more advanced than the H20 chips Nvidia developed specifically for the Chinese market, but the company would only be able to send H200s that are roughly 18 months old, Semafor reported.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An Nvidia spokesperson told TechCrunch of the development: “We applaud President Trump’s decision to allow America’s chip industry to compete to support high paying jobs and manufacturing in America. Offering H200 to approved commercial customers, vetted by the Department of Commerce, strikes a thoughtful balance that is great for America.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The news report comes a week after U.S. Commerce Secretary Howard Lutnick said the decision on exporting these H200 chips to China was in President Donald Trump’s hands.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The decision to send the chips to China conflicts with Congressional concerns about national security.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Pete Ricketts, a Republican senator from Nebraska, and Chris Coons, a Democratic senator from Delaware, introduced a bill on December 4 that would block the export of advanced AI chips to China for more than two years.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The Secure and Feasible Exports Act (SAFE) Chips Act would require the Department of Commerce to deny any export license on advanced AI chips to China for 30 months. It’s unclear when legislators will vote on the proposed bill especially now that the Trump administration has given the green light to sell the H200 chips.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Congress has long been clear about sending advanced AI chips to China — on both sides of the aisle — President Trump has waffled on whether or not to allow the exports.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration hit chip companies like Nvidia with licensing requirements to send their chips to China in April before it formally rescinded a Biden administration diffusion rule that would have regulated AI chip exports in May. Over the summer, the U.S. government signaled that companies would be able to start sending chips to China as long as the government got a 15% cut of all revenue, as chips became a bargaining tool in trade talks with China.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, by that point, the market for U.S.-developed chips in China was strained.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In September, China’s internet regulator, the Cyberspace Administration of China, banned domestic companies from buying Nvidia’s chips, leaving companies in the country to rely on less advanced domestic chips from Alibaba and Huawei.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, Trump said that Chinese president Xi Jinping “responded positively” to the latest H200 news in a Truth Social post. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated on December 8 when the proposed decision was confirmed. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/08/department-of-commerce-may-approve-nvidia-h200-chip-exports-to-china/</guid><pubDate>Mon, 08 Dec 2025 21:40:46 +0000</pubDate></item><item><title>[NEW] Z.ai debuts open source GLM-4.6V, a native tool-calling vision model for multimodal reasoning (AI | VentureBeat)</title><link>https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for</link><description>[unable to retrieve full-text content]&lt;p&gt;Chinese AI startup Zhipu AI aka &lt;a href="https://z.ai/blog/glm-4.6v"&gt;&lt;b&gt;Z.ai has released its GLM-4.6V series&lt;/b&gt;&lt;/a&gt;, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. &lt;/p&gt;&lt;p&gt;The release includes two models in &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; sizes: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V (106B)&lt;/b&gt;, a larger 106-billion parameter model aimed at cloud-scale inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V-Flash (9B)&lt;/b&gt;, a smaller model of only 9 billion parameters designed for low-latency, local applications&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Recall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.&lt;/p&gt;&lt;p&gt;However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.&lt;/p&gt;&lt;p&gt;The defining innovation in this series is the introduction of &lt;b&gt;native function calling&lt;/b&gt; in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. &lt;/p&gt;&lt;p&gt;With a 128,000 token context length (equivalent to a 300-page novel&amp;#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&amp;#x27;s available in the following formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/vlm/glm-4.6v"&gt;API access&lt;/a&gt; via OpenAI-compatible interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.z.ai"&gt;Try the demo&lt;/a&gt; on Zhipu’s web interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Download weights&lt;/a&gt; from Hugging Face&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Desktop assistant app available on &lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App"&gt;Hugging Face Spaces&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Licensing and Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM‑4.6V and GLM‑4.6V‑Flash are distributed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT license&lt;/a&gt;, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. &lt;/p&gt;&lt;p&gt;This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.&lt;/p&gt;&lt;p&gt;Model weights and documentation are publicly hosted on &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face&lt;/a&gt;, with supporting code and tooling available on &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Architecture and Technical Capabilities&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. &lt;/p&gt;&lt;p&gt;Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. &lt;/p&gt;&lt;p&gt;Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.&lt;/p&gt;&lt;p&gt;A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. &lt;/p&gt;&lt;p&gt;In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.&lt;/p&gt;&lt;p&gt;On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Native Multimodal Tool Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.&lt;/p&gt;&lt;p&gt;The tool invocation mechanism works bi-directionally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In practice, this means GLM-4.6V can complete tasks such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Generating structured reports from mixed-format documents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Performing visual audit of candidate images&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Automatically cropping figures from papers during generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conducting visual web search and answering multimodal queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;High Performance Benchmarks Compared to Other Similar-Sized Models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. &lt;/p&gt;&lt;p&gt;According to the benchmark chart released by Zhipu AI:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Example scores from the leaderboard include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Frontend Automation and Long-Context Workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Replicate pixel-accurate HTML/CSS/JS from UI screenshots&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accept natural language editing commands to modify layouts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify and manipulate specific UI components visually&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.&lt;/p&gt;&lt;p&gt;In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;150 pages of text (input)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;200 slide decks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1-hour videos&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Training and Reinforcement Learning&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progress&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial grounding&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Function-aware training: Uses structured tags (e.g., &amp;lt;think&amp;gt;, &amp;lt;answer&amp;gt;, &amp;lt;|begin_of_box|&amp;gt;) to align reasoning and answer formatting&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domains&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing (API)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash: Free&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Compared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:&lt;/p&gt;&lt;p&gt;&lt;i&gt;USD per 1M tokens — sorted lowest → highest total cost&lt;/i&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.56&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM‑4.6V&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.30&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.90&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.20&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/overview/pricing"&gt;Z.AI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/pricing"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$17.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$90.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;b&gt;Previous Releases: GLM‑4.5 Series and Enterprise Applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. &lt;/p&gt;&lt;p&gt;The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. &lt;/p&gt;&lt;p&gt;The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.&lt;/p&gt;&lt;p&gt;Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipel&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ecosystem Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Integrated visual tool usage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Structured multimodal generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Agent-oriented memory and decision logic&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. &lt;/p&gt;&lt;p&gt;The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Takeaway for Enterprise Leaders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems&lt;!-- --&gt;.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Chinese AI startup Zhipu AI aka &lt;a href="https://z.ai/blog/glm-4.6v"&gt;&lt;b&gt;Z.ai has released its GLM-4.6V series&lt;/b&gt;&lt;/a&gt;, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. &lt;/p&gt;&lt;p&gt;The release includes two models in &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; sizes: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V (106B)&lt;/b&gt;, a larger 106-billion parameter model aimed at cloud-scale inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V-Flash (9B)&lt;/b&gt;, a smaller model of only 9 billion parameters designed for low-latency, local applications&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Recall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.&lt;/p&gt;&lt;p&gt;However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.&lt;/p&gt;&lt;p&gt;The defining innovation in this series is the introduction of &lt;b&gt;native function calling&lt;/b&gt; in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. &lt;/p&gt;&lt;p&gt;With a 128,000 token context length (equivalent to a 300-page novel&amp;#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&amp;#x27;s available in the following formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/vlm/glm-4.6v"&gt;API access&lt;/a&gt; via OpenAI-compatible interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.z.ai"&gt;Try the demo&lt;/a&gt; on Zhipu’s web interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Download weights&lt;/a&gt; from Hugging Face&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Desktop assistant app available on &lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App"&gt;Hugging Face Spaces&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Licensing and Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM‑4.6V and GLM‑4.6V‑Flash are distributed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT license&lt;/a&gt;, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. &lt;/p&gt;&lt;p&gt;This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.&lt;/p&gt;&lt;p&gt;Model weights and documentation are publicly hosted on &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face&lt;/a&gt;, with supporting code and tooling available on &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Architecture and Technical Capabilities&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. &lt;/p&gt;&lt;p&gt;Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. &lt;/p&gt;&lt;p&gt;Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.&lt;/p&gt;&lt;p&gt;A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. &lt;/p&gt;&lt;p&gt;In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.&lt;/p&gt;&lt;p&gt;On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Native Multimodal Tool Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.&lt;/p&gt;&lt;p&gt;The tool invocation mechanism works bi-directionally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In practice, this means GLM-4.6V can complete tasks such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Generating structured reports from mixed-format documents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Performing visual audit of candidate images&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Automatically cropping figures from papers during generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conducting visual web search and answering multimodal queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;High Performance Benchmarks Compared to Other Similar-Sized Models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. &lt;/p&gt;&lt;p&gt;According to the benchmark chart released by Zhipu AI:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Example scores from the leaderboard include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Frontend Automation and Long-Context Workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Replicate pixel-accurate HTML/CSS/JS from UI screenshots&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accept natural language editing commands to modify layouts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify and manipulate specific UI components visually&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.&lt;/p&gt;&lt;p&gt;In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;150 pages of text (input)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;200 slide decks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1-hour videos&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Training and Reinforcement Learning&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progress&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial grounding&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Function-aware training: Uses structured tags (e.g., &amp;lt;think&amp;gt;, &amp;lt;answer&amp;gt;, &amp;lt;|begin_of_box|&amp;gt;) to align reasoning and answer formatting&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domains&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing (API)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash: Free&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Compared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:&lt;/p&gt;&lt;p&gt;&lt;i&gt;USD per 1M tokens — sorted lowest → highest total cost&lt;/i&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.56&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM‑4.6V&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.30&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.90&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.20&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/overview/pricing"&gt;Z.AI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/pricing"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$17.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$90.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;b&gt;Previous Releases: GLM‑4.5 Series and Enterprise Applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. &lt;/p&gt;&lt;p&gt;The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. &lt;/p&gt;&lt;p&gt;The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.&lt;/p&gt;&lt;p&gt;Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipel&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ecosystem Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Integrated visual tool usage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Structured multimodal generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Agent-oriented memory and decision logic&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. &lt;/p&gt;&lt;p&gt;The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Takeaway for Enterprise Leaders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems&lt;!-- --&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for</guid><pubDate>Tue, 09 Dec 2025 01:03:00 +0000</pubDate></item></channel></rss>