<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 25 Jun 2025 18:32:15 +0000</lastBuildDate><item><title>The Debrief: Power and energy (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1118458/the-debrief-power-and-energy/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;It may sound bluntly obvious, but energy is power. Those who can produce it, especially lots of it, get to exert authority in all sorts of ways. It brings revenue and enables manufacturing, data processing, transportation, and military might. Energy resources are arguably a nation’s most important asset. Look at Russia, or Saudi Arabia, or China, or Canada, or Qatar, or—for that matter—the US. For all these nations, energy production plays key roles in their economies and their outsize global status. (Qatar, for example, has a population roughly the size of metro Portland, Oregon.)&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1035426" src="https://wp.technologyreview.com/wp-content/uploads/2021/09/mat_final.png?w=1000" /&gt;&lt;/figure&gt;  &lt;p&gt;The US has always been a nation of energy and industry. It was a major producer of coal, which fed the Industrial Revolution. World War II was won in large part by the energy production in the United States—which fueled both manufacturing of the war machine at home and its ships, planes, and tanks in the Pacific and Europe. Throughout its history, the country has found strength in energy production.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Yet in many ways right now the US seems to be forgetting those lessons. It is moving backward in terms of its clean-­energy strategy, especially when it comes to powering the grid, in ways that will affect the nation for decades to come—even as China and others are surging forward. And that retreat is taking place just as electricity demand and usage are growing again after being flat for nearly two decades. That growth, according to the US Energy Information Administration, is “coming from the commercial sector, which includes data centers, and the industrial sector, which includes manufacturing establishments.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As &lt;em&gt;MIT Technology Review&lt;/em&gt; has extensively reported, energy demand from data centers is set to soar, not plateau, as AI inhales ever more electricity from the grid. As my colleagues James O’Donnell and Casey Crownhart reported, by 2028 the share of US electricity going to power data centers may &lt;em&gt;triple&lt;/em&gt;. (For the full report, see technologyreview.com/energy-ai.)&lt;/p&gt; 
 &lt;p&gt;Both manufacturing and data centers are obviously priorities for the US writ large and the Trump administration in particular. Given those priorities, it’s surprising to see the administration and Congress making moves that would both decrease our potential energy supply and increase demand by lowering efficiency.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This will be most true for electricity generation. The administration’s proposed budget, still being considered as we went to press, would roll back tax credits for wind, solar, and other forms of clean energy. In households, they would hit credits for rooftop solar panels and residential energy efficiency programs. Simultaneously, the US is trying to roll back efficiency standards for household appliances. These standards are key to keeping consumer electricity prices down by decreasing demand.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;In short, what most analysts are expecting is more strain on the grid, which means prices will go up for everyone. Meanwhile, rollbacks to the Inflation Reduction Act and to credits for advanced manufacturing mean that fewer future-facing energy sources will be built.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is just belligerently shortsighted.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s especially true because as the US takes steps to make energy less abundant and more expensive, China—our ostensible chief international antagonist—is moving in exactly the opposite direction. The country has made massive strides in renewable energy generation, hitting its goals six years ahead of schedule. In fact, China is now producing so much clean energy that its carbon dioxide emissions are declining as a result.&lt;/p&gt;  &lt;p&gt;This issue is about power, in all its forms. Yet whether you’re talking about the ability to act or the act of providing electricity, power comes from energy. So when it comes to energy, we need “ands,” not “ors.” We need nuclear and solar and wind and hydropower and hydrogen and geothermal and batteries on the grid. And we need efficiency. And yes, we even need oil and gas in the mid term while we ramp up cleaner sources. That is the way to maintain and increase our prosperity, and the only way we can possibly head off some of the worst consequences of climate change.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;It may sound bluntly obvious, but energy is power. Those who can produce it, especially lots of it, get to exert authority in all sorts of ways. It brings revenue and enables manufacturing, data processing, transportation, and military might. Energy resources are arguably a nation’s most important asset. Look at Russia, or Saudi Arabia, or China, or Canada, or Qatar, or—for that matter—the US. For all these nations, energy production plays key roles in their economies and their outsize global status. (Qatar, for example, has a population roughly the size of metro Portland, Oregon.)&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image alignright size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-1035426" src="https://wp.technologyreview.com/wp-content/uploads/2021/09/mat_final.png?w=1000" /&gt;&lt;/figure&gt;  &lt;p&gt;The US has always been a nation of energy and industry. It was a major producer of coal, which fed the Industrial Revolution. World War II was won in large part by the energy production in the United States—which fueled both manufacturing of the war machine at home and its ships, planes, and tanks in the Pacific and Europe. Throughout its history, the country has found strength in energy production.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Yet in many ways right now the US seems to be forgetting those lessons. It is moving backward in terms of its clean-­energy strategy, especially when it comes to powering the grid, in ways that will affect the nation for decades to come—even as China and others are surging forward. And that retreat is taking place just as electricity demand and usage are growing again after being flat for nearly two decades. That growth, according to the US Energy Information Administration, is “coming from the commercial sector, which includes data centers, and the industrial sector, which includes manufacturing establishments.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As &lt;em&gt;MIT Technology Review&lt;/em&gt; has extensively reported, energy demand from data centers is set to soar, not plateau, as AI inhales ever more electricity from the grid. As my colleagues James O’Donnell and Casey Crownhart reported, by 2028 the share of US electricity going to power data centers may &lt;em&gt;triple&lt;/em&gt;. (For the full report, see technologyreview.com/energy-ai.)&lt;/p&gt; 
 &lt;p&gt;Both manufacturing and data centers are obviously priorities for the US writ large and the Trump administration in particular. Given those priorities, it’s surprising to see the administration and Congress making moves that would both decrease our potential energy supply and increase demand by lowering efficiency.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This will be most true for electricity generation. The administration’s proposed budget, still being considered as we went to press, would roll back tax credits for wind, solar, and other forms of clean energy. In households, they would hit credits for rooftop solar panels and residential energy efficiency programs. Simultaneously, the US is trying to roll back efficiency standards for household appliances. These standards are key to keeping consumer electricity prices down by decreasing demand.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;In short, what most analysts are expecting is more strain on the grid, which means prices will go up for everyone. Meanwhile, rollbacks to the Inflation Reduction Act and to credits for advanced manufacturing mean that fewer future-facing energy sources will be built.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This is just belligerently shortsighted.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s especially true because as the US takes steps to make energy less abundant and more expensive, China—our ostensible chief international antagonist—is moving in exactly the opposite direction. The country has made massive strides in renewable energy generation, hitting its goals six years ahead of schedule. In fact, China is now producing so much clean energy that its carbon dioxide emissions are declining as a result.&lt;/p&gt;  &lt;p&gt;This issue is about power, in all its forms. Yet whether you’re talking about the ability to act or the act of providing electricity, power comes from energy. So when it comes to energy, we need “ands,” not “ors.” We need nuclear and solar and wind and hydropower and hydrogen and geothermal and batteries on the grid. And we need efficiency. And yes, we even need oil and gas in the mid term while we ramp up cleaner sources. That is the way to maintain and increase our prosperity, and the only way we can possibly head off some of the worst consequences of climate change.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1118458/the-debrief-power-and-energy/</guid><pubDate>Wed, 25 Jun 2025 09:00:00 +0000</pubDate></item><item><title>Job titles of the future: Pandemic oracle (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1118453/pandemic-oracle-biological-risk-consultants-future-jobs-epidemic/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/PandemicOracle_Final.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Officially, Conor Browne is a biorisk consultant. Based in Belfast, Northern Ireland, he has advanced degrees in security studies and medical and business ethics, along with United Nations certifications in counterterrorism and conflict resolution. He’s worked on teams with NATO’s Science for Peace and Security Programme and with the UN High Commissioner for Refugees, analyzing how diseases affect migration and border security.&lt;/p&gt;  &lt;p&gt;Early in the emergence of SARS-CoV-2, international energy conglomerates seeking expert guidance on navigating the potential turmoil in markets and transportation became his main clients. Having studied the 2002 SARS outbreak, he predicted the exponential spread of the new airborne virus. He forecast the epidemic’s broadscale impact and its implications for business so accurately that he has come to be seen as a pandemic oracle.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Browne produces independent research reports and works directly with companies of all sizes. One of his niches is consulting on new diagnostic tools&lt;em&gt;—&lt;/em&gt;for example, in his work with RAIsonance, a startup using machine learning to analyze cough sounds correlated with tuberculosis and covid-19. For multinational corporations, he models threats such as the possibility of avian influenza spreading from human to human. He builds most- and least-likely scenarios for how the global business community might react to an H5N1 outbreak in China or the US. “I never want to be right,” he says of worst-case predictions.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Navigating uncertainty&lt;/h3&gt;  &lt;p&gt;Biorisk consultants are often trained in fields related to epidemiology, security, and counterterrorism. Browne also studied psychology to understand how humans respond to disaster. In times of increasing geopolitical volatility, he says, biomedical risk assessment must include sociopolitical forecasting.&lt;/p&gt; 
 &lt;p&gt;Demand for this type of crisis planning exploded in the corporate world in the aftermath of 9/11. Executives learned to create contingency plans for loss of personnel and infrastructure as a result of terrorism, pandemics, and natural disasters. And resilience planning proved crucial early in the covid-19 pandemic, as business leaders were forced to adjust to supply chain disruptions and the realities of remote work.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Network effects&lt;/h3&gt;  &lt;p&gt;By adding nuanced qualitative analysis to hard data, Browne creates proprietary guidance that clients can act on. “I give businesses an idea of what is coming, and what they do with that information is up to them,” he says. “I basically tell the future.” &lt;/p&gt;  &lt;p&gt;&lt;em&gt;Britta Shoot is a freelance journalist focusing on&amp;nbsp;pandemics, protests, and how people occupy space.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/PandemicOracle_Final.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Officially, Conor Browne is a biorisk consultant. Based in Belfast, Northern Ireland, he has advanced degrees in security studies and medical and business ethics, along with United Nations certifications in counterterrorism and conflict resolution. He’s worked on teams with NATO’s Science for Peace and Security Programme and with the UN High Commissioner for Refugees, analyzing how diseases affect migration and border security.&lt;/p&gt;  &lt;p&gt;Early in the emergence of SARS-CoV-2, international energy conglomerates seeking expert guidance on navigating the potential turmoil in markets and transportation became his main clients. Having studied the 2002 SARS outbreak, he predicted the exponential spread of the new airborne virus. He forecast the epidemic’s broadscale impact and its implications for business so accurately that he has come to be seen as a pandemic oracle.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Browne produces independent research reports and works directly with companies of all sizes. One of his niches is consulting on new diagnostic tools&lt;em&gt;—&lt;/em&gt;for example, in his work with RAIsonance, a startup using machine learning to analyze cough sounds correlated with tuberculosis and covid-19. For multinational corporations, he models threats such as the possibility of avian influenza spreading from human to human. He builds most- and least-likely scenarios for how the global business community might react to an H5N1 outbreak in China or the US. “I never want to be right,” he says of worst-case predictions.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Navigating uncertainty&lt;/h3&gt;  &lt;p&gt;Biorisk consultants are often trained in fields related to epidemiology, security, and counterterrorism. Browne also studied psychology to understand how humans respond to disaster. In times of increasing geopolitical volatility, he says, biomedical risk assessment must include sociopolitical forecasting.&lt;/p&gt; 
 &lt;p&gt;Demand for this type of crisis planning exploded in the corporate world in the aftermath of 9/11. Executives learned to create contingency plans for loss of personnel and infrastructure as a result of terrorism, pandemics, and natural disasters. And resilience planning proved crucial early in the covid-19 pandemic, as business leaders were forced to adjust to supply chain disruptions and the realities of remote work.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Network effects&lt;/h3&gt;  &lt;p&gt;By adding nuanced qualitative analysis to hard data, Browne creates proprietary guidance that clients can act on. “I give businesses an idea of what is coming, and what they do with that information is up to them,” he says. “I basically tell the future.” &lt;/p&gt;  &lt;p&gt;&lt;em&gt;Britta Shoot is a freelance journalist focusing on&amp;nbsp;pandemics, protests, and how people occupy space.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1118453/pandemic-oracle-biological-risk-consultants-future-jobs-epidemic/</guid><pubDate>Wed, 25 Jun 2025 09:00:00 +0000</pubDate></item><item><title>3 things Rhiannon Williams is into right now (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1118448/rhiannon-williams-instagram-life-drawing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Rhiannon.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt;&lt;h3 class="wp-block-heading"&gt;The last good Instagram account&lt;/h3&gt;  &lt;p&gt;It’s a truth universally acknowledged that social media is a Bad Vibe. Thankfully, there is still one Instagram account worth following that’s just as incisive, funny, and scathing today as when it was founded back in 2016: Every Outfit (@everyoutfitonsatc). Originally conceived as an homage to &lt;em&gt;Sex and the City&lt;/em&gt;’s iconic fashion, Every Outfit has since evolved into a wider cultural critique and spawned a podcast of the same name that I love listening to while running. &lt;em&gt;Sex and the City&lt;/em&gt; may be over, but Every Outfit is forever.&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;em&gt;Glorious Exploits,&lt;/em&gt; by Ferdia Lennon&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;Glorious Exploits&lt;/em&gt; is one of those rare books that manage to pull off being both laugh-out-loud funny and deeply moving, which is no mean feat. Set in ancient Sicily, it tells the story of unemployed potters Lampo and Gelon’s grand plan to stage the Greek tragedy &lt;em&gt;Medea&lt;/em&gt; with a cast of defeated Athenian soldiers who’ve been imprisoned in quarries on the outskirts of Syracuse. The ancient backdrop combined with the characters’ contemporary Irish dialogue (the author was born in Dublin) makes it unlike anything I’ve ever read before; it’s so ambitious it’s hard to believe it’s Lennon’s debut novel. Completely engrossing.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Life drawing&lt;/h3&gt;  &lt;p&gt;The depressing wave of AI-generated art that’s flooded the internet in recent years has inspired me to explore the exact opposite and make art the old-fashioned way. My art teacher in college always said the best way to learn the correct proportions of the human body was to draw it in person, so I’ve started attending classes near where I live in London. Pencil and paper are generally my medium of choice. Spending a few hours interpreting what’s in front of you in your own artistic style is really rewarding—and has the added bonus of being completely screen-free. I can’t recommend it enough.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/Rhiannon.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt;&lt;h3 class="wp-block-heading"&gt;The last good Instagram account&lt;/h3&gt;  &lt;p&gt;It’s a truth universally acknowledged that social media is a Bad Vibe. Thankfully, there is still one Instagram account worth following that’s just as incisive, funny, and scathing today as when it was founded back in 2016: Every Outfit (@everyoutfitonsatc). Originally conceived as an homage to &lt;em&gt;Sex and the City&lt;/em&gt;’s iconic fashion, Every Outfit has since evolved into a wider cultural critique and spawned a podcast of the same name that I love listening to while running. &lt;em&gt;Sex and the City&lt;/em&gt; may be over, but Every Outfit is forever.&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;em&gt;Glorious Exploits,&lt;/em&gt; by Ferdia Lennon&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;Glorious Exploits&lt;/em&gt; is one of those rare books that manage to pull off being both laugh-out-loud funny and deeply moving, which is no mean feat. Set in ancient Sicily, it tells the story of unemployed potters Lampo and Gelon’s grand plan to stage the Greek tragedy &lt;em&gt;Medea&lt;/em&gt; with a cast of defeated Athenian soldiers who’ve been imprisoned in quarries on the outskirts of Syracuse. The ancient backdrop combined with the characters’ contemporary Irish dialogue (the author was born in Dublin) makes it unlike anything I’ve ever read before; it’s so ambitious it’s hard to believe it’s Lennon’s debut novel. Completely engrossing.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Life drawing&lt;/h3&gt;  &lt;p&gt;The depressing wave of AI-generated art that’s flooded the internet in recent years has inspired me to explore the exact opposite and make art the old-fashioned way. My art teacher in college always said the best way to learn the correct proportions of the human body was to draw it in person, so I’ve started attending classes near where I live in London. Pencil and paper are generally my medium of choice. Spending a few hours interpreting what’s in front of you in your own artistic style is really rewarding—and has the added bonus of being completely screen-free. I can’t recommend it enough.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1118448/rhiannon-williams-instagram-life-drawing/</guid><pubDate>Wed, 25 Jun 2025 09:00:00 +0000</pubDate></item><item><title>The AI Hype Index: AI-powered toys are coming (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1119286/ai-hype-index-toys-agents-openai-yoshua-bengio/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624_JuneThumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt;&lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry.&lt;/p&gt;  &lt;p&gt;AI agents might be the toast of the AI industry, but they’re still not that reliable. That’s why Yoshua Bengio, one of the world’s leading AI experts, is creating his own nonprofit dedicated to guarding against deceptive agents. Not only can they mislead you, but new research suggests that the weaker an AI model powering an agent is, the less likely it is to be able to negotiate you a good deal online. Elsewhere, OpenAI has inked a deal with toymaker Mattel to develop “age-appropriate” AI-infused products. What could possibly go wrong?&lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624_JuneThumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt;&lt;p&gt;Separating AI reality from hyped-up fiction isn’t always easy. That’s why we’ve created the AI Hype Index—a simple, at-a-glance summary of everything you need to know about the state of the industry.&lt;/p&gt;  &lt;p&gt;AI agents might be the toast of the AI industry, but they’re still not that reliable. That’s why Yoshua Bengio, one of the world’s leading AI experts, is creating his own nonprofit dedicated to guarding against deceptive agents. Not only can they mislead you, but new research suggests that the weaker an AI model powering an agent is, the less likely it is to be able to negotiate you a good deal online. Elsewhere, OpenAI has inked a deal with toymaker Mattel to develop “age-appropriate” AI-infused products. What could possibly go wrong?&lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1119286/ai-hype-index-toys-agents-openai-yoshua-bengio/</guid><pubDate>Wed, 25 Jun 2025 09:44:46 +0000</pubDate></item><item><title>The Bank Secrecy Act is failing everyone. It’s time to rethink financial surveillance. (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1119324/katie-haun-bank-secrecy-act-oped/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/GettyImages-811716778.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The US is on the brink of enacting rules for digital assets, with growing bipartisan momentum to modernize our financial system. But amid all the talk about innovation and global competitiveness, one issue has been glaringly absent: financial privacy. As we build the digital infrastructure of the 21st century, we need to talk about not just what’s possible but what’s acceptable. That means confronting the expanding surveillance powers quietly embedded in our financial system, which today can track nearly every transaction without a warrant.&lt;/p&gt;&lt;p&gt;Many Americans may associate financial surveillance with authoritarian regimes. Yet because of a Nixon-era law called the Bank Secrecy Act (BSA) and the digitization of finance over the past half-century, financial privacy is under increasingly serious threat here at home. Most Americans don’t realize they live under an expansive surveillance regime that likely violates their constitutional rights. Every purchase, deposit, and transaction, from the smallest Venmo payment for a coffee to a large hospital bill, creates a data point in a system that watches you—even if you’ve done nothing wrong.&lt;/p&gt;&lt;p&gt;As a former federal prosecutor, I care deeply about giving law enforcement the tools it needs to keep us safe. But the status quo doesn’t make us safer. It creates a false sense of security while quietly and permanently eroding the constitutional rights of millions of Americans.&lt;/p&gt;&lt;p&gt;When Congress enacted the BSA in 1970, cash was king and organized crime was the target. The law created a scheme whereby, ever since, banks have been required to keep certain records on their customers and turn them over to law enforcement upon request. Unlike a search warrant, which must be issued by a judge or magistrate upon a showing of probable cause that a crime was committed and that specific evidence of that crime exists in the place to be searched, this power is exercised with no checks or balances. A prosecutor can “cut a subpoena”—demanding all your bank records for the past 10 years—with no judicial oversight or limitation on scope, and at no cost to the government. The burden falls entirely on the bank. In contrast, a proper search warrant must be narrowly tailored, with probable cause and judicial authorization.&lt;/p&gt;&lt;p&gt;In &lt;em&gt;United States v. Miller&lt;/em&gt; (1976), the Supreme Court upheld the BSA, reasoning that citizens have no “legitimate expectation of privacy” about information shared with third parties, like banks. Thus began the third-party doctrine, enabling law enforcement to access financial records without a warrant. The BSA has been amended several times over the years (most notoriously in 2001 as a part of the Patriot Act), imposing an ever-growing list of recordkeeping obligations on an ever-growing list of financial institutions. Today, it is virtually inescapable for everyday Americans.&lt;/p&gt;&lt;p&gt;In the 1970s, when the BSA was enacted, banking and noncash payments were conducted predominantly through physical means: writing checks, visiting bank branches, and using passbooks. For cash transactions, the BSA required reporting of transactions over the kingly sum of $10,000, a figure that was not pegged to inflation and remains the same today. And given the nature of banking services and the technology available at the time, individuals conducted just a handful of noncash payments per month. Today, consumers make at least one payment or banking transaction a day, and just an estimated 16% of those are in cash.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, emerging technologies further expand the footprint of financial data. Add to this the massive pools of personal information already collected by technology platforms—location history, search activity, communications metadata—and you create a world where financial surveillance can be linked to virtually every aspect of your identity, movement, and behavior.&lt;/p&gt;&lt;p&gt;Nor does the BSA actually appear to be effective at achieving its aims. In fiscal year 2024, financial institutions filed about 4.7 million Suspicious Activity Reports (SARs) and over 20 million currency transaction reports. Instead of stopping major crime, the system floods law enforcement with low-value information, overwhelming agents and obscuring real threats. Mass surveillance often reduces effectiveness by drowning law enforcement in noise. But while it doesn’t stop hackers, the BSA creates a trove of permanent info on everyone.&lt;/p&gt;&lt;p&gt;Worse still, the incentives are misaligned and asymmetrical. To avoid liability, financial institutions are required to report anything remotely suspicious. If they fail to file a SAR, they risk serious penalties—even indictment. But they face no consequences for overreporting. The vast overcollection of data is the unsurprising result. These practices, developed under regulations, require clearer guardrails so that executive branch actors can more safely outsource surveillance duties to private institutions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;But courts have recognized that constitutional privacy must evolve alongside technology. In 2012, the Supreme Court ruled in &lt;em&gt;United States v. Jones&lt;/em&gt; that attaching a GPS tracker to a vehicle for prolonged surveillance constituted a search restricted by the Fourth Amendment. Justice Sonia Sotomayor, in a notable concurrence, argued that the third-party doctrine was ill suited to an era when individuals "reveal a great deal of information about themselves to third parties" merely by participating in daily life.&lt;/p&gt;&lt;p&gt;This legal evolution continued in 2018, when the Supreme Court held in &lt;em&gt;Carpenter v. United States &lt;/em&gt;that accessing historical cell-phone location records held by a third party required a warrant, recognizing that “seismic shifts in digital technology” necessitate stronger protections and warning that “the fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.”&lt;/p&gt;&lt;p&gt;The logic of &lt;em&gt;Carpenter&lt;/em&gt; applies directly to the mass of financial records being collected today. Just as tracking a person’s phone over time reveals the “whole of their physical movements,” tracking a person’s financial life exposes travel, daily patterns, medical treatments, political affiliations, and personal associations. In many ways, because of the velocity and digital nature of today’s digital payments, financial data is among the most personal and revealing data there is—and therefore deserves the highest level of constitutional protection.&lt;/p&gt;&lt;p&gt;Though &lt;em&gt;Miller&lt;/em&gt; remains formally intact, the writing is on the wall: Indiscriminate financial surveillance such as what we have today is fundamentally at odds with the Fourth Amendment in the digital age.&lt;/p&gt;&lt;p&gt;Technological innovations over the past several decades have brought incredible convenience to economic life. Now our privacy standards must catch up. With Congress considering landmark legislation on digital assets, it’s an important moment to consider what kind of financial system we want—not just in terms of efficiency and access, but in terms of freedom. Rather than striking down the BSA in its entirety, policymakers should narrow its reach, particularly around the bulk collection and warrantless sharing of Americans’ financial data.&lt;/p&gt;  &lt;p&gt;Financial surveillance shouldn’t be the price of participation in modern life. The systems we build now will shape what freedom looks like for the next century. It’s time to treat financial privacy like what it is: a cornerstone of democracy, and a right worth fighting for.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Katie Haun is the CEO and founder of Haun Ventures, a venture capital firm focused on frontier technologies. She is a former federal prosecutor who created the US Justice Department’s first cryptocurrency task force. She led investigations into the Mt. Gox hack and the corrupt agents on the Silk Road task force. She clerked for US Supreme Court Justice Anthony Kennedy and is an honors graduate of Stanford Law School.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/GettyImages-811716778.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;The US is on the brink of enacting rules for digital assets, with growing bipartisan momentum to modernize our financial system. But amid all the talk about innovation and global competitiveness, one issue has been glaringly absent: financial privacy. As we build the digital infrastructure of the 21st century, we need to talk about not just what’s possible but what’s acceptable. That means confronting the expanding surveillance powers quietly embedded in our financial system, which today can track nearly every transaction without a warrant.&lt;/p&gt;&lt;p&gt;Many Americans may associate financial surveillance with authoritarian regimes. Yet because of a Nixon-era law called the Bank Secrecy Act (BSA) and the digitization of finance over the past half-century, financial privacy is under increasingly serious threat here at home. Most Americans don’t realize they live under an expansive surveillance regime that likely violates their constitutional rights. Every purchase, deposit, and transaction, from the smallest Venmo payment for a coffee to a large hospital bill, creates a data point in a system that watches you—even if you’ve done nothing wrong.&lt;/p&gt;&lt;p&gt;As a former federal prosecutor, I care deeply about giving law enforcement the tools it needs to keep us safe. But the status quo doesn’t make us safer. It creates a false sense of security while quietly and permanently eroding the constitutional rights of millions of Americans.&lt;/p&gt;&lt;p&gt;When Congress enacted the BSA in 1970, cash was king and organized crime was the target. The law created a scheme whereby, ever since, banks have been required to keep certain records on their customers and turn them over to law enforcement upon request. Unlike a search warrant, which must be issued by a judge or magistrate upon a showing of probable cause that a crime was committed and that specific evidence of that crime exists in the place to be searched, this power is exercised with no checks or balances. A prosecutor can “cut a subpoena”—demanding all your bank records for the past 10 years—with no judicial oversight or limitation on scope, and at no cost to the government. The burden falls entirely on the bank. In contrast, a proper search warrant must be narrowly tailored, with probable cause and judicial authorization.&lt;/p&gt;&lt;p&gt;In &lt;em&gt;United States v. Miller&lt;/em&gt; (1976), the Supreme Court upheld the BSA, reasoning that citizens have no “legitimate expectation of privacy” about information shared with third parties, like banks. Thus began the third-party doctrine, enabling law enforcement to access financial records without a warrant. The BSA has been amended several times over the years (most notoriously in 2001 as a part of the Patriot Act), imposing an ever-growing list of recordkeeping obligations on an ever-growing list of financial institutions. Today, it is virtually inescapable for everyday Americans.&lt;/p&gt;&lt;p&gt;In the 1970s, when the BSA was enacted, banking and noncash payments were conducted predominantly through physical means: writing checks, visiting bank branches, and using passbooks. For cash transactions, the BSA required reporting of transactions over the kingly sum of $10,000, a figure that was not pegged to inflation and remains the same today. And given the nature of banking services and the technology available at the time, individuals conducted just a handful of noncash payments per month. Today, consumers make at least one payment or banking transaction a day, and just an estimated 16% of those are in cash.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Meanwhile, emerging technologies further expand the footprint of financial data. Add to this the massive pools of personal information already collected by technology platforms—location history, search activity, communications metadata—and you create a world where financial surveillance can be linked to virtually every aspect of your identity, movement, and behavior.&lt;/p&gt;&lt;p&gt;Nor does the BSA actually appear to be effective at achieving its aims. In fiscal year 2024, financial institutions filed about 4.7 million Suspicious Activity Reports (SARs) and over 20 million currency transaction reports. Instead of stopping major crime, the system floods law enforcement with low-value information, overwhelming agents and obscuring real threats. Mass surveillance often reduces effectiveness by drowning law enforcement in noise. But while it doesn’t stop hackers, the BSA creates a trove of permanent info on everyone.&lt;/p&gt;&lt;p&gt;Worse still, the incentives are misaligned and asymmetrical. To avoid liability, financial institutions are required to report anything remotely suspicious. If they fail to file a SAR, they risk serious penalties—even indictment. But they face no consequences for overreporting. The vast overcollection of data is the unsurprising result. These practices, developed under regulations, require clearer guardrails so that executive branch actors can more safely outsource surveillance duties to private institutions.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;But courts have recognized that constitutional privacy must evolve alongside technology. In 2012, the Supreme Court ruled in &lt;em&gt;United States v. Jones&lt;/em&gt; that attaching a GPS tracker to a vehicle for prolonged surveillance constituted a search restricted by the Fourth Amendment. Justice Sonia Sotomayor, in a notable concurrence, argued that the third-party doctrine was ill suited to an era when individuals "reveal a great deal of information about themselves to third parties" merely by participating in daily life.&lt;/p&gt;&lt;p&gt;This legal evolution continued in 2018, when the Supreme Court held in &lt;em&gt;Carpenter v. United States &lt;/em&gt;that accessing historical cell-phone location records held by a third party required a warrant, recognizing that “seismic shifts in digital technology” necessitate stronger protections and warning that “the fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.”&lt;/p&gt;&lt;p&gt;The logic of &lt;em&gt;Carpenter&lt;/em&gt; applies directly to the mass of financial records being collected today. Just as tracking a person’s phone over time reveals the “whole of their physical movements,” tracking a person’s financial life exposes travel, daily patterns, medical treatments, political affiliations, and personal associations. In many ways, because of the velocity and digital nature of today’s digital payments, financial data is among the most personal and revealing data there is—and therefore deserves the highest level of constitutional protection.&lt;/p&gt;&lt;p&gt;Though &lt;em&gt;Miller&lt;/em&gt; remains formally intact, the writing is on the wall: Indiscriminate financial surveillance such as what we have today is fundamentally at odds with the Fourth Amendment in the digital age.&lt;/p&gt;&lt;p&gt;Technological innovations over the past several decades have brought incredible convenience to economic life. Now our privacy standards must catch up. With Congress considering landmark legislation on digital assets, it’s an important moment to consider what kind of financial system we want—not just in terms of efficiency and access, but in terms of freedom. Rather than striking down the BSA in its entirety, policymakers should narrow its reach, particularly around the bulk collection and warrantless sharing of Americans’ financial data.&lt;/p&gt;  &lt;p&gt;Financial surveillance shouldn’t be the price of participation in modern life. The systems we build now will shape what freedom looks like for the next century. It’s time to treat financial privacy like what it is: a cornerstone of democracy, and a right worth fighting for.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Katie Haun is the CEO and founder of Haun Ventures, a venture capital firm focused on frontier technologies. She is a former federal prosecutor who created the US Justice Department’s first cryptocurrency task force. She led investigations into the Mt. Gox hack and the corrupt agents on the Silk Road task force. She clerked for US Supreme Court Justice Anthony Kennedy and is an honors graduate of Stanford Law School.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1119324/katie-haun-bank-secrecy-act-oped/</guid><pubDate>Wed, 25 Jun 2025 09:55:31 +0000</pubDate></item><item><title>The Download: Introducing the Power issue (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1119339/the-download-introducing-the-power-issue/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Introducing: the Power issue&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Energy is power. Those who can produce it, especially lots of it, get to exert authority in all sorts of ways.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The world is increasingly powered by both tangible electricity and intangible intelligence. Plus billionaires. The latest issue of MIT Technology Review explores those intersections, in all their forms.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Here’s just a taster of what you can expect from our latest issue:&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Are we ready to hand AI agents the keys?&lt;/strong&gt; We’re starting to give AI agents real autonomy, and we’re not prepared for what could happen next. Read the full story.&lt;/p&gt;&lt;p&gt;+ In Nebraska, a publicly owned electricity distribution system is an effective lens through which to examine the grid of the near future.&lt;/p&gt;&lt;p&gt;+ Cases of cancer, heart disease, and respiratory illnesses are on the rise in the area surrounding Puerto Rico’s only coal-fired power station. So why has it just been given permission to stay open for at least another seven years? Read the full story.&lt;/p&gt;&lt;p&gt;+ How AI is shaking up urban planning and helping make cities better.&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Tech billionaires are making a risky bet with humanity’s future.&lt;/strong&gt; They say they want to save humanity by creating superintelligent AI—but a new book argues that they’re steering humanity in a dangerous direction.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The Bank Secrecy Act is failing everyone. It’s time to rethink financial surveillance.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Katie Haun is the CEO and founder of Haun Ventures, a venture capital firm focused on frontier technologies.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;The US is on the brink of enacting rules for digital assets, with growing bipartisan momentum to modernize its financial system. But amid all the talk about innovation and global competitiveness, one issue has been glaringly absent: financial privacy.&lt;/p&gt;&lt;p&gt;As we build the digital infrastructure of the 21st century, we need to talk about not just what’s possible but what’s acceptable. That means confronting the expanding surveillance powers quietly embedded in our financial system, which today can track nearly every transaction without a warrant. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Copyrighted books are fair use for AI training&lt;/strong&gt;&lt;br /&gt;According to a federal court in the US. (WP $)&lt;br /&gt;+ &lt;em&gt;The court compared the way AI learns to how humans consume books. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;But pirating is still illegal, apparently&lt;/em&gt;. (404 Media)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;2&lt;/strong&gt; &lt;strong&gt;Recruiters are drowning in AI-generated résumés&lt;/strong&gt;&lt;br /&gt;Fake identities, agent-led applications, and identical résumés abound. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Extreme heat in the US is a growing threat&lt;/strong&gt;&lt;br /&gt;Alaska recently issued its first-ever heat advisory. (Vox)&lt;br /&gt;+ &lt;em&gt;And the heatwave is only going to intensify. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Here’s how much heat your body can take. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Big Balls no longer works for DOGE&lt;br /&gt;&lt;/strong&gt;One of the department’s most prominent hires has resigned. (Wired $)&lt;br /&gt;+ &lt;em&gt;What will he do next? &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 One of America’s best hackers is a bot&lt;/strong&gt;&lt;br /&gt;It’s the first time an AI has topped a hacking leaderboard by reputation. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Cyberattacks by AI agents are coming. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;6 Way fewer people are dying of heart attacks in the US&lt;br /&gt;&lt;/strong&gt;But deaths from chronic heart conditions are on the up. (New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 TikTok’s moderators have had enough&lt;br /&gt;&lt;/strong&gt;Groups are unionizing across the world to push for better treatment. (Rest of World)&lt;br /&gt;+ &lt;em&gt;How an undercover content moderator polices the metaverse. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Donald Trump’s social media use is even more erratic than usual&lt;/strong&gt;&lt;br /&gt;He keeps signing off “thank you for your attention to this matter!” (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;He’s also misspelling his name as ‘Donakd.’ &lt;/em&gt;(Fast Company $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Finally, a use for your old smartphone&lt;/strong&gt;&lt;br /&gt;It could have a second life as a teeny tiny data center. (IEEE Spectrum)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 AI models don’t understand Gen Alpha slang&lt;br /&gt;&lt;/strong&gt;Let him cook! (404 Media)&lt;br /&gt;+ &lt;em&gt;That’s not stopping youngsters from using models as advisors, though. &lt;/em&gt;(Fast Company $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Humans are wired to bond, and when we feel seen and soothed—even by a machine—we connect.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Psychiatrist Nina Vasan explains why humans may end up falling in love with AI systems to the Wall Street Journal.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfdtdKBopjxmw2FVUSbrhW_dlkHmlS20H6ATV4tpp4BWwVaiyvuD1U1l_nN0qEyu2HFrnEm8-cXcnNMS3EwhNtMBGbcto9-0lmbHgCm0JC3eHC-Of2I4K_wtoOWtsG2IKTy1xrE7g?key=Oio6VS2Nc5Dz8s3svWgiBA" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How Wi-Fi sensing became usable tech&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Wi-Fi sensing is a tantalizing concept: that the same routers bringing you the internet could also detect your movements. But, as a way to monitor health, it’s mostly been eclipsed by other technologies, like ultra-wideband radar.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Despite that, Wi-Fi sensing hasn’t gone away. Instead, it has quietly become available in millions of homes, supported by leading internet service providers, smart-home companies, and chip manufacturers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Soon it could be invisibly monitoring our day-to-day movements for all sorts of surprising—and sometimes alarming—purposes. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Meg Duff&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;+&amp;nbsp; How to keep your cool in a heatwave.&lt;br /&gt;+ Roblox fans can’t get enough of, err, gardening.&lt;br /&gt;+ Kate Moss, you are the reigning queen of festival fashion.&lt;br /&gt;+ A couple of intrepid brown bears managed to escape from a wildlife park in the UK—to consume a week’s worth of honey 🐻🍯&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Introducing: the Power issue&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Energy is power. Those who can produce it, especially lots of it, get to exert authority in all sorts of ways.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The world is increasingly powered by both tangible electricity and intangible intelligence. Plus billionaires. The latest issue of MIT Technology Review explores those intersections, in all their forms.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Here’s just a taster of what you can expect from our latest issue:&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Are we ready to hand AI agents the keys?&lt;/strong&gt; We’re starting to give AI agents real autonomy, and we’re not prepared for what could happen next. Read the full story.&lt;/p&gt;&lt;p&gt;+ In Nebraska, a publicly owned electricity distribution system is an effective lens through which to examine the grid of the near future.&lt;/p&gt;&lt;p&gt;+ Cases of cancer, heart disease, and respiratory illnesses are on the rise in the area surrounding Puerto Rico’s only coal-fired power station. So why has it just been given permission to stay open for at least another seven years? Read the full story.&lt;/p&gt;&lt;p&gt;+ How AI is shaking up urban planning and helping make cities better.&lt;/p&gt;&lt;p&gt;+ &lt;strong&gt;Tech billionaires are making a risky bet with humanity’s future.&lt;/strong&gt; They say they want to save humanity by creating superintelligent AI—but a new book argues that they’re steering humanity in a dangerous direction.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The Bank Secrecy Act is failing everyone. It’s time to rethink financial surveillance.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Katie Haun is the CEO and founder of Haun Ventures, a venture capital firm focused on frontier technologies.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;The US is on the brink of enacting rules for digital assets, with growing bipartisan momentum to modernize its financial system. But amid all the talk about innovation and global competitiveness, one issue has been glaringly absent: financial privacy.&lt;/p&gt;&lt;p&gt;As we build the digital infrastructure of the 21st century, we need to talk about not just what’s possible but what’s acceptable. That means confronting the expanding surveillance powers quietly embedded in our financial system, which today can track nearly every transaction without a warrant. Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Copyrighted books are fair use for AI training&lt;/strong&gt;&lt;br /&gt;According to a federal court in the US. (WP $)&lt;br /&gt;+ &lt;em&gt;The court compared the way AI learns to how humans consume books. &lt;/em&gt;(WSJ $)&lt;br /&gt;+ &lt;em&gt;But pirating is still illegal, apparently&lt;/em&gt;. (404 Media)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;2&lt;/strong&gt; &lt;strong&gt;Recruiters are drowning in AI-generated résumés&lt;/strong&gt;&lt;br /&gt;Fake identities, agent-led applications, and identical résumés abound. (NYT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Extreme heat in the US is a growing threat&lt;/strong&gt;&lt;br /&gt;Alaska recently issued its first-ever heat advisory. (Vox)&lt;br /&gt;+ &lt;em&gt;And the heatwave is only going to intensify. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Here’s how much heat your body can take. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Big Balls no longer works for DOGE&lt;br /&gt;&lt;/strong&gt;One of the department’s most prominent hires has resigned. (Wired $)&lt;br /&gt;+ &lt;em&gt;What will he do next? &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 One of America’s best hackers is a bot&lt;/strong&gt;&lt;br /&gt;It’s the first time an AI has topped a hacking leaderboard by reputation. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Cyberattacks by AI agents are coming. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;6 Way fewer people are dying of heart attacks in the US&lt;br /&gt;&lt;/strong&gt;But deaths from chronic heart conditions are on the up. (New Scientist $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 TikTok’s moderators have had enough&lt;br /&gt;&lt;/strong&gt;Groups are unionizing across the world to push for better treatment. (Rest of World)&lt;br /&gt;+ &lt;em&gt;How an undercover content moderator polices the metaverse. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Donald Trump’s social media use is even more erratic than usual&lt;/strong&gt;&lt;br /&gt;He keeps signing off “thank you for your attention to this matter!” (The Atlantic $)&lt;br /&gt;+ &lt;em&gt;He’s also misspelling his name as ‘Donakd.’ &lt;/em&gt;(Fast Company $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Finally, a use for your old smartphone&lt;/strong&gt;&lt;br /&gt;It could have a second life as a teeny tiny data center. (IEEE Spectrum)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 AI models don’t understand Gen Alpha slang&lt;br /&gt;&lt;/strong&gt;Let him cook! (404 Media)&lt;br /&gt;+ &lt;em&gt;That’s not stopping youngsters from using models as advisors, though. &lt;/em&gt;(Fast Company $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Humans are wired to bond, and when we feel seen and soothed—even by a machine—we connect.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Psychiatrist Nina Vasan explains why humans may end up falling in love with AI systems to the Wall Street Journal.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image"&gt;&lt;img alt="alt" src="https://lh7-rt.googleusercontent.com/docsz/AD_4nXfdtdKBopjxmw2FVUSbrhW_dlkHmlS20H6ATV4tpp4BWwVaiyvuD1U1l_nN0qEyu2HFrnEm8-cXcnNMS3EwhNtMBGbcto9-0lmbHgCm0JC3eHC-Of2I4K_wtoOWtsG2IKTy1xrE7g?key=Oio6VS2Nc5Dz8s3svWgiBA" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How Wi-Fi sensing became usable tech&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Wi-Fi sensing is a tantalizing concept: that the same routers bringing you the internet could also detect your movements. But, as a way to monitor health, it’s mostly been eclipsed by other technologies, like ultra-wideband radar.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Despite that, Wi-Fi sensing hasn’t gone away. Instead, it has quietly become available in millions of homes, supported by leading internet service providers, smart-home companies, and chip manufacturers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Soon it could be invisibly monitoring our day-to-day movements for all sorts of surprising—and sometimes alarming—purposes. Read the full story.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Meg Duff&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;+&amp;nbsp; How to keep your cool in a heatwave.&lt;br /&gt;+ Roblox fans can’t get enough of, err, gardening.&lt;br /&gt;+ Kate Moss, you are the reigning queen of festival fashion.&lt;br /&gt;+ A couple of intrepid brown bears managed to escape from a wildlife park in the UK—to consume a week’s worth of honey 🐻🍯&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1119339/the-download-introducing-the-power-issue/</guid><pubDate>Wed, 25 Jun 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Forget about AI costs: Google just changed the game with open-source Gemini CLI that will be free for most developers (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-is-redefining-enterprise-ai-economics-with-open-source-gemini-cli-that-will-be-free-for-the-majority-of-developers/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;For power users and many developers, the command line is the foundational interface for controlling a system and its applications.&lt;/p&gt;



&lt;p&gt;Also sometimes referred to as a terminal, the command line interface (CLI) is how users issue commands and build applications as an alternative, or as a complement, to an integrated developer environment (IDE) tool. While it might seem almost anachronistic that a text-only interface accessible with a keyboard (CLI doesn’t even use a mouse) can be modern, it remains a mainstay of developers around the world. In the modern era of generative AI, it’s becoming more powerful too.&lt;/p&gt;



&lt;p&gt;Today Google announced its open-source Gemini-CLI that brings natural language command execution directly to developer terminals. Beyond natural language, it brings the power of Google’s Gemini Pro 2.5 — and it does it mostly for free.&lt;/p&gt;



&lt;p&gt;The free tier provides 60 model requests per minute and 1,000 requests per day at no charge, limits that Google deliberately set above typical developer usage patterns. Google first measured its own developers’ usage patterns, then doubled that number to set the 1,000 limit.&lt;/p&gt;



&lt;p&gt;“To be very clear, for the vast majority of developers, Gemini CLI will be completely free of charge,” Ryan J. Salva, senior director for product management at Google, said in response to a question from VentureBeat during a press briefing. “We do not want you having to watch that token meter like it’s a taxi meter and holding back on your creativity.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-google-gemini-cli-disrupts-the-enterprise-ai-market"&gt;How Google Gemini CLI disrupts the enterprise AI market&lt;/h2&gt;



&lt;p&gt;Gemini CLI is far from being the first or only AI tool for the command line. OpenAI Codex has a CLI version, as does Anthropic with Claude Code.&lt;/p&gt;



&lt;p&gt;Google Gemini CLI, however, is quite different from its two primary commercial rivals in that the tool is open source under the Apache 2.0 license. Then, of course, is the cost. While Gemini CLI is mostly free, OpenAI and Anthropic’s tools are not.&lt;/p&gt;



&lt;p&gt;In response to another question from VentureBeat,  Google senior staff software engineer Taylor Mullen said he expects that Gemini CLI will be more widely used, simply because it is free. He noted that many users will not use OpenAI Codex or Claude code for just any task, as it carries a cost.&lt;/p&gt;



&lt;p&gt;“Being able to amplify literally anything and everything means it’s woven into the fabric of so much more of your workflow,” Mullen said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-extensibility-through-model-context-protocol-and-custom-extensions"&gt;Extensibility through Model Context Protocol and custom extensions&lt;/h2&gt;



&lt;p&gt;Another key differentiator for Gemini CLI lies in its extensibility architecture, built around the emerging Model Context Protocol (MCP) standard. This approach lets developers connect external services and add new capabilities and positions the tool as a platform rather than a single-purpose application.&lt;/p&gt;



&lt;p&gt;During the briefing, Google demonstrated this extensibility through a pre-recorded video showing Gemini CLI integrated with Google’s creative AI tools. An agent creating a cat video set in Australia first generated images using Imagen APIs, then wove them into an animated video using Veo technology.&lt;/p&gt;



&lt;p&gt;The extensibility model includes three layers: Built-in MCP server support, bundled extensions that combine MCP servers with configuration files and custom Gemini.md files for project-specific customization. This architecture allows individual developers to tailor their experience while enabling teams to standardize workflows across projects.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-google-starts-charging-enterprise-features-and-scale"&gt;Where Google starts charging: Enterprise features and scale&lt;/h2&gt;



&lt;p&gt;While individual developers enjoy generous free access, Google’s monetization strategy becomes clear for enterprise use cases. The company maintains a clear delineation between free individual use and paid enterprise features.&lt;/p&gt;



&lt;p&gt;Accessing Gemini CLI only requires a Google login. It does not require any sort of API key or credit card on file in order to use. While there is a very generous free tier, there can be costs involved for enterprise users.&lt;/p&gt;



&lt;p&gt;Salva noted that if an organization wants to run multiple Gemini CLI agents in parallel, or if there are specific policy, governance or data residency requirements, a paid API key comes in. The key could be for access to Google Vertex AI, which provides commercial access to a series of models including, but not limited to, Gemini Pro 2.5&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-technical-architecture-and-security-model"&gt;Technical architecture and security model&lt;/h2&gt;



&lt;p&gt;Gemini CLI operates as a local agent with built-in security measures that address common concerns about AI command execution. The system requires explicit user confirmation for each command, with options to “allow once,” “always allow” or deny specific operations.&lt;/p&gt;



&lt;p&gt;The tool’s security model includes multiple layers of protection. Users can use native macOS Seatbelt support for sandboxing, run the agent in Docker or Podman containers, and route all network traffic through proxies for inspection. The open-source nature under Apache 2.0 licensing allows complete code auditing.&lt;/p&gt;



&lt;p&gt;“You have complete transparency into it,” Salva noted. “The tool only has access to the information that you explicitly provide in a prompt or a reference file path and you decide what context to share with the model on a prompt by prompt by prompt basis.”&lt;/p&gt;



&lt;p&gt;While Gemini CLI runs as a local agent it’s important to note that it doesn’t currently run the models locally. That is, the Gemini Pro 2.5 model is accessed from the cloud and Google is not providing support to run a local model. Mullen noted that although there is a subset of tasks which could probably be done with a local model, Google is not shipping local model support today.&lt;/p&gt;







&lt;p&gt;For enterprises looking to lead in AI, the extremely generous free tier for Gemini CLI will be an option that should be considered for some use cases.&lt;/p&gt;



&lt;p&gt;To be clear, it’s not a full enterprise system, but it’s the foundation on which enterprise application and agentic AI systems can be developed. For individual developers within enterprises, it represents a no-barrier entry for AI access. The open-source architecture addresses common enterprise security concerns by enabling complete code auditing and on-premises deployment options. Organizations can evaluate production-grade AI capabilities without vendor lock-in risks or complex procurement cycles.&lt;/p&gt;



&lt;p&gt;“It doesn’t matter if you’ve got dust or dollars, whether you’re a student, hobbyist, a freelancer or a developer at a very well funded company, you should have access to the same tools,” said Salva. “So that is why we’re making Gemini CLI free with genuinely unmatched usage limits right from the get go.”&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;For power users and many developers, the command line is the foundational interface for controlling a system and its applications.&lt;/p&gt;



&lt;p&gt;Also sometimes referred to as a terminal, the command line interface (CLI) is how users issue commands and build applications as an alternative, or as a complement, to an integrated developer environment (IDE) tool. While it might seem almost anachronistic that a text-only interface accessible with a keyboard (CLI doesn’t even use a mouse) can be modern, it remains a mainstay of developers around the world. In the modern era of generative AI, it’s becoming more powerful too.&lt;/p&gt;



&lt;p&gt;Today Google announced its open-source Gemini-CLI that brings natural language command execution directly to developer terminals. Beyond natural language, it brings the power of Google’s Gemini Pro 2.5 — and it does it mostly for free.&lt;/p&gt;



&lt;p&gt;The free tier provides 60 model requests per minute and 1,000 requests per day at no charge, limits that Google deliberately set above typical developer usage patterns. Google first measured its own developers’ usage patterns, then doubled that number to set the 1,000 limit.&lt;/p&gt;



&lt;p&gt;“To be very clear, for the vast majority of developers, Gemini CLI will be completely free of charge,” Ryan J. Salva, senior director for product management at Google, said in response to a question from VentureBeat during a press briefing. “We do not want you having to watch that token meter like it’s a taxi meter and holding back on your creativity.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-google-gemini-cli-disrupts-the-enterprise-ai-market"&gt;How Google Gemini CLI disrupts the enterprise AI market&lt;/h2&gt;



&lt;p&gt;Gemini CLI is far from being the first or only AI tool for the command line. OpenAI Codex has a CLI version, as does Anthropic with Claude Code.&lt;/p&gt;



&lt;p&gt;Google Gemini CLI, however, is quite different from its two primary commercial rivals in that the tool is open source under the Apache 2.0 license. Then, of course, is the cost. While Gemini CLI is mostly free, OpenAI and Anthropic’s tools are not.&lt;/p&gt;



&lt;p&gt;In response to another question from VentureBeat,  Google senior staff software engineer Taylor Mullen said he expects that Gemini CLI will be more widely used, simply because it is free. He noted that many users will not use OpenAI Codex or Claude code for just any task, as it carries a cost.&lt;/p&gt;



&lt;p&gt;“Being able to amplify literally anything and everything means it’s woven into the fabric of so much more of your workflow,” Mullen said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-extensibility-through-model-context-protocol-and-custom-extensions"&gt;Extensibility through Model Context Protocol and custom extensions&lt;/h2&gt;



&lt;p&gt;Another key differentiator for Gemini CLI lies in its extensibility architecture, built around the emerging Model Context Protocol (MCP) standard. This approach lets developers connect external services and add new capabilities and positions the tool as a platform rather than a single-purpose application.&lt;/p&gt;



&lt;p&gt;During the briefing, Google demonstrated this extensibility through a pre-recorded video showing Gemini CLI integrated with Google’s creative AI tools. An agent creating a cat video set in Australia first generated images using Imagen APIs, then wove them into an animated video using Veo technology.&lt;/p&gt;



&lt;p&gt;The extensibility model includes three layers: Built-in MCP server support, bundled extensions that combine MCP servers with configuration files and custom Gemini.md files for project-specific customization. This architecture allows individual developers to tailor their experience while enabling teams to standardize workflows across projects.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-google-starts-charging-enterprise-features-and-scale"&gt;Where Google starts charging: Enterprise features and scale&lt;/h2&gt;



&lt;p&gt;While individual developers enjoy generous free access, Google’s monetization strategy becomes clear for enterprise use cases. The company maintains a clear delineation between free individual use and paid enterprise features.&lt;/p&gt;



&lt;p&gt;Accessing Gemini CLI only requires a Google login. It does not require any sort of API key or credit card on file in order to use. While there is a very generous free tier, there can be costs involved for enterprise users.&lt;/p&gt;



&lt;p&gt;Salva noted that if an organization wants to run multiple Gemini CLI agents in parallel, or if there are specific policy, governance or data residency requirements, a paid API key comes in. The key could be for access to Google Vertex AI, which provides commercial access to a series of models including, but not limited to, Gemini Pro 2.5&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-technical-architecture-and-security-model"&gt;Technical architecture and security model&lt;/h2&gt;



&lt;p&gt;Gemini CLI operates as a local agent with built-in security measures that address common concerns about AI command execution. The system requires explicit user confirmation for each command, with options to “allow once,” “always allow” or deny specific operations.&lt;/p&gt;



&lt;p&gt;The tool’s security model includes multiple layers of protection. Users can use native macOS Seatbelt support for sandboxing, run the agent in Docker or Podman containers, and route all network traffic through proxies for inspection. The open-source nature under Apache 2.0 licensing allows complete code auditing.&lt;/p&gt;



&lt;p&gt;“You have complete transparency into it,” Salva noted. “The tool only has access to the information that you explicitly provide in a prompt or a reference file path and you decide what context to share with the model on a prompt by prompt by prompt basis.”&lt;/p&gt;



&lt;p&gt;While Gemini CLI runs as a local agent it’s important to note that it doesn’t currently run the models locally. That is, the Gemini Pro 2.5 model is accessed from the cloud and Google is not providing support to run a local model. Mullen noted that although there is a subset of tasks which could probably be done with a local model, Google is not shipping local model support today.&lt;/p&gt;







&lt;p&gt;For enterprises looking to lead in AI, the extremely generous free tier for Gemini CLI will be an option that should be considered for some use cases.&lt;/p&gt;



&lt;p&gt;To be clear, it’s not a full enterprise system, but it’s the foundation on which enterprise application and agentic AI systems can be developed. For individual developers within enterprises, it represents a no-barrier entry for AI access. The open-source architecture addresses common enterprise security concerns by enabling complete code auditing and on-premises deployment options. Organizations can evaluate production-grade AI capabilities without vendor lock-in risks or complex procurement cycles.&lt;/p&gt;



&lt;p&gt;“It doesn’t matter if you’ve got dust or dollars, whether you’re a student, hobbyist, a freelancer or a developer at a very well funded company, you should have access to the same tools,” said Salva. “So that is why we’re making Gemini CLI free with genuinely unmatched usage limits right from the get go.”&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-is-redefining-enterprise-ai-economics-with-open-source-gemini-cli-that-will-be-free-for-the-majority-of-developers/</guid><pubDate>Wed, 25 Jun 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Ring cameras and doorbells now use AI to provide specific descriptions of motion activity (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/ring-cameras-and-doorbells-now-use-ai-to-provide-specific-descriptions-of-motion-activity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/ring_battery_doorbell_2024.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon-owned Ring announced on Wednesday that it’s introducing a new AI-powered feature to its doorbells and cameras, which offers users specific text descriptions of current motion activity. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, when users receive real-time notifications about happenings at their property, the updates will be more descriptive. For instance, “A person is walking up the steps with a black dog,” or “Two individuals are looking into a white car parked in the driveway.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The feature aims to improve upon the vague notifications that were previously available. Now, users will be able to know exactly what is happening and can quickly decide whether it requires immediate attention. It should be noted that the AI only describes the first few seconds of the motion-activated video clip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is being rolled out today as an English-only beta for Ring Home Premium subscribers in the U.S. and Canada. Users can choose to disable the feature by going to settings in the Ring app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a blog post by Jamie Siminoff, Ring’s founder and now Amazon’s VP of home security, the company plans to introduce additional AI features. One of these combines several motion events happening in and around a home into a single alert.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ring also intends to implement a “custom anomaly alert” that allows users to define what constitutes an anomaly for their property, enabling the camera to notify them when such an event occurs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Siminoff mentioned that Ring will “learn your routine,” so it can inform users when something is out of the ordinary. This may be unsettling for some users, especially considering Ring’s past privacy concerns.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“We are just starting to scratch the surface of AI. I feel like we are back to the very early days of Ring again—I see unlimited potential for new experiences we can invent for our neighbors,” Siminoff wrote.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement follows the recent launch of Ring’s AI-powered search feature, which enables users to locate specific moments within video recordings.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/ring_battery_doorbell_2024.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon-owned Ring announced on Wednesday that it’s introducing a new AI-powered feature to its doorbells and cameras, which offers users specific text descriptions of current motion activity. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, when users receive real-time notifications about happenings at their property, the updates will be more descriptive. For instance, “A person is walking up the steps with a black dog,” or “Two individuals are looking into a white car parked in the driveway.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The feature aims to improve upon the vague notifications that were previously available. Now, users will be able to know exactly what is happening and can quickly decide whether it requires immediate attention. It should be noted that the AI only describes the first few seconds of the motion-activated video clip.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature is being rolled out today as an English-only beta for Ring Home Premium subscribers in the U.S. and Canada. Users can choose to disable the feature by going to settings in the Ring app.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a blog post by Jamie Siminoff, Ring’s founder and now Amazon’s VP of home security, the company plans to introduce additional AI features. One of these combines several motion events happening in and around a home into a single alert.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ring also intends to implement a “custom anomaly alert” that allows users to define what constitutes an anomaly for their property, enabling the camera to notify them when such an event occurs.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Siminoff mentioned that Ring will “learn your routine,” so it can inform users when something is out of the ordinary. This may be unsettling for some users, especially considering Ring’s past privacy concerns.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“We are just starting to scratch the surface of AI. I feel like we are back to the very early days of Ring again—I see unlimited potential for new experiences we can invent for our neighbors,” Siminoff wrote.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This announcement follows the recent launch of Ring’s AI-powered search feature, which enables users to locate specific moments within video recordings.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/ring-cameras-and-doorbells-now-use-ai-to-provide-specific-descriptions-of-motion-activity/</guid><pubDate>Wed, 25 Jun 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Google unveils Gemini CLI, an open-source AI tool for terminals (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/google-unveils-gemini-cli-an-open-source-ai-tool-for-terminals/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Gemini_CLI_Hero_Final.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is launching a new agentic AI tool that will put its Gemini AI models closer to where developers are already coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced on Wednesday the launch of Gemini CLI, an agentic AI tool designed to run locally from your terminal. The new tool connects Google’s Gemini AI models to local codebases, and it allows developers to make natural language requests, such as asking Gemini CLI to explain confusing sections of code, write new features, debug code, or run commands.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Gemini CLI is part of Google’s efforts to get developers using its AI models in their coding workflows. Google now offers an array of AI coding tools, such as Gemini Code Assist and its asynchronous AI coding assistant, Jules. However, Gemini CLI competes directly with other command-line AI tools such as OpenAI’s Codex CLI and Anthropic’s Claude Code — tools that tend to be easier to integrate, faster, and more efficient than other AI coding tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Google launched Gemini 2.5 Pro in April, the company’s AI models have become a favorite among developers. The popularity of Gemini 2.5 Pro has driven usage of third-party AI coding tools, such as Cursor and GitHub Copilot, which have become massive businesses. In response, Google has tried in recent months to build a direct relationship with these developers by offering in-house products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While most people will use Gemini CLI for coding, the company says it designed the tool to handle other tasks as well. Developers can tap Gemini CLI to create videos with Google’s Veo 3 model, generate research reports with the company’s Deep Research agent, or access real-time information through Google Search. Google also says Gemini CLI can connect to MCP servers, allowing developers to connect to external databases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To encourage adoption, Google is also open-sourcing Gemini CLI under the Apache 2.0 license, which is typically considered one of the most permissive. The company says it expects a network of developers to contribute to the project on GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also offering generous usage limits to spur adoption of Gemini CLI. Free users can make 60 model requests per minute and 1,000 requests per day, which the company says is roughly double the average number of requests developers made when using the tool.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;While AI coding tools are rising rapidly in popularity, using them comes with risks. According to a 2024 survey from Stack Overflow, just 43% of developers trust the accuracy of AI tools. Several studies have shown that code-generating AI models can occasionally introduce errors or fail to fix security vulnerabilities.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Gemini_CLI_Hero_Final.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google is launching a new agentic AI tool that will put its Gemini AI models closer to where developers are already coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced on Wednesday the launch of Gemini CLI, an agentic AI tool designed to run locally from your terminal. The new tool connects Google’s Gemini AI models to local codebases, and it allows developers to make natural language requests, such as asking Gemini CLI to explain confusing sections of code, write new features, debug code, or run commands.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Gemini CLI is part of Google’s efforts to get developers using its AI models in their coding workflows. Google now offers an array of AI coding tools, such as Gemini Code Assist and its asynchronous AI coding assistant, Jules. However, Gemini CLI competes directly with other command-line AI tools such as OpenAI’s Codex CLI and Anthropic’s Claude Code — tools that tend to be easier to integrate, faster, and more efficient than other AI coding tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Google launched Gemini 2.5 Pro in April, the company’s AI models have become a favorite among developers. The popularity of Gemini 2.5 Pro has driven usage of third-party AI coding tools, such as Cursor and GitHub Copilot, which have become massive businesses. In response, Google has tried in recent months to build a direct relationship with these developers by offering in-house products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While most people will use Gemini CLI for coding, the company says it designed the tool to handle other tasks as well. Developers can tap Gemini CLI to create videos with Google’s Veo 3 model, generate research reports with the company’s Deep Research agent, or access real-time information through Google Search. Google also says Gemini CLI can connect to MCP servers, allowing developers to connect to external databases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To encourage adoption, Google is also open-sourcing Gemini CLI under the Apache 2.0 license, which is typically considered one of the most permissive. The company says it expects a network of developers to contribute to the project on GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also offering generous usage limits to spur adoption of Gemini CLI. Free users can make 60 model requests per minute and 1,000 requests per day, which the company says is roughly double the average number of requests developers made when using the tool.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;While AI coding tools are rising rapidly in popularity, using them comes with risks. According to a 2024 survey from Stack Overflow, just 43% of developers trust the accuracy of AI tools. Several studies have shown that code-generating AI models can occasionally introduce errors or fail to fix security vulnerabilities.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/google-unveils-gemini-cli-an-open-source-ai-tool-for-terminals/</guid><pubDate>Wed, 25 Jun 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Gemini CLI is a free, open source coding agent that brings AI to your terminal (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/google-is-bringing-vibe-coding-to-your-terminal-with-gemini-cli/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Now, devs who prefer the terminal can get AI assistance, too.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini_CLI_Hero-640x361.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini_CLI_Hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Some developers prefer to live in the command line interface (CLI), eschewing the flashy graphics and file management features of IDEs. Google's latest AI tool is for those terminal lovers. It's called Gemini CLI, and it shares a lot with Gemini Code Assist, but it works in your terminal environment instead of integrating with an IDE. And perhaps best of all, it's free and open source.&lt;/p&gt;
&lt;p&gt;Gemini CLI plugs into Gemini 2.5 Pro, Google's most advanced model for coding and simulated reasoning. It can create and modify code for you right inside the terminal, but you can also call on other Google models to generate images or videos without leaving the security of your terminal cocoon. It's essentially vibe coding from the command line.&lt;/p&gt;
&lt;p&gt;This tool is fully open source, so developers can inspect the code and help to improve it. The openness extends to how you configure the AI agent. It supports Model Context Protocol (MCP) and bundled extensions, allowing you to customize your terminal as you see fit. You can even include your own system prompts—Gemini CLI relies on GEMINI.md files, which you can use to tweak the model for different tasks or teams.&lt;/p&gt;
&lt;p&gt;Now that Gemini 2.5 Pro is generally available, Gemini Code Assist has been upgraded to use the same technology as Gemini CLI. Code Assist integrates with IDEs like VS Code for those times when you need a more feature-rich environment. The new agent mode in Code Assist allows you to give the AI more general instructions, like "Add support for dark mode to my application" or "Build my project and fix any errors."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2102628-1" preload="metadata" width="1280"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GenMedia-demo-keyword.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google wants as many people as possible to use Gemini in their development work, so Gemini CLI is available on Windows, Mac, and Linux—it's also free for almost everyone. Individual developers can sign up to get a free Gemini Code Assist license, which also covers Gemini CLI. The free plan includes 60 model requests per minute and 1,000 per day. Google says this is about twice what its internal team has been using, so almost everyone should be able to integrate Gemini CLI without bumping up against the limit. For professionals who need to run multiple agents at the same time or prefer to use custom models, Gemini CLI supports usage-based billing in Vertex AI or AI Studio.&lt;/p&gt;
&lt;p&gt;To get started, just download Gemini CLI from GitHub. That's also where you can report issues and make suggestions to improve Google's AI dev tools.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Now, devs who prefer the terminal can get AI assistance, too.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="361" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini_CLI_Hero-640x361.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini_CLI_Hero-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Some developers prefer to live in the command line interface (CLI), eschewing the flashy graphics and file management features of IDEs. Google's latest AI tool is for those terminal lovers. It's called Gemini CLI, and it shares a lot with Gemini Code Assist, but it works in your terminal environment instead of integrating with an IDE. And perhaps best of all, it's free and open source.&lt;/p&gt;
&lt;p&gt;Gemini CLI plugs into Gemini 2.5 Pro, Google's most advanced model for coding and simulated reasoning. It can create and modify code for you right inside the terminal, but you can also call on other Google models to generate images or videos without leaving the security of your terminal cocoon. It's essentially vibe coding from the command line.&lt;/p&gt;
&lt;p&gt;This tool is fully open source, so developers can inspect the code and help to improve it. The openness extends to how you configure the AI agent. It supports Model Context Protocol (MCP) and bundled extensions, allowing you to customize your terminal as you see fit. You can even include your own system prompts—Gemini CLI relies on GEMINI.md files, which you can use to tweak the model for different tasks or teams.&lt;/p&gt;
&lt;p&gt;Now that Gemini 2.5 Pro is generally available, Gemini Code Assist has been upgraded to use the same technology as Gemini CLI. Code Assist integrates with IDEs like VS Code for those times when you need a more feature-rich environment. The new agent mode in Code Assist allows you to give the AI more general instructions, like "Add support for dark mode to my application" or "Build my project and fix any errors."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="720" id="video-2102628-1" preload="metadata" width="1280"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GenMedia-demo-keyword.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Google wants as many people as possible to use Gemini in their development work, so Gemini CLI is available on Windows, Mac, and Linux—it's also free for almost everyone. Individual developers can sign up to get a free Gemini Code Assist license, which also covers Gemini CLI. The free plan includes 60 model requests per minute and 1,000 per day. Google says this is about twice what its internal team has been using, so almost everyone should be able to integrate Gemini CLI without bumping up against the limit. For professionals who need to run multiple agents at the same time or prefer to use custom models, Gemini CLI supports usage-based billing in Vertex AI or AI Studio.&lt;/p&gt;
&lt;p&gt;To get started, just download Gemini CLI from GitHub. That's also where you can report issues and make suggestions to improve Google's AI dev tools.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/google-is-bringing-vibe-coding-to-your-terminal-with-gemini-cli/</guid><pubDate>Wed, 25 Jun 2025 13:00:04 +0000</pubDate></item><item><title>[NEW] MUVERA: Making multi-vector retrieval as fast as single-vector search (The latest research from Google)</title><link>https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Neural embedding models have become a cornerstone of modern information retrieval (IR). Given a query from a user (e.g., “How tall is Mt Everest?”), the goal of IR is to find information relevant to the query from a very large collection of data (e.g., the billions of documents, images, or videos on the Web). Embedding models transform each datapoint into a single-vector “embedding”, such that &lt;i&gt;semantically&lt;/i&gt; similar datapoints are transformed into &lt;i&gt;mathematically&lt;/i&gt; similar vectors. The embeddings are generally compared via the inner-product similarity, enabling efficient retrieval through optimized maximum inner product search (MIPS) algorithms. However, recent advances, particularly the introduction of multi-vector models like ColBERT, have demonstrated significantly improved performance in IR tasks.&lt;/p&gt;&lt;p&gt;Unlike single-vector embeddings, multi-vector models represent each data point with a &lt;i&gt;set&lt;/i&gt; of embeddings, and leverage more sophisticated similarity functions that can capture richer relationships between datapoints. For example, the popular Chamfer similarity measure used in state-of-the-art multi-vector models captures when the information in one multi-vector embedding is contained within another multi-vector embedding. While this multi-vector approach boosts accuracy and enables retrieving more relevant documents, it introduces substantial computational challenges. In particular, the increased number of embeddings and the complexity of multi-vector similarity scoring make retrieval significantly more expensive.&lt;/p&gt;&lt;p&gt;In “MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings”, we introduce a novel multi-vector retrieval algorithm designed to bridge the efficiency gap between single- and multi-vector retrieval. We transform multi-vector retrieval into a simpler problem by constructing fixed dimensional encodings (FDEs) of queries and documents, which are single vectors whose inner product approximates multi-vector similarity, thus reducing complex multi-vector retrieval back to single-vector maximum inner product search (MIPS). This new approach allows us to leverage the highly-optimized MIPS algorithms to retrieve an initial set of candidates that can then be re-ranked with the exact multi-vector similarity, thereby enabling efficient multi-vector retrieval without sacrificing accuracy. We have provided an open-source implementation of our FDE construction algorithm on GitHub.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Neural embedding models have become a cornerstone of modern information retrieval (IR). Given a query from a user (e.g., “How tall is Mt Everest?”), the goal of IR is to find information relevant to the query from a very large collection of data (e.g., the billions of documents, images, or videos on the Web). Embedding models transform each datapoint into a single-vector “embedding”, such that &lt;i&gt;semantically&lt;/i&gt; similar datapoints are transformed into &lt;i&gt;mathematically&lt;/i&gt; similar vectors. The embeddings are generally compared via the inner-product similarity, enabling efficient retrieval through optimized maximum inner product search (MIPS) algorithms. However, recent advances, particularly the introduction of multi-vector models like ColBERT, have demonstrated significantly improved performance in IR tasks.&lt;/p&gt;&lt;p&gt;Unlike single-vector embeddings, multi-vector models represent each data point with a &lt;i&gt;set&lt;/i&gt; of embeddings, and leverage more sophisticated similarity functions that can capture richer relationships between datapoints. For example, the popular Chamfer similarity measure used in state-of-the-art multi-vector models captures when the information in one multi-vector embedding is contained within another multi-vector embedding. While this multi-vector approach boosts accuracy and enables retrieving more relevant documents, it introduces substantial computational challenges. In particular, the increased number of embeddings and the complexity of multi-vector similarity scoring make retrieval significantly more expensive.&lt;/p&gt;&lt;p&gt;In “MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings”, we introduce a novel multi-vector retrieval algorithm designed to bridge the efficiency gap between single- and multi-vector retrieval. We transform multi-vector retrieval into a simpler problem by constructing fixed dimensional encodings (FDEs) of queries and documents, which are single vectors whose inner product approximates multi-vector similarity, thus reducing complex multi-vector retrieval back to single-vector maximum inner product search (MIPS). This new approach allows us to leverage the highly-optimized MIPS algorithms to retrieve an initial set of candidates that can then be re-ranked with the exact multi-vector similarity, thereby enabling efficient multi-vector retrieval without sacrificing accuracy. We have provided an open-source implementation of our FDE construction algorithm on GitHub.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/muvera-making-multi-vector-retrieval-as-fast-as-single-vector-search/</guid><pubDate>Wed, 25 Jun 2025 13:22:00 +0000</pubDate></item><item><title>[NEW] Merging AI and underwater photography to reveal hidden ocean worlds (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/lobstger-merging-ai-underwater-photography-to-reveal-hidden-ocean-worlds-0625</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In the Northeastern United States, the Gulf of Maine represents one of the most biologically diverse marine ecosystems on the planet — home to whales, sharks, jellyfish, herring, plankton, and hundreds of other species. But even as this ecosystem supports rich biodiversity, it is undergoing rapid environmental change. The Gulf of Maine is warming faster than 99 percent of the world’s oceans, with consequences that are still unfolding.&lt;/p&gt;&lt;p&gt;A new research initiative developing at MIT Sea Grant, called LOBSTgER — short for Learning Oceanic Bioecological Systems Through Generative Representations — brings together artificial intelligence and underwater photography to document the ocean life left vulnerable to these changes and share them with the public in new visual ways. Co-led by underwater photographer and visiting artist at MIT Sea Grant Keith Ellenbogen and MIT mechanical engineering PhD student Andreas Mentzelopoulos, the project explores how generative AI can expand scientific storytelling by building on field-based photographic data.&lt;/p&gt;&lt;p&gt;Just as the 19th-century camera transformed our ability to document and reveal the natural world — capturing life with unprecedented detail and bringing distant or hidden environments into view — generative AI marks a new frontier in visual storytelling. Like early photography, AI opens a creative and conceptual space, challenging how we define authenticity and how we communicate scientific and artistic perspectives.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In the LOBSTgER project, generative models are trained exclusively on a curated library of Ellenbogen’s original underwater photographs — each image crafted with artistic intent, technical precision, accurate species identification, and clear geographic context. By building a high-quality dataset grounded in real-world observations, the project ensures that the resulting imagery maintains both visual integrity and ecological relevance. In addition, LOBSTgER’s models are built using custom code developed by Mentzelopoulos to protect the process and outputs from any potential biases from external data or models. LOBSTgER’s generative AI builds upon real photography, expanding the researchers’ visual vocabulary to deepen the public’s connection to the natural world.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="A photoreal image of a large oval ocean sunfish underwater. An orange LOBSTgER icon indicates this was made with AI." class="ondemand" height="499" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="751" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            This ocean sunfish (Mola mola) image was generated by LOBSTgER’s unconditional models.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            AI-generated image: Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER.        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;At its heart, LOBSTgER operates at the intersection of art, science, and technology. The project draws from the visual language of photography, the observational rigor of marine science, and the computational power of generative AI. By uniting these disciplines, the team is not only developing new ways to visualize ocean life — they are also reimagining how environmental stories can be told. This integrative approach makes LOBSTgER both a research tool and a creative experiment — one that reflects MIT’s long-standing tradition of interdisciplinary innovation.&lt;/p&gt;&lt;p&gt;Underwater photography in New England’s coastal waters is notoriously difficult. Limited visibility, swirling sediment, bubbles, and the unpredictable movement of marine life all pose constant challenges. For the past several years, Ellenbogen has navigated these challenges and is building a comprehensive record of the region’s biodiversity through the project, Space to Sea: Visualizing New England’s Ocean Wilderness.&amp;nbsp;This large dataset of underwater images provides the foundation for training LOBSTgER’s generative AI models. The images span diverse angles, lighting conditions, and animal behaviors, resulting in a visual archive that is both artistically striking and biologically accurate.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Image synthesis via reverse diffusion: This short video shows the de-noising trajectory from Gaussian latent noise to photorealistic output using LOBSTgER’s unconditional models. Iterative de-noising requires 1,000 forward passes through the trained neural network.&lt;br /&gt;Video: Keith Ellenbogen and Andreas Mentzelopoulos / MIT Sea Grant        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;LOBSTgER’s&amp;nbsp;custom diffusion models are trained to replicate not only the biodiversity Ellenbogen documents, but also the artistic style he uses to capture it. By learning from thousands of real underwater images, the models internalize fine-grained details such as natural lighting gradients, species-specific coloration, and even the atmospheric texture created by suspended particles and refracted sunlight. The result is imagery that not only appears visually accurate, but also feels immersive and moving.&lt;/p&gt;&lt;p&gt;The models can both generate new, synthetic, but scientifically accurate images unconditionally (i.e., requiring no user input/guidance), and enhance real photographs conditionally (i.e., image-to-image generation). By integrating AI into the photographic workflow, Ellenbogen will be able to use these tools to recover detail in turbid water, adjust lighting to emphasize key subjects, or even simulate scenes that would be nearly impossible to capture in the field. The team also believes this approach may benefit other underwater photographers and image editors facing similar challenges. This hybrid method is designed to accelerate the curation process and enable storytellers to construct a more complete and coherent visual narrative of life beneath the surface.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="Side-by-side images of an American lobster on the sea floor underneath seaweed. One has been enhanced by AI and is far more vibrant." class="ondemand" height="298" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="900" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            Left: Enhanced image of an American lobster using LOBSTgER’s image-to-image models. Right: Original image.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            Left: AI genertated image by Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER. Right: Keith Ellenbogen        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In one key series, Ellenbogen captured high-resolution images of lion’s mane jellyfish, blue sharks, American lobsters, and ocean sunfish (&lt;em&gt;Mola mola&lt;/em&gt;) while free diving in coastal waters. “Getting a high-quality dataset is not easy,” Ellenbogen says. “It requires multiple dives, missed opportunities, and unpredictable conditions. But these challenges are part of what makes underwater documentation both difficult and rewarding.”&lt;/p&gt;&lt;p&gt;Mentzelopoulos has developed original code to train a family of latent diffusion models for LOBSTgER grounded on Ellenbogen’s images. Developing such models requires a high level of technical expertise, and training models from scratch is a complex process demanding hundreds of hours of computation and meticulous hyperparameter tuning.&lt;/p&gt;&lt;p&gt;The project reflects a parallel process: field documentation through photography and model development through iterative training. Ellenbogen works in the field, capturing rare and fleeting encounters with marine animals; Mentzelopoulos works in the lab, translating those moments into machine-learning contexts that can extend and reinterpret the visual language of the ocean.&lt;/p&gt;&lt;p&gt;“The goal isn’t to replace photography,” Mentzelopoulos says. “It’s to build on and complement it — making the invisible visible, and helping people see environmental complexity in a way that resonates both emotionally and intellectually. Our models aim to capture not just biological realism, but the emotional charge that can drive real-world engagement and action.”&lt;/p&gt;&lt;p&gt;LOBSTgER points to a hybrid future that merges direct observation with technological interpretation. The team’s long-term goal is to develop a comprehensive model that can visualize a wide range of species found in the Gulf of Maine and, eventually, apply similar methods to marine ecosystems around the world.&lt;/p&gt;&lt;p&gt;The researchers suggest that photography and generative AI form a continuum, rather than a conflict. Photography captures what is — the texture, light, and animal behavior during actual encounters — while AI extends that vision beyond what is seen, toward what could be understood, inferred, or imagined based on scientific data and artistic vision. Together, they offer a powerful framework for communicating science through image-making.&lt;/p&gt;&lt;p&gt;In a region where ecosystems are changing rapidly, the act of visualizing becomes more than just documentation. It becomes a tool for awareness, engagement, and, ultimately, conservation. LOBSTgER is still in its infancy, and the team looks forward to sharing more discoveries, images, and insights as the project evolves.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Answer from the lead image: The left image was generated using using LOBSTgER’s unconditional models and the right image is real.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;For more information, contact Keith Ellenbogen and Andreas Mentzelopoulos.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In the Northeastern United States, the Gulf of Maine represents one of the most biologically diverse marine ecosystems on the planet — home to whales, sharks, jellyfish, herring, plankton, and hundreds of other species. But even as this ecosystem supports rich biodiversity, it is undergoing rapid environmental change. The Gulf of Maine is warming faster than 99 percent of the world’s oceans, with consequences that are still unfolding.&lt;/p&gt;&lt;p&gt;A new research initiative developing at MIT Sea Grant, called LOBSTgER — short for Learning Oceanic Bioecological Systems Through Generative Representations — brings together artificial intelligence and underwater photography to document the ocean life left vulnerable to these changes and share them with the public in new visual ways. Co-led by underwater photographer and visiting artist at MIT Sea Grant Keith Ellenbogen and MIT mechanical engineering PhD student Andreas Mentzelopoulos, the project explores how generative AI can expand scientific storytelling by building on field-based photographic data.&lt;/p&gt;&lt;p&gt;Just as the 19th-century camera transformed our ability to document and reveal the natural world — capturing life with unprecedented detail and bringing distant or hidden environments into view — generative AI marks a new frontier in visual storytelling. Like early photography, AI opens a creative and conceptual space, challenging how we define authenticity and how we communicate scientific and artistic perspectives.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In the LOBSTgER project, generative models are trained exclusively on a curated library of Ellenbogen’s original underwater photographs — each image crafted with artistic intent, technical precision, accurate species identification, and clear geographic context. By building a high-quality dataset grounded in real-world observations, the project ensures that the resulting imagery maintains both visual integrity and ecological relevance. In addition, LOBSTgER’s models are built using custom code developed by Mentzelopoulos to protect the process and outputs from any potential biases from external data or models. LOBSTgER’s generative AI builds upon real photography, expanding the researchers’ visual vocabulary to deepen the public’s connection to the natural world.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="A photoreal image of a large oval ocean sunfish underwater. An orange LOBSTgER icon indicates this was made with AI." class="ondemand" height="499" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="751" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            This ocean sunfish (Mola mola) image was generated by LOBSTgER’s unconditional models.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            AI-generated image: Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER.        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;At its heart, LOBSTgER operates at the intersection of art, science, and technology. The project draws from the visual language of photography, the observational rigor of marine science, and the computational power of generative AI. By uniting these disciplines, the team is not only developing new ways to visualize ocean life — they are also reimagining how environmental stories can be told. This integrative approach makes LOBSTgER both a research tool and a creative experiment — one that reflects MIT’s long-standing tradition of interdisciplinary innovation.&lt;/p&gt;&lt;p&gt;Underwater photography in New England’s coastal waters is notoriously difficult. Limited visibility, swirling sediment, bubbles, and the unpredictable movement of marine life all pose constant challenges. For the past several years, Ellenbogen has navigated these challenges and is building a comprehensive record of the region’s biodiversity through the project, Space to Sea: Visualizing New England’s Ocean Wilderness.&amp;nbsp;This large dataset of underwater images provides the foundation for training LOBSTgER’s generative AI models. The images span diverse angles, lighting conditions, and animal behaviors, resulting in a visual archive that is both artistically striking and biologically accurate.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Image synthesis via reverse diffusion: This short video shows the de-noising trajectory from Gaussian latent noise to photorealistic output using LOBSTgER’s unconditional models. Iterative de-noising requires 1,000 forward passes through the trained neural network.&lt;br /&gt;Video: Keith Ellenbogen and Andreas Mentzelopoulos / MIT Sea Grant        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;LOBSTgER’s&amp;nbsp;custom diffusion models are trained to replicate not only the biodiversity Ellenbogen documents, but also the artistic style he uses to capture it. By learning from thousands of real underwater images, the models internalize fine-grained details such as natural lighting gradients, species-specific coloration, and even the atmospheric texture created by suspended particles and refracted sunlight. The result is imagery that not only appears visually accurate, but also feels immersive and moving.&lt;/p&gt;&lt;p&gt;The models can both generate new, synthetic, but scientifically accurate images unconditionally (i.e., requiring no user input/guidance), and enhance real photographs conditionally (i.e., image-to-image generation). By integrating AI into the photographic workflow, Ellenbogen will be able to use these tools to recover detail in turbid water, adjust lighting to emphasize key subjects, or even simulate scenes that would be nearly impossible to capture in the field. The team also believes this approach may benefit other underwater photographers and image editors facing similar challenges. This hybrid method is designed to accelerate the curation process and enable storytellers to construct a more complete and coherent visual narrative of life beneath the surface.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="Side-by-side images of an American lobster on the sea floor underneath seaweed. One has been enhanced by AI and is far more vibrant." class="ondemand" height="298" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="900" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            Left: Enhanced image of an American lobster using LOBSTgER’s image-to-image models. Right: Original image.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            Left: AI genertated image by Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER. Right: Keith Ellenbogen        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In one key series, Ellenbogen captured high-resolution images of lion’s mane jellyfish, blue sharks, American lobsters, and ocean sunfish (&lt;em&gt;Mola mola&lt;/em&gt;) while free diving in coastal waters. “Getting a high-quality dataset is not easy,” Ellenbogen says. “It requires multiple dives, missed opportunities, and unpredictable conditions. But these challenges are part of what makes underwater documentation both difficult and rewarding.”&lt;/p&gt;&lt;p&gt;Mentzelopoulos has developed original code to train a family of latent diffusion models for LOBSTgER grounded on Ellenbogen’s images. Developing such models requires a high level of technical expertise, and training models from scratch is a complex process demanding hundreds of hours of computation and meticulous hyperparameter tuning.&lt;/p&gt;&lt;p&gt;The project reflects a parallel process: field documentation through photography and model development through iterative training. Ellenbogen works in the field, capturing rare and fleeting encounters with marine animals; Mentzelopoulos works in the lab, translating those moments into machine-learning contexts that can extend and reinterpret the visual language of the ocean.&lt;/p&gt;&lt;p&gt;“The goal isn’t to replace photography,” Mentzelopoulos says. “It’s to build on and complement it — making the invisible visible, and helping people see environmental complexity in a way that resonates both emotionally and intellectually. Our models aim to capture not just biological realism, but the emotional charge that can drive real-world engagement and action.”&lt;/p&gt;&lt;p&gt;LOBSTgER points to a hybrid future that merges direct observation with technological interpretation. The team’s long-term goal is to develop a comprehensive model that can visualize a wide range of species found in the Gulf of Maine and, eventually, apply similar methods to marine ecosystems around the world.&lt;/p&gt;&lt;p&gt;The researchers suggest that photography and generative AI form a continuum, rather than a conflict. Photography captures what is — the texture, light, and animal behavior during actual encounters — while AI extends that vision beyond what is seen, toward what could be understood, inferred, or imagined based on scientific data and artistic vision. Together, they offer a powerful framework for communicating science through image-making.&lt;/p&gt;&lt;p&gt;In a region where ecosystems are changing rapidly, the act of visualizing becomes more than just documentation. It becomes a tool for awareness, engagement, and, ultimately, conservation. LOBSTgER is still in its infancy, and the team looks forward to sharing more discoveries, images, and insights as the project evolves.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Answer from the lead image: The left image was generated using using LOBSTgER’s unconditional models and the right image is real.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;For more information, contact Keith Ellenbogen and Andreas Mentzelopoulos.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/lobstger-merging-ai-underwater-photography-to-reveal-hidden-ocean-worlds-0625</guid><pubDate>Wed, 25 Jun 2025 13:55:00 +0000</pubDate></item><item><title>[NEW] AlphaGenome: AI for better understanding the genome (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/</link><description>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-06-25"&gt;25 June 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Ziga Avsec and Natasha Latysheva&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="A central, light-blue DNA double helix stands in sharp focus, flanked by a series of DNA strands that fade into a soft, blurry background, giving the impression of a field of genetic information. The backdrop is bathed in a soft light that transitions from pink to purple." class="picture__image" height="603" src="https://lh3.googleusercontent.com/SZkcKUQyLUhSQ06Rq-PJbxAqn1OpMeEa3khkrBVB1MGyHfxyftoqWwEb2aLP9JxX7CjhpLFODcc5zIoMoNdu0bl6ELsZV2nP9fDwZC6SYS36lzAKDw=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.&lt;/p&gt;&lt;p&gt;The genome is our cellular instruction manual. It’s the complete set of DNA which guides nearly every part of a living organism, from appearance and function to growth and reproduction. Small variations in a genome’s DNA sequence can alter an organism’s response to its environment or its susceptibility to disease. But deciphering how the genome’s instructions are read at the molecular level — and what happens when a small DNA variation occurs — is still one of biology’s greatest mysteries.&lt;/p&gt;&lt;p&gt;Today, we introduce AlphaGenome, a new artificial intelligence (AI) tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes. This was enabled, among other factors, by technical advances allowing the model to process long DNA sequences and output high-resolution predictions.&lt;/p&gt;&lt;p&gt;To advance scientific research, we’re making AlphaGenome available in preview via our AlphaGenome API for non-commercial research, and planning to release the model in the future.&lt;/p&gt;&lt;p&gt;We believe AlphaGenome can be a valuable resource for the scientific community, helping scientists better understand genome function, disease biology, and ultimately, drive new biological discoveries and the development of new treatments.&lt;/p&gt;&lt;h2&gt;How AlphaGenome works&lt;/h2&gt;&lt;p&gt;Our AlphaGenome model takes a long DNA sequence as input — up to 1 million letters, also known as base-pairs — and predicts thousands of molecular properties characterising its regulatory activity. It can also score the effects of genetic variants or mutations by comparing predictions of mutated sequences with unmutated ones.&lt;/p&gt;&lt;p&gt;Predicted properties include where genes start and where they end in different cell types and tissues, where they get spliced, the amount of RNA being produced, and also which DNA bases are accessible, close to one another, or bound by certain proteins. Training data was sourced from large public consortia including ENCODE, GTEx, 4D Nucleome and FANTOM5, which experimentally measured these properties covering important modalities of gene regulation across hundreds of human and mouse cell types and tissues.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-d704db3e-f8f0-43ee-ae46-7a2a21b420ee"&gt;
    &lt;p&gt;Animation showing AlphaGenome taking one million DNA letters as input and predicting diverse molecular properties across different tissues and cell types.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;The AlphaGenome architecture uses convolutional layers to initially detect short patterns in the genome sequence, transformers to communicate information across all positions in the sequence, and a final series of layers to turn the detected patterns into predictions for different modalities. During training, this computation is distributed across multiple interconnected Tensor Processing Units (TPUs) for a single sequence.&lt;/p&gt;&lt;p&gt;This model builds on our previous genomics model, Enformer and is complementary to AlphaMissense, which specializes in categorizing the effects of variants within protein-coding regions. These regions cover 2% of the genome. The remaining 98%, called non-coding regions, are crucial for orchestrating gene activity and contain many variants linked to diseases. AlphaGenome offers a new perspective for interpreting these expansive sequences and the variants within them.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;AlphaGenome’s distinctive features&lt;/h2&gt;&lt;p&gt;AlphaGenome offers several distinctive features compared to existing DNA sequence models:&lt;/p&gt;&lt;h3&gt;Long sequence-context at high resolution&lt;/h3&gt;&lt;p&gt;Our model analyzes up to 1 million DNA letters and makes predictions at the resolution of individual letters. Long sequence context is important for covering regions regulating genes from far away and base-resolution is important for capturing fine-grained biological details.&lt;/p&gt;&lt;p&gt;Previous models had to trade off sequence length and resolution, which limited the range of modalities they could jointly model and accurately predict. Our technical advances address this limitation without significantly increasing the training resources — training a single AlphaGenome model (without distillation) took four hours and required half of the compute budget used to train our original Enformer model.&lt;/p&gt;&lt;h3&gt;Comprehensive multimodal prediction&lt;/h3&gt;&lt;p&gt;By unlocking high resolution prediction for long input sequences, AlphaGenome can predict the most diverse range of modalities. In doing so, AlphaGenome provides scientists with more comprehensive information about the complex steps of gene regulation.&lt;/p&gt;&lt;h3&gt;Efficient variant scoring&lt;/h3&gt;&lt;p&gt;In addition to predicting a diverse range of molecular properties, AlphaGenome can efficiently score the impact of a genetic variant on all of these properties in a second. It does this by contrasting predictions of mutated sequences with unmutated ones, and efficiently summarising that contrast using different approaches for different modalities.&lt;/p&gt;&lt;h3&gt;Novel splice-junction modeling&lt;/h3&gt;&lt;p&gt;Many rare genetic diseases, such as spinal muscular atrophy and some forms of cystic fibrosis, can be caused by errors in RNA splicing — a process where parts of the RNA molecule are removed, or “spliced out”, and the remaining ends rejoined. For the first time, AlphaGenome can explicitly model the location and expression level of these junctions directly from sequence, offering deeper insights about the consequences of genetic variants on RNA splicing.&lt;/p&gt;&lt;h2&gt;State-of-the-art performance across benchmarks&lt;/h2&gt;&lt;p&gt;AlphaGenome achieves state-of-the-art performance across a wide range of genomic prediction benchmarks, such as predicting which parts of the DNA molecule will be in close proximity, whether a genetic variant will increase or decrease expression of a gene, or whether it will change the gene’s splicing pattern.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-6cfc2442-d824-4dda-b563-3b5162675a6e"&gt;
    &lt;p&gt;Bar graph showing AlphaGenome’s relative improvements on selected DNA sequence and variant effect tasks, compared against results for the current best methods in each category.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;When producing predictions for single DNA sequences, AlphaGenome outperformed the best external models on 22 out of 24 evaluations. And when predicting the regulatory effect of a variant, it matched or exceeded the top-performing external models on 24 out of 26 evaluations.&lt;/p&gt;&lt;p&gt;This comparison included models specialized for individual tasks. AlphaGenome was the only model that could jointly predict all of the assessed modalities, highlighting its generality. Read more in our preprint.&lt;/p&gt;&lt;h2&gt;The benefits of a unifying model&lt;/h2&gt;&lt;p&gt;AlphaGenome’s generality allows scientists to simultaneously explore a variant's impact on a number of modalities with a single API call. This means that scientists can generate and test hypotheses more rapidly, without having to use multiple models to investigate different modalities.&lt;/p&gt;&lt;p&gt;Moreover AlphaGenome’s strong performance indicates it has learned a relatively general representation of DNA sequence in the context of gene regulation. This makes it a strong foundation for the wider community to build upon. Once the model is fully released, scientists will be able to adapt and fine-tune it on their own datasets to better tackle their unique research questions.&lt;/p&gt;&lt;p&gt;Finally, this approach provides a flexible and scalable architecture for the future. By extending the training data, AlphaGenome’s capabilities could be extended to yield better performance, cover more species, or include additional modalities to make the model even more comprehensive.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;It’s a milestone for the field. For the first time, we have a single model that unifies long-range context, base-level precision and state-of-the-art performance across a whole spectrum of genomic tasks.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Dr. Caleb Lareau, Memorial Sloan Kettering Cancer Center&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;A powerful research tool&lt;/h2&gt;&lt;p&gt;AlphaGenome's predictive capabilities could help several research avenues:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Disease understanding:&lt;/strong&gt; By more accurately predicting genetic disruptions, AlphaGenome could help researchers pinpoint the potential causes of disease more precisely, and better interpret the functional impact of variants linked to certain traits, potentially uncovering new therapeutic targets. We think the model is especially suitable for studying rare variants with potentially large effects, such as those causing rare Mendelian disorders.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Synthetic biology:&lt;/strong&gt; Its predictions could be used to guide the design of synthetic DNA with specific regulatory function — for example, only activating a gene in nerve cells but not muscle cells.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Fundamental research:&lt;/strong&gt; It could accelerate our understanding of the genome by assisting in mapping its crucial functional elements and defining their roles, identifying the most essential DNA instructions for regulating a specific cell type's function.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For example, we used AlphaGenome to investigate the potential mechanism of a cancer-associated mutation. In an existing study of patients with T-cell acute lymphoblastic leukemia (T-ALL), researchers observed mutations at particular locations in the genome. Using AlphaGenome, we predicted that the mutations would activate a nearby gene called TAL1 by introducing a MYB DNA binding motif, which replicated the known disease mechanism and highlighted AlphaGenome’s ability to link specific non-coding variants to disease genes.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;AlphaGenome will be a powerful tool for the field. Determining the relevance of different non-coding variants can be extremely challenging, particularly to do at scale. This tool will provide a crucial piece of the puzzle, allowing us to make better connections to understand diseases like cancer.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Professor Marc Mansour, University College London&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Current limitations&lt;/h2&gt;&lt;p&gt;AlphaGenome marks a significant step forward, but it's important to acknowledge its current limitations.&lt;/p&gt;&lt;p&gt;Like other sequence-based models, accurately capturing the influence of very distant regulatory elements, like those over 100,000 DNA letters away, is still an ongoing challenge. Another priority for future work is further increasing the model’s ability to capture cell- and tissue-specific patterns.&lt;/p&gt;&lt;p&gt;We haven't designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models. Instead, we focused more on characterising the performance on individual genetic variants. And while AlphaGenome can predict molecular outcomes, it doesn't give the full picture of how genetic variations lead to complex traits or diseases. These often involve broader biological processes, like developmental and environmental factors, that are beyond the direct scope of our model.&lt;/p&gt;&lt;p&gt;We’re continuing to improve our models and gathering feedback to help us address these gaps.&lt;/p&gt;&lt;h2&gt;Enabling the community to unlock AlphaGenome's potential&lt;/h2&gt;&lt;p&gt;AlphaGenome is now available for non-commercial use via our AlphaGenome API. Please note that our model’s predictions are intended only for research use and haven’t been designed or validated for direct clinical purposes.&lt;/p&gt;&lt;p&gt;Researchers worldwide are invited to get in touch with potential use-cases for AlphaGenome and to ask questions or share feedback through the community forum.&lt;/p&gt;&lt;p&gt;We hope AlphaGenome will be an important tool for better understanding the genome and we’re committed to working alongside external experts across academia, industry, and government organizations to ensure AlphaGenome benefits as many people as possible.&lt;/p&gt;&lt;p&gt;Together with the collective efforts of the wider scientific community, we hope it will deepen our understanding of the complex cellular processes encoded in the DNA sequence and the effects of variants, and drive exciting new discoveries in genomics and healthcare.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about AlphaGenome&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We would like to thank Juanita Bawagan, Arielle Bier, Stephanie Booth, Irina Andronic, Armin Senoner, Dhavanthi Hariharan, Rob Ashley, Agata Laydon and Kathryn Tunyasuvunakool for their help with the text and figures.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of the AlphaGenome co-authors: Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drotár, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis and Pushmeet Kohli.&lt;/p&gt;&lt;p&gt;We would also like to thank Dhavanthi Hariharan, Charlie Taylor, Ottavia Bertolli, Yannis Assael, Alex Botev, Anna Trostanetski, Lucas Tenório, Victoria Johnston, Richard Green, Kathryn Tunyasuvunakool, Molly Beck, Uchechi Okereke, Rachael Tremlett, Sarah Chakera, Ibrahim I. Taskiran, Andreea-Alexandra Muşat, Raiyan Khan, Ren Yi and the greater Google DeepMind team for their support, help and feedback.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</description><content:encoded>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-06-25"&gt;25 June 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Ziga Avsec and Natasha Latysheva&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="A central, light-blue DNA double helix stands in sharp focus, flanked by a series of DNA strands that fade into a soft, blurry background, giving the impression of a field of genetic information. The backdrop is bathed in a soft light that transitions from pink to purple." class="picture__image" height="603" src="https://lh3.googleusercontent.com/SZkcKUQyLUhSQ06Rq-PJbxAqn1OpMeEa3khkrBVB1MGyHfxyftoqWwEb2aLP9JxX7CjhpLFODcc5zIoMoNdu0bl6ELsZV2nP9fDwZC6SYS36lzAKDw=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.&lt;/p&gt;&lt;p&gt;The genome is our cellular instruction manual. It’s the complete set of DNA which guides nearly every part of a living organism, from appearance and function to growth and reproduction. Small variations in a genome’s DNA sequence can alter an organism’s response to its environment or its susceptibility to disease. But deciphering how the genome’s instructions are read at the molecular level — and what happens when a small DNA variation occurs — is still one of biology’s greatest mysteries.&lt;/p&gt;&lt;p&gt;Today, we introduce AlphaGenome, a new artificial intelligence (AI) tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes. This was enabled, among other factors, by technical advances allowing the model to process long DNA sequences and output high-resolution predictions.&lt;/p&gt;&lt;p&gt;To advance scientific research, we’re making AlphaGenome available in preview via our AlphaGenome API for non-commercial research, and planning to release the model in the future.&lt;/p&gt;&lt;p&gt;We believe AlphaGenome can be a valuable resource for the scientific community, helping scientists better understand genome function, disease biology, and ultimately, drive new biological discoveries and the development of new treatments.&lt;/p&gt;&lt;h2&gt;How AlphaGenome works&lt;/h2&gt;&lt;p&gt;Our AlphaGenome model takes a long DNA sequence as input — up to 1 million letters, also known as base-pairs — and predicts thousands of molecular properties characterising its regulatory activity. It can also score the effects of genetic variants or mutations by comparing predictions of mutated sequences with unmutated ones.&lt;/p&gt;&lt;p&gt;Predicted properties include where genes start and where they end in different cell types and tissues, where they get spliced, the amount of RNA being produced, and also which DNA bases are accessible, close to one another, or bound by certain proteins. Training data was sourced from large public consortia including ENCODE, GTEx, 4D Nucleome and FANTOM5, which experimentally measured these properties covering important modalities of gene regulation across hundreds of human and mouse cell types and tissues.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-d704db3e-f8f0-43ee-ae46-7a2a21b420ee"&gt;
    &lt;p&gt;Animation showing AlphaGenome taking one million DNA letters as input and predicting diverse molecular properties across different tissues and cell types.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;The AlphaGenome architecture uses convolutional layers to initially detect short patterns in the genome sequence, transformers to communicate information across all positions in the sequence, and a final series of layers to turn the detected patterns into predictions for different modalities. During training, this computation is distributed across multiple interconnected Tensor Processing Units (TPUs) for a single sequence.&lt;/p&gt;&lt;p&gt;This model builds on our previous genomics model, Enformer and is complementary to AlphaMissense, which specializes in categorizing the effects of variants within protein-coding regions. These regions cover 2% of the genome. The remaining 98%, called non-coding regions, are crucial for orchestrating gene activity and contain many variants linked to diseases. AlphaGenome offers a new perspective for interpreting these expansive sequences and the variants within them.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;AlphaGenome’s distinctive features&lt;/h2&gt;&lt;p&gt;AlphaGenome offers several distinctive features compared to existing DNA sequence models:&lt;/p&gt;&lt;h3&gt;Long sequence-context at high resolution&lt;/h3&gt;&lt;p&gt;Our model analyzes up to 1 million DNA letters and makes predictions at the resolution of individual letters. Long sequence context is important for covering regions regulating genes from far away and base-resolution is important for capturing fine-grained biological details.&lt;/p&gt;&lt;p&gt;Previous models had to trade off sequence length and resolution, which limited the range of modalities they could jointly model and accurately predict. Our technical advances address this limitation without significantly increasing the training resources — training a single AlphaGenome model (without distillation) took four hours and required half of the compute budget used to train our original Enformer model.&lt;/p&gt;&lt;h3&gt;Comprehensive multimodal prediction&lt;/h3&gt;&lt;p&gt;By unlocking high resolution prediction for long input sequences, AlphaGenome can predict the most diverse range of modalities. In doing so, AlphaGenome provides scientists with more comprehensive information about the complex steps of gene regulation.&lt;/p&gt;&lt;h3&gt;Efficient variant scoring&lt;/h3&gt;&lt;p&gt;In addition to predicting a diverse range of molecular properties, AlphaGenome can efficiently score the impact of a genetic variant on all of these properties in a second. It does this by contrasting predictions of mutated sequences with unmutated ones, and efficiently summarising that contrast using different approaches for different modalities.&lt;/p&gt;&lt;h3&gt;Novel splice-junction modeling&lt;/h3&gt;&lt;p&gt;Many rare genetic diseases, such as spinal muscular atrophy and some forms of cystic fibrosis, can be caused by errors in RNA splicing — a process where parts of the RNA molecule are removed, or “spliced out”, and the remaining ends rejoined. For the first time, AlphaGenome can explicitly model the location and expression level of these junctions directly from sequence, offering deeper insights about the consequences of genetic variants on RNA splicing.&lt;/p&gt;&lt;h2&gt;State-of-the-art performance across benchmarks&lt;/h2&gt;&lt;p&gt;AlphaGenome achieves state-of-the-art performance across a wide range of genomic prediction benchmarks, such as predicting which parts of the DNA molecule will be in close proximity, whether a genetic variant will increase or decrease expression of a gene, or whether it will change the gene’s splicing pattern.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-6cfc2442-d824-4dda-b563-3b5162675a6e"&gt;
    &lt;p&gt;Bar graph showing AlphaGenome’s relative improvements on selected DNA sequence and variant effect tasks, compared against results for the current best methods in each category.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;When producing predictions for single DNA sequences, AlphaGenome outperformed the best external models on 22 out of 24 evaluations. And when predicting the regulatory effect of a variant, it matched or exceeded the top-performing external models on 24 out of 26 evaluations.&lt;/p&gt;&lt;p&gt;This comparison included models specialized for individual tasks. AlphaGenome was the only model that could jointly predict all of the assessed modalities, highlighting its generality. Read more in our preprint.&lt;/p&gt;&lt;h2&gt;The benefits of a unifying model&lt;/h2&gt;&lt;p&gt;AlphaGenome’s generality allows scientists to simultaneously explore a variant's impact on a number of modalities with a single API call. This means that scientists can generate and test hypotheses more rapidly, without having to use multiple models to investigate different modalities.&lt;/p&gt;&lt;p&gt;Moreover AlphaGenome’s strong performance indicates it has learned a relatively general representation of DNA sequence in the context of gene regulation. This makes it a strong foundation for the wider community to build upon. Once the model is fully released, scientists will be able to adapt and fine-tune it on their own datasets to better tackle their unique research questions.&lt;/p&gt;&lt;p&gt;Finally, this approach provides a flexible and scalable architecture for the future. By extending the training data, AlphaGenome’s capabilities could be extended to yield better performance, cover more species, or include additional modalities to make the model even more comprehensive.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;It’s a milestone for the field. For the first time, we have a single model that unifies long-range context, base-level precision and state-of-the-art performance across a whole spectrum of genomic tasks.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Dr. Caleb Lareau, Memorial Sloan Kettering Cancer Center&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;A powerful research tool&lt;/h2&gt;&lt;p&gt;AlphaGenome's predictive capabilities could help several research avenues:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Disease understanding:&lt;/strong&gt; By more accurately predicting genetic disruptions, AlphaGenome could help researchers pinpoint the potential causes of disease more precisely, and better interpret the functional impact of variants linked to certain traits, potentially uncovering new therapeutic targets. We think the model is especially suitable for studying rare variants with potentially large effects, such as those causing rare Mendelian disorders.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Synthetic biology:&lt;/strong&gt; Its predictions could be used to guide the design of synthetic DNA with specific regulatory function — for example, only activating a gene in nerve cells but not muscle cells.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Fundamental research:&lt;/strong&gt; It could accelerate our understanding of the genome by assisting in mapping its crucial functional elements and defining their roles, identifying the most essential DNA instructions for regulating a specific cell type's function.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For example, we used AlphaGenome to investigate the potential mechanism of a cancer-associated mutation. In an existing study of patients with T-cell acute lymphoblastic leukemia (T-ALL), researchers observed mutations at particular locations in the genome. Using AlphaGenome, we predicted that the mutations would activate a nearby gene called TAL1 by introducing a MYB DNA binding motif, which replicated the known disease mechanism and highlighted AlphaGenome’s ability to link specific non-coding variants to disease genes.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;AlphaGenome will be a powerful tool for the field. Determining the relevance of different non-coding variants can be extremely challenging, particularly to do at scale. This tool will provide a crucial piece of the puzzle, allowing us to make better connections to understand diseases like cancer.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Professor Marc Mansour, University College London&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Current limitations&lt;/h2&gt;&lt;p&gt;AlphaGenome marks a significant step forward, but it's important to acknowledge its current limitations.&lt;/p&gt;&lt;p&gt;Like other sequence-based models, accurately capturing the influence of very distant regulatory elements, like those over 100,000 DNA letters away, is still an ongoing challenge. Another priority for future work is further increasing the model’s ability to capture cell- and tissue-specific patterns.&lt;/p&gt;&lt;p&gt;We haven't designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models. Instead, we focused more on characterising the performance on individual genetic variants. And while AlphaGenome can predict molecular outcomes, it doesn't give the full picture of how genetic variations lead to complex traits or diseases. These often involve broader biological processes, like developmental and environmental factors, that are beyond the direct scope of our model.&lt;/p&gt;&lt;p&gt;We’re continuing to improve our models and gathering feedback to help us address these gaps.&lt;/p&gt;&lt;h2&gt;Enabling the community to unlock AlphaGenome's potential&lt;/h2&gt;&lt;p&gt;AlphaGenome is now available for non-commercial use via our AlphaGenome API. Please note that our model’s predictions are intended only for research use and haven’t been designed or validated for direct clinical purposes.&lt;/p&gt;&lt;p&gt;Researchers worldwide are invited to get in touch with potential use-cases for AlphaGenome and to ask questions or share feedback through the community forum.&lt;/p&gt;&lt;p&gt;We hope AlphaGenome will be an important tool for better understanding the genome and we’re committed to working alongside external experts across academia, industry, and government organizations to ensure AlphaGenome benefits as many people as possible.&lt;/p&gt;&lt;p&gt;Together with the collective efforts of the wider scientific community, we hope it will deepen our understanding of the complex cellular processes encoded in the DNA sequence and the effects of variants, and drive exciting new discoveries in genomics and healthcare.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about AlphaGenome&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We would like to thank Juanita Bawagan, Arielle Bier, Stephanie Booth, Irina Andronic, Armin Senoner, Dhavanthi Hariharan, Rob Ashley, Agata Laydon and Kathryn Tunyasuvunakool for their help with the text and figures.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of the AlphaGenome co-authors: Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drotár, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis and Pushmeet Kohli.&lt;/p&gt;&lt;p&gt;We would also like to thank Dhavanthi Hariharan, Charlie Taylor, Ottavia Bertolli, Yannis Assael, Alex Botev, Anna Trostanetski, Lucas Tenório, Victoria Johnston, Richard Green, Kathryn Tunyasuvunakool, Molly Beck, Uchechi Okereke, Rachael Tremlett, Sarah Chakera, Ibrahim I. Taskiran, Andreea-Alexandra Muşat, Raiyan Khan, Ren Yi and the greater Google DeepMind team for their support, help and feedback.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/</guid><pubDate>Wed, 25 Jun 2025 13:59:51 +0000</pubDate></item><item><title>[NEW] Google’s new AI will help researchers understand how our genes work (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1119345/google-deepmind-alphagenome-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624_AlphaGenome.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn’t know what all those 3 billion genetic letters actually do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now Google’s DeepMind division says it’s made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene’s activity will go up or down. It’s just the sort of question biologists regularly assess in lab experiments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“We have, for the first time, created a single model that unifies many different challenges that come with understanding the genome,” says Pushmeet Kohli, a vice president for research at DeepMind.&lt;/p&gt;  &lt;p&gt;Five years ago, the Google AI division released AlphaFold, a technology for predicting the 3D shape of proteins. That work was honored with a Nobel Prize last year and spawned a drug-discovery spinout, Isomorphic Labs, and a boom of companies that hope AI will be able to propose new drugs.&lt;/p&gt; 
 &lt;p&gt;AlphaGenome is an attempt to further smooth biologists’ work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We have these 3 billion letters of DNA that make up a human genome, but every person is slightly different, and we don’t fully understand what those differences do,” says Caleb Lareau, a computational biologist at Memorial Sloan Kettering Cancer Center who has had early access to AlphaGenome. “This is the most powerful tool to date to model that.”&lt;/p&gt; 
 &lt;p&gt;Google says AlphaGenome will be free for noncommercial users and plans to release full details of the model in the future. According to Kohli, the company is exploring ways to “enable use of this model by commercial entities” such as biotech companies.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Lareau says AlphaGenome will allow certain types of experiments now done in the lab to be carried out virtually, on a computer. For instance, studies of people who’ve donated their DNA for research often turn up thousands of genetic differences, each slightly raising or lowering the chance a person gets a disease such as Alzheimer’s.&lt;/p&gt;  &lt;p&gt;Lareau says DeepMind’s software could be used to quickly make predictions about how each of those variants works at a molecular level, something that would otherwise require time-consuming lab experiments. “You’ll get this list of gene variants, but then I want to understand which of those are actually doing something, and where can I intervene,” he says. “This system pushes us closer to a good first guess about what any variant will be doing when we observe it in a human.”&lt;/p&gt;  &lt;p&gt;Don’t expect AlphaGenome to predict very much about individual people, however. It offers&amp;nbsp;clues to nitty-gritty molecular details of gene activity, not 23andMe-type revelations of a person’s traits or ancestry.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“We haven’t designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models,” Google said in a statement.&lt;/p&gt;  &lt;p&gt;Underlying the AI system is the so-called transformer architecture invented at Google that also powers large language models like GPT-4. This one was trained on troves of experimental data produced by public scientific projects.&lt;/p&gt;  &lt;p&gt;Lareau says the system will not broadly change how his lab works day to day but could permit new types of research. For instance, sometimes doctors encounter patients with ultra-rare cancers, bristling with unfamiliar mutations. AlphaGenome could suggest which of those mutations are really causing the root problem, possibly pointing to a treatment.&lt;/p&gt;  &lt;p&gt;“A hallmark of cancer is that specific mutations in DNA make the wrong genes express in the wrong context,” says Julien Gagneur, a professor of computational medicine at the Technical University of Munich. “This type of tool is instrumental in narrowing down which ones mess up proper gene expression.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The same approach could apply to patients with rare genetic disease, many of whom never learn the source of their condition, even if their DNA has been decoded. “We can obtain their genomes, but we are clueless as to which genetic alterations cause the disease,” says Gagneur. He thinks AlphaGenome could give medical scientists a new way to diagnose such cases.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Eventually, some researchers aspire to use AI to design entire genomes from the ground up and create new life forms. Others think the models will be used to create a fully virtual laboratory for drug studies. “My dream would be to simulate a virtual cell,” Demis Hassabis, CEO of Google DeepMind, said this year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Kohli calls AlphaGenome a “milestone” on the road to that kind of system. “AlphaGenome may not model the whole cell in its entirety … but it’s starting to sort of shed light on the broader semantics of DNA,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624_AlphaGenome.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn’t know what all those 3 billion genetic letters actually do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now Google’s DeepMind division says it’s made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene’s activity will go up or down. It’s just the sort of question biologists regularly assess in lab experiments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“We have, for the first time, created a single model that unifies many different challenges that come with understanding the genome,” says Pushmeet Kohli, a vice president for research at DeepMind.&lt;/p&gt;  &lt;p&gt;Five years ago, the Google AI division released AlphaFold, a technology for predicting the 3D shape of proteins. That work was honored with a Nobel Prize last year and spawned a drug-discovery spinout, Isomorphic Labs, and a boom of companies that hope AI will be able to propose new drugs.&lt;/p&gt; 
 &lt;p&gt;AlphaGenome is an attempt to further smooth biologists’ work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We have these 3 billion letters of DNA that make up a human genome, but every person is slightly different, and we don’t fully understand what those differences do,” says Caleb Lareau, a computational biologist at Memorial Sloan Kettering Cancer Center who has had early access to AlphaGenome. “This is the most powerful tool to date to model that.”&lt;/p&gt; 
 &lt;p&gt;Google says AlphaGenome will be free for noncommercial users and plans to release full details of the model in the future. According to Kohli, the company is exploring ways to “enable use of this model by commercial entities” such as biotech companies.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Lareau says AlphaGenome will allow certain types of experiments now done in the lab to be carried out virtually, on a computer. For instance, studies of people who’ve donated their DNA for research often turn up thousands of genetic differences, each slightly raising or lowering the chance a person gets a disease such as Alzheimer’s.&lt;/p&gt;  &lt;p&gt;Lareau says DeepMind’s software could be used to quickly make predictions about how each of those variants works at a molecular level, something that would otherwise require time-consuming lab experiments. “You’ll get this list of gene variants, but then I want to understand which of those are actually doing something, and where can I intervene,” he says. “This system pushes us closer to a good first guess about what any variant will be doing when we observe it in a human.”&lt;/p&gt;  &lt;p&gt;Don’t expect AlphaGenome to predict very much about individual people, however. It offers&amp;nbsp;clues to nitty-gritty molecular details of gene activity, not 23andMe-type revelations of a person’s traits or ancestry.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“We haven’t designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models,” Google said in a statement.&lt;/p&gt;  &lt;p&gt;Underlying the AI system is the so-called transformer architecture invented at Google that also powers large language models like GPT-4. This one was trained on troves of experimental data produced by public scientific projects.&lt;/p&gt;  &lt;p&gt;Lareau says the system will not broadly change how his lab works day to day but could permit new types of research. For instance, sometimes doctors encounter patients with ultra-rare cancers, bristling with unfamiliar mutations. AlphaGenome could suggest which of those mutations are really causing the root problem, possibly pointing to a treatment.&lt;/p&gt;  &lt;p&gt;“A hallmark of cancer is that specific mutations in DNA make the wrong genes express in the wrong context,” says Julien Gagneur, a professor of computational medicine at the Technical University of Munich. “This type of tool is instrumental in narrowing down which ones mess up proper gene expression.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The same approach could apply to patients with rare genetic disease, many of whom never learn the source of their condition, even if their DNA has been decoded. “We can obtain their genomes, but we are clueless as to which genetic alterations cause the disease,” says Gagneur. He thinks AlphaGenome could give medical scientists a new way to diagnose such cases.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Eventually, some researchers aspire to use AI to design entire genomes from the ground up and create new life forms. Others think the models will be used to create a fully virtual laboratory for drug studies. “My dream would be to simulate a virtual cell,” Demis Hassabis, CEO of Google DeepMind, said this year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Kohli calls AlphaGenome a “milestone” on the road to that kind of system. “AlphaGenome may not model the whole cell in its entirety … but it’s starting to sort of shed light on the broader semantics of DNA,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1119345/google-deepmind-alphagenome-ai/</guid><pubDate>Wed, 25 Jun 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Winning capital for your AI startup? Kleida Martiro is leading the conversation at TechCrunch All Stage (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/winning-capital-for-your-ai-startup-kleida-martiro-is-leading-the-conversation-at-techcrunch-all-stage/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-native startups are rewriting the rules of what early traction looks like — and too often, investors are still playing by the old ones. At &lt;strong&gt;TechCrunch All Stage&lt;/strong&gt;, happening in Boston on July 15, &lt;strong&gt;Kleida Martiro&lt;/strong&gt;, partner at Glasswing Ventures, will lead a breakout that cuts straight to the core of this disconnect.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Her session, &lt;strong&gt;Winning Capital in a Competitive Market: How to Fund Your AI-Native Startup,&lt;/strong&gt; on the &lt;strong&gt;Foundation Stage&lt;/strong&gt;, explores how early-stage AI founders can frame their growth story in a way that resonates with forward-thinking investors — and filter out the ones still stuck in SaaS-era metrics.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch All Stage 2025 Kleida Martiro" class="wp-image-3021950" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/Kleida-Martiro-SpeakerArticleImageHeader_TCAllStage.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-founders-will-take-away"&gt;What founders will take away&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Martiro brings the perfect blend of operator and investor experience to the table. Before joining Glasswing, she led machine learning efforts as Head of Data Science at SocialFlow and now serves on multiple AI startup boards. She’s also been named one of Business Insider’s “41 Most Important VCs in Boston” and a VentureBeat “Women in AI Rising Star.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Expect a tactical conversation on aligning your vision, funding needs, and long-term opportunity with the capital partners who actually get what you’re building. If you’re building AI from the ground up, this is the room to be in. You’ll have plenty of time during the session to ask your burning questions about launching and scaling an AI startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your pass to TechCrunch All Stage&lt;/strong&gt; before prices rise at the door.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-native startups are rewriting the rules of what early traction looks like — and too often, investors are still playing by the old ones. At &lt;strong&gt;TechCrunch All Stage&lt;/strong&gt;, happening in Boston on July 15, &lt;strong&gt;Kleida Martiro&lt;/strong&gt;, partner at Glasswing Ventures, will lead a breakout that cuts straight to the core of this disconnect.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Her session, &lt;strong&gt;Winning Capital in a Competitive Market: How to Fund Your AI-Native Startup,&lt;/strong&gt; on the &lt;strong&gt;Foundation Stage&lt;/strong&gt;, explores how early-stage AI founders can frame their growth story in a way that resonates with forward-thinking investors — and filter out the ones still stuck in SaaS-era metrics.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch All Stage 2025 Kleida Martiro" class="wp-image-3021950" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/Kleida-Martiro-SpeakerArticleImageHeader_TCAllStage.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-founders-will-take-away"&gt;What founders will take away&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Martiro brings the perfect blend of operator and investor experience to the table. Before joining Glasswing, she led machine learning efforts as Head of Data Science at SocialFlow and now serves on multiple AI startup boards. She’s also been named one of Business Insider’s “41 Most Important VCs in Boston” and a VentureBeat “Women in AI Rising Star.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Expect a tactical conversation on aligning your vision, funding needs, and long-term opportunity with the capital partners who actually get what you’re building. If you’re building AI from the ground up, this is the room to be in. You’ll have plenty of time during the session to ask your burning questions about launching and scaling an AI startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your pass to TechCrunch All Stage&lt;/strong&gt; before prices rise at the door.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/winning-capital-for-your-ai-startup-kleida-martiro-is-leading-the-conversation-at-techcrunch-all-stage/</guid><pubDate>Wed, 25 Jun 2025 14:30:00 +0000</pubDate></item><item><title>[NEW] Enterprises must rethink IAM as AI agents outnumber humans 10 to 1 (AI News | VentureBeat)</title><link>https://venturebeat.com/security/identity-becomes-the-control-plane-for-enterprise-ai-security/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Stolen credentials are responsible for 80% of enterprise breaches. Every major security vendor has converged on the same conclusion: Identity is now the control plane for AI security. Scale alone demands this shift. Enterprises managing 100,000 employees will handle more than one million identities when AI agents enter production.&lt;/p&gt;



&lt;p&gt;Traditional identity access management (IAM) architectures can’t scale to secure the proliferation of agentic AI. They were built for thousands of human users, not millions of autonomous agents operating at machine speed with human-level permissions. The industry response represents the most significant security transformation since the adoption of cloud computing.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-proximity-based-authentication-replaces-hardware-tokens"&gt;Proximity-based authentication replaces hardware tokens&lt;/h2&gt;



&lt;p&gt;Leading vendors now use Bluetooth Low Energy (BLE) between devices and laptops to prove physical proximity. Combined with cryptographic identities and biometrics, this creates four-factor authentication without the need for hardware tokens.&lt;/p&gt;



&lt;p&gt;Cisco’s Duo demonstrates this innovation at scale. Their proximity verification delivers phishing-resistant authentication using BLE-based proximity in conjunction with biometric verification. This capability, unveiled at Cisco Live 2025, represents a fundamental shift in authentication architecture.&lt;/p&gt;



&lt;p&gt;Microsoft’s Entra ID handles 10,000 AI agents in single pilot programs while processing 8 billion authentications daily. “Traditional directory services weren’t architected for autonomous systems operating at this velocity,” states Alex Simons, CVP of identity at Microsoft.&lt;/p&gt;



&lt;p&gt;Ping Identity’s DaVinci orchestration platform pushes further. The system processes more than 1 billion authentication events daily, with AI agents accounting for 60% of the traffic. Each verification completes in under 200 milliseconds while maintaining cryptographic proof.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-behavioral-analytics-catch-compromised-agents-in-real-time"&gt;Behavioral analytics catch compromised agents in real time&lt;/h2&gt;



&lt;p&gt;CrowdStrike treats AI agents like any other identity threat. Their Falcon platform establishes behavioral baselines for each agent within 24 hours. Deviations trigger automated containment within seconds.&lt;/p&gt;



&lt;p&gt;“When an AI agent suddenly accesses systems outside its established pattern, we treat it identically to a compromised employee credential,” Adam Meyers, head of counter adversary operations at CrowdStrike, told VentureBeat. The platform tracks 15 billion AI-related events daily across customer environments.&lt;/p&gt;



&lt;p&gt;That speed matters. CrowdStrike’s 2025 Global Threat Report documents that adversaries are achieving initial access in less than 10 minutes. They move laterally across 15 systems within the first hour. AI agents operating with compromised identities amplify this damage exponentially.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-identity-resilience-prevents-catastrophic-failures"&gt;Identity resilience prevents catastrophic failures&lt;/h2&gt;



&lt;p&gt;Enterprises average 89 different identity stores across cloud and on-premises systems, according to Gartner. This fragmentation creates blind spots that adversaries exploit daily. The fix applies networking principles to identity infrastructure.&lt;/p&gt;



&lt;p&gt;Okta’s Advanced Server Access implements redundancy, load balancing and automated failover across identity providers. When primary authentication fails, secondary systems activate within 50 milliseconds. This becomes mandatory when AI agents execute thousands of operations per second.&lt;/p&gt;



&lt;p&gt;“Identity is security,” Todd McKinnon, CEO of Okta, said at Oktane 2024. “When you move AI into production, you give agents access to real systems, real data and your customer data. One compromised agent identity cascades across millions of automated actions.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-zero-trust-scales-for-agent-proliferation"&gt;Zero trust scales for agent proliferation&lt;/h2&gt;



&lt;p&gt;Palo Alto Networks’ Cortex XSIAM completely abandons perimeter defense. The platform operates on the assumption of continuous compromise. Every AI agent undergoes verification before each action, not just at initial authentication.&lt;/p&gt;



&lt;p&gt;Mike Riemer, Field CISO at Ivanti, reinforced the zero trust approach in a recent interview with VenturBeat: “It operates on the principle of ‘never trust, always verify.’ By adopting a zero trust architecture, organizations can ensure that only authenticated users and devices gain access to sensitive data and applications.”&lt;/p&gt;



&lt;p&gt;Cisco’s Universal ZTNA extends this model to AI agents. The platform expands zero trust beyond humans and IoT devices to encompass autonomous AI systems, providing automated discovery and delegated authorization at scale.&lt;/p&gt;



&lt;p&gt;Automated playbooks respond instantly to identity anomalies. When malware triggers authentication irregularities, XSIAM revokes access and launches forensic analysis without human intervention. This zero-latency response becomes the operational baseline.&lt;/p&gt;



&lt;p&gt;Zscaler CEO Jay Chaudhry identified the core vulnerability at Zenith Live 2025: “Network protocols were designed to allow trusted devices to communicate freely. AI weaponizes this legacy architecture at scale. Adversaries craft phishing campaigns that compromise agent identities faster than humans can respond.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-universal-ztna-frameworks-enable-million-agent-deployments"&gt;Universal ZTNA frameworks enable million-agent deployments&lt;/h2&gt;



&lt;p&gt;The architectural requirements are clear. Universal zero trust network access (ZTNA) frameworks across the industry provide four capabilities essential for AI environments.&lt;/p&gt;



&lt;p&gt;Cisco’s implementation demonstrates the scale required. Their Universal ZTNA platform performs automated discovery scans every 60 seconds, cataloging new AI deployments and permission sets. This eliminates blind spots that attackers target. Cisco’s delegated authorization engine enforces least-privilege boundaries through policy engines processing 100,000 decisions per second.&lt;/p&gt;



&lt;p&gt;Comprehensive audit trails capture every agent action for forensic investigation. Security teams using platforms like Cisco’s can reconstruct incidents across millions of interactions. Native support for standards like the Model Context Protocol ensures interoperability as the ecosystem evolves.&lt;/p&gt;



&lt;p&gt;Ivanti’s approach complements these capabilities with AI-powered analytics. Daren Goeson, SVP of product management at Ivanti, emphasizes: “AI-powered endpoint security tools can analyze vast amounts of data to detect anomalies and predict potential threats faster and more accurately than any human analyst. These tools provide clear visibility across devices, users and networks, proactively identifying potential security gaps.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-cisco-s-ai-security-architecture-sets-industry-direction"&gt;Cisco’s AI security architecture sets industry direction&lt;/h2&gt;



&lt;p&gt;Cisco’s AI Secure Factory positions them as the first non-Nvidia silicon provider in Nvidia’s reference architecture. By combining post-quantum encryption with new devices, Cisco is building infrastructure to protect against threats that don’t yet exist. The enterprise takeaway: Securing AI isn’t optional; it’s architectural.&lt;/p&gt;



&lt;p&gt;At Cisco Live 2025, the company unveiled a comprehensive identity and AI security strategy that addresses every layer of the stack:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Announcement&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Core problem solved / strategic value&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Technical details&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Hybrid mesh firewall (incl. HyperShield)&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Distributed, fabric-native security; moves security from the perimeter into the network fabric&lt;/td&gt;&lt;td&gt;eBPF-based enforcement; hardware acceleration&lt;/td&gt;&lt;td&gt;New firewalls: Oct 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Live protect&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Closes “45-day patch vs. 3-day exploit” gap with rapid, kernel-level vulnerability shielding&lt;/td&gt;&lt;td&gt;Real-time patching without reboots&lt;/td&gt;&lt;td&gt;Nexus OS: Sept 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Splunk: Free firewall log ingestion&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Reduces SIEM costs up to 80%; incentivizes Cisco firewall adoption&lt;/td&gt;&lt;td&gt;Unlimited log ingestion from Cisco firewalls&lt;/td&gt;&lt;td&gt;Aug 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Splunk: Observability for AI&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Provides critical visibility into AI stack performance&lt;/td&gt;&lt;td&gt;Monitors GPU utilization and model performance&lt;/td&gt;&lt;td&gt;Sept 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Duo IAM&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Evolves from MFA to a complete security-first IAM platform&lt;/td&gt;&lt;td&gt;User Directory, SSO, Identity Routing Engine&lt;/td&gt;&lt;td&gt;Available Now&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Duo: Proximity verification&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Delivers phishing-resistant authentication without hardware tokens&lt;/td&gt;&lt;td&gt;BLE-based proximity, biometric verification&lt;/td&gt;&lt;td&gt;Part of the new Duo IAM&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Duo: Identity resilience&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Addresses critical IDP outage risks&lt;/td&gt;&lt;td&gt;Redundancy, load balancing and automated failover&lt;/td&gt;&lt;td&gt;In development&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Cisco universal ZTNA&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Expands zero trust to humans, IoT/OT devices and AI agents&lt;/td&gt;&lt;td&gt;Automated discovery, delegated authorization&lt;/td&gt;&lt;td&gt;Ongoing evolution&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Open-sourced security AI model&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Democratizes AI defense; 8B parameters match 70B model performance&lt;/td&gt;&lt;td&gt;Runs on CPU; 5B security tokens training&lt;/td&gt;&lt;td&gt;Available (Hugging Face)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;AI defense and Nvidia partnership&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Secures AI development pipeline&lt;/td&gt;&lt;td&gt;Nvidia NIM microservices optimization&lt;/td&gt;&lt;td&gt;Available now&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Post-quantum security&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Future-proof against quantum attacks&lt;/td&gt;&lt;td&gt;MACsec and IPsec encryption&lt;/td&gt;&lt;td&gt;New devices (June 2025)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Identity intelligence&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Continuous behavioral monitoring&lt;/td&gt;&lt;td&gt;AI-powered anomaly detection&lt;/td&gt;&lt;td&gt;Part of Security Cloud&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Secure access&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Converges VPN and ZTNA capabilities&lt;/td&gt;&lt;td&gt;Cloud-delivered secure access service edge&lt;/td&gt;&lt;td&gt;Available now&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-cross-vendor-collaboration-accelerates"&gt;Cross-vendor collaboration accelerates&lt;/h2&gt;



&lt;p&gt;The Cloud Security Alliance Zero Trust Advancement Center now includes every major security vendor. This unprecedented cooperation enables unified security policies across platforms.&lt;/p&gt;



&lt;p&gt;“Security vendors must unite against common threats,” George Kurtz, CEO of CrowdStrike, emphasized during a recent platform strategy discussion. “The data-centric approach wins given how fast adversaries and threats evolve.”&lt;/p&gt;



&lt;p&gt;Cisco President and CPO Jeetu Patel echoed this sentiment in an interview with VentureBeat: “Security is a prerequisite for adoption of AI. If people don’t trust the system, they’re not going to use it.”&lt;/p&gt;



&lt;p&gt;The organizational challenge remains. Robert Grazioli, CIO at Ivanti, identifies the critical barrier: “CISO and CIO alignment will be critical in 2025. This collaboration is essential if we are to safeguard modern businesses effectively. Executives need to consolidate resources — budgets, personnel, data and technology — to enhance an organization’s security posture.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-identity-reckoning"&gt;The identity reckoning&lt;/h2&gt;



&lt;p&gt;When Cisco, Okta, Zscaler, Palo Alto Networks and CrowdStrike independently reach identical conclusions about identity architecture, it’s confirmation, not coincidence.&lt;/p&gt;



&lt;p&gt;Identity infrastructure determines security outcomes. Organizations face two options: Architect identity as the control plane or accept breaches as inevitable. The gap between AI deployment speed and identity security maturity narrows daily.&lt;/p&gt;



&lt;p&gt;Three actions cannot wait. Audit every AI agent’s identity and permissions within 30 days. Deploy continuous verification for all non-human identities immediately. Establish 24/7 identity security operations to prevent adversaries from exploiting gaps.&lt;/p&gt;



&lt;p&gt;The vendor consensus sends a clear and unmistakable signal. Identity has become the control plane for AI security. Enterprises that fail to adapt will spend 2025 managing breaches instead of innovation.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Stolen credentials are responsible for 80% of enterprise breaches. Every major security vendor has converged on the same conclusion: Identity is now the control plane for AI security. Scale alone demands this shift. Enterprises managing 100,000 employees will handle more than one million identities when AI agents enter production.&lt;/p&gt;



&lt;p&gt;Traditional identity access management (IAM) architectures can’t scale to secure the proliferation of agentic AI. They were built for thousands of human users, not millions of autonomous agents operating at machine speed with human-level permissions. The industry response represents the most significant security transformation since the adoption of cloud computing.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-proximity-based-authentication-replaces-hardware-tokens"&gt;Proximity-based authentication replaces hardware tokens&lt;/h2&gt;



&lt;p&gt;Leading vendors now use Bluetooth Low Energy (BLE) between devices and laptops to prove physical proximity. Combined with cryptographic identities and biometrics, this creates four-factor authentication without the need for hardware tokens.&lt;/p&gt;



&lt;p&gt;Cisco’s Duo demonstrates this innovation at scale. Their proximity verification delivers phishing-resistant authentication using BLE-based proximity in conjunction with biometric verification. This capability, unveiled at Cisco Live 2025, represents a fundamental shift in authentication architecture.&lt;/p&gt;



&lt;p&gt;Microsoft’s Entra ID handles 10,000 AI agents in single pilot programs while processing 8 billion authentications daily. “Traditional directory services weren’t architected for autonomous systems operating at this velocity,” states Alex Simons, CVP of identity at Microsoft.&lt;/p&gt;



&lt;p&gt;Ping Identity’s DaVinci orchestration platform pushes further. The system processes more than 1 billion authentication events daily, with AI agents accounting for 60% of the traffic. Each verification completes in under 200 milliseconds while maintaining cryptographic proof.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-behavioral-analytics-catch-compromised-agents-in-real-time"&gt;Behavioral analytics catch compromised agents in real time&lt;/h2&gt;



&lt;p&gt;CrowdStrike treats AI agents like any other identity threat. Their Falcon platform establishes behavioral baselines for each agent within 24 hours. Deviations trigger automated containment within seconds.&lt;/p&gt;



&lt;p&gt;“When an AI agent suddenly accesses systems outside its established pattern, we treat it identically to a compromised employee credential,” Adam Meyers, head of counter adversary operations at CrowdStrike, told VentureBeat. The platform tracks 15 billion AI-related events daily across customer environments.&lt;/p&gt;



&lt;p&gt;That speed matters. CrowdStrike’s 2025 Global Threat Report documents that adversaries are achieving initial access in less than 10 minutes. They move laterally across 15 systems within the first hour. AI agents operating with compromised identities amplify this damage exponentially.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-identity-resilience-prevents-catastrophic-failures"&gt;Identity resilience prevents catastrophic failures&lt;/h2&gt;



&lt;p&gt;Enterprises average 89 different identity stores across cloud and on-premises systems, according to Gartner. This fragmentation creates blind spots that adversaries exploit daily. The fix applies networking principles to identity infrastructure.&lt;/p&gt;



&lt;p&gt;Okta’s Advanced Server Access implements redundancy, load balancing and automated failover across identity providers. When primary authentication fails, secondary systems activate within 50 milliseconds. This becomes mandatory when AI agents execute thousands of operations per second.&lt;/p&gt;



&lt;p&gt;“Identity is security,” Todd McKinnon, CEO of Okta, said at Oktane 2024. “When you move AI into production, you give agents access to real systems, real data and your customer data. One compromised agent identity cascades across millions of automated actions.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-zero-trust-scales-for-agent-proliferation"&gt;Zero trust scales for agent proliferation&lt;/h2&gt;



&lt;p&gt;Palo Alto Networks’ Cortex XSIAM completely abandons perimeter defense. The platform operates on the assumption of continuous compromise. Every AI agent undergoes verification before each action, not just at initial authentication.&lt;/p&gt;



&lt;p&gt;Mike Riemer, Field CISO at Ivanti, reinforced the zero trust approach in a recent interview with VenturBeat: “It operates on the principle of ‘never trust, always verify.’ By adopting a zero trust architecture, organizations can ensure that only authenticated users and devices gain access to sensitive data and applications.”&lt;/p&gt;



&lt;p&gt;Cisco’s Universal ZTNA extends this model to AI agents. The platform expands zero trust beyond humans and IoT devices to encompass autonomous AI systems, providing automated discovery and delegated authorization at scale.&lt;/p&gt;



&lt;p&gt;Automated playbooks respond instantly to identity anomalies. When malware triggers authentication irregularities, XSIAM revokes access and launches forensic analysis without human intervention. This zero-latency response becomes the operational baseline.&lt;/p&gt;



&lt;p&gt;Zscaler CEO Jay Chaudhry identified the core vulnerability at Zenith Live 2025: “Network protocols were designed to allow trusted devices to communicate freely. AI weaponizes this legacy architecture at scale. Adversaries craft phishing campaigns that compromise agent identities faster than humans can respond.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-universal-ztna-frameworks-enable-million-agent-deployments"&gt;Universal ZTNA frameworks enable million-agent deployments&lt;/h2&gt;



&lt;p&gt;The architectural requirements are clear. Universal zero trust network access (ZTNA) frameworks across the industry provide four capabilities essential for AI environments.&lt;/p&gt;



&lt;p&gt;Cisco’s implementation demonstrates the scale required. Their Universal ZTNA platform performs automated discovery scans every 60 seconds, cataloging new AI deployments and permission sets. This eliminates blind spots that attackers target. Cisco’s delegated authorization engine enforces least-privilege boundaries through policy engines processing 100,000 decisions per second.&lt;/p&gt;



&lt;p&gt;Comprehensive audit trails capture every agent action for forensic investigation. Security teams using platforms like Cisco’s can reconstruct incidents across millions of interactions. Native support for standards like the Model Context Protocol ensures interoperability as the ecosystem evolves.&lt;/p&gt;



&lt;p&gt;Ivanti’s approach complements these capabilities with AI-powered analytics. Daren Goeson, SVP of product management at Ivanti, emphasizes: “AI-powered endpoint security tools can analyze vast amounts of data to detect anomalies and predict potential threats faster and more accurately than any human analyst. These tools provide clear visibility across devices, users and networks, proactively identifying potential security gaps.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-cisco-s-ai-security-architecture-sets-industry-direction"&gt;Cisco’s AI security architecture sets industry direction&lt;/h2&gt;



&lt;p&gt;Cisco’s AI Secure Factory positions them as the first non-Nvidia silicon provider in Nvidia’s reference architecture. By combining post-quantum encryption with new devices, Cisco is building infrastructure to protect against threats that don’t yet exist. The enterprise takeaway: Securing AI isn’t optional; it’s architectural.&lt;/p&gt;



&lt;p&gt;At Cisco Live 2025, the company unveiled a comprehensive identity and AI security strategy that addresses every layer of the stack:&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Announcement&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Core problem solved / strategic value&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Technical details&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;Availability&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Hybrid mesh firewall (incl. HyperShield)&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Distributed, fabric-native security; moves security from the perimeter into the network fabric&lt;/td&gt;&lt;td&gt;eBPF-based enforcement; hardware acceleration&lt;/td&gt;&lt;td&gt;New firewalls: Oct 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Live protect&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Closes “45-day patch vs. 3-day exploit” gap with rapid, kernel-level vulnerability shielding&lt;/td&gt;&lt;td&gt;Real-time patching without reboots&lt;/td&gt;&lt;td&gt;Nexus OS: Sept 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Splunk: Free firewall log ingestion&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Reduces SIEM costs up to 80%; incentivizes Cisco firewall adoption&lt;/td&gt;&lt;td&gt;Unlimited log ingestion from Cisco firewalls&lt;/td&gt;&lt;td&gt;Aug 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Splunk: Observability for AI&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Provides critical visibility into AI stack performance&lt;/td&gt;&lt;td&gt;Monitors GPU utilization and model performance&lt;/td&gt;&lt;td&gt;Sept 2025&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Duo IAM&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Evolves from MFA to a complete security-first IAM platform&lt;/td&gt;&lt;td&gt;User Directory, SSO, Identity Routing Engine&lt;/td&gt;&lt;td&gt;Available Now&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Duo: Proximity verification&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Delivers phishing-resistant authentication without hardware tokens&lt;/td&gt;&lt;td&gt;BLE-based proximity, biometric verification&lt;/td&gt;&lt;td&gt;Part of the new Duo IAM&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Duo: Identity resilience&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Addresses critical IDP outage risks&lt;/td&gt;&lt;td&gt;Redundancy, load balancing and automated failover&lt;/td&gt;&lt;td&gt;In development&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Cisco universal ZTNA&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Expands zero trust to humans, IoT/OT devices and AI agents&lt;/td&gt;&lt;td&gt;Automated discovery, delegated authorization&lt;/td&gt;&lt;td&gt;Ongoing evolution&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Open-sourced security AI model&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Democratizes AI defense; 8B parameters match 70B model performance&lt;/td&gt;&lt;td&gt;Runs on CPU; 5B security tokens training&lt;/td&gt;&lt;td&gt;Available (Hugging Face)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;AI defense and Nvidia partnership&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Secures AI development pipeline&lt;/td&gt;&lt;td&gt;Nvidia NIM microservices optimization&lt;/td&gt;&lt;td&gt;Available now&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Post-quantum security&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Future-proof against quantum attacks&lt;/td&gt;&lt;td&gt;MACsec and IPsec encryption&lt;/td&gt;&lt;td&gt;New devices (June 2025)&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Identity intelligence&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Continuous behavioral monitoring&lt;/td&gt;&lt;td&gt;AI-powered anomaly detection&lt;/td&gt;&lt;td&gt;Part of Security Cloud&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Secure access&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;Converges VPN and ZTNA capabilities&lt;/td&gt;&lt;td&gt;Cloud-delivered secure access service edge&lt;/td&gt;&lt;td&gt;Available now&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-cross-vendor-collaboration-accelerates"&gt;Cross-vendor collaboration accelerates&lt;/h2&gt;



&lt;p&gt;The Cloud Security Alliance Zero Trust Advancement Center now includes every major security vendor. This unprecedented cooperation enables unified security policies across platforms.&lt;/p&gt;



&lt;p&gt;“Security vendors must unite against common threats,” George Kurtz, CEO of CrowdStrike, emphasized during a recent platform strategy discussion. “The data-centric approach wins given how fast adversaries and threats evolve.”&lt;/p&gt;



&lt;p&gt;Cisco President and CPO Jeetu Patel echoed this sentiment in an interview with VentureBeat: “Security is a prerequisite for adoption of AI. If people don’t trust the system, they’re not going to use it.”&lt;/p&gt;



&lt;p&gt;The organizational challenge remains. Robert Grazioli, CIO at Ivanti, identifies the critical barrier: “CISO and CIO alignment will be critical in 2025. This collaboration is essential if we are to safeguard modern businesses effectively. Executives need to consolidate resources — budgets, personnel, data and technology — to enhance an organization’s security posture.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-identity-reckoning"&gt;The identity reckoning&lt;/h2&gt;



&lt;p&gt;When Cisco, Okta, Zscaler, Palo Alto Networks and CrowdStrike independently reach identical conclusions about identity architecture, it’s confirmation, not coincidence.&lt;/p&gt;



&lt;p&gt;Identity infrastructure determines security outcomes. Organizations face two options: Architect identity as the control plane or accept breaches as inevitable. The gap between AI deployment speed and identity security maturity narrows daily.&lt;/p&gt;



&lt;p&gt;Three actions cannot wait. Audit every AI agent’s identity and permissions within 30 days. Deploy continuous verification for all non-human identities immediately. Establish 24/7 identity security operations to prevent adversaries from exploiting gaps.&lt;/p&gt;



&lt;p&gt;The vendor consensus sends a clear and unmistakable signal. Identity has become the control plane for AI security. Enterprises that fail to adapt will spend 2025 managing breaches instead of innovation.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/security/identity-becomes-the-control-plane-for-enterprise-ai-security/</guid><pubDate>Wed, 25 Jun 2025 15:05:00 +0000</pubDate></item><item><title>[NEW] NO FAKES Act: AI deepfakes protection or internet freedom threat? (AI News)</title><link>https://www.artificialintelligence-news.com/news/no-fakes-act-ai-deepfakes-protection-internet-freedom-threat/</link><description>&lt;p&gt;Critics fear the revised NO FAKES Act has morphed from targeted AI deepfakes protection into sweeping censorship powers.&lt;/p&gt;&lt;p&gt;What began as a seemingly reasonable attempt to tackle AI-generated deepfakes has snowballed into something far more troubling, according to digital rights advocates. The much-discussed Nurture Originals, Foster Art, and Keep Entertainment Safe (NO FAKES) Act – originally aimed at preventing unauthorised digital replicas of people – now threatens to fundamentally alter how the internet functions.&lt;/p&gt;&lt;p&gt;The bill’s expansion has set alarm bells ringing throughout the tech community. It’s gone well beyond simply protecting celebrities from fake videos to potentially creating a sweeping censorship framework.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-sensible-safeguards-to-sledgehammer-approach"&gt;From sensible safeguards to sledgehammer approach&lt;/h3&gt;&lt;p&gt;The initial idea wasn’t entirely misguided: to create protections against AI systems generating fake videos of real people without permission. We’ve all seen those unsettling deepfakes circulating online.&lt;/p&gt;&lt;p&gt;But rather than crafting narrow, targeted measures, lawmakers have opted for what the Electronic Frontier Foundation calls a “federalised image-licensing system” that goes far beyond reasonable protections.&lt;/p&gt;&lt;p&gt;“The updated bill doubles down on that initial mistaken approach,” the EFF notes, “by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them.”&lt;/p&gt;&lt;p&gt;What’s particularly worrying is the NO FAKES Act’s requirement for nearly every internet platform to implement systems that would not only remove content after receiving takedown notices but also prevent similar content from ever being uploaded again. Essentially, it’s forcing platforms to deploy content filters that have proven notoriously unreliable in other contexts.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-innovation-chilling"&gt;Innovation-chilling&lt;/h3&gt;&lt;p&gt;Perhaps most concerning for the AI sector is how the NO FAKES Act targets the tools themselves. The revised bill wouldn’t just go after harmful content; it would potentially shut down entire development platforms and software tools that could be used to create unauthorised images.&lt;/p&gt;&lt;p&gt;This approach feels reminiscent of trying to ban word processors because someone might use one to write defamatory content. The bill includes some limitations (e.g. tools must be “primarily designed” for making unauthorised replicas or have limited other commercial uses) but these distinctions are notoriously subject to interpretation.&lt;/p&gt;&lt;p&gt;Small UK startups venturing into AI image generation could find themselves caught in expensive legal battles based on flimsy allegations long before they have a chance to establish themselves. Meanwhile, tech giants with armies of lawyers can better weather such storms, potentially entrenching their dominance.&lt;/p&gt;&lt;p&gt;Anyone who’s dealt with YouTube’s ContentID system or similar copyright filtering tools knows how frustratingly imprecise they can be. These systems routinely flag legitimate content like musicians performing their own songs or creators using material under fair dealing provisions.&lt;/p&gt;&lt;p&gt;The NO FAKES Act would effectively mandate similar filtering systems across the internet. While it includes carve-outs for parody, satire, and commentary, enforcing these distinctions algorithmically has proven virtually impossible.&lt;/p&gt;&lt;p&gt;“These systems often flag things that are similar but not the same,” the EFF explains, “like two different people playing the same piece of public domain music.”&lt;/p&gt;&lt;p&gt;For smaller platforms without Google-scale resources, implementing such filters could prove prohibitively expensive. The likely outcome? Many would simply over-censor to avoid legal risk.&lt;/p&gt;&lt;p&gt;In fact, one might expect major tech companies to oppose such sweeping regulation. However, many have remained conspicuously quiet. Some industry observers suggest this isn’t coincidental—established giants can more easily absorb compliance costs that would crush smaller competitors.&lt;/p&gt;&lt;p&gt;“It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES,” the EFF notes.&lt;/p&gt;&lt;p&gt;This pattern repeats throughout tech regulation history—what appears to be regulation reigning in Big Tech often ends up cementing their market position by creating barriers too costly for newcomers to overcome.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-no-fakes-act-threatens-anonymous-speech"&gt;NO FAKES Act threatens anonymous speech&lt;/h3&gt;&lt;p&gt;Tucked away in the legislation is another troubling provision that could expose anonymous internet users based on mere allegations. The bill would allow anyone to obtain a subpoena from a court clerk – without judicial review or evidence – forcing services to reveal identifying information about users accused of creating unauthorised replicas.&lt;/p&gt;&lt;p&gt;History shows such mechanisms are ripe for abuse. Critics with valid points can be unmasked and potentially harassed when their commentary includes screenshots or quotes from the very people trying to silence them.&lt;/p&gt;&lt;p&gt;This vulnerability could have a profound effect on legitimate criticism and whistleblowing. Imagine exposing corporate misconduct only to have your identity revealed through a rubber-stamp subpoena process.&lt;/p&gt;&lt;p&gt;This push for additional regulation seems odd given that Congress recently passed the Take It Down Act, which already targets images involving intimate or sexual content. That legislation itself raised privacy concerns, particularly around monitoring encrypted communications.&lt;/p&gt;&lt;p&gt;Rather than assess the impacts of existing legislation, lawmakers seem determined to push forward with broader restrictions that could reshape internet governance for decades to come.&lt;/p&gt;&lt;p&gt;The coming weeks will prove critical as the NO FAKES Act moves through the legislative process. For anyone who values internet freedom, innovation, and balanced approaches to emerging technology challenges, this bears close watching indeed.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Markus Spiske)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;The OpenAI Files: Ex-staff claim profit greed betraying AI safety&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Critics fear the revised NO FAKES Act has morphed from targeted AI deepfakes protection into sweeping censorship powers.&lt;/p&gt;&lt;p&gt;What began as a seemingly reasonable attempt to tackle AI-generated deepfakes has snowballed into something far more troubling, according to digital rights advocates. The much-discussed Nurture Originals, Foster Art, and Keep Entertainment Safe (NO FAKES) Act – originally aimed at preventing unauthorised digital replicas of people – now threatens to fundamentally alter how the internet functions.&lt;/p&gt;&lt;p&gt;The bill’s expansion has set alarm bells ringing throughout the tech community. It’s gone well beyond simply protecting celebrities from fake videos to potentially creating a sweeping censorship framework.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-sensible-safeguards-to-sledgehammer-approach"&gt;From sensible safeguards to sledgehammer approach&lt;/h3&gt;&lt;p&gt;The initial idea wasn’t entirely misguided: to create protections against AI systems generating fake videos of real people without permission. We’ve all seen those unsettling deepfakes circulating online.&lt;/p&gt;&lt;p&gt;But rather than crafting narrow, targeted measures, lawmakers have opted for what the Electronic Frontier Foundation calls a “federalised image-licensing system” that goes far beyond reasonable protections.&lt;/p&gt;&lt;p&gt;“The updated bill doubles down on that initial mistaken approach,” the EFF notes, “by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them.”&lt;/p&gt;&lt;p&gt;What’s particularly worrying is the NO FAKES Act’s requirement for nearly every internet platform to implement systems that would not only remove content after receiving takedown notices but also prevent similar content from ever being uploaded again. Essentially, it’s forcing platforms to deploy content filters that have proven notoriously unreliable in other contexts.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-innovation-chilling"&gt;Innovation-chilling&lt;/h3&gt;&lt;p&gt;Perhaps most concerning for the AI sector is how the NO FAKES Act targets the tools themselves. The revised bill wouldn’t just go after harmful content; it would potentially shut down entire development platforms and software tools that could be used to create unauthorised images.&lt;/p&gt;&lt;p&gt;This approach feels reminiscent of trying to ban word processors because someone might use one to write defamatory content. The bill includes some limitations (e.g. tools must be “primarily designed” for making unauthorised replicas or have limited other commercial uses) but these distinctions are notoriously subject to interpretation.&lt;/p&gt;&lt;p&gt;Small UK startups venturing into AI image generation could find themselves caught in expensive legal battles based on flimsy allegations long before they have a chance to establish themselves. Meanwhile, tech giants with armies of lawyers can better weather such storms, potentially entrenching their dominance.&lt;/p&gt;&lt;p&gt;Anyone who’s dealt with YouTube’s ContentID system or similar copyright filtering tools knows how frustratingly imprecise they can be. These systems routinely flag legitimate content like musicians performing their own songs or creators using material under fair dealing provisions.&lt;/p&gt;&lt;p&gt;The NO FAKES Act would effectively mandate similar filtering systems across the internet. While it includes carve-outs for parody, satire, and commentary, enforcing these distinctions algorithmically has proven virtually impossible.&lt;/p&gt;&lt;p&gt;“These systems often flag things that are similar but not the same,” the EFF explains, “like two different people playing the same piece of public domain music.”&lt;/p&gt;&lt;p&gt;For smaller platforms without Google-scale resources, implementing such filters could prove prohibitively expensive. The likely outcome? Many would simply over-censor to avoid legal risk.&lt;/p&gt;&lt;p&gt;In fact, one might expect major tech companies to oppose such sweeping regulation. However, many have remained conspicuously quiet. Some industry observers suggest this isn’t coincidental—established giants can more easily absorb compliance costs that would crush smaller competitors.&lt;/p&gt;&lt;p&gt;“It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES,” the EFF notes.&lt;/p&gt;&lt;p&gt;This pattern repeats throughout tech regulation history—what appears to be regulation reigning in Big Tech often ends up cementing their market position by creating barriers too costly for newcomers to overcome.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-no-fakes-act-threatens-anonymous-speech"&gt;NO FAKES Act threatens anonymous speech&lt;/h3&gt;&lt;p&gt;Tucked away in the legislation is another troubling provision that could expose anonymous internet users based on mere allegations. The bill would allow anyone to obtain a subpoena from a court clerk – without judicial review or evidence – forcing services to reveal identifying information about users accused of creating unauthorised replicas.&lt;/p&gt;&lt;p&gt;History shows such mechanisms are ripe for abuse. Critics with valid points can be unmasked and potentially harassed when their commentary includes screenshots or quotes from the very people trying to silence them.&lt;/p&gt;&lt;p&gt;This vulnerability could have a profound effect on legitimate criticism and whistleblowing. Imagine exposing corporate misconduct only to have your identity revealed through a rubber-stamp subpoena process.&lt;/p&gt;&lt;p&gt;This push for additional regulation seems odd given that Congress recently passed the Take It Down Act, which already targets images involving intimate or sexual content. That legislation itself raised privacy concerns, particularly around monitoring encrypted communications.&lt;/p&gt;&lt;p&gt;Rather than assess the impacts of existing legislation, lawmakers seem determined to push forward with broader restrictions that could reshape internet governance for decades to come.&lt;/p&gt;&lt;p&gt;The coming weeks will prove critical as the NO FAKES Act moves through the legislative process. For anyone who values internet freedom, innovation, and balanced approaches to emerging technology challenges, this bears close watching indeed.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Markus Spiske)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;The OpenAI Files: Ex-staff claim profit greed betraying AI safety&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/no-fakes-act-ai-deepfakes-protection-internet-freedom-threat/</guid><pubDate>Wed, 25 Jun 2025 15:47:50 +0000</pubDate></item><item><title>[NEW] Bernie Sanders says that if AI makes us so productive, we should get a 4-day work week (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/bernie-sanders-says-that-if-ai-makes-us-so-productive-we-should-get-a-4-day-work-week/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/04/GettyImages-515797158.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies rave about how their products are revolutionizing productivity, Senator Bernie Sanders (I-VT) wants the tech industry to put its money where its automated mouth is. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent interview with podcaster Joe Rogan, Sanders argued that the time saved with AI tools should be given back to workers to spend with their families. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Technology is gonna work to improve us, not just the people who own the technology and the CEOs of large corporations,” Sanders said. “You are a worker, your productivity is increasing because we give you AI, right? Instead of throwing you out on the street, I’m gonna reduce your work week to 32 hours.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a concept that would be a relief to most people, and an abject horror to anyone who has ever been to Davos. What’s the point of life if you don’t take every moment you can to drive shareholder value? &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the tech elite, the promise of AI-driven increases in productivity means that companies can do even more, since their workers will be freed up to take on even more tasks — or, they can save money by just slashing their headcount. But for workers, this boost in efficiency could mean completing their existing workloads in less time with no loss in pay, so maybe if they’re lucky, they can make it to their kid’s Little League game. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And by the way, not a radical idea,” Sanders said. “There are companies around the world that are doing it with some success.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the United Kingdom, for instance, 61 companies (around 2,900 workers) piloted a four-day work week in the latter half of 2022. Out of 23 companies that shared financial data, the revenue from the beginning to the end of the trial remained about the same, rising by 1.4% on average. &lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Kickstarter has operated on a four-day work week since 2021, while Microsoft Japan piloted a four-day work week in 2019, which led to a reported 40% increase in productivity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Let’s use technology to benefit workers,” Sanders said. “That means give you more time with your family, with your friends, for education, whatever the hell you wanna do.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/04/GettyImages-515797158.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies rave about how their products are revolutionizing productivity, Senator Bernie Sanders (I-VT) wants the tech industry to put its money where its automated mouth is. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent interview with podcaster Joe Rogan, Sanders argued that the time saved with AI tools should be given back to workers to spend with their families. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Technology is gonna work to improve us, not just the people who own the technology and the CEOs of large corporations,” Sanders said. “You are a worker, your productivity is increasing because we give you AI, right? Instead of throwing you out on the street, I’m gonna reduce your work week to 32 hours.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a concept that would be a relief to most people, and an abject horror to anyone who has ever been to Davos. What’s the point of life if you don’t take every moment you can to drive shareholder value? &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the tech elite, the promise of AI-driven increases in productivity means that companies can do even more, since their workers will be freed up to take on even more tasks — or, they can save money by just slashing their headcount. But for workers, this boost in efficiency could mean completing their existing workloads in less time with no loss in pay, so maybe if they’re lucky, they can make it to their kid’s Little League game. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And by the way, not a radical idea,” Sanders said. “There are companies around the world that are doing it with some success.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the United Kingdom, for instance, 61 companies (around 2,900 workers) piloted a four-day work week in the latter half of 2022. Out of 23 companies that shared financial data, the revenue from the beginning to the end of the trial remained about the same, rising by 1.4% on average. &lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Kickstarter has operated on a four-day work week since 2021, while Microsoft Japan piloted a four-day work week in 2019, which led to a reported 40% increase in productivity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Let’s use technology to benefit workers,” Sanders said. “That means give you more time with your family, with your friends, for education, whatever the hell you wanna do.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/bernie-sanders-says-that-if-ai-makes-us-so-productive-we-should-get-a-4-day-work-week/</guid><pubDate>Wed, 25 Jun 2025 16:18:28 +0000</pubDate></item><item><title>[NEW] Getty drops key copyright claims against Stability AI, but UK lawsuit continues (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/getty-drops-key-copyright-claims-against-stability-ai-but-uk-lawsuit-continues/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GenerateScreenshot.png?resize=1200,867" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Getty Images dropped its primary claims of copyright infringement against Stability AI on Wednesday at London’s High Court, narrowing one of the most closely watched legal fights over how AI companies use copyrighted content to train their models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move doesn’t end the case entirely – Getty is still pursuing other claims as well as a separate lawsuit in the U.S. – but it underscores the gray areas surrounding the future of content ownership and usage in the age of generative AI. The development also comes just a day after a U.S. judge sided with Anthropic in a similar dispute over whether training AI on books without author permission violates copyright law.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Getty sued Stability AI — the startup behind AI image generator Stable Diffusion —  in January 2023 after alleging that Stability used millions of copyrighted images to train its AI model without permission.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image database company also claimed that many of the works generated by Stable Diffusion were similar to the copyrighted content used to train it. Some, Getty said, even had its watermarks on them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both of those claims were dropped as of Wednesday morning.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The training claim has likely been dropped due to Getty failing to establish a sufficient connection between the infringing acts and the UK jurisdiction for copyright law to bite,” Ben Maling, a partner at law firm EIP, told TechCrunch in an email. “Meanwhile, the output claim has likely been dropped due to Getty failing to establish that what the models reproduced reflects a substantial part of what was created in the images (e.g. by a photographer).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Getty’s closing arguments, the company’s lawyers said they dropped those claims due to weak evidence and a lack of knowledgeable witnesses from Stability AI. The company framed the move as strategic, allowing both it and the court to focus on what Getty believes are stronger and more winnable allegations.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;What remains in Getty’s lawsuit are a secondary infringement claim as well as claims for trademark infringement&lt;em&gt;. &lt;/em&gt;Regarding the secondary infringement claim, Getty is essentially arguing that the AI models themselves might infringe copyright law, and that using these models in the UK could constitute importing infringing articles, even if the training happened outside the UK.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Secondary infringement is the one with widest relevance to genAI companies training outside of the UK, namely via the models themselves potentially being ‘infringing articles’ that are subsequently imported into the UK,” Maling said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for Stability AI told TechCrunch the startup was “pleased to see Getty’s decision to drop multiple claims after the conclusion of the testimony.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In its closing arguments, Stability noted that it believed Getty’s trademark and passing off claims will fail because consumers don’t interpret the watermarks as a commercial message from Stability AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty’s U.S. division also sued Stability AI in February 2023 for trademark and copyright infringement. In that case, Getty alleged that Stability used as many as 12 million copyrighted images to train its AI model without permission. The company is seeking damages for 11,383 works at $150,000 per infringement, which would amount to a total of $1.7 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Getty spokesperson said the company’s decision to drop copyright infringement claims in the UK does not impact its U.S. case, which is pending a decision on Stability AI’s motion to dismiss. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Stability AI is also named in another complaint alongside Midjourney and DeviantArt after a group of visual artists sued the three companies for copyright infringement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty Images has its own generative AI offering that leverages AI models trained on Getty iStock stock photography and video libraries. The tool allows users to generate new licensable images and artwork.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to include more information from Getty on its U.S. case against Stability AI. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GenerateScreenshot.png?resize=1200,867" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Getty Images dropped its primary claims of copyright infringement against Stability AI on Wednesday at London’s High Court, narrowing one of the most closely watched legal fights over how AI companies use copyrighted content to train their models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move doesn’t end the case entirely – Getty is still pursuing other claims as well as a separate lawsuit in the U.S. – but it underscores the gray areas surrounding the future of content ownership and usage in the age of generative AI. The development also comes just a day after a U.S. judge sided with Anthropic in a similar dispute over whether training AI on books without author permission violates copyright law.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Getty sued Stability AI — the startup behind AI image generator Stable Diffusion —  in January 2023 after alleging that Stability used millions of copyrighted images to train its AI model without permission.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image database company also claimed that many of the works generated by Stable Diffusion were similar to the copyrighted content used to train it. Some, Getty said, even had its watermarks on them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both of those claims were dropped as of Wednesday morning.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The training claim has likely been dropped due to Getty failing to establish a sufficient connection between the infringing acts and the UK jurisdiction for copyright law to bite,” Ben Maling, a partner at law firm EIP, told TechCrunch in an email. “Meanwhile, the output claim has likely been dropped due to Getty failing to establish that what the models reproduced reflects a substantial part of what was created in the images (e.g. by a photographer).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Getty’s closing arguments, the company’s lawyers said they dropped those claims due to weak evidence and a lack of knowledgeable witnesses from Stability AI. The company framed the move as strategic, allowing both it and the court to focus on what Getty believes are stronger and more winnable allegations.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;What remains in Getty’s lawsuit are a secondary infringement claim as well as claims for trademark infringement&lt;em&gt;. &lt;/em&gt;Regarding the secondary infringement claim, Getty is essentially arguing that the AI models themselves might infringe copyright law, and that using these models in the UK could constitute importing infringing articles, even if the training happened outside the UK.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Secondary infringement is the one with widest relevance to genAI companies training outside of the UK, namely via the models themselves potentially being ‘infringing articles’ that are subsequently imported into the UK,” Maling said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for Stability AI told TechCrunch the startup was “pleased to see Getty’s decision to drop multiple claims after the conclusion of the testimony.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In its closing arguments, Stability noted that it believed Getty’s trademark and passing off claims will fail because consumers don’t interpret the watermarks as a commercial message from Stability AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty’s U.S. division also sued Stability AI in February 2023 for trademark and copyright infringement. In that case, Getty alleged that Stability used as many as 12 million copyrighted images to train its AI model without permission. The company is seeking damages for 11,383 works at $150,000 per infringement, which would amount to a total of $1.7 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Getty spokesperson said the company’s decision to drop copyright infringement claims in the UK does not impact its U.S. case, which is pending a decision on Stability AI’s motion to dismiss. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Stability AI is also named in another complaint alongside Midjourney and DeviantArt after a group of visual artists sued the three companies for copyright infringement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty Images has its own generative AI offering that leverages AI models trained on Getty iStock stock photography and video libraries. The tool allows users to generate new licensable images and artwork.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to include more information from Getty on its U.S. case against Stability AI. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/getty-drops-key-copyright-claims-against-stability-ai-but-uk-lawsuit-continues/</guid><pubDate>Wed, 25 Jun 2025 16:34:35 +0000</pubDate></item><item><title>[NEW] Anthropic just made every Claude user a no-code app developer (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/anthropic-just-made-every-claude-user-a-no-code-app-developer/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic announced Wednesday that it will transform its Claude AI assistant into a platform for creating interactive, shareable applications, marking a significant evolution from conversational chatbots toward functional software tools that users can build and distribute without coding knowledge.&lt;/p&gt;



&lt;p&gt;The San Francisco-based AI company revealed that millions of users have already created more than 500 million “artifacts” — interactive content ranging from educational games to data analysis tools — since the feature’s initial launch. Now, Anthropic is embedding Claude’s intelligence directly into these creations, enabling them to process user input and adapt content in real-time independently of ongoing conversations.&lt;/p&gt;



&lt;p&gt;The development represents a fundamental shift in how artificial intelligence interfaces with users, moving beyond static responses toward dynamic, interactive experiences that blur the lines between AI assistance and software development. The move intensifies competition with OpenAI’s Canvas feature, which launched in October with similar split-screen functionality for editing AI-generated content, though it lacks the same emphasis on shareable applications that defines Anthropic’s approach.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-claude-s-artifacts-eliminate-the-copy-paste-problem-plaguing-ai-workflows"&gt;&lt;strong&gt;How Claude’s artifacts eliminate the copy-paste problem plaguing AI workflows&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Traditional AI interactions follow a question-and-answer format, with users copying and pasting results into separate applications for practical use. Anthropic’s enhanced artifacts eliminate this friction by creating a dedicated workspace where AI-generated content becomes immediately functional and shareable.&lt;/p&gt;



&lt;p&gt;“Think bigger than ‘make me flashcards for Spanish,'” the company explains in its announcement blog post. “Try ‘build me a flashcard app.’ One request gets you static study materials. The other creates a shareable tool that generates cards for any topic.”&lt;/p&gt;



&lt;p&gt;The distinction highlights Anthropic’s strategic positioning against competitors. While OpenAI’s GPT Store focuses on conversational agents, Anthropic emphasizes functional applications with user interfaces.&lt;/p&gt;



&lt;p&gt;Early adopters are creating games with non-player characters that remember choices and adapt storylines, smart tutors that adjust explanations based on user understanding, and data analyzers that answer plain-English questions about uploaded spreadsheets.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-offering-free-ai-app-creation-makes-business-sense-for-anthropic"&gt;&lt;strong&gt;Why offering free AI app creation makes business sense for Anthropic&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The platform operates on Claude’s existing infrastructure, with users authenticating through their Claude accounts to access shared applications. This approach distributes computational load across subscription tiers rather than creating infrastructure strain from popular applications.&lt;/p&gt;



&lt;p&gt;Free users can create, view, and interact with artifacts, while Pro ($20/month) and Team ($25-30/month) subscribers gain additional capabilities and higher usage limits. The company views free access as a customer acquisition strategy, with Anthropic representatives noting that “free users experiencing the magic of creating with Claude become our best advocates.”&lt;/p&gt;



&lt;p&gt;The business model reflects broader industry trends toward freemium AI services, where basic functionality attracts users who eventually upgrade for enhanced features. Unlike traditional software marketplaces where creators might monetize applications, Anthropic’s platform emphasizes free sharing to build community engagement.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-content-moderation-becomes-critical-as-users-generate-millions-of-ai-apps"&gt;&lt;strong&gt;Content moderation becomes critical as users generate millions of AI apps&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The proliferation of user-generated AI applications raises content moderation concerns that Anthropic addresses through multiple layers of protection. The company implements built-in safeguards during content creation, manually curates featured galleries, and requires all shared artifacts to comply with content policies.&lt;/p&gt;



&lt;p&gt;For its broader AI safety approach, Anthropic says it will “implement a multi-layered approach to prevent misuse, including real-time and asynchronous monitoring, rapid response protocols, and thorough pre-deployment red teaming,” according to the company’s updated Responsible Scaling Policy. These enterprise-grade safety measures extend to the artifacts platform, where user-generated content undergoes similar scrutiny.&lt;/p&gt;



&lt;p&gt;Users can report problematic content for team review, though the company has not disclosed specific metrics about moderation volume or effectiveness. The approach mirrors content moderation strategies employed by major social media platforms, adapted for AI-generated applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-openai-s-canvas-feature-signals-intensifying-battle-for-ai-interface-supremacy"&gt;&lt;strong&gt;OpenAI’s Canvas feature signals intensifying battle for AI interface supremacy&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Anthropic’s announcement comes as artificial intelligence companies increasingly compete on user experience rather than raw model capabilities. OpenAI’s Canvas feature, launched in October, provides similar split-screen functionality for editing AI-generated content, though without the same emphasis on shareable applications.&lt;/p&gt;



&lt;p&gt;The competition reflects broader industry recognition that conversational interfaces, while groundbreaking, may not represent the ultimate form of AI interaction. Companies are exploring visual interfaces, interactive experiences, and embedded intelligence as potential successors to traditional chatbots.&lt;/p&gt;



&lt;p&gt;Music producer Rick Rubin’s documented use of Claude artifacts in “The Way of Code” demonstrates the technology’s appeal beyond technical users, suggesting potential for mainstream adoption across creative industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-software-developers-debate-whether-ai-app-builders-threaten-traditional-coding-jobs"&gt;&lt;strong&gt;Software developers debate whether AI app builders threaten traditional coding jobs&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The democratization of application creation through AI tools raises fundamental questions about the future of traditional software development, with industry data revealing a dramatic shift already underway. Gartner research shows that 70% of new applications will use low-code or no-code technologies by 2025, representing a massive jump from just 25% in 2020.&lt;/p&gt;



&lt;p&gt;This transformation is creating what analysts call “citizen developers” — business users who create applications without formal programming training. Already, 41% of businesses have active citizen development initiatives, and nearly 60% of custom applications are built outside traditional IT departments.&lt;/p&gt;



&lt;p&gt;Companies using these platforms report avoiding the need to hire an average of two IT developers, generating approximately $4.4 million in increased business value over three years, according to Forrester research.&lt;/p&gt;



&lt;p&gt;However, the relationship between AI-powered development tools and traditional coding appears more complementary than competitive. Anthropic positions artifacts as enabling rapid prototyping and personal tool creation while professional developers continue building production-grade applications. The platforms excel at business process automation and simple applications but struggle with complex, mission-critical systems that require custom functionality and enterprise-scale performance.&lt;/p&gt;



&lt;p&gt;Security and governance concerns also maintain demand for professional developers. With applications increasingly built outside IT departments, organizations require skilled developers to establish proper governance frameworks and ensure applications meet enterprise security standards. The most successful developers are adapting to work alongside these tools rather than competing against them, focusing on system architecture, performance optimization, and integration challenges that AI tools cannot yet address.&lt;/p&gt;



&lt;p&gt;The market dynamics suggest coexistence rather than replacement, with the global low-code development platform market projected to reach $187 billion by 2030 while traditional software development continues growing simultaneously.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-platform-wars-heat-up-as-ai-companies-seek-sustainable-revenue-beyond-api-calls"&gt;&lt;strong&gt;The platform wars heat up as AI companies seek sustainable revenue beyond API calls&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Anthropic’s move signals a broader industry evolution as AI companies mature beyond initial chatbot implementations. Rather than competing solely on model performance or API pricing, companies are building ecosystem features that create network effects and user lock-in.&lt;/p&gt;



&lt;p&gt;The enhanced artifacts feature becomes available Wednesday across Anthropic’s free and paid tiers, accessible through web browsers and mobile applications. Users can access the new functionality through a dedicated sidebar in the Claude interface, with full features available on desktop and basic functionality on mobile devices.&lt;/p&gt;



&lt;p&gt;As millions of users begin building and sharing AI-powered applications without writing a single line of code, the technology industry faces a fundamental question: Will the future belong to those who can prompt AI most effectively, or those who understand the underlying systems well enough to build them? Anthropic is betting that in the age of artificial intelligence, the most powerful code might just be a well-crafted conversation.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic announced Wednesday that it will transform its Claude AI assistant into a platform for creating interactive, shareable applications, marking a significant evolution from conversational chatbots toward functional software tools that users can build and distribute without coding knowledge.&lt;/p&gt;



&lt;p&gt;The San Francisco-based AI company revealed that millions of users have already created more than 500 million “artifacts” — interactive content ranging from educational games to data analysis tools — since the feature’s initial launch. Now, Anthropic is embedding Claude’s intelligence directly into these creations, enabling them to process user input and adapt content in real-time independently of ongoing conversations.&lt;/p&gt;



&lt;p&gt;The development represents a fundamental shift in how artificial intelligence interfaces with users, moving beyond static responses toward dynamic, interactive experiences that blur the lines between AI assistance and software development. The move intensifies competition with OpenAI’s Canvas feature, which launched in October with similar split-screen functionality for editing AI-generated content, though it lacks the same emphasis on shareable applications that defines Anthropic’s approach.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-claude-s-artifacts-eliminate-the-copy-paste-problem-plaguing-ai-workflows"&gt;&lt;strong&gt;How Claude’s artifacts eliminate the copy-paste problem plaguing AI workflows&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Traditional AI interactions follow a question-and-answer format, with users copying and pasting results into separate applications for practical use. Anthropic’s enhanced artifacts eliminate this friction by creating a dedicated workspace where AI-generated content becomes immediately functional and shareable.&lt;/p&gt;



&lt;p&gt;“Think bigger than ‘make me flashcards for Spanish,'” the company explains in its announcement blog post. “Try ‘build me a flashcard app.’ One request gets you static study materials. The other creates a shareable tool that generates cards for any topic.”&lt;/p&gt;



&lt;p&gt;The distinction highlights Anthropic’s strategic positioning against competitors. While OpenAI’s GPT Store focuses on conversational agents, Anthropic emphasizes functional applications with user interfaces.&lt;/p&gt;



&lt;p&gt;Early adopters are creating games with non-player characters that remember choices and adapt storylines, smart tutors that adjust explanations based on user understanding, and data analyzers that answer plain-English questions about uploaded spreadsheets.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-offering-free-ai-app-creation-makes-business-sense-for-anthropic"&gt;&lt;strong&gt;Why offering free AI app creation makes business sense for Anthropic&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The platform operates on Claude’s existing infrastructure, with users authenticating through their Claude accounts to access shared applications. This approach distributes computational load across subscription tiers rather than creating infrastructure strain from popular applications.&lt;/p&gt;



&lt;p&gt;Free users can create, view, and interact with artifacts, while Pro ($20/month) and Team ($25-30/month) subscribers gain additional capabilities and higher usage limits. The company views free access as a customer acquisition strategy, with Anthropic representatives noting that “free users experiencing the magic of creating with Claude become our best advocates.”&lt;/p&gt;



&lt;p&gt;The business model reflects broader industry trends toward freemium AI services, where basic functionality attracts users who eventually upgrade for enhanced features. Unlike traditional software marketplaces where creators might monetize applications, Anthropic’s platform emphasizes free sharing to build community engagement.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-content-moderation-becomes-critical-as-users-generate-millions-of-ai-apps"&gt;&lt;strong&gt;Content moderation becomes critical as users generate millions of AI apps&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The proliferation of user-generated AI applications raises content moderation concerns that Anthropic addresses through multiple layers of protection. The company implements built-in safeguards during content creation, manually curates featured galleries, and requires all shared artifacts to comply with content policies.&lt;/p&gt;



&lt;p&gt;For its broader AI safety approach, Anthropic says it will “implement a multi-layered approach to prevent misuse, including real-time and asynchronous monitoring, rapid response protocols, and thorough pre-deployment red teaming,” according to the company’s updated Responsible Scaling Policy. These enterprise-grade safety measures extend to the artifacts platform, where user-generated content undergoes similar scrutiny.&lt;/p&gt;



&lt;p&gt;Users can report problematic content for team review, though the company has not disclosed specific metrics about moderation volume or effectiveness. The approach mirrors content moderation strategies employed by major social media platforms, adapted for AI-generated applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-openai-s-canvas-feature-signals-intensifying-battle-for-ai-interface-supremacy"&gt;&lt;strong&gt;OpenAI’s Canvas feature signals intensifying battle for AI interface supremacy&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Anthropic’s announcement comes as artificial intelligence companies increasingly compete on user experience rather than raw model capabilities. OpenAI’s Canvas feature, launched in October, provides similar split-screen functionality for editing AI-generated content, though without the same emphasis on shareable applications.&lt;/p&gt;



&lt;p&gt;The competition reflects broader industry recognition that conversational interfaces, while groundbreaking, may not represent the ultimate form of AI interaction. Companies are exploring visual interfaces, interactive experiences, and embedded intelligence as potential successors to traditional chatbots.&lt;/p&gt;



&lt;p&gt;Music producer Rick Rubin’s documented use of Claude artifacts in “The Way of Code” demonstrates the technology’s appeal beyond technical users, suggesting potential for mainstream adoption across creative industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-software-developers-debate-whether-ai-app-builders-threaten-traditional-coding-jobs"&gt;&lt;strong&gt;Software developers debate whether AI app builders threaten traditional coding jobs&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;The democratization of application creation through AI tools raises fundamental questions about the future of traditional software development, with industry data revealing a dramatic shift already underway. Gartner research shows that 70% of new applications will use low-code or no-code technologies by 2025, representing a massive jump from just 25% in 2020.&lt;/p&gt;



&lt;p&gt;This transformation is creating what analysts call “citizen developers” — business users who create applications without formal programming training. Already, 41% of businesses have active citizen development initiatives, and nearly 60% of custom applications are built outside traditional IT departments.&lt;/p&gt;



&lt;p&gt;Companies using these platforms report avoiding the need to hire an average of two IT developers, generating approximately $4.4 million in increased business value over three years, according to Forrester research.&lt;/p&gt;



&lt;p&gt;However, the relationship between AI-powered development tools and traditional coding appears more complementary than competitive. Anthropic positions artifacts as enabling rapid prototyping and personal tool creation while professional developers continue building production-grade applications. The platforms excel at business process automation and simple applications but struggle with complex, mission-critical systems that require custom functionality and enterprise-scale performance.&lt;/p&gt;



&lt;p&gt;Security and governance concerns also maintain demand for professional developers. With applications increasingly built outside IT departments, organizations require skilled developers to establish proper governance frameworks and ensure applications meet enterprise security standards. The most successful developers are adapting to work alongside these tools rather than competing against them, focusing on system architecture, performance optimization, and integration challenges that AI tools cannot yet address.&lt;/p&gt;



&lt;p&gt;The market dynamics suggest coexistence rather than replacement, with the global low-code development platform market projected to reach $187 billion by 2030 while traditional software development continues growing simultaneously.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-platform-wars-heat-up-as-ai-companies-seek-sustainable-revenue-beyond-api-calls"&gt;&lt;strong&gt;The platform wars heat up as AI companies seek sustainable revenue beyond API calls&lt;/strong&gt;&lt;/h2&gt;



&lt;p&gt;Anthropic’s move signals a broader industry evolution as AI companies mature beyond initial chatbot implementations. Rather than competing solely on model performance or API pricing, companies are building ecosystem features that create network effects and user lock-in.&lt;/p&gt;



&lt;p&gt;The enhanced artifacts feature becomes available Wednesday across Anthropic’s free and paid tiers, accessible through web browsers and mobile applications. Users can access the new functionality through a dedicated sidebar in the Claude interface, with full features available on desktop and basic functionality on mobile devices.&lt;/p&gt;



&lt;p&gt;As millions of users begin building and sharing AI-powered applications without writing a single line of code, the technology industry faces a fundamental question: Will the future belong to those who can prompt AI most effectively, or those who understand the underlying systems well enough to build them? Anthropic is betting that in the age of artificial intelligence, the most powerful code might just be a well-crafted conversation.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropic-just-made-every-claude-user-a-no-code-app-developer/</guid><pubDate>Wed, 25 Jun 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] Rubrik acquires Predibase to accelerate adoption of AI agents (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/rubrik-acquires-predibase-to-accelerate-adoption-of-ai-agents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2197498845.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Data cybersecurity company Rubrik announced Wednesday its intent to acquire Predibase. Predibase is a venture-backed startup that helps companies train and fine-tune open source AI models to customize them to their needs.&amp;nbsp;Rubrik is the latest company to make an acquisition with the goal of boosting enterprise AI agent adoption.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Terms of the deal were not disclosed, though CNBC reported that the deal was between $100 million and $500 million, a sizable range. Rubrik declined to comment on the deal size. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Predibase was founded in 2021 by Devvret Rishi, current CEO, Piero Molino, current chief science officer, and Travis Addair, current CTO. The company has raised more than $28 million in VC money from investors including Felicis, Greylock and Sancus Ventures, among others.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The integration of Predibase will allow Rubrik users to accelerate building AI agents through Amazon Bedrock, Azure OpenAI, and Google Agentspace, according to a press release.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When [Predibase’s] capabilities are paired with secure data platforms like Rubrik’s, they can be transformative, helping ensure trusted data powers responsible and impactful AI,” Bipul Sinha, co-founder and CEO of Rubrik, wrote in a blog post about the deal. “Just as Rubrik has simplified access to secured, governed data for AI, Predibase is solving the performance and cost issues around deploying large language models for agentic and other AI applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rubrik is just one in a growing list of companies making acquisitions to bolster their technology stack for the creation of AI agents.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Salesforce acquired data management firm Informatica for $8 billion in May. Snowflake acquired Crunchy Data to bolster its AI agent offerings in early June, and Collibra acquired Raito for similar reasons a few days later.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Rubrik was founded in 2014 and raised more than $1.6 billion in venture capital, from firms including Khosla Ventures, IVP, and Lightspeed Venture Partners, among others, before going public in April 2024.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2197498845.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Data cybersecurity company Rubrik announced Wednesday its intent to acquire Predibase. Predibase is a venture-backed startup that helps companies train and fine-tune open source AI models to customize them to their needs.&amp;nbsp;Rubrik is the latest company to make an acquisition with the goal of boosting enterprise AI agent adoption.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Terms of the deal were not disclosed, though CNBC reported that the deal was between $100 million and $500 million, a sizable range. Rubrik declined to comment on the deal size. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Predibase was founded in 2021 by Devvret Rishi, current CEO, Piero Molino, current chief science officer, and Travis Addair, current CTO. The company has raised more than $28 million in VC money from investors including Felicis, Greylock and Sancus Ventures, among others.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The integration of Predibase will allow Rubrik users to accelerate building AI agents through Amazon Bedrock, Azure OpenAI, and Google Agentspace, according to a press release.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When [Predibase’s] capabilities are paired with secure data platforms like Rubrik’s, they can be transformative, helping ensure trusted data powers responsible and impactful AI,” Bipul Sinha, co-founder and CEO of Rubrik, wrote in a blog post about the deal. “Just as Rubrik has simplified access to secured, governed data for AI, Predibase is solving the performance and cost issues around deploying large language models for agentic and other AI applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rubrik is just one in a growing list of companies making acquisitions to bolster their technology stack for the creation of AI agents.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Salesforce acquired data management firm Informatica for $8 billion in May. Snowflake acquired Crunchy Data to bolster its AI agent offerings in early June, and Collibra acquired Raito for similar reasons a few days later.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Rubrik was founded in 2014 and raised more than $1.6 billion in venture capital, from firms including Khosla Ventures, IVP, and Lightspeed Venture Partners, among others, before going public in April 2024.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/rubrik-acquires-predibase-to-accelerate-adoption-of-ai-agents/</guid><pubDate>Wed, 25 Jun 2025 17:34:25 +0000</pubDate></item><item><title>[NEW] The new AI infrastructure reality: Bring compute to data, not data to compute (AI News | VentureBeat)</title><link>https://venturebeat.com/data-infrastructure/the-new-ai-infrastructure-reality-bring-compute-to-data-not-data-to-compute/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;As AI transforms enterprise operations across diverse industries, critical challenges continue to surface around data storage—no matter how advanced the model, its performance hinges on the ability to access vast amounts of data quickly, securely, and reliably. Without the right data storage infrastructure, even the most powerful AI systems can be brought to a crawl by slow, fragmented, or inefficient data pipelines.&lt;/p&gt;



&lt;p&gt;This topic took center stage on Day One of VB Transform, in a session focused on medical imaging AI innovations spearheaded by PEAK:AIO and Solidigm. Together, alongside the Medical Open Network for AI (MONAI) project—an open-source framework for developing and deploying medical imaging AI—they are redefining how data infrastructure supports real-time inference and training in hospitals, from enhancing diagnostics to powering advanced research and operational use cases.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-innovating-storage-at-the-edge-of-clinical-ai"&gt;Innovating storage at the edge of clinical AI&lt;/h2&gt;



&lt;p&gt;Moderated by Michael Stewart, managing partner at M12 (Microsoft’s venture fund), the session featured insights from Roger Cummings, CEO of PEAK:AIO, and Greg Matson, head of products and marketing at Solidigm. The conversation explored how next-generation, high-capacity storage architectures are opening new doors for medical AI by delivering the speed, security and scalability needed to handle massive datasets in clinical environments.&lt;/p&gt;



&lt;p&gt;Crucially, both companies have been deeply involved with MONAI since its early days. Developed in collaboration with King’s College London and others, MONAI is purpose-built to develop and deploy AI models in medical imaging. The open-source framework’s toolset—tailored to the unique demands of healthcare—includes libraries and tools for DICOM support, 3D image processing, and model pre-training, enabling researchers and clinicians to build high-performance models for tasks like tumor segmentation and organ classification.&lt;/p&gt;



&lt;p&gt;A crucial design goal of MONAI was to support on-premises deployment, allowing hospitals to maintain full control over sensitive patient data while leveraging standard GPU servers for training and inference. This ties the framework’s performance closely to the data infrastructure beneath it, requiring fast, scalable storage systems to fully support the demands of real-time clinical AI. This is where Solidigm and PEAK:AIO come into play: Solidigm brings high-density flash storage to the table, while PEAK:AIO specializes in storage systems purpose-built for AI workloads.&lt;/p&gt;



&lt;p&gt;“We were very fortunate to be working early on with King’s College in London and Professor Sebastien Orslund to develop MONAI,” Cummings explained. “Working with Orslund, we developed the underlying infrastructure that allows researchers, doctors, and biologists in the life sciences to build on top of this framework very quickly.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-meeting-dual-storage-demands-in-healthcare-ai"&gt;Meeting dual storage demands in healthcare AI&lt;/h2&gt;



&lt;p&gt;Matson pointed out that he’s seeing a clear bifurcation in storage hardware, with different solutions optimized for specific stages of the AI data pipeline. For use cases like MONAI, similar edge AI deployments—as well as scenarios involving the feeding of training clusters—ultra-high-capacity solid-state storage plays a critical role, as these environments are often space and power-constrained, yet require local access to massive datasets.&lt;/p&gt;



&lt;p&gt;For instance, MONAI was able to store more than two million full-body CT scans on a single node within a hospital’s existing IT infrastructure. “Very space-constrained, power-constrained, and very high-capacity storage enabled some fairly remarkable results,” Matson said. This kind of efficiency is a game-changer for edge AI in healthcare, allowing institutions to run advanced AI models on-premises without compromising performance, scalability, or data security.&lt;/p&gt;



&lt;p&gt;In contrast, workloads involving real-time inference and active model training place very different demands on the system. These tasks require storage solutions that can deliver exceptionally high input/output operations per second (IOPS) to keep up with the data throughput needed by high-bandwidth memory (HBM) and ensure GPUs remain fully utilized. PEAK:AIO’s software-defined storage layer, combined with Solidigm’s high-performance solid-state drives (SSDs), addresses both ends of this spectrum—delivering the capacity, efficiency, and speed required across the entire AI pipeline.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-software-defined-layer-for-clinical-ai-workloads-at-the-edge"&gt;A software-defined layer for clinical AI workloads at the edge&lt;/h2&gt;



&lt;p&gt;Cummings explained that PEAK:AIO’s software-defined AI storage technology, when paired with Solidigm’s high-performance SSDs, enables MONAI to read, write, and archive massive datasets at the speed clinical AI demands. This combination accelerates model training and enhances accuracy in medical imaging while operating within an open-source framework tailored to healthcare environments.&lt;/p&gt;



&lt;p&gt;“We provide a software-defined layer that can be deployed on any commodity server, transforming it into a high-performance system for AI or HPC workloads,” Cummings said. “In edge environments, we take that same capability and scale it down to a single node, bringing inference closer to where the data lives.”&lt;/p&gt;



&lt;p&gt;A key capability is how PEAK:AIO helps eliminate traditional memory bottlenecks by integrating memory more directly into the AI infrastructure. “We treat memory as part of the infrastructure itself—something that’s often overlooked. Our solution scales not just storage, but also the memory workspace and the metadata associated with it,” Cummings said. This makes a significant difference for customers who can’t afford—either in terms of space or cost—to re-run large models repeatedly. By keeping memory-resident tokens alive and accessible, PEAK:AIO enables efficient, localized inference without needing constant recomputation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-bringing-intelligence-closer-to-the-data"&gt;Bringing intelligence closer to the data&lt;/h2&gt;



&lt;p&gt;Cummings emphasized that enterprises will need to take a more strategic approach to managing AI workloads. “You can’t be just a destination. You have to understand the workloads. We do some incredible technology with Solidign and their infrastructure to be smarter on how that data is processed, starting with how to get performance out of a single node,” Cummings explained. “So with inference being such a large push, we’re seeing generalists becoming more specialized. And we’re now taking work that we’ve done from a single node and pushing it closer to the data to be more efficient. We want more intelligent data, right? The only way to do that is to get closer to that data.”&lt;/p&gt;&lt;p&gt;Some clear trends are emerging from large-scale AI deployments, particularly in newly built greenfield data centers. These facilities are designed with highly specialized hardware architectures that bring data as close as possible to the GPUs. To achieve this, they rely heavily on all solid-state storage—specifically ultra-high-capacity SSDs—designed to deliver petabyte-scale storage with the speed and accessibility needed to keep GPUs continuously fed with data at high throughput.&lt;/p&gt;&lt;p&gt;“Now that same technology is basically happening at a microcosm, at the edge, in the enterprise,” Cumming explained. “So it’s becoming critical to purchasers of AI systems to determine how you select your hardware and system vendor, even to make sure that if you want to get the most performance out of your system, that you’re running on all solid-state. This allows you to bring huge amounts of data, like the MONAI example—it was 15,000,000 plus images, in a single system. This enables incredible processing power, right there in a small system at the end.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;As AI transforms enterprise operations across diverse industries, critical challenges continue to surface around data storage—no matter how advanced the model, its performance hinges on the ability to access vast amounts of data quickly, securely, and reliably. Without the right data storage infrastructure, even the most powerful AI systems can be brought to a crawl by slow, fragmented, or inefficient data pipelines.&lt;/p&gt;



&lt;p&gt;This topic took center stage on Day One of VB Transform, in a session focused on medical imaging AI innovations spearheaded by PEAK:AIO and Solidigm. Together, alongside the Medical Open Network for AI (MONAI) project—an open-source framework for developing and deploying medical imaging AI—they are redefining how data infrastructure supports real-time inference and training in hospitals, from enhancing diagnostics to powering advanced research and operational use cases.&lt;/p&gt;


&lt;strong&gt;&amp;gt;&amp;gt;See all our Transform 2025 coverage here&amp;lt;&amp;lt;&lt;/strong&gt;


&lt;h2 class="wp-block-heading" id="h-innovating-storage-at-the-edge-of-clinical-ai"&gt;Innovating storage at the edge of clinical AI&lt;/h2&gt;



&lt;p&gt;Moderated by Michael Stewart, managing partner at M12 (Microsoft’s venture fund), the session featured insights from Roger Cummings, CEO of PEAK:AIO, and Greg Matson, head of products and marketing at Solidigm. The conversation explored how next-generation, high-capacity storage architectures are opening new doors for medical AI by delivering the speed, security and scalability needed to handle massive datasets in clinical environments.&lt;/p&gt;



&lt;p&gt;Crucially, both companies have been deeply involved with MONAI since its early days. Developed in collaboration with King’s College London and others, MONAI is purpose-built to develop and deploy AI models in medical imaging. The open-source framework’s toolset—tailored to the unique demands of healthcare—includes libraries and tools for DICOM support, 3D image processing, and model pre-training, enabling researchers and clinicians to build high-performance models for tasks like tumor segmentation and organ classification.&lt;/p&gt;



&lt;p&gt;A crucial design goal of MONAI was to support on-premises deployment, allowing hospitals to maintain full control over sensitive patient data while leveraging standard GPU servers for training and inference. This ties the framework’s performance closely to the data infrastructure beneath it, requiring fast, scalable storage systems to fully support the demands of real-time clinical AI. This is where Solidigm and PEAK:AIO come into play: Solidigm brings high-density flash storage to the table, while PEAK:AIO specializes in storage systems purpose-built for AI workloads.&lt;/p&gt;



&lt;p&gt;“We were very fortunate to be working early on with King’s College in London and Professor Sebastien Orslund to develop MONAI,” Cummings explained. “Working with Orslund, we developed the underlying infrastructure that allows researchers, doctors, and biologists in the life sciences to build on top of this framework very quickly.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-meeting-dual-storage-demands-in-healthcare-ai"&gt;Meeting dual storage demands in healthcare AI&lt;/h2&gt;



&lt;p&gt;Matson pointed out that he’s seeing a clear bifurcation in storage hardware, with different solutions optimized for specific stages of the AI data pipeline. For use cases like MONAI, similar edge AI deployments—as well as scenarios involving the feeding of training clusters—ultra-high-capacity solid-state storage plays a critical role, as these environments are often space and power-constrained, yet require local access to massive datasets.&lt;/p&gt;



&lt;p&gt;For instance, MONAI was able to store more than two million full-body CT scans on a single node within a hospital’s existing IT infrastructure. “Very space-constrained, power-constrained, and very high-capacity storage enabled some fairly remarkable results,” Matson said. This kind of efficiency is a game-changer for edge AI in healthcare, allowing institutions to run advanced AI models on-premises without compromising performance, scalability, or data security.&lt;/p&gt;



&lt;p&gt;In contrast, workloads involving real-time inference and active model training place very different demands on the system. These tasks require storage solutions that can deliver exceptionally high input/output operations per second (IOPS) to keep up with the data throughput needed by high-bandwidth memory (HBM) and ensure GPUs remain fully utilized. PEAK:AIO’s software-defined storage layer, combined with Solidigm’s high-performance solid-state drives (SSDs), addresses both ends of this spectrum—delivering the capacity, efficiency, and speed required across the entire AI pipeline.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-software-defined-layer-for-clinical-ai-workloads-at-the-edge"&gt;A software-defined layer for clinical AI workloads at the edge&lt;/h2&gt;



&lt;p&gt;Cummings explained that PEAK:AIO’s software-defined AI storage technology, when paired with Solidigm’s high-performance SSDs, enables MONAI to read, write, and archive massive datasets at the speed clinical AI demands. This combination accelerates model training and enhances accuracy in medical imaging while operating within an open-source framework tailored to healthcare environments.&lt;/p&gt;



&lt;p&gt;“We provide a software-defined layer that can be deployed on any commodity server, transforming it into a high-performance system for AI or HPC workloads,” Cummings said. “In edge environments, we take that same capability and scale it down to a single node, bringing inference closer to where the data lives.”&lt;/p&gt;



&lt;p&gt;A key capability is how PEAK:AIO helps eliminate traditional memory bottlenecks by integrating memory more directly into the AI infrastructure. “We treat memory as part of the infrastructure itself—something that’s often overlooked. Our solution scales not just storage, but also the memory workspace and the metadata associated with it,” Cummings said. This makes a significant difference for customers who can’t afford—either in terms of space or cost—to re-run large models repeatedly. By keeping memory-resident tokens alive and accessible, PEAK:AIO enables efficient, localized inference without needing constant recomputation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-bringing-intelligence-closer-to-the-data"&gt;Bringing intelligence closer to the data&lt;/h2&gt;



&lt;p&gt;Cummings emphasized that enterprises will need to take a more strategic approach to managing AI workloads. “You can’t be just a destination. You have to understand the workloads. We do some incredible technology with Solidign and their infrastructure to be smarter on how that data is processed, starting with how to get performance out of a single node,” Cummings explained. “So with inference being such a large push, we’re seeing generalists becoming more specialized. And we’re now taking work that we’ve done from a single node and pushing it closer to the data to be more efficient. We want more intelligent data, right? The only way to do that is to get closer to that data.”&lt;/p&gt;&lt;p&gt;Some clear trends are emerging from large-scale AI deployments, particularly in newly built greenfield data centers. These facilities are designed with highly specialized hardware architectures that bring data as close as possible to the GPUs. To achieve this, they rely heavily on all solid-state storage—specifically ultra-high-capacity SSDs—designed to deliver petabyte-scale storage with the speed and accessibility needed to keep GPUs continuously fed with data at high throughput.&lt;/p&gt;&lt;p&gt;“Now that same technology is basically happening at a microcosm, at the edge, in the enterprise,” Cumming explained. “So it’s becoming critical to purchasers of AI systems to determine how you select your hardware and system vendor, even to make sure that if you want to get the most performance out of your system, that you’re running on all solid-state. This allows you to bring huge amounts of data, like the MONAI example—it was 15,000,000 plus images, in a single system. This enables incredible processing power, right there in a small system at the end.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/data-infrastructure/the-new-ai-infrastructure-reality-bring-compute-to-data-not-data-to-compute/</guid><pubDate>Wed, 25 Jun 2025 17:57:52 +0000</pubDate></item><item><title>[NEW] Creative Commons debuts CC signals, a framework for an open AI ecosystem (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/creative-commons-debuts-cc-signals-a-framework-for-an-open-ai-ecosystem/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/CC-signals-2025-—-Social-Assets-2.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nonprofit Creative Commons, which spearheaded the licensing movement that allows creators to share their works while retaining copyright, is now preparing for the AI era. On Wednesday, the organization announced the launch of a new project, CC signals, which will allow dataset holders to detail how their content can or cannot be reused by machines, as in the case of training AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is meant to create a balance between the open nature of the internet and the demand for ever more data to fuel AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Creative Commons explains in a blog post, the continued data extraction underway could erode openness on the internet and could see entities walling off their sites or guarding them with paywalls, instead of sharing access to their data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project, on the other hand, aims to provide a legal and technical solution that would provide a framework for dataset sharing meant to be used between those who control the data and those who use it to train AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Demand is increasing for such a tool, as companies grapple with changing their policies and terms of service to either limit AI training on their data or explain to what extent they’ll use users’ data for purposes related to AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, X initially made a change that allowed third parties to train their models on its public data, then later reversed that. Reddit is using its robots.txt file, which is meant to tell automated web crawlers whether they can access its site, to restrict bots from scraping its data for training AI. Cloudflare is looking toward a solution that would charge AI bots for scraping, as well as tools for confusing them. And open source developers have also built tools to slow down and waste the resources of AI crawlers that didn’t respect their “no crawl” directives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project instead proposes a different solution: a set of tools that offers a range of legal enforceability, but all of which have an ethical weight to them, similar to the CC licenses that today cover billions of openly licensed creative works online.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“CC signals are designed to sustain the commons in the age of AI,” said Anna Tumadóttir, Creative Commons CEO, in an announcement. “Just as the CC licenses helped build the open web, we believe CC signals will help shape an open AI ecosystem grounded in reciprocity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project is only now beginning to take shape. Early designs have been published on the CC website and GitHub page. The organization is actively seeking public feedback ahead of its plans for an alpha launch (early test) in November 2025. It will also host a series of town halls for feedback and questions. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/CC-signals-2025-—-Social-Assets-2.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nonprofit Creative Commons, which spearheaded the licensing movement that allows creators to share their works while retaining copyright, is now preparing for the AI era. On Wednesday, the organization announced the launch of a new project, CC signals, which will allow dataset holders to detail how their content can or cannot be reused by machines, as in the case of training AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is meant to create a balance between the open nature of the internet and the demand for ever more data to fuel AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Creative Commons explains in a blog post, the continued data extraction underway could erode openness on the internet and could see entities walling off their sites or guarding them with paywalls, instead of sharing access to their data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project, on the other hand, aims to provide a legal and technical solution that would provide a framework for dataset sharing meant to be used between those who control the data and those who use it to train AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Demand is increasing for such a tool, as companies grapple with changing their policies and terms of service to either limit AI training on their data or explain to what extent they’ll use users’ data for purposes related to AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, X initially made a change that allowed third parties to train their models on its public data, then later reversed that. Reddit is using its robots.txt file, which is meant to tell automated web crawlers whether they can access its site, to restrict bots from scraping its data for training AI. Cloudflare is looking toward a solution that would charge AI bots for scraping, as well as tools for confusing them. And open source developers have also built tools to slow down and waste the resources of AI crawlers that didn’t respect their “no crawl” directives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project instead proposes a different solution: a set of tools that offers a range of legal enforceability, but all of which have an ethical weight to them, similar to the CC licenses that today cover billions of openly licensed creative works online.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“CC signals are designed to sustain the commons in the age of AI,” said Anna Tumadóttir, Creative Commons CEO, in an announcement. “Just as the CC licenses helped build the open web, we believe CC signals will help shape an open AI ecosystem grounded in reciprocity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project is only now beginning to take shape. Early designs have been published on the CC website and GitHub page. The organization is actively seeking public feedback ahead of its plans for an alpha launch (early test) in November 2025. It will also host a series of town halls for feedback and questions. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/creative-commons-debuts-cc-signals-a-framework-for-an-open-ai-ecosystem/</guid><pubDate>Wed, 25 Jun 2025 18:02:42 +0000</pubDate></item></channel></rss>