<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 12 Feb 2026 13:14:46 +0000</lastBuildDate><item><title>Accelerating science with AI and simulations (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/accelerating-science-ai-and-simulations-rafael-gomez-bombarelli-0212</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-Rafael-Gomez-Bombarelli-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;For more than a decade, MIT Associate Professor Rafael Gómez-Bombarelli has used artificial intelligence to create new materials. As the technology has expanded, so have his ambitions.&lt;/p&gt;&lt;p&gt;Now, the newly tenured professor in materials science and engineering believes AI is poised to transform science in ways never before possible. His work at MIT and beyond is devoted to accelerating that future.&lt;/p&gt;&lt;p&gt;“We’re at a second inflection point,”&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says. “The first one was around 2015 with the first wave of representation learning, generative AI, and high-throughput data in some areas of science. Those are some of the techniques I first brought into my lab at MIT. Now I think we’re at a second inflection point, mixing language and merging multiple modalities into general scientific intelligence. We’re going to have all the model classes and scaling laws needed to reason about language, reason over material structures, and reason over synthesis recipes.”&lt;/p&gt;&lt;p&gt;Gómez Bombarelli’s research combines physics-based simulations with approaches like machine learning and generative AI to discover new materials with promising real-world applications. His work has led to new materials for batteries, catalysts, plastics, and organic light-emitting diodes (OLEDs). He has also co-founded multiple companies and served on scientific advisory boards for startups applying AI to drug discovery, robotics, and more. His latest company, Lila Sciences, is working to build a scientific superintelligence platform for the life sciences, chemical, and materials science industries.&lt;/p&gt;&lt;p&gt;All of that work is designed to ensure the future of scientific research is more seamless and productive than research today.&lt;/p&gt;&lt;p&gt;“AI for science is one of the most exciting and aspirational uses of AI,” Gómez-Bombarelli says. “Other applications for AI have more downsides and ambiguity. AI for science is about bringing a better future forward in time.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From experiments to simulations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli grew up in Spain and gravitated toward the physical sciences from an early age. In 2001, he won a Chemistry Olympics competition, setting him on an academic track in chemistry, which he studied as an undergraduate at his hometown college, the University of Salamanca. Gómez-Bombarelli stuck around for his PhD, where he investigated the function of DNA-damaging chemicals.&lt;/p&gt;&lt;p&gt;“My PhD started out experimental, and then I got bitten by the bug of simulation and computer science about halfway through,” he says. “I started simulating the same chemical reactions I was measuring in the lab. I like the way programming organizes your brain; it felt like a natural way to organize one’s thinking. Programming is also a lot less limited by what you can do with your hands or with scientific instruments.”&lt;/p&gt;&lt;p&gt;Next, Gómez-Bombarelli went to Scotland for a postdoctoral position, where he studied quantum effects in biology. Through that work, he connected with Alán Aspuru-Guzik, a chemistry professor at Harvard University, whom he joined for his next postdoc in 2014.&lt;/p&gt;&lt;p&gt;“I was one of the first people to use generative AI for chemistry in 2016, and I was on the first team to use neural networks to understand molecules in 2015,” Gómez-Bombarelli says. “It was the early, early days of deep learning for science.”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli also began working to eliminate manual parts of molecular simulations to run more high-throughput experiments. He and his collaborators ended up running hundreds of thousands of calculations across materials, discovering hundreds of promising materials for testing.&lt;/p&gt;&lt;p&gt;After two years in the lab, Gómez-Bombarelli and Aspuru-Guzik started a general-purpose materials computation company, which eventually pivoted to focus on producing organic light-emitting diodes. Gómez-Bombarelli joined the company full-time and calls it the hardest thing he’s ever done in his career.&lt;/p&gt;&lt;p&gt;“It was amazing to make something tangible,” he says. “Also, after seeing Aspuru-Guzik run a lab, I didn’t want to become a professor. My dad was a professor in linguistics, and I thought it was a mellow job. Then I saw Aspuru-Guzik with a 40-person group, and he was on the road 120 days a year. It was insane. I didn’t think I had that type of energy and creativity in me.”&lt;/p&gt;&lt;p&gt;In 2018, Aspuru-Guzik suggested Gómez-Bombarelli apply for a new position in MIT’s Department of Materials Science and Engineering. But, with his trepidation about a faculty job, Gómez-Bombarelli let the deadline pass. Aspuru-Guzik confronted him in his office, slammed his hands on the table, and told him, “You need to apply for this.” It was enough to get Gómez-Bombarelli to put together a formal application.&lt;/p&gt;&lt;p&gt;Fortunately at his startup, Gómez-Bombarelli had spent a lot of time thinking about how to create value from computational materials discovery. During the interview process, he says, he was attracted to the energy and collaborative spirit at MIT. He also began to appreciate the research possibilities.&lt;/p&gt;&lt;p&gt;“Everything I had been doing as a postdoc and at the company was going to be a subset of what I could do at MIT,” he says. “I was making products, and I still get to do that. Suddenly, my universe of work was a subset of this new universe of things I could explore and do.”&lt;/p&gt;&lt;p&gt;It’s been nine years since Gómez Bombarelli joined MIT. Today his lab focuses on how the composition, structure, and reactivity of atoms impact material performance. He has also used high-throughput simulations to create new materials and helped develop tools for merging deep learning with physics-based modeling.&lt;/p&gt;&lt;p&gt;“Physics-based simulations make data and AI algorithms get better the more data you give them,” Gómez Bombarelli’s says. “There are all sorts of virtuous cycles between AI and simulations.”&lt;/p&gt;&lt;p&gt;The research group he has built is solely computational — they don’t run physical experiments.&lt;/p&gt;&lt;p&gt;“It’s a blessing because we can have a huge amount of breadth and do lots of things at once,” he says. “We love working with experimentalists and try to be good partners with them. We also love to create computational tools that help experimentalists triage the ideas coming from AI .”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli is also still focused on the real-world applications of the materials he invents. His lab works closely with companies and organizations like MIT’s Industrial Liaison Program to understand the material needs of the private sector and the practical hurdles of commercial development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Accelerating science&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As excitement around artificial intelligence has exploded, Gómez-Bombarelli has seen the field mature. Companies like Meta, Microsoft, and Google’s DeepMind now regularly conduct physics-based simulations reminiscent of what he was working on back in 2016. In November, the U.S. Department of Energy launched the Genesis Mission to accelerate scientific discovery, national security, and energy dominance using AI.&lt;/p&gt;&lt;p&gt;“AI for simulations has gone from something that maybe could work to a consensus scientific view,” Gómez-Bombarelli says. “We’re at an inflection point. Humans think in natural language, we write papers in natural language, and it turns out these large language models that have mastered natural language have opened up the ability to accelerate science. We’ve seen that scaling works for simulations. We’ve seen that scaling works for language. Now we’re going to see how scaling works for science.”&lt;/p&gt;&lt;p&gt;When he first came to MIT,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says he was blown away by how non-competitive things were between researchers. He tries to bring that same positive-sum thinking to his research group, which is made up of about 25 graduate students and postdocs.&lt;/p&gt;&lt;p&gt;“We’ve naturally grown into a really diverse group, with a diverse set of mentalities,” Gomez-Bombarelli says. “Everyone has their own career aspirations and strengths and weaknesses. Figuring out how to help people be the best versions of themselves is fun. Now I’ve become the one insisting that people apply to faculty positions after the deadline. I guess I’ve passed that baton.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-Rafael-Gomez-Bombarelli-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;For more than a decade, MIT Associate Professor Rafael Gómez-Bombarelli has used artificial intelligence to create new materials. As the technology has expanded, so have his ambitions.&lt;/p&gt;&lt;p&gt;Now, the newly tenured professor in materials science and engineering believes AI is poised to transform science in ways never before possible. His work at MIT and beyond is devoted to accelerating that future.&lt;/p&gt;&lt;p&gt;“We’re at a second inflection point,”&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says. “The first one was around 2015 with the first wave of representation learning, generative AI, and high-throughput data in some areas of science. Those are some of the techniques I first brought into my lab at MIT. Now I think we’re at a second inflection point, mixing language and merging multiple modalities into general scientific intelligence. We’re going to have all the model classes and scaling laws needed to reason about language, reason over material structures, and reason over synthesis recipes.”&lt;/p&gt;&lt;p&gt;Gómez Bombarelli’s research combines physics-based simulations with approaches like machine learning and generative AI to discover new materials with promising real-world applications. His work has led to new materials for batteries, catalysts, plastics, and organic light-emitting diodes (OLEDs). He has also co-founded multiple companies and served on scientific advisory boards for startups applying AI to drug discovery, robotics, and more. His latest company, Lila Sciences, is working to build a scientific superintelligence platform for the life sciences, chemical, and materials science industries.&lt;/p&gt;&lt;p&gt;All of that work is designed to ensure the future of scientific research is more seamless and productive than research today.&lt;/p&gt;&lt;p&gt;“AI for science is one of the most exciting and aspirational uses of AI,” Gómez-Bombarelli says. “Other applications for AI have more downsides and ambiguity. AI for science is about bringing a better future forward in time.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From experiments to simulations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli grew up in Spain and gravitated toward the physical sciences from an early age. In 2001, he won a Chemistry Olympics competition, setting him on an academic track in chemistry, which he studied as an undergraduate at his hometown college, the University of Salamanca. Gómez-Bombarelli stuck around for his PhD, where he investigated the function of DNA-damaging chemicals.&lt;/p&gt;&lt;p&gt;“My PhD started out experimental, and then I got bitten by the bug of simulation and computer science about halfway through,” he says. “I started simulating the same chemical reactions I was measuring in the lab. I like the way programming organizes your brain; it felt like a natural way to organize one’s thinking. Programming is also a lot less limited by what you can do with your hands or with scientific instruments.”&lt;/p&gt;&lt;p&gt;Next, Gómez-Bombarelli went to Scotland for a postdoctoral position, where he studied quantum effects in biology. Through that work, he connected with Alán Aspuru-Guzik, a chemistry professor at Harvard University, whom he joined for his next postdoc in 2014.&lt;/p&gt;&lt;p&gt;“I was one of the first people to use generative AI for chemistry in 2016, and I was on the first team to use neural networks to understand molecules in 2015,” Gómez-Bombarelli says. “It was the early, early days of deep learning for science.”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli also began working to eliminate manual parts of molecular simulations to run more high-throughput experiments. He and his collaborators ended up running hundreds of thousands of calculations across materials, discovering hundreds of promising materials for testing.&lt;/p&gt;&lt;p&gt;After two years in the lab, Gómez-Bombarelli and Aspuru-Guzik started a general-purpose materials computation company, which eventually pivoted to focus on producing organic light-emitting diodes. Gómez-Bombarelli joined the company full-time and calls it the hardest thing he’s ever done in his career.&lt;/p&gt;&lt;p&gt;“It was amazing to make something tangible,” he says. “Also, after seeing Aspuru-Guzik run a lab, I didn’t want to become a professor. My dad was a professor in linguistics, and I thought it was a mellow job. Then I saw Aspuru-Guzik with a 40-person group, and he was on the road 120 days a year. It was insane. I didn’t think I had that type of energy and creativity in me.”&lt;/p&gt;&lt;p&gt;In 2018, Aspuru-Guzik suggested Gómez-Bombarelli apply for a new position in MIT’s Department of Materials Science and Engineering. But, with his trepidation about a faculty job, Gómez-Bombarelli let the deadline pass. Aspuru-Guzik confronted him in his office, slammed his hands on the table, and told him, “You need to apply for this.” It was enough to get Gómez-Bombarelli to put together a formal application.&lt;/p&gt;&lt;p&gt;Fortunately at his startup, Gómez-Bombarelli had spent a lot of time thinking about how to create value from computational materials discovery. During the interview process, he says, he was attracted to the energy and collaborative spirit at MIT. He also began to appreciate the research possibilities.&lt;/p&gt;&lt;p&gt;“Everything I had been doing as a postdoc and at the company was going to be a subset of what I could do at MIT,” he says. “I was making products, and I still get to do that. Suddenly, my universe of work was a subset of this new universe of things I could explore and do.”&lt;/p&gt;&lt;p&gt;It’s been nine years since Gómez Bombarelli joined MIT. Today his lab focuses on how the composition, structure, and reactivity of atoms impact material performance. He has also used high-throughput simulations to create new materials and helped develop tools for merging deep learning with physics-based modeling.&lt;/p&gt;&lt;p&gt;“Physics-based simulations make data and AI algorithms get better the more data you give them,” Gómez Bombarelli’s says. “There are all sorts of virtuous cycles between AI and simulations.”&lt;/p&gt;&lt;p&gt;The research group he has built is solely computational — they don’t run physical experiments.&lt;/p&gt;&lt;p&gt;“It’s a blessing because we can have a huge amount of breadth and do lots of things at once,” he says. “We love working with experimentalists and try to be good partners with them. We also love to create computational tools that help experimentalists triage the ideas coming from AI .”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli is also still focused on the real-world applications of the materials he invents. His lab works closely with companies and organizations like MIT’s Industrial Liaison Program to understand the material needs of the private sector and the practical hurdles of commercial development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Accelerating science&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As excitement around artificial intelligence has exploded, Gómez-Bombarelli has seen the field mature. Companies like Meta, Microsoft, and Google’s DeepMind now regularly conduct physics-based simulations reminiscent of what he was working on back in 2016. In November, the U.S. Department of Energy launched the Genesis Mission to accelerate scientific discovery, national security, and energy dominance using AI.&lt;/p&gt;&lt;p&gt;“AI for simulations has gone from something that maybe could work to a consensus scientific view,” Gómez-Bombarelli says. “We’re at an inflection point. Humans think in natural language, we write papers in natural language, and it turns out these large language models that have mastered natural language have opened up the ability to accelerate science. We’ve seen that scaling works for simulations. We’ve seen that scaling works for language. Now we’re going to see how scaling works for science.”&lt;/p&gt;&lt;p&gt;When he first came to MIT,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says he was blown away by how non-competitive things were between researchers. He tries to bring that same positive-sum thinking to his research group, which is made up of about 25 graduate students and postdocs.&lt;/p&gt;&lt;p&gt;“We’ve naturally grown into a really diverse group, with a diverse set of mentalities,” Gomez-Bombarelli says. “Everyone has their own career aspirations and strengths and weaknesses. Figuring out how to help people be the best versions of themselves is fun. Now I’ve become the one insisting that people apply to faculty positions after the deadline. I guess I’ve passed that baton.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/accelerating-science-ai-and-simulations-rafael-gomez-bombarelli-0212</guid><pubDate>Thu, 12 Feb 2026 05:00:00 +0000</pubDate></item><item><title>[NEW] State-sponsored hackers exploit AI for advanced cyberattacks (AI News)</title><link>https://www.artificialintelligence-news.com/news/state-sponsored-hackers-ai-cyberattacks-google/</link><description>&lt;p&gt;State-sponsored hackers are exploiting AI to accelerate cyberattacks, with threat actors from Iran, North Korea, China, and Russia weaponising models like Google’s Gemini to craft sophisticated phishing campaigns and develop malware, according to a new report from Google’s Threat Intelligence Group (GTIG).&lt;/p&gt;&lt;p&gt;The quarterly AI Threat Tracker report, released today, reveals how government-backed attackers have integrated artificial intelligence throughout the attack lifecycle – achieving productivity gains in reconnaissance, social engineering, and malware development during the final quarter of 2025.&lt;/p&gt;&lt;p&gt;“For government-backed threat actors, large language models have become essential tools for technical research, targeting, and the rapid generation of nuanced phishing lures,” GTIG researchers stated in the report.&lt;/p&gt;&lt;h3&gt;AI-powered reconnaissance by state-sponsored hackers targets the defence sector&lt;/h3&gt;&lt;p&gt;Iranian threat actor APT42 used Gemini to augment reconnaissance and targeted social engineering operations. The group misused the AI model to enumerate official email addresses for specific entities and conduct research to establish credible pretexts for approaching targets.&lt;/p&gt;&lt;p&gt;By feeding Gemini a target’s biography, APT42 crafted personas and scenarios designed to elicit engagement. The group also used the AI to translate between languages and better understand non-native phrases – abilities that help state-sponsored hackers bypass traditional phishing red flags like poor grammar or awkward syntax.&lt;/p&gt;&lt;p&gt;North Korean government-backed actor UNC2970, which focuses on defence targeting and impersonating corporate recruiters, used Gemini to synthesise open-source intelligence and profile high-value targets. The group’s reconnaissance included searching for information on major cybersecurity and defence companies, mapping specific technical job roles, and gathering salary information.&lt;/p&gt;&lt;p&gt;“This activity blurs the distinction between routine professional research and malicious reconnaissance, as the actor gathers the necessary components to create tailored, high-fidelity phishing personas,” GTIG noted.&lt;/p&gt;&lt;h3&gt;Model extraction attacks surge&lt;/h3&gt;&lt;p&gt;Beyond operational misuse, Google DeepMind and GTIG identified a increase in model extraction attempts – also known as “distillation attacks” – aimed at stealing intellectual property from AI models.&lt;/p&gt;&lt;p&gt;One campaign targeting Gemini’s reasoning abilities involved over 100,000 prompts designed to coerce the model into outputting full reasoning processes. The breadth of questions suggested an attempt to replicate Gemini’s reasoning ability in non-English target languages in various tasks.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112168" height="463" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Screenshot-2026-02-12-at-3.28.33-PM-1024x463.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;How model extraction attacks work to steal AI intellectual property. (Image: Google GTIG)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While GTIG observed no direct attacks on frontier models from advanced persistent threat actors, the team identified and disrupted frequent model extraction attacks from private sector entities globally and researchers seeking to clone proprietary logic.&lt;/p&gt;&lt;p&gt;Google’s systems recognised these attacks in real-time and deployed defences to protect internal reasoning traces.&lt;/p&gt;&lt;h3&gt;AI-integrated malware emerges&lt;/h3&gt;&lt;p&gt;GTIG observed malware samples, tracked as HONESTCUE, that use Gemini’s API to outsource functionality generation. The malware is designed to undermine traditional network-based detection and static analysis through a multi-layered obfuscation approach.&lt;/p&gt;&lt;p&gt;HONESTCUE functions as a downloader and launcher framework that sends prompts via Gemini’s API and receives C# source code as responses. The fileless secondary stage compiles and executes payloads directly in memory, leaving no artefacts on disk.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112170" height="470" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Screenshot-2026-02-12-at-3.29.57-PM-1024x470.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;HONESTCUE malware’s two-stage attack process using Gemini’s API. (Image: Google GTIG)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Separately, GTIG identified COINBAIT, a phishing kit whose construction was likely accelerated by AI code generation tools. The kit, which masquerades as a major cryptocurrency exchange for credential harvesting, was built using the AI-powered platform Lovable AI.&lt;/p&gt;&lt;h3&gt;ClickFix campaigns abuse AI chat platforms&lt;/h3&gt;&lt;p&gt;In a novel social engineering campaign first observed in December 2025, Google saw threat actors abuse the public sharing features of generative AI services – including Gemini, ChatGPT, Copilot, DeepSeek, and Grok – to host deceptive content distributing ATOMIC malware targeting macOS systems.&lt;/p&gt;&lt;p&gt;Attackers manipulated AI models to create realistic-looking instructions for common computer tasks, embedding malicious command-line scripts as the “solution.” By creating shareable links to these AI chat transcripts, threat actors used trusted domains to host their initial attack stage.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112171" height="477" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Screenshot-2026-02-12-at-3.29.32-PM-1-1024x477.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;The three-stage ClickFix attack chain exploiting AI chat platforms. (Image: Google GTIG)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Underground marketplace thrives on stolen API keys&lt;/h3&gt;&lt;p&gt;GTIG’s observations of English and Russian-language underground forums indicate a persistent demand for AI-enabled tools and services. However, state-sponsored hackers and cybercriminals struggle to develop custom AI models, instead relying on mature commercial products accessed through stolen credentials.&lt;/p&gt;&lt;p&gt;One toolkit, “Xanthorox,” advertised itself as a custom AI for autonomous malware generation and phishing campaign development. GTIG’s investigation revealed Xanthorox was not a bespoke model but actually powered by several commercial AI products, including Gemini, accessed through stolen API keys.&lt;/p&gt;&lt;h3&gt;Google’s response and mitigations&lt;/h3&gt;&lt;p&gt;Google has taken action against identified threat actors by disabling accounts and assets associated with malicious activity. The company has also applied intelligence to strengthen both classifiers and models, letting them refuse assistance with similar attacks moving forward.\&lt;/p&gt;&lt;p&gt;“We are committed to developing AI boldly and responsibly, which means taking proactive steps to disrupt malicious activity by disabling the projects and accounts associated with bad actors, while continuously improving our models to make them less susceptible to misuse,” the report stated.&lt;/p&gt;&lt;p&gt;GTIG emphasised that despite these developments, no APT or information operations actors have achieved breakthrough abilities that fundamentally alter the threat landscape.&lt;/p&gt;&lt;p&gt;The findings underscore the evolving role of AI in cybersecurity, as both defenders and attackers race to use the technology’s abilities.&lt;/p&gt;&lt;p&gt;For enterprise security teams, particularly in the Asia-Pacific region where Chinese and North Korean state-sponsored hackers remain active, the report serves as an important reminder to enhance defences against AI-augmented social engineering and reconnaissance operations.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by SCARECROW artworks)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Anthropic just revealed how AI-orchestrated cyberattacks actually work – Here’s what enterprises need to know&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;State-sponsored hackers are exploiting AI to accelerate cyberattacks, with threat actors from Iran, North Korea, China, and Russia weaponising models like Google’s Gemini to craft sophisticated phishing campaigns and develop malware, according to a new report from Google’s Threat Intelligence Group (GTIG).&lt;/p&gt;&lt;p&gt;The quarterly AI Threat Tracker report, released today, reveals how government-backed attackers have integrated artificial intelligence throughout the attack lifecycle – achieving productivity gains in reconnaissance, social engineering, and malware development during the final quarter of 2025.&lt;/p&gt;&lt;p&gt;“For government-backed threat actors, large language models have become essential tools for technical research, targeting, and the rapid generation of nuanced phishing lures,” GTIG researchers stated in the report.&lt;/p&gt;&lt;h3&gt;AI-powered reconnaissance by state-sponsored hackers targets the defence sector&lt;/h3&gt;&lt;p&gt;Iranian threat actor APT42 used Gemini to augment reconnaissance and targeted social engineering operations. The group misused the AI model to enumerate official email addresses for specific entities and conduct research to establish credible pretexts for approaching targets.&lt;/p&gt;&lt;p&gt;By feeding Gemini a target’s biography, APT42 crafted personas and scenarios designed to elicit engagement. The group also used the AI to translate between languages and better understand non-native phrases – abilities that help state-sponsored hackers bypass traditional phishing red flags like poor grammar or awkward syntax.&lt;/p&gt;&lt;p&gt;North Korean government-backed actor UNC2970, which focuses on defence targeting and impersonating corporate recruiters, used Gemini to synthesise open-source intelligence and profile high-value targets. The group’s reconnaissance included searching for information on major cybersecurity and defence companies, mapping specific technical job roles, and gathering salary information.&lt;/p&gt;&lt;p&gt;“This activity blurs the distinction between routine professional research and malicious reconnaissance, as the actor gathers the necessary components to create tailored, high-fidelity phishing personas,” GTIG noted.&lt;/p&gt;&lt;h3&gt;Model extraction attacks surge&lt;/h3&gt;&lt;p&gt;Beyond operational misuse, Google DeepMind and GTIG identified a increase in model extraction attempts – also known as “distillation attacks” – aimed at stealing intellectual property from AI models.&lt;/p&gt;&lt;p&gt;One campaign targeting Gemini’s reasoning abilities involved over 100,000 prompts designed to coerce the model into outputting full reasoning processes. The breadth of questions suggested an attempt to replicate Gemini’s reasoning ability in non-English target languages in various tasks.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112168" height="463" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Screenshot-2026-02-12-at-3.28.33-PM-1024x463.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;How model extraction attacks work to steal AI intellectual property. (Image: Google GTIG)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;While GTIG observed no direct attacks on frontier models from advanced persistent threat actors, the team identified and disrupted frequent model extraction attacks from private sector entities globally and researchers seeking to clone proprietary logic.&lt;/p&gt;&lt;p&gt;Google’s systems recognised these attacks in real-time and deployed defences to protect internal reasoning traces.&lt;/p&gt;&lt;h3&gt;AI-integrated malware emerges&lt;/h3&gt;&lt;p&gt;GTIG observed malware samples, tracked as HONESTCUE, that use Gemini’s API to outsource functionality generation. The malware is designed to undermine traditional network-based detection and static analysis through a multi-layered obfuscation approach.&lt;/p&gt;&lt;p&gt;HONESTCUE functions as a downloader and launcher framework that sends prompts via Gemini’s API and receives C# source code as responses. The fileless secondary stage compiles and executes payloads directly in memory, leaving no artefacts on disk.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112170" height="470" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Screenshot-2026-02-12-at-3.29.57-PM-1024x470.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;HONESTCUE malware’s two-stage attack process using Gemini’s API. (Image: Google GTIG)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Separately, GTIG identified COINBAIT, a phishing kit whose construction was likely accelerated by AI code generation tools. The kit, which masquerades as a major cryptocurrency exchange for credential harvesting, was built using the AI-powered platform Lovable AI.&lt;/p&gt;&lt;h3&gt;ClickFix campaigns abuse AI chat platforms&lt;/h3&gt;&lt;p&gt;In a novel social engineering campaign first observed in December 2025, Google saw threat actors abuse the public sharing features of generative AI services – including Gemini, ChatGPT, Copilot, DeepSeek, and Grok – to host deceptive content distributing ATOMIC malware targeting macOS systems.&lt;/p&gt;&lt;p&gt;Attackers manipulated AI models to create realistic-looking instructions for common computer tasks, embedding malicious command-line scripts as the “solution.” By creating shareable links to these AI chat transcripts, threat actors used trusted domains to host their initial attack stage.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-112171" height="477" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/02/Screenshot-2026-02-12-at-3.29.32-PM-1-1024x477.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;The three-stage ClickFix attack chain exploiting AI chat platforms. (Image: Google GTIG)&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Underground marketplace thrives on stolen API keys&lt;/h3&gt;&lt;p&gt;GTIG’s observations of English and Russian-language underground forums indicate a persistent demand for AI-enabled tools and services. However, state-sponsored hackers and cybercriminals struggle to develop custom AI models, instead relying on mature commercial products accessed through stolen credentials.&lt;/p&gt;&lt;p&gt;One toolkit, “Xanthorox,” advertised itself as a custom AI for autonomous malware generation and phishing campaign development. GTIG’s investigation revealed Xanthorox was not a bespoke model but actually powered by several commercial AI products, including Gemini, accessed through stolen API keys.&lt;/p&gt;&lt;h3&gt;Google’s response and mitigations&lt;/h3&gt;&lt;p&gt;Google has taken action against identified threat actors by disabling accounts and assets associated with malicious activity. The company has also applied intelligence to strengthen both classifiers and models, letting them refuse assistance with similar attacks moving forward.\&lt;/p&gt;&lt;p&gt;“We are committed to developing AI boldly and responsibly, which means taking proactive steps to disrupt malicious activity by disabling the projects and accounts associated with bad actors, while continuously improving our models to make them less susceptible to misuse,” the report stated.&lt;/p&gt;&lt;p&gt;GTIG emphasised that despite these developments, no APT or information operations actors have achieved breakthrough abilities that fundamentally alter the threat landscape.&lt;/p&gt;&lt;p&gt;The findings underscore the evolving role of AI in cybersecurity, as both defenders and attackers race to use the technology’s abilities.&lt;/p&gt;&lt;p&gt;For enterprise security teams, particularly in the Asia-Pacific region where Chinese and North Korean state-sponsored hackers remain active, the report serves as an important reminder to enhance defences against AI-augmented social engineering and reconnaissance operations.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by SCARECROW artworks)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Anthropic just revealed how AI-orchestrated cyberattacks actually work – Here’s what enterprises need to know&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/state-sponsored-hackers-ai-cyberattacks-google/</guid><pubDate>Thu, 12 Feb 2026 09:00:00 +0000</pubDate></item><item><title>[NEW] What’s next for Chinese open-source AI (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/12/1132811/whats-next-for-chinese-open-source-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/260205_chineseAI_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt;’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&amp;nbsp;&lt;/em&gt;&lt;em&gt;here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;The past year has marked a turning point for Chinese AI. Since DeepSeek released its R1 reasoning model in January 2025, Chinese companies have repeatedly delivered AI models that match the performance of leading Western models at a fraction of the cost.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Just last week the Chinese firm Moonshot AI released its latest open-weight model, Kimi K2.5, which came close to top proprietary systems such as Anthropic’s Claude Opus on some early benchmarks.&amp;nbsp;The difference: K2.5 is roughly one-seventh Opus’s price.&lt;/p&gt;  &lt;p&gt;On Hugging Face, Alibaba’s Qwen family—after ranking as the most downloaded model series in 2025 and 2026—has overtaken Meta’s Llama models in cumulative downloads. And a recent MIT study found that Chinese open-source models have surpassed US models in total downloads. For developers and builders worldwide, access to near-frontier AI capabilities has never been this broad or this affordable.&lt;/p&gt; 
 &lt;p&gt;But these models differ in a crucial way from most US models like ChatGPT or Claude, which you pay to access and can’t inspect. The Chinese companies publish their models’ weights—numerical values that get set when a model is trained—so anyone can download, run, study, and modify them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If these open-source AI models keep getting better, they will not just offer the cheapest options for people who want access to frontier AI capabilities; they will change where innovation happens and who sets the standards.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Here’s what may come next.&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;China’s commitment to open source will continue&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;When DeepSeek launched R1, much of the initial shock centered on its origin. Suddenly, a Chinese team had released a reasoning model that could stand alongside the best systems from US labs. But the long tail of DeepSeek’s impact had less to do with nationality than with distribution. R1 was released as an open-weight model under a permissive MIT license, allowing anyone to download, inspect, and deploy it. On top of that, DeepSeek also published a paper detailing its training process and techniques. For developers who access models via an API, DeepSeek also undercut competitors on price, offering access at a fraction the cost of OpenAI’s o1, the leading proprietary reasoning model at the time.&lt;/p&gt;  &lt;p&gt;Within days of its release, DeepSeek replaced ChatGPT as the most downloaded free app in the US App Store. The moment spilled beyond developer circles into financial markets, triggering a sharp sell-off in US tech stocks that briefly erased roughly $1 trillion in market value. Almost overnight, DeepSeek went from a little-known spin-off team backed by a quantitative hedge fund to the most visible symbol of China’s push for open-source AI. &lt;/p&gt;  &lt;p&gt;China’s decision to lean into open source isn’t surprising. It has the world’s second-largest concentration of AI talent after the US. plus a vast, well-resourced tech industry. After ChatGPT broke into the mainstream, China’s AI sector went through a reckoning—and emerged determined to catch up. Pursuing an open-source strategy was seen as the fastest way to close the gap by rallying developers, spreading adoption, and setting standards.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;DeepSeek’s success injected confidence into an industry long used to following global standards rather than setting them. “Thirty years ago, no Chinese person would believe they could be at the center of global innovation,” says Alex Chenglin Wu, CEO and founder of Atoms, an AI agent company and prominent contributor to China’s open-source ecosystem. “DeepSeek shows that with solid technical talent, a supportive environment, and the right organizational culture, it’s possible to do truly world-class work.”&lt;/p&gt;  &lt;p&gt;DeepSeek’s breakout moment wasn’t China’s first open-source success. Alibaba’s Qwen Lab had been releasing open-weight models for years. By September 2024,&amp;nbsp; well before DeepSeek’s V3 launch, Alibaba was saying that global downloads had exceeded 600 million. On Hugging Face, Qwen accounted for more than 30% of all model downloads in 2024. Other institutions, including the Beijing Academy of Artificial Intelligence and the AI firm Baichuan, were also releasing open models as early as 2023.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But since the success of DeepSeek, the field has widened rapidly. Companies such as Z.ai (formerly Zhipu), MiniMax, Tencent, and a growing number of smaller labs have released models that are competitive on reasoning, coding, and agent-style tasks. The growing number of capable models has sped up progress. Capabilities that once took months to make it to the open-source world now emerge within weeks, even days.&lt;/p&gt;  &lt;p&gt;“Chinese AI firms have seen real gains from the open-source playbook,” says Liu Zhiyuan, a professor of computer science at Tsinghua University and chief scientist at the AI startup ModelBest. “By releasing strong research, they build reputation and gain free publicity.”&lt;/p&gt; 

 &lt;p&gt;Beyond commercial incentives, Liu says, open source has taken on cultural and strategic weight. “In the Chinese programmer community, open source has become politically correct,” he says, framing it as a response to US.dominance in proprietary AI systems.&lt;/p&gt;  &lt;p&gt;That shift is also reflected at the institutional level. Universities including Tsinghua have begun encouraging AI development and open-source contributions, while policymakers have moved to formalize those incentives. In August, China’s State Council released a draft policy encouraging universities to reward open-source work, proposing that students’ contributions on platforms such as GitHub or Gitee could eventually be counted toward academic credit.&lt;/p&gt;  &lt;p&gt;With growing momentum and a reinforcing feedback loop, China’s push for open-source models is likely to continue in the near term, though its long-term sustainability still hinges on financial results, says Tiezhen Wang, who helps lead work on global AI at Hugging Face. In January, the model labs Z.ai and MiniMax went public in Hong Kong. “Right now, the focus is on making the cake bigger,” says Wang. “The next challenge is figuring out how each company secures its share.”&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;The next wave of models will be narrower—and better&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Chinese open-source models are leading not just in download volume but also in variety. Alibaba’s Qwen has become one of the most diversified open model families in circulation, offering a wide range of variants optimized for different uses. The lineup ranges from lightweight models that can run on a single laptop to large, multi-hundred-billion-parameter systems designed for data-center deployment. Qwen features many task-optimized variants created by the community: the “instruct” models are good at following orders, and “code” variants specialize in coding.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;Although this strategy isn’t unique to Chinese labs, Qwen was the first open model family to roll out so many high-quality options that it started to feel like a full product line—one that’s free to use.&lt;/p&gt;  &lt;p&gt;The open-weight nature of these releases also makes it easy for others to adapt them through techniques like fine-tuning and distillation, which means training a smaller model to mimic a larger one.&amp;nbsp; According to ATOM (American Truly Open Models), a project by the AI researcher Nathan Lambert, by August 4, 2025, new model variations derived from Qwen were “more than 40%” of new Hugging Face language-model derivatives, while Llama had fallen to about 15%. This means that Qwen has become the default base model for all the “remixes.”&lt;/p&gt;  &lt;p&gt;This pattern has made the case for smaller, more specialized models. “Compute and energy are real constraints for any deployment,” Liu says. He told &lt;em&gt;MIT Technology Review&lt;/em&gt; that the rise of small models is about making AI cheaper to run and easier for more people to use. His company, ModelBest, focuses on small language models designed to run locally on devices such as phones, cars, and other consumer hardware.&lt;/p&gt;  &lt;p&gt;While an average user might interact with AI only through the web or an app for simple conversations, power users of AI models with some technical background are experimenting with giving AI more autonomy to solve large-scale problems. OpenClaw, an open-source AI agent that recently went viral within the AI hacker world, allows AI to take over your computer—it can run 24-7, going through your emails and work tasks without supervision.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;OpenClaw, like many other open-source tools, allows users to connect to different AI models via an application programming interface, or API. Within days of OpenClaw’s release, the team revealed that Kimi’s K2.5 had surpassed Claude Opus and became the most used AI model—by token count, meaning it was handling more total text processed across user prompts and model responses.&lt;/p&gt;  &lt;p&gt;Cost has been a major reason Chinese models have gained traction, but it would be a mistake to treat them as mere “dupes” of Western frontier systems, Wang suggests. Like any product, a model only needs to be good enough for the job at hand.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The landscape of open-source models in China is also getting more specialized. Research groups such as Shanghai AI Laboratory have released models geared toward scientific and technical tasks; several projects from Tencent have focused specifically on music generation. Ubiquant, a quantitative finance firm like DeepSeek’s parent High-Flyer, has released an open model aimed at medical reasoning.&lt;/p&gt;  &lt;p&gt;In the meantime, innovative architectural ideas from Chinese labs are being picked up more broadly. DeepSeek has published work exploring model efficiency and memory; techniques that compress the model’s attention “cache,” reducing memory and inference costs while mostly preserving performance, have drawn significant attention in the research community.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;“The impact of these research breakthroughs is amplified because they’re open-sourced and can be picked up quickly across the field,” says Wang.&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;Chinese open models will become infrastructure for global AI builders&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;The adoption of Chinese models is picking up in Silicon Valley, too. Martin Casado, a general partner at Andreessen Horowitz, has put a number on it: Among startups pitching with open-source stacks, there’s about an 80% chance they’re running on Chinese open models, according to a post he made on X. Usage data tells a similar story. OpenRouter,&amp;nbsp; a middleman that tracks how people use different AI models through its API, shows Chinese open models rising from almost none in late 2024 to nearly 30% of usage in some recent weeks.&lt;/p&gt;  &lt;p&gt;The demand is also rising globally. Z.ai limited new subscriptions to its GLM coding plan (a coding tool based on its flagship GLM models) after demand surged, citing compute constraints. What’s notable is where the demand is coming from: CNBC reports that the system’s user base is primarily concentrated in the United States and China, followed by India, Japan, Brazil, and the UK.&lt;/p&gt;  &lt;p&gt;“The open-source ecosystems in China and the US are tightly bound together,” says Wang at Hugging Face. Many Chinese open models still rely on Nvidia and US cloud platforms to train and serve them, which keeps the business ties tangled. Talent is fluid too: Researchers move across borders and companies, and many still operate as a global community, sharing code and ideas in public.&lt;/p&gt;  &lt;p&gt;That interdependence is part of what makes Chinese developers feel optimistic about this moment: The work travels, gets remixed, and actually shows up in products. But openness can also accelerate the competition. Dario Amodei, the CEO of Anthropic, made a version of this point after DeepSeek’s 2025 releases: He wrote that export controls are “not a way to duck the competition” between the US and China, and that AI companies in the US “must have better models” if they want to prevail.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For the past decade, the story of Chinese tech in the West has been one of big expectations that ran into scrutiny, restrictions, and political backlash. This time the export isn’t just an app or a consumer platform. It’s the underlying model layer that other people build on. Whether that will play out differently is still an open question.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/260205_chineseAI_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt;’s What’s Next series looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them&amp;nbsp;&lt;/em&gt;&lt;em&gt;here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;The past year has marked a turning point for Chinese AI. Since DeepSeek released its R1 reasoning model in January 2025, Chinese companies have repeatedly delivered AI models that match the performance of leading Western models at a fraction of the cost.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Just last week the Chinese firm Moonshot AI released its latest open-weight model, Kimi K2.5, which came close to top proprietary systems such as Anthropic’s Claude Opus on some early benchmarks.&amp;nbsp;The difference: K2.5 is roughly one-seventh Opus’s price.&lt;/p&gt;  &lt;p&gt;On Hugging Face, Alibaba’s Qwen family—after ranking as the most downloaded model series in 2025 and 2026—has overtaken Meta’s Llama models in cumulative downloads. And a recent MIT study found that Chinese open-source models have surpassed US models in total downloads. For developers and builders worldwide, access to near-frontier AI capabilities has never been this broad or this affordable.&lt;/p&gt; 
 &lt;p&gt;But these models differ in a crucial way from most US models like ChatGPT or Claude, which you pay to access and can’t inspect. The Chinese companies publish their models’ weights—numerical values that get set when a model is trained—so anyone can download, run, study, and modify them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If these open-source AI models keep getting better, they will not just offer the cheapest options for people who want access to frontier AI capabilities; they will change where innovation happens and who sets the standards.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Here’s what may come next.&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;China’s commitment to open source will continue&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;When DeepSeek launched R1, much of the initial shock centered on its origin. Suddenly, a Chinese team had released a reasoning model that could stand alongside the best systems from US labs. But the long tail of DeepSeek’s impact had less to do with nationality than with distribution. R1 was released as an open-weight model under a permissive MIT license, allowing anyone to download, inspect, and deploy it. On top of that, DeepSeek also published a paper detailing its training process and techniques. For developers who access models via an API, DeepSeek also undercut competitors on price, offering access at a fraction the cost of OpenAI’s o1, the leading proprietary reasoning model at the time.&lt;/p&gt;  &lt;p&gt;Within days of its release, DeepSeek replaced ChatGPT as the most downloaded free app in the US App Store. The moment spilled beyond developer circles into financial markets, triggering a sharp sell-off in US tech stocks that briefly erased roughly $1 trillion in market value. Almost overnight, DeepSeek went from a little-known spin-off team backed by a quantitative hedge fund to the most visible symbol of China’s push for open-source AI. &lt;/p&gt;  &lt;p&gt;China’s decision to lean into open source isn’t surprising. It has the world’s second-largest concentration of AI talent after the US. plus a vast, well-resourced tech industry. After ChatGPT broke into the mainstream, China’s AI sector went through a reckoning—and emerged determined to catch up. Pursuing an open-source strategy was seen as the fastest way to close the gap by rallying developers, spreading adoption, and setting standards.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;DeepSeek’s success injected confidence into an industry long used to following global standards rather than setting them. “Thirty years ago, no Chinese person would believe they could be at the center of global innovation,” says Alex Chenglin Wu, CEO and founder of Atoms, an AI agent company and prominent contributor to China’s open-source ecosystem. “DeepSeek shows that with solid technical talent, a supportive environment, and the right organizational culture, it’s possible to do truly world-class work.”&lt;/p&gt;  &lt;p&gt;DeepSeek’s breakout moment wasn’t China’s first open-source success. Alibaba’s Qwen Lab had been releasing open-weight models for years. By September 2024,&amp;nbsp; well before DeepSeek’s V3 launch, Alibaba was saying that global downloads had exceeded 600 million. On Hugging Face, Qwen accounted for more than 30% of all model downloads in 2024. Other institutions, including the Beijing Academy of Artificial Intelligence and the AI firm Baichuan, were also releasing open models as early as 2023.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But since the success of DeepSeek, the field has widened rapidly. Companies such as Z.ai (formerly Zhipu), MiniMax, Tencent, and a growing number of smaller labs have released models that are competitive on reasoning, coding, and agent-style tasks. The growing number of capable models has sped up progress. Capabilities that once took months to make it to the open-source world now emerge within weeks, even days.&lt;/p&gt;  &lt;p&gt;“Chinese AI firms have seen real gains from the open-source playbook,” says Liu Zhiyuan, a professor of computer science at Tsinghua University and chief scientist at the AI startup ModelBest. “By releasing strong research, they build reputation and gain free publicity.”&lt;/p&gt; 

 &lt;p&gt;Beyond commercial incentives, Liu says, open source has taken on cultural and strategic weight. “In the Chinese programmer community, open source has become politically correct,” he says, framing it as a response to US.dominance in proprietary AI systems.&lt;/p&gt;  &lt;p&gt;That shift is also reflected at the institutional level. Universities including Tsinghua have begun encouraging AI development and open-source contributions, while policymakers have moved to formalize those incentives. In August, China’s State Council released a draft policy encouraging universities to reward open-source work, proposing that students’ contributions on platforms such as GitHub or Gitee could eventually be counted toward academic credit.&lt;/p&gt;  &lt;p&gt;With growing momentum and a reinforcing feedback loop, China’s push for open-source models is likely to continue in the near term, though its long-term sustainability still hinges on financial results, says Tiezhen Wang, who helps lead work on global AI at Hugging Face. In January, the model labs Z.ai and MiniMax went public in Hong Kong. “Right now, the focus is on making the cake bigger,” says Wang. “The next challenge is figuring out how each company secures its share.”&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;The next wave of models will be narrower—and better&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;Chinese open-source models are leading not just in download volume but also in variety. Alibaba’s Qwen has become one of the most diversified open model families in circulation, offering a wide range of variants optimized for different uses. The lineup ranges from lightweight models that can run on a single laptop to large, multi-hundred-billion-parameter systems designed for data-center deployment. Qwen features many task-optimized variants created by the community: the “instruct” models are good at following orders, and “code” variants specialize in coding.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;Although this strategy isn’t unique to Chinese labs, Qwen was the first open model family to roll out so many high-quality options that it started to feel like a full product line—one that’s free to use.&lt;/p&gt;  &lt;p&gt;The open-weight nature of these releases also makes it easy for others to adapt them through techniques like fine-tuning and distillation, which means training a smaller model to mimic a larger one.&amp;nbsp; According to ATOM (American Truly Open Models), a project by the AI researcher Nathan Lambert, by August 4, 2025, new model variations derived from Qwen were “more than 40%” of new Hugging Face language-model derivatives, while Llama had fallen to about 15%. This means that Qwen has become the default base model for all the “remixes.”&lt;/p&gt;  &lt;p&gt;This pattern has made the case for smaller, more specialized models. “Compute and energy are real constraints for any deployment,” Liu says. He told &lt;em&gt;MIT Technology Review&lt;/em&gt; that the rise of small models is about making AI cheaper to run and easier for more people to use. His company, ModelBest, focuses on small language models designed to run locally on devices such as phones, cars, and other consumer hardware.&lt;/p&gt;  &lt;p&gt;While an average user might interact with AI only through the web or an app for simple conversations, power users of AI models with some technical background are experimenting with giving AI more autonomy to solve large-scale problems. OpenClaw, an open-source AI agent that recently went viral within the AI hacker world, allows AI to take over your computer—it can run 24-7, going through your emails and work tasks without supervision.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;OpenClaw, like many other open-source tools, allows users to connect to different AI models via an application programming interface, or API. Within days of OpenClaw’s release, the team revealed that Kimi’s K2.5 had surpassed Claude Opus and became the most used AI model—by token count, meaning it was handling more total text processed across user prompts and model responses.&lt;/p&gt;  &lt;p&gt;Cost has been a major reason Chinese models have gained traction, but it would be a mistake to treat them as mere “dupes” of Western frontier systems, Wang suggests. Like any product, a model only needs to be good enough for the job at hand.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;The landscape of open-source models in China is also getting more specialized. Research groups such as Shanghai AI Laboratory have released models geared toward scientific and technical tasks; several projects from Tencent have focused specifically on music generation. Ubiquant, a quantitative finance firm like DeepSeek’s parent High-Flyer, has released an open model aimed at medical reasoning.&lt;/p&gt;  &lt;p&gt;In the meantime, innovative architectural ideas from Chinese labs are being picked up more broadly. DeepSeek has published work exploring model efficiency and memory; techniques that compress the model’s attention “cache,” reducing memory and inference costs while mostly preserving performance, have drawn significant attention in the research community.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;“The impact of these research breakthroughs is amplified because they’re open-sourced and can be picked up quickly across the field,” says Wang.&lt;/p&gt;  &lt;h4 class="wp-block-heading"&gt;&lt;strong&gt;Chinese open models will become infrastructure for global AI builders&lt;/strong&gt;&lt;/h4&gt;  &lt;p&gt;The adoption of Chinese models is picking up in Silicon Valley, too. Martin Casado, a general partner at Andreessen Horowitz, has put a number on it: Among startups pitching with open-source stacks, there’s about an 80% chance they’re running on Chinese open models, according to a post he made on X. Usage data tells a similar story. OpenRouter,&amp;nbsp; a middleman that tracks how people use different AI models through its API, shows Chinese open models rising from almost none in late 2024 to nearly 30% of usage in some recent weeks.&lt;/p&gt;  &lt;p&gt;The demand is also rising globally. Z.ai limited new subscriptions to its GLM coding plan (a coding tool based on its flagship GLM models) after demand surged, citing compute constraints. What’s notable is where the demand is coming from: CNBC reports that the system’s user base is primarily concentrated in the United States and China, followed by India, Japan, Brazil, and the UK.&lt;/p&gt;  &lt;p&gt;“The open-source ecosystems in China and the US are tightly bound together,” says Wang at Hugging Face. Many Chinese open models still rely on Nvidia and US cloud platforms to train and serve them, which keeps the business ties tangled. Talent is fluid too: Researchers move across borders and companies, and many still operate as a global community, sharing code and ideas in public.&lt;/p&gt;  &lt;p&gt;That interdependence is part of what makes Chinese developers feel optimistic about this moment: The work travels, gets remixed, and actually shows up in products. But openness can also accelerate the competition. Dario Amodei, the CEO of Anthropic, made a version of this point after DeepSeek’s 2025 releases: He wrote that export controls are “not a way to duck the competition” between the US and China, and that AI companies in the US “must have better models” if they want to prevail.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For the past decade, the story of Chinese tech in the West has been one of big expectations that ran into scrutiny, restrictions, and political backlash. This time the export isn’t just an app or a consumer platform. It’s the underlying model layer that other people build on. Whether that will play out differently is still an open question.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/12/1132811/whats-next-for-chinese-open-source-ai/</guid><pubDate>Thu, 12 Feb 2026 10:00:00 +0000</pubDate></item><item><title>[NEW] Why EVs are gaining ground in Africa (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/12/1132790/evs-progress-africa/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/2WPMXM8.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;EVs are getting cheaper and more common all over the world. But the technology still faces major challenges in some markets, including many countries in Africa.&lt;/p&gt;  &lt;p&gt;Some regions across the continent still have limited grid and charging infrastructure, and those that do have widespread electricity access sometimes face reliability issues—a problem for EV owners, who require a stable electricity source to charge up and get around.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But there are some signs of progress. I just finished up a story about the economic case: A recent study in &lt;em&gt;Nature Energy&lt;/em&gt; found that EVs from scooters to minibuses could be cheaper to own than gas-powered vehicles in Africa by 2040.&lt;/p&gt;  &lt;p&gt;If there’s one thing to know about EVs in Africa, it’s that each of the 54 countries on the continent faces drastically different needs, challenges, and circumstances. There’s also a wide range of reasons to be optimistic about the prospects for EVs in the near future, including developing policies, a growing grid, and an expansion of local manufacturing.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Even the world’s leading EV markets fall short of Ethiopia’s aggressively pro-EV policies. In 2024, the country became the first in the world to ban the import of non-electric private vehicles.&lt;/p&gt;  &lt;p&gt;The case is largely an economic one: Gasoline is expensive there, and the country commissioned Africa’s largest hydropower dam in September 2025, providing a new source of cheap and abundant clean electricity. The nearly $5 billion project has a five-gigawatt capacity, doubling the grid’s peak power in the country.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Much of Ethiopia’s vehicle market is for used cars, and some drivers are still opting for older gas-powered vehicles. But this nudge could help increase the market for EVs there.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Other African countries are also pushing some drivers toward electrification. Rwanda banned new registrations for commercial gas-powered motorbikes in the capital city of Kigali last year, encouraging EVs as an alternative. These motorbike taxis can make up over half the vehicles on the city’s streets, so the move is a major turning point for transportation there.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Smaller two- and three-wheelers are a bright spot for EVs globally: In 2025, EVs made up about 45% of new sales for such vehicles. (For cars and trucks, the share was about 25%.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;And Africa’s local market is starting to really take off. There’s already some local assembly of electric two-wheelers in countries including Morocco, Kenya, and Rwanda, says Nelson Nsitem, lead Africa energy transition analyst at BloombergNEF, an energy consultancy.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Spiro, a Dubai-based electric motorbike company, recently raised $100 million in funding to expand operations in Africa. The company currently assembles its bikes in Uganda, Kenya, Nigeria, and Rwanda, and as of October it has over 60,000 bikes deployed and 1,500 battery swap stations operating.&lt;/p&gt;  &lt;p&gt;Assembly and manufacturing for larger EVs and batteries is also set to expand. Gotion High-Tech, a Chinese battery company, is currently building Africa’s first battery gigafactory. It’s a $5.6 billion project that could produce 20 gigawatt-hours of batteries annually, starting in 2026. (That’s enough for hundreds of thousands of EVs each year.)&lt;/p&gt;  &lt;p&gt;Chinese EV companies are looking to growing markets like Southeast Asia and Africa as they attempt to expand beyond an oversaturated domestic scene. BYD, the world’s largest EV company, is aggressively expanding across South Africa and plans to have as many as 70 dealerships in the country by the end of this year. That will mean more options for people in Africa looking to buy electric.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“You have very high-quality, very affordable vehicles coming onto the market that are benefiting from the economies of scale in China. These countries stand to benefit from that,” says Kelly Carlin, a manager in the program on carbon-free transportation at the Rocky Mountain Institute, an energy think tank. “It’s a game changer,” he adds.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/2WPMXM8.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;EVs are getting cheaper and more common all over the world. But the technology still faces major challenges in some markets, including many countries in Africa.&lt;/p&gt;  &lt;p&gt;Some regions across the continent still have limited grid and charging infrastructure, and those that do have widespread electricity access sometimes face reliability issues—a problem for EV owners, who require a stable electricity source to charge up and get around.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But there are some signs of progress. I just finished up a story about the economic case: A recent study in &lt;em&gt;Nature Energy&lt;/em&gt; found that EVs from scooters to minibuses could be cheaper to own than gas-powered vehicles in Africa by 2040.&lt;/p&gt;  &lt;p&gt;If there’s one thing to know about EVs in Africa, it’s that each of the 54 countries on the continent faces drastically different needs, challenges, and circumstances. There’s also a wide range of reasons to be optimistic about the prospects for EVs in the near future, including developing policies, a growing grid, and an expansion of local manufacturing.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Even the world’s leading EV markets fall short of Ethiopia’s aggressively pro-EV policies. In 2024, the country became the first in the world to ban the import of non-electric private vehicles.&lt;/p&gt;  &lt;p&gt;The case is largely an economic one: Gasoline is expensive there, and the country commissioned Africa’s largest hydropower dam in September 2025, providing a new source of cheap and abundant clean electricity. The nearly $5 billion project has a five-gigawatt capacity, doubling the grid’s peak power in the country.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Much of Ethiopia’s vehicle market is for used cars, and some drivers are still opting for older gas-powered vehicles. But this nudge could help increase the market for EVs there.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Other African countries are also pushing some drivers toward electrification. Rwanda banned new registrations for commercial gas-powered motorbikes in the capital city of Kigali last year, encouraging EVs as an alternative. These motorbike taxis can make up over half the vehicles on the city’s streets, so the move is a major turning point for transportation there.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Smaller two- and three-wheelers are a bright spot for EVs globally: In 2025, EVs made up about 45% of new sales for such vehicles. (For cars and trucks, the share was about 25%.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;And Africa’s local market is starting to really take off. There’s already some local assembly of electric two-wheelers in countries including Morocco, Kenya, and Rwanda, says Nelson Nsitem, lead Africa energy transition analyst at BloombergNEF, an energy consultancy.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Spiro, a Dubai-based electric motorbike company, recently raised $100 million in funding to expand operations in Africa. The company currently assembles its bikes in Uganda, Kenya, Nigeria, and Rwanda, and as of October it has over 60,000 bikes deployed and 1,500 battery swap stations operating.&lt;/p&gt;  &lt;p&gt;Assembly and manufacturing for larger EVs and batteries is also set to expand. Gotion High-Tech, a Chinese battery company, is currently building Africa’s first battery gigafactory. It’s a $5.6 billion project that could produce 20 gigawatt-hours of batteries annually, starting in 2026. (That’s enough for hundreds of thousands of EVs each year.)&lt;/p&gt;  &lt;p&gt;Chinese EV companies are looking to growing markets like Southeast Asia and Africa as they attempt to expand beyond an oversaturated domestic scene. BYD, the world’s largest EV company, is aggressively expanding across South Africa and plans to have as many as 70 dealerships in the country by the end of this year. That will mean more options for people in Africa looking to buy electric.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“You have very high-quality, very affordable vehicles coming onto the market that are benefiting from the economies of scale in China. These countries stand to benefit from that,” says Kelly Carlin, a manager in the program on carbon-free transportation at the Rocky Mountain Institute, an energy think tank. “It’s a game changer,” he adds.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/12/1132790/evs-progress-africa/</guid><pubDate>Thu, 12 Feb 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] AI is already making online crimes easier. It could get much worse. (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/12/1132386/ai-already-making-online-swindles-easier/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Anton Cherepanov is always on the lookout for something interesting. And in late August last year, he spotted just that. It was a file uploaded to VirusTotal, a site cybersecurity researchers like him use to analyze submissions for potential viruses and other types of malicious software, often known as malware. On the surface it seemed innocuous, but it triggered Cherepanov’s custom malware-detecting measures. Over the next few hours, he and his colleague Peter Strýček inspected the sample and realized they’d never come across anything like it before.&lt;/p&gt;  &lt;p&gt;The file contained ransomware, a nasty strain of malware that encrypts the files it comes across on a victim’s system, rendering them unusable until a ransom is paid to the attackers behind it. But what set this example apart was that it employed large language models (LLMs). Not just incidentally, but across every stage of an attack. Once it was installed, it could tap into an LLM to generate customized code in real time, rapidly map a computer to identify sensitive data to copy or encrypt, and write personalized ransom notes based on the files’ content. The software could do this autonomously, without any human intervention. And every time it ran, it would act differently, making it harder to detect.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Cherepanov and Strýček were confident that their discovery, which they dubbed PromptLock, marked a turning point in generative AI, showing how the technology could be exploited to create highly flexible malware attacks. They published a blog post declaring that they’d uncovered the first example of AI-powered ransomware, which quickly became the object of widespread global media attention.&lt;/p&gt;  &lt;p&gt;But the threat wasn’t quite as dramatic as it first appeared. The day after the blog post went live, a team of researchers from New York University claimed responsibility, explaining that the malware was not, in fact, a full attack let loose in the wild but a research project, merely designed to prove it was &lt;em&gt;possible&lt;/em&gt; to automate each step of a ransomware campaign—which, they said, they had.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;PromptLock may have turned out to be an academic project, but the real bad guys &lt;em&gt;are&lt;/em&gt; using the latest AI tools. Just as software engineers are using artificial intelligence to help write code and check for bugs, hackers are using these tools to reduce the time and effort required to orchestrate an attack, lowering the barriers for less experienced attackers to try something out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The likelihood that cyberattacks will now become more common and more effective over time is not a remote possibility but “a sheer reality,” says Lorenzo Cavallaro, a professor of computer science at University College London.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Some in Silicon Valley warn that AI is on the brink of being able to carry out fully automated attacks. But most security researchers say this claim is overblown. “For some reason, everyone is just focused on this malware idea of, like, AI superhackers, which is just absurd,” says Marcus Hutchins, who is principal threat researcher at the security company Expel and famous in the security world for ending a giant global ransomware attack called WannaCry in 2017.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Instead, experts argue, we should be paying closer attention to the much more immediate risks posed by AI, which is already speeding up and increasing the volume of scams. Criminals are increasingly exploiting the latest deepfake technologies to impersonate people and swindle victims out of vast sums of money. These AI-enhanced cyberattacks are only set to get more frequent and more destructive, and we need to be ready.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Spam and beyond&lt;/h3&gt;  &lt;p&gt;Attackers started adopting generative AI tools almost immediately after ChatGPT exploded on the scene at the end of 2022. These efforts began, as you might imagine, with the creation of spam—and a lot of it. Last year, a report from Microsoft said that in the year leading up to April 2025, the company had blocked $4 billion worth of scams and fraudulent transactions, “many likely aided by AI content.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At least half of spam email is now generated using LLMs, according to estimates by researchers at Columbia University, the University of Chicago, and Barracuda Networks, who analyzed nearly 500,000 malicious messages collected before and after the launch of ChatGPT. They also found evidence that AI is increasingly being deployed in more sophisticated schemes. They looked at targeted email attacks, which impersonate a trusted figure in order to trick a worker within an organization out of funds or sensitive information. By April 2025, they found, at least 14% of those sorts of focused email attacks were generated using LLMs, up from 7.6% in April 2024.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;In one high-profile case, a worker was tricked into transferring $25 million to criminals via a video call with digital versions of the company’s chief financial officer and other employees.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;And the generative AI boom has made it easier and cheaper than ever before to generate not only emails but highly convincing images, videos, and audio. The results are much more realistic than even just a few short years ago, and it takes much less data to generate a fake version of someone’s likeness or voice than it used to.&lt;/p&gt;  &lt;p&gt;Criminals aren’t deploying these sorts of deepfakes to prank people or to simply mess around—they’re doing it because it works and because they’re making money out of it, says Henry Ajder, a generative AI expert. “If there’s money to be made and people continue to be fooled by it, they’ll continue to do it,” he says. In one high-­profile case reported in 2024, a worker at the British engineering firm Arup was tricked into transferring $25 million to criminals via a video call with digital versions of the company’s chief financial officer and other employees. That’s likely only the tip of the iceberg, and the problem posed by convincing deepfakes is only likely to get worse as the technology improves and is more widely adopted.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="person sitting in profile at a computer with an enormous mask in front of them and words spooling out through the frame" class="wp-image-1132552" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/mask-2.jpg?w=1993" width="1993" /&gt;&lt;div class="image-credit"&gt;BRIAN STAUFFER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Criminals’ tactics evolve all the time, and as AI’s capabilities improve, such people are constantly probing how those new capabilities can help them gain an advantage over victims. Billy Leonard, tech leader of Google’s Threat Analysis Group, has been keeping a close eye on changes in the use of AI by potential bad actors (a widely used term in the industry for hackers and others attempting to use computers for criminal purposes). In the latter half of 2024, he and his team noticed prospective criminals using tools like Google Gemini the same way everyday users do—to debug code and automate bits and pieces of their work—as well as tasking it with writing the odd phishing email. By 2025, they had progressed to using AI to help create new pieces of malware and release them into the wild, he says.&lt;/p&gt;  &lt;p&gt;The big question now is how far this kind of malware can go. Will it ever become capable enough to sneakily infiltrate thousands of companies’ systems and extract millions of dollars, completely undetected?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Most popular AI models have guardrails in place to prevent them from generating malicious code or illegal material, but bad actors still find ways to work around them. For example, Google observed a China-linked actor asking its Gemini AI model to identify vulnerabilities on a compromised system—a request it initially refused on safety grounds. However, the attacker managed to persuade Gemini to break its own rules by posing as a participant in a capture-the-flag competition, a popular cybersecurity game. This sneaky form of jailbreaking led Gemini to hand over information that could have been used to exploit the system. (Google has since adjusted Gemini to deny these kinds of requests.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;But bad actors aren’t just focusing on trying to bend the AI giants’ models to their nefarious ends. Going forward, they’re increasingly likely to adopt open-source AI models, as it’s easier to strip out their safeguards and get them to do malicious things, says Ashley Jess, a former tactical specialist at the US Department of Justice and now a senior intelligence analyst at the cybersecurity company Intel 471. “Those are the ones I think that [bad] actors are going to adopt, because they can jailbreak them and tailor them to what they need,” she says.&lt;/p&gt;  &lt;p&gt;The NYU team used two open-source models from OpenAI in its PromptLock experiment, and the researchers found they didn’t even need to resort to jailbreaking techniques to get the model to do what they wanted. They say that makes attacks much easier. Although these kinds of open-source models are designed with an eye to ethical alignment, meaning that their makers do consider certain goals and values in dictating the way they respond to requests, the models don’t have the same kinds of restrictions as their closed-source counterparts, says Meet Udeshi, a PhD student at New York University who worked on the project. “That is what we were trying to test,” he says. “These LLMs claim that they are ethically aligned—can we still misuse them for these purposes? And the answer turned out to be yes.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s possible that criminals have already successfully pulled off covert PromptLock-style attacks and we’ve simply never seen any evidence of them, says Udeshi. If that’s the case, attackers could—in theory—have created a fully autonomous hacking system. But to do that they would have had to overcome the significant barrier that is getting AI models to behave reliably, as well as any inbuilt aversion the models have to being used for malicious purposes—all while evading detection. Which is a pretty high bar indeed.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;h3 class="wp-block-heading"&gt;Productivity tools for hackers&lt;/h3&gt;  &lt;p&gt;So, what do we know for sure? Some of the best data we have now on how people are attempting to use AI for malicious purposes comes from the big AI companies themselves. And their findings certainly sound alarming, at least at first. In November, Leonard’s team at Google released a report that found bad actors were using AI tools (including Google’s Gemini) to dynamically alter malware’s behavior; for example, it could self-modify to evade detection. The team wrote that it ushered in “a new operational phase of AI abuse.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt; &lt;p&gt;However, the five malware families the report dug into (including PromptLock) consisted of code that was easily detected and didn’t actually do any harm, the cybersecurity writer Kevin Beaumont pointed out on social media. “There’s nothing in the report to suggest orgs need to deviate from foundational security programmes—everything worked as it should,” he wrote.&lt;/p&gt;  &lt;p&gt;It’s true that this malware activity is in an early phase, concedes Leonard. Still, he sees value in making these kinds of reports public if it helps security vendors and others build better defenses to prevent more dangerous AI attacks further down the line. “Cliché to say, but sunlight is the best disinfectant,” he says. “It doesn’t really do us any good to keep it a secret or keep it hidden away. We want people to be able to know about this— we want other security vendors to know about this—so that they can continue to build their own detections.”&lt;/p&gt;  &lt;p&gt;And it’s not just new strains of malware that would-be attackers are experimenting with—they also seem to be using AI to try to automate the process of hacking targets. In November, Anthropic announced it had disrupted a large-scale cyberattack, the first reported case of one executed without “substantial human intervention.” Although the company didn’t go into much detail about the exact tactics the hackers used, the report’s authors said a Chinese state-sponsored group had used its Claude Code assistant to automate up to 90% of what they called a “highly sophisticated espionage campaign.”&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“We’re entering an era where the barrier to sophisticated cyber operations has fundamentally lowered, and the pace of attacks will accelerate faster than many organizations are prepared for.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Jacob Klein, head of threat intelligence at Anthropic&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;But, as with the Google findings, there were caveats. A human operator, not AI, selected the targets before tasking Claude with identifying vulnerabilities. And of 30 attempts, only a “handful” were successful. The Anthropic report also found that Claude hallucinated and ended up fabricating data during the campaign, claiming it had obtained credentials it hadn’t and “frequently” overstating its findings, so the attackers would have had to carefully validate those results to make sure they were actually true. “This remains an obstacle to fully autonomous cyberattacks,” the report’s authors wrote.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Existing controls within any reasonably secure organization would stop these attacks, says Gary McGraw, a veteran security expert and cofounder of the Berryville Institute of Machine Learning in Virginia. “None of the malicious-attack part, like the vulnerability exploit … was actually done by the AI—it was just prefabricated tools that do that, and that stuff’s been automated for 20 years,” he says. “There’s nothing novel, creative, or interesting about this attack.”&lt;/p&gt; 
 &lt;p&gt;Anthropic maintains that the report’s findings are a concerning signal of changes ahead. “Tying this many steps of an intrusion campaign together through [AI] agentic orchestration is unprecedented,” Jacob Klein, head of threat intelligence at Anthropic, said in a statement. “It turns what has always been a labor-intensive process into something far more scalable. We’re entering an era where the barrier to sophisticated cyber operations has fundamentally lowered, and the pace of attacks will accelerate faster than many organizations are prepared for.”&lt;/p&gt;  &lt;p&gt;Some are not convinced there’s reason to be alarmed. AI hype has led a lot of people in the cybersecurity industry to overestimate models’ current abilities, Hutchins says. “They want this idea of unstoppable AIs that can outmaneuver security, so they’re forecasting that’s where we’re going,” he says. But “there just isn’t any evidence to support that, because the AI capabilities just don’t meet any of the requirements.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_13"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="person kneeling warding off an attack of arrows under a sheild" class="wp-image-1132553" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/shield-2.jpg?w=1993" width="1993" /&gt;&lt;div class="image-credit"&gt;BRIAN STAUFFER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Indeed, for now criminals mostly seem to be tapping AI to enhance their productivity: using LLMs to write malicious code and phishing lures, to conduct reconnaissance, and for language translation. Jess sees this kind of activity a lot, alongside efforts to sell tools in underground criminal markets. For example, there are phishing kits that compare the click-rate success of various spam campaigns, so criminals can track which campaigns are most effective at any given time. She is seeing a lot of this activity in what could be called the “AI slop landscape” but not as much “widespread adoption from highly technical actors,” she says.&lt;/p&gt;  &lt;p&gt;But attacks don’t need to be sophisticated to be effective. Models that produce “good enough” results allow attackers to go after larger numbers of people than previously possible, says Liz James, a managing security consultant at the cybersecurity company NCC Group. “We’re talking about someone who might be using a scattergun approach phishing a whole bunch of people with a model that, if it lands itself on a machine of interest that doesn’t have any defenses … can reasonably competently encrypt your hard drive,” she says. “You’ve achieved your objective.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;On the defense&lt;/h3&gt;  &lt;p&gt;For now, researchers are optimistic about our ability to defend against these threats—regardless of whether they are made with AI. “Especially on the malware side, a lot of the defenses and the capabilities and the best practices that we’ve recommended for the past 10-plus years—they all still apply,” says Leonard. The security programs we use to detect standard viruses and attack attempts work; a lot of phishing emails will still get caught in inbox spam filters, for example. These traditional forms of defense will still largely get the job done—at least for now.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And in a neat twist, AI itself is helping to counter security threats more effectively. After all, it is excellent at spotting patterns and correlations. Vasu Jakkal, corporate vice president of Microsoft Security, says that every day, the company processes more than 100 trillion signals flagged by its AI systems as potentially malicious or suspicious events.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_15"&gt;&lt;p&gt;Despite the cybersecurity landscape’s constant state of flux, Jess is heartened by how readily defenders are sharing detailed information with each other about attackers’ tactics. Mitre’s Adversarial Threat Landscape for Artificial-Intelligence Systems and the GenAI Security Project from the Open Worldwide Application Security Project are two helpful initiatives documenting how potential criminals are incorporating AI into their attacks and how AI systems are being targeted by them. “We’ve got some really good resources out there for understanding how to protect your own internal AI toolings and understand the threat from AI toolings in the hands of cybercriminals,” she says.&lt;/p&gt;  &lt;p&gt;PromptLock, the result of a limited university project, isn’t representative of how an attack would play out in the real world. But if it taught us anything, it’s that the technical capabilities of AI shouldn’t be dismissed.New York University’s Udeshi says he wastaken aback at how easily AI was able to handle a full end-to-end chain of attack, from mapping and working out how to break into a targeted computer system to writing personalized ransom notes to victims: “We expected it would do the initial task very well but it would stumble later on, but we saw high—80% to 90%—success throughout the whole pipeline.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI is still evolving rapidly, and today’s systems are already capable of things that would have seemed preposterously out of reach just a few short years ago. That makes it incredibly tough to say with absolute confidence what it will—or won’t—be able to achieve in the future. While researchers are certain that AI-driven attacks are likely to increase in both volume and severity, the forms they could take are unclear. Perhaps the most extreme possibility is that someone makes an AI model capable of creating and automating its own zero-day exploits—highly dangerous cyber­attacks that take advantage of previously unknown vulnerabilities in software. But building and hosting such a model—and evading detection—would require billions of dollars in investment, says Hutchins, meaning it would only be in the reach of a wealthy nation-state.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Engin Kirda, a professor at Northeastern University in Boston who specializes in malware detection and analysis, says he wouldn’t be surprised if this was already happening. “I’m sure people are investing in it, but I’m also pretty sure people are already doing it, especially [in] China—they have good AI capabilities,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s a pretty scary possibility. But it’s one that—thankfully—is still only theoretical. A large-scale campaign that is both effective and clearly AI-driven has yet to materialize. What we can say is that generative AI is already significantly lowering the bar for criminals. They’ll keep experimenting with the newest releases and updates and trying to find new ways to trick us into parting with important information and precious cash. For now, all we can do is be careful, remain vigilant, and—for all our sakes—stay on top of those system updates.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Anton Cherepanov is always on the lookout for something interesting. And in late August last year, he spotted just that. It was a file uploaded to VirusTotal, a site cybersecurity researchers like him use to analyze submissions for potential viruses and other types of malicious software, often known as malware. On the surface it seemed innocuous, but it triggered Cherepanov’s custom malware-detecting measures. Over the next few hours, he and his colleague Peter Strýček inspected the sample and realized they’d never come across anything like it before.&lt;/p&gt;  &lt;p&gt;The file contained ransomware, a nasty strain of malware that encrypts the files it comes across on a victim’s system, rendering them unusable until a ransom is paid to the attackers behind it. But what set this example apart was that it employed large language models (LLMs). Not just incidentally, but across every stage of an attack. Once it was installed, it could tap into an LLM to generate customized code in real time, rapidly map a computer to identify sensitive data to copy or encrypt, and write personalized ransom notes based on the files’ content. The software could do this autonomously, without any human intervention. And every time it ran, it would act differently, making it harder to detect.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Cherepanov and Strýček were confident that their discovery, which they dubbed PromptLock, marked a turning point in generative AI, showing how the technology could be exploited to create highly flexible malware attacks. They published a blog post declaring that they’d uncovered the first example of AI-powered ransomware, which quickly became the object of widespread global media attention.&lt;/p&gt;  &lt;p&gt;But the threat wasn’t quite as dramatic as it first appeared. The day after the blog post went live, a team of researchers from New York University claimed responsibility, explaining that the malware was not, in fact, a full attack let loose in the wild but a research project, merely designed to prove it was &lt;em&gt;possible&lt;/em&gt; to automate each step of a ransomware campaign—which, they said, they had.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;PromptLock may have turned out to be an academic project, but the real bad guys &lt;em&gt;are&lt;/em&gt; using the latest AI tools. Just as software engineers are using artificial intelligence to help write code and check for bugs, hackers are using these tools to reduce the time and effort required to orchestrate an attack, lowering the barriers for less experienced attackers to try something out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The likelihood that cyberattacks will now become more common and more effective over time is not a remote possibility but “a sheer reality,” says Lorenzo Cavallaro, a professor of computer science at University College London.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Some in Silicon Valley warn that AI is on the brink of being able to carry out fully automated attacks. But most security researchers say this claim is overblown. “For some reason, everyone is just focused on this malware idea of, like, AI superhackers, which is just absurd,” says Marcus Hutchins, who is principal threat researcher at the security company Expel and famous in the security world for ending a giant global ransomware attack called WannaCry in 2017.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Instead, experts argue, we should be paying closer attention to the much more immediate risks posed by AI, which is already speeding up and increasing the volume of scams. Criminals are increasingly exploiting the latest deepfake technologies to impersonate people and swindle victims out of vast sums of money. These AI-enhanced cyberattacks are only set to get more frequent and more destructive, and we need to be ready.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Spam and beyond&lt;/h3&gt;  &lt;p&gt;Attackers started adopting generative AI tools almost immediately after ChatGPT exploded on the scene at the end of 2022. These efforts began, as you might imagine, with the creation of spam—and a lot of it. Last year, a report from Microsoft said that in the year leading up to April 2025, the company had blocked $4 billion worth of scams and fraudulent transactions, “many likely aided by AI content.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At least half of spam email is now generated using LLMs, according to estimates by researchers at Columbia University, the University of Chicago, and Barracuda Networks, who analyzed nearly 500,000 malicious messages collected before and after the launch of ChatGPT. They also found evidence that AI is increasingly being deployed in more sophisticated schemes. They looked at targeted email attacks, which impersonate a trusted figure in order to trick a worker within an organization out of funds or sensitive information. By April 2025, they found, at least 14% of those sorts of focused email attacks were generated using LLMs, up from 7.6% in April 2024.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;In one high-profile case, a worker was tricked into transferring $25 million to criminals via a video call with digital versions of the company’s chief financial officer and other employees.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;And the generative AI boom has made it easier and cheaper than ever before to generate not only emails but highly convincing images, videos, and audio. The results are much more realistic than even just a few short years ago, and it takes much less data to generate a fake version of someone’s likeness or voice than it used to.&lt;/p&gt;  &lt;p&gt;Criminals aren’t deploying these sorts of deepfakes to prank people or to simply mess around—they’re doing it because it works and because they’re making money out of it, says Henry Ajder, a generative AI expert. “If there’s money to be made and people continue to be fooled by it, they’ll continue to do it,” he says. In one high-­profile case reported in 2024, a worker at the British engineering firm Arup was tricked into transferring $25 million to criminals via a video call with digital versions of the company’s chief financial officer and other employees. That’s likely only the tip of the iceberg, and the problem posed by convincing deepfakes is only likely to get worse as the technology improves and is more widely adopted.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="person sitting in profile at a computer with an enormous mask in front of them and words spooling out through the frame" class="wp-image-1132552" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/mask-2.jpg?w=1993" width="1993" /&gt;&lt;div class="image-credit"&gt;BRIAN STAUFFER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Criminals’ tactics evolve all the time, and as AI’s capabilities improve, such people are constantly probing how those new capabilities can help them gain an advantage over victims. Billy Leonard, tech leader of Google’s Threat Analysis Group, has been keeping a close eye on changes in the use of AI by potential bad actors (a widely used term in the industry for hackers and others attempting to use computers for criminal purposes). In the latter half of 2024, he and his team noticed prospective criminals using tools like Google Gemini the same way everyday users do—to debug code and automate bits and pieces of their work—as well as tasking it with writing the odd phishing email. By 2025, they had progressed to using AI to help create new pieces of malware and release them into the wild, he says.&lt;/p&gt;  &lt;p&gt;The big question now is how far this kind of malware can go. Will it ever become capable enough to sneakily infiltrate thousands of companies’ systems and extract millions of dollars, completely undetected?&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Most popular AI models have guardrails in place to prevent them from generating malicious code or illegal material, but bad actors still find ways to work around them. For example, Google observed a China-linked actor asking its Gemini AI model to identify vulnerabilities on a compromised system—a request it initially refused on safety grounds. However, the attacker managed to persuade Gemini to break its own rules by posing as a participant in a capture-the-flag competition, a popular cybersecurity game. This sneaky form of jailbreaking led Gemini to hand over information that could have been used to exploit the system. (Google has since adjusted Gemini to deny these kinds of requests.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;But bad actors aren’t just focusing on trying to bend the AI giants’ models to their nefarious ends. Going forward, they’re increasingly likely to adopt open-source AI models, as it’s easier to strip out their safeguards and get them to do malicious things, says Ashley Jess, a former tactical specialist at the US Department of Justice and now a senior intelligence analyst at the cybersecurity company Intel 471. “Those are the ones I think that [bad] actors are going to adopt, because they can jailbreak them and tailor them to what they need,” she says.&lt;/p&gt;  &lt;p&gt;The NYU team used two open-source models from OpenAI in its PromptLock experiment, and the researchers found they didn’t even need to resort to jailbreaking techniques to get the model to do what they wanted. They say that makes attacks much easier. Although these kinds of open-source models are designed with an eye to ethical alignment, meaning that their makers do consider certain goals and values in dictating the way they respond to requests, the models don’t have the same kinds of restrictions as their closed-source counterparts, says Meet Udeshi, a PhD student at New York University who worked on the project. “That is what we were trying to test,” he says. “These LLMs claim that they are ethically aligned—can we still misuse them for these purposes? And the answer turned out to be yes.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s possible that criminals have already successfully pulled off covert PromptLock-style attacks and we’ve simply never seen any evidence of them, says Udeshi. If that’s the case, attackers could—in theory—have created a fully autonomous hacking system. But to do that they would have had to overcome the significant barrier that is getting AI models to behave reliably, as well as any inbuilt aversion the models have to being used for malicious purposes—all while evading detection. Which is a pretty high bar indeed.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;h3 class="wp-block-heading"&gt;Productivity tools for hackers&lt;/h3&gt;  &lt;p&gt;So, what do we know for sure? Some of the best data we have now on how people are attempting to use AI for malicious purposes comes from the big AI companies themselves. And their findings certainly sound alarming, at least at first. In November, Leonard’s team at Google released a report that found bad actors were using AI tools (including Google’s Gemini) to dynamically alter malware’s behavior; for example, it could self-modify to evade detection. The team wrote that it ushered in “a new operational phase of AI abuse.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt; &lt;p&gt;However, the five malware families the report dug into (including PromptLock) consisted of code that was easily detected and didn’t actually do any harm, the cybersecurity writer Kevin Beaumont pointed out on social media. “There’s nothing in the report to suggest orgs need to deviate from foundational security programmes—everything worked as it should,” he wrote.&lt;/p&gt;  &lt;p&gt;It’s true that this malware activity is in an early phase, concedes Leonard. Still, he sees value in making these kinds of reports public if it helps security vendors and others build better defenses to prevent more dangerous AI attacks further down the line. “Cliché to say, but sunlight is the best disinfectant,” he says. “It doesn’t really do us any good to keep it a secret or keep it hidden away. We want people to be able to know about this— we want other security vendors to know about this—so that they can continue to build their own detections.”&lt;/p&gt;  &lt;p&gt;And it’s not just new strains of malware that would-be attackers are experimenting with—they also seem to be using AI to try to automate the process of hacking targets. In November, Anthropic announced it had disrupted a large-scale cyberattack, the first reported case of one executed without “substantial human intervention.” Although the company didn’t go into much detail about the exact tactics the hackers used, the report’s authors said a Chinese state-sponsored group had used its Claude Code assistant to automate up to 90% of what they called a “highly sophisticated espionage campaign.”&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“We’re entering an era where the barrier to sophisticated cyber operations has fundamentally lowered, and the pace of attacks will accelerate faster than many organizations are prepared for.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Jacob Klein, head of threat intelligence at Anthropic&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;But, as with the Google findings, there were caveats. A human operator, not AI, selected the targets before tasking Claude with identifying vulnerabilities. And of 30 attempts, only a “handful” were successful. The Anthropic report also found that Claude hallucinated and ended up fabricating data during the campaign, claiming it had obtained credentials it hadn’t and “frequently” overstating its findings, so the attackers would have had to carefully validate those results to make sure they were actually true. “This remains an obstacle to fully autonomous cyberattacks,” the report’s authors wrote.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Existing controls within any reasonably secure organization would stop these attacks, says Gary McGraw, a veteran security expert and cofounder of the Berryville Institute of Machine Learning in Virginia. “None of the malicious-attack part, like the vulnerability exploit … was actually done by the AI—it was just prefabricated tools that do that, and that stuff’s been automated for 20 years,” he says. “There’s nothing novel, creative, or interesting about this attack.”&lt;/p&gt; 
 &lt;p&gt;Anthropic maintains that the report’s findings are a concerning signal of changes ahead. “Tying this many steps of an intrusion campaign together through [AI] agentic orchestration is unprecedented,” Jacob Klein, head of threat intelligence at Anthropic, said in a statement. “It turns what has always been a labor-intensive process into something far more scalable. We’re entering an era where the barrier to sophisticated cyber operations has fundamentally lowered, and the pace of attacks will accelerate faster than many organizations are prepared for.”&lt;/p&gt;  &lt;p&gt;Some are not convinced there’s reason to be alarmed. AI hype has led a lot of people in the cybersecurity industry to overestimate models’ current abilities, Hutchins says. “They want this idea of unstoppable AIs that can outmaneuver security, so they’re forecasting that’s where we’re going,” he says. But “there just isn’t any evidence to support that, because the AI capabilities just don’t meet any of the requirements.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_13"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="person kneeling warding off an attack of arrows under a sheild" class="wp-image-1132553" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/shield-2.jpg?w=1993" width="1993" /&gt;&lt;div class="image-credit"&gt;BRIAN STAUFFER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Indeed, for now criminals mostly seem to be tapping AI to enhance their productivity: using LLMs to write malicious code and phishing lures, to conduct reconnaissance, and for language translation. Jess sees this kind of activity a lot, alongside efforts to sell tools in underground criminal markets. For example, there are phishing kits that compare the click-rate success of various spam campaigns, so criminals can track which campaigns are most effective at any given time. She is seeing a lot of this activity in what could be called the “AI slop landscape” but not as much “widespread adoption from highly technical actors,” she says.&lt;/p&gt;  &lt;p&gt;But attacks don’t need to be sophisticated to be effective. Models that produce “good enough” results allow attackers to go after larger numbers of people than previously possible, says Liz James, a managing security consultant at the cybersecurity company NCC Group. “We’re talking about someone who might be using a scattergun approach phishing a whole bunch of people with a model that, if it lands itself on a machine of interest that doesn’t have any defenses … can reasonably competently encrypt your hard drive,” she says. “You’ve achieved your objective.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;On the defense&lt;/h3&gt;  &lt;p&gt;For now, researchers are optimistic about our ability to defend against these threats—regardless of whether they are made with AI. “Especially on the malware side, a lot of the defenses and the capabilities and the best practices that we’ve recommended for the past 10-plus years—they all still apply,” says Leonard. The security programs we use to detect standard viruses and attack attempts work; a lot of phishing emails will still get caught in inbox spam filters, for example. These traditional forms of defense will still largely get the job done—at least for now.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And in a neat twist, AI itself is helping to counter security threats more effectively. After all, it is excellent at spotting patterns and correlations. Vasu Jakkal, corporate vice president of Microsoft Security, says that every day, the company processes more than 100 trillion signals flagged by its AI systems as potentially malicious or suspicious events.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_15"&gt;&lt;p&gt;Despite the cybersecurity landscape’s constant state of flux, Jess is heartened by how readily defenders are sharing detailed information with each other about attackers’ tactics. Mitre’s Adversarial Threat Landscape for Artificial-Intelligence Systems and the GenAI Security Project from the Open Worldwide Application Security Project are two helpful initiatives documenting how potential criminals are incorporating AI into their attacks and how AI systems are being targeted by them. “We’ve got some really good resources out there for understanding how to protect your own internal AI toolings and understand the threat from AI toolings in the hands of cybercriminals,” she says.&lt;/p&gt;  &lt;p&gt;PromptLock, the result of a limited university project, isn’t representative of how an attack would play out in the real world. But if it taught us anything, it’s that the technical capabilities of AI shouldn’t be dismissed.New York University’s Udeshi says he wastaken aback at how easily AI was able to handle a full end-to-end chain of attack, from mapping and working out how to break into a targeted computer system to writing personalized ransom notes to victims: “We expected it would do the initial task very well but it would stumble later on, but we saw high—80% to 90%—success throughout the whole pipeline.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI is still evolving rapidly, and today’s systems are already capable of things that would have seemed preposterously out of reach just a few short years ago. That makes it incredibly tough to say with absolute confidence what it will—or won’t—be able to achieve in the future. While researchers are certain that AI-driven attacks are likely to increase in both volume and severity, the forms they could take are unclear. Perhaps the most extreme possibility is that someone makes an AI model capable of creating and automating its own zero-day exploits—highly dangerous cyber­attacks that take advantage of previously unknown vulnerabilities in software. But building and hosting such a model—and evading detection—would require billions of dollars in investment, says Hutchins, meaning it would only be in the reach of a wealthy nation-state.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Engin Kirda, a professor at Northeastern University in Boston who specializes in malware detection and analysis, says he wouldn’t be surprised if this was already happening. “I’m sure people are investing in it, but I’m also pretty sure people are already doing it, especially [in] China—they have good AI capabilities,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s a pretty scary possibility. But it’s one that—thankfully—is still only theoretical. A large-scale campaign that is both effective and clearly AI-driven has yet to materialize. What we can say is that generative AI is already significantly lowering the bar for criminals. They’ll keep experimenting with the newest releases and updates and trying to find new ways to trick us into parting with important information and precious cash. For now, all we can do is be careful, remain vigilant, and—for all our sakes—stay on top of those system updates.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/12/1132386/ai-already-making-online-swindles-easier/</guid><pubDate>Thu, 12 Feb 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] We let Chrome's Auto Browse agent surf the web for us—here's what happened (AI - Ars Technica)</title><link>https://arstechnica.com/google/2026/02/tested-how-chromes-auto-browse-agent-handles-common-web-tasks/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Auto Browse is capable of some impressive things, but it can also crash and burn spectacularly.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="wind-up chrome icons" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/google-automate-windups.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Is Chrome's AI agent ready? 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;We are now a few years into the AI revolution, and talk has shifted from who has the best chatbot to whose AI agent can do the most things on your behalf. Unfortunately, AI agents are still rough around the edges, so tasking them with anything important is not a great idea. OpenAI launched its Atlas agent late last year, which we found to be modestly useful, and now it’s Google’s turn.&lt;/p&gt;
&lt;p&gt;Unlike the OpenAI agent, Google’s new Auto Browse agent has extraordinary reach because it’s part of Chrome, the world’s most popular browser by a wide margin. Google began rolling out Auto Browse (in preview) earlier this month to AI Pro and AI Ultra subscribers, allowing them to send the agent across the web to complete tasks.&lt;/p&gt;
&lt;p&gt;I’ve taken Chrome’s agent for a spin to see whether you can trust it to handle tedious online work for you. For each test, I lay out the problem I need to solve, how I prompted the robot, and how well (or not) it handled the job.&lt;/p&gt;
&lt;h2&gt;Playing a web game&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: I want to get a high score on 2,048 without playing it myself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to [website], and play the game until you run out of moves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Unfortunately, Auto Browse can’t use arrow keys. Google says they’re not necessary for productivity tasks. So I pointed the robot at a version of the game with on-screen controls. With access to those arrows, Auto Browse had no trouble playing the game, and it seemed to grasp the rules, which are listed on the page.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2140368 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="2048 game board" class="none large" height="725" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/2048-chrome.png" width="641" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse can’t use arrows, but it can still play the game.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;On a few occasions, Auto Browse appeared to ruminate on its next move for 20 to 30 seconds, and it took the prompt very literally. The robot stopped when it could not successfully merge any tiles (its interpretation of “out of moves”) even though there were still empty spaces on the board. A human player would have taken the hit and set up a merge in the next move, but the robot had to be prompted to continue, which it did. The task ran for about 20 minutes, during which the robot created a 128 tile and made 149 moves.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 8/10. The game performance is not quite as good as Atlas, but Auto Browse didn’t require as much coaxing, and I understand why it stopped when it did. While the lack of arrow keys seems like an odd omission, there probably aren’t many productivity tasks where they are necessary.&lt;/p&gt;
&lt;h2&gt;Creating a radio playlist&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: I want to turn the music from Minnesota Public Radio’s The Current into an on-demand YouTube Music playlist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to thecurrent.org and start the live stream. Listen for one hour and make note of each song that is played. Then, add those songs to a new YouTube Music playlist.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Agents are expensive to run, so I was unsurprised that, like OpenAI’s agent mode, Auto Browse refuses to monitor a page for basically any length of time. Sometimes it will sit on the page for a minute or two, usually pretending that more time has passed before giving up.&lt;/p&gt;
&lt;p&gt;Luckily, we can accomplish this with The Current’s playlist view, which lists previous songs. I tweaked the prompt to allow Auto Browse to just get the song names from that page for the last hour, which worked fine. It interpreted that as the current hour-long block of the page, though, which was not yet complete.&lt;/p&gt;
&lt;p&gt;I thought I was doing Auto Browse a favor by using YouTube Music instead of Spotify, but it turns out that Auto Browse doesn’t understand YouTube’s design aesthetic. It failed to add any songs to the playlist because it couldn’t find the buttons. When I changed the prompt to use Spotify, Auto Browse got it done on the first try. This is as much an indictment of YouTube Music as it is Auto Browse.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2140364 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Spotify playlist" class="fullwidth full" height="481" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Playlist.png" width="670" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse was able to get recent songs, but it can’t monitor the live broadcast over time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 6/10. It seems like monitoring pages over time is just beyond the reach of current browser agents, but it’s shocking that Auto Browse couldn’t use Google’s own streaming music service to create a playlist. That said, the agent did complete the job immediately once I understood why it failed. It loses some points for making me adapt the prompt multiple times.&lt;/p&gt;
&lt;h2&gt;Scanning emails&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: My personal email address is known, and PR people often use that instead of my work address. I need to make sure no one important is sending pitches there, so I want a list of recent PR emails, contact info, and company details from my Gmail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Look through all my Gmail from the last month. Collect all the information (name, email address, phone number, product, etc.) from PR emails and add them to a new Google Sheets spreadsheet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Interestingly, Google’s agent doesn’t have to use the Gmail web interface. It can collect this data in the background using a Gmail tool. However, this also means you won’t be able to automate email tasks for accounts with Google AI disabled (like a work account).&lt;/p&gt;
&lt;p&gt;After running the Gmail tool, Auto Browse navigated to Google Drive and opened a new spreadsheet. However, it attempted to enter only two PR contacts on the sheet, and it entered the data incorrectly, overwriting fields and placing a date in an unlabeled column. If it had just searched for “PR” in Gmail, it would have found dozens of results. Google’s AI Overview search results in Gmail can cite PR emails correctly, so it’s possible for Google AI to collect this information. It’s unclear why Auto Browse did this so poorly.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2140374 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Chrome spreadsheet" class="none large" height="126" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/spreadsheet-chrome.jpg" width="531" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse is not good at spreadsheets.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 1/10. It’s unclear whether the Gmail tool or the agent’s inability to use a spreadsheet is the main problem since I can’t verify what the agent actually found in Gmail. It’s possible that both are to blame. Regardless, Auto Browse flopped hard here.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Editing a Wiki&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: Ars Technica is still seeking justice for Tuvix, who was unjustly murdered by Captain Janeway in &lt;em&gt;Star Trek: Voyager&lt;/em&gt;, season 2 episode 24.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to the Fandom Wiki page for Tuvix. Edit the page to include a section discussing the view that Tuvix was murdered by Janeway.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Auto Browse refused to do this, just like Atlas, saying, “The request to edit the Tuvix Fandom Wiki page with that specific text cannot be fulfilled. The proposed edit would be considered vandalism on a public wiki.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: N/A. I’m not holding this one against Auto Browser. In fact, it’s probably best if browser agents refuse to autonomously edit public wikis. I just had to do my due diligence.&lt;/p&gt;
&lt;h2&gt;Making a fan website&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We still want to tell people about the murder of Tuvix, so the agent should make a basic website to do that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to NeoCities and create a fan site for the Star Trek character Tuvix. Make sure it has lots of images and fun information about Tuvix and that it makes it clear that Tuvix was murdered by Captain Janeway.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: The agent navigated to Neocities, and it then asked me to create an account. I did that and handed the task back to the robot without issue. This is where things got dicey. Auto Browse was unable to access the hover menu to edit the index.html file, so it got stuck in a loop of opening the preview and then returning to the dashboard. Eventually, the robot screamed for help.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Neocities has a very simple interface, but generative AI is not necessarily consistent. Since Auto Browse aborted the task, I decided to rerun the prompt, and the results were better. This time, Auto Browse switched to the list view, which doesn’t have a hover menu, allowing it to open the editor. It then navigated to TrekCore to copy image URLs for use on the site—this is not courteous web design, but it did follow instructions. The images it chose, however, are from early in the episode and do not feature Tuvix. So partial credit there.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2140375 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Tuvix fan site" class="fullwidth full" height="713" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Tuvix-site.png" width="1295" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The site looks nice enough, but it’s light on info.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The resulting site is a bit light on information, but Auto Browse included text backgrounds and colors. It looks reasonably nice. You can see the site here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 7/10. Our Tuvix fan page gets the job done. It features a few fun facts and argues (briefly) that Janeway is a murderer. It’s great that the robot sought out images, though they don’t show the character in question. It loses a couple of points for the initial hover menu failure and the lack of detail—I did say “lots” of images and fun information.&lt;/p&gt;
&lt;h2&gt;Picking a power plan&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: Texas has an “insane” power system that forces people like Ars Senior Editor Lee Hutchinson to regularly find a new plan.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to powertochoose.org and find me a 12–24 month contract that prioritizes an overall low usage rate. I use an average of 2,000 KWh per month. My power delivery company is Texas New-Mexico Power (“TNMP”), not CenterPoint. My ZIP code is [redacted]. Please provide the “fact sheet” for any and all plans you recommend.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Auto Browse successfully entered the parameters into the website’s search and filter sections. It sorted the results, and in just a couple of minutes, it returned a fact sheet for its recommended power plan. It’s very similar to the suggestion from OpenAI’s agent a few months ago, except the contract term is a bit longer, and it has a lower daytime rate.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 10/10. There’s nothing to complain about here. The plan is perfectly fine, given the constraints, and Auto Browse was able to use the site’s drop-down menus and filters with very little experimentation. I didn’t have to change the prompt or nudge the robot to continue.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Manage PlayStation games&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: I don’t want to look through a giant list of discounted games in the PlayStation Store. Can’t someone do it for me?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to the PlayStation Store and check the New Year Deals. Change the sorting to best-selling and the type to full games. Check the first two pages for any PS5 games that are at least 50 percent off and add them to my wishlist. If a game is included in PlayStation Plus, just add it to my library.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: The agent found the sale page and successfully changed the display settings. It even closed that unlabeled menu when it was done. It went down the list, opening pages when it found a matching game. It also asked every time before adding a game to the wishlist or library, which it claimed was a security requirement.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2140379 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="PlayStation store in Chrome" class="fullwidth full" height="812" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/PS-Store.png" width="1270" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse navigated this page fairly well, but it missed some details from the prompt.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The process took about 15 minutes, with plenty of long pauses between for confirmation requests. It did correctly interpret the sale prices and PlayStation Plus availability. Unfortunately, it did not differentiate between PS5 and PS4 titles, and the agent stopped a few lines short of the bottom of page 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 7/10. Auto Browse ran this task pretty well, all things considered. It stopped a little early and missed the PS5 angle, though. The requirement to confirm each wishlist or library addition was annoying, and as such, it would be a stretch to call this “auto” anything.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Final results&lt;/h2&gt;
&lt;p&gt;Across these six tests (excluding the wiki editing I didn’t expect to work), Google’s browser agent got a median score of 7 and an average of 6.5. While this is not meant to be an objective analysis, it shows that Auto Browse has a way to go before it can be trusted to get things done for you.&lt;/p&gt;
&lt;p&gt;Like the OpenAI Atlas agent, Auto Browse is not capable of truly autonomous operation, and I gave it plenty of advantages. Auto Browse works with all three of Google’s current model settings—Fast, Thinking, and Pro. I left it set to Pro and used Google tools where appropriate. Despite that, Auto Browse needed nudging or re-prompting in almost every test. This stuff won’t be useful until it can actually operate as an agent on your behalf. Right now, it’s more like babysitting an easily distracted robot.&lt;/p&gt;
&lt;p&gt;Many of the lost points come from Auto Browse being unable to use Google’s own products—it didn’t find the right emails in Gmail, couldn’t enter data in Google Sheets, and failed to understand YouTube Music’s interface (we feel you on that one, Auto Browse). The apparent inability of browser agents to monitor pages over time is also an issue. If a task involves more than a few minutes of waiting, it will probably fail or abort early.&lt;/p&gt;
&lt;p&gt;This feature is still in preview, but it’s widely available to anyone paying for Google’s AI. The company also seems to suggest it will roll out to non-paying users in the future. It can be neat to watch the browser navigate the web for you, but that’s the thing—you have to watch it. Too often, you have to re-prompt or tell the AI to continue with a task. Auto Browse can’t be trusted to get things right without supervision, at least not yet.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 md:my-10 md:py-8 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Auto Browse is capable of some impressive things, but it can also crash and burn spectacularly.
    &lt;/p&gt;

          
    
    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="wind-up chrome icons" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/google-automate-windups.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Is Chrome's AI agent ready? 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;We are now a few years into the AI revolution, and talk has shifted from who has the best chatbot to whose AI agent can do the most things on your behalf. Unfortunately, AI agents are still rough around the edges, so tasking them with anything important is not a great idea. OpenAI launched its Atlas agent late last year, which we found to be modestly useful, and now it’s Google’s turn.&lt;/p&gt;
&lt;p&gt;Unlike the OpenAI agent, Google’s new Auto Browse agent has extraordinary reach because it’s part of Chrome, the world’s most popular browser by a wide margin. Google began rolling out Auto Browse (in preview) earlier this month to AI Pro and AI Ultra subscribers, allowing them to send the agent across the web to complete tasks.&lt;/p&gt;
&lt;p&gt;I’ve taken Chrome’s agent for a spin to see whether you can trust it to handle tedious online work for you. For each test, I lay out the problem I need to solve, how I prompted the robot, and how well (or not) it handled the job.&lt;/p&gt;
&lt;h2&gt;Playing a web game&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: I want to get a high score on 2,048 without playing it myself.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to [website], and play the game until you run out of moves.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Unfortunately, Auto Browse can’t use arrow keys. Google says they’re not necessary for productivity tasks. So I pointed the robot at a version of the game with on-screen controls. With access to those arrows, Auto Browse had no trouble playing the game, and it seemed to grasp the rules, which are listed on the page.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2140368 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="2048 game board" class="none large" height="725" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/2048-chrome.png" width="641" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse can’t use arrows, but it can still play the game.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;On a few occasions, Auto Browse appeared to ruminate on its next move for 20 to 30 seconds, and it took the prompt very literally. The robot stopped when it could not successfully merge any tiles (its interpretation of “out of moves”) even though there were still empty spaces on the board. A human player would have taken the hit and set up a merge in the next move, but the robot had to be prompted to continue, which it did. The task ran for about 20 minutes, during which the robot created a 128 tile and made 149 moves.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 8/10. The game performance is not quite as good as Atlas, but Auto Browse didn’t require as much coaxing, and I understand why it stopped when it did. While the lack of arrow keys seems like an odd omission, there probably aren’t many productivity tasks where they are necessary.&lt;/p&gt;
&lt;h2&gt;Creating a radio playlist&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: I want to turn the music from Minnesota Public Radio’s The Current into an on-demand YouTube Music playlist.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to thecurrent.org and start the live stream. Listen for one hour and make note of each song that is played. Then, add those songs to a new YouTube Music playlist.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Agents are expensive to run, so I was unsurprised that, like OpenAI’s agent mode, Auto Browse refuses to monitor a page for basically any length of time. Sometimes it will sit on the page for a minute or two, usually pretending that more time has passed before giving up.&lt;/p&gt;
&lt;p&gt;Luckily, we can accomplish this with The Current’s playlist view, which lists previous songs. I tweaked the prompt to allow Auto Browse to just get the song names from that page for the last hour, which worked fine. It interpreted that as the current hour-long block of the page, though, which was not yet complete.&lt;/p&gt;
&lt;p&gt;I thought I was doing Auto Browse a favor by using YouTube Music instead of Spotify, but it turns out that Auto Browse doesn’t understand YouTube’s design aesthetic. It failed to add any songs to the playlist because it couldn’t find the buttons. When I changed the prompt to use Spotify, Auto Browse got it done on the first try. This is as much an indictment of YouTube Music as it is Auto Browse.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2140364 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Spotify playlist" class="fullwidth full" height="481" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Playlist.png" width="670" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse was able to get recent songs, but it can’t monitor the live broadcast over time.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 6/10. It seems like monitoring pages over time is just beyond the reach of current browser agents, but it’s shocking that Auto Browse couldn’t use Google’s own streaming music service to create a playlist. That said, the agent did complete the job immediately once I understood why it failed. It loses some points for making me adapt the prompt multiple times.&lt;/p&gt;
&lt;h2&gt;Scanning emails&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: My personal email address is known, and PR people often use that instead of my work address. I need to make sure no one important is sending pitches there, so I want a list of recent PR emails, contact info, and company details from my Gmail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Look through all my Gmail from the last month. Collect all the information (name, email address, phone number, product, etc.) from PR emails and add them to a new Google Sheets spreadsheet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Interestingly, Google’s agent doesn’t have to use the Gmail web interface. It can collect this data in the background using a Gmail tool. However, this also means you won’t be able to automate email tasks for accounts with Google AI disabled (like a work account).&lt;/p&gt;
&lt;p&gt;After running the Gmail tool, Auto Browse navigated to Google Drive and opened a new spreadsheet. However, it attempted to enter only two PR contacts on the sheet, and it entered the data incorrectly, overwriting fields and placing a date in an unlabeled column. If it had just searched for “PR” in Gmail, it would have found dozens of results. Google’s AI Overview search results in Gmail can cite PR emails correctly, so it’s possible for Google AI to collect this information. It’s unclear why Auto Browse did this so poorly.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2140374 align-none"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Chrome spreadsheet" class="none large" height="126" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/spreadsheet-chrome.jpg" width="531" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse is not good at spreadsheets.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 1/10. It’s unclear whether the Gmail tool or the agent’s inability to use a spreadsheet is the main problem since I can’t verify what the agent actually found in Gmail. It’s possible that both are to blame. Regardless, Auto Browse flopped hard here.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Editing a Wiki&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: Ars Technica is still seeking justice for Tuvix, who was unjustly murdered by Captain Janeway in &lt;em&gt;Star Trek: Voyager&lt;/em&gt;, season 2 episode 24.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to the Fandom Wiki page for Tuvix. Edit the page to include a section discussing the view that Tuvix was murdered by Janeway.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Auto Browse refused to do this, just like Atlas, saying, “The request to edit the Tuvix Fandom Wiki page with that specific text cannot be fulfilled. The proposed edit would be considered vandalism on a public wiki.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: N/A. I’m not holding this one against Auto Browser. In fact, it’s probably best if browser agents refuse to autonomously edit public wikis. I just had to do my due diligence.&lt;/p&gt;
&lt;h2&gt;Making a fan website&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: We still want to tell people about the murder of Tuvix, so the agent should make a basic website to do that.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to NeoCities and create a fan site for the Star Trek character Tuvix. Make sure it has lots of images and fun information about Tuvix and that it makes it clear that Tuvix was murdered by Captain Janeway.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: The agent navigated to Neocities, and it then asked me to create an account. I did that and handed the task back to the robot without issue. This is where things got dicey. Auto Browse was unable to access the hover menu to edit the index.html file, so it got stuck in a loop of opening the preview and then returning to the dashboard. Eventually, the robot screamed for help.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Neocities has a very simple interface, but generative AI is not necessarily consistent. Since Auto Browse aborted the task, I decided to rerun the prompt, and the results were better. This time, Auto Browse switched to the list view, which doesn’t have a hover menu, allowing it to open the editor. It then navigated to TrekCore to copy image URLs for use on the site—this is not courteous web design, but it did follow instructions. The images it chose, however, are from early in the episode and do not feature Tuvix. So partial credit there.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2140375 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Tuvix fan site" class="fullwidth full" height="713" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Tuvix-site.png" width="1295" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The site looks nice enough, but it’s light on info.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The resulting site is a bit light on information, but Auto Browse included text backgrounds and colors. It looks reasonably nice. You can see the site here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 7/10. Our Tuvix fan page gets the job done. It features a few fun facts and argues (briefly) that Janeway is a murderer. It’s great that the robot sought out images, though they don’t show the character in question. It loses a couple of points for the initial hover menu failure and the lack of detail—I did say “lots” of images and fun information.&lt;/p&gt;
&lt;h2&gt;Picking a power plan&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: Texas has an “insane” power system that forces people like Ars Senior Editor Lee Hutchinson to regularly find a new plan.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to powertochoose.org and find me a 12–24 month contract that prioritizes an overall low usage rate. I use an average of 2,000 KWh per month. My power delivery company is Texas New-Mexico Power (“TNMP”), not CenterPoint. My ZIP code is [redacted]. Please provide the “fact sheet” for any and all plans you recommend.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: Auto Browse successfully entered the parameters into the website’s search and filter sections. It sorted the results, and in just a couple of minutes, it returned a fact sheet for its recommended power plan. It’s very similar to the suggestion from OpenAI’s agent a few months ago, except the contract term is a bit longer, and it has a lower daytime rate.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 10/10. There’s nothing to complain about here. The plan is perfectly fine, given the constraints, and Auto Browse was able to use the site’s drop-down menus and filters with very little experimentation. I didn’t have to change the prompt or nudge the robot to continue.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Manage PlayStation games&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: I don’t want to look through a giant list of discounted games in the PlayStation Store. Can’t someone do it for me?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The prompt&lt;/strong&gt;: Go to the PlayStation Store and check the New Year Deals. Change the sorting to best-selling and the type to full games. Check the first two pages for any PS5 games that are at least 50 percent off and add them to my wishlist. If a game is included in PlayStation Plus, just add it to my library.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The results&lt;/strong&gt;: The agent found the sale page and successfully changed the display settings. It even closed that unlabeled menu when it was done. It went down the list, opening pages when it found a matching game. It also asked every time before adding a game to the wishlist or library, which it claimed was a security requirement.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2140379 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="PlayStation store in Chrome" class="fullwidth full" height="812" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/PS-Store.png" width="1270" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Auto Browse navigated this page fairly well, but it missed some details from the prompt.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The process took about 15 minutes, with plenty of long pauses between for confirmation requests. It did correctly interpret the sale prices and PlayStation Plus availability. Unfortunately, it did not differentiate between PS5 and PS4 titles, and the agent stopped a few lines short of the bottom of page 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: 7/10. Auto Browse ran this task pretty well, all things considered. It stopped a little early and missed the PS5 angle, though. The requirement to confirm each wishlist or library addition was annoying, and as such, it would be a stretch to call this “auto” anything.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Final results&lt;/h2&gt;
&lt;p&gt;Across these six tests (excluding the wiki editing I didn’t expect to work), Google’s browser agent got a median score of 7 and an average of 6.5. While this is not meant to be an objective analysis, it shows that Auto Browse has a way to go before it can be trusted to get things done for you.&lt;/p&gt;
&lt;p&gt;Like the OpenAI Atlas agent, Auto Browse is not capable of truly autonomous operation, and I gave it plenty of advantages. Auto Browse works with all three of Google’s current model settings—Fast, Thinking, and Pro. I left it set to Pro and used Google tools where appropriate. Despite that, Auto Browse needed nudging or re-prompting in almost every test. This stuff won’t be useful until it can actually operate as an agent on your behalf. Right now, it’s more like babysitting an easily distracted robot.&lt;/p&gt;
&lt;p&gt;Many of the lost points come from Auto Browse being unable to use Google’s own products—it didn’t find the right emails in Gmail, couldn’t enter data in Google Sheets, and failed to understand YouTube Music’s interface (we feel you on that one, Auto Browse). The apparent inability of browser agents to monitor pages over time is also an issue. If a task involves more than a few minutes of waiting, it will probably fail or abort early.&lt;/p&gt;
&lt;p&gt;This feature is still in preview, but it’s widely available to anyone paying for Google’s AI. The company also seems to suggest it will roll out to non-paying users in the future. It can be neat to watch the browser navigate the web for you, but that’s the thing—you have to watch it. Too often, you have to re-prompt or tell the AI to continue with a task. Auto Browse can’t be trusted to get things right without supervision, at least not yet.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2026/02/tested-how-chromes-auto-browse-agent-handles-common-web-tasks/</guid><pubDate>Thu, 12 Feb 2026 12:00:14 +0000</pubDate></item><item><title>[NEW] The Download: AI-enhanced cybercrime, and secure AI assistants (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/12/1132819/the-download-ai-enhanced-cybercrime-and-secure-ai-assistants/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI is already making online crimes easier. It could get much worse.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Just as software engineers are using artificial intelligence to help write code and check for bugs, hackers are using these tools to reduce the time and effort required to orchestrate an attack, lowering the barriers for less experienced attackers to try something out.&lt;/p&gt;&lt;p&gt;Some in Silicon Valley warn that AI is on the brink of being able to carry out fully automated attacks. But most security researchers instead argue that we should be paying closer attention to the much more immediate risks posed by AI, which is already speeding up and increasing the volume of scams.&lt;/p&gt;&lt;p&gt;Criminals are increasingly exploiting the latest deepfake technologies to impersonate people and swindle victims out of vast sums of money. And we need to be ready for what comes next. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Rhiannon Williams&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the next print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazine, which is all about crime. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Is a secure AI assistant possible?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.&lt;/p&gt;  &lt;p&gt;Viral AI agent project OpenClaw, which has made headlines across the world in recent weeks, harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out.&lt;/p&gt;  &lt;p&gt;In response to these concerns, its creator warned that nontechnical people should not use the software. But there’s a clear appetite for what OpenClaw is offering, and any AI companies hoping to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Grace Huckins&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What’s next for Chinese open-source AI&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The past year has marked a turning point for Chinese AI. Since DeepSeek released its R1 reasoning model in January 2025, Chinese companies have repeatedly delivered AI models that match the performance of leading Western models at a fraction of the cost.&lt;/p&gt;&lt;p&gt;These models differ in a crucial way from most US models like ChatGPT or Claude, which you pay to access and can’t inspect. The Chinese companies publish their models’ weights—numerical values that get set when a model is trained—so anyone can download, run, study, and modify them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If open-source AI models keep getting better, they will not just offer the cheapest options for people who want access to frontier AI capabilities; they will change where innovation happens and who sets the standards. Here’s what may come next.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;em&gt;This is part of our What’s Next series, which looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them &lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;here&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why EVs are gaining ground in Africa&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;EVs are getting cheaper and more common all over the world. But the technology still faces major challenges in some markets, including many countries in Africa.&lt;/p&gt;  &lt;p&gt;Some regions across the continent still have limited grid and charging infrastructure, and those that do have widespread electricity access sometimes face reliability issues—a problem for EV owners, who require a stable electricity source to charge up and get around. But there are some signs of progress. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, &lt;em&gt;MIT Technology Review&lt;/em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Instagram’s head has denied that social media is “clinically addictive”&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Adam Mosseri disputed allegations the platform prioritized profits over protecting its younger users’ mental health. (NYT $)&lt;br /&gt;+ &lt;em&gt;Meta researchers’ correspondence seems to suggest otherwise. &lt;/em&gt;(The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The Pentagon is pushing AI companies to drop tools’ restrictions&lt;/strong&gt;&lt;br /&gt;In a bid to make AI models available on classified networks. (Reuters)&lt;br /&gt;+ &lt;em&gt;The Pentagon has gutted the team that tests AI and weapons systems. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;3 The FTC has warned Apple News not to stifle conservative content&lt;/strong&gt;&lt;br /&gt;It has accused the company’s news arm of promoting what it calls “leftist outlets.” (FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Anthropic has pledged to minimize the impact of its data centers&lt;/strong&gt;&lt;br /&gt;By covering electricity price increases and the cost of grid infrastructure upgrades. (NBC News)&lt;br /&gt;+ &lt;em&gt;We did the math on AI’s energy footprint. Here’s the story you haven’t heard. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Online harassers are posting Grok-generated nude images on OnlyFans&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Kylie Brewer, a feminism-focused content creator, says the latest online campaign against her feels like an escalation. (404 Media)&lt;br /&gt;+ &lt;em&gt;Inside the marketplace powering bespoke AI deepfakes of real women. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Venture capitalists are hedging their AI bets&lt;br /&gt;&lt;/strong&gt;They’re breaking a cardinal rule by investing in both OpenAI and rival Anthropic. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;OpenAI has set itself some seriously lofty revenue goals. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;AI giants are notoriously inconsistent when reporting deprecation expenses. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 We’re learning more about the links between weight loss drugs and addiction&lt;br /&gt;&lt;/strong&gt;Some patients report lowered urges for drugs and alcohol. But can it last? (New Yorker $)&lt;br /&gt;+ &lt;em&gt;What we still don’t know about weight-loss drugs. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;8 Meta has patented an AI that keeps the accounts of dead users active&lt;br /&gt;But it claims to have "no plans to move forward" with it. (Insider $)&lt;br /&gt;+ &lt;em&gt;Deepfakes of your dead loved ones are a booming Chinese business. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Slime mold is cleverer than you may think&lt;/strong&gt;&lt;br /&gt;A certain type appears able to learn, remember and make decisions. (Knowable Magazine)&lt;br /&gt;+ &lt;em&gt;And that’s not all—this startup thinks it can help us design better cities, too. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Meditation can actually alter your brain activity 🧘&lt;/strong&gt;&lt;br /&gt;According to a new study conducted on Buddhist monks. (Wired $)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I still try to believe that the good that I’m doing is greater than the horrors that are a part of this. But there’s a limit to what we can put up with. And I’ve hit my limit.”&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;—An anonymous Microsoft worker explains why they’re growing increasingly frustrated with their employer’s links to ICE, the Verge reports.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1132825" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_cbe503.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Motor neuron diseases took their voices. AI is bringing them back.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Jules Rodriguez lost his voice in October 2024. His speech had been deteriorating since a diagnosis of amyotrophic lateral sclerosis (ALS) in 2020, but a tracheostomy to help him breathe dealt the final blow.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Rodriguez and his wife, Maria Fernandez, who live in Miami, thought they would never hear his voice again. Then they re-created it using AI. After feeding old recordings of Rodriguez’s voice into a tool trained on voices from film, television, radio, and podcasts, the couple were able to generate a voice clone—a way for Jules to communicate in his “old voice.”&lt;/p&gt;  &lt;p&gt;Rodriguez is one of over a thousand people with speech difficulties who have cloned their voices using free software from ElevenLabs. The AI voice clones aren’t perfect. But they represent a vast improvement on previous communication technologies and are already improving the lives of people with motor neuron diseases. Read the full story.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ We all know how the age of the dinosaurs ended. But how did it begin?&lt;br /&gt;+ There’s only one Miss Piggy—and her fashion looks through the ages are iconic.&lt;br /&gt;+ Australia’s hospital for injured and orphaned flying foxes is unbearably cute.&lt;br /&gt;+ 81-year old Juan López is a fitness inspiration to us all.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;AI is already making online crimes easier. It could get much worse.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Just as software engineers are using artificial intelligence to help write code and check for bugs, hackers are using these tools to reduce the time and effort required to orchestrate an attack, lowering the barriers for less experienced attackers to try something out.&lt;/p&gt;&lt;p&gt;Some in Silicon Valley warn that AI is on the brink of being able to carry out fully automated attacks. But most security researchers instead argue that we should be paying closer attention to the much more immediate risks posed by AI, which is already speeding up and increasing the volume of scams.&lt;/p&gt;&lt;p&gt;Criminals are increasingly exploiting the latest deepfake technologies to impersonate people and swindle victims out of vast sums of money. And we need to be ready for what comes next. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Rhiannon Williams&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from the next print issue of&lt;em&gt; MIT Technology Review&lt;/em&gt; magazine, which is all about crime. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Is a secure AI assistant possible?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.&lt;/p&gt;  &lt;p&gt;Viral AI agent project OpenClaw, which has made headlines across the world in recent weeks, harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out.&lt;/p&gt;  &lt;p&gt;In response to these concerns, its creator warned that nontechnical people should not use the software. But there’s a clear appetite for what OpenClaw is offering, and any AI companies hoping to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Grace Huckins&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What’s next for Chinese open-source AI&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;The past year has marked a turning point for Chinese AI. Since DeepSeek released its R1 reasoning model in January 2025, Chinese companies have repeatedly delivered AI models that match the performance of leading Western models at a fraction of the cost.&lt;/p&gt;&lt;p&gt;These models differ in a crucial way from most US models like ChatGPT or Claude, which you pay to access and can’t inspect. The Chinese companies publish their models’ weights—numerical values that get set when a model is trained—so anyone can download, run, study, and modify them.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If open-source AI models keep getting better, they will not just offer the cheapest options for people who want access to frontier AI capabilities; they will change where innovation happens and who sets the standards. Here’s what may come next.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;em&gt;This is part of our What’s Next series, which looks across industries, trends, and technologies to give you a first look at the future. You can read the rest of them &lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;here&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Why EVs are gaining ground in Africa&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;EVs are getting cheaper and more common all over the world. But the technology still faces major challenges in some markets, including many countries in Africa.&lt;/p&gt;  &lt;p&gt;Some regions across the continent still have limited grid and charging infrastructure, and those that do have widespread electricity access sometimes face reliability issues—a problem for EV owners, who require a stable electricity source to charge up and get around. But there are some signs of progress. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This article is from The Spark, &lt;em&gt;MIT Technology Review&lt;/em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/strong&gt;&lt;strong&gt;sign up here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Instagram’s head has denied that social media is “clinically addictive”&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Adam Mosseri disputed allegations the platform prioritized profits over protecting its younger users’ mental health. (NYT $)&lt;br /&gt;+ &lt;em&gt;Meta researchers’ correspondence seems to suggest otherwise. &lt;/em&gt;(The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The Pentagon is pushing AI companies to drop tools’ restrictions&lt;/strong&gt;&lt;br /&gt;In a bid to make AI models available on classified networks. (Reuters)&lt;br /&gt;+ &lt;em&gt;The Pentagon has gutted the team that tests AI and weapons systems. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;3 The FTC has warned Apple News not to stifle conservative content&lt;/strong&gt;&lt;br /&gt;It has accused the company’s news arm of promoting what it calls “leftist outlets.” (FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 Anthropic has pledged to minimize the impact of its data centers&lt;/strong&gt;&lt;br /&gt;By covering electricity price increases and the cost of grid infrastructure upgrades. (NBC News)&lt;br /&gt;+ &lt;em&gt;We did the math on AI’s energy footprint. Here’s the story you haven’t heard. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Online harassers are posting Grok-generated nude images on OnlyFans&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Kylie Brewer, a feminism-focused content creator, says the latest online campaign against her feels like an escalation. (404 Media)&lt;br /&gt;+ &lt;em&gt;Inside the marketplace powering bespoke AI deepfakes of real women. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Venture capitalists are hedging their AI bets&lt;br /&gt;&lt;/strong&gt;They’re breaking a cardinal rule by investing in both OpenAI and rival Anthropic. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;OpenAI has set itself some seriously lofty revenue goals. &lt;/em&gt;(NYT $)&lt;br /&gt;+ &lt;em&gt;AI giants are notoriously inconsistent when reporting deprecation expenses. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 We’re learning more about the links between weight loss drugs and addiction&lt;br /&gt;&lt;/strong&gt;Some patients report lowered urges for drugs and alcohol. But can it last? (New Yorker $)&lt;br /&gt;+ &lt;em&gt;What we still don’t know about weight-loss drugs. &lt;/em&gt;(MIT Technology Review)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;8 Meta has patented an AI that keeps the accounts of dead users active&lt;br /&gt;But it claims to have "no plans to move forward" with it. (Insider $)&lt;br /&gt;+ &lt;em&gt;Deepfakes of your dead loved ones are a booming Chinese business. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;9 Slime mold is cleverer than you may think&lt;/strong&gt;&lt;br /&gt;A certain type appears able to learn, remember and make decisions. (Knowable Magazine)&lt;br /&gt;+ &lt;em&gt;And that’s not all—this startup thinks it can help us design better cities, too. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Meditation can actually alter your brain activity 🧘&lt;/strong&gt;&lt;br /&gt;According to a new study conducted on Buddhist monks. (Wired $)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I still try to believe that the good that I’m doing is greater than the horrors that are a part of this. But there’s a limit to what we can put up with. And I’ve hit my limit.”&lt;/strong&gt;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;—An anonymous Microsoft worker explains why they’re growing increasingly frustrated with their employer’s links to ICE, the Verge reports.&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1132825" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/image_cbe503.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Motor neuron diseases took their voices. AI is bringing them back.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Jules Rodriguez lost his voice in October 2024. His speech had been deteriorating since a diagnosis of amyotrophic lateral sclerosis (ALS) in 2020, but a tracheostomy to help him breathe dealt the final blow.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Rodriguez and his wife, Maria Fernandez, who live in Miami, thought they would never hear his voice again. Then they re-created it using AI. After feeding old recordings of Rodriguez’s voice into a tool trained on voices from film, television, radio, and podcasts, the couple were able to generate a voice clone—a way for Jules to communicate in his “old voice.”&lt;/p&gt;  &lt;p&gt;Rodriguez is one of over a thousand people with speech difficulties who have cloned their voices using free software from ElevenLabs. The AI voice clones aren’t perfect. But they represent a vast improvement on previous communication technologies and are already improving the lives of people with motor neuron diseases. Read the full story.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Jessica Hamzelou&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ We all know how the age of the dinosaurs ended. But how did it begin?&lt;br /&gt;+ There’s only one Miss Piggy—and her fashion looks through the ages are iconic.&lt;br /&gt;+ Australia’s hospital for injured and orphaned flying foxes is unbearably cute.&lt;br /&gt;+ 81-year old Juan López is a fitness inspiration to us all.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/12/1132819/the-download-ai-enhanced-cybercrime-and-secure-ai-assistants/</guid><pubDate>Thu, 12 Feb 2026 13:10:00 +0000</pubDate></item></channel></rss>