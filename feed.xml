<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 28 Feb 2026 06:42:38 +0000</lastBuildDate><item><title>Anthropic vs. the Pentagon: What’s actually at stake? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/27/anthropic-vs-the-pentagon-whats-actually-at-stake/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2218106494.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The past two weeks have been defined by a clash between Anthropic CEO Dario Amodei and Defense Secretary Pete Hegseth as the two battle over the military’s use of AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic refuses to allow its AI models to be used for mass surveillance of Americans or for fully autonomous weapons that conduct strikes without human input. At the same time, Secretary Hegseth has argued the Department of Defense shouldn’t be limited by the rules of a vendor, arguing any “lawful use” of the technology should be permitted.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Thursday, Amodei publicly signaled that Anthropic isn’t backing down — despite threats that his company could be designated as a supply chain risk as a result. But with the news cycle moving fast, it’s worth revisiting exactly what’s at stake in the fight.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At its core, this fight is about who controls powerful AI systems — the companies that build them, or the government that wants to deploy them. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-anthropic-worried-about"&gt;What is Anthropic worried about?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As we said above, Anthropic doesn’t want its AI models to be used for mass surveillance of Americans or for autonomous weapons with no humans in the loop for targeting and firing decisions. Traditional defense contractors typically have little say in how their products will be used, but Anthropic has argued from its inception that AI technology poses unique risks and therefore requires unique safeguards. From the company’s perspective, the question is how to maintain those safeguards when the technology is being used by the military.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. military already relies on highly automated systems, some of which are lethal. The decision to use lethal force has historically been left to humans, but there are few legal restrictions on military use of autonomous weapons. The DoD doesn’t categorically ban fully autonomous weapons systems. According to a 2023 DOD directive, AI systems can select and engage targets without human intervention, as long as they meet certain standards and pass review by senior defense officials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s precisely what makes Anthropic nervous. Military technology is secretive by nature, so if the U.S. military were taking steps to automate lethal decision-making, we might not know about it until it was operational. And if it used Anthropic’s models, it could count as “lawful use.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic’s position isn’t that such uses should be permanently off the table. It’s that its models aren’t capable enough to support them safely yet. Imagine an autonomous system misidentifying a target, escalating a conflict without human authorization, or making a split-second lethal decision that no one can reverse. Put a less-capable AI in charge of weapons, and you get a very fast, very confident machine that’s bad at making high-stakes calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI also has the power to supercharge lawful surveillance of American citizens to a concerning degree. Under current U.S. laws, surveillance of American citizens is already possible, whether through collection of texts, emails, and other communication. AI changes the equation by enabling automated large-scale pattern detection, entity resolution across datasets, predictive risk scoring, and continuous behavioral analysis.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-does-the-pentagon-want"&gt;What does the Pentagon want?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pentagon’s argument is that it should be able to deploy Anthropic’s technology for any lawful use it deems necessary, rather than be limited by Anthropic’s internal policies on things like autonomous weapons or surveillance.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;More specifically, Secretary Hegseth has argued the Department of Defense shouldn’t be limited by the rules of a vendor and that it would engage in “lawful use” of the technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sean Parnell, the Pentagon’s chief spokesperson, said in a Thursday X post that the department has no interest in conducting mass domestic surveillance or deploying autonomous weapons.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Here’s what we’re asking: Allow the Pentagon to use Anthropic’s model for all lawful purposes,” Parnell said. “This is a simple, common-sense request that will prevent Anthropic from jeopardizing critical military operations and potentially putting our warfighters at risk. We will not let ANY company dictate the terms regarding how we make operational decisions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that Anthropic has until 5:01 p.m. ET on Friday to decide. “Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk for DOW,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the DoD’s stance that it simply doesn’t believe it should be limited by a corporation’s usage policies, Secretary Hegseth’s concerns about Anthropic have at times&amp;nbsp;seemed connected to cultural grievance. In a speech at SpaceX and xAI offices in January, Hegseth railed against “woke AI” in a speech that some saw as a preview of his feud with Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Department of War AI will not be woke,” Hegseth said. “We’re building war-ready weapons and systems, not chatbots for an Ivy League faculty lounge.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-so-what-now"&gt;So what now?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pentagon has threatened to either declare Anthropic a “supply chain risk” — which effectively blacklists Anthropic from doing business with the government — or invoke the Defense Production Act (DPA) to force the company to tailor its model to the military’s needs. Hegseth has given Anthropic until 5:01 p.m. on Friday to respond. But with the deadline approaching, it’s anyone’s guess whether the Pentagon will make good on its threat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is not a fight either party can easily walk away from. Sachin Seth, a VC at Trousdale Ventures who focuses on defense tech, says a supply chain risk label for Anthropic could mean “lights out” for the company.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, he said, if Anthropic is dropped from the DoD, it could be a national security issue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[The Department] would have to wait six to 12 months for either OpenAI or xAI to catch up,” Seth told TechCrunch. “That leaves a window of up to a year where they might be working from not the best model, but the second or third best.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI is gearing up to become classified-ready and replace Anthropic, and it’s fair to say given owner Elon Musk’s rhetoric on the matter that the company would have no problem giving the DoD total control over its technology. Recent reports indicate that OpenAI may stick to the same red lines as Anthropic.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2218106494.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The past two weeks have been defined by a clash between Anthropic CEO Dario Amodei and Defense Secretary Pete Hegseth as the two battle over the military’s use of AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic refuses to allow its AI models to be used for mass surveillance of Americans or for fully autonomous weapons that conduct strikes without human input. At the same time, Secretary Hegseth has argued the Department of Defense shouldn’t be limited by the rules of a vendor, arguing any “lawful use” of the technology should be permitted.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Thursday, Amodei publicly signaled that Anthropic isn’t backing down — despite threats that his company could be designated as a supply chain risk as a result. But with the news cycle moving fast, it’s worth revisiting exactly what’s at stake in the fight.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At its core, this fight is about who controls powerful AI systems — the companies that build them, or the government that wants to deploy them. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-anthropic-worried-about"&gt;What is Anthropic worried about?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As we said above, Anthropic doesn’t want its AI models to be used for mass surveillance of Americans or for autonomous weapons with no humans in the loop for targeting and firing decisions. Traditional defense contractors typically have little say in how their products will be used, but Anthropic has argued from its inception that AI technology poses unique risks and therefore requires unique safeguards. From the company’s perspective, the question is how to maintain those safeguards when the technology is being used by the military.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The U.S. military already relies on highly automated systems, some of which are lethal. The decision to use lethal force has historically been left to humans, but there are few legal restrictions on military use of autonomous weapons. The DoD doesn’t categorically ban fully autonomous weapons systems. According to a 2023 DOD directive, AI systems can select and engage targets without human intervention, as long as they meet certain standards and pass review by senior defense officials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s precisely what makes Anthropic nervous. Military technology is secretive by nature, so if the U.S. military were taking steps to automate lethal decision-making, we might not know about it until it was operational. And if it used Anthropic’s models, it could count as “lawful use.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic’s position isn’t that such uses should be permanently off the table. It’s that its models aren’t capable enough to support them safely yet. Imagine an autonomous system misidentifying a target, escalating a conflict without human authorization, or making a split-second lethal decision that no one can reverse. Put a less-capable AI in charge of weapons, and you get a very fast, very confident machine that’s bad at making high-stakes calls.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI also has the power to supercharge lawful surveillance of American citizens to a concerning degree. Under current U.S. laws, surveillance of American citizens is already possible, whether through collection of texts, emails, and other communication. AI changes the equation by enabling automated large-scale pattern detection, entity resolution across datasets, predictive risk scoring, and continuous behavioral analysis.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-does-the-pentagon-want"&gt;What does the Pentagon want?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pentagon’s argument is that it should be able to deploy Anthropic’s technology for any lawful use it deems necessary, rather than be limited by Anthropic’s internal policies on things like autonomous weapons or surveillance.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;More specifically, Secretary Hegseth has argued the Department of Defense shouldn’t be limited by the rules of a vendor and that it would engage in “lawful use” of the technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sean Parnell, the Pentagon’s chief spokesperson, said in a Thursday X post that the department has no interest in conducting mass domestic surveillance or deploying autonomous weapons.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Here’s what we’re asking: Allow the Pentagon to use Anthropic’s model for all lawful purposes,” Parnell said. “This is a simple, common-sense request that will prevent Anthropic from jeopardizing critical military operations and potentially putting our warfighters at risk. We will not let ANY company dictate the terms regarding how we make operational decisions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that Anthropic has until 5:01 p.m. ET on Friday to decide. “Otherwise, we will terminate our partnership with Anthropic and deem them a supply chain risk for DOW,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the DoD’s stance that it simply doesn’t believe it should be limited by a corporation’s usage policies, Secretary Hegseth’s concerns about Anthropic have at times&amp;nbsp;seemed connected to cultural grievance. In a speech at SpaceX and xAI offices in January, Hegseth railed against “woke AI” in a speech that some saw as a preview of his feud with Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Department of War AI will not be woke,” Hegseth said. “We’re building war-ready weapons and systems, not chatbots for an Ivy League faculty lounge.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-so-what-now"&gt;So what now?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pentagon has threatened to either declare Anthropic a “supply chain risk” — which effectively blacklists Anthropic from doing business with the government — or invoke the Defense Production Act (DPA) to force the company to tailor its model to the military’s needs. Hegseth has given Anthropic until 5:01 p.m. on Friday to respond. But with the deadline approaching, it’s anyone’s guess whether the Pentagon will make good on its threat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is not a fight either party can easily walk away from. Sachin Seth, a VC at Trousdale Ventures who focuses on defense tech, says a supply chain risk label for Anthropic could mean “lights out” for the company.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, he said, if Anthropic is dropped from the DoD, it could be a national security issue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[The Department] would have to wait six to 12 months for either OpenAI or xAI to catch up,” Seth told TechCrunch. “That leaves a window of up to a year where they might be working from not the best model, but the second or third best.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI is gearing up to become classified-ready and replace Anthropic, and it’s fair to say given owner Elon Musk’s rhetoric on the matter that the company would have no problem giving the DoD total control over its technology. Recent reports indicate that OpenAI may stick to the same red lines as Anthropic.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/27/anthropic-vs-the-pentagon-whats-actually-at-stake/</guid><pubDate>Fri, 27 Feb 2026 19:11:04 +0000</pubDate></item><item><title>Musk bashes OpenAI in deposition, saying ‘nobody committed suicide because of Grok’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/27/musk-bashes-openai-in-deposition-saying-nobody-committed-suicide-because-of-grok/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a newly released deposition filed in Elon Musk’s case against OpenAI, the tech executive attacked OpenAI’s safety record, claiming that his company, xAI, better prioritizes safety.  He went so far as to say that “Nobody has committed suicide because of Grok, but apparently they have because of ChatGPT.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The comment came up in a line of questioning about a public letter Musk signed in March 2023. In it, he called on AI labs to pause development of AI systems more powerful than GPT-4, OpenAI’s flagship model at the time, for at least six months. The letter, which was signed by over 1,100 people, including many AI experts, stated there was not enough planning and management taking place at AI labs, as they were locked in an “out-of-control race to develop and deploy ever more powerful digital minds that no one — not even their creators — can understand, predict, or reliably control.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Those fears have since gained credibility. OpenAI now faces a series of lawsuits alleging that ChatGPT’s manipulative conversation tactics have led several people to experience negative mental health effects, with some dying by suicide. Musk’s comment suggests that these incidents could be used as fodder in his case against OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The transcript of Musk’s video testimony, which took place back in September, was filed publicly this week, ahead of the expected jury trial next month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The&amp;nbsp;lawsuit&amp;nbsp;against OpenAI centers on the company’s shift from a nonprofit AI research lab to a for-profit company, which Musk claims violated its founding agreements. As part of his arguments, Musk claims that AI safety could be compromised by OpenAI’s commercial relationships, as such relationships would place speed, scale, and revenue above safety concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, since that recording, xAI has faced safety concerns of its own. Last month, Musk’s social network X was flooded with nonconsensual nude images generated by xAI’s Grok, some of which were said to be of minors. This led the California Attorney General’s office to open an investigation into the matter. The EU is also running its own investigation, and other governments have taken action, too, with some imposing blocks and bans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the newly filed deposition, Musk claimed he had signed the AI safety letter because “it seemed like a good idea,” not because he had just incorporated an AI company looking to compete with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I signed it, as many people did, to urge caution with AI development,” Musk said. “I just wanted&amp;nbsp;… AI safety to be prioritized.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3097909" height="500" src="https://techcrunch.com/wp-content/uploads/2026/02/sure-jan-elon.jpg?w=500" width="500" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;imgflip&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Musk also responded to other questions in the deposition, including those about artificial general intelligence, or AGI — the concept of AI that can match or surpass human reasoning across a broad range of tasks — saying “it has a risk.” He also confirmed that he “was mistaken” about his supposed $100 million donation to OpenAI; the second amended complaint in the case puts the actual figure closer to $44.8 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also recalled why OpenAI was founded, which, from his perspective, was because he was “increasingly concerned about the danger of Google being a monopoly in AI,” adding that his conversations with Google co-founder Larry Page were “alarming, in that he did not seem to be taking AI safety seriously.” OpenAI was formed as a counterweight to that threat, Musk claimed.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a newly released deposition filed in Elon Musk’s case against OpenAI, the tech executive attacked OpenAI’s safety record, claiming that his company, xAI, better prioritizes safety.  He went so far as to say that “Nobody has committed suicide because of Grok, but apparently they have because of ChatGPT.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The comment came up in a line of questioning about a public letter Musk signed in March 2023. In it, he called on AI labs to pause development of AI systems more powerful than GPT-4, OpenAI’s flagship model at the time, for at least six months. The letter, which was signed by over 1,100 people, including many AI experts, stated there was not enough planning and management taking place at AI labs, as they were locked in an “out-of-control race to develop and deploy ever more powerful digital minds that no one — not even their creators — can understand, predict, or reliably control.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Those fears have since gained credibility. OpenAI now faces a series of lawsuits alleging that ChatGPT’s manipulative conversation tactics have led several people to experience negative mental health effects, with some dying by suicide. Musk’s comment suggests that these incidents could be used as fodder in his case against OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The transcript of Musk’s video testimony, which took place back in September, was filed publicly this week, ahead of the expected jury trial next month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The&amp;nbsp;lawsuit&amp;nbsp;against OpenAI centers on the company’s shift from a nonprofit AI research lab to a for-profit company, which Musk claims violated its founding agreements. As part of his arguments, Musk claims that AI safety could be compromised by OpenAI’s commercial relationships, as such relationships would place speed, scale, and revenue above safety concerns.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, since that recording, xAI has faced safety concerns of its own. Last month, Musk’s social network X was flooded with nonconsensual nude images generated by xAI’s Grok, some of which were said to be of minors. This led the California Attorney General’s office to open an investigation into the matter. The EU is also running its own investigation, and other governments have taken action, too, with some imposing blocks and bans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the newly filed deposition, Musk claimed he had signed the AI safety letter because “it seemed like a good idea,” not because he had just incorporated an AI company looking to compete with OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I signed it, as many people did, to urge caution with AI development,” Musk said. “I just wanted&amp;nbsp;… AI safety to be prioritized.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3097909" height="500" src="https://techcrunch.com/wp-content/uploads/2026/02/sure-jan-elon.jpg?w=500" width="500" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;imgflip&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Musk also responded to other questions in the deposition, including those about artificial general intelligence, or AGI — the concept of AI that can match or surpass human reasoning across a broad range of tasks — saying “it has a risk.” He also confirmed that he “was mistaken” about his supposed $100 million donation to OpenAI; the second amended complaint in the case puts the actual figure closer to $44.8 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He also recalled why OpenAI was founded, which, from his perspective, was because he was “increasingly concerned about the danger of Google being a monopoly in AI,” adding that his conversations with Google co-founder Larry Page were “alarming, in that he did not seem to be taking AI safety seriously.” OpenAI was formed as a counterweight to that threat, Musk claimed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/27/musk-bashes-openai-in-deposition-saying-nobody-committed-suicide-because-of-grok/</guid><pubDate>Fri, 27 Feb 2026 19:42:00 +0000</pubDate></item><item><title>MIT Technology Review is a 2026 ASME finalist in reporting (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/27/1133769/asme-finalist-reporting/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/monster-thumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;AI is often described as a black box, but it’s not just its inner workings that are mysterious. Leading AI companies have kept figures on energy use closely guarded, making it hard to determine its climate impact. In a rigorous investigation, senior AI reporter James O’Donnell and senior climate reporter Casey Crownhart spent six months digging through hundreds of pages of reports, interviewing experts, and crunching the numbers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The team drilled down into the energy cost of a single prompt, and then zoomed out to build a broader picture illustrating the potential impacts of AI’s current and future energy demand. Their work revealed just how big AI’s energy footprint is, where that energy comes from, and who will pay for it. In the months following the project’s publication, major AI companies including Open AI, Mistral, and Google published details about their models’ energy and water usage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The 2026 awards will be presented in New York City on May 19.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/monster-thumb.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;AI is often described as a black box, but it’s not just its inner workings that are mysterious. Leading AI companies have kept figures on energy use closely guarded, making it hard to determine its climate impact. In a rigorous investigation, senior AI reporter James O’Donnell and senior climate reporter Casey Crownhart spent six months digging through hundreds of pages of reports, interviewing experts, and crunching the numbers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The team drilled down into the energy cost of a single prompt, and then zoomed out to build a broader picture illustrating the potential impacts of AI’s current and future energy demand. Their work revealed just how big AI’s energy footprint is, where that energy comes from, and who will pay for it. In the months following the project’s publication, major AI companies including Open AI, Mistral, and Google published details about their models’ energy and water usage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The 2026 awards will be presented in New York City on May 19.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/27/1133769/asme-finalist-reporting/</guid><pubDate>Fri, 27 Feb 2026 21:41:31 +0000</pubDate></item><item><title>Pentagon moves to designate Anthropic as a supply-chain risk (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/27/pentagon-moves-to-designate-anthropic-as-a-supply-chain-risk/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225249178.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a post on Truth Social, President Trump directed federal agencies to cease use of all Anthropic products after the company’s public dispute with the Department of Defense. The president allowed for a six-month phase-out period for departments using the products, but emphasized that Anthropic was no longer welcome as a federal contractor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t need it, we don’t want it, and will not do business with them again,” the president wrote in the post.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, the president’s post did not mention any plans to designate Anthropic as a supply chain risk, as had been previously mentioned as a consequence. However, a subsequent tweet from Secretary of Defense Pete Hegseth made good on the threat.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“In conjunction with the President’s directive for the Federal Government to cease all use of Anthropic’s technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security,” Secretary Hegseth wrote. “Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pentagon dispute centered on Anthropic’s refusal to allow its AI models to be used to power either mass domestic surveillance or fully autonomous weapons, which Secretary Hegseth found unduly restrictive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Dario Amodei reiterated his stance in a public post on Thursday, refusing to compromise on the two points.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our strong preference is to continue to serve the Department and our warfighters — with our two requested safeguards in place,” Amodei wrote at the time. “Should the Department choose to offboard Anthropic, we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations, or other critical missions.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI reportedly came out in support of Anthropic’s decision. Per the BBC, CEO Sam Altman sent a memo to staff on Thursday saying he shared the same “red lines” and that any OpenAI-related defense contracts would also reject uses that were “unlawful or unsuited to cloud deployments, such as domestic surveillance and autonomous offensive weapons.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI co-founder Ilya Sutskever, who very publicly fell out with Altman in November 2023 and has since co-founded his own AI company, also waded into the conversation on Friday, writing on X: “It’s extremely good that Anthropic has not backed down, and it’s significant that OpenAI has taken a similar stance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the future, there will be much more challenging situations of this nature, and it will be critical for the relevant leaders to rise up to the occasion, for fierce competitors to put their differences aside. Good to see that happen today.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But within hours of the Trump administration ordering federal agencies to cut ties with Anthropic, OpenAI moved to fill the void, announcing a deal with the Pentagon that Altman said preserved the same core principles Anthropic had fought for — prohibitions on domestic surveillance and autonomous weapons. According to the New York Times, OpenAI and the government began meeting about a potential tie-up on Wednesday of this week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Surely, there will be more twists to come.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic, OpenAI and Google each received contract awards from the U.S. Defense Department last July. While some Google employees have come out in support of Anthropic, Google and its parent company have yet to comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Update: This story has been updated with additional reporting&lt;/em&gt;.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2225249178.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a post on Truth Social, President Trump directed federal agencies to cease use of all Anthropic products after the company’s public dispute with the Department of Defense. The president allowed for a six-month phase-out period for departments using the products, but emphasized that Anthropic was no longer welcome as a federal contractor.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t need it, we don’t want it, and will not do business with them again,” the president wrote in the post.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, the president’s post did not mention any plans to designate Anthropic as a supply chain risk, as had been previously mentioned as a consequence. However, a subsequent tweet from Secretary of Defense Pete Hegseth made good on the threat.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“In conjunction with the President’s directive for the Federal Government to cease all use of Anthropic’s technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security,” Secretary Hegseth wrote. “Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pentagon dispute centered on Anthropic’s refusal to allow its AI models to be used to power either mass domestic surveillance or fully autonomous weapons, which Secretary Hegseth found unduly restrictive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Dario Amodei reiterated his stance in a public post on Thursday, refusing to compromise on the two points.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our strong preference is to continue to serve the Department and our warfighters — with our two requested safeguards in place,” Amodei wrote at the time. “Should the Department choose to offboard Anthropic, we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations, or other critical missions.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI reportedly came out in support of Anthropic’s decision. Per the BBC, CEO Sam Altman sent a memo to staff on Thursday saying he shared the same “red lines” and that any OpenAI-related defense contracts would also reject uses that were “unlawful or unsuited to cloud deployments, such as domestic surveillance and autonomous offensive weapons.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI co-founder Ilya Sutskever, who very publicly fell out with Altman in November 2023 and has since co-founded his own AI company, also waded into the conversation on Friday, writing on X: “It’s extremely good that Anthropic has not backed down, and it’s significant that OpenAI has taken a similar stance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the future, there will be much more challenging situations of this nature, and it will be critical for the relevant leaders to rise up to the occasion, for fierce competitors to put their differences aside. Good to see that happen today.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But within hours of the Trump administration ordering federal agencies to cut ties with Anthropic, OpenAI moved to fill the void, announcing a deal with the Pentagon that Altman said preserved the same core principles Anthropic had fought for — prohibitions on domestic surveillance and autonomous weapons. According to the New York Times, OpenAI and the government began meeting about a potential tie-up on Wednesday of this week.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Surely, there will be more twists to come.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic, OpenAI and Google each received contract awards from the U.S. Defense Department last July. While some Google employees have come out in support of Anthropic, Google and its parent company have yet to comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Update: This story has been updated with additional reporting&lt;/em&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/27/pentagon-moves-to-designate-anthropic-as-a-supply-chain-risk/</guid><pubDate>Fri, 27 Feb 2026 21:53:14 +0000</pubDate></item><item><title>Featured video: Coding for underwater robotics (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/featured-video-coding-underwater-robotics-0227</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/Ivy-Mahncke-Lincoln-Laboratory-00.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During a summer internship at MIT Lincoln Laboratory, Ivy Mahncke, an undergraduate student of robotics engineering at Olin College of Engineering, took a hands-on approach to testing algorithms for underwater navigation. She first discovered her love for working with underwater robotics as an intern at the Woods Hole Oceanographic Institution in 2024. Drawn by the chance to tackle new problems and cutting-edge algorithm development, Mahncke began an internship with Lincoln Laboratory's Advanced Undersea Systems and Technology Group in 2025.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mahncke spent the summer developing and troubleshooting an algorithm that would help a human diver and robotic vehicle collaboratively navigate underwater. The lack of traditional localization aids — such as the Global Positioning System, or GPS — in an underwater environment posed challenges for navigation that Mahncke and her mentors sought to overcome. Her work in the laboratory culminated in field tests of the algorithm on an operational underwater vehicle. Accompanying group staff to field test sites in the Atlantic Ocean, Charles River, and Lake Superior, Mahncke had the opportunity see her software in action in the real world.&lt;/p&gt;&lt;p&gt;"One of the lead engineers on the project had split off to go do other work. And she said, 'Here's my laptop. Here are the things that you need to do. I trust you to go do them.' And so I got to be out on the water as not just an extra pair of hands, but as one of the lead field testers," Mahncke says. "I really felt that my supervisors saw me as the future generation of engineers, either at Lincoln Lab or just in the broader industry."&lt;/p&gt;&lt;p&gt;Says Madeline Miller, Mahncke's internship supervisor: "Ivy's internship coincided with a rigorous series of field tests at the end of an ambitious program. We figuratively threw her right in the water, and she not only floated, but played an integral part in our program's ability to hit several reach goals."&lt;/p&gt;&lt;p&gt;Lincoln Laboratory's summer research program runs from mid-May to August. Applications are now open.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Video by Tim Briggs/MIT Lincoln Laboratory | 2 minutes, 59 seconds&lt;/em&gt;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/Ivy-Mahncke-Lincoln-Laboratory-00.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During a summer internship at MIT Lincoln Laboratory, Ivy Mahncke, an undergraduate student of robotics engineering at Olin College of Engineering, took a hands-on approach to testing algorithms for underwater navigation. She first discovered her love for working with underwater robotics as an intern at the Woods Hole Oceanographic Institution in 2024. Drawn by the chance to tackle new problems and cutting-edge algorithm development, Mahncke began an internship with Lincoln Laboratory's Advanced Undersea Systems and Technology Group in 2025.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Mahncke spent the summer developing and troubleshooting an algorithm that would help a human diver and robotic vehicle collaboratively navigate underwater. The lack of traditional localization aids — such as the Global Positioning System, or GPS — in an underwater environment posed challenges for navigation that Mahncke and her mentors sought to overcome. Her work in the laboratory culminated in field tests of the algorithm on an operational underwater vehicle. Accompanying group staff to field test sites in the Atlantic Ocean, Charles River, and Lake Superior, Mahncke had the opportunity see her software in action in the real world.&lt;/p&gt;&lt;p&gt;"One of the lead engineers on the project had split off to go do other work. And she said, 'Here's my laptop. Here are the things that you need to do. I trust you to go do them.' And so I got to be out on the water as not just an extra pair of hands, but as one of the lead field testers," Mahncke says. "I really felt that my supervisors saw me as the future generation of engineers, either at Lincoln Lab or just in the broader industry."&lt;/p&gt;&lt;p&gt;Says Madeline Miller, Mahncke's internship supervisor: "Ivy's internship coincided with a rigorous series of field tests at the end of an ambitious program. We figuratively threw her right in the water, and she not only floated, but played an integral part in our program's ability to hit several reach goals."&lt;/p&gt;&lt;p&gt;Lincoln Laboratory's summer research program runs from mid-May to August. Applications are now open.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Video by Tim Briggs/MIT Lincoln Laboratory | 2 minutes, 59 seconds&lt;/em&gt;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/featured-video-coding-underwater-robotics-0227</guid><pubDate>Fri, 27 Feb 2026 22:15:00 +0000</pubDate></item></channel></rss>