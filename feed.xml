<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 21 Aug 2025 01:43:08 +0000</lastBuildDate><item><title>FieldAI raises $405M to build universal robot brains (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/fieldai-raises-405m-to-build-universal-robot-brains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/FieldAI-Robot1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;FieldAI, an Irvine, California-based robotics startup, has raised $405 million across multiple previously undisclosed rounds to develop what it calls “foundational embodied AI models” — essentially robot brains designed to help everything from humanoids to quadrupeds to self-driving cars adapt to new environments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced the funding Wednesday; the most recent round raised $314 million in August and was co-led by Bezos Expeditions, Prysm, and Temasek. FieldAI’s other backers include Khosla Ventures, Intel Capital, and Canaan Partners, among others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Unlike traditional AI that processes text or images, embodied AI refers to AI that controls physical robots moving through real-world environments. FieldAI builds “Field Foundation Models,” which are general-purpose embodied AI models rooted in physics. This approach gives robots the ability to quickly learn and adapt to new environments while being conscious of risk, FieldAI founder and CEO Ali Agha told TechCrunch in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The mission is to build a single robot brain that can generalize across different robot types and a diverse set of environments,” Agha said. “To get there, you need to manage risk and safety as you go to these new environments. And that has been a fundamental gap in robotics, that traditional models and traditional approaches were never designed to manage that risk and safety.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agha said the key to getting robots to be able to safely learn in new environments is to add a layer of physics into these AI models. This addition gives robots a second set of information to pull from to make decisions —&amp;nbsp;especially in a new environment —&amp;nbsp;as opposed to just reacting to whatever a model says to do next as traditional LLMs do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that while a small amount of AI hallucination isn’t detrimental in certain circumstances, it can be for robots working in dangerous environments or alongside people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Suddenly you start to have that sense of, how much I know, and if I don’t know something, or if I’m making a decision, how confident I am in it,” Agha said. “Once [the] network starts getting access to that, it starts making much safer decisions. Not just this spits out that, ‘Hey, here’s the next sort of an action,’ but it tells you how confident it is, and you as a customer can define this risk threshold, and [the] robot will be reactive to that.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Agha has been working on this idea for decades across various roles at places ranging from NASA to Massachusetts Institute of Technology (MIT). He decided to launch FieldAI when he achieved a technological breakthrough that allowed one robot brain to work across different types of robots performing both the same and individual actions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching the company in 2023, FieldAI has secured contracts across industries including construction, energy, and urban delivery. The company declined to disclose any customers by name.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The funding will support research and development while helping the company ramp up production to deploy its models to its customers and to further expand its reach abroad.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agha compares FieldAI’s approach to human evolution. “You evolve to be able to do various different tasks in different environments, and you have the ability to rapidly learn, [and] we believe that is a necessity in robotics. Yes, definitely you can optimize for one specific use case, but that is not the market we are going after.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/FieldAI-Robot1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;FieldAI, an Irvine, California-based robotics startup, has raised $405 million across multiple previously undisclosed rounds to develop what it calls “foundational embodied AI models” — essentially robot brains designed to help everything from humanoids to quadrupeds to self-driving cars adapt to new environments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced the funding Wednesday; the most recent round raised $314 million in August and was co-led by Bezos Expeditions, Prysm, and Temasek. FieldAI’s other backers include Khosla Ventures, Intel Capital, and Canaan Partners, among others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Unlike traditional AI that processes text or images, embodied AI refers to AI that controls physical robots moving through real-world environments. FieldAI builds “Field Foundation Models,” which are general-purpose embodied AI models rooted in physics. This approach gives robots the ability to quickly learn and adapt to new environments while being conscious of risk, FieldAI founder and CEO Ali Agha told TechCrunch in an interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The mission is to build a single robot brain that can generalize across different robot types and a diverse set of environments,” Agha said. “To get there, you need to manage risk and safety as you go to these new environments. And that has been a fundamental gap in robotics, that traditional models and traditional approaches were never designed to manage that risk and safety.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Agha said the key to getting robots to be able to safely learn in new environments is to add a layer of physics into these AI models. This addition gives robots a second set of information to pull from to make decisions —&amp;nbsp;especially in a new environment —&amp;nbsp;as opposed to just reacting to whatever a model says to do next as traditional LLMs do.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that while a small amount of AI hallucination isn’t detrimental in certain circumstances, it can be for robots working in dangerous environments or alongside people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Suddenly you start to have that sense of, how much I know, and if I don’t know something, or if I’m making a decision, how confident I am in it,” Agha said. “Once [the] network starts getting access to that, it starts making much safer decisions. Not just this spits out that, ‘Hey, here’s the next sort of an action,’ but it tells you how confident it is, and you as a customer can define this risk threshold, and [the] robot will be reactive to that.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Agha has been working on this idea for decades across various roles at places ranging from NASA to Massachusetts Institute of Technology (MIT). He decided to launch FieldAI when he achieved a technological breakthrough that allowed one robot brain to work across different types of robots performing both the same and individual actions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching the company in 2023, FieldAI has secured contracts across industries including construction, energy, and urban delivery. The company declined to disclose any customers by name.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The funding will support research and development while helping the company ramp up production to deploy its models to its customers and to further expand its reach abroad.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Agha compares FieldAI’s approach to human evolution. “You evolve to be able to do various different tasks in different environments, and you have the ability to rapidly learn, [and] we believe that is a necessity in robotics. Yes, definitely you can optimize for one specific use case, but that is not the market we are going after.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/fieldai-raises-405m-to-build-universal-robot-brains/</guid><pubDate>Wed, 20 Aug 2025 14:00:00 +0000</pubDate></item><item><title>Thousands of Grok chats are now searchable on Google (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/thousands-of-grok-chats-are-now-searchable-on-google/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2203472418.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hundreds of thousands of conversations that users had with Elon Musk’s xAI chatbot Grok are easily accessible through Google Search, reports Forbes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whenever a Grok user clicks the “share” button on a conversation with the chatbot, it creates a unique URL that the user can use to share the conversation via email, text, or on social media. According to Forbes, those URLs are being indexed by search engines like Google, Bing, and DuckDuckGo, which in turn lets anyone look up those conversations on the web.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users of Meta‘s and OpenAI‘s chatbots were recently affected by a similar problem, and like those cases, the chats leaked by Grok give us a glimpse into users’ less-than-respectable desires — questions about how to hack crypto wallets; dirty chats with an explicit AI persona; and asking for instructions on cooking meth.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI’s rules prohibit the use of its bot to “promote critically harming human life” or developing “bioweapons, chemical weapons, or weapons of mass destruction,” though that obviously hasn’t stopped users from asking Grok for help with such things anyway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to conversations made accessible by Google, Grok gave users instructions on making fentanyl, listed various suicide methods, handed out bomb construction tips, and even provided a detailed plan for the assassination of Elon Musk.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI did not immediately respond to a request for comment. We’ve also asked when xAI began indexing Grok conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Late last month, ChatGPT users sounded the alarm that their chats were being indexed on Google, which OpenAI described as a “short-lived experiment.” In a post Musk quote-tweeted with the words “Grok ftw,” Grok explained that it had “no such sharing feature” and “prioritize[s] privacy.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2203472418.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Hundreds of thousands of conversations that users had with Elon Musk’s xAI chatbot Grok are easily accessible through Google Search, reports Forbes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whenever a Grok user clicks the “share” button on a conversation with the chatbot, it creates a unique URL that the user can use to share the conversation via email, text, or on social media. According to Forbes, those URLs are being indexed by search engines like Google, Bing, and DuckDuckGo, which in turn lets anyone look up those conversations on the web.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Users of Meta‘s and OpenAI‘s chatbots were recently affected by a similar problem, and like those cases, the chats leaked by Grok give us a glimpse into users’ less-than-respectable desires — questions about how to hack crypto wallets; dirty chats with an explicit AI persona; and asking for instructions on cooking meth.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI’s rules prohibit the use of its bot to “promote critically harming human life” or developing “bioweapons, chemical weapons, or weapons of mass destruction,” though that obviously hasn’t stopped users from asking Grok for help with such things anyway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to conversations made accessible by Google, Grok gave users instructions on making fentanyl, listed various suicide methods, handed out bomb construction tips, and even provided a detailed plan for the assassination of Elon Musk.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI did not immediately respond to a request for comment. We’ve also asked when xAI began indexing Grok conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Late last month, ChatGPT users sounded the alarm that their chats were being indexed on Google, which OpenAI described as a “short-lived experiment.” In a post Musk quote-tweeted with the words “Grok ftw,” Grok explained that it had “no such sharing feature” and “prioritize[s] privacy.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;rebecca.bellan@techcrunch.com&amp;nbsp;and Maxwell Zeff at&amp;nbsp;maxwell.zeff@techcrunch.com. For secure communication, you can contact us via Signal at&amp;nbsp;@rebeccabellan.491 and&amp;nbsp;@mzeff.88.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/thousands-of-grok-chats-are-now-searchable-on-google/</guid><pubDate>Wed, 20 Aug 2025 14:04:43 +0000</pubDate></item><item><title>Dex is an AI-powered camera device that helps children learn new languages (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/dex-is-an-ai-powered-camera-device-that-helps-children-learn-new-languages/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Three parents — Reni Cao, Xiao Zhang, and Susan Rosenthal — were worried about their children’s screen time, so they left their tech jobs to create a product that encourages children to engage with the real world while also helping them learn a new language. Their move has paid off, as the company recently raised $4.8 million in funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly launched gadget is called Dex and resembles a high-tech magnifying glass with a camera lens on one side and a touchscreen on the other. When kids use the device to take pictures of objects, the AI utilizes image recognition technology to identify the object and translate the word into the selected language. It also features interactive story lessons and games.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While kid-focused language learning apps like Duolingo Kids exist, Dex argues that it takes a more engaging approach that emphasizes hands-on experiences, allowing children to immerse themselves in the language.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re trying to teach authentic language in the real world in a way that’s interactive,” Cao told TechCrunch. “The kids are not only listening or doing what they are told to do, but rather, they are actually thinking, creating, interacting, running around, and just being curious about things, and acquire the necessary language associated with those concepts and objects.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is designed for kids ages 3 to 8 years old and currently supports Chinese, French, German, Hindi, Italian, Japanese, Korean, and Spanish. It also offers support for 34 dialects, including Egyptian Arabic, Taiwanese Mandarin, and Mexican Spanish.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to object recognition, Dex features a library of interactive stories that encourage children to actively participate in the narrative. As the story unfolds, kids are prompted to respond, such as greeting characters in the language they are learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device comes with a dedicated app for parents to see a detailed overview of their child’s progress, including the vocabulary words they’ve learned, the stories they’ve engaged with, and the number of consecutive days they’ve used Dex.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038093" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/dex-app-screenshot.jpeg?w=345" width="345" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Dex is currently developing a feature that allows kids to ask an AI chatbot questions and engage in free-form conversations. This feature is already available to some testers, but the company admits it isn’t ready for a wider rollout. Parents might also be cautious about introducing AI chatbots to their children. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing of Dex, we had concerns about the possibility of a child learning inappropriate words. Cao assured us that “rigid safety prompts” are included whenever the large language model is used across vision, reasoning, and text-to-speech. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said, “We have an always-on safety agent that evaluates conversations in real time and filters conversations with a safe stop word list. The agent will suppress conversation if any of the stop words are mentioned, including but not limited to those related to sexuality, religion, politics, etc. Parents will soon be able to further add to personalized stop-word lists.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, it said that the AI is trained using vocabulary standards similar to those found in Britannica Kids and other children’s encyclopedias.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In our testing, the AI successfully ignored topics related to nudity. However, it did recognize and accurately translate the term “gun,” something parents should consider when purchasing the device. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to our findings, Cao told us, “Regulation-wise, I’m not worried, but I do think this presents a concern, especially among [some] parents.” He added that these concerns have pushed the company to soon introduce an option in settings to filter out specific words, such as guns, cigarettes, vape pens, fireworks, marijuana, and beer bottles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex also has a zero data retention policy. While this means there’s no risk of sensitive or personal images being stored, one downside could be that parents are left in the dark about the type of content their kids may be capturing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is also actively working toward obtaining COPPA Safe Harbor status, which would make it compliant with the Children’s Online Privacy Protection Act.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Dex founders Reni Cao (CEO), Charlie Zhang (CTO), and Susan Rosenthal (Head of Ops)" class="wp-image-3038091" height="453" src="https://techcrunch.com/wp-content/uploads/2025/08/Dex-founders.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Dex founders Reni Cao (CEO), Xiao Zhang (CTO), and Susan Rosenthal (Head of Ops)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The funding round was led by Parable, with participation from Eduardo Vivas (Curated co-founder), UpscaleX, ClayVC, and EmbeddingVC. Notable angel investors include Pinterest founder Ben Silbermann, Lilian Weng, who is the former head of safety at OpenAI, and Richard Wong (ex-Coursera).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device is priced at $250, which feels steep for a product designed for children. However, Dex positions itself as a more affordable alternative to hiring a tutor, which can charge up to $80 per hour, or attending a language immersion school, which can cost several hundred to even thousands of dollars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex says that hundreds of families have already purchased the device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated after publication.&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Three parents — Reni Cao, Xiao Zhang, and Susan Rosenthal — were worried about their children’s screen time, so they left their tech jobs to create a product that encourages children to engage with the real world while also helping them learn a new language. Their move has paid off, as the company recently raised $4.8 million in funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The newly launched gadget is called Dex and resembles a high-tech magnifying glass with a camera lens on one side and a touchscreen on the other. When kids use the device to take pictures of objects, the AI utilizes image recognition technology to identify the object and translate the word into the selected language. It also features interactive story lessons and games.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While kid-focused language learning apps like Duolingo Kids exist, Dex argues that it takes a more engaging approach that emphasizes hands-on experiences, allowing children to immerse themselves in the language.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re trying to teach authentic language in the real world in a way that’s interactive,” Cao told TechCrunch. “The kids are not only listening or doing what they are told to do, but rather, they are actually thinking, creating, interacting, running around, and just being curious about things, and acquire the necessary language associated with those concepts and objects.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is designed for kids ages 3 to 8 years old and currently supports Chinese, French, German, Hindi, Italian, Japanese, Korean, and Spanish. It also offers support for 34 dialects, including Egyptian Arabic, Taiwanese Mandarin, and Mexican Spanish.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to object recognition, Dex features a library of interactive stories that encourage children to actively participate in the narrative. As the story unfolds, kids are prompted to respond, such as greeting characters in the language they are learning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device comes with a dedicated app for parents to see a detailed overview of their child’s progress, including the vocabulary words they’ve learned, the stories they’ve engaged with, and the number of consecutive days they’ve used Dex.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038093" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/dex-app-screenshot.jpeg?w=345" width="345" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, Dex is currently developing a feature that allows kids to ask an AI chatbot questions and engage in free-form conversations. This feature is already available to some testers, but the company admits it isn’t ready for a wider rollout. Parents might also be cautious about introducing AI chatbots to their children. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During our testing of Dex, we had concerns about the possibility of a child learning inappropriate words. Cao assured us that “rigid safety prompts” are included whenever the large language model is used across vision, reasoning, and text-to-speech. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He said, “We have an always-on safety agent that evaluates conversations in real time and filters conversations with a safe stop word list. The agent will suppress conversation if any of the stop words are mentioned, including but not limited to those related to sexuality, religion, politics, etc. Parents will soon be able to further add to personalized stop-word lists.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, it said that the AI is trained using vocabulary standards similar to those found in Britannica Kids and other children’s encyclopedias.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In our testing, the AI successfully ignored topics related to nudity. However, it did recognize and accurately translate the term “gun,” something parents should consider when purchasing the device. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In response to our findings, Cao told us, “Regulation-wise, I’m not worried, but I do think this presents a concern, especially among [some] parents.” He added that these concerns have pushed the company to soon introduce an option in settings to filter out specific words, such as guns, cigarettes, vape pens, fireworks, marijuana, and beer bottles.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex also has a zero data retention policy. While this means there’s no risk of sensitive or personal images being stored, one downside could be that parents are left in the dark about the type of content their kids may be capturing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex is also actively working toward obtaining COPPA Safe Harbor status, which would make it compliant with the Children’s Online Privacy Protection Act.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Dex founders Reni Cao (CEO), Charlie Zhang (CTO), and Susan Rosenthal (Head of Ops)" class="wp-image-3038091" height="453" src="https://techcrunch.com/wp-content/uploads/2025/08/Dex-founders.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Dex founders Reni Cao (CEO), Xiao Zhang (CTO), and Susan Rosenthal (Head of Ops)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Dex&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The funding round was led by Parable, with participation from Eduardo Vivas (Curated co-founder), UpscaleX, ClayVC, and EmbeddingVC. Notable angel investors include Pinterest founder Ben Silbermann, Lilian Weng, who is the former head of safety at OpenAI, and Richard Wong (ex-Coursera).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The device is priced at $250, which feels steep for a product designed for children. However, Dex positions itself as a more affordable alternative to hiring a tutor, which can charge up to $80 per hour, or attending a language immersion school, which can cost several hundred to even thousands of dollars.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dex says that hundreds of families have already purchased the device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated after publication.&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/dex-is-an-ai-powered-camera-device-that-helps-children-learn-new-languages/</guid><pubDate>Wed, 20 Aug 2025 14:19:54 +0000</pubDate></item><item><title>Google Cloud unveils AI ally for security teams (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-cloud-unveils-ai-ally-for-security-teams/</link><description>&lt;p&gt;Google Cloud believes the answer to overworked security teams isn’t just more tools, but an AI-powered ally.&lt;/p&gt;&lt;p&gt;At its Security Summit 2025, Google laid out its vision for a future where AI frees up human security experts from tedious work to focus on what matters most.&lt;/p&gt;&lt;p&gt;The central idea is to use AI to defend your organisation while securing your own AI initiatives from attack. As businesses increasingly rely on AI agents, these agents themselves become a new frontier for security concerns.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-securing-the-ai-ecosystem"&gt;Securing the AI ecosystem&lt;/h3&gt;&lt;p&gt;Before AI can become a trusted defender, its own environment must be secure. To this end, Google Cloud is enhancing its AI Protection solution within the Security Command Center.&lt;/p&gt;&lt;p&gt;New capabilities, arriving soon in preview, will automatically discover all the AI agents and servers in your environment. This will give security teams a clear view of their entire AI agent ecosystem, helping them to spot vulnerabilities, misconfigurations, and risky interactions.&lt;/p&gt;&lt;p&gt;Real-time protection is also getting a boost. Model Armor’s in-line protection is being extended to prompts and responses within Agentspace, helping to block threats like prompt injection and data leaks as they happen.&lt;/p&gt;&lt;p&gt;To ensure AI agents are always playing by the rules, new posture controls will help them stick to company security policies. And with new threat detections powered by intelligence from Mandiant and Google Cloud, security teams can now better spot and respond to unusual or suspicious behaviour from their AI assets.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rise-of-the-agentic-soc"&gt;Rise of the agentic SOC&lt;/h3&gt;&lt;p&gt;Perhaps the most forward-looking announcement is Google’s vision for an “agentic security operations centre (SOC)”. Imagine a system where AI agents collaborate to manage threats, automate alert investigations, and even help engineers create new detections to fill security gaps.&lt;/p&gt;&lt;p&gt;The first step in this direction is the new Alert Investigation agent, which is now in preview. This tool acts like a junior analyst, autonomously looking into security events, analysing command-line activity, and mapping out process trees based on the proven methods of Mandiant’s frontline experts. The agent provides its verdict on alerts and suggests next steps for human analysts, promising to cut down on manual work and speed up response times.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-security-built-on-google-cloud-s-unified-foundation"&gt;AI security built on Google Cloud’s unified foundation&lt;/h3&gt;&lt;p&gt;In Google Security Operations, the new SecOps Labs gives users early access to powerful capabilities, many of which are powered by Gemini AI. New dashboards that bring together security orchestration, automation, and response (SOAR) data are also now generally available, giving a clearer picture of an organisation’s security posture.&lt;/p&gt;&lt;p&gt;The platform’s foundation, the Trusted Cloud, is also receiving upgrades:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Compliance and risk:&lt;/strong&gt; A new Compliance Manager simplifies the complex world of audits and policy enforcement, while new Risk Reports use virtual red team technology to find security gaps that attackers could exploit.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Smarter access control:&lt;/strong&gt; The tedious task of granting permissions gets an AI assist with the new IAM role picker, now in preview. You can simply describe what a person or service needs to do, and Gemini will recommend the most secure, least-permissive role. To guard against account takeovers, re-authentication will now be triggered for highly sensitive actions.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Expanded data and network security:&lt;/strong&gt; Sensitive Data Protection has been expanded to cover AI tools like Vertex AI, and Cloud NGFW now helps apply Zero Trust principles to high-performance computing workloads, including AI.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By embedding AI into the core of its offerings, Google Cloud is working to create a foundation where security enables business goals and helps defenders tackle the challenges of a new era.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Ameer Basheer)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Why security chiefs demand urgent regulation of AI like DeepSeek&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google Cloud believes the answer to overworked security teams isn’t just more tools, but an AI-powered ally.&lt;/p&gt;&lt;p&gt;At its Security Summit 2025, Google laid out its vision for a future where AI frees up human security experts from tedious work to focus on what matters most.&lt;/p&gt;&lt;p&gt;The central idea is to use AI to defend your organisation while securing your own AI initiatives from attack. As businesses increasingly rely on AI agents, these agents themselves become a new frontier for security concerns.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-securing-the-ai-ecosystem"&gt;Securing the AI ecosystem&lt;/h3&gt;&lt;p&gt;Before AI can become a trusted defender, its own environment must be secure. To this end, Google Cloud is enhancing its AI Protection solution within the Security Command Center.&lt;/p&gt;&lt;p&gt;New capabilities, arriving soon in preview, will automatically discover all the AI agents and servers in your environment. This will give security teams a clear view of their entire AI agent ecosystem, helping them to spot vulnerabilities, misconfigurations, and risky interactions.&lt;/p&gt;&lt;p&gt;Real-time protection is also getting a boost. Model Armor’s in-line protection is being extended to prompts and responses within Agentspace, helping to block threats like prompt injection and data leaks as they happen.&lt;/p&gt;&lt;p&gt;To ensure AI agents are always playing by the rules, new posture controls will help them stick to company security policies. And with new threat detections powered by intelligence from Mandiant and Google Cloud, security teams can now better spot and respond to unusual or suspicious behaviour from their AI assets.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-rise-of-the-agentic-soc"&gt;Rise of the agentic SOC&lt;/h3&gt;&lt;p&gt;Perhaps the most forward-looking announcement is Google’s vision for an “agentic security operations centre (SOC)”. Imagine a system where AI agents collaborate to manage threats, automate alert investigations, and even help engineers create new detections to fill security gaps.&lt;/p&gt;&lt;p&gt;The first step in this direction is the new Alert Investigation agent, which is now in preview. This tool acts like a junior analyst, autonomously looking into security events, analysing command-line activity, and mapping out process trees based on the proven methods of Mandiant’s frontline experts. The agent provides its verdict on alerts and suggests next steps for human analysts, promising to cut down on manual work and speed up response times.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-ai-security-built-on-google-cloud-s-unified-foundation"&gt;AI security built on Google Cloud’s unified foundation&lt;/h3&gt;&lt;p&gt;In Google Security Operations, the new SecOps Labs gives users early access to powerful capabilities, many of which are powered by Gemini AI. New dashboards that bring together security orchestration, automation, and response (SOAR) data are also now generally available, giving a clearer picture of an organisation’s security posture.&lt;/p&gt;&lt;p&gt;The platform’s foundation, the Trusted Cloud, is also receiving upgrades:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Compliance and risk:&lt;/strong&gt; A new Compliance Manager simplifies the complex world of audits and policy enforcement, while new Risk Reports use virtual red team technology to find security gaps that attackers could exploit.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Smarter access control:&lt;/strong&gt; The tedious task of granting permissions gets an AI assist with the new IAM role picker, now in preview. You can simply describe what a person or service needs to do, and Gemini will recommend the most secure, least-permissive role. To guard against account takeovers, re-authentication will now be triggered for highly sensitive actions.&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Expanded data and network security:&lt;/strong&gt; Sensitive Data Protection has been expanded to cover AI tools like Vertex AI, and Cloud NGFW now helps apply Zero Trust principles to high-performance computing workloads, including AI.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;By embedding AI into the core of its offerings, Google Cloud is working to create a foundation where security enables business goals and helps defenders tackle the challenges of a new era.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Ameer Basheer)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Why security chiefs demand urgent regulation of AI like DeepSeek&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-cloud-unveils-ai-ally-for-security-teams/</guid><pubDate>Wed, 20 Aug 2025 15:21:44 +0000</pubDate></item><item><title>MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called MindJourney, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While VLMs&amp;nbsp;are strong&amp;nbsp;at identifying objects in&amp;nbsp;static&amp;nbsp;images,&amp;nbsp;they struggle to&amp;nbsp;interpret&amp;nbsp;the interactive 3D world behind 2D images.&amp;nbsp;This&amp;nbsp;gap shows up&amp;nbsp;in spatial&amp;nbsp;questions&amp;nbsp;like&amp;nbsp;“If I sit on the couch&amp;nbsp;that is on my right&amp;nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&amp;nbsp;interpret&amp;nbsp;its&amp;nbsp;position and movement through space.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;People&amp;nbsp;overcome this challenge by mentally exploring a space,&amp;nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&amp;nbsp;MindJourney&amp;nbsp;applies the same process&amp;nbsp;to&amp;nbsp;AI agents,&amp;nbsp;letting&amp;nbsp;them roam a virtual&amp;nbsp;space before answering spatial questions.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space"&gt;How&amp;nbsp;MindJourney&amp;nbsp;navigates 3D space&lt;/h2&gt;



&lt;p&gt;To perform this type of spatial navigation,&amp;nbsp;MindJourney&amp;nbsp;uses a&amp;nbsp;&lt;em&gt;world model&lt;/em&gt;—in this case,&amp;nbsp;a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left of right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.&lt;/p&gt;



&lt;p&gt;At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user’s question.&lt;/p&gt;



&lt;p&gt;These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM’s spatial interpretation through generated observations when encountering new challenges.&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;To make its search through&amp;nbsp;a simulated&amp;nbsp;space both effective and efficient,&amp;nbsp;MindJourney&amp;nbsp;uses a &lt;em&gt;spatial beam search&lt;/em&gt;—an&amp;nbsp;algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables&amp;nbsp;MindJourney&amp;nbsp;to gather strong supporting evidence.&amp;nbsp;This process is illustrated in Figure 2.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MindJourney pipeline diagram" class="wp-image-1147897" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p class="has-text-align-left"&gt;By iterating through&amp;nbsp;simulation,&amp;nbsp;evaluation, and integration,&amp;nbsp;MindJourney&amp;nbsp;can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training.&amp;nbsp;On&amp;nbsp;the&amp;nbsp;Spatial Aptitude Training (SAT)&amp;nbsp;benchmark,&amp;nbsp;it improved the accuracy of&amp;nbsp;VLMs&amp;nbsp;by&amp;nbsp;8%&amp;nbsp;over&amp;nbsp;their&amp;nbsp;baseline&amp;nbsp;performance.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="building-smarter-agents"&gt;Building&amp;nbsp;smarter agents&amp;nbsp;&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;MindJourney&amp;nbsp;showed&amp;nbsp;strong performance&amp;nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&amp;nbsp;improved&amp;nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&amp;nbsp;capabilities&amp;nbsp;of VLMs, create a more complete spatial capability&amp;nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&amp;nbsp;interpret&amp;nbsp;the physical world&amp;nbsp;more accurately.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of&amp;nbsp;interpreting&amp;nbsp;and acting in real-world environments. This opens the way to&amp;nbsp;possible&amp;nbsp;applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By converting systems that simply describe static images into active agents that continually evaluate where to look next,&amp;nbsp;MindJourney&amp;nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&amp;nbsp;potentially&amp;nbsp;reducing wear, energy use, and collision risk.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, we plan to extend the framework to&amp;nbsp;use&amp;nbsp;world models that&amp;nbsp;not only&amp;nbsp;predict&amp;nbsp;new viewpoints&amp;nbsp;but also forecast&amp;nbsp;how the scene might change over time.&amp;nbsp;We envision&amp;nbsp;MindJourney&amp;nbsp;working&amp;nbsp;alongside VLMs that interpret&amp;nbsp;those predictions&amp;nbsp;and use&amp;nbsp;them to&amp;nbsp;plan&amp;nbsp;what to do&amp;nbsp;next. This&amp;nbsp;enhancement could enable&amp;nbsp;agents&amp;nbsp;more accurately&amp;nbsp;interpret&amp;nbsp;spatial relationships and physical dynamics, helping them to operate effectively&amp;nbsp;in changing environments.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called MindJourney, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While VLMs&amp;nbsp;are strong&amp;nbsp;at identifying objects in&amp;nbsp;static&amp;nbsp;images,&amp;nbsp;they struggle to&amp;nbsp;interpret&amp;nbsp;the interactive 3D world behind 2D images.&amp;nbsp;This&amp;nbsp;gap shows up&amp;nbsp;in spatial&amp;nbsp;questions&amp;nbsp;like&amp;nbsp;“If I sit on the couch&amp;nbsp;that is on my right&amp;nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&amp;nbsp;interpret&amp;nbsp;its&amp;nbsp;position and movement through space.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;People&amp;nbsp;overcome this challenge by mentally exploring a space,&amp;nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&amp;nbsp;MindJourney&amp;nbsp;applies the same process&amp;nbsp;to&amp;nbsp;AI agents,&amp;nbsp;letting&amp;nbsp;them roam a virtual&amp;nbsp;space before answering spatial questions.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space"&gt;How&amp;nbsp;MindJourney&amp;nbsp;navigates 3D space&lt;/h2&gt;



&lt;p&gt;To perform this type of spatial navigation,&amp;nbsp;MindJourney&amp;nbsp;uses a&amp;nbsp;&lt;em&gt;world model&lt;/em&gt;—in this case,&amp;nbsp;a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left of right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.&lt;/p&gt;



&lt;p&gt;At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user’s question.&lt;/p&gt;



&lt;p&gt;These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM’s spatial interpretation through generated observations when encountering new challenges.&lt;em&gt;&amp;nbsp;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;







&lt;p&gt;To make its search through&amp;nbsp;a simulated&amp;nbsp;space both effective and efficient,&amp;nbsp;MindJourney&amp;nbsp;uses a &lt;em&gt;spatial beam search&lt;/em&gt;—an&amp;nbsp;algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables&amp;nbsp;MindJourney&amp;nbsp;to gather strong supporting evidence.&amp;nbsp;This process is illustrated in Figure 2.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MindJourney pipeline diagram" class="wp-image-1147897" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" width="1400" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p class="has-text-align-left"&gt;By iterating through&amp;nbsp;simulation,&amp;nbsp;evaluation, and integration,&amp;nbsp;MindJourney&amp;nbsp;can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training.&amp;nbsp;On&amp;nbsp;the&amp;nbsp;Spatial Aptitude Training (SAT)&amp;nbsp;benchmark,&amp;nbsp;it improved the accuracy of&amp;nbsp;VLMs&amp;nbsp;by&amp;nbsp;8%&amp;nbsp;over&amp;nbsp;their&amp;nbsp;baseline&amp;nbsp;performance.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="building-smarter-agents"&gt;Building&amp;nbsp;smarter agents&amp;nbsp;&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;MindJourney&amp;nbsp;showed&amp;nbsp;strong performance&amp;nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&amp;nbsp;improved&amp;nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&amp;nbsp;capabilities&amp;nbsp;of VLMs, create a more complete spatial capability&amp;nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&amp;nbsp;interpret&amp;nbsp;the physical world&amp;nbsp;more accurately.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of&amp;nbsp;interpreting&amp;nbsp;and acting in real-world environments. This opens the way to&amp;nbsp;possible&amp;nbsp;applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;By converting systems that simply describe static images into active agents that continually evaluate where to look next,&amp;nbsp;MindJourney&amp;nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&amp;nbsp;potentially&amp;nbsp;reducing wear, energy use, and collision risk.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Looking ahead, we plan to extend the framework to&amp;nbsp;use&amp;nbsp;world models that&amp;nbsp;not only&amp;nbsp;predict&amp;nbsp;new viewpoints&amp;nbsp;but also forecast&amp;nbsp;how the scene might change over time.&amp;nbsp;We envision&amp;nbsp;MindJourney&amp;nbsp;working&amp;nbsp;alongside VLMs that interpret&amp;nbsp;those predictions&amp;nbsp;and use&amp;nbsp;them to&amp;nbsp;plan&amp;nbsp;what to do&amp;nbsp;next. This&amp;nbsp;enhancement could enable&amp;nbsp;agents&amp;nbsp;more accurately&amp;nbsp;interpret&amp;nbsp;spatial relationships and physical dynamics, helping them to operate effectively&amp;nbsp;in changing environments.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>Google brings improved Gemini features to its new Pixel Buds (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/google-brings-improved-gemini-features-to-its-new-pixel-buds/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Similar to last year, the Made By Google event held on Wednesday showcased numerous Gemini features, including AI photo-taking and editing tools for the Pixel 10 series, along with enhancements for the Pixel Watch 4. So it was expected that Google would unveil Gemini functionalities for its latest earbuds—the Pixel Buds 2a and a revamped Pixel Buds Pro 2.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some key features showcased at the event include the introduction of active noise cancellation and an AI feature that minimizes background noise for the Pixel Buds 2a. Additionally, the Pro 2 earbuds will receive a major update that allows users to accept or decline calls and messages by shaking or nodding their heads.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036714" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBuds2a_Iris_BothBudsinCase.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-2a"&gt;Pixel Buds 2a&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a represent the second generation of the A-Series earbuds, which are the tech giant’s more affordable option in the earbud lineup.&amp;nbsp;They come with significant upgrades, making them almost up to par with the original Pixel Buds Pro.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the first time, the A-Series earbuds now include active noise cancellation, a significant upgrade from the previous version, which offered zero noise reduction. Now the Pixel Buds 2a can block disruptive external noises, allowing for clear conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another feature coming to the earbuds is Clear Calling, which uses Google AI to reduce background noise and works in conjunction with the earbuds’ wind-blocking mesh covers to eliminate distractions, ensuring that both the user and the person on the other end can hear each other well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the Pixel Buds 2a is getting Transparency Mode, a feature that was previously available only on the Pixel Buds Pro. This mode lets users hear their surroundings while still listening to audio, helping them stay aware of what’s going on around them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a also comes with Gemini hands-free support, allowing you to communicate with the AI assistant to help with tasks like finding nearby restaurants, getting directions, reading new emails, pausing songs, and more. You can go live with Gemini just by speaking, without needing to take out your phone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The battery life is also getting a major boost, exceeding that of the original A-Series by twofold.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In terms of design, these are the smallest and lightest A-Series earbuds to date, accompanied by a compact charging case. They are also IP54-rated, making them sweat- and water-resistant, ideal for workouts or unexpected rain.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new design feature that brings the budget-friendly earbuds closer in line with the Pixel Buds Pro is the twist stabilizer. This allows users to adjust the fit by twisting counterclockwise for a snug fit or clockwise for a more comfortable, looser fit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflecting the new features, the Pixel Buds 2a come with a higher price tag. They retail for $130, compared to their predecessor under $100, and are available in two new colors: Hazel and Iris.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036712" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBudsPro2_Moonstone_Front-Facing-Buds.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-pro-2"&gt;Pixel Buds Pro 2 &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2, first released last year, are also receiving significant upgrades.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As expected, these earbuds feature Gemini hands-free support, but they come with additional perks as well. Now you can accept or decline calls and texts by simply shaking or nodding your head. The earbuds will detect these gestures using built-in accelerometers and sensors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, when interacting with Gemini, you can now connect with other apps, such as adding items to a grocery list in Keep or scheduling events in Calendar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other notable features include Adaptive Audio, which automatically adjusts to the noise level around you to reduce distracting sounds in noisy settings while still helping you stay aware of your environment. Additionally, Loud Noise Protection automatically dampens abrupt loud sounds, protecting your hearing from loud noises, such as the siren of a nearby fire truck.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2 retail for $230 and come in a new Moonstone color. However, it’s important to note that most of these new features won’t be available; they’ll be introduced via a software update later in September. All existing Pro 2 users will get the new features. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers can preorder the Pixel Buds 2a and improved Pro 2 earbuds today. The Pro 2 will ship on August 28, whereas the Pixel Buds 2a won’t hit the shelves until October 9.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Similar to last year, the Made By Google event held on Wednesday showcased numerous Gemini features, including AI photo-taking and editing tools for the Pixel 10 series, along with enhancements for the Pixel Watch 4. So it was expected that Google would unveil Gemini functionalities for its latest earbuds—the Pixel Buds 2a and a revamped Pixel Buds Pro 2.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some key features showcased at the event include the introduction of active noise cancellation and an AI feature that minimizes background noise for the Pixel Buds 2a. Additionally, the Pro 2 earbuds will receive a major update that allows users to accept or decline calls and messages by shaking or nodding their heads.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036714" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBuds2a_Iris_BothBudsinCase.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-2a"&gt;Pixel Buds 2a&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a represent the second generation of the A-Series earbuds, which are the tech giant’s more affordable option in the earbud lineup.&amp;nbsp;They come with significant upgrades, making them almost up to par with the original Pixel Buds Pro.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the first time, the A-Series earbuds now include active noise cancellation, a significant upgrade from the previous version, which offered zero noise reduction. Now the Pixel Buds 2a can block disruptive external noises, allowing for clear conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another feature coming to the earbuds is Clear Calling, which uses Google AI to reduce background noise and works in conjunction with the earbuds’ wind-blocking mesh covers to eliminate distractions, ensuring that both the user and the person on the other end can hear each other well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the Pixel Buds 2a is getting Transparency Mode, a feature that was previously available only on the Pixel Buds Pro. This mode lets users hear their surroundings while still listening to audio, helping them stay aware of what’s going on around them.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds 2a also comes with Gemini hands-free support, allowing you to communicate with the AI assistant to help with tasks like finding nearby restaurants, getting directions, reading new emails, pausing songs, and more. You can go live with Gemini just by speaking, without needing to take out your phone.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The battery life is also getting a major boost, exceeding that of the original A-Series by twofold.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;In terms of design, these are the smallest and lightest A-Series earbuds to date, accompanied by a compact charging case. They are also IP54-rated, making them sweat- and water-resistant, ideal for workouts or unexpected rain.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new design feature that brings the budget-friendly earbuds closer in line with the Pixel Buds Pro is the twist stabilizer. This allows users to adjust the fit by twisting counterclockwise for a snug fit or clockwise for a more comfortable, looser fit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reflecting the new features, the Pixel Buds 2a come with a higher price tag. They retail for $130, compared to their predecessor under $100, and are available in two new colors: Hazel and Iris.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3036712" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/PixelBudsPro2_Moonstone_Front-Facing-Buds.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-pixel-buds-pro-2"&gt;Pixel Buds Pro 2 &lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2, first released last year, are also receiving significant upgrades.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As expected, these earbuds feature Gemini hands-free support, but they come with additional perks as well. Now you can accept or decline calls and texts by simply shaking or nodding your head. The earbuds will detect these gestures using built-in accelerometers and sensors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, when interacting with Gemini, you can now connect with other apps, such as adding items to a grocery list in Keep or scheduling events in Calendar.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Other notable features include Adaptive Audio, which automatically adjusts to the noise level around you to reduce distracting sounds in noisy settings while still helping you stay aware of your environment. Additionally, Loud Noise Protection automatically dampens abrupt loud sounds, protecting your hearing from loud noises, such as the siren of a nearby fire truck.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Pixel Buds Pro 2 retail for $230 and come in a new Moonstone color. However, it’s important to note that most of these new features won’t be available; they’ll be introduced via a software update later in September. All existing Pro 2 users will get the new features. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers can preorder the Pixel Buds 2a and improved Pro 2 earbuds today. The Pro 2 will ship on August 28, whereas the Pixel Buds 2a won’t hit the shelves until October 9.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/google-brings-improved-gemini-features-to-its-new-pixel-buds/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>You can now talk to Google Photos to make your edits (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/you-can-now-talk-to-google-photos-to-make-your-edits/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At Wednesday’s Made by Google event, the company announced new features in Google Photos that will allow users to ask the app to edit their pictures for them. The functionality will launch first on Pixel 10 devices in the U.S., allowing people to describe whatever edits they want to make to the photo by either voice or text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding support for C2PA Content Credentials in Google Photos. The Pixel 10 phones will be the first from Google to adopt this standard, which is designed to improve transparency around how images are made and whether AI is involved. On Pixel devices, C2PA is supported with the Camera app itself and in any photos taken with it, even if AI is not used.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038274" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new “edit by asking” feature in Google Photos leverages Gemini so you can ask for changes to a photo using natural language. For instance, you can say things like “remove the cars in the background,” or something less specific, like “restore this old photo,” and Google Photos will take action. The addition could help those who aren’t as tech-savvy or have a good understanding of editing tools to still make adjustments to improve their photos.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038275" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature can handle tasks like lighting adjustments and removing distractions from the images, as well as more creative edits, like changing the background or adding items to the photo. Google suggests you could use this to add sunglasses and a party hat to the photo’s subject, among other things, for example.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if you don’t know what to ask for, you can start with a request for help like “make it better,” and Google Photos will automatically make changes to the image. The app can also offer suggestions of what to fix, and it supports follow-up requests as you continue to fine-tune your edits. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038277" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-1.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the support for C2PA will come first to Pixel 10 devices and then will roll out gradually to Google Photos across iOS and Android in the weeks ahead. “Edit by asking” launches Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At Wednesday’s Made by Google event, the company announced new features in Google Photos that will allow users to ask the app to edit their pictures for them. The functionality will launch first on Pixel 10 devices in the U.S., allowing people to describe whatever edits they want to make to the photo by either voice or text.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also adding support for C2PA Content Credentials in Google Photos. The Pixel 10 phones will be the first from Google to adopt this standard, which is designed to improve transparency around how images are made and whether AI is involved. On Pixel devices, C2PA is supported with the Camera app itself and in any photos taken with it, even if AI is not used.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038274" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The new “edit by asking” feature in Google Photos leverages Gemini so you can ask for changes to a photo using natural language. For instance, you can say things like “remove the cars in the background,” or something less specific, like “restore this old photo,” and Google Photos will take action. The addition could help those who aren’t as tech-savvy or have a good understanding of editing tools to still make adjustments to improve their photos.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038275" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature can handle tasks like lighting adjustments and removing distractions from the images, as well as more creative edits, like changing the background or adding items to the photo. Google suggests you could use this to add sunglasses and a party hat to the photo’s subject, among other things, for example.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Even if you don’t know what to ask for, you can start with a request for help like “make it better,” and Google Photos will automatically make changes to the image. The app can also offer suggestions of what to fix, and it supports follow-up requests as you continue to fine-tune your edits. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038277" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/unnamed-1.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google says the support for C2PA will come first to Pixel 10 devices and then will roll out gradually to Google Photos across iOS and Android in the weeks ahead. “Edit by asking” launches Wednesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/you-can-now-talk-to-google-photos-to-make-your-edits/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>Harvard dropouts to launch ‘always on’ AI smart glasses that listen and record every conversation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/halo-ai-smart-glasses-rendering.jpeg?resize=1200,1078" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two former Harvard students are launching a pair of “always-on” AI-powered smart glasses that listen to, record, and transcribe every conversation and then display relevant information to the wearer in real time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our goal is to make glasses that make you super intelligent the moment you put them on,” said AnhPhu Nguyen, co-founder of Halo, a startup that’s developing the technology.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, as his co-founder Caine Ardayfio put it, the glasses “give you infinite memory.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The AI listens to every conversation you have and uses that knowledge to tell you what to say&amp;nbsp;… kinda like IRL Cluely,” Ardayfio told TechCrunch, referring to the startup that claims to help users “cheat” on everything from job interviews to school exams.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If somebody says a complex word or asks you a question, like, ‘What’s 37 to the third power?’ or something like that, then it’ll pop up on the glasses,” Ardayfio added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio and Nguyen have raised $1 million to develop the glasses, led by Pillar VC, with support from Soma Capital, Village Global, and Morningside Venture. The glasses will be priced at $249 and will be available for preorder starting Wednesday. Ardayfio called the glasses “the first real step towards vibe thinking.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two Ivy League dropouts, who have since moved into their own version of the Hacker Hostel in the San Francisco Bay Area, recently caused a stir after developing a facial-recognition app for Meta’s smart Ray-Ban glasses to prove that the tech could be used to dox people. As a potential early competitor to Meta’s smart glasses, Ardayfio said Meta, given its history of security and privacy scandals, had to rein in its product in ways that Halo can ultimately capitalize on.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Meta doesn’t have a great reputation for caring about user privacy, and for them to release something that’s always there with you — which obviously brings a ton of utility — is just a huge reputational risk for them that they probably won’t take before a startup does it at scale first,” Nguyen added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while Nguyen has a point, users may not yet have a good reason to trust the technology of a couple of college-aged students purporting to send people out into the world with covert recording equipment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta’s glasses have an indicator light when their cameras and microphones are watching and listening as a mechanism to warn others that they are being recorded, Ardayfio said that the Halo glasses, dubbed Halo X, do not have an external indicator to warn people of their customers’ recording.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“For the hardware we’re making, we want it to be discreet, like normal glasses,” said Ardayfio, who added that the glasses record every word, transcribe it, and then delete the audio file.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Privacy advocates are warning about the normalization of covert recording devices in public.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Small and discreet recording devices are not new,” Eva Galperin, the director of cybersecurity at the Electronic Frontier Foundation, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In some ways, this sounds like a variation on the microphone spy pen,” said Galperin. “But I think that normalizing the use of an always-on recording device, which in many circumstances would require the user to get the consent of everyone within recording distance, eats away at the expectation of privacy we have for our conversations in all kinds of spaces.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are several states in the U.S. that make it illegal to covertly record conversations without the other persons’ consent. Ardayfio said they are aware of this but that it is up to their customer to obtain consent before using the glasses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We trust our users to get consent if they are in a two-party consent state,” said Ardayfio, referring to the laws of a dozen U.S. states that require the consent of all recorded parties.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I would also be very concerned about where the recorded data is being kept, how it is being stored, and who has access to it,” Galperin added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio said Halo relies on Soniox for audio transcription, which claims to never store&amp;nbsp;recordings. Nguyen claimed when the finished product is released to customers, it will be end-to-end encrypted but provided no evidence of how this would work. He also noted that Halo is aiming to get SOC 2 compliance, which means it has been independently audited and demonstrates adequate protection of customer data. A date for the completed SOC 2 compliance was not provided.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the two students are not new to privacy-invasive controversial projects.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While still at Harvard last year, Ardayfio and Nguyen developed I-XRAY, a demo project that added facial-recognition capabilities to the Meta Ray-Ban smart glasses, demonstrating how easily the tech could be bolted onto a device not meant to identify people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo never released the code behind I-XRAY, but they did test the glasses on random passersby without consent. In a demo video, Ardayfio showed the glasses detecting faces and pulling up personal information of strangers within seconds. The video featured reactions of people who were doxed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with 404 Media, they acknowledged the risks: “Some dude could just find some girl’s home address on the train and just follow them home,” Nguyen told the tech news website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, Halo X glasses only have a display and a microphone, but no camera, although the two are exploring the possibility of adding it to a future model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users still need to have their smartphones handy to help power the glasses and get “real time info prompts and answers to questions,” per Nguyen. The glasses, which are manufactured by another company that the startup didn’t name, are tethered to an accompanying app on the owner’s phone, where the glasses essentially outsource the computing since they don’t have enough power to do it on the device itself.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the hood, the smart glasses use Google’s Gemini and Perplexity as its chatbot engine, according to the two co-founders. Gemini is better for math and reasoning, whereas they use Perplexity to scrape the internet, they said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During an interview, TechCrunch asked if their glasses knew when the next season of “The Witcher” would come out. Responding in a way reminiscent of C-3PO, Ardayfio said: “‘The Witcher’ season four will be released on Netflix in 2025, but there’s no exact date yet. Most sources expect it in the second half of 2025.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I don’t know if that’s correct,” he added.&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/halo-ai-smart-glasses-rendering.jpeg?resize=1200,1078" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Two former Harvard students are launching a pair of “always-on” AI-powered smart glasses that listen to, record, and transcribe every conversation and then display relevant information to the wearer in real time.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our goal is to make glasses that make you super intelligent the moment you put them on,” said AnhPhu Nguyen, co-founder of Halo, a startup that’s developing the technology.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Or, as his co-founder Caine Ardayfio put it, the glasses “give you infinite memory.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The AI listens to every conversation you have and uses that knowledge to tell you what to say&amp;nbsp;… kinda like IRL Cluely,” Ardayfio told TechCrunch, referring to the startup that claims to help users “cheat” on everything from job interviews to school exams.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If somebody says a complex word or asks you a question, like, ‘What’s 37 to the third power?’ or something like that, then it’ll pop up on the glasses,” Ardayfio added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio and Nguyen have raised $1 million to develop the glasses, led by Pillar VC, with support from Soma Capital, Village Global, and Morningside Venture. The glasses will be priced at $249 and will be available for preorder starting Wednesday. Ardayfio called the glasses “the first real step towards vibe thinking.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The two Ivy League dropouts, who have since moved into their own version of the Hacker Hostel in the San Francisco Bay Area, recently caused a stir after developing a facial-recognition app for Meta’s smart Ray-Ban glasses to prove that the tech could be used to dox people. As a potential early competitor to Meta’s smart glasses, Ardayfio said Meta, given its history of security and privacy scandals, had to rein in its product in ways that Halo can ultimately capitalize on.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Meta doesn’t have a great reputation for caring about user privacy, and for them to release something that’s always there with you — which obviously brings a ton of utility — is just a huge reputational risk for them that they probably won’t take before a startup does it at scale first,” Nguyen added.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And while Nguyen has a point, users may not yet have a good reason to trust the technology of a couple of college-aged students purporting to send people out into the world with covert recording equipment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Meta’s glasses have an indicator light when their cameras and microphones are watching and listening as a mechanism to warn others that they are being recorded, Ardayfio said that the Halo glasses, dubbed Halo X, do not have an external indicator to warn people of their customers’ recording.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“For the hardware we’re making, we want it to be discreet, like normal glasses,” said Ardayfio, who added that the glasses record every word, transcribe it, and then delete the audio file.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Privacy advocates are warning about the normalization of covert recording devices in public.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Small and discreet recording devices are not new,” Eva Galperin, the director of cybersecurity at the Electronic Frontier Foundation, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In some ways, this sounds like a variation on the microphone spy pen,” said Galperin. “But I think that normalizing the use of an always-on recording device, which in many circumstances would require the user to get the consent of everyone within recording distance, eats away at the expectation of privacy we have for our conversations in all kinds of spaces.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There are several states in the U.S. that make it illegal to covertly record conversations without the other persons’ consent. Ardayfio said they are aware of this but that it is up to their customer to obtain consent before using the glasses.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We trust our users to get consent if they are in a two-party consent state,” said Ardayfio, referring to the laws of a dozen U.S. states that require the consent of all recorded parties.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I would also be very concerned about where the recorded data is being kept, how it is being stored, and who has access to it,” Galperin added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ardayfio said Halo relies on Soniox for audio transcription, which claims to never store&amp;nbsp;recordings. Nguyen claimed when the finished product is released to customers, it will be end-to-end encrypted but provided no evidence of how this would work. He also noted that Halo is aiming to get SOC 2 compliance, which means it has been independently audited and demonstrates adequate protection of customer data. A date for the completed SOC 2 compliance was not provided.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the two students are not new to privacy-invasive controversial projects.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While still at Harvard last year, Ardayfio and Nguyen developed I-XRAY, a demo project that added facial-recognition capabilities to the Meta Ray-Ban smart glasses, demonstrating how easily the tech could be bolted onto a device not meant to identify people.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo never released the code behind I-XRAY, but they did test the glasses on random passersby without consent. In a demo video, Ardayfio showed the glasses detecting faces and pulling up personal information of strangers within seconds. The video featured reactions of people who were doxed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview with 404 Media, they acknowledged the risks: “Some dude could just find some girl’s home address on the train and just follow them home,” Nguyen told the tech news website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, Halo X glasses only have a display and a microphone, but no camera, although the two are exploring the possibility of adding it to a future model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users still need to have their smartphones handy to help power the glasses and get “real time info prompts and answers to questions,” per Nguyen. The glasses, which are manufactured by another company that the startup didn’t name, are tethered to an accompanying app on the owner’s phone, where the glasses essentially outsource the computing since they don’t have enough power to do it on the device itself.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Under the hood, the smart glasses use Google’s Gemini and Perplexity as its chatbot engine, according to the two co-founders. Gemini is better for math and reasoning, whereas they use Perplexity to scrape the internet, they said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During an interview, TechCrunch asked if their glasses knew when the next season of “The Witcher” would come out. Responding in a way reminiscent of C-3PO, Ardayfio said: “‘The Witcher’ season four will be released on Netflix in 2025, but there’s no exact date yet. Most sources expect it in the second half of 2025.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I don’t know if that’s correct,” he added.&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/</guid><pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate></item><item><title>Google doubles down on ‘AI phones’ with its Pixel 10 series (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/google-doubles-down-on-ai-phones-with-its-pixel-10-series/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With the launch of the new Pixel 10 series, Google is rushing ahead of Apple to deliver AI-powered smartphones to consumers. The devices, announced during Wednesday’s Made by Google event, come just weeks ahead of Apple’s expected iPhone 17 reveal, which promises to be more of the same — better cameras, possibly thinner devices, and new colors to choose from.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, meanwhile, has been rapidly integrating its AI platform into its devices. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, its Pixel 9 series added a number of AI features, like Gemini Live (Gemini’s voice mode), image-generation tools, call notes, searchable screenshots, and more. Since then, Google says that Gemini Live conversations have proven to be 5x longer than text-based conversations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3038318" height="1055" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live.jpg" width="1773" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the tech giant is rolling out even more AI-powered upgrades with the launch of its Pixel 10, including a Visual Overlays feature for the camera, a proactive “Magic Cue” feature, Camera Coach, Voice Translate for calls, an assistant-like “Take a Message” feature, Pixel Journal, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Combined, the updates allow Google to showcase what its latest AI technology can do when enhanced by its Tensor G5 processor, an upgrade to the company’s custom silicon designed for AI experiences and the first to run its newest Gemini Nano model. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Google announced that Gemini Live will gain a new audio model that will detect your tone — like whether you’re excited or concerned — and adjust its response accordingly.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038388" height="379" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the addition of a feature called Visual Overlays, Gemini Live will be able to see what you see through the lens of your camera and provide guidance by highlighting things on your screen. For example, while traveling in a foreign country, you could hold up your phone to see if the street signs around offer information about parking along the roadside.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038389" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature, Magic Cue, lets the AI be more proactive by offering contextual suggestions in real time, across apps like Gmail, Calendar, Messages, Screenshots, and others. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea of a more proactive interaction between people and Google technology is something the company has dreamed of for years, long before the AI era. In the early 2010s, for example, Google introduced an Android feature called Google Now that would pop up cards with real-time information related to your daily schedule or the time of day, like nearby restaurants at lunchtime, upcoming meetings, or flight details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Years later, Magic Cue is the AI-powered reintroduction of this feature, but one where it inserts itself into your everyday apps and interactions.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038365" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/magic-cue-airline.jpg?w=332" width="332" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google demonstrated how Magic Cue could suggest a restaurant to dine at with a friend, offering quick access to place a call to the restaurant to make a reservation. It could propose a reply to your friend with the reservation details or point you to your calendar to check your availability. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Magic Cue’s suggestions appear within the app you’re using and are wrapped with a rainbow-colored outline to differentiate them, as well as within Daily Hub, a personalized daily digest in your Discover feed. You can also tap on its suggestions to take action.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3038303" height="340" src="https://techcrunch.com/wp-content/uploads/2025/08/daily-hub.png?w=340" width="340" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Also similar to Google Now, Magic Cue will be able to surface reminders. But it goes a step further by popping up reminders and notifications more intuitively. For instance, it may remind you of errands you need to handle, like a return of an online order, suggest topics you may want to research, or recommend new playlists to stream. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, Magic Cue’s suggestions will be limited to select activities, like settling up a tab, adding events to your calendar, and showing the forecast for an upcoming trip in the weather app. Over time, Google will add other options and let you configure which data source the feature has access to. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple, it should be noted, is trying to do something similar by allowing users to speak to Siri to interact with and take action within their apps, but unfortunately, its AI-powered Siri has been delayed until 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another one of the more interesting additions in the Pixel 10 series is Camera Coach, an AI-powered assistant that aims to make you a better photographer. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038368" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach.jpg?w=322" width="322" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will be launching in preview with the new devices and uses Gemini models to offer suggestions about how to better frame and compose your shot. You can even choose a “get inspired” option that will suggest scenes you may not have considered.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038392" height="628" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the camera will now be able to recognize when you’re taking a group photo. The “Auto Best Take” feature activates and analyzes up to 150 images shot over several seconds to find the best one — whether that’s a shot you snapped yourself or one made by blending others together via AI.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038399" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Auto-best-take.jpg?w=374" width="374" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Then, with the AI-powered “Ask Photos” tool, you can edit the shot to do other things, like fix the lighting, change the framing, or remove an object from the photo by either speaking to or texting Photos’ AI assistant.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Pro devices, the Pro Res Zoom option will also use AI to allow you to “zoom” in on things like architecture and landscapes at 30x-60x or 30x-60x for animals and wildlife.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038397" height="511" src="https://techcrunch.com/wp-content/uploads/2025/08/pro-res.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Related to its enhanced use of AI in photography, Pixel 10 phones will also be the first to implement C2PA, a standard that establishes the origin and edits of digital content, which will help to identify when photos have been modified by AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new AI feature, Voice Translate, will use on-device AI to translate your phone call in real time in what sounds like each speaker’s own voice. This could be a potential game changer, particularly for business users and world travelers, if it works as well as described. (This still needs to be tested by reviewers, of course.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038409" height="373" src="https://techcrunch.com/wp-content/uploads/2025/08/auto-translate.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will translate to or from English and Spanish, German, Japanese, French, Hindi, Italian, Portuguese, Swedish, Russian, and Indonesian. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Pixel’s Phone app, a new addition called Take a Message provides real-time transcripts for missed and declined calls and then uses AI to identify the next steps you need to take based on the caller’s voicemail. (That update will come to Call Notes, too.) &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038408" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/take-a-message.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Pixel Journal, meanwhile, is Google’s answer to Apple’s Journal app, but one that uses AI to prompt you to share your thoughts, track your progress toward goals, and offer insights over time.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038258" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/Pixel-Journal-feat.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Other minor AI upgrades to the Pixel 10 lineup include writing tools integrated into the Gboard keyboard, updates to Pixel screenshots in Pixel Studio, and Notebook LM integrations with Recorder and screenshots.&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With the launch of the new Pixel 10 series, Google is rushing ahead of Apple to deliver AI-powered smartphones to consumers. The devices, announced during Wednesday’s Made by Google event, come just weeks ahead of Apple’s expected iPhone 17 reveal, which promises to be more of the same — better cameras, possibly thinner devices, and new colors to choose from.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google, meanwhile, has been rapidly integrating its AI platform into its devices. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Last year, its Pixel 9 series added a number of AI features, like Gemini Live (Gemini’s voice mode), image-generation tools, call notes, searchable screenshots, and more. Since then, Google says that Gemini Live conversations have proven to be 5x longer than text-based conversations.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3038318" height="1055" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live.jpg" width="1773" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the tech giant is rolling out even more AI-powered upgrades with the launch of its Pixel 10, including a Visual Overlays feature for the camera, a proactive “Magic Cue” feature, Camera Coach, Voice Translate for calls, an assistant-like “Take a Message” feature, Pixel Journal, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Combined, the updates allow Google to showcase what its latest AI technology can do when enhanced by its Tensor G5 processor, an upgrade to the company’s custom silicon designed for AI experiences and the first to run its newest Gemini Nano model. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Google announced that Gemini Live will gain a new audio model that will detect your tone — like whether you’re excited or concerned — and adjust its response accordingly.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038388" height="379" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the addition of a feature called Visual Overlays, Gemini Live will be able to see what you see through the lens of your camera and provide guidance by highlighting things on your screen. For example, while traveling in a foreign country, you could hold up your phone to see if the street signs around offer information about parking along the roadside.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038389" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/gemini-live-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature, Magic Cue, lets the AI be more proactive by offering contextual suggestions in real time, across apps like Gmail, Calendar, Messages, Screenshots, and others. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea of a more proactive interaction between people and Google technology is something the company has dreamed of for years, long before the AI era. In the early 2010s, for example, Google introduced an Android feature called Google Now that would pop up cards with real-time information related to your daily schedule or the time of day, like nearby restaurants at lunchtime, upcoming meetings, or flight details.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Years later, Magic Cue is the AI-powered reintroduction of this feature, but one where it inserts itself into your everyday apps and interactions.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038365" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/magic-cue-airline.jpg?w=332" width="332" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google demonstrated how Magic Cue could suggest a restaurant to dine at with a friend, offering quick access to place a call to the restaurant to make a reservation. It could propose a reply to your friend with the reservation details or point you to your calendar to check your availability. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Magic Cue’s suggestions appear within the app you’re using and are wrapped with a rainbow-colored outline to differentiate them, as well as within Daily Hub, a personalized daily digest in your Discover feed. You can also tap on its suggestions to take action.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3038303" height="340" src="https://techcrunch.com/wp-content/uploads/2025/08/daily-hub.png?w=340" width="340" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Also similar to Google Now, Magic Cue will be able to surface reminders. But it goes a step further by popping up reminders and notifications more intuitively. For instance, it may remind you of errands you need to handle, like a return of an online order, suggest topics you may want to research, or recommend new playlists to stream. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At launch, Magic Cue’s suggestions will be limited to select activities, like settling up a tab, adding events to your calendar, and showing the forecast for an upcoming trip in the weather app. Over time, Google will add other options and let you configure which data source the feature has access to. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple, it should be noted, is trying to do something similar by allowing users to speak to Siri to interact with and take action within their apps, but unfortunately, its AI-powered Siri has been delayed until 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another one of the more interesting additions in the Pixel 10 series is Camera Coach, an AI-powered assistant that aims to make you a better photographer. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038368" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach.jpg?w=322" width="322" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will be launching in preview with the new devices and uses Gemini models to offer suggestions about how to better frame and compose your shot. You can even choose a “get inspired” option that will suggest scenes you may not have considered.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038392" height="628" src="https://techcrunch.com/wp-content/uploads/2025/08/camera-coach-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the camera will now be able to recognize when you’re taking a group photo. The “Auto Best Take” feature activates and analyzes up to 150 images shot over several seconds to find the best one — whether that’s a shot you snapped yourself or one made by blending others together via AI.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038399" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Auto-best-take.jpg?w=374" width="374" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Then, with the AI-powered “Ask Photos” tool, you can edit the shot to do other things, like fix the lighting, change the framing, or remove an object from the photo by either speaking to or texting Photos’ AI assistant.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Pro devices, the Pro Res Zoom option will also use AI to allow you to “zoom” in on things like architecture and landscapes at 30x-60x or 30x-60x for animals and wildlife.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038397" height="511" src="https://techcrunch.com/wp-content/uploads/2025/08/pro-res.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Related to its enhanced use of AI in photography, Pixel 10 phones will also be the first to implement C2PA, a standard that establishes the origin and edits of digital content, which will help to identify when photos have been modified by AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new AI feature, Voice Translate, will use on-device AI to translate your phone call in real time in what sounds like each speaker’s own voice. This could be a potential game changer, particularly for business users and world travelers, if it works as well as described. (This still needs to be tested by reviewers, of course.)&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038409" height="373" src="https://techcrunch.com/wp-content/uploads/2025/08/auto-translate.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will translate to or from English and Spanish, German, Japanese, French, Hindi, Italian, Portuguese, Swedish, Russian, and Indonesian. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Pixel’s Phone app, a new addition called Take a Message provides real-time transcripts for missed and declined calls and then uses AI to identify the next steps you need to take based on the caller’s voicemail. (That update will come to Call Notes, too.) &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038408" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/take-a-message.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Pixel Journal, meanwhile, is Google’s answer to Apple’s Journal app, but one that uses AI to prompt you to share your thoughts, track your progress toward goals, and offer insights over time.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3038258" height="382" src="https://techcrunch.com/wp-content/uploads/2025/08/Pixel-Journal-feat.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Other minor AI upgrades to the Pixel 10 lineup include writing tools integrated into the Gboard keyboard, updates to Pixel screenshots in Pixel Studio, and Notebook LM integrations with Recorder and screenshots.&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/google-doubles-down-on-ai-phones-with-its-pixel-10-series/</guid><pubDate>Wed, 20 Aug 2025 16:01:00 +0000</pubDate></item><item><title>Forging connections in space with cellular technology (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/20/1121888/forging-connections-in-space-with-cellular-technology/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/nokiamittrimage-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/nokiamittrimage-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;Nokia&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/20/1121888/forging-connections-in-space-with-cellular-technology/</guid><pubDate>Wed, 20 Aug 2025 16:02:16 +0000</pubDate></item><item><title>Anthropic bundles Claude Code into enterprise plans (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/20/anthropic-bundles-claude-code-into-enterprise-plans/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-Anthropic-Kaplan.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday announced a new subscription offering that will incorporate Claude Code into Claude for Enterprise. Previously available only through individual accounts, Anthropic’s command-line coding tool can now be purchased as part of a broader enterprise suite, allowing for more sophisticated integrations and more powerful admin tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is the most requested feature from our business team and enterprise customers,” Anthropic product lead Scott White told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The integration positions Anthropic to better compete with command-line tools from Google and GitHub, both of which included enterprise integrations on launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Claude Code has quickly become one of the most popular command-line programming tools, offering a more agentic approach than traditional IDE-based tools. That popularity has come with some growing pains, as individual users of the service have struggled with unexpected usage limits. The new enterprise offering is partially a response to these issues, allowing businesses to set granular spending controls that can be scaled up for intense usage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is particularly bullish about integrations between Claude Code and the Claude.ai chatbot, which can now be managed more flexibly in an enterprise context. Businesses that subscribe to the new bundle can develop Claude Code prompts in conjunction with the Claude chatbot, or integrate the command-line tool more deeply into internal data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his work on Claude.ai, White said enterprise integrations involving customer feedback tools were particularly transformative, using Claude to summarize large quantities of feedback from different sources and translate everything into concrete product changes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s something magical about blending customer feedback, getting the voice of your customer and then helping to think about solutions that you might be able to prototype and build that address their unique challenges,” White said. “It’s something that as a product manager was simply not possible for me even a year ago.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/AI-Sessions-Anthropic-Kaplan.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday announced a new subscription offering that will incorporate Claude Code into Claude for Enterprise. Previously available only through individual accounts, Anthropic’s command-line coding tool can now be purchased as part of a broader enterprise suite, allowing for more sophisticated integrations and more powerful admin tools.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is the most requested feature from our business team and enterprise customers,” Anthropic product lead Scott White told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The integration positions Anthropic to better compete with command-line tools from Google and GitHub, both of which included enterprise integrations on launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Claude Code has quickly become one of the most popular command-line programming tools, offering a more agentic approach than traditional IDE-based tools. That popularity has come with some growing pains, as individual users of the service have struggled with unexpected usage limits. The new enterprise offering is partially a response to these issues, allowing businesses to set granular spending controls that can be scaled up for intense usage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic is particularly bullish about integrations between Claude Code and the Claude.ai chatbot, which can now be managed more flexibly in an enterprise context. Businesses that subscribe to the new bundle can develop Claude Code prompts in conjunction with the Claude chatbot, or integrate the command-line tool more deeply into internal data sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his work on Claude.ai, White said enterprise integrations involving customer feedback tools were particularly transformative, using Claude to summarize large quantities of feedback from different sources and translate everything into concrete product changes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“There’s something magical about blending customer feedback, getting the voice of your customer and then helping to think about solutions that you might be able to prototype and build that address their unique challenges,” White said. “It’s something that as a product manager was simply not possible for me even a year ago.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/20/anthropic-bundles-claude-code-into-enterprise-plans/</guid><pubDate>Wed, 20 Aug 2025 18:00:00 +0000</pubDate></item><item><title>[NEW] CodeSignal’s new AI tutoring app Cosmo wants to be the ‘Duolingo for job skills’ (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/codesignals-new-ai-tutoring-app-cosmo-wants-to-be-the-duolingo-for-job-skills/</link><description>&lt;p&gt;CodeSignal Inc., the San Francisco-based skills assessment platform trusted by Netflix, Meta, and Capital One, launched Cosmo on Wednesday, a mobile learning application that transforms spare minutes into career-ready skills through artificial intelligence-powered micro-courses.&lt;/p&gt;&lt;p&gt;The app represents a strategic pivot for CodeSignal, which built its reputation assessing technical talent for major corporations but always harbored ambitions to revolutionize workplace education. Cosmo delivers over 300 bite-sized courses across generative AI, coding, marketing, finance, and leadership through an interactive chat interface powered by an AI tutor.&lt;/p&gt;&lt;p&gt;“Cosmo is like having an AI tutor in your pocket that can teach you anything from GenAI to coding to marketing to finance to leadership, and it does it through practice,” said Tigran Sloyan, CodeSignal’s co-founder and CEO in an exclusive interview with VentureBeat. “Instead of watching a video or reading about something, you immediately start practicing.”&lt;/p&gt;&lt;p&gt;The launch comes as organizations grapple with massive skills gaps created by rapid AI adoption. According to the 2024 Stack Overflow Developer Survey, 76% of developers are now using or plan to use AI tools, yet most workers lack the practical knowledge to harness these tools effectively. Traditional corporate training programs, which can cost $20,000 to $40,000 per person for executive-level instruction, have proven inadequate for scaling AI literacy across entire workforces.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-codesignal-pivoted-from-tech-hiring-platform-to-mobile-education-powerhouse"&gt;How CodeSignal pivoted from tech hiring platform to mobile education powerhouse&lt;/h2&gt;



&lt;p&gt;CodeSignal’s journey into mobile learning culminates a decade-long vision that took an unexpected detour through the hiring technology space. Sloyan originally founded the company in 2015 with educational ambitions but quickly realized that without skills-based hiring practices, alternative education would fail to gain traction.&lt;/p&gt;



&lt;p&gt;“I started the company with that dream and mission: I want to help more humans achieve their true potential, which naturally leads to better education,” Sloyan explained in an interview. “But roughly two years into the company’s history, I realized that without knowing companies would actually care about the skills you build through alternative education — rather than just asking ‘where did you go to college?’ or ‘what did you major in?’ — it wouldn’t work.”&lt;/p&gt;



&lt;p&gt;The company spent the next six years building what became the leading technical assessment platform, processing millions of coding evaluations for over 3,000 companies. This hiring-focused period provided CodeSignal with crucial intelligence about which skills employers actually value — data that now informs Cosmo’s curriculum development.&lt;/p&gt;



&lt;p&gt;“We know exactly what companies are looking for,” Sloyan said. “Without that, I feel like you’re shooting in the dark when you’re trying to prepare people for what is going to help them get that job, what is going to help them advance their career.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-ai-tutors-could-finally-solve-the-personalized-learning-problem"&gt;Why AI tutors could finally solve the personalized learning problem&lt;/h2&gt;



&lt;p&gt;Cosmo differentiates itself through what CodeSignal calls “practice-first learning,” where users immediately engage with realistic workplace scenarios rather than consuming passive video content. The app’s AI tutor, also named Cosmo, guides learners through conversational exchanges that adapt to individual knowledge levels and learning pace.&lt;/p&gt;



&lt;p&gt;The platform addresses what educational psychologists call “Bloom’s two sigma problem” — a 1984 study showing that one-on-one tutoring produces learning outcomes two standard deviations above traditional classroom instruction. For four decades, this remained theoretically interesting but practically impossible to scale.&lt;/p&gt;



&lt;p&gt;“We know one-on-one personalization and tutoring really makes a difference in learning, but it can’t be done at scale. How do you get a tutor for every human?” Sloyan said. “In 2023, when I saw early versions of generative AI, I thought: this is the moment. This technology, especially if it keeps getting better, can be uniquely used to help humans learn the way learning was meant to happen.”&lt;/p&gt;



&lt;p&gt;The app combines predetermined course content with real-time personalization. Each lesson follows a structured curriculum, but learners can interrupt with questions that prompt immediate AI-generated explanations before returning to the main content thread.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-generative-ai-skills-training-takes-center-stage-as-workforce-scrambles-to-adapt"&gt;Generative AI skills training takes center stage as workforce scrambles to adapt&lt;/h2&gt;



&lt;p&gt;Nearly one-third of Cosmo’s launch content focuses on generative AI applications, reflecting what CodeSignal identifies as the most critical skills gap in today’s market. The app offers role-specific AI training paths for sales professionals, marketers, engineers, healthcare workers, and other specialties.&lt;/p&gt;



&lt;p&gt;“The biggest emphasis is on generative AI skills, because that’s the biggest career skills gap right now for both students and working adults,” Sloyan explained. “Everything from how to understand and use GenAI, how to think about its limitations, how to be better at prompting, and how to understand the entire landscape.”&lt;/p&gt;



&lt;p&gt;This focus addresses a broader workforce transformation driven by AI adoption. While some fear job displacement, Sloyan predicts increased demand for skilled workers who can effectively collaborate with AI systems.&lt;/p&gt;



&lt;p&gt;“I don’t believe we’re going to reach a point where humans are no longer needed in the workforce. I think it’s going to be the opposite. We’re going to need more humans, because what an individual human can do in the age of AI is going to be so much bigger than what we could do before,” he said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-mobile-first-learning-strategy-targets-both-individual-workers-and-corporate-clients"&gt;Mobile-first learning strategy targets both individual workers and corporate clients&lt;/h2&gt;



&lt;p&gt;CodeSignal positions Cosmo as fundamentally a consumer application that also serves enterprise customers — a reflection of how workplace learning actually occurs. The company already provides its GenAI Skills Academy to corporate clients, and Cosmo extends this training to mobile devices for on-the-go learning.&lt;/p&gt;



&lt;p&gt;“Even though some of the largest educational companies, like Coursera and Udemy, are making the majority of their income, or at least half, from companies, at the end of the day, education is a consumer business,” Sloyan noted. “Who are you educating? You’re not educating a company — you’re educating individuals.”&lt;/p&gt;



&lt;p&gt;The app launches free on iOS with premium subscriptions at $24.99 monthly or $149.99 annually, unlocking unlimited practice sessions and faster progression. Android availability follows on August 28.&lt;/p&gt;



&lt;p&gt;Enterprise customers who already use CodeSignal’s learning platform will receive Cosmo access as part of their existing subscriptions, creating what the company describes as a “companion relationship” between desktop-based deep learning and mobile-based habit formation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-cosmo-faces-crowded-edtech-market-with-unique-career-skills-focus"&gt;Cosmo faces crowded EdTech market with unique career-skills focus&lt;/h2&gt;



&lt;p&gt;Cosmo enters a crowded educational technology market but targets a largely underserved niche: comprehensive career skills training optimized for mobile consumption. While competitors like Codecademy focus on specific technical skills and Duolingo dominates language learning, Cosmo’s breadth across business and technical disciplines marks a more ambitious scope.&lt;/p&gt;



&lt;p&gt;Early user feedback suggests strong market demand. Beta testers describe the app as “Duolingo for job skills” and praise its convenience for mobile learning. CodeSignal’s broader learning platform has attracted one million users in less than a year, with usage doubling every two months.&lt;/p&gt;



&lt;p&gt;The app’s foundation in hiring intelligence provides a competitive advantage over traditional educational publishers. CodeSignal’s assessment data reveals which skills actually influence hiring decisions, ensuring curriculum relevance in a rapidly evolving job market.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-corporate-training-industry-grapples-with-low-engagement-and-poor-roi"&gt;Corporate training industry grapples with low engagement and poor ROI&lt;/h2&gt;



&lt;p&gt;Cosmo’s launch reflects broader shifts in how organizations approach workforce development. Traditional corporate training often suffers from poor engagement and retention, with utilization rates frequently in single digits despite significant investments.&lt;/p&gt;



&lt;p&gt;“The number one problem enterprise learning products have is retention. Organizations buy, deploy, and their utilization is like single digits, and that’s horrible,” Sloyan said. “The way these products should be measured is how many more skilled humans are there in my organization, and how many more of them are skilled in the skills that I care about.”&lt;/p&gt;



&lt;p&gt;The mobile-first approach acknowledges how working professionals actually consume educational content — in brief sessions during commutes, breaks, or other downtime rather than dedicated desktop learning blocks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-skills-revolution-accelerates-as-ai-transforms-every-industry"&gt;Skills revolution accelerates as AI transforms every industry&lt;/h2&gt;



&lt;p&gt;CodeSignal’s expansion into mobile learning comes as the company continues innovating across the skills assessment and development spectrum. Recent product launches include AI-Assisted Coding Assessments that evaluate how candidates collaborate with AI tools, and Interviewer Agents that automate technical interviews.&lt;/p&gt;



&lt;p&gt;The company has also expanded its educational partnerships, including a collaboration with Amazon Web Services to provide free generative AI training to over 30,000 students globally through the AWS Skills to Jobs Tech Alliance.&lt;/p&gt;



&lt;p&gt;Sloyan frames these initiatives within a broader mission to help workers navigate technological disruption. As AI transforms virtually every industry, the ability to quickly acquire new skills becomes increasingly critical for career resilience.&lt;/p&gt;



&lt;p&gt;“We’ve entered an era of accelerating technological change, which will bring a lot of disruption,” he said. “There’s a massive skills transformation needed, and right now, I wish more companies were doing more to help individuals find and grow those skills.”&lt;/p&gt;



&lt;p&gt;Cosmo’s success may determine whether mobile-native, AI-powered learning can finally deliver on the long-promised potential of personalized education at scale. For CodeSignal, the launch marks a homecoming of sorts — after spending years teaching companies how to identify skilled workers, the company is now teaching workers how to become skilled in the first place. In an era where artificial intelligence threatens to displace human workers, CodeSignal is betting that the solution lies in using AI to make humans more capable than ever before.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;CodeSignal Inc., the San Francisco-based skills assessment platform trusted by Netflix, Meta, and Capital One, launched Cosmo on Wednesday, a mobile learning application that transforms spare minutes into career-ready skills through artificial intelligence-powered micro-courses.&lt;/p&gt;&lt;p&gt;The app represents a strategic pivot for CodeSignal, which built its reputation assessing technical talent for major corporations but always harbored ambitions to revolutionize workplace education. Cosmo delivers over 300 bite-sized courses across generative AI, coding, marketing, finance, and leadership through an interactive chat interface powered by an AI tutor.&lt;/p&gt;&lt;p&gt;“Cosmo is like having an AI tutor in your pocket that can teach you anything from GenAI to coding to marketing to finance to leadership, and it does it through practice,” said Tigran Sloyan, CodeSignal’s co-founder and CEO in an exclusive interview with VentureBeat. “Instead of watching a video or reading about something, you immediately start practicing.”&lt;/p&gt;&lt;p&gt;The launch comes as organizations grapple with massive skills gaps created by rapid AI adoption. According to the 2024 Stack Overflow Developer Survey, 76% of developers are now using or plan to use AI tools, yet most workers lack the practical knowledge to harness these tools effectively. Traditional corporate training programs, which can cost $20,000 to $40,000 per person for executive-level instruction, have proven inadequate for scaling AI literacy across entire workforces.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-codesignal-pivoted-from-tech-hiring-platform-to-mobile-education-powerhouse"&gt;How CodeSignal pivoted from tech hiring platform to mobile education powerhouse&lt;/h2&gt;



&lt;p&gt;CodeSignal’s journey into mobile learning culminates a decade-long vision that took an unexpected detour through the hiring technology space. Sloyan originally founded the company in 2015 with educational ambitions but quickly realized that without skills-based hiring practices, alternative education would fail to gain traction.&lt;/p&gt;



&lt;p&gt;“I started the company with that dream and mission: I want to help more humans achieve their true potential, which naturally leads to better education,” Sloyan explained in an interview. “But roughly two years into the company’s history, I realized that without knowing companies would actually care about the skills you build through alternative education — rather than just asking ‘where did you go to college?’ or ‘what did you major in?’ — it wouldn’t work.”&lt;/p&gt;



&lt;p&gt;The company spent the next six years building what became the leading technical assessment platform, processing millions of coding evaluations for over 3,000 companies. This hiring-focused period provided CodeSignal with crucial intelligence about which skills employers actually value — data that now informs Cosmo’s curriculum development.&lt;/p&gt;



&lt;p&gt;“We know exactly what companies are looking for,” Sloyan said. “Without that, I feel like you’re shooting in the dark when you’re trying to prepare people for what is going to help them get that job, what is going to help them advance their career.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-ai-tutors-could-finally-solve-the-personalized-learning-problem"&gt;Why AI tutors could finally solve the personalized learning problem&lt;/h2&gt;



&lt;p&gt;Cosmo differentiates itself through what CodeSignal calls “practice-first learning,” where users immediately engage with realistic workplace scenarios rather than consuming passive video content. The app’s AI tutor, also named Cosmo, guides learners through conversational exchanges that adapt to individual knowledge levels and learning pace.&lt;/p&gt;



&lt;p&gt;The platform addresses what educational psychologists call “Bloom’s two sigma problem” — a 1984 study showing that one-on-one tutoring produces learning outcomes two standard deviations above traditional classroom instruction. For four decades, this remained theoretically interesting but practically impossible to scale.&lt;/p&gt;



&lt;p&gt;“We know one-on-one personalization and tutoring really makes a difference in learning, but it can’t be done at scale. How do you get a tutor for every human?” Sloyan said. “In 2023, when I saw early versions of generative AI, I thought: this is the moment. This technology, especially if it keeps getting better, can be uniquely used to help humans learn the way learning was meant to happen.”&lt;/p&gt;



&lt;p&gt;The app combines predetermined course content with real-time personalization. Each lesson follows a structured curriculum, but learners can interrupt with questions that prompt immediate AI-generated explanations before returning to the main content thread.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-generative-ai-skills-training-takes-center-stage-as-workforce-scrambles-to-adapt"&gt;Generative AI skills training takes center stage as workforce scrambles to adapt&lt;/h2&gt;



&lt;p&gt;Nearly one-third of Cosmo’s launch content focuses on generative AI applications, reflecting what CodeSignal identifies as the most critical skills gap in today’s market. The app offers role-specific AI training paths for sales professionals, marketers, engineers, healthcare workers, and other specialties.&lt;/p&gt;



&lt;p&gt;“The biggest emphasis is on generative AI skills, because that’s the biggest career skills gap right now for both students and working adults,” Sloyan explained. “Everything from how to understand and use GenAI, how to think about its limitations, how to be better at prompting, and how to understand the entire landscape.”&lt;/p&gt;



&lt;p&gt;This focus addresses a broader workforce transformation driven by AI adoption. While some fear job displacement, Sloyan predicts increased demand for skilled workers who can effectively collaborate with AI systems.&lt;/p&gt;



&lt;p&gt;“I don’t believe we’re going to reach a point where humans are no longer needed in the workforce. I think it’s going to be the opposite. We’re going to need more humans, because what an individual human can do in the age of AI is going to be so much bigger than what we could do before,” he said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-mobile-first-learning-strategy-targets-both-individual-workers-and-corporate-clients"&gt;Mobile-first learning strategy targets both individual workers and corporate clients&lt;/h2&gt;



&lt;p&gt;CodeSignal positions Cosmo as fundamentally a consumer application that also serves enterprise customers — a reflection of how workplace learning actually occurs. The company already provides its GenAI Skills Academy to corporate clients, and Cosmo extends this training to mobile devices for on-the-go learning.&lt;/p&gt;



&lt;p&gt;“Even though some of the largest educational companies, like Coursera and Udemy, are making the majority of their income, or at least half, from companies, at the end of the day, education is a consumer business,” Sloyan noted. “Who are you educating? You’re not educating a company — you’re educating individuals.”&lt;/p&gt;



&lt;p&gt;The app launches free on iOS with premium subscriptions at $24.99 monthly or $149.99 annually, unlocking unlimited practice sessions and faster progression. Android availability follows on August 28.&lt;/p&gt;



&lt;p&gt;Enterprise customers who already use CodeSignal’s learning platform will receive Cosmo access as part of their existing subscriptions, creating what the company describes as a “companion relationship” between desktop-based deep learning and mobile-based habit formation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-cosmo-faces-crowded-edtech-market-with-unique-career-skills-focus"&gt;Cosmo faces crowded EdTech market with unique career-skills focus&lt;/h2&gt;



&lt;p&gt;Cosmo enters a crowded educational technology market but targets a largely underserved niche: comprehensive career skills training optimized for mobile consumption. While competitors like Codecademy focus on specific technical skills and Duolingo dominates language learning, Cosmo’s breadth across business and technical disciplines marks a more ambitious scope.&lt;/p&gt;



&lt;p&gt;Early user feedback suggests strong market demand. Beta testers describe the app as “Duolingo for job skills” and praise its convenience for mobile learning. CodeSignal’s broader learning platform has attracted one million users in less than a year, with usage doubling every two months.&lt;/p&gt;



&lt;p&gt;The app’s foundation in hiring intelligence provides a competitive advantage over traditional educational publishers. CodeSignal’s assessment data reveals which skills actually influence hiring decisions, ensuring curriculum relevance in a rapidly evolving job market.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-corporate-training-industry-grapples-with-low-engagement-and-poor-roi"&gt;Corporate training industry grapples with low engagement and poor ROI&lt;/h2&gt;



&lt;p&gt;Cosmo’s launch reflects broader shifts in how organizations approach workforce development. Traditional corporate training often suffers from poor engagement and retention, with utilization rates frequently in single digits despite significant investments.&lt;/p&gt;



&lt;p&gt;“The number one problem enterprise learning products have is retention. Organizations buy, deploy, and their utilization is like single digits, and that’s horrible,” Sloyan said. “The way these products should be measured is how many more skilled humans are there in my organization, and how many more of them are skilled in the skills that I care about.”&lt;/p&gt;



&lt;p&gt;The mobile-first approach acknowledges how working professionals actually consume educational content — in brief sessions during commutes, breaks, or other downtime rather than dedicated desktop learning blocks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-skills-revolution-accelerates-as-ai-transforms-every-industry"&gt;Skills revolution accelerates as AI transforms every industry&lt;/h2&gt;



&lt;p&gt;CodeSignal’s expansion into mobile learning comes as the company continues innovating across the skills assessment and development spectrum. Recent product launches include AI-Assisted Coding Assessments that evaluate how candidates collaborate with AI tools, and Interviewer Agents that automate technical interviews.&lt;/p&gt;



&lt;p&gt;The company has also expanded its educational partnerships, including a collaboration with Amazon Web Services to provide free generative AI training to over 30,000 students globally through the AWS Skills to Jobs Tech Alliance.&lt;/p&gt;



&lt;p&gt;Sloyan frames these initiatives within a broader mission to help workers navigate technological disruption. As AI transforms virtually every industry, the ability to quickly acquire new skills becomes increasingly critical for career resilience.&lt;/p&gt;



&lt;p&gt;“We’ve entered an era of accelerating technological change, which will bring a lot of disruption,” he said. “There’s a massive skills transformation needed, and right now, I wish more companies were doing more to help individuals find and grow those skills.”&lt;/p&gt;



&lt;p&gt;Cosmo’s success may determine whether mobile-native, AI-powered learning can finally deliver on the long-promised potential of personalized education at scale. For CodeSignal, the launch marks a homecoming of sorts — after spending years teaching companies how to identify skilled workers, the company is now teaching workers how to become skilled in the first place. In an era where artificial intelligence threatens to displace human workers, CodeSignal is betting that the solution lies in using AI to make humans more capable than ever before.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/codesignals-new-ai-tutoring-app-cosmo-wants-to-be-the-duolingo-for-job-skills/</guid><pubDate>Wed, 20 Aug 2025 18:26:57 +0000</pubDate></item><item><title>[NEW] Gen AI makes no financial difference in 95% of cases (AI News)</title><link>https://www.artificialintelligence-news.com/news/gen-ai-makes-no-financial-difference-in-95-of-cases/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/genai-ineffective-in-95-percent-of-cases-survey.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Stocks in US AI technology companies fell in value at the close of trading yesterday, with the NASDAQ Composite index down 1.4%. Among those losing value were Palantir, down 9.4% and Arm Holdings down 5%. According to the &lt;em&gt;Financial Times&lt;/em&gt; [paywall], Tuesday saw the biggest one-day fall in the market since the beginning of August.&lt;/p&gt;&lt;p&gt;Some traders put the falls down to a report released [PDF] by an AI company, NANDA, which noted the high failure rate of many generative AI projects in commercial organisations. Project NANDA originated at the Massachusetts Institute of Technology Media Lab and describes itself as an organisation that’s building an “agentic web.” The paper has, since publication, been placed behind a survey wall, but is available for download from this site.&lt;/p&gt;&lt;p&gt;The research authors state only 5% of gen AI pilots reach production and actually produce measurable monetary value, with the vast majority of projects creating little impact on profit &amp;amp; loss metrics. The research undertaken by NANDA comprised of the content of 52 structured interviews with enterprise decision-makers, researchers’ analysis of 300+ public AI initiatives and announcements, and a survey questionnaire completed by 153 company leaders. It measured return on investment over six months after gen AI projects left pilot status.&lt;/p&gt;&lt;p&gt;While many organisations deploy AI in front-office or customer-facing business functions, successful projects tend to be found among back-office workflows, the paper says. It’s in the mundane tasks of the back office where savings are accrued, largely from a lowered need for third-party agencies and BPOs. The survey found there was little impact by AI projects on overall internal staff levels.&lt;/p&gt;&lt;p&gt;While 90% of staff stated they have personally benefited from using publicly-available AIs, typically in the form of large language models like ChatGPT, those subjective gains are not translated at institution level. Around 40% of the companies surveyed pay for a subscription to LLMs.&lt;/p&gt;&lt;p&gt;Many of the failed projects’ owners cited the lack of contextual awareness exhibited by generative AI models – that is, adapting to circumstances, changing over time, and remembering previous enquiries. NANDA states that forming a partnership with an organisation that can supply such a system and ensure it adapts to an organisation’s specific circumstances is the critical element for success. The paper highlights several quotes “derived from interviews” that include between 60%-70% agreeing with the statements, “[The AI system] doesn’t learn from our feedback,” and “Too much manual context required each time.”&lt;/p&gt;&lt;p&gt;The vertical most positively affected by gen AI was media &amp;amp; telecom, followed by professional services, healthcare &amp;amp; pharma, consumer &amp;amp; retail, and financial services. The energy &amp;amp; materials sector’s rate of generative AI project launch is currently negligible, the paper says. In terms of business units, sales &amp;amp; marketing is where most projects are or were based, with finance &amp;amp; procurement least popular as a place where AI projects might be begun.&lt;/p&gt;&lt;p&gt;The area in a typical organisation where generative AI is deployed most is in sales &amp;amp; marketing, with finance and procurement being the least popular site. And complex tasks are those least likely to be expected to be completed by AI; managers would assign projects like client management to an AI only 10% of the time, while tasks like summarising a report or writing an email would go to a human on 70% of occasions.&lt;/p&gt;&lt;p&gt;The language of the published report and its lack of academic rigour suggest that its provenance and purpose are more akin to marketing than intellectual and technological discussion. The paper’s authors urge for strategic partnerships with a knowledgeable vendor to increase the chances of generative AI projects’ success, a partnership which NANDA is, purely coincidentally, able to form one half of. There are “unprecedented opportunities for vendors who can deliver learning-capable, deeply integrated AI systems,” the paper’s conclusions state.&lt;/p&gt;&lt;p&gt;The headlines from the NANDA report make for sobering reading among decision-makers tasked with generative AI implementations, yet the paper’s underlying messages are weakened by the intentions behind its publication. Stock prices this week could have been affected by partisan surveys from authors with obvious skin in the game, but it seems more likely that the NANDA publication simply reflects trading floors’ concerns about generative AI’s practical effectiveness as a business tool.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Arthur Daley” by Tim Dennell is licensed under CC BY-NC-ND 2.0.&lt;/em&gt;)&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/genai-ineffective-in-95-percent-of-cases-survey.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Stocks in US AI technology companies fell in value at the close of trading yesterday, with the NASDAQ Composite index down 1.4%. Among those losing value were Palantir, down 9.4% and Arm Holdings down 5%. According to the &lt;em&gt;Financial Times&lt;/em&gt; [paywall], Tuesday saw the biggest one-day fall in the market since the beginning of August.&lt;/p&gt;&lt;p&gt;Some traders put the falls down to a report released [PDF] by an AI company, NANDA, which noted the high failure rate of many generative AI projects in commercial organisations. Project NANDA originated at the Massachusetts Institute of Technology Media Lab and describes itself as an organisation that’s building an “agentic web.” The paper has, since publication, been placed behind a survey wall, but is available for download from this site.&lt;/p&gt;&lt;p&gt;The research authors state only 5% of gen AI pilots reach production and actually produce measurable monetary value, with the vast majority of projects creating little impact on profit &amp;amp; loss metrics. The research undertaken by NANDA comprised of the content of 52 structured interviews with enterprise decision-makers, researchers’ analysis of 300+ public AI initiatives and announcements, and a survey questionnaire completed by 153 company leaders. It measured return on investment over six months after gen AI projects left pilot status.&lt;/p&gt;&lt;p&gt;While many organisations deploy AI in front-office or customer-facing business functions, successful projects tend to be found among back-office workflows, the paper says. It’s in the mundane tasks of the back office where savings are accrued, largely from a lowered need for third-party agencies and BPOs. The survey found there was little impact by AI projects on overall internal staff levels.&lt;/p&gt;&lt;p&gt;While 90% of staff stated they have personally benefited from using publicly-available AIs, typically in the form of large language models like ChatGPT, those subjective gains are not translated at institution level. Around 40% of the companies surveyed pay for a subscription to LLMs.&lt;/p&gt;&lt;p&gt;Many of the failed projects’ owners cited the lack of contextual awareness exhibited by generative AI models – that is, adapting to circumstances, changing over time, and remembering previous enquiries. NANDA states that forming a partnership with an organisation that can supply such a system and ensure it adapts to an organisation’s specific circumstances is the critical element for success. The paper highlights several quotes “derived from interviews” that include between 60%-70% agreeing with the statements, “[The AI system] doesn’t learn from our feedback,” and “Too much manual context required each time.”&lt;/p&gt;&lt;p&gt;The vertical most positively affected by gen AI was media &amp;amp; telecom, followed by professional services, healthcare &amp;amp; pharma, consumer &amp;amp; retail, and financial services. The energy &amp;amp; materials sector’s rate of generative AI project launch is currently negligible, the paper says. In terms of business units, sales &amp;amp; marketing is where most projects are or were based, with finance &amp;amp; procurement least popular as a place where AI projects might be begun.&lt;/p&gt;&lt;p&gt;The area in a typical organisation where generative AI is deployed most is in sales &amp;amp; marketing, with finance and procurement being the least popular site. And complex tasks are those least likely to be expected to be completed by AI; managers would assign projects like client management to an AI only 10% of the time, while tasks like summarising a report or writing an email would go to a human on 70% of occasions.&lt;/p&gt;&lt;p&gt;The language of the published report and its lack of academic rigour suggest that its provenance and purpose are more akin to marketing than intellectual and technological discussion. The paper’s authors urge for strategic partnerships with a knowledgeable vendor to increase the chances of generative AI projects’ success, a partnership which NANDA is, purely coincidentally, able to form one half of. There are “unprecedented opportunities for vendors who can deliver learning-capable, deeply integrated AI systems,” the paper’s conclusions state.&lt;/p&gt;&lt;p&gt;The headlines from the NANDA report make for sobering reading among decision-makers tasked with generative AI implementations, yet the paper’s underlying messages are weakened by the intentions behind its publication. Stock prices this week could have been affected by partisan surveys from authors with obvious skin in the game, but it seems more likely that the NANDA publication simply reflects trading floors’ concerns about generative AI’s practical effectiveness as a business tool.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: “Arthur Daley” by Tim Dennell is licensed under CC BY-NC-ND 2.0.&lt;/em&gt;)&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/gen-ai-makes-no-financial-difference-in-95-of-cases/</guid><pubDate>Wed, 20 Aug 2025 18:57:06 +0000</pubDate></item><item><title>[NEW] Securing private data at scale with differentially private partition selection (The latest research from Google)</title><link>https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Large, user-based datasets are invaluable for advancing AI and machine learning models. They drive innovation that directly benefits users through improved services, more accurate predictions, and personalized experiences. Collaborating on and sharing such datasets can accelerate research, foster new applications, and contribute to the broader scientific community. However, leveraging these powerful datasets also comes with potential data privacy risks.&lt;/p&gt;&lt;p&gt;The process of identifying a specific, meaningful subset of unique items that can be shared safely from a vast collection based on how frequently or prominently they appear across many individual contributions (like finding all the common words used across a huge set of documents) is called “differentially private (DP) partition selection”. By applying differential privacy protections in partition selection, it’s possible to perform that selection in a way that prevents anyone from knowing whether any single individual's data contributed a specific item to the final list. This is done by adding controlled noise and only selecting items that are sufficiently common even after that noise is included, ensuring individual privacy. DP is the first step in many important data science and machine learning tasks, including extracting vocabulary (or &lt;i&gt;n&lt;/i&gt;-grams) from a large private corpus (a necessary step of many textual analysis and language modeling applications), analyzing data streams in a privacy preserving way, obtaining histograms over user data, and increasing efficiency in private model fine-tuning.&lt;/p&gt;&lt;p&gt;In the context of massive datasets like user queries, a &lt;i&gt;parallel&lt;/i&gt; algorithm is crucial. Instead of processing data one piece at a time (like a &lt;i&gt;sequential&lt;/i&gt; algorithm would), a parallel algorithm breaks the problem down into many smaller parts that can be computed simultaneously across multiple processors or machines. This practice isn't just for optimization; it's a fundamental necessity when dealing with the scale of modern data. Parallelization allows the processing of vast amounts of information all at once, enabling researchers to handle datasets with hundreds of billions of items. With this, it’s possible to achieve robust privacy guarantees without sacrificing the utility derived from large datasets.&lt;/p&gt;&lt;p&gt;In our recent publication, “Scalable Private Partition Selection via Adaptive Weighting”, which appeared at ICML2025, we introduce an efficient parallel algorithm that makes it possible to apply DP partition selection to various data releases. Our algorithm provides the best results across the board among parallel algorithms and scales to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed by prior sequential algorithms. To encourage collaboration and innovation by the research community, we are open-sourcing DP partition selection on GitHub.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Large, user-based datasets are invaluable for advancing AI and machine learning models. They drive innovation that directly benefits users through improved services, more accurate predictions, and personalized experiences. Collaborating on and sharing such datasets can accelerate research, foster new applications, and contribute to the broader scientific community. However, leveraging these powerful datasets also comes with potential data privacy risks.&lt;/p&gt;&lt;p&gt;The process of identifying a specific, meaningful subset of unique items that can be shared safely from a vast collection based on how frequently or prominently they appear across many individual contributions (like finding all the common words used across a huge set of documents) is called “differentially private (DP) partition selection”. By applying differential privacy protections in partition selection, it’s possible to perform that selection in a way that prevents anyone from knowing whether any single individual's data contributed a specific item to the final list. This is done by adding controlled noise and only selecting items that are sufficiently common even after that noise is included, ensuring individual privacy. DP is the first step in many important data science and machine learning tasks, including extracting vocabulary (or &lt;i&gt;n&lt;/i&gt;-grams) from a large private corpus (a necessary step of many textual analysis and language modeling applications), analyzing data streams in a privacy preserving way, obtaining histograms over user data, and increasing efficiency in private model fine-tuning.&lt;/p&gt;&lt;p&gt;In the context of massive datasets like user queries, a &lt;i&gt;parallel&lt;/i&gt; algorithm is crucial. Instead of processing data one piece at a time (like a &lt;i&gt;sequential&lt;/i&gt; algorithm would), a parallel algorithm breaks the problem down into many smaller parts that can be computed simultaneously across multiple processors or machines. This practice isn't just for optimization; it's a fundamental necessity when dealing with the scale of modern data. Parallelization allows the processing of vast amounts of information all at once, enabling researchers to handle datasets with hundreds of billions of items. With this, it’s possible to achieve robust privacy guarantees without sacrificing the utility derived from large datasets.&lt;/p&gt;&lt;p&gt;In our recent publication, “Scalable Private Partition Selection via Adaptive Weighting”, which appeared at ICML2025, we introduce an efficient parallel algorithm that makes it possible to apply DP partition selection to various data releases. Our algorithm provides the best results across the board among parallel algorithms and scales to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed by prior sequential algorithms. To encourage collaboration and innovation by the research community, we are open-sourcing DP partition selection on GitHub.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/</guid><pubDate>Wed, 20 Aug 2025 19:24:05 +0000</pubDate></item><item><title>[NEW] TikTok parent company ByteDance releases new open source Seed-OSS-36B model with 512K token context (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/tiktok-parent-company-bytedance-releases-new-open-source-seed-oss-36b-model-with-512k-token-context/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The company’s&lt;strong&gt; Seed Team of AI researchers today released Seed-OSS-36B on AI code sharing website Hugging Face.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Seed-OSS-36B is new line of open source, large language models (LLM) designed for advanced reasoning, and developer-focused usability with a &lt;strong&gt;longer token context&lt;/strong&gt; — that is, how much information the models can accept as inputs and then output in a single exchange — &lt;strong&gt;than many competing LLMs from U.S. tech companies&lt;/strong&gt;, even leaders such as OpenAI and Anthropic.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Seed-OSS-36B-Base&lt;/strong&gt; with synthetic data&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seed-OSS-36B-Base&lt;/strong&gt; &lt;strong&gt;without synthetic data&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seed-OSS-36B-Instruct&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;In releasing both synthetic and non-synthetic versions of the Seed-OSS-36B-Base model, the Seed Team sought to balance practical performance with research flexibility. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;synthetic-data variant,&lt;/strong&gt; trained with additional instruction data, consistently &lt;strong&gt;delivers stronger scores on standard benchmarks&lt;/strong&gt; and is intended as a higher-performing general-purpose option. &lt;/p&gt;



&lt;p&gt;The&lt;strong&gt; non-synthetic model,&lt;/strong&gt; by contrast, omits these augmentations, creating &lt;strong&gt;a cleaner foundation that avoids potential bias or distortion&lt;/strong&gt; introduced by synthetic instruction data. &lt;/p&gt;



&lt;p&gt;By providing both, the team gives applied users access to improved results while ensuring researchers retain a neutral baseline for studying post-training methods.&lt;/p&gt;



&lt;p&gt;Meanwhile, the &lt;strong&gt;Seed-OSS-36B-Instruct model &lt;/strong&gt;differs in that it is &lt;strong&gt;post-trained with instruction data&lt;/strong&gt; to prioritize task execution and instruction following, rather than serving purely as a foundation model.&lt;/p&gt;



&lt;p&gt;All three models are released under the Apache-2.0 license, allowing free use, modification, and redistribution by researchers and developers working for enterprises.&lt;/p&gt;



&lt;p&gt;That means&lt;strong&gt; they can be used to power commercial applications, internal to a company or external/customer-facing, without paying ByteDance any licensing fees or for application programming interface (API) usage.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This continues the summer 2025 trend of Chinese companies shipping powerful open source models with OpenAI attempting to catch up with its own open source gpt-oss duet released earlier this month.&lt;/p&gt;



&lt;p&gt;The Seed Team positions&lt;strong&gt; Seed-OSS for international applications&lt;/strong&gt;, emphasizing versatility across reasoning, agent-like task execution, and multilingual settings.&lt;/p&gt;



&lt;p&gt;The Seed Team, formed in 2023, has concentrated on building foundation models that can serve both research and applied use cases. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-design-and-core-features"&gt;Design and core features&lt;/h2&gt;



&lt;p&gt;The architecture behind Seed-OSS-36B combines familiar design choices such as causal language modeling, grouped query attention, SwiGLU activation, RMSNorm, and RoPE positional encoding.&lt;/p&gt;



&lt;p&gt;Each model carries 36 billion parameters across 64 layers and supports a vocabulary of 155,000 tokens.&lt;/p&gt;



&lt;p&gt;One of the defining features is its&lt;strong&gt; native long-context capability, with a maximum length of 512,000 tokens,&lt;/strong&gt; designed to process extended documents and reasoning chains without performance loss.&lt;/p&gt;



&lt;p&gt;That’s twice the length of OpenAI’s new GPT-5 model family and is &lt;strong&gt;roughly equivalent to about 1,600 pages of text, &lt;/strong&gt;the length of a Christian Bible. &lt;/p&gt;



&lt;p&gt;Another distinguishing element is the introduction of a &lt;strong&gt;thinking budget&lt;/strong&gt;, which lets developers specify how much reasoning the model should perform before delivering an answer. &lt;/p&gt;



&lt;p&gt;It’s something we’ve seen from other recent open source models as well, including Nvidia’s new Nemotron-Nano-9B-v2, also available on Hugging Face.&lt;/p&gt;



&lt;p&gt;In practice, this means teams can tune performance depending on the complexity of the task and the efficiency requirements of deployment. &lt;/p&gt;



&lt;p&gt;Budgets are recommended in multiples of 512 tokens, with 0 providing a direct response mode/&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-competitive-performance-on-third-party-benchmarks"&gt;Competitive performance on third-party benchmarks&lt;/h2&gt;



&lt;p&gt;Benchmarks published with the release position Seed-OSS-36B among the stronger large open-source models. The Instruct variant, in particular, posts state-of-the-art results in multiple areas.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Math and reasoning&lt;/strong&gt;: Seed-OSS-36B-Instruct achieves &lt;strong&gt;91.7 percent on AIME24&lt;/strong&gt; and &lt;strong&gt;65 on BeyondAIME&lt;/strong&gt;, both representing open-source “state-of-the-art” (SOTA).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt;: On LiveCodeBench v6, the Instruct model records &lt;strong&gt;67.4&lt;/strong&gt;, another SOTA score.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Long-context handling&lt;/strong&gt;: On RULER at 128K context length, it reaches &lt;strong&gt;94.6&lt;/strong&gt;, marking the highest open-source result reported.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Base model performance&lt;/strong&gt;: The synthetic-data Base variant delivers &lt;strong&gt;65.1 on MMLU-Pro&lt;/strong&gt; and &lt;strong&gt;81.7 on MATH&lt;/strong&gt;, both state-of-the-art results in their categories.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The no-synthetic Base version, while slightly behind on many measures, proves competitive in its own right. &lt;/p&gt;



&lt;p&gt;It &lt;strong&gt;outperforms its synthetic counterpart on GPQA-D,&lt;/strong&gt; providing researchers with a cleaner, instruction-free baseline for experimentation.&lt;/p&gt;



&lt;p&gt;For enterprises comparing open options, these results &lt;strong&gt;suggest Seed-OSS offers strong potential across math-heavy, coding, and long-context workloads&lt;/strong&gt; while still providing flexibility for research use cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-access-and-deployment"&gt;Access and deployment&lt;/h2&gt;



&lt;p&gt;Beyond performance, the Seed Team highlights accessibility for developers and practitioners. The models &lt;strong&gt;can be deployed using Hugging Face Transformers&lt;/strong&gt;, with &lt;strong&gt;quantization support in both 4-bit and 8-bit formats&lt;/strong&gt; to reduce memory requirements. &lt;/p&gt;



&lt;p&gt;They also&lt;strong&gt; integrate with vLLM for scalable serving&lt;/strong&gt;, including configuration examples and API server instructions.&lt;/p&gt;



&lt;p&gt;To lower barriers further, the team includes scripts for inference, prompt customization, and tool integration. &lt;/p&gt;



&lt;p&gt;For&lt;strong&gt; technical leaders managing small teams or working under budget constraints&lt;/strong&gt;, these provisions are positioned to make experimentation with 36-billion-parameter models more approachable.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-considerations-for-enterprise-decision-makers"&gt;Licensing and considerations for enterprise decision-makers&lt;/h2&gt;



&lt;p&gt;With the models offered under Apache-2.0, organizations can adopt them without restrictive licensing terms, an important factor for teams balancing legal and operational concerns.&lt;/p&gt;



&lt;p&gt;For decision makers evaluating the open-source landscape, the release brings three takeaways:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;State-of-the-art benchmarks across math, coding, and long-context reasoning.&lt;/li&gt;



&lt;li&gt;A balance between higher-performing synthetic-trained models and clean research baselines.&lt;/li&gt;



&lt;li&gt;Accessibility features that lower operational overhead for lean engineering teams.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;By placing strong performance and flexible deployment under an open license, ByteDance’s Seed Team has added new options for enterprises, researchers, and developers alike. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The company’s&lt;strong&gt; Seed Team of AI researchers today released Seed-OSS-36B on AI code sharing website Hugging Face.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Seed-OSS-36B is new line of open source, large language models (LLM) designed for advanced reasoning, and developer-focused usability with a &lt;strong&gt;longer token context&lt;/strong&gt; — that is, how much information the models can accept as inputs and then output in a single exchange — &lt;strong&gt;than many competing LLMs from U.S. tech companies&lt;/strong&gt;, even leaders such as OpenAI and Anthropic.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Seed-OSS-36B-Base&lt;/strong&gt; with synthetic data&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seed-OSS-36B-Base&lt;/strong&gt; &lt;strong&gt;without synthetic data&lt;/strong&gt;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Seed-OSS-36B-Instruct&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;In releasing both synthetic and non-synthetic versions of the Seed-OSS-36B-Base model, the Seed Team sought to balance practical performance with research flexibility. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;synthetic-data variant,&lt;/strong&gt; trained with additional instruction data, consistently &lt;strong&gt;delivers stronger scores on standard benchmarks&lt;/strong&gt; and is intended as a higher-performing general-purpose option. &lt;/p&gt;



&lt;p&gt;The&lt;strong&gt; non-synthetic model,&lt;/strong&gt; by contrast, omits these augmentations, creating &lt;strong&gt;a cleaner foundation that avoids potential bias or distortion&lt;/strong&gt; introduced by synthetic instruction data. &lt;/p&gt;



&lt;p&gt;By providing both, the team gives applied users access to improved results while ensuring researchers retain a neutral baseline for studying post-training methods.&lt;/p&gt;



&lt;p&gt;Meanwhile, the &lt;strong&gt;Seed-OSS-36B-Instruct model &lt;/strong&gt;differs in that it is &lt;strong&gt;post-trained with instruction data&lt;/strong&gt; to prioritize task execution and instruction following, rather than serving purely as a foundation model.&lt;/p&gt;



&lt;p&gt;All three models are released under the Apache-2.0 license, allowing free use, modification, and redistribution by researchers and developers working for enterprises.&lt;/p&gt;



&lt;p&gt;That means&lt;strong&gt; they can be used to power commercial applications, internal to a company or external/customer-facing, without paying ByteDance any licensing fees or for application programming interface (API) usage.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This continues the summer 2025 trend of Chinese companies shipping powerful open source models with OpenAI attempting to catch up with its own open source gpt-oss duet released earlier this month.&lt;/p&gt;



&lt;p&gt;The Seed Team positions&lt;strong&gt; Seed-OSS for international applications&lt;/strong&gt;, emphasizing versatility across reasoning, agent-like task execution, and multilingual settings.&lt;/p&gt;



&lt;p&gt;The Seed Team, formed in 2023, has concentrated on building foundation models that can serve both research and applied use cases. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-design-and-core-features"&gt;Design and core features&lt;/h2&gt;



&lt;p&gt;The architecture behind Seed-OSS-36B combines familiar design choices such as causal language modeling, grouped query attention, SwiGLU activation, RMSNorm, and RoPE positional encoding.&lt;/p&gt;



&lt;p&gt;Each model carries 36 billion parameters across 64 layers and supports a vocabulary of 155,000 tokens.&lt;/p&gt;



&lt;p&gt;One of the defining features is its&lt;strong&gt; native long-context capability, with a maximum length of 512,000 tokens,&lt;/strong&gt; designed to process extended documents and reasoning chains without performance loss.&lt;/p&gt;



&lt;p&gt;That’s twice the length of OpenAI’s new GPT-5 model family and is &lt;strong&gt;roughly equivalent to about 1,600 pages of text, &lt;/strong&gt;the length of a Christian Bible. &lt;/p&gt;



&lt;p&gt;Another distinguishing element is the introduction of a &lt;strong&gt;thinking budget&lt;/strong&gt;, which lets developers specify how much reasoning the model should perform before delivering an answer. &lt;/p&gt;



&lt;p&gt;It’s something we’ve seen from other recent open source models as well, including Nvidia’s new Nemotron-Nano-9B-v2, also available on Hugging Face.&lt;/p&gt;



&lt;p&gt;In practice, this means teams can tune performance depending on the complexity of the task and the efficiency requirements of deployment. &lt;/p&gt;



&lt;p&gt;Budgets are recommended in multiples of 512 tokens, with 0 providing a direct response mode/&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-competitive-performance-on-third-party-benchmarks"&gt;Competitive performance on third-party benchmarks&lt;/h2&gt;



&lt;p&gt;Benchmarks published with the release position Seed-OSS-36B among the stronger large open-source models. The Instruct variant, in particular, posts state-of-the-art results in multiple areas.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Math and reasoning&lt;/strong&gt;: Seed-OSS-36B-Instruct achieves &lt;strong&gt;91.7 percent on AIME24&lt;/strong&gt; and &lt;strong&gt;65 on BeyondAIME&lt;/strong&gt;, both representing open-source “state-of-the-art” (SOTA).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Coding&lt;/strong&gt;: On LiveCodeBench v6, the Instruct model records &lt;strong&gt;67.4&lt;/strong&gt;, another SOTA score.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Long-context handling&lt;/strong&gt;: On RULER at 128K context length, it reaches &lt;strong&gt;94.6&lt;/strong&gt;, marking the highest open-source result reported.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Base model performance&lt;/strong&gt;: The synthetic-data Base variant delivers &lt;strong&gt;65.1 on MMLU-Pro&lt;/strong&gt; and &lt;strong&gt;81.7 on MATH&lt;/strong&gt;, both state-of-the-art results in their categories.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;The no-synthetic Base version, while slightly behind on many measures, proves competitive in its own right. &lt;/p&gt;



&lt;p&gt;It &lt;strong&gt;outperforms its synthetic counterpart on GPQA-D,&lt;/strong&gt; providing researchers with a cleaner, instruction-free baseline for experimentation.&lt;/p&gt;



&lt;p&gt;For enterprises comparing open options, these results &lt;strong&gt;suggest Seed-OSS offers strong potential across math-heavy, coding, and long-context workloads&lt;/strong&gt; while still providing flexibility for research use cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-access-and-deployment"&gt;Access and deployment&lt;/h2&gt;



&lt;p&gt;Beyond performance, the Seed Team highlights accessibility for developers and practitioners. The models &lt;strong&gt;can be deployed using Hugging Face Transformers&lt;/strong&gt;, with &lt;strong&gt;quantization support in both 4-bit and 8-bit formats&lt;/strong&gt; to reduce memory requirements. &lt;/p&gt;



&lt;p&gt;They also&lt;strong&gt; integrate with vLLM for scalable serving&lt;/strong&gt;, including configuration examples and API server instructions.&lt;/p&gt;



&lt;p&gt;To lower barriers further, the team includes scripts for inference, prompt customization, and tool integration. &lt;/p&gt;



&lt;p&gt;For&lt;strong&gt; technical leaders managing small teams or working under budget constraints&lt;/strong&gt;, these provisions are positioned to make experimentation with 36-billion-parameter models more approachable.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-considerations-for-enterprise-decision-makers"&gt;Licensing and considerations for enterprise decision-makers&lt;/h2&gt;



&lt;p&gt;With the models offered under Apache-2.0, organizations can adopt them without restrictive licensing terms, an important factor for teams balancing legal and operational concerns.&lt;/p&gt;



&lt;p&gt;For decision makers evaluating the open-source landscape, the release brings three takeaways:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;State-of-the-art benchmarks across math, coding, and long-context reasoning.&lt;/li&gt;



&lt;li&gt;A balance between higher-performing synthetic-trained models and clean research baselines.&lt;/li&gt;



&lt;li&gt;Accessibility features that lower operational overhead for lean engineering teams.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;By placing strong performance and flexible deployment under an open license, ByteDance’s Seed Team has added new options for enterprises, researchers, and developers alike. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/tiktok-parent-company-bytedance-releases-new-open-source-seed-oss-36b-model-with-512k-token-context/</guid><pubDate>Wed, 20 Aug 2025 22:04:24 +0000</pubDate></item><item><title>[NEW] Enterprise Claude gets admin, compliance tools—just not unlimited usage (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/enterprise-claude-gets-admin-compliance-tools-just-not-unlimited-usage/</link><description>[unable to retrieve full-text content]Anthropic upgraded its Claude Enterprise and Team subscription to offer seats with access to Claude Code and offer additional admin controls.</description><content:encoded>[unable to retrieve full-text content]Anthropic upgraded its Claude Enterprise and Team subscription to offer seats with access to Claude Code and offer additional admin controls.</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/enterprise-claude-gets-admin-compliance-tools-just-not-unlimited-usage/</guid><pubDate>Wed, 20 Aug 2025 23:28:55 +0000</pubDate></item></channel></rss>