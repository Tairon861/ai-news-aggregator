<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 05 Aug 2025 18:37:24 +0000</lastBuildDate><item><title>A glimpse into OpenAI’s largest ambitions (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/05/1121052/a-glimpse-into-openais-largest-ambitions/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250803_algo-openai_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI has given itself a dual mandate. On the one hand, it’s a tech giant rooted in products, including of course ChatGPT, which people around the world reportedly send 2.5 billion requests to each day. But its original mission is to serve as a research lab that will not only create “artificial general intelligence” but ensure that it benefits all of humanity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My colleague Will Douglas Heaven recently sat down for an exclusive conversation with the two figures at OpenAI most responsible for pursuing the latter ambitions: chief research officer Mark Chen and chief scientist Jakub Pachocki. If you haven’t already, you must read his piece.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It provides a rare glimpse into how the company thinks beyond marginal improvements to chatbots and contemplates the biggest unknowns in AI: whether it could someday reason like a human, whether it &lt;em&gt;should&lt;/em&gt;, and how tech companies conceptualize the societal implications.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The whole story is worth reading for all it reveals—about how OpenAI thinks about the safety of its products, what AGI actually means, and more—but here’s one thing that stood out to me.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;As Will points out, there were two recent wins for OpenAI in its efforts to build AI that outcompetes humans. Its models took second place at a top-level coding competition and—alongside those from Google DeepMind—achieved gold-medal-level results in the 2025 International Math Olympiad.&lt;/p&gt;  &lt;p&gt;People who believe that AI doesn’t pose genuine competition to human-level intelligence might actually take some comfort in that. AI is good at the mathematical and analytical, which are on full display in olympiads and coding competitions. That doesn’t mean it’s any good at grappling with the messiness of human emotions, making hard decisions, or creating art that resonates with anyone.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But that distinction—between machine-like reasoning and the ability to think creatively—is not one OpenAI’s heads of research are inclined to make.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We’re talking about programming and math here,” said Pachocki. “But it’s really about creativity, coming up with novel ideas, connecting ideas from different places.”&lt;/p&gt;  &lt;p&gt;That’s why, the researchers say, these testing grounds for AI will produce models that have an increasing ability to reason like a person, one of the most important goals OpenAI is working toward. Reasoning models break problems down into more discrete steps, but even the best have limited ability to chain pieces of information together and approach problems logically.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI is throwing a massive amount of money and talent at that problem not because its researchers think it will result in higher scores at math contests, but because they believe it will allow their AI models to come closer to human intelligence.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;As Will recalls in the piece, he said he thought maybe it’s fine for AI to excel at math and coding, but the idea of having an AI acquire people skills and replace politicians is perhaps not. Chen pulled a face and looked up at the ceiling: “Why not?”&lt;/p&gt;  &lt;p&gt;Read the full story from Will Douglas Heaven.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.&lt;/em&gt;&lt;br /&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250803_algo-openai_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI has given itself a dual mandate. On the one hand, it’s a tech giant rooted in products, including of course ChatGPT, which people around the world reportedly send 2.5 billion requests to each day. But its original mission is to serve as a research lab that will not only create “artificial general intelligence” but ensure that it benefits all of humanity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My colleague Will Douglas Heaven recently sat down for an exclusive conversation with the two figures at OpenAI most responsible for pursuing the latter ambitions: chief research officer Mark Chen and chief scientist Jakub Pachocki. If you haven’t already, you must read his piece.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;It provides a rare glimpse into how the company thinks beyond marginal improvements to chatbots and contemplates the biggest unknowns in AI: whether it could someday reason like a human, whether it &lt;em&gt;should&lt;/em&gt;, and how tech companies conceptualize the societal implications.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The whole story is worth reading for all it reveals—about how OpenAI thinks about the safety of its products, what AGI actually means, and more—but here’s one thing that stood out to me.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;As Will points out, there were two recent wins for OpenAI in its efforts to build AI that outcompetes humans. Its models took second place at a top-level coding competition and—alongside those from Google DeepMind—achieved gold-medal-level results in the 2025 International Math Olympiad.&lt;/p&gt;  &lt;p&gt;People who believe that AI doesn’t pose genuine competition to human-level intelligence might actually take some comfort in that. AI is good at the mathematical and analytical, which are on full display in olympiads and coding competitions. That doesn’t mean it’s any good at grappling with the messiness of human emotions, making hard decisions, or creating art that resonates with anyone.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But that distinction—between machine-like reasoning and the ability to think creatively—is not one OpenAI’s heads of research are inclined to make.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We’re talking about programming and math here,” said Pachocki. “But it’s really about creativity, coming up with novel ideas, connecting ideas from different places.”&lt;/p&gt;  &lt;p&gt;That’s why, the researchers say, these testing grounds for AI will produce models that have an increasing ability to reason like a person, one of the most important goals OpenAI is working toward. Reasoning models break problems down into more discrete steps, but even the best have limited ability to chain pieces of information together and approach problems logically.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI is throwing a massive amount of money and talent at that problem not because its researchers think it will result in higher scores at math contests, but because they believe it will allow their AI models to come closer to human intelligence.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;As Will recalls in the piece, he said he thought maybe it’s fine for AI to excel at math and coding, but the idea of having an AI acquire people skills and replace politicians is perhaps not. Chen pulled a face and looked up at the ceiling: “Why not?”&lt;/p&gt;  &lt;p&gt;Read the full story from Will Douglas Heaven.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here.&lt;/em&gt;&lt;br /&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/05/1121052/a-glimpse-into-openais-largest-ambitions/</guid><pubDate>Tue, 05 Aug 2025 09:00:00 +0000</pubDate></item><item><title>The Download: AI agent infrastructure, and OpenAI’s ambitions (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/05/1121056/the-download-ai-agent-infrastructure-openai-ambitions/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;These protocols will help AI agents navigate our messy lives&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;A growing number of companies are launching AI agents that can do things on your behalf—actions like sending an email, making a document, or editing a database. Initial reviews for these agents have been mixed at best, though, because they struggle to interact with all the different components of our digital lives.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Anthropic and Google are among the companies and groups working to fix that. Over the past year, they have both introduced protocols that try to define how AI agents should interact with each other and the world around them. If they work as planned, they could give us a crucial part of the infrastructure we need for agents to be useful.&amp;nbsp;Read our&amp;nbsp;story to learn more.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Peter Hall&lt;/em&gt;&lt;/p&gt; 
   &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A glimpse into OpenAI’s largest ambitions&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;OpenAI has given itself a dual mandate: on the one hand, it’s a tech giant rooted in products, including of course ChatGPT, which people around the world reportedly send 2.5 billion messages to each day. But its original mission is as a research lab that will not only create “artificial general intelligence” but ensure that it benefits all of humanity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My colleague Will Douglas Heaven recently sat down for an exclusive conversation with the two figures at OpenAI most responsible for the latter ambitions. The&amp;nbsp;whole story is worth reading&amp;nbsp;for all it reveals—about how OpenAI thinks about the safety of its products, what AGI actually means, and more—but here’s one thing that stood out to me.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from The Algorithm, our weekly newsletter all about the latest goings-on in AI.&amp;nbsp;Sign up&amp;nbsp;to receive it in your inbox every Monday.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 OpenAI is adding mental health guardrails to ChatGPT&lt;/strong&gt;&lt;br /&gt;It’s set to give less direct advice, and encourage users to take breaks from lengthy chats. (NBC)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;What happens when doctors fail to spot AI’s mistakes?&lt;/em&gt;&amp;nbsp;(The Verge)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;OpenAI has released its first research into how using ChatGPT affects people’s emotional well-being&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The US wants to build a nuclear reactor on the moon&lt;/strong&gt;&lt;br /&gt;And it hopes to do that before Russia and China, who are planning to do exactly the same. (Politico)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;NASA’s latest mission to the moon just failed.&lt;/em&gt;&amp;nbsp;(Engadget)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Nokia is putting the first cellular network on the moon&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 How to live forever (or at least get rich trying)&amp;nbsp;👴🤑&lt;/strong&gt;&lt;br /&gt;Love them or hate them, the people behind the explosion in longevity research are a fascinating bunch. (New Yorker $)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Longevity clinics around the world are selling unproven treatments.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Welcome to Silicon Valley’s ‘hard tech’ era&lt;/strong&gt;&lt;br /&gt;Goodbye, consumer software. Hello, massive military contracts. (NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Phase two of military AI has arrived.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 There’s a big problem with the Gulf’s trillion-dollar AI dream&lt;/strong&gt;&lt;br /&gt;Building data centers in a region that already has water scarcity issues seems...unwise. (Rest of Water)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;There’s a data center boom in the US desert too.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Google has promised to scale back its energy usage during certain times to reduce stress on the grid.&lt;/em&gt;&amp;nbsp;(Quartz&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Tesla’s board awarded about $30 billion of shares to Elon Musk&lt;/strong&gt;&lt;br /&gt;“Retaining Elon is more important than ever before,” they wrote in a letter to shareholders yesterday. (FT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Tech CEOs pay packets are reaching stratospheric new records.&lt;/em&gt;&amp;nbsp;(WSJ&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 What happens if you respond to those scam job texts?&lt;/strong&gt;&lt;br /&gt;You get exploited, obviously—but you’d be surprised just how weird it can get along the way. (Slate)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Why there’s so much uproar over Vogue’s AI-generated ad&lt;/strong&gt;&lt;br /&gt;It’s the latest flashpoint in the war over when AI should (and shouldn’t) be used. (TechCrunch)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;9 Earth’s core seems to be up and leaking out of Earth’s surface&amp;nbsp;🌋&lt;/strong&gt;&lt;br /&gt;It’s a finding that’s forcing geoscientists to rethink some long-held assumptions. (Quanta&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;How a volcanic eruption turned a human brain into glass.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Could lasers help us see inside people’s heads?&lt;/strong&gt;&lt;br /&gt;It seems possible, but big hurdles remain to this new method being adopted in clinical settings. (IEEE Spectrum)&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&amp;nbsp;"Hate it! Don't want anything to do with it."&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Weezy Simes, a 27-year-old florist, sums up her feelings about AI to&amp;nbsp;Business Insider.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="woman holding a native blanket while hands cut pieces from it" class="wp-image-1077521" src="https://wp.technologyreview.com/wp-content/uploads/2023/08/SO23_kiva_thumb.jpeg?w=2306" /&gt;&lt;div class="image-credit"&gt;ANDREA D'AQUINO&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;What happened to the microfinance organization Kiva?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Since it was founded in 2005, the San Francisco-based nonprofit Kiva has helped everyday people make microloans to borrowers around the world. It connects lenders in richer communities to fund all sorts of entrepreneurs, from bakers in Mexico to farmers in Albania. Its overarching aim is helping poor people help themselves.&lt;/p&gt; 
 &lt;p&gt;But back in August 2021, Kiva lenders started to notice that information that felt essential in deciding who to lend to was suddenly harder to find. Now, lenders are worried that the organization now seems more focused on how to make money than how to create change.&amp;nbsp;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Mara Kardas-Nelson&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ I want&amp;nbsp;this guy&amp;nbsp;to draw my portrait.&amp;nbsp;&lt;br /&gt;+ Highly recommend making&amp;nbsp;these&amp;nbsp;lemongrass chicken lettuce wraps. So tasty and easy!&lt;br /&gt;+ This&amp;nbsp;encyclopedia&amp;nbsp;teaches you about ancient gods and forgotten deities from around the world.&lt;br /&gt;+ Some of the&amp;nbsp;architecture in Iran&amp;nbsp;looks breathtakingly beautiful.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;  &lt;h3 class="wp-block-heading has-medium-font-size"&gt;&lt;strong&gt;These protocols will help AI agents navigate our messy lives&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;A growing number of companies are launching AI agents that can do things on your behalf—actions like sending an email, making a document, or editing a database. Initial reviews for these agents have been mixed at best, though, because they struggle to interact with all the different components of our digital lives.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Anthropic and Google are among the companies and groups working to fix that. Over the past year, they have both introduced protocols that try to define how AI agents should interact with each other and the world around them. If they work as planned, they could give us a crucial part of the infrastructure we need for agents to be useful.&amp;nbsp;Read our&amp;nbsp;story to learn more.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Peter Hall&lt;/em&gt;&lt;/p&gt; 
   &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;A glimpse into OpenAI’s largest ambitions&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;&lt;em&gt;—James O’Donnell&lt;/em&gt;&lt;/p&gt; 
 &lt;p&gt;OpenAI has given itself a dual mandate: on the one hand, it’s a tech giant rooted in products, including of course ChatGPT, which people around the world reportedly send 2.5 billion messages to each day. But its original mission is as a research lab that will not only create “artificial general intelligence” but ensure that it benefits all of humanity.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My colleague Will Douglas Heaven recently sat down for an exclusive conversation with the two figures at OpenAI most responsible for the latter ambitions. The&amp;nbsp;whole story is worth reading&amp;nbsp;for all it reveals—about how OpenAI thinks about the safety of its products, what AGI actually means, and more—but here’s one thing that stood out to me.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;This story is from The Algorithm, our weekly newsletter all about the latest goings-on in AI.&amp;nbsp;Sign up&amp;nbsp;to receive it in your inbox every Monday.&lt;/strong&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 OpenAI is adding mental health guardrails to ChatGPT&lt;/strong&gt;&lt;br /&gt;It’s set to give less direct advice, and encourage users to take breaks from lengthy chats. (NBC)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;What happens when doctors fail to spot AI’s mistakes?&lt;/em&gt;&amp;nbsp;(The Verge)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;OpenAI has released its first research into how using ChatGPT affects people’s emotional well-being&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 The US wants to build a nuclear reactor on the moon&lt;/strong&gt;&lt;br /&gt;And it hopes to do that before Russia and China, who are planning to do exactly the same. (Politico)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;NASA’s latest mission to the moon just failed.&lt;/em&gt;&amp;nbsp;(Engadget)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Nokia is putting the first cellular network on the moon&lt;/em&gt;. (MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 How to live forever (or at least get rich trying)&amp;nbsp;👴🤑&lt;/strong&gt;&lt;br /&gt;Love them or hate them, the people behind the explosion in longevity research are a fascinating bunch. (New Yorker $)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Longevity clinics around the world are selling unproven treatments.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 Welcome to Silicon Valley’s ‘hard tech’ era&lt;/strong&gt;&lt;br /&gt;Goodbye, consumer software. Hello, massive military contracts. (NYT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Phase two of military AI has arrived.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 There’s a big problem with the Gulf’s trillion-dollar AI dream&lt;/strong&gt;&lt;br /&gt;Building data centers in a region that already has water scarcity issues seems...unwise. (Rest of Water)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;There’s a data center boom in the US desert too.&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Google has promised to scale back its energy usage during certain times to reduce stress on the grid.&lt;/em&gt;&amp;nbsp;(Quartz&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Tesla’s board awarded about $30 billion of shares to Elon Musk&lt;/strong&gt;&lt;br /&gt;“Retaining Elon is more important than ever before,” they wrote in a letter to shareholders yesterday. (FT&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Tech CEOs pay packets are reaching stratospheric new records.&lt;/em&gt;&amp;nbsp;(WSJ&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 What happens if you respond to those scam job texts?&lt;/strong&gt;&lt;br /&gt;You get exploited, obviously—but you’d be surprised just how weird it can get along the way. (Slate)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Why there’s so much uproar over Vogue’s AI-generated ad&lt;/strong&gt;&lt;br /&gt;It’s the latest flashpoint in the war over when AI should (and shouldn’t) be used. (TechCrunch)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;9 Earth’s core seems to be up and leaking out of Earth’s surface&amp;nbsp;🌋&lt;/strong&gt;&lt;br /&gt;It’s a finding that’s forcing geoscientists to rethink some long-held assumptions. (Quanta&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;How a volcanic eruption turned a human brain into glass.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;10 Could lasers help us see inside people’s heads?&lt;/strong&gt;&lt;br /&gt;It seems possible, but big hurdles remain to this new method being adopted in clinical settings. (IEEE Spectrum)&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;&amp;nbsp;"Hate it! Don't want anything to do with it."&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Weezy Simes, a 27-year-old florist, sums up her feelings about AI to&amp;nbsp;Business Insider.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="woman holding a native blanket while hands cut pieces from it" class="wp-image-1077521" src="https://wp.technologyreview.com/wp-content/uploads/2023/08/SO23_kiva_thumb.jpeg?w=2306" /&gt;&lt;div class="image-credit"&gt;ANDREA D'AQUINO&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;strong&gt;What happened to the microfinance organization Kiva?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Since it was founded in 2005, the San Francisco-based nonprofit Kiva has helped everyday people make microloans to borrowers around the world. It connects lenders in richer communities to fund all sorts of entrepreneurs, from bakers in Mexico to farmers in Albania. Its overarching aim is helping poor people help themselves.&lt;/p&gt; 
 &lt;p&gt;But back in August 2021, Kiva lenders started to notice that information that felt essential in deciding who to lend to was suddenly harder to find. Now, lenders are worried that the organization now seems more focused on how to make money than how to create change.&amp;nbsp;Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Mara Kardas-Nelson&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ I want&amp;nbsp;this guy&amp;nbsp;to draw my portrait.&amp;nbsp;&lt;br /&gt;+ Highly recommend making&amp;nbsp;these&amp;nbsp;lemongrass chicken lettuce wraps. So tasty and easy!&lt;br /&gt;+ This&amp;nbsp;encyclopedia&amp;nbsp;teaches you about ancient gods and forgotten deities from around the world.&lt;br /&gt;+ Some of the&amp;nbsp;architecture in Iran&amp;nbsp;looks breathtakingly beautiful.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/05/1121056/the-download-ai-agent-infrastructure-openai-ambitions/</guid><pubDate>Tue, 05 Aug 2025 12:00:00 +0000</pubDate></item><item><title>[NEW] Genie 3: A new frontier for world models (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/pXnO9nBpgw0RnwPCDrcbdFqO0cHkaIsQYNuOyXd7L3iFrp0Mr_yLMROiWzz3EqG8W-qq-QzGPTEAs1pr75kX7SzVC-y4jnON7b7P9Bwqd19Nl-bkJYc=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;Genie 3 was made possible due to key research and engineering contributions from Phil Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleks Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang and Jessica Yung.&lt;/p&gt;&lt;p&gt;We thank Andrew Audibert, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Nilesh Ray, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young and Vadim Zubov for their invaluable partnership in developing and refining key components of this project.&lt;/p&gt;&lt;p&gt;Thanks to Tim Rocktäschel, Satinder Singh, Adrian Bolton, Inbar Mosseri, Aäron van den Oord, Douglas Eck, Dumitru Erhan, Raia Hadsell, Zoubin Gharamani, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process.&lt;/p&gt;&lt;p&gt;Feature video was produced by Suz Chambers, Matthew Carey, Alex Chen, Andrew Rhee, JR Schmidt, Scotch Johnson, Heysu Oh, Kaloyan Kolev, Arden Schager, Sam Lawton, Hana Tanimura, Zach Velasco, Ben Wiley, and Dev Valladares. Including samples generated by Signe Norly, Eleni Shaw, Andeep Toor, Gregory Shaw, and Irina Blok.&lt;/p&gt;&lt;p&gt;Finally, we extend our gratitude to Mohammad Babaeizadeh, Gabe Barth-Maron, Parker Beak, Jenny Brennan, Tim Brooks, Max Cant, Harris Chan, Jeff Clune, Kaspar Daugaard, Dumitru Erhan, Ashley Feden, Simon Green, Nik Hemmings, Michael Huber, Jony Hudson, Dirichi Ike-Njoku, Bonnie Li, Simon Osindero, Georg Ostrovski, Ryan Poplin, Alex Rizkowsky, Giles Ruscoe, Ana Salazar, Guy Simmons, Jeff Stanway, Metin Toksoz-Exley, Petko Yotov, Mingda Zhang and Martin Zlocha for their insights and support.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://lh3.googleusercontent.com/pXnO9nBpgw0RnwPCDrcbdFqO0cHkaIsQYNuOyXd7L3iFrp0Mr_yLMROiWzz3EqG8W-qq-QzGPTEAs1pr75kX7SzVC-y4jnON7b7P9Bwqd19Nl-bkJYc=w1200-h630-n-nu" /&gt;&lt;/div&gt;&lt;h2&gt;Acknowledgments&lt;/h2&gt;&lt;p&gt;Genie 3 was made possible due to key research and engineering contributions from Phil Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleks Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, Frank Perbet, Guy Scully, Jeremy Shar, Stephen Spencer, Omer Tov, Ruben Villegas, Emma Wang and Jessica Yung.&lt;/p&gt;&lt;p&gt;We thank Andrew Audibert, Cip Baetu, Jordi Berbel, David Bridson, Jake Bruce, Gavin Buttimore, Sarah Chakera, Bilva Chandra, Paul Collins, Alex Cullum, Bogdan Damoc, Vibha Dasagi, Maxime Gazeau, Charles Gbadamosi, Woohyun Han, Ed Hirst, Ashyana Kachra, Lucie Kerley, Kristian Kjems, Eva Knoepfel, Vika Koriakin, Jessica Lo, Cong Lu, Zeb Mehring, Alex Moufarek, Henna Nandwani, Valeria Oliveira, Fabio Pardo, Jane Park, Andrew Pierson, Ben Poole, Helen Ran, Nilesh Ray, Tim Salimans, Manuel Sanchez, Igor Saprykin, Amy Shen, Sailesh Sidhwani, Duncan Smith, Joe Stanton, Hamish Tomlinson, Dimple Vijaykumar, Luyu Wang, Piers Wingfield, Nat Wong, Keyang Xu, Christopher Yew, Nick Young and Vadim Zubov for their invaluable partnership in developing and refining key components of this project.&lt;/p&gt;&lt;p&gt;Thanks to Tim Rocktäschel, Satinder Singh, Adrian Bolton, Inbar Mosseri, Aäron van den Oord, Douglas Eck, Dumitru Erhan, Raia Hadsell, Zoubin Gharamani, Koray Kavukcuoglu and Demis Hassabis for their insightful guidance and support throughout the research process.&lt;/p&gt;&lt;p&gt;Feature video was produced by Suz Chambers, Matthew Carey, Alex Chen, Andrew Rhee, JR Schmidt, Scotch Johnson, Heysu Oh, Kaloyan Kolev, Arden Schager, Sam Lawton, Hana Tanimura, Zach Velasco, Ben Wiley, and Dev Valladares. Including samples generated by Signe Norly, Eleni Shaw, Andeep Toor, Gregory Shaw, and Irina Blok.&lt;/p&gt;&lt;p&gt;Finally, we extend our gratitude to Mohammad Babaeizadeh, Gabe Barth-Maron, Parker Beak, Jenny Brennan, Tim Brooks, Max Cant, Harris Chan, Jeff Clune, Kaspar Daugaard, Dumitru Erhan, Ashley Feden, Simon Green, Nik Hemmings, Michael Huber, Jony Hudson, Dirichi Ike-Njoku, Bonnie Li, Simon Osindero, Georg Ostrovski, Ryan Poplin, Alex Rizkowsky, Giles Ruscoe, Ana Salazar, Guy Simmons, Jeff Stanway, Metin Toksoz-Exley, Petko Yotov, Mingda Zhang and Martin Zlocha for their insights and support.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/</guid><pubDate>Tue, 05 Aug 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Only 2 days left to save $675 on your TechCrunch Disrupt 2025 ticket (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/only-2-days-left-to-save-675-on-your-disrupt-2025-ticket/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; marks 20 years of driving the startup world forward — and you’ve got just two days left to join the movement and save up to $675 on your ticket.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From October 27-29, Moscone West becomes the home base for over 10,000 movers and makers in the tech world. Join us in San Francisco as we celebrate two decades of startup breakthroughs, VC connections, and industry-shifting ideas.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deadline to lock in savings is August 6 at 11:59 p.m. PT. Don’t wait — &lt;strong&gt;register now to save big&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 2 days left" class="wp-image-3011644" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/TC25_2Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-go-to-disrupt-2025"&gt;Why go to Disrupt 2025?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is more than a tech event. It’s where momentum meets opportunity. You’ll leave with sharp insights, fresh thinking, and a network that moves fast. That one conversation could change your company’s course. That one session could unlock your next big idea. This is where game-changers gather.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re launching, scaling, or just curious, expect tools you can use and connections you’ll keep. This year, we’re going even deeper on AI — from practical demos to expert-led sessions.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="StrictlyVC San Francisco 2025 Ryan Petersen" class="wp-image-2990494" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/54430820378_2a8380bffc_h.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Ryan Petersen of Flexport will take the Builders Stage at TechCrunch Disrupt 2025, taking place from October 27-29, 2025 in Moscone West, San Francisco&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer / TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Roelof Botha, Sequoia Capital&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Jeff Cardenas, Apptronik&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;David George, Andreessen Horowitz&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Alex Kendall, Wayve&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Eunice Kim, Netflix&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Tekedra Mawakana, Waymo&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Alejandro Matamala Ortiz, Runway&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Ryan Petersen, Flexport&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mati Staniszewski, ElevenLabs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Astro Teller, X, The Moonshot Factory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Ethan Thornton, Mach Industries&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Nirav Tolia, Nextdoor&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Raquel Urtasun, Waabi&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Eric Yuan, Zoom&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Sangeen Zeb, GV (Google Ventures)&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;More speakers are added weekly — &lt;strong&gt;check the speaker page&lt;/strong&gt; often.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-s-happening-across-the-floor-at-disrupt"&gt;What’s happening across the floor at Disrupt?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Industry Stages&lt;br /&gt;&lt;/strong&gt;Dive into the tools, trends, and tactics founders need to grow, scale, and lead in a competitive market. Stay up-to-date as new sessions are added to the &lt;strong&gt;agenda&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;Builders Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Startup strategy, growth playbooks, founder realities&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;AI Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;: &lt;/strong&gt;Two days of intelligence that matters&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;Space Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Where tech and the cosmos meet&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;Going Public Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Real stories from founders who made it big&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Interactive sessions&lt;br /&gt;&lt;/strong&gt;Sit down with startup pros in small groups. Ask questions. Get honest answers. Dive deep into candid conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Watch 20 of the world’s best early-stage startups pitch live on the Disrupt Stage for a shot at a $100,000 equity-free prize. Hear top VCs on the judging panel deliver real-time feedback — insights you won’t find anywhere else.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-2758461" height="680" src="https://techcrunch.com/wp-content/uploads/2024/05/sb200_trophy.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Explore and connect&lt;br /&gt;&lt;/strong&gt;The Expo Hall is packed with startups, tools, and tech shaping tomorrow. Are you a startup ready to showcase your innovation?&amp;nbsp;&lt;strong&gt;Book your exhibit table&lt;/strong&gt; for all three days before we sell out.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Network intentionally&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Disrupt is built for connection. Braindate’s smart matching pairs you with the right people — for the right conversations — not just more of them.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-investor-founder-connections"&gt;Investor-founder connections&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt isn’t just where the future of tech unfolds — it’s where the right investors meet the right founders, and vice versa. Because the right match changes everything.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Deal Flow Café is your go-to spot for casual, high-value conversations between founders and investors — over coffee, of course.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Investors: &lt;/strong&gt;Get a curated tour of the Expo Hall, with formal introductions to the startups that best match your investment focus. You’ll also have a reserved seat at the exclusive &lt;strong&gt;StrictlyVC&lt;/strong&gt; session on day 2 of Disrupt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Founders: &lt;/strong&gt;Connect directly with investors who are actively scouting for their next big bet. Whether you’re raising now or building for later, the Deal Flow Café is where momentum begins.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A group of founders networking in the Investor Lounge called the Deal Flow Cafe at TechCrunch Disrupt 2024. Moscone West, San Francisco, California." class="wp-image-2953562" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-founders-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-lock-in-your-savings-before-august-7"&gt;Lock in your savings before August 7&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Your chance to save up to $675 ends August 6 at 11:59 p.m. PT. &lt;strong&gt;Lock in your Disrupt 2025 pass today&lt;/strong&gt; — and help shape the future of tech.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; marks 20 years of driving the startup world forward — and you’ve got just two days left to join the movement and save up to $675 on your ticket.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From October 27-29, Moscone West becomes the home base for over 10,000 movers and makers in the tech world. Join us in San Francisco as we celebrate two decades of startup breakthroughs, VC connections, and industry-shifting ideas.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The deadline to lock in savings is August 6 at 11:59 p.m. PT. Don’t wait — &lt;strong&gt;register now to save big&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 2 days left" class="wp-image-3011644" height="383" src="https://techcrunch.com/wp-content/uploads/2025/05/TC25_2Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-go-to-disrupt-2025"&gt;Why go to Disrupt 2025?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is more than a tech event. It’s where momentum meets opportunity. You’ll leave with sharp insights, fresh thinking, and a network that moves fast. That one conversation could change your company’s course. That one session could unlock your next big idea. This is where game-changers gather.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re launching, scaling, or just curious, expect tools you can use and connections you’ll keep. This year, we’re going even deeper on AI — from practical demos to expert-led sessions.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="StrictlyVC San Francisco 2025 Ryan Petersen" class="wp-image-2990494" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/54430820378_2a8380bffc_h.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Ryan Petersen of Flexport will take the Builders Stage at TechCrunch Disrupt 2025, taking place from October 27-29, 2025 in Moscone West, San Francisco&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer / TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Roelof Botha, Sequoia Capital&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Jeff Cardenas, Apptronik&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;David George, Andreessen Horowitz&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Alex Kendall, Wayve&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Eunice Kim, Netflix&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Tekedra Mawakana, Waymo&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Alejandro Matamala Ortiz, Runway&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Ryan Petersen, Flexport&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Mati Staniszewski, ElevenLabs&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Astro Teller, X, The Moonshot Factory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Ethan Thornton, Mach Industries&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Nirav Tolia, Nextdoor&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Raquel Urtasun, Waabi&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Eric Yuan, Zoom&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;Sangeen Zeb, GV (Google Ventures)&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;More speakers are added weekly — &lt;strong&gt;check the speaker page&lt;/strong&gt; often.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-s-happening-across-the-floor-at-disrupt"&gt;What’s happening across the floor at Disrupt?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Industry Stages&lt;br /&gt;&lt;/strong&gt;Dive into the tools, trends, and tactics founders need to grow, scale, and lead in a competitive market. Stay up-to-date as new sessions are added to the &lt;strong&gt;agenda&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;Builders Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Startup strategy, growth playbooks, founder realities&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;AI Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;: &lt;/strong&gt;Two days of intelligence that matters&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;Space Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Where tech and the cosmos meet&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;&lt;em&gt;Going Public Stage&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;:&lt;/strong&gt; Real stories from founders who made it big&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Interactive sessions&lt;br /&gt;&lt;/strong&gt;Sit down with startup pros in small groups. Ask questions. Get honest answers. Dive deep into candid conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Watch 20 of the world’s best early-stage startups pitch live on the Disrupt Stage for a shot at a $100,000 equity-free prize. Hear top VCs on the judging panel deliver real-time feedback — insights you won’t find anywhere else.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-2758461" height="680" src="https://techcrunch.com/wp-content/uploads/2024/05/sb200_trophy.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Explore and connect&lt;br /&gt;&lt;/strong&gt;The Expo Hall is packed with startups, tools, and tech shaping tomorrow. Are you a startup ready to showcase your innovation?&amp;nbsp;&lt;strong&gt;Book your exhibit table&lt;/strong&gt; for all three days before we sell out.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Network intentionally&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;Disrupt is built for connection. Braindate’s smart matching pairs you with the right people — for the right conversations — not just more of them.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-investor-founder-connections"&gt;Investor-founder connections&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt isn’t just where the future of tech unfolds — it’s where the right investors meet the right founders, and vice versa. Because the right match changes everything.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Deal Flow Café is your go-to spot for casual, high-value conversations between founders and investors — over coffee, of course.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Investors: &lt;/strong&gt;Get a curated tour of the Expo Hall, with formal introductions to the startups that best match your investment focus. You’ll also have a reserved seat at the exclusive &lt;strong&gt;StrictlyVC&lt;/strong&gt; session on day 2 of Disrupt.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Founders: &lt;/strong&gt;Connect directly with investors who are actively scouting for their next big bet. Whether you’re raising now or building for later, the Deal Flow Café is where momentum begins.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A group of founders networking in the Investor Lounge called the Deal Flow Cafe at TechCrunch Disrupt 2024. Moscone West, San Francisco, California." class="wp-image-2953562" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-founders-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White/Getty Images for TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-lock-in-your-savings-before-august-7"&gt;Lock in your savings before August 7&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Your chance to save up to $675 ends August 6 at 11:59 p.m. PT. &lt;strong&gt;Lock in your Disrupt 2025 pass today&lt;/strong&gt; — and help shape the future of tech.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/only-2-days-left-to-save-675-on-your-disrupt-2025-ticket/</guid><pubDate>Tue, 05 Aug 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] DeepMind reveals Genie 3 “world model” that creates real-time interactive simulations (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/08/deepmind-reveals-genie-3-world-model-that-creates-real-time-interactive-simulations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Genie 3 creates detailed worlds from a prompt or image.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/vlcsnap-2025-08-04-14h32m46s918-640x360.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/vlcsnap-2025-08-04-14h32m46s918-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;While no one has figured out how to make money from generative artificial intelligence, that hasn't stopped Google DeepMind from pushing the boundaries of what's possible with a big pile of inference. The capabilities (and costs) of these models have been on an impressive upward trajectory, a trend exemplified by the reveal of Genie 3. A mere seven months after showing off the Genie 2 "foundational world model," which was itself a significant improvement over its predecessor, Google now has Genie 3.&lt;/p&gt;
&lt;p&gt;With Genie 3, all it takes is a prompt or image to create an interactive world. Since the environment is continuously generated, it can be changed on the fly. You can add or change objects, alter weather conditions, or insert new characters—DeepMind calls these "promptable events." The ability to create alterable 3D environments could make games more dynamic for players and offer developers new ways to prove out concepts and level designs. However, many in the gaming industry have expressed doubt that such tools would help.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Genie 3: building better worlds.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;It's tempting to think of Genie 3 simply as a way to create games, but DeepMind sees this as a research tool, too. Games play a significant role in the development of artificial intelligence because they provide challenging, interactive environments with measurable progress. That's why DeepMind previously turned to games like &lt;em&gt;Go&lt;/em&gt; and &lt;em&gt;StarCraft&lt;/em&gt; to expand the bounds of AI.&lt;/p&gt;
&lt;p&gt;World models take that to the next level, generating an interactive world frame by frame. This provides an opportunity to refine how AI models—including so-called "embodied agents"—behave when they encounter real-world situations. One of the primary limitations as companies work toward the goal of artificial general intelligence (AGI) is the scarcity of reliable training data. After piping basically every webpage and video on the planet into AI models, researchers are turning toward synthetic data for many applications. DeepMind believes world models could be a key part of this effort, as they can be used to train AI agents with essentially unlimited interactive worlds.&lt;/p&gt;
&lt;p&gt;DeepMind says Genie 3 is an important advancement because it offers much higher visual fidelity than Genie 2, and it's truly real-time. Using keyboard input, it's possible to navigate the simulated world in 720p resolution at 24 frames per second. Perhaps even more importantly, Genie 3 can remember the world it creates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Visual elements remain consistent when they return to the frame.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;One of the most glaring limitations of Genie 2 was its limited memory, which topped out at around 10 seconds in most simulations. Similarly to a chatbot that exceeds its context window, the model would forget what parts of the world looked like after they were out of view for a brief time. Google called Genie 2's meager retention "long horizon memory" when it unveiled that model. How quickly things change. The horizon for Genie 3 is &lt;em&gt;much&lt;/em&gt; longer, pushing the bounds of world models with multiple minutes of visual consistency.&lt;/p&gt;
&lt;h2&gt;An imperfect world&lt;/h2&gt;
&lt;p&gt;Genie 3 is not a perfect world builder yet. The ability to retain details for multiple minutes could unlock more uses, but the team acknowledges that you'd ideally want a model to remain consistent for hours at least. The model also can't simulate real-world locations—everything it generates is unique and non-deterministic. That means it's also prone to the typical AI hallucinations. The team says Genie 3 has made great strides in accuracy, but it does still produce incorrect video elements. For example, the nuance of human locomotion sometimes gets lost in the generative shuffle, producing people who appear to walk backward. Text in these AI worlds is also a jumble unless the prompt includes specific strings for the model to include.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Genie 3 offers much higher visual fidelity compared to Genie 2.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The way AI agents integrate into world models is limited, too. While you can create worlds and promptable events with realistic conditions, agents don't have a role in that. Their interaction with the simulated world is limited to moving around inside it, as current agents lack the high-level reasoning necessary to alter the simulation. DeepMind is also still experimenting with ways to allow multiple AI agents to interact with each other inside a shared environment. So maybe we'll see that in Genie 4 in a few more months?&lt;/p&gt;
&lt;p&gt;Even those willing to pay hundreds of dollars per month for premium AI subscriptions have learned there are limits on usage for the largest and most expensive models. Genie 3 is essentially rendering a very long video so quickly that it appears interactive, which surely uses a ton of processing power. Google DeepMind isn't offering any specifics on this, but the fact that you can't use it says volumes.&lt;/p&gt;
&lt;p&gt;Genie 3 remains a research tool, but one with capabilities DeepMind clearly wants to show off. The team plans to grant access to a group of experts and researchers who will help refine the model. They suggest, however, that the plan is to open access to Genie world models to more people.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Genie 3 creates detailed worlds from a prompt or image.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/vlcsnap-2025-08-04-14h32m46s918-640x360.png" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/vlcsnap-2025-08-04-14h32m46s918-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google DeepMind

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;While no one has figured out how to make money from generative artificial intelligence, that hasn't stopped Google DeepMind from pushing the boundaries of what's possible with a big pile of inference. The capabilities (and costs) of these models have been on an impressive upward trajectory, a trend exemplified by the reveal of Genie 3. A mere seven months after showing off the Genie 2 "foundational world model," which was itself a significant improvement over its predecessor, Google now has Genie 3.&lt;/p&gt;
&lt;p&gt;With Genie 3, all it takes is a prompt or image to create an interactive world. Since the environment is continuously generated, it can be changed on the fly. You can add or change objects, alter weather conditions, or insert new characters—DeepMind calls these "promptable events." The ability to create alterable 3D environments could make games more dynamic for players and offer developers new ways to prove out concepts and level designs. However, many in the gaming industry have expressed doubt that such tools would help.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Genie 3: building better worlds.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;It's tempting to think of Genie 3 simply as a way to create games, but DeepMind sees this as a research tool, too. Games play a significant role in the development of artificial intelligence because they provide challenging, interactive environments with measurable progress. That's why DeepMind previously turned to games like &lt;em&gt;Go&lt;/em&gt; and &lt;em&gt;StarCraft&lt;/em&gt; to expand the bounds of AI.&lt;/p&gt;
&lt;p&gt;World models take that to the next level, generating an interactive world frame by frame. This provides an opportunity to refine how AI models—including so-called "embodied agents"—behave when they encounter real-world situations. One of the primary limitations as companies work toward the goal of artificial general intelligence (AGI) is the scarcity of reliable training data. After piping basically every webpage and video on the planet into AI models, researchers are turning toward synthetic data for many applications. DeepMind believes world models could be a key part of this effort, as they can be used to train AI agents with essentially unlimited interactive worlds.&lt;/p&gt;
&lt;p&gt;DeepMind says Genie 3 is an important advancement because it offers much higher visual fidelity than Genie 2, and it's truly real-time. Using keyboard input, it's possible to navigate the simulated world in 720p resolution at 24 frames per second. Perhaps even more importantly, Genie 3 can remember the world it creates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Visual elements remain consistent when they return to the frame.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;One of the most glaring limitations of Genie 2 was its limited memory, which topped out at around 10 seconds in most simulations. Similarly to a chatbot that exceeds its context window, the model would forget what parts of the world looked like after they were out of view for a brief time. Google called Genie 2's meager retention "long horizon memory" when it unveiled that model. How quickly things change. The horizon for Genie 3 is &lt;em&gt;much&lt;/em&gt; longer, pushing the bounds of world models with multiple minutes of visual consistency.&lt;/p&gt;
&lt;h2&gt;An imperfect world&lt;/h2&gt;
&lt;p&gt;Genie 3 is not a perfect world builder yet. The ability to retain details for multiple minutes could unlock more uses, but the team acknowledges that you'd ideally want a model to remain consistent for hours at least. The model also can't simulate real-world locations—everything it generates is unique and non-deterministic. That means it's also prone to the typical AI hallucinations. The team says Genie 3 has made great strides in accuracy, but it does still produce incorrect video elements. For example, the nuance of human locomotion sometimes gets lost in the generative shuffle, producing people who appear to walk backward. Text in these AI worlds is also a jumble unless the prompt includes specific strings for the model to include.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Genie 3 offers much higher visual fidelity compared to Genie 2.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;The way AI agents integrate into world models is limited, too. While you can create worlds and promptable events with realistic conditions, agents don't have a role in that. Their interaction with the simulated world is limited to moving around inside it, as current agents lack the high-level reasoning necessary to alter the simulation. DeepMind is also still experimenting with ways to allow multiple AI agents to interact with each other inside a shared environment. So maybe we'll see that in Genie 4 in a few more months?&lt;/p&gt;
&lt;p&gt;Even those willing to pay hundreds of dollars per month for premium AI subscriptions have learned there are limits on usage for the largest and most expensive models. Genie 3 is essentially rendering a very long video so quickly that it appears interactive, which surely uses a ton of processing power. Google DeepMind isn't offering any specifics on this, but the fact that you can't use it says volumes.&lt;/p&gt;
&lt;p&gt;Genie 3 remains a research tool, but one with capabilities DeepMind clearly wants to show off. The team plans to grant access to a group of experts and researchers who will help refine the model. They suggest, however, that the plan is to open access to Genie world models to more people.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/08/deepmind-reveals-genie-3-world-model-that-creates-real-time-interactive-simulations/</guid><pubDate>Tue, 05 Aug 2025 14:00:36 +0000</pubDate></item><item><title>[NEW] DeepMind thinks its new Genie 3 world model presents a stepping stone toward AGI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/deepmind-thinks-genie-3-world-model-presents-stepping-stone-towards-agi/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind has revealed Genie 3, its latest foundation world model that can be used to train general-purpose AI agents, a capability that the AI lab says makes for a crucial stepping stone on the path to “artificial general intelligence,” or human-like intelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Genie 3 is the first real-time interactive general-purpose world model,” Shlomi Fruchter, a research director at DeepMind, said during a press briefing. “It goes beyond narrow world models that existed before. It’s not specific to any particular environment. It can generate both photo-realistic and imaginary worlds, and everything in between.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still in research preview and not publicly available, Genie 3 builds on both its predecessor Genie 2 (which can generate new environments for agents) and DeepMind’s latest video generation model Veo 3 (which is said to have a deep understanding of physics).&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3034135" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/Real-time-Interactivity.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With a simple text prompt, Genie 3 can generate multiple minutes of interactive 3D environments at 720p resolution at 24 frames per second — a significant jump from the 10 to 20 seconds Genie 2 could produce. The model also features “promptable world events,” or the ability to use a prompt to change the generated world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps most importantly, Genie 3’s simulations stay physically consistent over time because the model can remember what it previously generated — a capability that DeepMind says its researchers didn’t explicitly program into the model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fruchter said that while Genie 3 has implications for educational experiences, gaming or prototyping creative concepts, its real unlock will manifest in training agents for general-purpose tasks, which he said is essential to reaching AGI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think world models are key on the path to AGI, specifically for embodied agents, where simulating real world scenarios is particularly challenging,” Jack Parker-Holder, a research scientist on DeepMind’s open-endedness team, said during the briefing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3034136" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/Prompt-to-World.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Genie 3 is supposedly designed to solve that bottleneck. Like Veo, it doesn’t rely on a hard-coded physics engine; instead, DeepMind says, the model teaches itself how the world works — how objects move, fall, and interact — by remembering what it has generated and reasoning over long time horizons.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The model is auto-regressive, meaning it generates one frame at a time,” Fruchter told TechCrunch in an interview. “It has to look back at what was generated before to decide what’s going to happen next. That’s a key part of the architecture.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That memory, the company says, lends to consistency in Genie 3’s simulated worlds, which in turn allows it to develop a grasp of physics, similar to how humans understand that a glass teetering on the edge of a table is about to fall, or that they should duck to avoid a falling object.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Notably, DeepMind says the model also has the potential to push AI agents to their limits — forcing them to learn from their own experience, similar to how humans learn in the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an example, DeepMind shared its test of Genie 3 with a recent version of its generalist Scalable Instructable Multiworld Agent (SIMA), instructing it to pursue a set of goals. In a warehouse setting, they asked the agent to perform tasks like “approach the bright green trash compactor” or “walk to the packed red forklift.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In all three cases, the SIMA agent is able to achieve the goal,” Parker-Holder said. “It just receives the actions from the agent. So the agent takes the goal, sees the world simulated around it, and then takes the actions in the world. Genie 3 simulates forward, and the fact that it’s able to achieve it is because Genie 3 remains consistent.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3034137" height="270" src="https://techcrunch.com/wp-content/uploads/2025/08/Prompt-Event.gif?w=480" width="480" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Genie 3 has its limitations. For example, while the researchers claim it can understand physics, the demo showing a skier barreling down a mountain didn’t reflect how snow would move in relation to the skier. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the range of actions an agent can take is limited. For example, the promptable world events allow for a wide range of environmental interventions, but they’re not necessarily performed by the agent itself. And it’s still difficult to accurately model complex interactions between multiple independent agents in a shared environment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Genie 3 can also only support a few minutes of continuous interaction, when hours would be necessary for proper training.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the model presents a compelling step forward in teaching agents to go beyond reacting to inputs, letting them potentially plan, explore, seek out uncertainty, and improve through trial and error — the kind of self-driven, embodied learning that many say is key to moving toward general intelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We haven’t really had a Move 37 moment for embodied agents yet, where they can actually take novel actions in the real world,” Parker-Holder said, referring to the legendary moment in the 2016 game of Go between DeepMind’s AI agent AlphaGo and world champion Lee Sedol, in which Alpha Go played an unconventional and brilliant move that became symbolic of AI’s ability to discover new strategies beyond human understanding.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“But now, we can potentially usher in a new era,” he said.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google DeepMind has revealed Genie 3, its latest foundation world model that can be used to train general-purpose AI agents, a capability that the AI lab says makes for a crucial stepping stone on the path to “artificial general intelligence,” or human-like intelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Genie 3 is the first real-time interactive general-purpose world model,” Shlomi Fruchter, a research director at DeepMind, said during a press briefing. “It goes beyond narrow world models that existed before. It’s not specific to any particular environment. It can generate both photo-realistic and imaginary worlds, and everything in between.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still in research preview and not publicly available, Genie 3 builds on both its predecessor Genie 2 (which can generate new environments for agents) and DeepMind’s latest video generation model Veo 3 (which is said to have a deep understanding of physics).&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3034135" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/Real-time-Interactivity.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With a simple text prompt, Genie 3 can generate multiple minutes of interactive 3D environments at 720p resolution at 24 frames per second — a significant jump from the 10 to 20 seconds Genie 2 could produce. The model also features “promptable world events,” or the ability to use a prompt to change the generated world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps most importantly, Genie 3’s simulations stay physically consistent over time because the model can remember what it previously generated — a capability that DeepMind says its researchers didn’t explicitly program into the model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fruchter said that while Genie 3 has implications for educational experiences, gaming or prototyping creative concepts, its real unlock will manifest in training agents for general-purpose tasks, which he said is essential to reaching AGI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We think world models are key on the path to AGI, specifically for embodied agents, where simulating real world scenarios is particularly challenging,” Jack Parker-Holder, a research scientist on DeepMind’s open-endedness team, said during the briefing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3034136" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/Prompt-to-World.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Genie 3 is supposedly designed to solve that bottleneck. Like Veo, it doesn’t rely on a hard-coded physics engine; instead, DeepMind says, the model teaches itself how the world works — how objects move, fall, and interact — by remembering what it has generated and reasoning over long time horizons.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The model is auto-regressive, meaning it generates one frame at a time,” Fruchter told TechCrunch in an interview. “It has to look back at what was generated before to decide what’s going to happen next. That’s a key part of the architecture.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That memory, the company says, lends to consistency in Genie 3’s simulated worlds, which in turn allows it to develop a grasp of physics, similar to how humans understand that a glass teetering on the edge of a table is about to fall, or that they should duck to avoid a falling object.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Notably, DeepMind says the model also has the potential to push AI agents to their limits — forcing them to learn from their own experience, similar to how humans learn in the real world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an example, DeepMind shared its test of Genie 3 with a recent version of its generalist Scalable Instructable Multiworld Agent (SIMA), instructing it to pursue a set of goals. In a warehouse setting, they asked the agent to perform tasks like “approach the bright green trash compactor” or “walk to the packed red forklift.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In all three cases, the SIMA agent is able to achieve the goal,” Parker-Holder said. “It just receives the actions from the agent. So the agent takes the goal, sees the world simulated around it, and then takes the actions in the world. Genie 3 simulates forward, and the fact that it’s able to achieve it is because Genie 3 remains consistent.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3034137" height="270" src="https://techcrunch.com/wp-content/uploads/2025/08/Prompt-Event.gif?w=480" width="480" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google DeepMind&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;That said, Genie 3 has its limitations. For example, while the researchers claim it can understand physics, the demo showing a skier barreling down a mountain didn’t reflect how snow would move in relation to the skier. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, the range of actions an agent can take is limited. For example, the promptable world events allow for a wide range of environmental interventions, but they’re not necessarily performed by the agent itself. And it’s still difficult to accurately model complex interactions between multiple independent agents in a shared environment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Genie 3 can also only support a few minutes of continuous interaction, when hours would be necessary for proper training.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the model presents a compelling step forward in teaching agents to go beyond reacting to inputs, letting them potentially plan, explore, seek out uncertainty, and improve through trial and error — the kind of self-driven, embodied learning that many say is key to moving toward general intelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We haven’t really had a Move 37 moment for embodied agents yet, where they can actually take novel actions in the real world,” Parker-Holder said, referring to the legendary moment in the 2016 game of Go between DeepMind’s AI agent AlphaGo and world champion Lee Sedol, in which Alpha Go played an unconventional and brilliant move that became symbolic of AI’s ability to discover new strategies beyond human understanding.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“But now, we can potentially usher in a new era,” he said.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/deepmind-thinks-genie-3-world-model-presents-stepping-stone-towards-agi/</guid><pubDate>Tue, 05 Aug 2025 14:10:14 +0000</pubDate></item><item><title>[NEW] Crack the code to startup traction with insights from Chef Robotics, NEA, and ICONIQ at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/crack-the-code-to-startup-traction-with-insights-from-chef-robotics-nea-and-iconiq-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Finding product-market fit isn’t a milestone — it’s a messy, make-or-break journey. And at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, a founder who’s been through the fire and two investors who’ve helped startups hit escape velocity will break down how to do it right.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the &lt;strong&gt;Builders Stage&lt;/strong&gt;, Rajat Bhageria (Founder &amp;amp; CEO, Chef Robotics), Ann Bordetsky (Partner, NEA), and Murali Joshi (Partner, ICONIQ) join forces to unpack the most critical — and elusive — phase in a startup’s life cycle. They’ll dive into smart testing strategies, real-time iteration, and how to listen to your users without getting lost in the noise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this Disrupt 2025 conversation — and don’t miss your chance to save up to $675. Prices rise after tomorrow, August 6, at 11:59 p.m. PT. &lt;strong&gt;Register now.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Rajat Bhageria, Ann Bordetsky, Murali Joshi" class="wp-image-3034069" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Bhageria-Bordetsky-Joshi-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-no-more-guessing-just-growth"&gt;No more guessing — just growth&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Bhageria brings the founder POV, having scaled Chef Robotics with AI-powered automation that’s now transforming food production. Bordetsky, with her background at Uber, Twitter, and now NEA, knows how to spot the kind of scrappy ingenuity that leads to breakout success. Joshi, fresh off a spot on the Forbes Midas Brink List, has helped drive over $2.5 billion in investments in companies like Drata, 1Password, and Fivetran.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, they’ll offer a rare behind-the-scenes look at what product-market fit &lt;em&gt;actually&lt;/em&gt; looks like — and how to know when you’ve got it.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-building-a-hit-product-starts-here"&gt;Building a hit product starts here&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re still in prototype mode or trying to scale something that already has traction, this panel is for founders who are tired of guesswork and ready to build something customers can’t live without.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Catch it on the &lt;strong&gt;Builders Stage&lt;/strong&gt; at TechCrunch Disrupt 2025, happening October 27-29 at Moscone West in San Francisco. Join 10,000+ startup and VC leaders for the most important conversation in tech. &lt;strong&gt;Grab your pass&lt;/strong&gt; before tomorrow ends to save up to $675.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Finding product-market fit isn’t a milestone — it’s a messy, make-or-break journey. And at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, a founder who’s been through the fire and two investors who’ve helped startups hit escape velocity will break down how to do it right.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On the &lt;strong&gt;Builders Stage&lt;/strong&gt;, Rajat Bhageria (Founder &amp;amp; CEO, Chef Robotics), Ann Bordetsky (Partner, NEA), and Murali Joshi (Partner, ICONIQ) join forces to unpack the most critical — and elusive — phase in a startup’s life cycle. They’ll dive into smart testing strategies, real-time iteration, and how to listen to your users without getting lost in the noise.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this Disrupt 2025 conversation — and don’t miss your chance to save up to $675. Prices rise after tomorrow, August 6, at 11:59 p.m. PT. &lt;strong&gt;Register now.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Rajat Bhageria, Ann Bordetsky, Murali Joshi" class="wp-image-3034069" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Bhageria-Bordetsky-Joshi-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-no-more-guessing-just-growth"&gt;No more guessing — just growth&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Bhageria brings the founder POV, having scaled Chef Robotics with AI-powered automation that’s now transforming food production. Bordetsky, with her background at Uber, Twitter, and now NEA, knows how to spot the kind of scrappy ingenuity that leads to breakout success. Joshi, fresh off a spot on the Forbes Midas Brink List, has helped drive over $2.5 billion in investments in companies like Drata, 1Password, and Fivetran.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Together, they’ll offer a rare behind-the-scenes look at what product-market fit &lt;em&gt;actually&lt;/em&gt; looks like — and how to know when you’ve got it.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-building-a-hit-product-starts-here"&gt;Building a hit product starts here&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re still in prototype mode or trying to scale something that already has traction, this panel is for founders who are tired of guesswork and ready to build something customers can’t live without.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Catch it on the &lt;strong&gt;Builders Stage&lt;/strong&gt; at TechCrunch Disrupt 2025, happening October 27-29 at Moscone West in San Francisco. Join 10,000+ startup and VC leaders for the most important conversation in tech. &lt;strong&gt;Grab your pass&lt;/strong&gt; before tomorrow ends to save up to $675.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/crack-the-code-to-startup-traction-with-insights-from-chef-robotics-nea-and-iconiq-at-techcrunch-disrupt-2025/</guid><pubDate>Tue, 05 Aug 2025 14:30:00 +0000</pubDate></item><item><title>[NEW] Trump says he’ll announce semiconductor and chip tariffs (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/trump-says-hell-announce-semiconductor-and-chip-tariffs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-1223301957.jpg?resize=1200,646" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The semiconductor industry’s rollercoaster year continues with another major development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;President Donald Trump said on CNBC’s Squawk Box on Tuesday that his administration is planning to announce tariffs on semiconductors and chips as soon as next week. However, the specifics of these tariffs remain unclear.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Such tariffs could cause quite a disruption for U.S. hardware and AI companies. When the U.S. CHIPs and Science Act was signed in 2022 — providing $52 billion in subsidies to boost domestic chip manufacturing — the U.S. produced only about 10% of global chips. Despite this small manufacturing footprint, more than half of global semiconductor companies are headquartered in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since then, some progress has been made toward boosting domestic chip manufacturing. Both Intel and Taiwan Semiconductor Manufacturing Company (TSMC) have received funding from the CHIPs Act. TSMC has also committed to spending “at least” $100 billion over the next four years on chip manufacturing plants in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But setting up chip manufacturing plants takes time. Intel recently announced it was delaying construction on its Ohio chip manufacturing facility, again, highlighting the challenges of rapidly scaling up production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tariff announcement comes as the industry awaits the administration’s decision on AI chip export restrictions — rules that control which countries can purchase advanced semiconductors used in AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration formally rescinded the Biden administration’s chip AI export rules in May. Those rules had established a country-specific, multi-tier approach to restricting chip exports based on national security concerns. The Trump administration then released its AI Action Plan in July, which emphasized the need for the U.S. to implement chip export restrictions but was light on the details of what that could look like.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;According to reporting from Semafor citing industry sources, the Trump administration is now debating whether or not it should go through with its plan to rescind and replace Biden’s AI export rules.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For more on the semiconductor industry’s tumultuous year, we’ve compiled a regularly updated timeline of market news since the beginning of 2025.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/GettyImages-1223301957.jpg?resize=1200,646" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The semiconductor industry’s rollercoaster year continues with another major development.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;President Donald Trump said on CNBC’s Squawk Box on Tuesday that his administration is planning to announce tariffs on semiconductors and chips as soon as next week. However, the specifics of these tariffs remain unclear.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Such tariffs could cause quite a disruption for U.S. hardware and AI companies. When the U.S. CHIPs and Science Act was signed in 2022 — providing $52 billion in subsidies to boost domestic chip manufacturing — the U.S. produced only about 10% of global chips. Despite this small manufacturing footprint, more than half of global semiconductor companies are headquartered in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since then, some progress has been made toward boosting domestic chip manufacturing. Both Intel and Taiwan Semiconductor Manufacturing Company (TSMC) have received funding from the CHIPs Act. TSMC has also committed to spending “at least” $100 billion over the next four years on chip manufacturing plants in the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But setting up chip manufacturing plants takes time. Intel recently announced it was delaying construction on its Ohio chip manufacturing facility, again, highlighting the challenges of rapidly scaling up production.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tariff announcement comes as the industry awaits the administration’s decision on AI chip export restrictions — rules that control which countries can purchase advanced semiconductors used in AI systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Trump administration formally rescinded the Biden administration’s chip AI export rules in May. Those rules had established a country-specific, multi-tier approach to restricting chip exports based on national security concerns. The Trump administration then released its AI Action Plan in July, which emphasized the need for the U.S. to implement chip export restrictions but was light on the details of what that could look like.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;According to reporting from Semafor citing industry sources, the Trump administration is now debating whether or not it should go through with its plan to rescind and replace Biden’s AI export rules.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For more on the semiconductor industry’s tumultuous year, we’ve compiled a regularly updated timeline of market news since the beginning of 2025.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/trump-says-hell-announce-semiconductor-and-chip-tariffs/</guid><pubDate>Tue, 05 Aug 2025 15:07:12 +0000</pubDate></item><item><title>[NEW] ElevenLabs launches an AI music generator, which it claims is cleared for commercial use (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/elevenlabs-launches-an-ai-music-generator-which-it-claims-is-cleared-for-commercial-use/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-co-founders.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI audio-generation unicorn ElevenLabs announced a new model on Tuesday that allows users to generate music, which it claims is cleared for commercial use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move marks ElevenLabs’ expansion beyond its main focus thus far in its three years of existence, which has been building AI audio tools. ElevenLabs is a leader among companies making text-to-speech AI products, and it has expanded into conversational bots and tools that translate speech into other languages. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Eleven Labs shared samples of its AI-generated music.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One features a synthetic voice rapping about how it “came up through the cracks with ambition in my pocket” and left its hometown, traveling from “Compton to the Cosmos.” It’s unsettling to hear a computer reflect the influence and language of artists like Dr. Dre, N.W.A., and Kendrick Lamar, who actually lived the experiences that this technology is attempting to emulate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given these concerns around what material AI music generation tools are trained on, it’s not so straightforward for startups to delve into music generation. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Suno and Udio were sued by the Recording Industry Association of America (RIAA), the trade organization that represents the U.S. music industry. These lawsuits allege that Suno and Udio trained their music-generation models on copyrighted material. The companies are now reportedly discussing licensing deals with major record labels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs also announced deals with Merlin Network and Kobalt Music Group, two digital publishing platforms for independent musicians, to use their materials for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Merlin’s website, the company represents major artists like Adele, Nirvana, Mitski, Carly Rae Jepsen, and Phoebe Bridgers; Kobalt represents stars like Beck, Bon Iver, and Childish Gambino.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A representative from Kobalt told TechCrunch that artists have to voluntarily opt-in for their music to be licensed for AI use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our clients benefit directly from this agreement in several key ways: it opens a new revenue stream in a growing market, includes revenue sharing so they participate in the upside, provides strong safeguards against infringement and misuse, and offers favorable terms comparable to other publishing and recording rightsholders,” the Kobalt representative told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated, 12:50 PM ET, with comment from Kobalt.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/ElevenLabs-co-founders.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The AI audio-generation unicorn ElevenLabs announced a new model on Tuesday that allows users to generate music, which it claims is cleared for commercial use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This move marks ElevenLabs’ expansion beyond its main focus thus far in its three years of existence, which has been building AI audio tools. ElevenLabs is a leader among companies making text-to-speech AI products, and it has expanded into conversational bots and tools that translate speech into other languages. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alongside the launch, Eleven Labs shared samples of its AI-generated music.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One features a synthetic voice rapping about how it “came up through the cracks with ambition in my pocket” and left its hometown, traveling from “Compton to the Cosmos.” It’s unsettling to hear a computer reflect the influence and language of artists like Dr. Dre, N.W.A., and Kendrick Lamar, who actually lived the experiences that this technology is attempting to emulate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given these concerns around what material AI music generation tools are trained on, it’s not so straightforward for startups to delve into music generation. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Suno and Udio were sued by the Recording Industry Association of America (RIAA), the trade organization that represents the U.S. music industry. These lawsuits allege that Suno and Udio trained their music-generation models on copyrighted material. The companies are now reportedly discussing licensing deals with major record labels.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ElevenLabs also announced deals with Merlin Network and Kobalt Music Group, two digital publishing platforms for independent musicians, to use their materials for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Merlin’s website, the company represents major artists like Adele, Nirvana, Mitski, Carly Rae Jepsen, and Phoebe Bridgers; Kobalt represents stars like Beck, Bon Iver, and Childish Gambino.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A representative from Kobalt told TechCrunch that artists have to voluntarily opt-in for their music to be licensed for AI use. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our clients benefit directly from this agreement in several key ways: it opens a new revenue stream in a growing market, includes revenue sharing so they participate in the upside, provides strong safeguards against infringement and misuse, and offers favorable terms comparable to other publishing and recording rightsholders,” the Kobalt representative told TechCrunch.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Updated, 12:50 PM ET, with comment from Kobalt.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/elevenlabs-launches-an-ai-music-generator-which-it-claims-is-cleared-for-commercial-use/</guid><pubDate>Tue, 05 Aug 2025 15:10:35 +0000</pubDate></item><item><title>[NEW] Three weeks after acquiring Windsurf, Cognition offers staff the exit door (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/three-weeks-after-acquiring-windsurf-cognition-offers-staff-the-exit-door/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.02.07AM.png?resize=1200,645" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cognition, the AI coding startup that acquired rival company Windsurf three weeks ago, laid off 30 employees last week and is offering buyouts to the roughly 200 remaining employees on the team, reports The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the latest bout of whiplash that Windsurf employees have faced after a tumultuous stretch for the company. The startup was initially almost acquired by OpenAI, then lost its CEO, co-founder, and research leads to Google in a $2.4 billion deal known as a reverse-acquihire (where Google hired the key talent rather than buying the company), before ultimately getting acquired by Cognition.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the time of the acquisition, Cognition stated that 100% of Windsurf employees would receive financial compensation as part of the deal and stressed that the company was excited to bring on Windsurf’s “world-class people” to develop top-notch coding tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, it’s becoming clear that Windsurf’s intellectual property, not its talent, was the real buy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to an email The Information viewed, employees were given until August 10 to decide whether they want to take the buyout, which amounts to nine months of salary. Those who choose to stay are reportedly required to spend six days at the office and clock more than 80-hour weeks — draconian conditions that have become table stakes among workers at top AI firms.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t believe in work-life balance—building the future of software engineering is a mission we all care so deeply about that we couldn’t possibly separate the two,” wrote Cognition CEO Scott Wu in the email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Cognition for more details. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.02.07AM.png?resize=1200,645" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Cognition, the AI coding startup that acquired rival company Windsurf three weeks ago, laid off 30 employees last week and is offering buyouts to the roughly 200 remaining employees on the team, reports The Information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the latest bout of whiplash that Windsurf employees have faced after a tumultuous stretch for the company. The startup was initially almost acquired by OpenAI, then lost its CEO, co-founder, and research leads to Google in a $2.4 billion deal known as a reverse-acquihire (where Google hired the key talent rather than buying the company), before ultimately getting acquired by Cognition.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At the time of the acquisition, Cognition stated that 100% of Windsurf employees would receive financial compensation as part of the deal and stressed that the company was excited to bring on Windsurf’s “world-class people” to develop top-notch coding tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, it’s becoming clear that Windsurf’s intellectual property, not its talent, was the real buy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to an email The Information viewed, employees were given until August 10 to decide whether they want to take the buyout, which amounts to nine months of salary. Those who choose to stay are reportedly required to spend six days at the office and clock more than 80-hour weeks — draconian conditions that have become table stakes among workers at top AI firms.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t believe in work-life balance—building the future of software engineering is a mission we all care so deeply about that we couldn’t possibly separate the two,” wrote Cognition CEO Scott Wu in the email.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Cognition for more details. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/three-weeks-after-acquiring-windsurf-cognition-offers-staff-the-exit-door/</guid><pubDate>Tue, 05 Aug 2025 15:24:19 +0000</pubDate></item><item><title>[NEW] VeriTrail: Detecting hallucination and tracing provenance in multi-step AI workflows (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: Two white icons on a blue-to-green gradient background—one showing a central figure linked to others, representing a network, and the other depicting lines connecting to a document, symbolizing data flow." class="wp-image-1145818" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;







&lt;p&gt;Many applications of language models (LMs) involve generating content based on source material, such as answering questions, summarizing information, and drafting documents. A critical challenge for these applications is that LMs may produce content that is not supported by the source text – a phenomenon known as “closed-domain hallucination.”&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Existing methods for detecting closed-domain hallucination typically compare a given LM output to the source text, implicitly assuming that there is only a single output to evaluate. However, applications of LMs increasingly involve processes with multiple generative steps: LMs generate intermediate outputs that serve as inputs to subsequent steps and culminate in a final output. Many agentic workflows follow this paradigm (e.g., each agent is responsible for a specific document or sub-task, and their outputs are synthesized into a final response). &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability,” we argue that, given the complexity of processes with multiple generative steps, detecting hallucination in the final output is necessary but not sufficient. We also need &lt;strong&gt;traceability&lt;/strong&gt;, which has two components:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;if the final output is supported by the source text, we should be able to trace its path through the intermediate outputs to the source.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;if the final output is not supported by the source text, we should be able to trace where the error was likely introduced.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;Our paper presents VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for processes with any number of generative steps. We also demonstrate that VeriTrail outperforms baseline methods commonly used for hallucination detection. In this blog post, we provide an overview of VeriTrail’s design and performance.&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="veritrail-s-hallucination-detection-process"&gt;VeriTrail’s hallucination detection process&lt;/h2&gt;



&lt;p&gt;A key idea leveraged by VeriTrail is that a wide range of generative processes can be represented as a &lt;strong&gt;directed acyclic graph (DAG)&lt;/strong&gt;. Each &lt;strong&gt;node &lt;/strong&gt;in the DAG represents a piece of text (i.e., source material, an intermediate output, or the final output) and each &lt;strong&gt;edge &lt;/strong&gt;from node A to node B indicates that A was used as an input to produce B. Each node is assigned a unique ID, as well as a &lt;strong&gt;stage&lt;/strong&gt; reflecting its position in the generative process. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;An example of a process with multiple generative steps is GraphRAG. A DAG representing a GraphRAG run is illustrated in Figure 1, where the boxes and arrows correspond to nodes and edges, respectively.&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A GraphRAG run is depicted as a directed acyclic graph. The Stage 1 nodes represent source text chunks. Each Stage 1 node has an edge pointing to a Stage 2 node, which corresponds to an entity or a relationship. Entity 3 was extracted from two source text chunks, so its descriptions are summarized. The summarized entity description forms a Stage 3 node. The Stage 2 and 3 nodes have edges pointing to Stage 4 nodes, which represent community reports. The Stage 4 nodes have edges pointing to Stage 5 nodes, which correspond to map-level answers. The Stage 5 nodes each have an edge pointing to the terminal node, which represents the final answer. The terminal node is the only node in Stage 6." class="wp-image-1145841" height="542" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1.jpg" width="881" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: GraphRAG splits the source text into chunks (Stage 1). For each chunk, an LM extracts entities and relationships (the latter are denoted by “⭤ “), along with short descriptions (Stage 2). If an entity or a relationship was extracted from multiple chunks, an LM summarizes the descriptions (Stage 3). A knowledge graph is constructed from the final set of entities and relationships, and a community detection algorithm, such as Leiden clustering, groups entities into communities. For each community, an LM generates a “community report” that summarizes the entities and relationships (Stage 4). To answer a user’s question, an LM generates “map-level answers” based on groups of community reports (Stage 5), then synthesizes them into a final answer (Stage 6).&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;VeriTrail takes as input a DAG representing a completed generative process and aims to determine whether the final output is fully supported by the source text. It begins by extracting claims (i.e., self-contained, verifiable statements) from the final output using Claimify. VeriTrail verifies claims in the reverse order of the generative process: it starts from the final output and moves toward the source text. Each claim is verified separately. Below, we include two case studies that illustrate how VeriTrail works, using the DAG from Figure 1. &lt;/p&gt;



&lt;h3 class="wp-block-heading" id="case-study-1-a-fully-supported-claim"&gt;Case study 1: A “Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Fully Supported.” A claim extracted from the terminal node, Node 17, is “Legislative efforts have been made to address the high cost of diabetes-related supplies in the US.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. The sentence “The general assembly in North Carolina is considering legislation to set a cap on insulin prices, which indicates that high insulin prices are a contributing factor to the high cost of diabetes-related supplies in the US” is selected as evidence from Node 15. The tentative verdict is “Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12 and 13, which are the source nodes of Node 15. The sentence “The General Assembly in North Carolina is considering legislation to set a cap on insulin prices” is selected as evidence from Node 13. The verdict remains “Fully Supported.” In Iteration 3, VeriTrail checks Nodes 4, 5, and 11, which are the source nodes of Node 13. The sentence “The General Assembly is the legislative body in North Carolina considering legislation to cap insulin prices” is selected as evidence from Node 4. The verdict is still “Fully Supported.” In Iteration 4, VeriTrail checks Node 1, which is the source node of Node 4. The selected evidence is “‘There’s actually legislation in North Carolina at the General Assembly to set a cap on insulin…’ Stein said.” The corresponding verdict is “Fully Supported.” Since Node 1 represents a raw text chunk, it does not have any source nodes to check. Therefore, verification terminates and the “Fully Supported” verdict is deemed final." class="wp-image-1145843" height="720" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Fully Supported” claim.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 2 shows an example of a claim that VeriTrail determined was not hallucinated: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes that were used as inputs for the final answer: Nodes 15 and 16. Each identified node was split into sentences, and each sentence was programmatically assigned a unique ID.
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;An LM then performed &lt;strong&gt;Evidence Selection&lt;/strong&gt;, selecting all sentence IDs that strongly implied the truth or falsehood of the claim. The LM also generated a summary of the selected sentences (not shown in Figure 2). In this example, a sentence was selected from Node 15.&lt;/li&gt;



&lt;li&gt;Next, an LM performed &lt;strong&gt;Verdict Generation&lt;/strong&gt;. If no sentences had been selected in the Evidence Selection step, the claim would have been assigned a “Not Fully Supported” verdict. Instead, an LM was prompted to classify the claim as “Fully Supported,” “Not Fully Supported,” or “Inconclusive” based on the evidence. In this case, the verdict was “Fully Supported.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;Since the verdict in Iteration 1 was “Fully Supported,” VeriTrail proceeded to Iteration 2. It considered the nodes from which at least one sentence was selected in the latest Evidence Selection step (Node 15) and identified their input nodes (Nodes 12 and 13). VeriTrail repeated Evidence Selection and Verdict Generation for the identified nodes. Once again, the verdict was “Fully Supported.” This process – identifying candidate nodes, performing Evidence Selection and Verdict Generation – was repeated in Iteration 3, where the verdict was still “Fully Supported,” and likewise in Iteration 4. &lt;/li&gt;



&lt;li&gt;In Iteration 4, a single source text chunk was verified. Since the source text, by definition, does not have any inputs, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



&lt;h3 class="wp-block-heading" id="case-study-2-a-not-fully-supported-claim"&gt;Case study 2: A “Not Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Not Fully Supported.” We assume that the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. A claim extracted from the terminal node, Node 17, is “Challenges related to electric vehicle battery repairability contribute to sluggish retail auto sales in China.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. Two sentences are selected as evidence. The first sentence is “Challenges with electric vehicle (EV) battery disposal and repair may also contribute to the sluggishness in retail auto sales.” The second sentence is “Junkyards are accumulating discarded EV battery packs, while collision shops face limitations in repairing EV battery packs, which could affect consumer confidence and demand.” These sentences are both from Node 15. The tentative verdict is “Not Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12, 13, and 14. Nodes 12 and 13 are the source nodes of Node 15. Node 14 is the source node of Node 16, which was checked in Iteration 1. The sentence “The electric vehicle market in China is influenced by challenges associated with EV battery disposal and repair” is selected as evidence from Node 12. The verdict remains “Not Fully Supported.” Since two consecutive “Not Fully Supported” verdicts have been reached, which was the maximum, verification terminates and the final verdict is “Not Fully Supported.”" class="wp-image-1145842" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Not Fully Supported” claim, where the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 3 provides an example of a claim where VeriTrail identified hallucination:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes used as inputs for the final answer: Nodes 15 and 16. After Evidence Selection and Verdict Generation, the verdict was “Not Fully Supported.” Users can configure the maximum number of consecutive “Not Fully Supported” verdicts permitted. If the maximum had been set to 1, verification would have terminated here, and the verdict would have been deemed final. Let’s assume the maximum was set to 2, meaning that VeriTrail had to perform at least one more iteration.&lt;/li&gt;



&lt;li&gt;Even though evidence was selected only from Node 15 in Iteration 1, VeriTrail checked the input nodes for both Node 15 and Node 16 (i.e., Nodes 12, 13, and 14) in Iteration 2. Recall that in Case Study 1 where the verdict was “Fully Supported,” VeriTrail only checked the input nodes for Node 15. Why was the “Not Fully Supported” claim handled differently? If the Evidence Selection step overlooked relevant evidence, the “Not Fully Supported” verdict might be incorrect. In this case, continuing verification based solely on the selected evidence (i.e., Node 15) would propagate the mistake, defeating the purpose of repeated verification.&lt;/li&gt;



&lt;li&gt;In Iteration 2, Evidence Selection and Verdict Generation were repeated for Nodes 12, 13, and 14. Once again, the verdict was “Not Fully Supported.” Since this was the second consecutive “Not Fully Supported” verdict, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="providing-traceability"&gt;Providing traceability&lt;/h2&gt;



&lt;p&gt;In addition to assigning a final “Fully Supported,” “Not Fully Supported,” or “Inconclusive” verdict to each claim, VeriTrail returns (a) all Verdict Generation results and (b) an evidence trail composed of all Evidence Selection results: the selected sentences, their corresponding node IDs, and the generated summaries. Collectively, these outputs provide traceability:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;For “Fully Supported” and “Inconclusive” claims, the evidence trail traces a path from the source material to the final output, helping users understand how the output may have been derived. For example, in Case Study 1, the evidence trail consists of Sentence 8 from Node 15, Sentence 11 from Node 13, Sentence 26 from Node 4, and Sentence 79 from Node 1.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;For “Not Fully Supported” claims, VeriTrail uses the Verdict Generation results to identify the stage(s) of the process where the unsupported content was likely introduced. For instance, in Case Study 2, where none of the verified intermediate outputs supported the claim, VeriTrail would indicate that the hallucination occurred in the final answer (Stage 6). Error stage identification helps users address hallucinations and understand where in the process they are most likely to occur.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The evidence trail also helps users verify the verdict: instead of reading through all nodes – which may be infeasible for processes that generate large amounts of text – users can simply review the evidence sentences and summaries. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="key-design-features"&gt;Key design features&lt;/h2&gt;



&lt;p&gt;VeriTrail’s design prioritizes reliability, efficiency, scalability, and user agency. Notable features include: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;During Evidence Selection (introduced in Case Study 1), the sentence IDs returned by the LM are checked against the programmatically assigned IDs. If a returned ID does not match an assigned ID, it is discarded; otherwise, it is mapped to its corresponding sentence. This approach &lt;strong&gt;guarantees that the sentences included in the evidence trail are not hallucinated&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;After a claim is assigned an interim “Fully Supported” or “Inconclusive” verdict (as in Case Study 1), VeriTrail verifies the input nodes of only the nodes from which evidence was previously selected – not all possible input nodes. By progressively narrowing the search space, VeriTrail limits the number of nodes the LM must evaluate. In particular, since VeriTrail starts from the final output and moves toward the source text, it tends to verify a smaller proportion of nodes as it approaches the source text. Nodes closer to the source text tend to be larger (e.g., a book chapter should be larger than its summary), so verifying fewer of them helps &lt;strong&gt;reduce computational cost&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;VeriTrail is designed to &lt;strong&gt;handle input graphs with any number of nodes&lt;/strong&gt;, regardless of whether they fit in a single prompt. Users can specify an input size limit per prompt. For Evidence Selection, inputs that exceed the limit are split across multiple prompts. If the resulting evidence exceeds the input size limit for Verdict Generation, VeriTrail reruns Evidence Selection to compress the evidence further. Users can configure the maximum number of Evidence Selection reruns.  &lt;/li&gt;



&lt;li&gt;The configurable maximum number of consecutive “Not Fully Supported” verdicts (introduced in Case Study 2) allows the user to find their desired &lt;strong&gt;balance between computational cost and how conservative VeriTrail is in flagging hallucinations&lt;/strong&gt;. A lower maximum reduces cost by limiting the number of checks. A higher maximum increases confidence that a flagged claim is truly hallucinated since it requires repeated confirmation of the “Not Fully Supported” verdict. &lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="evaluating-veritrail-s-performance"&gt;Evaluating VeriTrail’s performance&lt;/h2&gt;



&lt;p&gt;We tested VeriTrail on two datasets covering distinct generative processes (hierarchical summarization&lt;sup&gt;4&lt;/sup&gt; and GraphRAG), tasks (summarization and question-answering), and types of source material (fiction novels and news articles). For the source material, we focused on long documents and large collections of documents (i.e., &amp;gt;100K tokens), where hallucination detection is especially challenging and processes with multiple generative steps are typically most valuable. The resulting DAGs were much more complex than the examples provided above (e.g., in one of the datasets, the average number of nodes was 114,368).&lt;/p&gt;



&lt;p&gt;We compared VeriTrail to three types of baseline methods commonly used for closed-domain hallucination detection: Natural Language Inference models (AlignScore and INFUSE); Retrieval-Augmented Generation; and long-context models (Gemini 1.5 Pro and GPT-4.1 mini). Across both datasets and all language models tested, VeriTrail outperformed the baseline methods in detecting hallucination.&lt;sup&gt;5&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Most importantly, VeriTrail traces claims through intermediate outputs – unlike the baseline methods, which directly compare the final output to the source material. As a result, it can identify where hallucinated content was likely introduced and how faithful content may have been derived from the source. By providing traceability, VeriTrail brings transparency to generative processes, helping users understand, verify, debug, and, ultimately, trust their outputs. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;For an in-depth discussion of VeriTrail, please see our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability.”&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; The term “closed-domain hallucination” was introduced by OpenAI in the GPT-4 Technical Report&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; VeriTrail is currently used for research purposes only and is not available commercially.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; We focus on GraphRAG’s global search method.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;4&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/sup&gt; In hierarchical summarization, an LM summarizes each source text chunk individually, then the resulting summaries are repeatedly grouped and summarized until a final summary is produced (Wu et al., 2021&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; Chang et al., 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;).&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; The only exception was the mistral-large-2411 model, where VeriTrail had the highest balanced accuracy, but not the highest macro F1 score.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Alt text: Two white icons on a blue-to-green gradient background—one showing a central figure linked to others, representing a network, and the other depicting lines connecting to a document, symbolizing data flow." class="wp-image-1145818" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;







&lt;p&gt;Many applications of language models (LMs) involve generating content based on source material, such as answering questions, summarizing information, and drafting documents. A critical challenge for these applications is that LMs may produce content that is not supported by the source text – a phenomenon known as “closed-domain hallucination.”&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Existing methods for detecting closed-domain hallucination typically compare a given LM output to the source text, implicitly assuming that there is only a single output to evaluate. However, applications of LMs increasingly involve processes with multiple generative steps: LMs generate intermediate outputs that serve as inputs to subsequent steps and culminate in a final output. Many agentic workflows follow this paradigm (e.g., each agent is responsible for a specific document or sub-task, and their outputs are synthesized into a final response). &amp;nbsp;&lt;/p&gt;



&lt;p&gt;In our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability,” we argue that, given the complexity of processes with multiple generative steps, detecting hallucination in the final output is necessary but not sufficient. We also need &lt;strong&gt;traceability&lt;/strong&gt;, which has two components:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;if the final output is supported by the source text, we should be able to trace its path through the intermediate outputs to the source.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;if the final output is not supported by the source text, we should be able to trace where the error was likely introduced.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;Our paper presents VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for processes with any number of generative steps. We also demonstrate that VeriTrail outperforms baseline methods commonly used for hallucination detection. In this blog post, we provide an overview of VeriTrail’s design and performance.&lt;sup&gt;2&lt;/sup&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="veritrail-s-hallucination-detection-process"&gt;VeriTrail’s hallucination detection process&lt;/h2&gt;



&lt;p&gt;A key idea leveraged by VeriTrail is that a wide range of generative processes can be represented as a &lt;strong&gt;directed acyclic graph (DAG)&lt;/strong&gt;. Each &lt;strong&gt;node &lt;/strong&gt;in the DAG represents a piece of text (i.e., source material, an intermediate output, or the final output) and each &lt;strong&gt;edge &lt;/strong&gt;from node A to node B indicates that A was used as an input to produce B. Each node is assigned a unique ID, as well as a &lt;strong&gt;stage&lt;/strong&gt; reflecting its position in the generative process. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;An example of a process with multiple generative steps is GraphRAG. A DAG representing a GraphRAG run is illustrated in Figure 1, where the boxes and arrows correspond to nodes and edges, respectively.&lt;sup&gt;3&lt;/sup&gt;&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="A GraphRAG run is depicted as a directed acyclic graph. The Stage 1 nodes represent source text chunks. Each Stage 1 node has an edge pointing to a Stage 2 node, which corresponds to an entity or a relationship. Entity 3 was extracted from two source text chunks, so its descriptions are summarized. The summarized entity description forms a Stage 3 node. The Stage 2 and 3 nodes have edges pointing to Stage 4 nodes, which represent community reports. The Stage 4 nodes have edges pointing to Stage 5 nodes, which correspond to map-level answers. The Stage 5 nodes each have an edge pointing to the terminal node, which represents the final answer. The terminal node is the only node in Stage 6." class="wp-image-1145841" height="542" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-1.jpg" width="881" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1: GraphRAG splits the source text into chunks (Stage 1). For each chunk, an LM extracts entities and relationships (the latter are denoted by “⭤ “), along with short descriptions (Stage 2). If an entity or a relationship was extracted from multiple chunks, an LM summarizes the descriptions (Stage 3). A knowledge graph is constructed from the final set of entities and relationships, and a community detection algorithm, such as Leiden clustering, groups entities into communities. For each community, an LM generates a “community report” that summarizes the entities and relationships (Stage 4). To answer a user’s question, an LM generates “map-level answers” based on groups of community reports (Stage 5), then synthesizes them into a final answer (Stage 6).&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;VeriTrail takes as input a DAG representing a completed generative process and aims to determine whether the final output is fully supported by the source text. It begins by extracting claims (i.e., self-contained, verifiable statements) from the final output using Claimify. VeriTrail verifies claims in the reverse order of the generative process: it starts from the final output and moves toward the source text. Each claim is verified separately. Below, we include two case studies that illustrate how VeriTrail works, using the DAG from Figure 1. &lt;/p&gt;



&lt;h3 class="wp-block-heading" id="case-study-1-a-fully-supported-claim"&gt;Case study 1: A “Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Fully Supported.” A claim extracted from the terminal node, Node 17, is “Legislative efforts have been made to address the high cost of diabetes-related supplies in the US.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. The sentence “The general assembly in North Carolina is considering legislation to set a cap on insulin prices, which indicates that high insulin prices are a contributing factor to the high cost of diabetes-related supplies in the US” is selected as evidence from Node 15. The tentative verdict is “Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12 and 13, which are the source nodes of Node 15. The sentence “The General Assembly in North Carolina is considering legislation to set a cap on insulin prices” is selected as evidence from Node 13. The verdict remains “Fully Supported.” In Iteration 3, VeriTrail checks Nodes 4, 5, and 11, which are the source nodes of Node 13. The sentence “The General Assembly is the legislative body in North Carolina considering legislation to cap insulin prices” is selected as evidence from Node 4. The verdict is still “Fully Supported.” In Iteration 4, VeriTrail checks Node 1, which is the source node of Node 4. The selected evidence is “‘There’s actually legislation in North Carolina at the General Assembly to set a cap on insulin…’ Stein said.” The corresponding verdict is “Fully Supported.” Since Node 1 represents a raw text chunk, it does not have any source nodes to check. Therefore, verification terminates and the “Fully Supported” verdict is deemed final." class="wp-image-1145843" height="720" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VeriTrail-blog-figure-2.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Fully Supported” claim.&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 2 shows an example of a claim that VeriTrail determined was not hallucinated: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes that were used as inputs for the final answer: Nodes 15 and 16. Each identified node was split into sentences, and each sentence was programmatically assigned a unique ID.
&lt;ul class="wp-block-list"&gt;
&lt;li&gt;An LM then performed &lt;strong&gt;Evidence Selection&lt;/strong&gt;, selecting all sentence IDs that strongly implied the truth or falsehood of the claim. The LM also generated a summary of the selected sentences (not shown in Figure 2). In this example, a sentence was selected from Node 15.&lt;/li&gt;



&lt;li&gt;Next, an LM performed &lt;strong&gt;Verdict Generation&lt;/strong&gt;. If no sentences had been selected in the Evidence Selection step, the claim would have been assigned a “Not Fully Supported” verdict. Instead, an LM was prompted to classify the claim as “Fully Supported,” “Not Fully Supported,” or “Inconclusive” based on the evidence. In this case, the verdict was “Fully Supported.”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;



&lt;li&gt;Since the verdict in Iteration 1 was “Fully Supported,” VeriTrail proceeded to Iteration 2. It considered the nodes from which at least one sentence was selected in the latest Evidence Selection step (Node 15) and identified their input nodes (Nodes 12 and 13). VeriTrail repeated Evidence Selection and Verdict Generation for the identified nodes. Once again, the verdict was “Fully Supported.” This process – identifying candidate nodes, performing Evidence Selection and Verdict Generation – was repeated in Iteration 3, where the verdict was still “Fully Supported,” and likewise in Iteration 4. &lt;/li&gt;



&lt;li&gt;In Iteration 4, a single source text chunk was verified. Since the source text, by definition, does not have any inputs, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



&lt;h3 class="wp-block-heading" id="case-study-2-a-not-fully-supported-claim"&gt;Case study 2: A “Not Fully Supported” claim&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="An example of VeriTrail's claim verification process where the claim is found “Not Fully Supported.” We assume that the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. A claim extracted from the terminal node, Node 17, is “Challenges related to electric vehicle battery repairability contribute to sluggish retail auto sales in China.” In Iteration 1, VeriTrail checks Nodes 15 and 16, which are the source nodes of the terminal node. Two sentences are selected as evidence. The first sentence is “Challenges with electric vehicle (EV) battery disposal and repair may also contribute to the sluggishness in retail auto sales.” The second sentence is “Junkyards are accumulating discarded EV battery packs, while collision shops face limitations in repairing EV battery packs, which could affect consumer confidence and demand.” These sentences are both from Node 15. The tentative verdict is “Not Fully Supported.” In Iteration 2, VeriTrail checks Nodes 12, 13, and 14. Nodes 12 and 13 are the source nodes of Node 15. Node 14 is the source node of Node 16, which was checked in Iteration 1. The sentence “The electric vehicle market in China is influenced by challenges associated with EV battery disposal and repair” is selected as evidence from Node 12. The verdict remains “Not Fully Supported.” Since two consecutive “Not Fully Supported” verdicts have been reached, which was the maximum, verification terminates and the final verdict is “Not Fully Supported.”" class="wp-image-1145842" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/VerTrail-blog-figure-3.jpg" width="1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3: Left: GraphRAG as a DAG. Right: VeriTrail’s hallucination detection process for a “Not Fully Supported” claim, where the maximum number of consecutive “Not Fully Supported” verdicts was set to 2. &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Figure 3 provides an example of a claim where VeriTrail identified hallucination:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;In Iteration 1, VeriTrail identified the nodes used as inputs for the final answer: Nodes 15 and 16. After Evidence Selection and Verdict Generation, the verdict was “Not Fully Supported.” Users can configure the maximum number of consecutive “Not Fully Supported” verdicts permitted. If the maximum had been set to 1, verification would have terminated here, and the verdict would have been deemed final. Let’s assume the maximum was set to 2, meaning that VeriTrail had to perform at least one more iteration.&lt;/li&gt;



&lt;li&gt;Even though evidence was selected only from Node 15 in Iteration 1, VeriTrail checked the input nodes for both Node 15 and Node 16 (i.e., Nodes 12, 13, and 14) in Iteration 2. Recall that in Case Study 1 where the verdict was “Fully Supported,” VeriTrail only checked the input nodes for Node 15. Why was the “Not Fully Supported” claim handled differently? If the Evidence Selection step overlooked relevant evidence, the “Not Fully Supported” verdict might be incorrect. In this case, continuing verification based solely on the selected evidence (i.e., Node 15) would propagate the mistake, defeating the purpose of repeated verification.&lt;/li&gt;



&lt;li&gt;In Iteration 2, Evidence Selection and Verdict Generation were repeated for Nodes 12, 13, and 14. Once again, the verdict was “Not Fully Supported.” Since this was the second consecutive “Not Fully Supported” verdict, verification terminated and the verdict was deemed final.&lt;/li&gt;
&lt;/ul&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: Event Series&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum"&gt;Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="providing-traceability"&gt;Providing traceability&lt;/h2&gt;



&lt;p&gt;In addition to assigning a final “Fully Supported,” “Not Fully Supported,” or “Inconclusive” verdict to each claim, VeriTrail returns (a) all Verdict Generation results and (b) an evidence trail composed of all Evidence Selection results: the selected sentences, their corresponding node IDs, and the generated summaries. Collectively, these outputs provide traceability:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Provenance: &lt;/strong&gt;For “Fully Supported” and “Inconclusive” claims, the evidence trail traces a path from the source material to the final output, helping users understand how the output may have been derived. For example, in Case Study 1, the evidence trail consists of Sentence 8 from Node 15, Sentence 11 from Node 13, Sentence 26 from Node 4, and Sentence 79 from Node 1.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Error Localization: &lt;/strong&gt;For “Not Fully Supported” claims, VeriTrail uses the Verdict Generation results to identify the stage(s) of the process where the unsupported content was likely introduced. For instance, in Case Study 2, where none of the verified intermediate outputs supported the claim, VeriTrail would indicate that the hallucination occurred in the final answer (Stage 6). Error stage identification helps users address hallucinations and understand where in the process they are most likely to occur.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;The evidence trail also helps users verify the verdict: instead of reading through all nodes – which may be infeasible for processes that generate large amounts of text – users can simply review the evidence sentences and summaries. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="key-design-features"&gt;Key design features&lt;/h2&gt;



&lt;p&gt;VeriTrail’s design prioritizes reliability, efficiency, scalability, and user agency. Notable features include: &lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;During Evidence Selection (introduced in Case Study 1), the sentence IDs returned by the LM are checked against the programmatically assigned IDs. If a returned ID does not match an assigned ID, it is discarded; otherwise, it is mapped to its corresponding sentence. This approach &lt;strong&gt;guarantees that the sentences included in the evidence trail are not hallucinated&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;After a claim is assigned an interim “Fully Supported” or “Inconclusive” verdict (as in Case Study 1), VeriTrail verifies the input nodes of only the nodes from which evidence was previously selected – not all possible input nodes. By progressively narrowing the search space, VeriTrail limits the number of nodes the LM must evaluate. In particular, since VeriTrail starts from the final output and moves toward the source text, it tends to verify a smaller proportion of nodes as it approaches the source text. Nodes closer to the source text tend to be larger (e.g., a book chapter should be larger than its summary), so verifying fewer of them helps &lt;strong&gt;reduce computational cost&lt;/strong&gt;.&lt;/li&gt;



&lt;li&gt;VeriTrail is designed to &lt;strong&gt;handle input graphs with any number of nodes&lt;/strong&gt;, regardless of whether they fit in a single prompt. Users can specify an input size limit per prompt. For Evidence Selection, inputs that exceed the limit are split across multiple prompts. If the resulting evidence exceeds the input size limit for Verdict Generation, VeriTrail reruns Evidence Selection to compress the evidence further. Users can configure the maximum number of Evidence Selection reruns.  &lt;/li&gt;



&lt;li&gt;The configurable maximum number of consecutive “Not Fully Supported” verdicts (introduced in Case Study 2) allows the user to find their desired &lt;strong&gt;balance between computational cost and how conservative VeriTrail is in flagging hallucinations&lt;/strong&gt;. A lower maximum reduces cost by limiting the number of checks. A higher maximum increases confidence that a flagged claim is truly hallucinated since it requires repeated confirmation of the “Not Fully Supported” verdict. &lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="evaluating-veritrail-s-performance"&gt;Evaluating VeriTrail’s performance&lt;/h2&gt;



&lt;p&gt;We tested VeriTrail on two datasets covering distinct generative processes (hierarchical summarization&lt;sup&gt;4&lt;/sup&gt; and GraphRAG), tasks (summarization and question-answering), and types of source material (fiction novels and news articles). For the source material, we focused on long documents and large collections of documents (i.e., &amp;gt;100K tokens), where hallucination detection is especially challenging and processes with multiple generative steps are typically most valuable. The resulting DAGs were much more complex than the examples provided above (e.g., in one of the datasets, the average number of nodes was 114,368).&lt;/p&gt;



&lt;p&gt;We compared VeriTrail to three types of baseline methods commonly used for closed-domain hallucination detection: Natural Language Inference models (AlignScore and INFUSE); Retrieval-Augmented Generation; and long-context models (Gemini 1.5 Pro and GPT-4.1 mini). Across both datasets and all language models tested, VeriTrail outperformed the baseline methods in detecting hallucination.&lt;sup&gt;5&lt;/sup&gt;&lt;/p&gt;



&lt;p&gt;Most importantly, VeriTrail traces claims through intermediate outputs – unlike the baseline methods, which directly compare the final output to the source material. As a result, it can identify where hallucinated content was likely introduced and how faithful content may have been derived from the source. By providing traceability, VeriTrail brings transparency to generative processes, helping users understand, verify, debug, and, ultimately, trust their outputs. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;For an in-depth discussion of VeriTrail, please see our paper “VeriTrail: Closed-Domain Hallucination Detection with Traceability.”&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; The term “closed-domain hallucination” was introduced by OpenAI in the GPT-4 Technical Report&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; VeriTrail is currently used for research purposes only and is not available commercially.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; We focus on GraphRAG’s global search method.&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;4&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/sup&gt; In hierarchical summarization, an LM summarizes each source text chunk individually, then the resulting summaries are repeatedly grouped and summarized until a final summary is produced (Wu et al., 2021&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;; Chang et al., 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;).&lt;/p&gt;



&lt;p&gt;&lt;sup&gt;5&lt;/sup&gt; The only exception was the mistral-large-2411 model, where VeriTrail had the highest balanced accuracy, but not the highest macro F1 score.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/veritrail-detecting-hallucination-and-tracing-provenance-in-multi-step-ai-workflows/</guid><pubDate>Tue, 05 Aug 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Project Ire autonomously identifies malware at scale (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Stylized digital illustration of a multi-layered circuit board. A glowing blue microchip sits at the top center, with intricate circuitry radiating outward. Beneath it, four stacked layers transition in color from blue to orange, each featuring circuit-like patterns. Smaller rectangular and circular components are connected around the layers, all set against a dark background with scattered geometric shapes." class="wp-image-1145541" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Today, we are excited to introduce an autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, Project Ire, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign.&lt;/p&gt;



&lt;p&gt;Project Ire&amp;nbsp;emerged&amp;nbsp;from a collaboration&amp;nbsp;between&amp;nbsp;Microsoft Research, Microsoft Defender Research, and Microsoft Discovery &amp;amp; Quantum, bringing together security&amp;nbsp;expertise, operational knowledge, data from global malware telemetry, and AI research. It is built on the same collaborative and agentic foundation behind&amp;nbsp;GraphRAG&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and&amp;nbsp;Microsoft Discovery&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;The system&amp;nbsp;uses advanced language models and a suite of callable reverse engineering and binary analysis tools to drive investigation and adjudication.&lt;/p&gt;



&lt;p&gt;As of this writing, Project Ire has achieved a precision&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.98 and a recall&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.83 using public datasets of Windows drivers. It was the first reverse engineer at Microsoft, human or machine, to author a conviction case—a detection strong enough to justify automatic blocking—for a specific advanced persistent threat (APT) malware sample, which has since been identified and blocked by Microsoft Defender.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="malware-classification-at-a-global-scale"&gt;Malware classification at a global scale&lt;/h2&gt;



&lt;p&gt;Microsoft’s Defender platform scans more than one billion monthly&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; active devices through the company’s Defender suite of products, which routinely require manual review of software by experts.&lt;/p&gt;



&lt;p&gt;This kind of work is challenging. Analysts often face error and alert fatigue, and there’s no easy way to compare and standardize how different people review and classify threats over time. For both of these reasons, today’s overloaded experts are vulnerable to burnout, a well-documented issue in the field.&lt;/p&gt;



&lt;p&gt;Unlike other AI applications in security, malware classification lacks a computable validator&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The AI must make judgment calls without definitive validation beyond expert review. Many behaviors found in software, like reverse engineering protections, don’t clearly indicate whether a sample is malicious or benign.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This ambiguity requires analysts to investigate each sample incrementally, building enough evidence to determine whether it’s malicious or benign despite opposition from adaptive, active adversaries. This&amp;nbsp;has long made it difficult to automate and scale what is inherently a complex and expensive process.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="technical-foundation"&gt;Technical foundation&lt;/h2&gt;



&lt;p&gt;Project Ire attempts to address these challenges by acting as an autonomous system that uses specialized tools to reverse engineer software. The system’s architecture allows for reasoning at multiple levels, from low-level binary analysis to control flow reconstruction and high-level interpretation of code behavior.&lt;/p&gt;



&lt;p&gt;Its tool-use API enables the system to update its understanding of a file using a wide range of reverse engineering tools, including Microsoft memory analysis sandboxes based on Project Freta&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, custom and open-source tools, documentation search, and multiple decompilers.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="reaching-a-verdict"&gt;Reaching a verdict&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The evaluation process begins with a triage, where automated reverse engineering tools identify the file type, its structure, and potential areas of interest. From there, the system reconstructs the software’s control flow graph using frameworks such as angr&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Ghidra&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, building a graph that forms the backbone of Project Ire’s memory model and guides the rest of the analysis.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Through iterative function analysis, the LLM calls specialized tools through an API to identify and summarize key functions. Each result feeds into a “chain of evidence,” a detailed, auditable trail that shows how the system reached its conclusion. This traceable evidence log supports secondary review by security teams and helps refine the system in cases of misclassification.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To verify its findings, Project Ire can invoke a validator tool that cross-checks claims in the report against the chain of evidence. This tool draws on expert statements from malware reverse engineers on the Project Ire team. Drawing on this evidence and its internal model, the system creates a final report and classifies the sample as malicious or benign.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: AI-POWERED EXPERIENCE&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft research copilot experience&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-copilot-experience"&gt;Discover more about research at Microsoft through our AI-powered experience&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="preliminary-testing-shows-promise"&gt;Preliminary testing shows promise&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Two early evaluations tested Project Ire’s effectiveness as an autonomous malware classifier. In the first, we assessed Project Ire on a dataset of publicly accessible Windows drivers, some known to be malicious, others benign. Malicious samples came from the &lt;em&gt;Living off the Land Drivers&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; database, which includes a collection of Windows drivers used by attackers to bypass security controls, while known benign drivers were sourced from Windows Update.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This classifier performed well, correctly identifying 90% of all files and flagging only 2% of benign files as threats. It achieved a precision of 0.98 and a recall of 0.83. This low false-positive rate suggests clear potential for deployment in security operations, alongside expert reverse engineering reviews.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For each file it analyzes, Project Ire generates a report that includes an evidence section, summaries of all examined code functions, and other technical artifacts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Figures 1 and 2 present reports for two successful malware classification cases generated during testing. The first involves a kernel-level rootkit, &lt;em&gt;Trojan:Win64/Rootkit.EH!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The system identified several key features, including jump-hooking, process termination, and web-based command and control. It then correctly flagged the sample as malicious.&lt;/p&gt;





  
  Figure 1 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains a function named ‘MonitorAndTerminateExplorerThread_16f64’ that runs an infinite loop waiting on synchronization objects and terminates system threads upon certain conditions. It queries system or process information, iterates over processes comparing their names case-insensitively to ‘Explorer.exe’, and manipulates registry values related to ‘Explorer.exe’. This function appears to monitor and potentially terminate or manipulate the ‘Explorer.exe’ process, a critical Windows shell process. Such behavior is suspicious and consistent with malware that aims to disrupt or control system processes.&lt;/p&gt;
    &lt;p&gt;Another function, ‘HttpGetRequestAndResponse_174a4’, performs HTTP GET requests by parsing URLs, resolving hostnames, opening sockets, sending requests, and reading responses. This network communication capability could be leveraged for command and control or data exfiltration, common in malware.&lt;/p&gt;
    &lt;p&gt;The binary also includes a function ‘PatchProcessEntryPointWithHook_12b5c’ that patches the entry point of a process by writing a hook or trampoline that redirects execution to a specified address. This technique is commonly used for process injection or hooking, allowing malware to alter process behavior or inject malicious code.&lt;/p&gt;
    &lt;p&gt;Other functions related to sending IOCTL requests to device drivers were identified, but their maliciousness could not be conclusively determined without additional context.&lt;/p&gt;
    &lt;p&gt;Overall, the binary exhibits multiple indicators of malicious behavior, including process manipulation, network communication, and code injection techniques, suggesting it is likely malware designed to interfere with system processes and communicate with remote servers.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Project Ire report, sample with SHA256: 86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The second sample, &lt;em&gt;HackTool:Win64/KillAV!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, was designed to disable antivirus software. Project Ire correctly identified the code that locates and disables antivirus programs, providing evidence that the file was malicious.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one section of the code, however, the system misidentified a function as anti-debugging behavior. To maintain accuracy, the system used the validator tool to flag the claim as unsupported. The issue was later resolved by updating decompiler rules, but this example illustrates how Project Ire navigates uncertainty during analysis. Figure 2 shows the corresponding report.&amp;nbsp;&lt;/p&gt;





  
  Figure 2 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains several functions indicative of malicious intent. The function register_and_log_known_processes_140001000 logs and registers process names associated with antivirus and security software, such as ‘avp.exe’, ‘avpui.exe’, and ‘360Tray.exe’. It calls another function, TerminateProcessesByNameSubstring_1400010f4, which enumerates system processes and terminates those whose names contain specified substrings. This behavior is typical of malware attempting to disable or evade security software by killing their processes.&lt;/p&gt;
    &lt;p&gt;Another function, check_and_handle_special_state_14000502c, performs checks on a global variable and triggers software interrupts if certain conditions are not met. While the exact purpose of these interrupts (int 0x29 and int 0x3) is unclear, they could represent an anti-debug or anti-analysis mechanism to detect or interfere with debugging or tampering attempts. However, this assumption could not be fully validated against expert statements.&lt;/p&gt;
    &lt;p&gt;Other functions include initialization routines and simple logging wrappers, but the core malicious behavior centers on process termination targeting security software. This indicates the binary is designed to compromise system security by disabling protective processes, a hallmark of malware such as trojans or rootkits.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Project Ire report, sample with SHA256: b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="real-world-evaluation-with-microsoft-defender"&gt;Real-world evaluation with Microsoft Defender&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The more demanding test involved nearly 4,000 “hard-target” files not classified by automated systems and slated for manual review by expert reverse engineers.&lt;/p&gt;



&lt;p&gt;In this real-world scenario, Project Ire operated fully autonomously on files created after the language models’ training cutoff, files that no other automated tools at Microsoft could classify at the time.&lt;/p&gt;



&lt;p&gt;The system achieved a high precision score of 0.89, meaning nearly 9 out of 10 files flagged malicious were correctly identified as malicious. Recall was 0.26, indicating that under these challenging conditions, the system detected roughly a quarter of all actual malware.&lt;/p&gt;



&lt;p&gt;The system correctly identified many of the malicious files, with few false alarms, just a 4% false positive rate. While overall performance was moderate, this combination of accuracy and a low error rate suggests real potential for future deployment.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead"&gt;Looking ahead&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Based on these early successes, the Project Ire prototype will be leveraged inside Microsoft’s Defender organization as &lt;em&gt;Binary Analyzer&lt;/em&gt; for threat detection and software classification.&lt;/p&gt;



&lt;p&gt;Our goal is to scale the system’s speed and accuracy so that it can correctly classify files from any source, even on first encounter. Ultimately, our vision is to detect novel malware directly in memory, at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Project Ire acknowledges the following additional developers that contributed to the results in this publication: Dayenne de Souza, Raghav Pande, Ryan Terry, Shauharda Khadka, and Bob Fleck for their independent review of the system.&lt;/p&gt;



&lt;p&gt;The system incorporates multiple tools, including the&amp;nbsp;angr&amp;nbsp;framework developed by&amp;nbsp;Emotion Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Microsoft has collaborated extensively with Emotion Labs, a pioneer in cyber autonomy, throughout the development of Project Ire, and thanks them for the innovations and insights that contributed to the successes reported here.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Stylized digital illustration of a multi-layered circuit board. A glowing blue microchip sits at the top center, with intricate circuitry radiating outward. Beneath it, four stacked layers transition in color from blue to orange, each featuring circuit-like patterns. Smaller rectangular and circular components are connected around the layers, all set against a dark background with scattered geometric shapes." class="wp-image-1145541" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/IreSocial-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Today, we are excited to introduce an autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, Project Ire, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign.&lt;/p&gt;



&lt;p&gt;Project Ire&amp;nbsp;emerged&amp;nbsp;from a collaboration&amp;nbsp;between&amp;nbsp;Microsoft Research, Microsoft Defender Research, and Microsoft Discovery &amp;amp; Quantum, bringing together security&amp;nbsp;expertise, operational knowledge, data from global malware telemetry, and AI research. It is built on the same collaborative and agentic foundation behind&amp;nbsp;GraphRAG&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&amp;nbsp;and&amp;nbsp;Microsoft Discovery&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&amp;nbsp;The system&amp;nbsp;uses advanced language models and a suite of callable reverse engineering and binary analysis tools to drive investigation and adjudication.&lt;/p&gt;



&lt;p&gt;As of this writing, Project Ire has achieved a precision&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.98 and a recall&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; of 0.83 using public datasets of Windows drivers. It was the first reverse engineer at Microsoft, human or machine, to author a conviction case—a detection strong enough to justify automatic blocking—for a specific advanced persistent threat (APT) malware sample, which has since been identified and blocked by Microsoft Defender.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="malware-classification-at-a-global-scale"&gt;Malware classification at a global scale&lt;/h2&gt;



&lt;p&gt;Microsoft’s Defender platform scans more than one billion monthly&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; active devices through the company’s Defender suite of products, which routinely require manual review of software by experts.&lt;/p&gt;



&lt;p&gt;This kind of work is challenging. Analysts often face error and alert fatigue, and there’s no easy way to compare and standardize how different people review and classify threats over time. For both of these reasons, today’s overloaded experts are vulnerable to burnout, a well-documented issue in the field.&lt;/p&gt;



&lt;p&gt;Unlike other AI applications in security, malware classification lacks a computable validator&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The AI must make judgment calls without definitive validation beyond expert review. Many behaviors found in software, like reverse engineering protections, don’t clearly indicate whether a sample is malicious or benign.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This ambiguity requires analysts to investigate each sample incrementally, building enough evidence to determine whether it’s malicious or benign despite opposition from adaptive, active adversaries. This&amp;nbsp;has long made it difficult to automate and scale what is inherently a complex and expensive process.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="technical-foundation"&gt;Technical foundation&lt;/h2&gt;



&lt;p&gt;Project Ire attempts to address these challenges by acting as an autonomous system that uses specialized tools to reverse engineer software. The system’s architecture allows for reasoning at multiple levels, from low-level binary analysis to control flow reconstruction and high-level interpretation of code behavior.&lt;/p&gt;



&lt;p&gt;Its tool-use API enables the system to update its understanding of a file using a wide range of reverse engineering tools, including Microsoft memory analysis sandboxes based on Project Freta&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, custom and open-source tools, documentation search, and multiple decompilers.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="reaching-a-verdict"&gt;Reaching a verdict&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The evaluation process begins with a triage, where automated reverse engineering tools identify the file type, its structure, and potential areas of interest. From there, the system reconstructs the software’s control flow graph using frameworks such as angr&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and Ghidra&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, building a graph that forms the backbone of Project Ire’s memory model and guides the rest of the analysis.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Through iterative function analysis, the LLM calls specialized tools through an API to identify and summarize key functions. Each result feeds into a “chain of evidence,” a detailed, auditable trail that shows how the system reached its conclusion. This traceable evidence log supports secondary review by security teams and helps refine the system in cases of misclassification.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;To verify its findings, Project Ire can invoke a validator tool that cross-checks claims in the report against the chain of evidence. This tool draws on expert statements from malware reverse engineers on the Project Ire team. Drawing on this evidence and its internal model, the system creates a final report and classifies the sample as malicious or benign.&lt;/p&gt;



	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;Spotlight: AI-POWERED EXPERIENCE&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft research copilot experience&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-copilot-experience"&gt;Discover more about research at Microsoft through our AI-powered experience&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="preliminary-testing-shows-promise"&gt;Preliminary testing shows promise&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Two early evaluations tested Project Ire’s effectiveness as an autonomous malware classifier. In the first, we assessed Project Ire on a dataset of publicly accessible Windows drivers, some known to be malicious, others benign. Malicious samples came from the &lt;em&gt;Living off the Land Drivers&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; database, which includes a collection of Windows drivers used by attackers to bypass security controls, while known benign drivers were sourced from Windows Update.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This classifier performed well, correctly identifying 90% of all files and flagging only 2% of benign files as threats. It achieved a precision of 0.98 and a recall of 0.83. This low false-positive rate suggests clear potential for deployment in security operations, alongside expert reverse engineering reviews.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For each file it analyzes, Project Ire generates a report that includes an evidence section, summaries of all examined code functions, and other technical artifacts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Figures 1 and 2 present reports for two successful malware classification cases generated during testing. The first involves a kernel-level rootkit, &lt;em&gt;Trojan:Win64/Rootkit.EH!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. The system identified several key features, including jump-hooking, process termination, and web-based command and control. It then correctly flagged the sample as malicious.&lt;/p&gt;





  
  Figure 1 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains a function named ‘MonitorAndTerminateExplorerThread_16f64’ that runs an infinite loop waiting on synchronization objects and terminates system threads upon certain conditions. It queries system or process information, iterates over processes comparing their names case-insensitively to ‘Explorer.exe’, and manipulates registry values related to ‘Explorer.exe’. This function appears to monitor and potentially terminate or manipulate the ‘Explorer.exe’ process, a critical Windows shell process. Such behavior is suspicious and consistent with malware that aims to disrupt or control system processes.&lt;/p&gt;
    &lt;p&gt;Another function, ‘HttpGetRequestAndResponse_174a4’, performs HTTP GET requests by parsing URLs, resolving hostnames, opening sockets, sending requests, and reading responses. This network communication capability could be leveraged for command and control or data exfiltration, common in malware.&lt;/p&gt;
    &lt;p&gt;The binary also includes a function ‘PatchProcessEntryPointWithHook_12b5c’ that patches the entry point of a process by writing a hook or trampoline that redirects execution to a specified address. This technique is commonly used for process injection or hooking, allowing malware to alter process behavior or inject malicious code.&lt;/p&gt;
    &lt;p&gt;Other functions related to sending IOCTL requests to device drivers were identified, but their maliciousness could not be conclusively determined without additional context.&lt;/p&gt;
    &lt;p&gt;Overall, the binary exhibits multiple indicators of malicious behavior, including process manipulation, network communication, and code injection techniques, suggesting it is likely malware designed to interfere with system processes and communicate with remote servers.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. Project Ire report, sample with SHA256: 86047bb1969d1db455493955fd450d18c62a3f36294d0a6c3732c88dfbcc4f62&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The second sample, &lt;em&gt;HackTool:Win64/KillAV!MTB&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, was designed to disable antivirus software. Project Ire correctly identified the code that locates and disables antivirus programs, providing evidence that the file was malicious.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one section of the code, however, the system misidentified a function as anti-debugging behavior. To maintain accuracy, the system used the validator tool to flag the claim as unsupported. The issue was later resolved by updating decompiler rules, but this example illustrates how Project Ire navigates uncertainty during analysis. Figure 2 shows the corresponding report.&amp;nbsp;&lt;/p&gt;





  
  Figure 2 Analysis
  


  &lt;div class="code-block"&gt;
    &lt;p&gt;The binary contains several functions indicative of malicious intent. The function register_and_log_known_processes_140001000 logs and registers process names associated with antivirus and security software, such as ‘avp.exe’, ‘avpui.exe’, and ‘360Tray.exe’. It calls another function, TerminateProcessesByNameSubstring_1400010f4, which enumerates system processes and terminates those whose names contain specified substrings. This behavior is typical of malware attempting to disable or evade security software by killing their processes.&lt;/p&gt;
    &lt;p&gt;Another function, check_and_handle_special_state_14000502c, performs checks on a global variable and triggers software interrupts if certain conditions are not met. While the exact purpose of these interrupts (int 0x29 and int 0x3) is unclear, they could represent an anti-debug or anti-analysis mechanism to detect or interfere with debugging or tampering attempts. However, this assumption could not be fully validated against expert statements.&lt;/p&gt;
    &lt;p&gt;Other functions include initialization routines and simple logging wrappers, but the core malicious behavior centers on process termination targeting security software. This indicates the binary is designed to compromise system security by disabling protective processes, a hallmark of malware such as trojans or rootkits.&lt;/p&gt;
  &lt;/div&gt;


&lt;figure class="wp-block-video aligncenter"&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. Project Ire report, sample with SHA256: b6cb163089f665c05d607a465f1b6272cdd5c949772ab9ce7227120cf61f971a&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="real-world-evaluation-with-microsoft-defender"&gt;Real-world evaluation with Microsoft Defender&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;The more demanding test involved nearly 4,000 “hard-target” files not classified by automated systems and slated for manual review by expert reverse engineers.&lt;/p&gt;



&lt;p&gt;In this real-world scenario, Project Ire operated fully autonomously on files created after the language models’ training cutoff, files that no other automated tools at Microsoft could classify at the time.&lt;/p&gt;



&lt;p&gt;The system achieved a high precision score of 0.89, meaning nearly 9 out of 10 files flagged malicious were correctly identified as malicious. Recall was 0.26, indicating that under these challenging conditions, the system detected roughly a quarter of all actual malware.&lt;/p&gt;



&lt;p&gt;The system correctly identified many of the malicious files, with few false alarms, just a 4% false positive rate. While overall performance was moderate, this combination of accuracy and a low error rate suggests real potential for future deployment.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead"&gt;Looking ahead&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Based on these early successes, the Project Ire prototype will be leveraged inside Microsoft’s Defender organization as &lt;em&gt;Binary Analyzer&lt;/em&gt; for threat detection and software classification.&lt;/p&gt;



&lt;p&gt;Our goal is to scale the system’s speed and accuracy so that it can correctly classify files from any source, even on first encounter. Ultimately, our vision is to detect novel malware directly in memory, at scale.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Project Ire acknowledges the following additional developers that contributed to the results in this publication: Dayenne de Souza, Raghav Pande, Ryan Terry, Shauharda Khadka, and Bob Fleck for their independent review of the system.&lt;/p&gt;



&lt;p&gt;The system incorporates multiple tools, including the&amp;nbsp;angr&amp;nbsp;framework developed by&amp;nbsp;Emotion Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. Microsoft has collaborated extensively with Emotion Labs, a pioneer in cyber autonomy, throughout the development of Project Ire, and thanks them for the innovations and insights that contributed to the successes reported here.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/</guid><pubDate>Tue, 05 Aug 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] No Backdoors. No Kill Switches. No Spyware. (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/no-backdoors-no-kill-switches-no-spyware/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA GPUs are at the heart of modern computing. They’re used across industries — from healthcare and finance to scientific research, autonomous systems and AI infrastructure. NVIDIA GPUs are embedded into CT scanners and MRI machines, DNA sequencers, air-traffic radar tracking systems, city traffic-management systems, self-driving cars, supercomputers, TV broadcasting systems, casino machines and game consoles.&lt;/p&gt;
&lt;p&gt;To mitigate the risk of misuse, some pundits and policymakers propose requiring hardware “kill switches” or built-in controls that can remotely disable GPUs without user knowledge and consent. Some suspect they might already exist.&lt;/p&gt;
&lt;p&gt;NVIDIA GPUs do not and should not have kill switches and backdoors.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hard-Coded, Single-Point Controls Are Always a Bad Idea&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA has been designing processors for over 30 years. Embedding backdoors and kill switches into chips would be a gift to hackers and hostile actors. It would undermine global digital infrastructure and fracture trust in U.S. technology. Established law wisely requires companies to fix vulnerabilities — not create them.&lt;/p&gt;
&lt;p&gt;Until recently, that policy was universally held and beyond question. When security researchers discovered vulnerabilities such as “Spectre” and “Meltdown” for CPUs, governments and industry responded with speed and unity to eliminate the risk.&lt;/p&gt;
&lt;p&gt;That principle still holds. There is no such thing as a “good” secret backdoor — only dangerous vulnerabilities that need to be eliminated. Product security must always be done the right way: through rigorous internal testing, independent validation and full compliance with global cybersecurity standards. Robust security is built on the principle of “defense in depth”: layering multiple safeguards so that no single-point vulnerability can compromise or shut down a system. For decades, that’s how NVIDIA and American industry have promoted innovation while protecting users and growing the economy. This is no time to depart from that winning formula.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Historical Lessons: The Clipper Chip Debacle &lt;/b&gt;&lt;strong&gt;—&lt;/strong&gt;&lt;b&gt; a Policy and Technical Failure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The cybersecurity community learned these lessons the hard way during the 1990s with the NSA’s Clipper Chip initiative. Introduced in 1993, the Clipper Chip was designed to provide strong encryption while maintaining government backdoor access through a key escrow system.&lt;/p&gt;
&lt;p&gt;The Clipper Chip represented everything wrong with built-in backdoors. Security researchers discovered fundamental flaws in the system that could allow malicious parties to tamper with the software. It created centralized vulnerabilities that could be exploited by adversaries. The mere existence of government backdoors undermined user confidence in the security of systems.&lt;/p&gt;
&lt;p&gt;Kill switches and built-in backdoors create single points of failure and violate the fundamental principles of cybersecurity.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Promote Smart Software Tools, Not Dangerous Hardware Traps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Some point to smartphone features like “find my phone” or “remote wipe” as models for a GPU kill switch. That comparison doesn’t hold water — optional software features, controlled by the user, are not hardware backdoors.&lt;/p&gt;
&lt;p&gt;NVIDIA has always supported open, transparent software that helps customers get the most from their GPU-powered systems — diagnostics, performance monitoring, bug reporting and timely patching — with the user’s knowledge and consent. That’s responsible, secure computing. It helps our customers excel, and industry stay ahead.&lt;/p&gt;
&lt;p&gt;Hardwiring a kill switch into a chip is something entirely different: a permanent flaw beyond user control, and an open invitation for disaster. It’s like buying a car where the dealership keeps a remote control for the parking brake — just in case they decide you shouldn’t be driving. That’s not sound policy. It’s an overreaction that would irreparably harm America’s economic and national security interests.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hardware Integrity Should Be Nonpartisan and Nonnegotiable&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For decades, policymakers have championed industry’s efforts to create secure, trustworthy hardware. Governments have many tools to protect nations, consumers and the economy. Deliberately weakening critical infrastructure should never be one of them.&lt;/p&gt;
&lt;p&gt;There are no back doors in NVIDIA chips. No kill switches. No spyware. That’s not how trustworthy systems are built — and never will be.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2024/04/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA GPUs are at the heart of modern computing. They’re used across industries — from healthcare and finance to scientific research, autonomous systems and AI infrastructure. NVIDIA GPUs are embedded into CT scanners and MRI machines, DNA sequencers, air-traffic radar tracking systems, city traffic-management systems, self-driving cars, supercomputers, TV broadcasting systems, casino machines and game consoles.&lt;/p&gt;
&lt;p&gt;To mitigate the risk of misuse, some pundits and policymakers propose requiring hardware “kill switches” or built-in controls that can remotely disable GPUs without user knowledge and consent. Some suspect they might already exist.&lt;/p&gt;
&lt;p&gt;NVIDIA GPUs do not and should not have kill switches and backdoors.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hard-Coded, Single-Point Controls Are Always a Bad Idea&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA has been designing processors for over 30 years. Embedding backdoors and kill switches into chips would be a gift to hackers and hostile actors. It would undermine global digital infrastructure and fracture trust in U.S. technology. Established law wisely requires companies to fix vulnerabilities — not create them.&lt;/p&gt;
&lt;p&gt;Until recently, that policy was universally held and beyond question. When security researchers discovered vulnerabilities such as “Spectre” and “Meltdown” for CPUs, governments and industry responded with speed and unity to eliminate the risk.&lt;/p&gt;
&lt;p&gt;That principle still holds. There is no such thing as a “good” secret backdoor — only dangerous vulnerabilities that need to be eliminated. Product security must always be done the right way: through rigorous internal testing, independent validation and full compliance with global cybersecurity standards. Robust security is built on the principle of “defense in depth”: layering multiple safeguards so that no single-point vulnerability can compromise or shut down a system. For decades, that’s how NVIDIA and American industry have promoted innovation while protecting users and growing the economy. This is no time to depart from that winning formula.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Historical Lessons: The Clipper Chip Debacle &lt;/b&gt;&lt;strong&gt;—&lt;/strong&gt;&lt;b&gt; a Policy and Technical Failure&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The cybersecurity community learned these lessons the hard way during the 1990s with the NSA’s Clipper Chip initiative. Introduced in 1993, the Clipper Chip was designed to provide strong encryption while maintaining government backdoor access through a key escrow system.&lt;/p&gt;
&lt;p&gt;The Clipper Chip represented everything wrong with built-in backdoors. Security researchers discovered fundamental flaws in the system that could allow malicious parties to tamper with the software. It created centralized vulnerabilities that could be exploited by adversaries. The mere existence of government backdoors undermined user confidence in the security of systems.&lt;/p&gt;
&lt;p&gt;Kill switches and built-in backdoors create single points of failure and violate the fundamental principles of cybersecurity.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Promote Smart Software Tools, Not Dangerous Hardware Traps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Some point to smartphone features like “find my phone” or “remote wipe” as models for a GPU kill switch. That comparison doesn’t hold water — optional software features, controlled by the user, are not hardware backdoors.&lt;/p&gt;
&lt;p&gt;NVIDIA has always supported open, transparent software that helps customers get the most from their GPU-powered systems — diagnostics, performance monitoring, bug reporting and timely patching — with the user’s knowledge and consent. That’s responsible, secure computing. It helps our customers excel, and industry stay ahead.&lt;/p&gt;
&lt;p&gt;Hardwiring a kill switch into a chip is something entirely different: a permanent flaw beyond user control, and an open invitation for disaster. It’s like buying a car where the dealership keeps a remote control for the parking brake — just in case they decide you shouldn’t be driving. That’s not sound policy. It’s an overreaction that would irreparably harm America’s economic and national security interests.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Hardware Integrity Should Be Nonpartisan and Nonnegotiable&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;For decades, policymakers have championed industry’s efforts to create secure, trustworthy hardware. Governments have many tools to protect nations, consumers and the economy. Deliberately weakening critical infrastructure should never be one of them.&lt;/p&gt;
&lt;p&gt;There are no back doors in NVIDIA chips. No kill switches. No spyware. That’s not how trustworthy systems are built — and never will be.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/no-backdoors-no-kill-switches-no-spyware/</guid><pubDate>Tue, 05 Aug 2025 16:03:36 +0000</pubDate></item><item><title>[NEW] Google’s NotebookLM is now available to younger users as competition in the AI education space intensifies (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/googles-notebooklm-is-now-available-to-younger-users-as-competition-in-the-ai-education-space-intensifies/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1225201409.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s AI note-taking app is now open to younger users, having previously been limited to users 18 and older. The tech giant announced that NotebookLM is available to Google Workspace for Education users of any age and for consumers ages 13 and up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The removal of age restrictions is intended to provide younger students with access to the AI research tool, allowing them to better understand their class materials. Now, students can access features such as the ability to convert notes into podcast-like Audio Overviews, visually summarize ideas with interactive Mind Maps, and more. NotebookLM recently released Video Overviews to let users turn notes, PDFs, and images into visual presentations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This expansion comes amid increasing concerns about the use of AI in education regarding data privacy and potential misuse. Google says that NotebookLM enforces stricter content policies for users under 18 to prevent inappropriate responses, and user chats and uploads are not reviewed by humans or used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The availability of NotebookLM for younger users follows OpenAI’s introduction of a study mode for ChatGPT, indicating that companies are ramping up competition in the AI education sector.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/01/GettyImages-1225201409.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google’s AI note-taking app is now open to younger users, having previously been limited to users 18 and older. The tech giant announced that NotebookLM is available to Google Workspace for Education users of any age and for consumers ages 13 and up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The removal of age restrictions is intended to provide younger students with access to the AI research tool, allowing them to better understand their class materials. Now, students can access features such as the ability to convert notes into podcast-like Audio Overviews, visually summarize ideas with interactive Mind Maps, and more. NotebookLM recently released Video Overviews to let users turn notes, PDFs, and images into visual presentations.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This expansion comes amid increasing concerns about the use of AI in education regarding data privacy and potential misuse. Google says that NotebookLM enforces stricter content policies for users under 18 to prevent inappropriate responses, and user chats and uploads are not reviewed by humans or used for AI training.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The availability of NotebookLM for younger users follows OpenAI’s introduction of a study mode for ChatGPT, indicating that companies are ramping up competition in the AI education sector.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/googles-notebooklm-is-now-available-to-younger-users-as-competition-in-the-ai-education-space-intensifies/</guid><pubDate>Tue, 05 Aug 2025 16:03:39 +0000</pubDate></item><item><title>[NEW] The EU AI Act aims to create a level playing field for AI innovation: Here’s what it is (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/the-eu-ai-act-aims-to-create-a-level-playing-field-for-ai-innovation-heres-what-it-is/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/11/GettyImages-1146371917.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The European Union’s Artificial Intelligence Act, known as the EU AI Act, has been described by the European Commission as “the world’s first comprehensive AI law.” After years in the making, it is progressively becoming a part of reality for the 450 million people living in the 27 countries that comprise the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act, however, is more than a European affair. It applies to companies both local and foreign, and it can affect both providers and deployers of AI systems; the European Commission cites examples of how it would apply to a developer of a CV screening tool and to a bank that buys that tool. Now all of these parties have a legal framework that sets the stage for their use of AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-does-the-eu-ai-act-exist"&gt;Why does the EU AI Act exist?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As usual with EU legislation, the EU AI Act exists to make sure there is a uniform legal framework applying to a certain topic across EU countries — the topic this time being AI. Now that the regulation is in place, it should “ensure the free movement, cross-border, of AI-based goods and services” without diverging local restrictions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With timely regulation, the EU seeks to create a level playing field across the region and foster trust, which could also create opportunities for emerging companies. However, the common framework that it has adopted is not exactly permissive: Despite the relatively early stage of widespread AI adoption in most sectors, the EU AI Act sets a high bar for what AI should and shouldn’t do for society more broadly.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-the-purpose-of-the-eu-ai-act"&gt;What is the purpose of the EU AI Act?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;According to European lawmakers, the framework’s main goal is to “promote the uptake of human centric and trustworthy AI while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union, including democracy, the rule of law and environmental protection, to protect against the harmful effects of AI systems in the Union, and to support innovation.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, that’s quite a mouthful, but it’s worth parsing carefully. First, because a lot will depend on how you define “human centric” and “trustworthy” AI. And second, because it gives a good sense of the precarious balance to maintain between&amp;nbsp;diverging goals: innovation vs. harm prevention, as well as uptake of AI vs. environmental protection. As usual with EU legislation, again, the devil will be in the details.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-the-eu-ai-act-balance-its-different-goals"&gt;How does the EU AI Act balance its different goals?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;To balance harm prevention against the potential benefits of AI, the EU AI Act adopted a risk-based approach: banning a handful of “unacceptable risk” use cases; flagging a set of “high-risk” uses calling for tight regulation; and applying lighter obligations to “limited risk” scenarios.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-has-the-eu-ai-act-come-into-effect"&gt;Has the EU AI Act come into effect?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Yes and no. The EU AI Act rollout started on August 1, 2024, but it will only come into force through a series of staggered compliance deadlines. In most cases, it will also apply sooner to new entrants than to companies that already offer AI products and services in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first deadline came into effect on February 2, 2025, and focused on enforcing bans on a small number of prohibited uses of AI, such as untargeted scraping of internet or CCTV for facial images to build up or expand databases. Many others will follow, but unless the schedule changes, most provisions will apply by mid-2026.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-changed-on-august-2-2025"&gt;What changed on August 2, 2025?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Since August 2, 2025, the EU AI Act applies to “general-purpose AI models with systemic risk.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPAI (general-purpose AI) models are AI models trained with a large amount of data, and that can be used for a wide range of tasks. That’s where the risk element comes in. According to the EU AI Act, GPAI models can come with systemic risks —  “for example, through the lowering of barriers for chemical or biological weapons development, or unintended issues of control over autonomous [GPAI] models.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ahead of the deadline, the EU published guidelines for providers of GPAI models, which include both European companies and non-European players such as Anthropic, Google, Meta, and OpenAI. But since these companies already have models on the market, they will also have until August 2, 2027, to comply, unlike new entrants.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-does-the-eu-ai-act-have-teeth"&gt;Does the EU AI Act have teeth?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act comes with penalties that lawmakers wanted to be simultaneously “effective, proportionate and dissuasive” — even for large global players.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Details will be laid down by EU countries, but the regulation sets out the overall spirit — that penalties will vary depending on the deemed risk level — as well as thresholds for each level. Infringement on prohibited AI applications leads to the highest penalty of “up to €35 million or 7% of the total worldwide annual turnover of the preceding financial year (whichever is higher).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The European Commission can also inflict fines of up to €15 million or 3% of annual turnover on providers of GPAI models.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-fast-do-existing-players-intend-to-comply"&gt;How fast do existing players intend to comply?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The voluntary GPAI code of practice, including commitments such as not training models on pirated content, is a good indicator of how companies may engage with the framework law until forced to do so.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Meta announced it wouldn’t sign the voluntary GPAI code of practice meant to help such providers comply with the EU AI Act. However, Google soon after confirmed it would sign, despite reservations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Signatories so far include Aleph Alpha, Amazon, Anthropic, Cohere, Google, IBM, Microsoft, Mistral AI, and OpenAI, among others. But as we have seen with Google’s example, signing does not equal a full-on endorsement.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-have-some-tech-companies-been-fighting-these-rules-nbsp"&gt;Why have (some) tech companies been fighting these rules?&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While stating in a blog post that Google would sign the voluntary GPAI code of practice, its president of global affairs, Kent Walker, still had reservations. “We remain concerned that the AI Act and Code risk slowing Europe’s development and deployment of AI,” he wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta was more radical, with its chief global affairs officer Joel Kaplan stating in a post on LinkedIn that “Europe is heading down the wrong path on AI.” Calling the EU’s implementation of the AI Act “overreach,” he stated that the code of practice “introduces a number of legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;European companies have expressed concerns as well. Arthur Mensch, the CEO of French AI champion Mistral AI, was part of a group of European CEOs who signed an open letter in July 2025 urging Brussels to “stop the clock” for two years before key obligations of the EU AI Act came into force.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-will-the-schedule-change"&gt;Will the schedule change?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In early July 2025, the European Union responded negatively to lobbying efforts calling for a pause, saying it would still stick to its timeline for implementing the EU AI Act. It went ahead with the August 2, 2025, deadline as planned, and we will update this story if anything changes.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/11/GettyImages-1146371917.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The European Union’s Artificial Intelligence Act, known as the EU AI Act, has been described by the European Commission as “the world’s first comprehensive AI law.” After years in the making, it is progressively becoming a part of reality for the 450 million people living in the 27 countries that comprise the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act, however, is more than a European affair. It applies to companies both local and foreign, and it can affect both providers and deployers of AI systems; the European Commission cites examples of how it would apply to a developer of a CV screening tool and to a bank that buys that tool. Now all of these parties have a legal framework that sets the stage for their use of AI.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-does-the-eu-ai-act-exist"&gt;Why does the EU AI Act exist?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As usual with EU legislation, the EU AI Act exists to make sure there is a uniform legal framework applying to a certain topic across EU countries — the topic this time being AI. Now that the regulation is in place, it should “ensure the free movement, cross-border, of AI-based goods and services” without diverging local restrictions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;With timely regulation, the EU seeks to create a level playing field across the region and foster trust, which could also create opportunities for emerging companies. However, the common framework that it has adopted is not exactly permissive: Despite the relatively early stage of widespread AI adoption in most sectors, the EU AI Act sets a high bar for what AI should and shouldn’t do for society more broadly.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-is-the-purpose-of-the-eu-ai-act"&gt;What is the purpose of the EU AI Act?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;According to European lawmakers, the framework’s main goal is to “promote the uptake of human centric and trustworthy AI while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union, including democracy, the rule of law and environmental protection, to protect against the harmful effects of AI systems in the Union, and to support innovation.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yes, that’s quite a mouthful, but it’s worth parsing carefully. First, because a lot will depend on how you define “human centric” and “trustworthy” AI. And second, because it gives a good sense of the precarious balance to maintain between&amp;nbsp;diverging goals: innovation vs. harm prevention, as well as uptake of AI vs. environmental protection. As usual with EU legislation, again, the devil will be in the details.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-does-the-eu-ai-act-balance-its-different-goals"&gt;How does the EU AI Act balance its different goals?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;To balance harm prevention against the potential benefits of AI, the EU AI Act adopted a risk-based approach: banning a handful of “unacceptable risk” use cases; flagging a set of “high-risk” uses calling for tight regulation; and applying lighter obligations to “limited risk” scenarios.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-has-the-eu-ai-act-come-into-effect"&gt;Has the EU AI Act come into effect?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Yes and no. The EU AI Act rollout started on August 1, 2024, but it will only come into force through a series of staggered compliance deadlines. In most cases, it will also apply sooner to new entrants than to companies that already offer AI products and services in the EU.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first deadline came into effect on February 2, 2025, and focused on enforcing bans on a small number of prohibited uses of AI, such as untargeted scraping of internet or CCTV for facial images to build up or expand databases. Many others will follow, but unless the schedule changes, most provisions will apply by mid-2026.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-what-changed-on-august-2-2025"&gt;What changed on August 2, 2025?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Since August 2, 2025, the EU AI Act applies to “general-purpose AI models with systemic risk.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;GPAI (general-purpose AI) models are AI models trained with a large amount of data, and that can be used for a wide range of tasks. That’s where the risk element comes in. According to the EU AI Act, GPAI models can come with systemic risks —  “for example, through the lowering of barriers for chemical or biological weapons development, or unintended issues of control over autonomous [GPAI] models.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ahead of the deadline, the EU published guidelines for providers of GPAI models, which include both European companies and non-European players such as Anthropic, Google, Meta, and OpenAI. But since these companies already have models on the market, they will also have until August 2, 2027, to comply, unlike new entrants.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-does-the-eu-ai-act-have-teeth"&gt;Does the EU AI Act have teeth?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The EU AI Act comes with penalties that lawmakers wanted to be simultaneously “effective, proportionate and dissuasive” — even for large global players.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Details will be laid down by EU countries, but the regulation sets out the overall spirit — that penalties will vary depending on the deemed risk level — as well as thresholds for each level. Infringement on prohibited AI applications leads to the highest penalty of “up to €35 million or 7% of the total worldwide annual turnover of the preceding financial year (whichever is higher).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The European Commission can also inflict fines of up to €15 million or 3% of annual turnover on providers of GPAI models.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-how-fast-do-existing-players-intend-to-comply"&gt;How fast do existing players intend to comply?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The voluntary GPAI code of practice, including commitments such as not training models on pirated content, is a good indicator of how companies may engage with the framework law until forced to do so.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July 2025, Meta announced it wouldn’t sign the voluntary GPAI code of practice meant to help such providers comply with the EU AI Act. However, Google soon after confirmed it would sign, despite reservations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Signatories so far include Aleph Alpha, Amazon, Anthropic, Cohere, Google, IBM, Microsoft, Mistral AI, and OpenAI, among others. But as we have seen with Google’s example, signing does not equal a full-on endorsement.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-why-have-some-tech-companies-been-fighting-these-rules-nbsp"&gt;Why have (some) tech companies been fighting these rules?&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;While stating in a blog post that Google would sign the voluntary GPAI code of practice, its president of global affairs, Kent Walker, still had reservations. “We remain concerned that the AI Act and Code risk slowing Europe’s development and deployment of AI,” he wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta was more radical, with its chief global affairs officer Joel Kaplan stating in a post on LinkedIn that “Europe is heading down the wrong path on AI.” Calling the EU’s implementation of the AI Act “overreach,” he stated that the code of practice “introduces a number of legal uncertainties for model developers, as well as measures which go far beyond the scope of the AI Act.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;European companies have expressed concerns as well. Arthur Mensch, the CEO of French AI champion Mistral AI, was part of a group of European CEOs who signed an open letter in July 2025 urging Brussels to “stop the clock” for two years before key obligations of the EU AI Act came into force.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-will-the-schedule-change"&gt;Will the schedule change?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;In early July 2025, the European Union responded negatively to lobbying efforts calling for a pause, saying it would still stick to its timeline for implementing the EU AI Act. It went ahead with the August 2, 2025, deadline as planned, and we will update this story if anything changes.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/the-eu-ai-act-aims-to-create-a-level-playing-field-for-ai-innovation-heres-what-it-is/</guid><pubDate>Tue, 05 Aug 2025 16:24:10 +0000</pubDate></item><item><title>[NEW] Some people are defending Perplexity after Cloudflare ‘named and shamed’ it (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/some-people-are-defending-perplexity-after-cloudflare-named-and-shamed-it/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/45A2342_VGAEbHsG.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Cloudflare accused AI search engine Perplexity of stealthily scraping websites on Monday, while ignoring a site’s specific methods to block it, this wasn’t a clear-cut case of an AI web crawler gone wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many people came to Perplexity’s defense. They argued that Perplexity accessing sites in defiance of the website owner’s wishes, while controversial, is acceptable. And this is a controversy that will certainly grow as AI agents flood the internet: Should an agent accessing a website on behalf of its user be treated like a bot? Or like a human making the same request?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cloudflare is known for providing anti-bot crawling and other web security services to millions of websites. Essentially, Cloudflare’s test case involved setting up a new website with a new domain that had never been crawled by any bot, setting up a robots.txt file that specifically blocked Perplexity’s known AI crawling bots, and then asking Perplexity about the website’s content.&amp;nbsp;And Perplexity answered the question. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare researchers found the AI search engine used “a generic browser intended to impersonate Google Chrome on macOS” when its web crawler itself was blocked. Cloudflare CEO Matthew Prince posted the research on X, writing, “Some supposedly ‘reputable’ AI companies act more like North Korean hackers. Time to name, shame, and hard block them.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But many people disagreed with Prince’s assessment that this was actual bad behavior. Those defending Perplexity on sites like X and Hacker News pointed out that what Cloudflare seemed to document was the AI accessing a specific public website when its user asked about that specific website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If I as a human request a website, then I should be shown the content,” one person on Hacker News wrote, adding, “why would the LLM accessing the website on my behalf be in a different legal category as my Firefox web browser?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Perplexity spokesperson previously denied to TechCrunch that the bots were the company’s and called Cloudflare’s blog post a sales pitch for Cloudflare. Then on Tuesday, Perplexity published a blog in its defense (and generally attacking Cloudflare), claiming the behavior was from a third-party service it uses occasionally.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But the crux of Perplexity’s post made a similar appeal as its online defenders did.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The difference between automated crawling and user-driven fetching isn’t just technical — it’s about who gets to access information on the open web,” the post said. “This controversy reveals that Cloudflare’s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity’s accusations aren’t exactly fair, either. One argument that Prince and Cloudflare used for calling out Perplexity’s methods was that OpenAI doesn’t behave in the same way.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“OpenAI is an example of a leading AI company that follows these best practices,” Cloudflare wrote.&amp;nbsp;“They respect robots.txt and do not try to evade either a robots.txt directive or a network level block. And ChatGPT Agent is signing http requests using the newly proposed open standard Web Bot Auth.” &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Web Bot Auth is a Cloudflare-supported standard being developed by the Internet Engineering Task Force that hopes to create a cryptographic method for identifying AI agent web requests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The debate comes as bot activity reshapes the internet. As TechCrunch has previously reported, bots seeking to scrape massive amounts of content to train AI models have become a menace, especially to smaller sites.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the first time in the internet’s history, bot activity is currently outstripping human activity online, with AI traffic accounting for over 50%, according to Imperva’s Bad Bot report released last month. Most of that activity is coming from LLMs. But the report also found that malicious bots now make up 37% of all internet traffic. That’s activity that includes everything from persistent scraping to unauthorized login attempts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until LLMs, the internet generally accepted that websites could and should block most bot activity given how often it was malicious by using CAPTCHAs and other services (such as Cloudflare). Websites also had a clear incentive to work with specific good actors, such as Googlebot, guiding it on what not to index through robots.txt. Google indexed the internet, which sent traffic to sites. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, LLMs are eating an increasing amount of that traffic. Gartner predicts that search engine volume will drop by 25% by 2026.&amp;nbsp;Right now humans tend to click website links from LLMs at the point they are most valuable to the website, which is when they are ready to conduct a transaction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if humans adopt agents as the tech industry predicts they will — to arrange our travel, book our dinner reservations, and shop for us — would websites hurt their business interests by blocking them? The debate on X captured the dilemma perfectly:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I WANT perplexity to visit any public content on my behalf when I give it a request/task!” wrote one person in response to Cloudflare calling out Perplexity. &lt;/p&gt;&lt;p&gt;“What if the site owners don’t want it? they just want you [to] directly visit the home, see their stuff” argued another, pointing out that the site owner who created the content wants the traffic and potential ad revenue, not to let Perplexity take it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is why I can’t see ‘agentic browsing’ really working — much harder problem than people think. Most website owners will just block,” a third predicted.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/45A2342_VGAEbHsG.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Cloudflare accused AI search engine Perplexity of stealthily scraping websites on Monday, while ignoring a site’s specific methods to block it, this wasn’t a clear-cut case of an AI web crawler gone wild.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Many people came to Perplexity’s defense. They argued that Perplexity accessing sites in defiance of the website owner’s wishes, while controversial, is acceptable. And this is a controversy that will certainly grow as AI agents flood the internet: Should an agent accessing a website on behalf of its user be treated like a bot? Or like a human making the same request?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cloudflare is known for providing anti-bot crawling and other web security services to millions of websites. Essentially, Cloudflare’s test case involved setting up a new website with a new domain that had never been crawled by any bot, setting up a robots.txt file that specifically blocked Perplexity’s known AI crawling bots, and then asking Perplexity about the website’s content.&amp;nbsp;And Perplexity answered the question. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cloudflare researchers found the AI search engine used “a generic browser intended to impersonate Google Chrome on macOS” when its web crawler itself was blocked. Cloudflare CEO Matthew Prince posted the research on X, writing, “Some supposedly ‘reputable’ AI companies act more like North Korean hackers. Time to name, shame, and hard block them.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But many people disagreed with Prince’s assessment that this was actual bad behavior. Those defending Perplexity on sites like X and Hacker News pointed out that what Cloudflare seemed to document was the AI accessing a specific public website when its user asked about that specific website.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If I as a human request a website, then I should be shown the content,” one person on Hacker News wrote, adding, “why would the LLM accessing the website on my behalf be in a different legal category as my Firefox web browser?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Perplexity spokesperson previously denied to TechCrunch that the bots were the company’s and called Cloudflare’s blog post a sales pitch for Cloudflare. Then on Tuesday, Perplexity published a blog in its defense (and generally attacking Cloudflare), claiming the behavior was from a third-party service it uses occasionally.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But the crux of Perplexity’s post made a similar appeal as its online defenders did.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The difference between automated crawling and user-driven fetching isn’t just technical — it’s about who gets to access information on the open web,” the post said. “This controversy reveals that Cloudflare’s systems are fundamentally inadequate for distinguishing between legitimate AI assistants and actual threats.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perplexity’s accusations aren’t exactly fair, either. One argument that Prince and Cloudflare used for calling out Perplexity’s methods was that OpenAI doesn’t behave in the same way.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“OpenAI is an example of a leading AI company that follows these best practices,” Cloudflare wrote.&amp;nbsp;“They respect robots.txt and do not try to evade either a robots.txt directive or a network level block. And ChatGPT Agent is signing http requests using the newly proposed open standard Web Bot Auth.” &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Web Bot Auth is a Cloudflare-supported standard being developed by the Internet Engineering Task Force that hopes to create a cryptographic method for identifying AI agent web requests.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The debate comes as bot activity reshapes the internet. As TechCrunch has previously reported, bots seeking to scrape massive amounts of content to train AI models have become a menace, especially to smaller sites.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the first time in the internet’s history, bot activity is currently outstripping human activity online, with AI traffic accounting for over 50%, according to Imperva’s Bad Bot report released last month. Most of that activity is coming from LLMs. But the report also found that malicious bots now make up 37% of all internet traffic. That’s activity that includes everything from persistent scraping to unauthorized login attempts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until LLMs, the internet generally accepted that websites could and should block most bot activity given how often it was malicious by using CAPTCHAs and other services (such as Cloudflare). Websites also had a clear incentive to work with specific good actors, such as Googlebot, guiding it on what not to index through robots.txt. Google indexed the internet, which sent traffic to sites. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, LLMs are eating an increasing amount of that traffic. Gartner predicts that search engine volume will drop by 25% by 2026.&amp;nbsp;Right now humans tend to click website links from LLMs at the point they are most valuable to the website, which is when they are ready to conduct a transaction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if humans adopt agents as the tech industry predicts they will — to arrange our travel, book our dinner reservations, and shop for us — would websites hurt their business interests by blocking them? The debate on X captured the dilemma perfectly:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I WANT perplexity to visit any public content on my behalf when I give it a request/task!” wrote one person in response to Cloudflare calling out Perplexity. &lt;/p&gt;&lt;p&gt;“What if the site owners don’t want it? they just want you [to] directly visit the home, see their stuff” argued another, pointing out that the site owner who created the content wants the traffic and potential ad revenue, not to let Perplexity take it.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is why I can’t see ‘agentic browsing’ really working — much harder problem than people think. Most website owners will just block,” a third predicted.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/some-people-are-defending-perplexity-after-cloudflare-named-and-shamed-it/</guid><pubDate>Tue, 05 Aug 2025 16:33:03 +0000</pubDate></item><item><title>[NEW] OpenAI has finally released open-weight language models (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/08/05/1121092/openai-has-finally-released-open-weight-language-models/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250805_openaiopenmodel_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI has finally released its first open-weight large language models since 2019’s GPT-2. These new “gpt-oss” models are available in two different sizes and score similarly to the company’s o3-mini and o4-mini models on several benchmarks. Unlike the models available through OpenAI’s web interface, these new open models can be freely downloaded, run, and even modified on laptops and other local devices.&lt;/p&gt;  &lt;p&gt;In the company’s many years without an open LLM release, some users have taken to referring to it with the pejorative “ClosedAI.” That sense of frustration had escalated in the past few months as these long-awaited models were delayed twice—first in June and then in July. With their release, however, OpenAI is reestablishing itself as a presence for users of open models.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That’s particularly notable at a time when Meta, which had previously dominated the American open-model landscape with its Llama models, may be reorienting toward closed releases—and when Chinese open models, such as DeepSeek’s offerings, Kimi K2, and Alibaba’s Qwen series, are becoming more popular than their American competitors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“The vast majority of our [enterprise and startup] customers are already using a lot of open models,” said Casey Dvorak, a research program manager at OpenAI, in a media briefing about the model release. “Because there is no [competitive] open model from OpenAI, we wanted to plug that gap and actually allow them to use our technology across the board.”&lt;/p&gt; 
 &lt;p&gt;The new models come in two different sizes, the smaller of which can theoretically run on 16 GB of RAM—the minimum amount that Apple currently offers on its computers. The larger model requires a high-end laptop or specialized hardware.&lt;/p&gt;  &lt;p&gt;Open models have a few key use cases. Some organizations may want to customize models for their own purposes or save money by running models on their own equipment, though that equipment comes at a substantial upfront cost. Others—such hospitals, law firms, and governments—might need models that they can run locally for data security reasons.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;OpenAI has facilitated such activity by releasing its open models under a permissive Apache 2.0 license, which allows the models to be used for commercial purposes. Nathan Lambert, post-training lead at the Allen Institute for AI, says that this choice is commendable: Such licenses are typical for Chinese open-model releases, but Meta released its Llama models under a bespoke, more restrictive license. “It’s a very good thing for the open community,” he says.&lt;/p&gt;  &lt;p&gt;Researchers who study how LLMs work also need open models, so that they can examine and manipulate those models in detail. “In part, this is about reasserting OpenAI’s dominance in the research ecosystem,” says Peter Henderson, an assistant professor at Princeton University who has worked extensively with open models. If researchers do adopt gpt-oss as new workhorses, OpenAI could see some concrete benefits, Henderson says—it might adopt innovations discovered by other researchers into its own model ecosystem.&lt;/p&gt;  &lt;p&gt;More broadly, Lambert says, releasing an open model now could help OpenAI reestablish its status in an increasingly crowded AI environment. “It kind of goes back to years ago, where they were seen as &lt;em&gt;the&lt;/em&gt; AI company,” he says. Users who want to use open models will now have the option to meet all their needs with OpenAI products, rather than turning to Meta’s Llama or Alibaba’s Qwen when they need to run something locally.&lt;/p&gt;  &lt;p&gt;The rise of Chinese open models like Qwen over the past year may have been a particularly salient factor in OpenAI’s calculus. An employee from OpenAI emphasized at the media briefing that the company doesn’t see these open models as a response to actions taken by any other AI company, but OpenAI is clearly attuned to the geopolitical implications of China’s open-model dominance. “Broad access to these capable‬‭ open-weights models created in the US helps expand democratic AI rails,” the company wrote in a blog post announcing the models’ release.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Since DeepSeek exploded onto the AI scene at the start of 2025, observers have noted that Chinese models often refuse to speak about topics that the Chinese Communist Party has deemed verboten, such as Tiananmen Square. Such observations—as well as longer-term risks, like the possibility that agentic models could purposefully write vulnerable code—have made some AI experts concerned about the growing adoption of Chinese models. “Open models are a form of soft power,” Henderson says.&lt;/p&gt;  &lt;p&gt;Lambert released a report on Monday documenting how Chinese models are overtaking American offerings like Llama and advocating for a renewed commitment to domestic open models. Several prominent AI researchers and entrepreneurs, such as HuggingFace CEO Clement Delangue, Stanford’s Percy Liang, and former OpenAI researcher Miles Brundage, have signed on.&lt;/p&gt;  &lt;p&gt;The Trump administration, too, has emphasized development of open models in its AI Action Plan. With both this model release and previous statements, OpenAI is aligning itself with that stance. “In their filings about the action plan, [OpenAI] pretty clearly indicated that they see US–China as a key issue and want to position themselves as very important to the US system,” says Rishi Bommasani, a senior research scholar at the Stanford Institute for Human-Centered Artificial Intelligence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And OpenAI may see concrete political advantages from aligning with the administration’s AI priorities, Lambert says. As the company continues to build out its extensive computational infrastructure, it will need political support and approvals, and sympathetic leadership could go a long way.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250805_openaiopenmodel_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;OpenAI has finally released its first open-weight large language models since 2019’s GPT-2. These new “gpt-oss” models are available in two different sizes and score similarly to the company’s o3-mini and o4-mini models on several benchmarks. Unlike the models available through OpenAI’s web interface, these new open models can be freely downloaded, run, and even modified on laptops and other local devices.&lt;/p&gt;  &lt;p&gt;In the company’s many years without an open LLM release, some users have taken to referring to it with the pejorative “ClosedAI.” That sense of frustration had escalated in the past few months as these long-awaited models were delayed twice—first in June and then in July. With their release, however, OpenAI is reestablishing itself as a presence for users of open models.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That’s particularly notable at a time when Meta, which had previously dominated the American open-model landscape with its Llama models, may be reorienting toward closed releases—and when Chinese open models, such as DeepSeek’s offerings, Kimi K2, and Alibaba’s Qwen series, are becoming more popular than their American competitors.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;“The vast majority of our [enterprise and startup] customers are already using a lot of open models,” said Casey Dvorak, a research program manager at OpenAI, in a media briefing about the model release. “Because there is no [competitive] open model from OpenAI, we wanted to plug that gap and actually allow them to use our technology across the board.”&lt;/p&gt; 
 &lt;p&gt;The new models come in two different sizes, the smaller of which can theoretically run on 16 GB of RAM—the minimum amount that Apple currently offers on its computers. The larger model requires a high-end laptop or specialized hardware.&lt;/p&gt;  &lt;p&gt;Open models have a few key use cases. Some organizations may want to customize models for their own purposes or save money by running models on their own equipment, though that equipment comes at a substantial upfront cost. Others—such hospitals, law firms, and governments—might need models that they can run locally for data security reasons.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;OpenAI has facilitated such activity by releasing its open models under a permissive Apache 2.0 license, which allows the models to be used for commercial purposes. Nathan Lambert, post-training lead at the Allen Institute for AI, says that this choice is commendable: Such licenses are typical for Chinese open-model releases, but Meta released its Llama models under a bespoke, more restrictive license. “It’s a very good thing for the open community,” he says.&lt;/p&gt;  &lt;p&gt;Researchers who study how LLMs work also need open models, so that they can examine and manipulate those models in detail. “In part, this is about reasserting OpenAI’s dominance in the research ecosystem,” says Peter Henderson, an assistant professor at Princeton University who has worked extensively with open models. If researchers do adopt gpt-oss as new workhorses, OpenAI could see some concrete benefits, Henderson says—it might adopt innovations discovered by other researchers into its own model ecosystem.&lt;/p&gt;  &lt;p&gt;More broadly, Lambert says, releasing an open model now could help OpenAI reestablish its status in an increasingly crowded AI environment. “It kind of goes back to years ago, where they were seen as &lt;em&gt;the&lt;/em&gt; AI company,” he says. Users who want to use open models will now have the option to meet all their needs with OpenAI products, rather than turning to Meta’s Llama or Alibaba’s Qwen when they need to run something locally.&lt;/p&gt;  &lt;p&gt;The rise of Chinese open models like Qwen over the past year may have been a particularly salient factor in OpenAI’s calculus. An employee from OpenAI emphasized at the media briefing that the company doesn’t see these open models as a response to actions taken by any other AI company, but OpenAI is clearly attuned to the geopolitical implications of China’s open-model dominance. “Broad access to these capable‬‭ open-weights models created in the US helps expand democratic AI rails,” the company wrote in a blog post announcing the models’ release.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Since DeepSeek exploded onto the AI scene at the start of 2025, observers have noted that Chinese models often refuse to speak about topics that the Chinese Communist Party has deemed verboten, such as Tiananmen Square. Such observations—as well as longer-term risks, like the possibility that agentic models could purposefully write vulnerable code—have made some AI experts concerned about the growing adoption of Chinese models. “Open models are a form of soft power,” Henderson says.&lt;/p&gt;  &lt;p&gt;Lambert released a report on Monday documenting how Chinese models are overtaking American offerings like Llama and advocating for a renewed commitment to domestic open models. Several prominent AI researchers and entrepreneurs, such as HuggingFace CEO Clement Delangue, Stanford’s Percy Liang, and former OpenAI researcher Miles Brundage, have signed on.&lt;/p&gt;  &lt;p&gt;The Trump administration, too, has emphasized development of open models in its AI Action Plan. With both this model release and previous statements, OpenAI is aligning itself with that stance. “In their filings about the action plan, [OpenAI] pretty clearly indicated that they see US–China as a key issue and want to position themselves as very important to the US system,” says Rishi Bommasani, a senior research scholar at the Stanford Institute for Human-Centered Artificial Intelligence.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And OpenAI may see concrete political advantages from aligning with the administration’s AI priorities, Lambert says. As the company continues to build out its extensive computational infrastructure, it will need political support and approvals, and sympathetic leadership could go a long way.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/08/05/1121092/openai-has-finally-released-open-weight-language-models/</guid><pubDate>Tue, 05 Aug 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI returns to open source roots with new models gpt-oss-120b and gpt-oss-20b (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b/</link><description>&lt;p&gt;OpenAI is &lt;strong&gt;getting back to its roots as an open source AI company &lt;/strong&gt;with today’s announcement and release of two new, open source, frontier large language models (LLMs): &lt;strong&gt;gpt-oss-120b and gpt-oss-20b.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The former is a 120-billion parameter model as the name would suggest, capable of running on a single Nvidia H100 graphics processing unit (GPU) and the latter is only 20 billion, &lt;strong&gt;small enough to run locally on a consumer laptop or desktop PC. &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Both are &lt;strong&gt;text-only language models&lt;/strong&gt;, which means unlike the multimodal AI that we’ve had for nearly two years that allows users to upload files and images and have the AI analyze them, users will be confined to only inputting text messages to the models and receiving text back out. &lt;/p&gt;&lt;p&gt;However, they can still of course write code and provide math problems and numerics, and in terms of their performance on tasks, they &lt;strong&gt;rank above some of OpenAI’s paid models&lt;/strong&gt; and much of the competition globally. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;They can also be connected to external tools including &lt;strong&gt;web search&lt;/strong&gt; to perform research on behalf of the user. More on this below. &lt;/p&gt;



&lt;p&gt;Most importantly:&lt;strong&gt; they’re free,&lt;/strong&gt; they’re &lt;strong&gt;available for enterprises and indie developers to download the code and use right now&lt;/strong&gt;, modifying according to their needs, and&lt;strong&gt; can be run locally without a web connection&lt;/strong&gt;, ensuring &lt;strong&gt;maximum privacy,&lt;/strong&gt; unlike the other top OpenAI models and those from leading U.S.-based rivals Google and Anthropic.&lt;/p&gt;



&lt;p&gt;The models can be downloaded today with full weights (the settings guiding its behavior) on the AI code sharing community Hugging Face and GitHub.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-high-benchmark-scores"&gt;High benchmark scores&lt;/h2&gt;



&lt;p&gt;According to OpenAI, gpt-oss-120b matches or exceeds its proprietary &lt;strong&gt;o4-mini&lt;/strong&gt; model on reasoning and tool-use benchmarks, including &lt;strong&gt;competition mathematics (AIME 2024 &amp;amp; 2025)&lt;/strong&gt;, &lt;strong&gt;general problem solving (MMLU and HLE)&lt;/strong&gt;, &lt;strong&gt;agentic evaluations (TauBench)&lt;/strong&gt;, and &lt;strong&gt;health-specific evaluations (HealthBench)&lt;/strong&gt;. The &lt;strong&gt;smaller gpt-oss-20b model is comparable to o3-mini and even surpasses it&lt;/strong&gt; in some benchmarks.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015140" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.32.35%E2%80%AFAM.png?w=531" width="531" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015141" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.32.30%E2%80%AFAM.png?w=523" width="523" /&gt;&lt;/figure&gt;



&lt;p&gt;The models are multilingual and perform well across a variety of non-English languages, though OpenAI declined to specify which and how many. &lt;/p&gt;



&lt;p&gt;While these capabilities are available out of the box, OpenAI notes that localized fine-tuning — such as an ongoing collaboration with the Swedish government to produce a version fine-tuned on the country’s language —can still meaningfully enhance performance for specific regional or linguistic contexts.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-hugely-advantageous-license-for-enterprises-and-privacy-minded-users"&gt;A hugely advantageous license for enterprises and privacy-minded users &lt;/h2&gt;



&lt;p&gt;But the &lt;strong&gt;biggest feature is the licensing terms for both: Apache 2.0,&lt;/strong&gt; the same as the wave of Chinese open source models that have been released over the last several weeks, and&lt;strong&gt; a more enterprise-friendly license than Meta’s trickier and more nuanced open-ish Llama license&lt;/strong&gt;, which requires that users who operate a service with more than 700 million monthly active users obtain a paid license to keep using the company’s family of LLMs.&lt;/p&gt;



&lt;p&gt;By contrast, OpenAI’s new gpt-oss series of models offer no such restrictions. In keeping with Chinese competitors and counterparts, any consumer, developer, independent entrepreneur or enterprise large and small is empowered by the Apache 2.0 license to be able to download the new gpt-oss models at will, fine-tune and alter them to fit their specific needs, and use them to generate revenue or operate paid services,&lt;strong&gt; all without paying OpenAI a dime (or anything!).&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This also means enterprises can &lt;strong&gt;use a powerful, near topline OpenAI model on their own hardware totally privately and securely, without sending any data up to the cloud, on web servers, or anywhere else&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt; For highly regulated industries like finance, healthcare, and legal services, not to mention organizations in military, intelligence, and government, this may be a requirement. &lt;/p&gt;



&lt;p&gt;Before today, anyone using ChatGPT or its application programming interface (API) — the service that acts like a switching board and allows third-party software developers to connect their own apps and services to these OpenAI’s proprietary/paid models like GPT-4o and o3 — &lt;strong&gt;was sending data up to OpenAI servers that could technically be subpoenaed by government agencies and accessed &lt;/strong&gt;without a user’s knowledge. That’s still the case for anyone using ChatGPT or the API going forward, as&lt;strong&gt; OpenAI co-founder and Sam Altman recently warned.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;And while running the new gpt-oss models locally on a user’s own hardware disconnected from the web would allow for maximum privacy, as soon as the user decides to connect it to external web search or other web enabled tools, some of the same privacy risks and issues would then arise — through any third-party web services the user or developer was relying on when hooking the models up to said tools. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-last-openai-open-source-language-model-was-released-more-than-six-years-ago"&gt;The last OpenAI open source language model was released more than six years ago&lt;/h2&gt;



&lt;p&gt;“This is the first time we’re releasing an open-weight language model in a long time…&lt;strong&gt; We view this as complementary to our other products,” said OpenAI co-founder and president Greg Brockman&lt;/strong&gt; on an embargoed press video call with VentureBeat and other journalists last night. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;last time OpenAI released a fully open source language model was GPT-2 in 2019,&lt;/strong&gt; &lt;strong&gt;more than six years ago, and three years before the release of ChatGPT&lt;/strong&gt;.&lt;/p&gt;



&lt;p&gt;This fact has &lt;strong&gt;sparked the ire of &lt;/strong&gt;— and resulted in several lawsuits from — &lt;strong&gt;former OpenAI co-founder and backer turned rival Elon Musk&lt;/strong&gt;, who, along with many other critics, have spent the last several years &lt;strong&gt;accusing OpenAI of betraying its mission&lt;/strong&gt; and founding principles and namesake &lt;strong&gt;by eschewing open source AI releases in favor of paid proprietary models&lt;/strong&gt; available only to customers of OpenAI’s API or paying ChatGPT subscribers (though there is a free tier for the latter).&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OpenAI co-founder CEO Sam Altman did express regret &lt;/strong&gt;about being on the “wrong side of history” but not releasing more open source AI sooner in a Reddit AMA (ask me anything) QA with users in February of this year, and Altman committed to releasing a new open source model back in March, but ultimately the company delayed its release from a planned July date until now. &lt;/p&gt;



&lt;p&gt;Now &lt;strong&gt;OpenAI is tacking back toward open source, and the question is, why&lt;/strong&gt;?&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-would-openai-release-a-set-of-free-open-source-models-that-it-makes-no-money-from"&gt;Why would OpenAI release a set of free open source models that it makes no money from?&lt;/h2&gt;



&lt;p&gt;To paraphrase Jesse Plemons’ character’s memorable line from the film &lt;em&gt;Game Night&lt;/em&gt;: “How can that be profitable for OpenAI?”&lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3015128" height="292" src="https://venturebeat.com/wp-content/uploads/2025/08/GLfcXKYWUAAYKZW.jpg" width="496" /&gt;&lt;/figure&gt;



&lt;p&gt;After all, business to OpenAI’s paid offerings appears to be booming. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Revenue has skyrocketed alongside the rapid expansion of its ChatGPT user base&lt;/strong&gt;, &lt;strong&gt;now at 700 million weekly active users&lt;/strong&gt;. As of August 2025, OpenAI reported $13 billion in annual recurring revenue, up from $10 billion in June. That &lt;strong&gt;growth is driven by a sharp rise in paying business customers — now 5 million, up from 3 million just two months earlier&lt;/strong&gt; — and surging daily engagement, with over 3 billion user messages sent every day. &lt;/p&gt;



&lt;p&gt;The financial momentum follows an $8.3 billion funding round that valued OpenAI at $300 billion and provides the foundation for the company’s aggressive infrastructure expansion and global ambitions.&lt;/p&gt;



&lt;p&gt;Compare that to &lt;strong&gt;closed/proprietary rival AI startup Anthropic’s reported $5 billion in total annual recurring revenue&lt;/strong&gt;, but interestingly,&lt;strong&gt; Anthropic is said to be getting more money from its API, $3.1 billion in revenue compared to OpenAI’s $2.9 billion&lt;/strong&gt;, according to &lt;em&gt;The Information&lt;/em&gt;.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;OpenAI and Anthropic both are showing pretty spectacular growth in 2025, with OpenAI doubling ARR in the last 6 months from $6bn to $12bn and Anthropic increasing 5x from $1bn to $5bn in 7 months.&lt;/p&gt;&lt;p&gt;If we compare the sources of revenue, the picture is quite interesting:&lt;br /&gt;– OpenAI… pic.twitter.com/8OaN1RSm9E&lt;/p&gt;— Peter Gostev (@petergostev) August 4, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;So, given how well the &lt;em&gt;paid AI business is already doing&lt;/em&gt;, the business strategy behind these open source offerings is less clear — especially since &lt;strong&gt;the new OpenAI gpt-oss models will almost certainly cut into some (perhaps a lot of) usage of OpenAI’s paid models.&lt;/strong&gt; Why go back to offering open source LLMs now when so much money is flowing into paid and none will, by virtue of its very intent, go directly toward open source models?&lt;/p&gt;



&lt;p&gt;Put simply: because &lt;strong&gt;open source competitors,&lt;/strong&gt; beginning with the release of the impressively efficient &lt;strong&gt;DeepSeek R1&lt;/strong&gt; by the Chinese AI division of the same name in January 2025, are &lt;strong&gt;offering near parity on performance benchmarks to paid proprietary models&lt;/strong&gt;, &lt;strong&gt;for free&lt;/strong&gt;, &lt;strong&gt;with fewer (basically zero) implementation restrictions&lt;/strong&gt; for enterprises and end users. And increasingly, &lt;strong&gt;enterprises are adopting these open source models&lt;/strong&gt; in production. &lt;/p&gt;



&lt;p&gt;As&lt;strong&gt; OpenAI executives and team members revealed&lt;/strong&gt; to VentureBeat and many other journalists on an embargoed video call last night about the new models that when it comes to&lt;strong&gt; OpenAI’s API&lt;/strong&gt;,&lt;strong&gt; the majority of customers are using a mix of paid OpenAI models &lt;em&gt;and open source models from other providers&lt;/em&gt;. &lt;/strong&gt;(I asked, but OpenAI declined to specify what percentage or total number of API customers are using open source models and which ones). &lt;/p&gt;



&lt;p&gt;At least, until now. &lt;strong&gt;OpenAI clearly hopes these new gpt-oss offerings will get more of these users to switch away from competing open source offerings and back into OpenAI’s ecosystem&lt;/strong&gt;, even if OpenAI doesn’t see any direct revenue or data from that usage.&lt;/p&gt;



&lt;p&gt;On a grander scale, it seems &lt;strong&gt;OpenAI wants to be a full-service, full-stack, one-stop shop AI offering&lt;/strong&gt; for &lt;em&gt;all of an enterprise, indie developer’s, or regular consumer’s&lt;/em&gt; machine intelligence needs — from a clean chatbot interface to an API to build services and apps atop of to agent frameworks for building AI agents through said API to an image generation model (gpt-4o native image generation), video model (Sora), audio transcription model (gpt-4o-transcribe), and now, open source offerings as well. Can a music generation and “world model” be far behind?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OpenAI seeks to span the AI market, propriety and open source alike,&lt;/strong&gt; even if the latter is worth nothing in terms of actual, direct dollars and cents. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-training-and-architecture"&gt;Training and architecture&lt;/h2&gt;



&lt;p&gt;Feedback from developers directly influenced gpt-oss’s design. OpenAI says the top request was for a permissive license, which led to the adoption of Apache 2.0 for both models. Both models use a &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; architecture with a &lt;strong&gt;Transformer backbone&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;The larger gpt-oss-120b activates 5.1 billion parameters per token (out of 117 billion total), and gpt-oss-20b activates 3.6 billion (out of 21 billion total). &lt;/p&gt;



&lt;p&gt;Both support &lt;strong&gt;128,000 token context length&lt;/strong&gt; (about 300-400 pages of a novel’s worth of text a user can upload at once), and employ &lt;strong&gt;locally banded sparse attention&lt;/strong&gt; and use &lt;strong&gt;Rotary Positional Embeddings&lt;/strong&gt; for encoding.&lt;/p&gt;



&lt;p&gt;The tokenizer — the program that converts words and chunks of words into the numerical tokens that the LLMs can understand, dubbed&lt;strong&gt; “o200k_harmony&lt;/strong&gt;“&lt;strong&gt; &lt;/strong&gt;— is also being open-sourced.&lt;/p&gt;



&lt;p&gt;Developers can select among &lt;strong&gt;low, medium, or high reasoning effort&lt;/strong&gt; settings based on latency and performance needs. While these models can reason across complex agentic tasks, OpenAI emphasizes they were not trained with &lt;strong&gt;direct supervision of CoT outputs&lt;/strong&gt;, to preserve the observability of reasoning behavior—an approach OpenAI considers important for safety monitoring.&lt;/p&gt;



&lt;p&gt;Another &lt;strong&gt;common request from OpenAI’s developer community was for strong support for function calling, particularly for agentic workloads&lt;/strong&gt;, which OpenAI believes gpt-oss now delivers.&lt;/p&gt;



&lt;p&gt;The models are engineered for &lt;strong&gt;chain-of-thought reasoning&lt;/strong&gt;, &lt;strong&gt;tool use&lt;/strong&gt;, and &lt;strong&gt;few-shot function calling&lt;/strong&gt;, and are compatible with OpenAI’s &lt;strong&gt;Responses API&lt;/strong&gt; introduced back in March, which allows developers to augment their apps by connecting an OpenAI LLM of their choice to three powerful built-in tools — &lt;strong&gt;web search&lt;/strong&gt;, &lt;strong&gt;file search&lt;/strong&gt;, and &lt;strong&gt;computer use&lt;/strong&gt; — within a single API call.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;But for the new gpt-oss models, tool use capabilities — including web search and code execution — are not tied to OpenAI infrastructure. &lt;/strong&gt;OpenAI provides the schemas and examples used during training, such as a basic browser implementation using the Exa API and a Python interpreter that operates in a Docker container. &lt;/p&gt;



&lt;p&gt;It is&lt;strong&gt; up to individual inference providers or developers to define how tools are implemented.&lt;/strong&gt; Providers like vLLM, for instance, allow users to configure their own MCP (Model-Controller-Proxy) server to specify the browser backend.&lt;/p&gt;



&lt;p&gt;While these models can reason across complex agentic tasks, OpenAI emphasizes they were not trained with &lt;strong&gt;direct supervision of CoT outputs&lt;/strong&gt;, to preserve the observability of reasoning behavior—an approach OpenAI considers important for safety monitoring.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-safety-evaluations-and-measures"&gt;Safety evaluations and measures&lt;/h2&gt;



&lt;p&gt;OpenAI conducted safety training using its &lt;strong&gt;Preparedness Framework&lt;/strong&gt;, a document that outlines the procedural commitments, risk‑assessment criteria, capability categories, thresholds, evaluations, and governance mechanisms OpenAI uses to monitor, evaluate, and mitigate frontier AI risks.&lt;/p&gt;



&lt;p&gt;These included &lt;strong&gt;filtering chemical, biological, radiological, and nuclear threat (CBRN) related data&lt;/strong&gt; out during pretraining, and applying advanced post-training safety methods such as &lt;strong&gt;deliberative alignment&lt;/strong&gt; and an &lt;strong&gt;instruction hierarchy&lt;/strong&gt; to enforce refusal behavior on harmful prompts.&lt;/p&gt;



&lt;p&gt;To test worst-case misuse potential, OpenAI adversarially fine-tuned gpt-oss-120b on sensitive biology and cybersecurity data using its internal RL training stack. These &lt;strong&gt;malicious fine-tuning (MFT)&lt;/strong&gt; scenarios—one of the most sophisticated evaluations of this kind to date—included enabling browsing and disabling refusal behavior, simulating real-world attack potential.&lt;/p&gt;



&lt;p&gt;The resulting models were benchmarked against both open and proprietary LLMs, including DeepSeek R1-0528, Qwen 3 Thinking, Kimi K2, and OpenAI’s o3. Despite enhanced access to tools and targeted training, OpenAI found that even the fine-tuned gpt-oss models remained &lt;strong&gt;below the “High” capability threshold&lt;/strong&gt; for frontier risk domains such as biorisk and cybersecurity. These conclusions were reviewed by &lt;strong&gt;three independent expert groups&lt;/strong&gt;, whose recommendations were incorporated into the final methodology.&lt;/p&gt;



&lt;p&gt;In parallel, OpenAI partnered with &lt;strong&gt;SecureBio&lt;/strong&gt; to run external evaluations on biology-focused benchmarks like &lt;strong&gt;Human Pathogen Capabilities Test (HPCT)&lt;/strong&gt;, &lt;strong&gt;Molecular Biology Capabilities Test (MBCT)&lt;/strong&gt;, and others. Results showed that gpt-oss’s fine-tuned models performed close to OpenAI’s o3 model, which is not classified as frontier-high under OpenAI’s safety definitions.&lt;/p&gt;



&lt;p&gt;According to OpenAI, these findings contributed directly to the decision to release gpt-oss openly. The release is also intended to support safety research, especially around monitoring and controlling open-weight models in complex domains.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-availability-and-ecosystem-support"&gt;Availability and ecosystem support&lt;/h2&gt;



&lt;p&gt;The gpt-oss models are now available on Hugging Face, with pre-built support through major deployment platforms including &lt;strong&gt;Azure&lt;/strong&gt;, &lt;strong&gt;AWS&lt;/strong&gt;, &lt;strong&gt;Databricks&lt;/strong&gt;, &lt;strong&gt;Cloudflare&lt;/strong&gt;, &lt;strong&gt;Vercel&lt;/strong&gt;, &lt;strong&gt;Together AI&lt;/strong&gt;, &lt;strong&gt;OpenRouter&lt;/strong&gt;, and others. Hardware partners include &lt;strong&gt;NVIDIA&lt;/strong&gt;, &lt;strong&gt;AMD&lt;/strong&gt;, and &lt;strong&gt;Cerebras&lt;/strong&gt;, and Microsoft is making GPU-optimized builds available on &lt;strong&gt;Windows via ONNX Runtime&lt;/strong&gt;.&lt;/p&gt;



&lt;p&gt;OpenAI has also announced a &lt;strong&gt;$500,000 Red Teaming Challenge&lt;/strong&gt; hosted on Kaggle, inviting researchers and developers to probe the limits of gpt-oss and identify novel misuse pathways. A public report and an open-source evaluation dataset will follow, aiming to accelerate open model safety research across the AI community.&lt;/p&gt;



&lt;p&gt;Early adopters such as &lt;strong&gt;AI Sweden&lt;/strong&gt;, &lt;strong&gt;Orange&lt;/strong&gt;, and &lt;strong&gt;Snowflake&lt;/strong&gt; have collaborated with OpenAI to explore deployments ranging from localized fine-tuning to secure on-premise use cases. OpenAI characterizes the launch as an invitation for developers, enterprises, and governments to run state-of-the-art language models on their own terms.&lt;/p&gt;



&lt;p&gt;While OpenAI has not committed to a fixed cadence for future open-weight releases, it signals that gpt-oss represents a strategic expansion of its approach — balancing openness with aligned safety methodologies to shape how large models are shared and governed in the years ahead.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-big-question-with-so-much-competition-in-open-source-ai-will-openai-s-own-efforts-pay-off"&gt;The big question: with so much competition in open source AI, will OpenAI’s own efforts pay off?&lt;/h2&gt;



&lt;p&gt;OpenAI re-enters the open source model market in &lt;strong&gt;the most competitive moment&lt;/strong&gt; yet.&lt;/p&gt;



&lt;p&gt;At the top of public AI benchmarking leaderboards, U.S. frontier models remain &lt;strong&gt;proprietary&lt;/strong&gt; — &lt;strong&gt;OpenAI&lt;/strong&gt; (GPT-4o/o3), &lt;strong&gt;Google&lt;/strong&gt; (Gemini), and &lt;strong&gt;Anthropic&lt;/strong&gt; (Claude).&lt;/p&gt;



&lt;p&gt;But they now compete directly with a surge of &lt;strong&gt;open-weights&lt;/strong&gt; contenders. From China: &lt;strong&gt;DeepSeek-R1 (open source, MIT)&lt;/strong&gt; and &lt;strong&gt;DeepSeek-V3 (open-weights under a DeepSeek Model License that permits commercial use)&lt;/strong&gt;; &lt;strong&gt;Alibaba’s Qwen 3 (open-weights, Apache-2.0)&lt;/strong&gt;; &lt;strong&gt;MoonshotAI’s Kimi K2 (open-weights; public repo and model cards)&lt;/strong&gt;; and &lt;strong&gt;Z.ai’s GLM-4.5&lt;/strong&gt; (also Apache 2.0 licensed). &lt;/p&gt;



&lt;p&gt;Europe’s &lt;strong&gt;Mistral (Mixtral/Mistral, open-weights, Apache-2.0)&lt;/strong&gt; anchors the EU push; the UAE’s &lt;strong&gt;Falcon 2/3&lt;/strong&gt; publish &lt;strong&gt;open-weights&lt;/strong&gt; under TII’s Apache-based license. In the U.S. open-weights camp, &lt;strong&gt;Meta’s Llama 3.1&lt;/strong&gt; ships under a &lt;strong&gt;community (source-available) license&lt;/strong&gt;, &lt;strong&gt;Google’s Gemma&lt;/strong&gt; under &lt;strong&gt;Gemma terms&lt;/strong&gt; (open weights with use restrictions), and &lt;strong&gt;Microsoft’s Phi-3.5&lt;/strong&gt; under &lt;strong&gt;MIT&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Developer pull mirrors that split.&lt;/strong&gt; On Hugging Face, &lt;strong&gt;Qwen2.5-7B-Instruct (open-weights, Apache-2.0)&lt;/strong&gt; sits near the top by “downloads last month,” while &lt;strong&gt;DeepSeek-R1 (MIT)&lt;/strong&gt; and &lt;strong&gt;DeepSeek-V3 (model-licensed open weights)&lt;/strong&gt; also post heavy traction. Open-weights stalwarts &lt;strong&gt;Mistral-7B / Mixtral&lt;/strong&gt; (Apache-2.0), &lt;strong&gt;Llama-3.1-8B/70B&lt;/strong&gt; (Meta community license), &lt;strong&gt;Gemma-2&lt;/strong&gt; (Gemma terms), &lt;strong&gt;Phi-3.5&lt;/strong&gt; (MIT), &lt;strong&gt;GLM-4.5&lt;/strong&gt; (open-weights), and &lt;strong&gt;Falcon-2-11B&lt;/strong&gt; (TII Falcon License 2.0) round out the most-pulled families —underscoring that the open ecosystem spans the U.S., Europe, the Middle East, and China. Hugging Face signals adoption, not market share, but they show where builders are experimenting and deploying today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Consumer usage remains concentrated in proprietary apps even as weights open up.&lt;/strong&gt; &lt;strong&gt;ChatGPT&lt;/strong&gt; still drives the largest engagement globally (about &lt;strong&gt;2.5 billion prompts/day&lt;/strong&gt;, proprietary service), while in China the leading assistants — &lt;strong&gt;ByteDance’s Doubao&lt;/strong&gt;, &lt;strong&gt;DeepSeek’s app&lt;/strong&gt;, &lt;strong&gt;Moonshot’s Kimi&lt;/strong&gt;, and &lt;strong&gt;Baidu’s ERNIE Bot&lt;/strong&gt; — are delivered as &lt;strong&gt;proprietary products&lt;/strong&gt;, even as several base models (GLM-4.5, ERNIE 4.5 variants) now ship as &lt;strong&gt;open-weights&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;But now that a range of powerful open source models are available to businesses and consumers — all nearing one another in terms of performance — and can be downloaded on consumer hardware, the &lt;strong&gt;big question facing OpenAI is: who will pay for intelligence at all? Will the convenience of the web-based chatbot interface, multimodal capabilities, and more powerful performance be enough to keep the dollars flowing?&lt;/strong&gt; Or has machine intelligence already become, in the words of Atlman himself, “too cheap to meter”? And if so, how to build a successful business atop it, especially with OpenAI and other AI firms’ sky-high valuations and expenditures. &lt;/p&gt;



&lt;p&gt;One clue: OpenAI is already said to be offering in-house engineers to help its enterprise customers customize and deploy fine-tuned models, similar to Palantir’s “forward deployed” software engineers (SWEs), essentially charging for experts to come in, set up the models correctly, and train employees how to use them for best results.&lt;/p&gt;



&lt;p&gt;Perhaps the world will migrate toward a majority of AI usage going to open source models, or a sizeable minority, with OpenAI and other AI model providers offering experts to help install said models into enterprises. Is that enough of a service to build a multi-billion dollar business upon? Or will enough people continue paying $20, $200 or more each month to have access to even more powerful proprietary models? &lt;/p&gt;



&lt;p&gt;I don’t envy the folks at OpenAI figuring out all the business calculations — despite what I assume to be hefty compensation as a result, at least for now. But for end users and enterprises, the release of the gpt-oss series is undoubtedly compelling. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;OpenAI is &lt;strong&gt;getting back to its roots as an open source AI company &lt;/strong&gt;with today’s announcement and release of two new, open source, frontier large language models (LLMs): &lt;strong&gt;gpt-oss-120b and gpt-oss-20b.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;The former is a 120-billion parameter model as the name would suggest, capable of running on a single Nvidia H100 graphics processing unit (GPU) and the latter is only 20 billion, &lt;strong&gt;small enough to run locally on a consumer laptop or desktop PC. &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Both are &lt;strong&gt;text-only language models&lt;/strong&gt;, which means unlike the multimodal AI that we’ve had for nearly two years that allows users to upload files and images and have the AI analyze them, users will be confined to only inputting text messages to the models and receiving text back out. &lt;/p&gt;&lt;p&gt;However, they can still of course write code and provide math problems and numerics, and in terms of their performance on tasks, they &lt;strong&gt;rank above some of OpenAI’s paid models&lt;/strong&gt; and much of the competition globally. &lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;The AI Impact Series Returns to San Francisco - August 5&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;The next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Secure your spot now - space is limited: https://bit.ly/3GuuPLF&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;They can also be connected to external tools including &lt;strong&gt;web search&lt;/strong&gt; to perform research on behalf of the user. More on this below. &lt;/p&gt;



&lt;p&gt;Most importantly:&lt;strong&gt; they’re free,&lt;/strong&gt; they’re &lt;strong&gt;available for enterprises and indie developers to download the code and use right now&lt;/strong&gt;, modifying according to their needs, and&lt;strong&gt; can be run locally without a web connection&lt;/strong&gt;, ensuring &lt;strong&gt;maximum privacy,&lt;/strong&gt; unlike the other top OpenAI models and those from leading U.S.-based rivals Google and Anthropic.&lt;/p&gt;



&lt;p&gt;The models can be downloaded today with full weights (the settings guiding its behavior) on the AI code sharing community Hugging Face and GitHub.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-high-benchmark-scores"&gt;High benchmark scores&lt;/h2&gt;



&lt;p&gt;According to OpenAI, gpt-oss-120b matches or exceeds its proprietary &lt;strong&gt;o4-mini&lt;/strong&gt; model on reasoning and tool-use benchmarks, including &lt;strong&gt;competition mathematics (AIME 2024 &amp;amp; 2025)&lt;/strong&gt;, &lt;strong&gt;general problem solving (MMLU and HLE)&lt;/strong&gt;, &lt;strong&gt;agentic evaluations (TauBench)&lt;/strong&gt;, and &lt;strong&gt;health-specific evaluations (HealthBench)&lt;/strong&gt;. The &lt;strong&gt;smaller gpt-oss-20b model is comparable to o3-mini and even surpasses it&lt;/strong&gt; in some benchmarks.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015140" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.32.35%E2%80%AFAM.png?w=531" width="531" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015141" height="600" src="https://venturebeat.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-11.32.30%E2%80%AFAM.png?w=523" width="523" /&gt;&lt;/figure&gt;



&lt;p&gt;The models are multilingual and perform well across a variety of non-English languages, though OpenAI declined to specify which and how many. &lt;/p&gt;



&lt;p&gt;While these capabilities are available out of the box, OpenAI notes that localized fine-tuning — such as an ongoing collaboration with the Swedish government to produce a version fine-tuned on the country’s language —can still meaningfully enhance performance for specific regional or linguistic contexts.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-hugely-advantageous-license-for-enterprises-and-privacy-minded-users"&gt;A hugely advantageous license for enterprises and privacy-minded users &lt;/h2&gt;



&lt;p&gt;But the &lt;strong&gt;biggest feature is the licensing terms for both: Apache 2.0,&lt;/strong&gt; the same as the wave of Chinese open source models that have been released over the last several weeks, and&lt;strong&gt; a more enterprise-friendly license than Meta’s trickier and more nuanced open-ish Llama license&lt;/strong&gt;, which requires that users who operate a service with more than 700 million monthly active users obtain a paid license to keep using the company’s family of LLMs.&lt;/p&gt;



&lt;p&gt;By contrast, OpenAI’s new gpt-oss series of models offer no such restrictions. In keeping with Chinese competitors and counterparts, any consumer, developer, independent entrepreneur or enterprise large and small is empowered by the Apache 2.0 license to be able to download the new gpt-oss models at will, fine-tune and alter them to fit their specific needs, and use them to generate revenue or operate paid services,&lt;strong&gt; all without paying OpenAI a dime (or anything!).&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;This also means enterprises can &lt;strong&gt;use a powerful, near topline OpenAI model on their own hardware totally privately and securely, without sending any data up to the cloud, on web servers, or anywhere else&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt; For highly regulated industries like finance, healthcare, and legal services, not to mention organizations in military, intelligence, and government, this may be a requirement. &lt;/p&gt;



&lt;p&gt;Before today, anyone using ChatGPT or its application programming interface (API) — the service that acts like a switching board and allows third-party software developers to connect their own apps and services to these OpenAI’s proprietary/paid models like GPT-4o and o3 — &lt;strong&gt;was sending data up to OpenAI servers that could technically be subpoenaed by government agencies and accessed &lt;/strong&gt;without a user’s knowledge. That’s still the case for anyone using ChatGPT or the API going forward, as&lt;strong&gt; OpenAI co-founder and Sam Altman recently warned.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;And while running the new gpt-oss models locally on a user’s own hardware disconnected from the web would allow for maximum privacy, as soon as the user decides to connect it to external web search or other web enabled tools, some of the same privacy risks and issues would then arise — through any third-party web services the user or developer was relying on when hooking the models up to said tools. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-last-openai-open-source-language-model-was-released-more-than-six-years-ago"&gt;The last OpenAI open source language model was released more than six years ago&lt;/h2&gt;



&lt;p&gt;“This is the first time we’re releasing an open-weight language model in a long time…&lt;strong&gt; We view this as complementary to our other products,” said OpenAI co-founder and president Greg Brockman&lt;/strong&gt; on an embargoed press video call with VentureBeat and other journalists last night. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;last time OpenAI released a fully open source language model was GPT-2 in 2019,&lt;/strong&gt; &lt;strong&gt;more than six years ago, and three years before the release of ChatGPT&lt;/strong&gt;.&lt;/p&gt;



&lt;p&gt;This fact has &lt;strong&gt;sparked the ire of &lt;/strong&gt;— and resulted in several lawsuits from — &lt;strong&gt;former OpenAI co-founder and backer turned rival Elon Musk&lt;/strong&gt;, who, along with many other critics, have spent the last several years &lt;strong&gt;accusing OpenAI of betraying its mission&lt;/strong&gt; and founding principles and namesake &lt;strong&gt;by eschewing open source AI releases in favor of paid proprietary models&lt;/strong&gt; available only to customers of OpenAI’s API or paying ChatGPT subscribers (though there is a free tier for the latter).&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OpenAI co-founder CEO Sam Altman did express regret &lt;/strong&gt;about being on the “wrong side of history” but not releasing more open source AI sooner in a Reddit AMA (ask me anything) QA with users in February of this year, and Altman committed to releasing a new open source model back in March, but ultimately the company delayed its release from a planned July date until now. &lt;/p&gt;



&lt;p&gt;Now &lt;strong&gt;OpenAI is tacking back toward open source, and the question is, why&lt;/strong&gt;?&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-would-openai-release-a-set-of-free-open-source-models-that-it-makes-no-money-from"&gt;Why would OpenAI release a set of free open source models that it makes no money from?&lt;/h2&gt;



&lt;p&gt;To paraphrase Jesse Plemons’ character’s memorable line from the film &lt;em&gt;Game Night&lt;/em&gt;: “How can that be profitable for OpenAI?”&lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3015128" height="292" src="https://venturebeat.com/wp-content/uploads/2025/08/GLfcXKYWUAAYKZW.jpg" width="496" /&gt;&lt;/figure&gt;



&lt;p&gt;After all, business to OpenAI’s paid offerings appears to be booming. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Revenue has skyrocketed alongside the rapid expansion of its ChatGPT user base&lt;/strong&gt;, &lt;strong&gt;now at 700 million weekly active users&lt;/strong&gt;. As of August 2025, OpenAI reported $13 billion in annual recurring revenue, up from $10 billion in June. That &lt;strong&gt;growth is driven by a sharp rise in paying business customers — now 5 million, up from 3 million just two months earlier&lt;/strong&gt; — and surging daily engagement, with over 3 billion user messages sent every day. &lt;/p&gt;



&lt;p&gt;The financial momentum follows an $8.3 billion funding round that valued OpenAI at $300 billion and provides the foundation for the company’s aggressive infrastructure expansion and global ambitions.&lt;/p&gt;



&lt;p&gt;Compare that to &lt;strong&gt;closed/proprietary rival AI startup Anthropic’s reported $5 billion in total annual recurring revenue&lt;/strong&gt;, but interestingly,&lt;strong&gt; Anthropic is said to be getting more money from its API, $3.1 billion in revenue compared to OpenAI’s $2.9 billion&lt;/strong&gt;, according to &lt;em&gt;The Information&lt;/em&gt;.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;OpenAI and Anthropic both are showing pretty spectacular growth in 2025, with OpenAI doubling ARR in the last 6 months from $6bn to $12bn and Anthropic increasing 5x from $1bn to $5bn in 7 months.&lt;/p&gt;&lt;p&gt;If we compare the sources of revenue, the picture is quite interesting:&lt;br /&gt;– OpenAI… pic.twitter.com/8OaN1RSm9E&lt;/p&gt;— Peter Gostev (@petergostev) August 4, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;So, given how well the &lt;em&gt;paid AI business is already doing&lt;/em&gt;, the business strategy behind these open source offerings is less clear — especially since &lt;strong&gt;the new OpenAI gpt-oss models will almost certainly cut into some (perhaps a lot of) usage of OpenAI’s paid models.&lt;/strong&gt; Why go back to offering open source LLMs now when so much money is flowing into paid and none will, by virtue of its very intent, go directly toward open source models?&lt;/p&gt;



&lt;p&gt;Put simply: because &lt;strong&gt;open source competitors,&lt;/strong&gt; beginning with the release of the impressively efficient &lt;strong&gt;DeepSeek R1&lt;/strong&gt; by the Chinese AI division of the same name in January 2025, are &lt;strong&gt;offering near parity on performance benchmarks to paid proprietary models&lt;/strong&gt;, &lt;strong&gt;for free&lt;/strong&gt;, &lt;strong&gt;with fewer (basically zero) implementation restrictions&lt;/strong&gt; for enterprises and end users. And increasingly, &lt;strong&gt;enterprises are adopting these open source models&lt;/strong&gt; in production. &lt;/p&gt;



&lt;p&gt;As&lt;strong&gt; OpenAI executives and team members revealed&lt;/strong&gt; to VentureBeat and many other journalists on an embargoed video call last night about the new models that when it comes to&lt;strong&gt; OpenAI’s API&lt;/strong&gt;,&lt;strong&gt; the majority of customers are using a mix of paid OpenAI models &lt;em&gt;and open source models from other providers&lt;/em&gt;. &lt;/strong&gt;(I asked, but OpenAI declined to specify what percentage or total number of API customers are using open source models and which ones). &lt;/p&gt;



&lt;p&gt;At least, until now. &lt;strong&gt;OpenAI clearly hopes these new gpt-oss offerings will get more of these users to switch away from competing open source offerings and back into OpenAI’s ecosystem&lt;/strong&gt;, even if OpenAI doesn’t see any direct revenue or data from that usage.&lt;/p&gt;



&lt;p&gt;On a grander scale, it seems &lt;strong&gt;OpenAI wants to be a full-service, full-stack, one-stop shop AI offering&lt;/strong&gt; for &lt;em&gt;all of an enterprise, indie developer’s, or regular consumer’s&lt;/em&gt; machine intelligence needs — from a clean chatbot interface to an API to build services and apps atop of to agent frameworks for building AI agents through said API to an image generation model (gpt-4o native image generation), video model (Sora), audio transcription model (gpt-4o-transcribe), and now, open source offerings as well. Can a music generation and “world model” be far behind?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;OpenAI seeks to span the AI market, propriety and open source alike,&lt;/strong&gt; even if the latter is worth nothing in terms of actual, direct dollars and cents. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-training-and-architecture"&gt;Training and architecture&lt;/h2&gt;



&lt;p&gt;Feedback from developers directly influenced gpt-oss’s design. OpenAI says the top request was for a permissive license, which led to the adoption of Apache 2.0 for both models. Both models use a &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; architecture with a &lt;strong&gt;Transformer backbone&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;The larger gpt-oss-120b activates 5.1 billion parameters per token (out of 117 billion total), and gpt-oss-20b activates 3.6 billion (out of 21 billion total). &lt;/p&gt;



&lt;p&gt;Both support &lt;strong&gt;128,000 token context length&lt;/strong&gt; (about 300-400 pages of a novel’s worth of text a user can upload at once), and employ &lt;strong&gt;locally banded sparse attention&lt;/strong&gt; and use &lt;strong&gt;Rotary Positional Embeddings&lt;/strong&gt; for encoding.&lt;/p&gt;



&lt;p&gt;The tokenizer — the program that converts words and chunks of words into the numerical tokens that the LLMs can understand, dubbed&lt;strong&gt; “o200k_harmony&lt;/strong&gt;“&lt;strong&gt; &lt;/strong&gt;— is also being open-sourced.&lt;/p&gt;



&lt;p&gt;Developers can select among &lt;strong&gt;low, medium, or high reasoning effort&lt;/strong&gt; settings based on latency and performance needs. While these models can reason across complex agentic tasks, OpenAI emphasizes they were not trained with &lt;strong&gt;direct supervision of CoT outputs&lt;/strong&gt;, to preserve the observability of reasoning behavior—an approach OpenAI considers important for safety monitoring.&lt;/p&gt;



&lt;p&gt;Another &lt;strong&gt;common request from OpenAI’s developer community was for strong support for function calling, particularly for agentic workloads&lt;/strong&gt;, which OpenAI believes gpt-oss now delivers.&lt;/p&gt;



&lt;p&gt;The models are engineered for &lt;strong&gt;chain-of-thought reasoning&lt;/strong&gt;, &lt;strong&gt;tool use&lt;/strong&gt;, and &lt;strong&gt;few-shot function calling&lt;/strong&gt;, and are compatible with OpenAI’s &lt;strong&gt;Responses API&lt;/strong&gt; introduced back in March, which allows developers to augment their apps by connecting an OpenAI LLM of their choice to three powerful built-in tools — &lt;strong&gt;web search&lt;/strong&gt;, &lt;strong&gt;file search&lt;/strong&gt;, and &lt;strong&gt;computer use&lt;/strong&gt; — within a single API call.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;But for the new gpt-oss models, tool use capabilities — including web search and code execution — are not tied to OpenAI infrastructure. &lt;/strong&gt;OpenAI provides the schemas and examples used during training, such as a basic browser implementation using the Exa API and a Python interpreter that operates in a Docker container. &lt;/p&gt;



&lt;p&gt;It is&lt;strong&gt; up to individual inference providers or developers to define how tools are implemented.&lt;/strong&gt; Providers like vLLM, for instance, allow users to configure their own MCP (Model-Controller-Proxy) server to specify the browser backend.&lt;/p&gt;



&lt;p&gt;While these models can reason across complex agentic tasks, OpenAI emphasizes they were not trained with &lt;strong&gt;direct supervision of CoT outputs&lt;/strong&gt;, to preserve the observability of reasoning behavior—an approach OpenAI considers important for safety monitoring.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-safety-evaluations-and-measures"&gt;Safety evaluations and measures&lt;/h2&gt;



&lt;p&gt;OpenAI conducted safety training using its &lt;strong&gt;Preparedness Framework&lt;/strong&gt;, a document that outlines the procedural commitments, risk‑assessment criteria, capability categories, thresholds, evaluations, and governance mechanisms OpenAI uses to monitor, evaluate, and mitigate frontier AI risks.&lt;/p&gt;



&lt;p&gt;These included &lt;strong&gt;filtering chemical, biological, radiological, and nuclear threat (CBRN) related data&lt;/strong&gt; out during pretraining, and applying advanced post-training safety methods such as &lt;strong&gt;deliberative alignment&lt;/strong&gt; and an &lt;strong&gt;instruction hierarchy&lt;/strong&gt; to enforce refusal behavior on harmful prompts.&lt;/p&gt;



&lt;p&gt;To test worst-case misuse potential, OpenAI adversarially fine-tuned gpt-oss-120b on sensitive biology and cybersecurity data using its internal RL training stack. These &lt;strong&gt;malicious fine-tuning (MFT)&lt;/strong&gt; scenarios—one of the most sophisticated evaluations of this kind to date—included enabling browsing and disabling refusal behavior, simulating real-world attack potential.&lt;/p&gt;



&lt;p&gt;The resulting models were benchmarked against both open and proprietary LLMs, including DeepSeek R1-0528, Qwen 3 Thinking, Kimi K2, and OpenAI’s o3. Despite enhanced access to tools and targeted training, OpenAI found that even the fine-tuned gpt-oss models remained &lt;strong&gt;below the “High” capability threshold&lt;/strong&gt; for frontier risk domains such as biorisk and cybersecurity. These conclusions were reviewed by &lt;strong&gt;three independent expert groups&lt;/strong&gt;, whose recommendations were incorporated into the final methodology.&lt;/p&gt;



&lt;p&gt;In parallel, OpenAI partnered with &lt;strong&gt;SecureBio&lt;/strong&gt; to run external evaluations on biology-focused benchmarks like &lt;strong&gt;Human Pathogen Capabilities Test (HPCT)&lt;/strong&gt;, &lt;strong&gt;Molecular Biology Capabilities Test (MBCT)&lt;/strong&gt;, and others. Results showed that gpt-oss’s fine-tuned models performed close to OpenAI’s o3 model, which is not classified as frontier-high under OpenAI’s safety definitions.&lt;/p&gt;



&lt;p&gt;According to OpenAI, these findings contributed directly to the decision to release gpt-oss openly. The release is also intended to support safety research, especially around monitoring and controlling open-weight models in complex domains.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-availability-and-ecosystem-support"&gt;Availability and ecosystem support&lt;/h2&gt;



&lt;p&gt;The gpt-oss models are now available on Hugging Face, with pre-built support through major deployment platforms including &lt;strong&gt;Azure&lt;/strong&gt;, &lt;strong&gt;AWS&lt;/strong&gt;, &lt;strong&gt;Databricks&lt;/strong&gt;, &lt;strong&gt;Cloudflare&lt;/strong&gt;, &lt;strong&gt;Vercel&lt;/strong&gt;, &lt;strong&gt;Together AI&lt;/strong&gt;, &lt;strong&gt;OpenRouter&lt;/strong&gt;, and others. Hardware partners include &lt;strong&gt;NVIDIA&lt;/strong&gt;, &lt;strong&gt;AMD&lt;/strong&gt;, and &lt;strong&gt;Cerebras&lt;/strong&gt;, and Microsoft is making GPU-optimized builds available on &lt;strong&gt;Windows via ONNX Runtime&lt;/strong&gt;.&lt;/p&gt;



&lt;p&gt;OpenAI has also announced a &lt;strong&gt;$500,000 Red Teaming Challenge&lt;/strong&gt; hosted on Kaggle, inviting researchers and developers to probe the limits of gpt-oss and identify novel misuse pathways. A public report and an open-source evaluation dataset will follow, aiming to accelerate open model safety research across the AI community.&lt;/p&gt;



&lt;p&gt;Early adopters such as &lt;strong&gt;AI Sweden&lt;/strong&gt;, &lt;strong&gt;Orange&lt;/strong&gt;, and &lt;strong&gt;Snowflake&lt;/strong&gt; have collaborated with OpenAI to explore deployments ranging from localized fine-tuning to secure on-premise use cases. OpenAI characterizes the launch as an invitation for developers, enterprises, and governments to run state-of-the-art language models on their own terms.&lt;/p&gt;



&lt;p&gt;While OpenAI has not committed to a fixed cadence for future open-weight releases, it signals that gpt-oss represents a strategic expansion of its approach — balancing openness with aligned safety methodologies to shape how large models are shared and governed in the years ahead.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-big-question-with-so-much-competition-in-open-source-ai-will-openai-s-own-efforts-pay-off"&gt;The big question: with so much competition in open source AI, will OpenAI’s own efforts pay off?&lt;/h2&gt;



&lt;p&gt;OpenAI re-enters the open source model market in &lt;strong&gt;the most competitive moment&lt;/strong&gt; yet.&lt;/p&gt;



&lt;p&gt;At the top of public AI benchmarking leaderboards, U.S. frontier models remain &lt;strong&gt;proprietary&lt;/strong&gt; — &lt;strong&gt;OpenAI&lt;/strong&gt; (GPT-4o/o3), &lt;strong&gt;Google&lt;/strong&gt; (Gemini), and &lt;strong&gt;Anthropic&lt;/strong&gt; (Claude).&lt;/p&gt;



&lt;p&gt;But they now compete directly with a surge of &lt;strong&gt;open-weights&lt;/strong&gt; contenders. From China: &lt;strong&gt;DeepSeek-R1 (open source, MIT)&lt;/strong&gt; and &lt;strong&gt;DeepSeek-V3 (open-weights under a DeepSeek Model License that permits commercial use)&lt;/strong&gt;; &lt;strong&gt;Alibaba’s Qwen 3 (open-weights, Apache-2.0)&lt;/strong&gt;; &lt;strong&gt;MoonshotAI’s Kimi K2 (open-weights; public repo and model cards)&lt;/strong&gt;; and &lt;strong&gt;Z.ai’s GLM-4.5&lt;/strong&gt; (also Apache 2.0 licensed). &lt;/p&gt;



&lt;p&gt;Europe’s &lt;strong&gt;Mistral (Mixtral/Mistral, open-weights, Apache-2.0)&lt;/strong&gt; anchors the EU push; the UAE’s &lt;strong&gt;Falcon 2/3&lt;/strong&gt; publish &lt;strong&gt;open-weights&lt;/strong&gt; under TII’s Apache-based license. In the U.S. open-weights camp, &lt;strong&gt;Meta’s Llama 3.1&lt;/strong&gt; ships under a &lt;strong&gt;community (source-available) license&lt;/strong&gt;, &lt;strong&gt;Google’s Gemma&lt;/strong&gt; under &lt;strong&gt;Gemma terms&lt;/strong&gt; (open weights with use restrictions), and &lt;strong&gt;Microsoft’s Phi-3.5&lt;/strong&gt; under &lt;strong&gt;MIT&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Developer pull mirrors that split.&lt;/strong&gt; On Hugging Face, &lt;strong&gt;Qwen2.5-7B-Instruct (open-weights, Apache-2.0)&lt;/strong&gt; sits near the top by “downloads last month,” while &lt;strong&gt;DeepSeek-R1 (MIT)&lt;/strong&gt; and &lt;strong&gt;DeepSeek-V3 (model-licensed open weights)&lt;/strong&gt; also post heavy traction. Open-weights stalwarts &lt;strong&gt;Mistral-7B / Mixtral&lt;/strong&gt; (Apache-2.0), &lt;strong&gt;Llama-3.1-8B/70B&lt;/strong&gt; (Meta community license), &lt;strong&gt;Gemma-2&lt;/strong&gt; (Gemma terms), &lt;strong&gt;Phi-3.5&lt;/strong&gt; (MIT), &lt;strong&gt;GLM-4.5&lt;/strong&gt; (open-weights), and &lt;strong&gt;Falcon-2-11B&lt;/strong&gt; (TII Falcon License 2.0) round out the most-pulled families —underscoring that the open ecosystem spans the U.S., Europe, the Middle East, and China. Hugging Face signals adoption, not market share, but they show where builders are experimenting and deploying today.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Consumer usage remains concentrated in proprietary apps even as weights open up.&lt;/strong&gt; &lt;strong&gt;ChatGPT&lt;/strong&gt; still drives the largest engagement globally (about &lt;strong&gt;2.5 billion prompts/day&lt;/strong&gt;, proprietary service), while in China the leading assistants — &lt;strong&gt;ByteDance’s Doubao&lt;/strong&gt;, &lt;strong&gt;DeepSeek’s app&lt;/strong&gt;, &lt;strong&gt;Moonshot’s Kimi&lt;/strong&gt;, and &lt;strong&gt;Baidu’s ERNIE Bot&lt;/strong&gt; — are delivered as &lt;strong&gt;proprietary products&lt;/strong&gt;, even as several base models (GLM-4.5, ERNIE 4.5 variants) now ship as &lt;strong&gt;open-weights&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;But now that a range of powerful open source models are available to businesses and consumers — all nearing one another in terms of performance — and can be downloaded on consumer hardware, the &lt;strong&gt;big question facing OpenAI is: who will pay for intelligence at all? Will the convenience of the web-based chatbot interface, multimodal capabilities, and more powerful performance be enough to keep the dollars flowing?&lt;/strong&gt; Or has machine intelligence already become, in the words of Atlman himself, “too cheap to meter”? And if so, how to build a successful business atop it, especially with OpenAI and other AI firms’ sky-high valuations and expenditures. &lt;/p&gt;



&lt;p&gt;One clue: OpenAI is already said to be offering in-house engineers to help its enterprise customers customize and deploy fine-tuned models, similar to Palantir’s “forward deployed” software engineers (SWEs), essentially charging for experts to come in, set up the models correctly, and train employees how to use them for best results.&lt;/p&gt;



&lt;p&gt;Perhaps the world will migrate toward a majority of AI usage going to open source models, or a sizeable minority, with OpenAI and other AI model providers offering experts to help install said models into enterprises. Is that enough of a service to build a multi-billion dollar business upon? Or will enough people continue paying $20, $200 or more each month to have access to even more powerful proprietary models? &lt;/p&gt;



&lt;p&gt;I don’t envy the folks at OpenAI figuring out all the business calculations — despite what I assume to be hefty compensation as a result, at least for now. But for end users and enterprises, the release of the gpt-oss series is undoubtedly compelling. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b/</guid><pubDate>Tue, 05 Aug 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI launches two ‘open’ AI reasoning models (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of two open-weight AI reasoning models with similar capabilities to its o-series. Both are freely available to download from the online developer platform Hugging Face, the company said, describing the models as “state-of-the-art” when measured across several benchmarks for comparing open models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models come in two sizes: a larger and more capable gpt-oss-120b model that can run on a single Nvidia GPU, and a lighter-weight gpt-oss-20b model that can run on a consumer laptop with 16GB of memory.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks OpenAI’s first ‘open’ language model since GPT-2, which was released more than five years ago. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. That means if OpenAI’s open model is not capable of a certain task, such as processing an image, developers can connect the open model to one of the company’s more capable closed models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI open sourced AI models in its early days, the company has generally favored a proprietary, closed source development approach. The latter strategy has helped OpenAI build a large business selling access to its AI models via an API to enterprises and developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, CEO Sam Altman said in January he believes OpenAI has been “on the wrong side of history” when it comes to open sourcing its technologies. The company today faces growing pressure from Chinese AI labs — including DeepSeek, Alibaba’s Qwen, and Moonshot AI — which have developed several of the world’s most capable and popular open models. (While Meta previously dominated the open AI space, the company’s Llama AI models have fallen behind in the last year.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July, the Trump administration also urged U.S. AI developers to open source more technology to promote global adoption of AI aligned with American values.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;With the release of gpt-oss, OpenAI hopes to curry favor with developers and the Trump administration alike, both of which have watched the Chinese AI labs rise to prominence in the open source space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Going back to when we started in 2015, OpenAI’s mission is to ensure AGI that benefits all of humanity,” said Altman in a statement shared with TechCrunch. “To that end, we are excited for the world to be building on an open AI stack created in the United States, based on democratic values, available for free to all and for wide benefit.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="Open AI CEO Sam Altman" class="wp-image-2993943" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tomohiro Ohsumi / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-the-models-performed"&gt;How the models performed&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI aimed to make its open model a leader among other open-weight AI models, and the company claims to have done just that. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Codeforces (with tools), a competitive coding test, gpt-oss-120b and gpt-oss-20b score 2622 and 2516, respectively, outperforming DeepSeek’s R1 while underperforming o3 and o4-mini.&lt;/p&gt;

&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3034265" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.21.54PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on codeforces.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On Humanity’s Last Exam (HLE), a challenging test of crowdsourced questions across a variety of subjects (with tools), gpt-oss-120b and gpt-oss-20b score 19% and 17.3%, respectively. Similarly, this underperforms o3 but outperforms leading open models from DeepSeek and Qwen.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3034259" height="438" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.18.20PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on HLE.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, OpenAI’s open models hallucinate significantly more than its latest AI reasoning models, o3 and o4-mini. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hallucinations have been getting more severe in OpenAI’s latest AI reasoning models, and the company previously said it doesn’t quite understand why. In a white paper, OpenAI says this is “expected, as smaller models have less world knowledge than larger frontier models and tend to hallucinate more.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI found that gpt-oss-120b and gpt-oss-20b hallucinated in response to 49% and 53% of questions on PersonQA, the company’s in-house benchmark for measuring the accuracy of a model’s knowledge about people. That’s more than triple the hallucination rate of OpenAI’s o1 model, which scored 16%, and higher than its o4-mini model, which scored 36%.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-training-the-new-models"&gt;Training the new models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its open models were trained with similar processes to its proprietary models. The company says each open model leverages mixture-of-experts (MoE) to tap fewer parameters for any given question, making it run more efficiently. For gpt-oss-120b, which has 117 billion total parameters, OpenAI says the model only activates 5.1 billion parameters per token.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also says its open model was trained using high-compute reinforcement learning (RL) — a post-training process to teach AI models right from wrong in simulated environments using large clusters of Nvidia GPUs. This was also used to train OpenAI’s o-series of models, and the open models have a similar chain-of-thought process in which they take additional time and computational resources to work through their answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. However, OpenAI says its open models are text-only, meaning they will not be able to process or generate images and audio like the company’s other models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is releasing gpt-oss-120b and gpt-oss-20b under the Apache 2.0 license, which is generally considered one of the most permissive. This license will allow enterprises to monetize OpenAI’s open models without having to pay or obtain permission from the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, unlike fully open source offerings from AI labs like AI2, OpenAI says it will not be releasing the training data used to create its open models. This decision is not surprising given that several active lawsuits against AI model providers, including OpenAI, have alleged that these companies inappropriately trained their AI models on copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI delayed the release of its open models several times in recent months, partially to address safety concerns. Beyond the company’s typical safety policies, OpenAI says in a white paper that it also investigated whether bad actors could fine-tune its gpt-oss models to be more helpful in cyberattacks or the creation of biological or chemical weapons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After testing from OpenAI and third-party evaluators, the company says gpt-oss may marginally increase biological capabilities. However, it did not find evidence that these open models could reach its “high capability” threshold for danger in these domains, even after fine-tuning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI’s model appears to be state-of-the-art among open models, developers are eagerly awaiting the release of DeepSeek R2, its next AI reasoning model, as well as a new open model from Meta’s Superintelligence Lab.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI announced Tuesday the launch of two open-weight AI reasoning models with similar capabilities to its o-series. Both are freely available to download from the online developer platform Hugging Face, the company said, describing the models as “state-of-the-art” when measured across several benchmarks for comparing open models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The models come in two sizes: a larger and more capable gpt-oss-120b model that can run on a single Nvidia GPU, and a lighter-weight gpt-oss-20b model that can run on a consumer laptop with 16GB of memory.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks OpenAI’s first ‘open’ language model since GPT-2, which was released more than five years ago. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a briefing, OpenAI said its open models will be capable of sending complex queries to AI models in the cloud, as TechCrunch previously reported. That means if OpenAI’s open model is not capable of a certain task, such as processing an image, developers can connect the open model to one of the company’s more capable closed models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI open sourced AI models in its early days, the company has generally favored a proprietary, closed source development approach. The latter strategy has helped OpenAI build a large business selling access to its AI models via an API to enterprises and developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, CEO Sam Altman said in January he believes OpenAI has been “on the wrong side of history” when it comes to open sourcing its technologies. The company today faces growing pressure from Chinese AI labs — including DeepSeek, Alibaba’s Qwen, and Moonshot AI — which have developed several of the world’s most capable and popular open models. (While Meta previously dominated the open AI space, the company’s Llama AI models have fallen behind in the last year.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In July, the Trump administration also urged U.S. AI developers to open source more technology to promote global adoption of AI aligned with American values.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;With the release of gpt-oss, OpenAI hopes to curry favor with developers and the Trump administration alike, both of which have watched the Chinese AI labs rise to prominence in the open source space.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Going back to when we started in 2015, OpenAI’s mission is to ensure AGI that benefits all of humanity,” said Altman in a statement shared with TechCrunch. “To that end, we are excited for the world to be building on an open AI stack created in the United States, based on democratic values, available for free to all and for wide benefit.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="Open AI CEO Sam Altman" class="wp-image-2993943" height="453" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tomohiro Ohsumi / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-how-the-models-performed"&gt;How the models performed&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI aimed to make its open model a leader among other open-weight AI models, and the company claims to have done just that. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;On Codeforces (with tools), a competitive coding test, gpt-oss-120b and gpt-oss-20b score 2622 and 2516, respectively, outperforming DeepSeek’s R1 while underperforming o3 and o4-mini.&lt;/p&gt;

&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3034265" height="459" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.21.54PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on codeforces.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;On Humanity’s Last Exam (HLE), a challenging test of crowdsourced questions across a variety of subjects (with tools), gpt-oss-120b and gpt-oss-20b score 19% and 17.3%, respectively. Similarly, this underperforms o3 but outperforms leading open models from DeepSeek and Qwen.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3034259" height="438" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-05-at-12.18.20PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s open model performance on HLE.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, OpenAI’s open models hallucinate significantly more than its latest AI reasoning models, o3 and o4-mini. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hallucinations have been getting more severe in OpenAI’s latest AI reasoning models, and the company previously said it doesn’t quite understand why. In a white paper, OpenAI says this is “expected, as smaller models have less world knowledge than larger frontier models and tend to hallucinate more.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI found that gpt-oss-120b and gpt-oss-20b hallucinated in response to 49% and 53% of questions on PersonQA, the company’s in-house benchmark for measuring the accuracy of a model’s knowledge about people. That’s more than triple the hallucination rate of OpenAI’s o1 model, which scored 16%, and higher than its o4-mini model, which scored 36%.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-training-the-new-models"&gt;Training the new models&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI says its open models were trained with similar processes to its proprietary models. The company says each open model leverages mixture-of-experts (MoE) to tap fewer parameters for any given question, making it run more efficiently. For gpt-oss-120b, which has 117 billion total parameters, OpenAI says the model only activates 5.1 billion parameters per token.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also says its open model was trained using high-compute reinforcement learning (RL) — a post-training process to teach AI models right from wrong in simulated environments using large clusters of Nvidia GPUs. This was also used to train OpenAI’s o-series of models, and the open models have a similar chain-of-thought process in which they take additional time and computational resources to work through their answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As a result of the post-training process, OpenAI says its open AI models excel at powering AI agents and are capable of calling tools such as web search or Python code execution as part of its chain-of-thought process. However, OpenAI says its open models are text-only, meaning they will not be able to process or generate images and audio like the company’s other models.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI is releasing gpt-oss-120b and gpt-oss-20b under the Apache 2.0 license, which is generally considered one of the most permissive. This license will allow enterprises to monetize OpenAI’s open models without having to pay or obtain permission from the company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, unlike fully open source offerings from AI labs like AI2, OpenAI says it will not be releasing the training data used to create its open models. This decision is not surprising given that several active lawsuits against AI model providers, including OpenAI, have alleged that these companies inappropriately trained their AI models on copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI delayed the release of its open models several times in recent months, partially to address safety concerns. Beyond the company’s typical safety policies, OpenAI says in a white paper that it also investigated whether bad actors could fine-tune its gpt-oss models to be more helpful in cyberattacks or the creation of biological or chemical weapons.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After testing from OpenAI and third-party evaluators, the company says gpt-oss may marginally increase biological capabilities. However, it did not find evidence that these open models could reach its “high capability” threshold for danger in these domains, even after fine-tuning.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI’s model appears to be state-of-the-art among open models, developers are eagerly awaiting the release of DeepSeek R2, its next AI reasoning model, as well as a new open model from Meta’s Superintelligence Lab.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/05/openai-launches-two-open-ai-reasoning-models/</guid><pubDate>Tue, 05 Aug 2025 17:00:00 +0000</pubDate></item><item><title>[NEW] OpenAI announces two “gpt-oss” open AI models, and you can download them today (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/08/openai-releases-its-first-open-source-models-since-2019/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI's new open models can run on your hardware instead of in the cloud.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI is releasing new generative AI models today, and no, GPT-5 is not one of them. Depending on how you feel about generative AI, these new models may be even more interesting, though. The company is rolling out gpt-oss-120b and gpt-oss-20b, its first open-weight models since the release of GPT-2 in 2019. You can download and run these models on your own hardware, with support for simulated reasoning, tool use, and deep customization.&lt;/p&gt;
&lt;p&gt;When you access the company's proprietary models in the cloud, they're running on powerful server infrastructure that cannot be replicated easily, even in enterprise. The new OpenAI models come in two variants (120b and 20b) to run on less powerful hardware configurations. Both are transformers with a configurable chain of thought (CoT), supporting low, medium, and high settings. The lower settings are faster and use fewer compute resources, but the outputs are better with the highest setting. You can set the CoT level with a single line in the system prompt.&lt;/p&gt;
&lt;p&gt;The smaller gpt-oss-20b has a total of 21 billion parameters, utilizing mixture-of-experts (MoE) to reduce that to 3.6 billion parameters per token. As for gpt-oss-120b, its 117 billion parameters come down to 5.1 billion per token with MoE. The company says the smaller model can run on a consumer-level machine with 16GB or more of memory. To run gpt-oss-120b, you need 80GB of memory, which is more than you're likely to find in the average consumer machine. It should fit on a single AI accelerator GPU like the Nvidia H100, though. Both models have a context window of 128,000 tokens.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2110136 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="894" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/gpt-oss-bench.png" width="705" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The team says users of gpt-oss can expect robust performance similar to its leading cloud-based models. The larger one benchmarks between the o3 and o4-mini proprietary models in most tests, with the smaller version running just a little behind. It gets closest in math and coding tasks. In the knowledge-based Humanity's Last Exam, o3 is far out in front with 24.9 percent (with tools), while gpt-oss-120b only manages 19 percent. For comparison, Google's leading Gemini Deep Think hits 34.8 percent in that test.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Not good at being evil&lt;/h2&gt;
&lt;p&gt;OpenAI says it doesn't intend for anyone to replace its proprietary models with the new OSS releases. It did not set out to replicate what you can do with the mainline GPT releases here, and there are some notable limitations. For example, gpt-oss-120b and gpt-oss-20b are text-only with no multimodality out of the box. However, the company acknowledges there are times when someone might not want to rely on a big cloud-based AI—locally managed AI has lower latency and more opportunities for customization, and it can keep sensitive data secure on site.&lt;/p&gt;
&lt;p&gt;OpenAI is cognizant that many users of the company's proprietary models are also leveraging open source models for these reasons. Currently, those firms are using non-OpenAI products for local AI, but the team designed the gpt-oss models to integrate with the proprietary GPT models. So customers can now use end-to-end OpenAI products even if they need to process some data locally.&lt;/p&gt;
&lt;p&gt;Because these models are fully open and governed by the Apache 2.0 license, developers will be able to tune them for specific use cases. Like all AI firms, OpenAI builds controls into its models to limit malicious behavior, but it's been a few years since the company released an open model—the gpt-oss models are much more powerful than GPT-2 was in 2019.&lt;/p&gt;
&lt;p&gt;To ensure it was doing all it could in terms of safety, OpenAI decided to test some worst-case scenarios by tuning gpt-oss to be evil. The devs say that even after trying to make the model misbehave, it never reached a high level of quality doing evil things, based on the company's Preparedness Framework. OpenAI claims this means its use of deliberative alignment and instruction hierarchy will prevent serious misuse of the open models.&lt;/p&gt;
&lt;p&gt;If you want to test that claim yourself, gpt-oss-120b and gpt-oss-20b are available for download today on HuggingFace. There are also GitHub repos for your perusal, and OpenAI will host stock versions of the models on its own infrastructure for testing. If you are interested in more technical details, the company has provided both a model card and a research blog post.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI's new open models can run on your hardware instead of in the cloud.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo over a tectonic shift in the background." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/09/openai_tectonic_shift-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI is releasing new generative AI models today, and no, GPT-5 is not one of them. Depending on how you feel about generative AI, these new models may be even more interesting, though. The company is rolling out gpt-oss-120b and gpt-oss-20b, its first open-weight models since the release of GPT-2 in 2019. You can download and run these models on your own hardware, with support for simulated reasoning, tool use, and deep customization.&lt;/p&gt;
&lt;p&gt;When you access the company's proprietary models in the cloud, they're running on powerful server infrastructure that cannot be replicated easily, even in enterprise. The new OpenAI models come in two variants (120b and 20b) to run on less powerful hardware configurations. Both are transformers with a configurable chain of thought (CoT), supporting low, medium, and high settings. The lower settings are faster and use fewer compute resources, but the outputs are better with the highest setting. You can set the CoT level with a single line in the system prompt.&lt;/p&gt;
&lt;p&gt;The smaller gpt-oss-20b has a total of 21 billion parameters, utilizing mixture-of-experts (MoE) to reduce that to 3.6 billion parameters per token. As for gpt-oss-120b, its 117 billion parameters come down to 5.1 billion per token with MoE. The company says the smaller model can run on a consumer-level machine with 16GB or more of memory. To run gpt-oss-120b, you need 80GB of memory, which is more than you're likely to find in the average consumer machine. It should fit on a single AI accelerator GPU like the Nvidia H100, though. Both models have a context window of 128,000 tokens.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2110136 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="894" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/gpt-oss-bench.png" width="705" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;The team says users of gpt-oss can expect robust performance similar to its leading cloud-based models. The larger one benchmarks between the o3 and o4-mini proprietary models in most tests, with the smaller version running just a little behind. It gets closest in math and coding tasks. In the knowledge-based Humanity's Last Exam, o3 is far out in front with 24.9 percent (with tools), while gpt-oss-120b only manages 19 percent. For comparison, Google's leading Gemini Deep Think hits 34.8 percent in that test.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Not good at being evil&lt;/h2&gt;
&lt;p&gt;OpenAI says it doesn't intend for anyone to replace its proprietary models with the new OSS releases. It did not set out to replicate what you can do with the mainline GPT releases here, and there are some notable limitations. For example, gpt-oss-120b and gpt-oss-20b are text-only with no multimodality out of the box. However, the company acknowledges there are times when someone might not want to rely on a big cloud-based AI—locally managed AI has lower latency and more opportunities for customization, and it can keep sensitive data secure on site.&lt;/p&gt;
&lt;p&gt;OpenAI is cognizant that many users of the company's proprietary models are also leveraging open source models for these reasons. Currently, those firms are using non-OpenAI products for local AI, but the team designed the gpt-oss models to integrate with the proprietary GPT models. So customers can now use end-to-end OpenAI products even if they need to process some data locally.&lt;/p&gt;
&lt;p&gt;Because these models are fully open and governed by the Apache 2.0 license, developers will be able to tune them for specific use cases. Like all AI firms, OpenAI builds controls into its models to limit malicious behavior, but it's been a few years since the company released an open model—the gpt-oss models are much more powerful than GPT-2 was in 2019.&lt;/p&gt;
&lt;p&gt;To ensure it was doing all it could in terms of safety, OpenAI decided to test some worst-case scenarios by tuning gpt-oss to be evil. The devs say that even after trying to make the model misbehave, it never reached a high level of quality doing evil things, based on the company's Preparedness Framework. OpenAI claims this means its use of deliberative alignment and instruction hierarchy will prevent serious misuse of the open models.&lt;/p&gt;
&lt;p&gt;If you want to test that claim yourself, gpt-oss-120b and gpt-oss-20b are available for download today on HuggingFace. There are also GitHub repos for your perusal, and OpenAI will host stock versions of the models on its own infrastructure for testing. If you are interested in more technical details, the company has provided both a model card and a research blog post.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/08/openai-releases-its-first-open-source-models-since-2019/</guid><pubDate>Tue, 05 Aug 2025 17:00:27 +0000</pubDate></item><item><title>[NEW] OpenAI and NVIDIA Propel AI Innovation With New Open Models Optimized for the World’s Largest AI Inference Infrastructure (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/openai-gpt-oss/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/openai-nvidia-featured-image-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Two new open-weight AI reasoning models from OpenAI released today bring cutting-edge AI development directly into the hands of developers, enthusiasts, enterprises, startups and governments everywhere — across every industry and at every scale.&lt;/p&gt;
&lt;p&gt;NVIDIA’s collaboration with OpenAI on these open models — gpt-oss-120b and gpt-oss-20b — is a testament to the power of community-driven innovation and highlights NVIDIA’s foundational role in making AI accessible worldwide.&lt;/p&gt;
&lt;p&gt;Anyone can use the models to develop breakthrough applications in generative, reasoning and physical AI, healthcare and manufacturing — or even unlock new industries as the next industrial revolution driven by AI continues to unfold.&lt;/p&gt;
&lt;p&gt;OpenAI’s new flexible, open-weight text-reasoning large language models (LLMs) were trained on NVIDIA H100 GPUs and run inference best on the hundreds of millions of GPUs running the NVIDIA CUDA platform across the globe.&lt;/p&gt;
&lt;p&gt;With software optimizations for the NVIDIA Blackwell platform, the models offer optimal inference on NVIDIA GB200 NVL72 systems, achieving 1.5 million tokens per second — driving massive efficiency for inference.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Blackwell Delivers Advanced Reasoning&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As advanced reasoning models like gpt-oss generate exponentially more tokens, the demand on compute infrastructure increases dramatically. Meeting this demand calls for purpose-built AI factories powered by NVIDIA Blackwell, an architecture designed to deliver the scale, efficiency and return on investment required to run inference at the highest level.&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell includes innovations such as NVFP4 4-bit precision, which enables ultra-efficient, high-accuracy inference while significantly reducing power and memory requirements. This makes it possible to deploy trillion-parameter LLMs in real time, which can unlock billions of dollars in value for organizations.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Open Development for Millions of AI Builders Worldwide&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA CUDA is the world’s most widely available computing infrastructure, letting users deploy and run AI models anywhere, from the powerful NVIDIA DGX Cloud platform to NVIDIA GeForce RTX– and NVIDIA RTX PRO-powered PCs and workstations.&lt;/p&gt;
&lt;p&gt;There are over 450 million NVIDIA CUDA downloads to date, and starting today, the massive community of CUDA developers gains access to these latest models, optimized to run on the NVIDIA technology stack they already use.&lt;/p&gt;
&lt;p&gt;Demonstrating their commitment to open-sourcing software, OpenAI and NVIDIA have collaborated with top open framework providers to provide model optimizations for FlashInfer, Hugging Face, llama.cpp, Ollama and vLLM, in addition to NVIDIA Tensor-RT LLM and other libraries, so developers can build with their framework of choice.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;A History of Collaboration, Building on Open Source&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today’s model releases underscore how NVIDIA’s full-stack approach helps bring the world’s most ambitious AI projects to the broadest user base possible.&lt;/p&gt;
&lt;p&gt;It’s a story that goes back to the earliest days of NVIDIA’s collaboration with OpenAI, which began in 2016 when Huang hand-delivered the first NVIDIA DGX-1 AI supercomputer to OpenAI’s headquarters in San Francisco.&lt;/p&gt;
&lt;p&gt;Since then, the companies have been working together to push the boundaries of what’s possible with AI, providing the core technologies and expertise needed for massive-scale training runs.&lt;/p&gt;
&lt;p&gt;And by optimizing OpenAI’s gpt-oss models for NVIDIA Blackwell and RTX GPUs, along with NVIDIA’s extensive software stack, NVIDIA is enabling faster, more cost-effective AI advancements for its 6.5 million developers across 250 countries using 900+ NVIDIA software development kits and AI models — and counting.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by reading the &lt;span&gt;NVIDIA Technical Blog&lt;/span&gt;&lt;span&gt; and&lt;/span&gt; &lt;/i&gt;&lt;i&gt;latest installment of the NVIDIA RTX AI Garage blog series&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/openai-nvidia-featured-image-1280x680-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Two new open-weight AI reasoning models from OpenAI released today bring cutting-edge AI development directly into the hands of developers, enthusiasts, enterprises, startups and governments everywhere — across every industry and at every scale.&lt;/p&gt;
&lt;p&gt;NVIDIA’s collaboration with OpenAI on these open models — gpt-oss-120b and gpt-oss-20b — is a testament to the power of community-driven innovation and highlights NVIDIA’s foundational role in making AI accessible worldwide.&lt;/p&gt;
&lt;p&gt;Anyone can use the models to develop breakthrough applications in generative, reasoning and physical AI, healthcare and manufacturing — or even unlock new industries as the next industrial revolution driven by AI continues to unfold.&lt;/p&gt;
&lt;p&gt;OpenAI’s new flexible, open-weight text-reasoning large language models (LLMs) were trained on NVIDIA H100 GPUs and run inference best on the hundreds of millions of GPUs running the NVIDIA CUDA platform across the globe.&lt;/p&gt;
&lt;p&gt;With software optimizations for the NVIDIA Blackwell platform, the models offer optimal inference on NVIDIA GB200 NVL72 systems, achieving 1.5 million tokens per second — driving massive efficiency for inference.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA Blackwell Delivers Advanced Reasoning&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As advanced reasoning models like gpt-oss generate exponentially more tokens, the demand on compute infrastructure increases dramatically. Meeting this demand calls for purpose-built AI factories powered by NVIDIA Blackwell, an architecture designed to deliver the scale, efficiency and return on investment required to run inference at the highest level.&lt;/p&gt;
&lt;p&gt;NVIDIA Blackwell includes innovations such as NVFP4 4-bit precision, which enables ultra-efficient, high-accuracy inference while significantly reducing power and memory requirements. This makes it possible to deploy trillion-parameter LLMs in real time, which can unlock billions of dollars in value for organizations.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Open Development for Millions of AI Builders Worldwide&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA CUDA is the world’s most widely available computing infrastructure, letting users deploy and run AI models anywhere, from the powerful NVIDIA DGX Cloud platform to NVIDIA GeForce RTX– and NVIDIA RTX PRO-powered PCs and workstations.&lt;/p&gt;
&lt;p&gt;There are over 450 million NVIDIA CUDA downloads to date, and starting today, the massive community of CUDA developers gains access to these latest models, optimized to run on the NVIDIA technology stack they already use.&lt;/p&gt;
&lt;p&gt;Demonstrating their commitment to open-sourcing software, OpenAI and NVIDIA have collaborated with top open framework providers to provide model optimizations for FlashInfer, Hugging Face, llama.cpp, Ollama and vLLM, in addition to NVIDIA Tensor-RT LLM and other libraries, so developers can build with their framework of choice.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;A History of Collaboration, Building on Open Source&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Today’s model releases underscore how NVIDIA’s full-stack approach helps bring the world’s most ambitious AI projects to the broadest user base possible.&lt;/p&gt;
&lt;p&gt;It’s a story that goes back to the earliest days of NVIDIA’s collaboration with OpenAI, which began in 2016 when Huang hand-delivered the first NVIDIA DGX-1 AI supercomputer to OpenAI’s headquarters in San Francisco.&lt;/p&gt;
&lt;p&gt;Since then, the companies have been working together to push the boundaries of what’s possible with AI, providing the core technologies and expertise needed for massive-scale training runs.&lt;/p&gt;
&lt;p&gt;And by optimizing OpenAI’s gpt-oss models for NVIDIA Blackwell and RTX GPUs, along with NVIDIA’s extensive software stack, NVIDIA is enabling faster, more cost-effective AI advancements for its 6.5 million developers across 250 countries using 900+ NVIDIA software development kits and AI models — and counting.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more by reading the &lt;span&gt;NVIDIA Technical Blog&lt;/span&gt;&lt;span&gt; and&lt;/span&gt; &lt;/i&gt;&lt;i&gt;latest installment of the NVIDIA RTX AI Garage blog series&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/openai-gpt-oss/</guid><pubDate>Tue, 05 Aug 2025 17:01:23 +0000</pubDate></item><item><title>[NEW] OpenAI’s New Open Models Accelerated Locally on NVIDIA GeForce RTX and RTX PRO GPUs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In collaboration with OpenAI, NVIDIA has optimized the company’s new open-source gpt-oss models for NVIDIA GPUs, delivering smart, fast inference from the cloud to the PC. These new reasoning models enable agentic AI applications such as web search, in-depth research and many more.&lt;/p&gt;
&lt;p&gt;With the launch of gpt-oss-20b and gpt-oss-120b, OpenAI has opened cutting-edge models to millions of users. AI enthusiasts and developers can use the optimized models on NVIDIA RTX AI PCs and workstations through popular tools and frameworks like Ollama, llama.cpp and Microsoft AI Foundry Local, and expect performance of up to 256 tokens per second on the NVIDIA GeForce RTX 5090 GPU.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;p&gt;The models’ release highlights NVIDIA’s AI leadership from training to inference and from cloud to AI PC.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Open for All &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Both gpt-oss-20b and gpt-oss-120b are flexible, open-weight reasoning models with chain-of-thought capabilities and adjustable reasoning effort levels using the popular mixture-of-experts architecture. The models are designed to support features like instruction-following and tool use, and were trained on NVIDIA H100 GPUs. &lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentStart CommentHighlightPipeRest CommentHighlightRest SCXW24568032 BCX0"&gt;AI developers can learn more and get started &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;using instructions &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;from&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt; the &lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun Underlined SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;NVIDIA &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;T&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;ech&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;nical B&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These models can support up to 131,072 context lengths, among the longest available in local inference. This means the models can reason through context problems, ideal for tasks such as web search, coding assistance, document comprehension and in-depth research.&lt;/p&gt;
&lt;p&gt;The OpenAI open models are the first MXFP4 models supported on NVIDIA RTX. MXFP4 allows for high model quality, offering fast, efficient performance while requiring fewer resources compared with other precision types.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Run the OpenAI Models on NVIDIA RTX With Ollama&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The easiest way to test these models on RTX AI PCs, on GPUs with at least 24GB of VRAM, is using the new Ollama app. Ollama is popular with AI enthusiasts and developers for its ease of integration, and the new user interface (UI) includes out-of-the-box support for OpenAI’s open-weight models. Ollama is fully optimized for RTX, making it ideal for consumers looking to experience the power of personal AI on their PC or workstation.&lt;/p&gt;
&lt;p&gt;Once installed, Ollama enables quick, easy chatting with the models. Simply select the model from the dropdown menu and send a message. Because Ollama is optimized for RTX, there are no additional configurations or commands required to ensure top performance on supported GPUs.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83414"&gt;&lt;img alt="alt" class="size-large wp-image-83414" height="893" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/rtx-ai-garage-3-steps-20b_2-1680x893.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83414"&gt;Testing OpenAI’s open models in Ollama is easy.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ollama’s new app includes other new features, like easy support for PDF or text files within chats, multimodal support on applicable models so users can include images in their prompts, and easily customizable context lengths when working with large documents or chats.&lt;/p&gt;
&lt;p&gt;Developers can also use Ollama via command line interface or the app’s software development kit (SDK) to power their applications and workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Other Ways to Use the New OpenAI Models on RTX&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Enthusiasts and developers can also try the gpt-oss models on RTX AI PCs through various other applications and frameworks, all powered by RTX, on GPUs that have at least 16GB of VRAM.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to collaborate with the open-source community on both llama.cpp and the GGML tensor library to optimize performance on RTX GPUs. Recent contributions include implementing CUDA Graphs to reduce overhead and adding algorithms that reduce CPU overheads. Check out the llama.cpp GitHub repository to get started.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83402"&gt;&lt;img alt="RTX performance for OpenAI's new open models." class="size-full wp-image-83402" height="351" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/oai-perf.png" width="624" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83402"&gt;Overall performance of the gpt-oss-20b model on various RTX AI PCs.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Windows developers can also access OpenAI’s new models via Microsoft AI Foundry Local, currently in public preview. Foundry Local is an on-device AI inferencing solution that integrates into workflows via the command line, SDK or application programming interfaces. Foundry Local uses ONNX Runtime, optimized through CUDA, with support for NVIDIA TensorRT for RTX coming soon. Getting started is easy: install Foundry Local and invoke “Foundry model run gpt-oss-20b” in a terminal.&lt;/p&gt;
&lt;p&gt;The release of these open-source models kicks off the next wave of AI innovation from enthusiasts and developers looking to add reasoning to their AI-accelerated Windows applications.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Each week, the &lt;/em&gt;&lt;em&gt;RTX AI Garage&lt;/em&gt; &lt;em&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/em&gt;&lt;em&gt;AI agents&lt;/em&gt;&lt;em&gt;, creative workflows, productivity apps and more on AI PCs and workstations. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Plug in to NVIDIA AI PC on &lt;/em&gt;&lt;em&gt;Facebook&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Instagram&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;TikTok&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt; — and stay informed by subscribing to the &lt;/em&gt;&lt;em&gt;RTX AI PC newsletter&lt;/em&gt;&lt;em&gt;. Join NVIDIA’s &lt;/em&gt;&lt;em&gt;Discord server&lt;/em&gt;&lt;em&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Follow NVIDIA Workstation on &lt;/em&gt;&lt;em&gt;LinkedIn&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt;. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See &lt;/em&gt;&lt;em&gt;notice&lt;/em&gt;&lt;em&gt; regarding software product information.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In collaboration with OpenAI, NVIDIA has optimized the company’s new open-source gpt-oss models for NVIDIA GPUs, delivering smart, fast inference from the cloud to the PC. These new reasoning models enable agentic AI applications such as web search, in-depth research and many more.&lt;/p&gt;
&lt;p&gt;With the launch of gpt-oss-20b and gpt-oss-120b, OpenAI has opened cutting-edge models to millions of users. AI enthusiasts and developers can use the optimized models on NVIDIA RTX AI PCs and workstations through popular tools and frameworks like Ollama, llama.cpp and Microsoft AI Foundry Local, and expect performance of up to 256 tokens per second on the NVIDIA GeForce RTX 5090 GPU.&lt;/p&gt;
&lt;p&gt;“OpenAI showed the world what could be built on NVIDIA AI — and now they’re advancing innovation in open-source software,” said Jensen Huang, founder and CEO of NVIDIA. “The gpt-oss models let developers everywhere build on that state-of-the-art open-source foundation, strengthening U.S. technology leadership in AI — all on the world’s largest AI compute infrastructure.”&lt;/p&gt;
&lt;p&gt;The models’ release highlights NVIDIA’s AI leadership from training to inference and from cloud to AI PC.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Open for All &lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Both gpt-oss-20b and gpt-oss-120b are flexible, open-weight reasoning models with chain-of-thought capabilities and adjustable reasoning effort levels using the popular mixture-of-experts architecture. The models are designed to support features like instruction-following and tool use, and were trained on NVIDIA H100 GPUs. &lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentStart CommentHighlightPipeRest CommentHighlightRest SCXW24568032 BCX0"&gt;AI developers can learn more and get started &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;using instructions &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;from&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt; the &lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun Underlined SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;NVIDIA &lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;T&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;ech&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;nical B&lt;/span&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;log&lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun SCXW24568032 BCX0" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun CommentHighlightRest SCXW24568032 BCX0"&gt;.&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These models can support up to 131,072 context lengths, among the longest available in local inference. This means the models can reason through context problems, ideal for tasks such as web search, coding assistance, document comprehension and in-depth research.&lt;/p&gt;
&lt;p&gt;The OpenAI open models are the first MXFP4 models supported on NVIDIA RTX. MXFP4 allows for high model quality, offering fast, efficient performance while requiring fewer resources compared with other precision types.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Run the OpenAI Models on NVIDIA RTX With Ollama&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The easiest way to test these models on RTX AI PCs, on GPUs with at least 24GB of VRAM, is using the new Ollama app. Ollama is popular with AI enthusiasts and developers for its ease of integration, and the new user interface (UI) includes out-of-the-box support for OpenAI’s open-weight models. Ollama is fully optimized for RTX, making it ideal for consumers looking to experience the power of personal AI on their PC or workstation.&lt;/p&gt;
&lt;p&gt;Once installed, Ollama enables quick, easy chatting with the models. Simply select the model from the dropdown menu and send a message. Because Ollama is optimized for RTX, there are no additional configurations or commands required to ensure top performance on supported GPUs.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83414"&gt;&lt;img alt="alt" class="size-large wp-image-83414" height="893" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/rtx-ai-garage-3-steps-20b_2-1680x893.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83414"&gt;Testing OpenAI’s open models in Ollama is easy.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Ollama’s new app includes other new features, like easy support for PDF or text files within chats, multimodal support on applicable models so users can include images in their prompts, and easily customizable context lengths when working with large documents or chats.&lt;/p&gt;
&lt;p&gt;Developers can also use Ollama via command line interface or the app’s software development kit (SDK) to power their applications and workflows.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Other Ways to Use the New OpenAI Models on RTX&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Enthusiasts and developers can also try the gpt-oss models on RTX AI PCs through various other applications and frameworks, all powered by RTX, on GPUs that have at least 16GB of VRAM.&lt;/p&gt;
&lt;p&gt;NVIDIA continues to collaborate with the open-source community on both llama.cpp and the GGML tensor library to optimize performance on RTX GPUs. Recent contributions include implementing CUDA Graphs to reduce overhead and adding algorithms that reduce CPU overheads. Check out the llama.cpp GitHub repository to get started.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83402"&gt;&lt;img alt="RTX performance for OpenAI's new open models." class="size-full wp-image-83402" height="351" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/oai-perf.png" width="624" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83402"&gt;Overall performance of the gpt-oss-20b model on various RTX AI PCs.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;Windows developers can also access OpenAI’s new models via Microsoft AI Foundry Local, currently in public preview. Foundry Local is an on-device AI inferencing solution that integrates into workflows via the command line, SDK or application programming interfaces. Foundry Local uses ONNX Runtime, optimized through CUDA, with support for NVIDIA TensorRT for RTX coming soon. Getting started is easy: install Foundry Local and invoke “Foundry model run gpt-oss-20b” in a terminal.&lt;/p&gt;
&lt;p&gt;The release of these open-source models kicks off the next wave of AI innovation from enthusiasts and developers looking to add reasoning to their AI-accelerated Windows applications.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Each week, the &lt;/em&gt;&lt;em&gt;RTX AI Garage&lt;/em&gt; &lt;em&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/em&gt;&lt;em&gt;AI agents&lt;/em&gt;&lt;em&gt;, creative workflows, productivity apps and more on AI PCs and workstations. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Plug in to NVIDIA AI PC on &lt;/em&gt;&lt;em&gt;Facebook&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;Instagram&lt;/em&gt;&lt;em&gt;, &lt;/em&gt;&lt;em&gt;TikTok&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt; — and stay informed by subscribing to the &lt;/em&gt;&lt;em&gt;RTX AI PC newsletter&lt;/em&gt;&lt;em&gt;. Join NVIDIA’s &lt;/em&gt;&lt;em&gt;Discord server&lt;/em&gt;&lt;em&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Follow NVIDIA Workstation on &lt;/em&gt;&lt;em&gt;LinkedIn&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;X&lt;/em&gt;&lt;em&gt;. &lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;See &lt;/em&gt;&lt;em&gt;notice&lt;/em&gt;&lt;em&gt; regarding software product information.&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/</guid><pubDate>Tue, 05 Aug 2025 17:01:26 +0000</pubDate></item><item><title>[NEW] OpenAI offers 20 million user chats in ChatGPT lawsuit. NYT wants 120 million. (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/08/openai-offers-20-million-user-chats-in-chatgpt-lawsuit-nyt-wants-120-million/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI asks judge to drastically limit NYT access to ChatGPT logs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="470" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-640x470.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          arthobbit | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI is preparing to raise what could be its final defense to stop The New York Times from digging through a spectacularly broad range of ChatGPT logs to hunt for any copyright-infringing outputs that could become the most damning evidence in the hotly watched case.&lt;/p&gt;
&lt;p&gt;In a joint letter Thursday, both sides requested to hold a confidential settlement conference on August 7. Ars confirmed with the NYT's legal team that the conference is not about settling the case but instead was scheduled to settle one of the most disputed aspects of the case: news plaintiffs searching through millions of ChatGPT logs.&lt;/p&gt;
&lt;p&gt;That means it's possible that this week, ChatGPT users will have a much clearer understanding of whether their private chats might be accessed in the lawsuit. In the meantime, OpenAI has broken down the "highly complex" process required to make deleted chats searchable in order to block the NYT's request for broader access.&lt;/p&gt;
&lt;p&gt;Previously, OpenAI had vowed to stop what it deemed was the NYT's attempt to conduct "mass surveillance" of ChatGPT users. But ultimately, OpenAI lost its fight to keep news plaintiffs away from all ChatGPT logs.&lt;/p&gt;
&lt;p&gt;After that loss, OpenAI appears to have pivoted and is now doing everything in its power to limit the number of logs accessed in the case—short of settling—as its customers fretted over serious privacy concerns. For the most vulnerable users, the lawsuit threatened to expose ChatGPT outputs from sensitive chats that OpenAI had previously promised would be deleted.&lt;/p&gt;
&lt;p&gt;Most recently, OpenAI floated a compromise, asking the court to agree that news organizations didn't need to search all ChatGPT logs. The AI company cited the "only expert" who has so far weighed in on what could be a statistically relevant, appropriate sample size—computer science researcher Taylor Berg-Kirkpatrick. He suggested that a sample of 20 million logs would be sufficient to determine how frequently ChatGPT users may be using the chatbot to regurgitate articles and circumvent news sites' paywalls.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But the NYT and other news organizations rejected the compromise, OpenAI said in a filing yesterday. Instead, news plaintiffs have made what OpenAI said was an "extraordinary request that OpenAI produce the individual log files of 120 million ChatGPT consumer conversations."&lt;/p&gt;
&lt;p&gt;That's six times more data than Berg-Kirkpatrick recommended, OpenAI argued. Complying with the request threatens to "increase the scope of user privacy concerns" by delaying the outcome of the case "by months," OpenAI argued. If the request is granted, it would likely trouble many users by extending the amount of time that users' deleted chats will be stored and potentially making them vulnerable to a breach or leak.&lt;/p&gt;
&lt;p&gt;As negotiations potentially end this week, OpenAI's co-defendant, Microsoft, has picked its own fight with the NYT over its internal ChatGPT equivalent tool that could potentially push the NYT to settle the disputes over ChatGPT logs.&lt;/p&gt;
&lt;h2&gt;OpenAI burdened by making deleted chats searchable&lt;/h2&gt;
&lt;p&gt;According to the NYT, it's necessary to search through 120 million ChatGPT users' conversations. News plaintiffs want the opportunity to prove not just that infringing outputs may be happening frequently, but they also want to document any patterns showing spikes in infringement.&lt;/p&gt;
&lt;p&gt;As OpenAI explained, the NYT and other news plaintiffs suing "insist that they should be entitled to conduct a full-scale analysis on every single month during the relevant 23-month time period—notwithstanding the burden—so that they can evaluate how the product has changed over time."&lt;/p&gt;
&lt;p&gt;OpenAI argued that the NYT shouldn’t be allowed to search for evidence of how "the prevalence of regurgitation changed over time. That "kind of extraordinarily granular analysis is disproportionate to the issues in dispute," they claimed. However, the news plaintiffs seemingly want to make the most of the access granted to search the logs to plead their best case.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There's no telling if the judge who immediately granted the NYT such broad access, Ona Wang, will be sympathetic to OpenAI's arguments at this stage of the battle. But OpenAI has stressed that by neglecting to limit the sample size, the court will be dragging out the case, since each user's individual chat logs will take substantial time to make searchable:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Plaintiffs seek 120 million records from OpenAI’s offline storage system, which is composed of individual conversation logs. The logs are not rows in a spreadsheet; they are large, unstructured data files—meaning that they do not follow a predefined format—consisting of over 5,000 words, even for very short conversations. The logs must be decompressed before being searched and contain identifying information (e.g., addresses) and other private information (e.g., passwords) that must be scrubbed before making it available.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For OpenAI, this process is "highly complex," requiring it to retrieve each log from "the tens of billions of logs in OpenAI’s offline data storage." The company will then incur costs of storing those logs, making the NYT's request for 120 million user conversations six times as expensive as OpenAI's.&lt;/p&gt;
&lt;p&gt;"Each of these steps requires time, computational resources, and OpenAI engineers to design, debug, operate, and monitor the relevant systems," OpenAI argued, estimating that 20 million logs would take 12 weeks, while 120 million logs would take 36 weeks to decompress and de-identify.&lt;/p&gt;
&lt;p&gt;Because of this supposed burden, OpenAI has asked the court to deny the NYT's request or else proceed with searching 20 million logs until news plaintiffs can "demonstrate that their ability to prosecute their claims will be materially prejudiced absent another sample."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why NYT might agree to limit chat searches&lt;/h2&gt;
&lt;p&gt;It's unclear if the NYT will agree to limit the sample as part of this week's settlement conference. But the NYT may be motivated to settle, as the newspaper has recently strongly opposed Microsoft's requests to compel NYT reporters' privileged logs from its internal alternative to ChatGPT, a service called ChatExplorer.&lt;/p&gt;
&lt;p&gt;In its defense, NYT has argued that Microsoft's request is too broad—demanding more than 80,000 logs, including logs from journalists and NYT lawyers "who have nothing to do with this case." If that defense sounds like OpenAI's arguments over ChatGPT logs to you, don't worry, the NYT explains why the two requests for chat samples are supposedly very different.&lt;/p&gt;
&lt;p&gt;According to the NYT, its request for ChatGPT logs properly seeks "direct evidence of copyright infringement," while Microsoft "does not need" to access ChatExplorer data, which allegedly might only be used to "support its substantial non-infringing uses and fair use defenses."&lt;/p&gt;
&lt;p&gt;Since the NYT has already provided evidence that shows that its journalists use "the accused products" for "transformative purposes" in service of Microsoft's defenses—and Microsoft failed to tailor its request to certain employees or search terms—the newspaper has argued that Microsoft's request would needlessly pull in privileged logs of 58 NYT reporters and lawyers without furthering those arguments.&lt;/p&gt;
&lt;p&gt;It's possible that the NYT's defense is strong enough to give news plaintiffs leverage in the settlement that could come this week over ChatGPT logs. Recognizing that possibility could be the reason OpenAI CEO Sam Altman recently floated the idea of "AI privilege," where any chats between users and chatbots are considered confidential, VentureBeat reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI asks judge to drastically limit NYT access to ChatGPT logs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="470" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-640x470.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-1265611885-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          arthobbit | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI is preparing to raise what could be its final defense to stop The New York Times from digging through a spectacularly broad range of ChatGPT logs to hunt for any copyright-infringing outputs that could become the most damning evidence in the hotly watched case.&lt;/p&gt;
&lt;p&gt;In a joint letter Thursday, both sides requested to hold a confidential settlement conference on August 7. Ars confirmed with the NYT's legal team that the conference is not about settling the case but instead was scheduled to settle one of the most disputed aspects of the case: news plaintiffs searching through millions of ChatGPT logs.&lt;/p&gt;
&lt;p&gt;That means it's possible that this week, ChatGPT users will have a much clearer understanding of whether their private chats might be accessed in the lawsuit. In the meantime, OpenAI has broken down the "highly complex" process required to make deleted chats searchable in order to block the NYT's request for broader access.&lt;/p&gt;
&lt;p&gt;Previously, OpenAI had vowed to stop what it deemed was the NYT's attempt to conduct "mass surveillance" of ChatGPT users. But ultimately, OpenAI lost its fight to keep news plaintiffs away from all ChatGPT logs.&lt;/p&gt;
&lt;p&gt;After that loss, OpenAI appears to have pivoted and is now doing everything in its power to limit the number of logs accessed in the case—short of settling—as its customers fretted over serious privacy concerns. For the most vulnerable users, the lawsuit threatened to expose ChatGPT outputs from sensitive chats that OpenAI had previously promised would be deleted.&lt;/p&gt;
&lt;p&gt;Most recently, OpenAI floated a compromise, asking the court to agree that news organizations didn't need to search all ChatGPT logs. The AI company cited the "only expert" who has so far weighed in on what could be a statistically relevant, appropriate sample size—computer science researcher Taylor Berg-Kirkpatrick. He suggested that a sample of 20 million logs would be sufficient to determine how frequently ChatGPT users may be using the chatbot to regurgitate articles and circumvent news sites' paywalls.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But the NYT and other news organizations rejected the compromise, OpenAI said in a filing yesterday. Instead, news plaintiffs have made what OpenAI said was an "extraordinary request that OpenAI produce the individual log files of 120 million ChatGPT consumer conversations."&lt;/p&gt;
&lt;p&gt;That's six times more data than Berg-Kirkpatrick recommended, OpenAI argued. Complying with the request threatens to "increase the scope of user privacy concerns" by delaying the outcome of the case "by months," OpenAI argued. If the request is granted, it would likely trouble many users by extending the amount of time that users' deleted chats will be stored and potentially making them vulnerable to a breach or leak.&lt;/p&gt;
&lt;p&gt;As negotiations potentially end this week, OpenAI's co-defendant, Microsoft, has picked its own fight with the NYT over its internal ChatGPT equivalent tool that could potentially push the NYT to settle the disputes over ChatGPT logs.&lt;/p&gt;
&lt;h2&gt;OpenAI burdened by making deleted chats searchable&lt;/h2&gt;
&lt;p&gt;According to the NYT, it's necessary to search through 120 million ChatGPT users' conversations. News plaintiffs want the opportunity to prove not just that infringing outputs may be happening frequently, but they also want to document any patterns showing spikes in infringement.&lt;/p&gt;
&lt;p&gt;As OpenAI explained, the NYT and other news plaintiffs suing "insist that they should be entitled to conduct a full-scale analysis on every single month during the relevant 23-month time period—notwithstanding the burden—so that they can evaluate how the product has changed over time."&lt;/p&gt;
&lt;p&gt;OpenAI argued that the NYT shouldn’t be allowed to search for evidence of how "the prevalence of regurgitation changed over time. That "kind of extraordinarily granular analysis is disproportionate to the issues in dispute," they claimed. However, the news plaintiffs seemingly want to make the most of the access granted to search the logs to plead their best case.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There's no telling if the judge who immediately granted the NYT such broad access, Ona Wang, will be sympathetic to OpenAI's arguments at this stage of the battle. But OpenAI has stressed that by neglecting to limit the sample size, the court will be dragging out the case, since each user's individual chat logs will take substantial time to make searchable:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Plaintiffs seek 120 million records from OpenAI’s offline storage system, which is composed of individual conversation logs. The logs are not rows in a spreadsheet; they are large, unstructured data files—meaning that they do not follow a predefined format—consisting of over 5,000 words, even for very short conversations. The logs must be decompressed before being searched and contain identifying information (e.g., addresses) and other private information (e.g., passwords) that must be scrubbed before making it available.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For OpenAI, this process is "highly complex," requiring it to retrieve each log from "the tens of billions of logs in OpenAI’s offline data storage." The company will then incur costs of storing those logs, making the NYT's request for 120 million user conversations six times as expensive as OpenAI's.&lt;/p&gt;
&lt;p&gt;"Each of these steps requires time, computational resources, and OpenAI engineers to design, debug, operate, and monitor the relevant systems," OpenAI argued, estimating that 20 million logs would take 12 weeks, while 120 million logs would take 36 weeks to decompress and de-identify.&lt;/p&gt;
&lt;p&gt;Because of this supposed burden, OpenAI has asked the court to deny the NYT's request or else proceed with searching 20 million logs until news plaintiffs can "demonstrate that their ability to prosecute their claims will be materially prejudiced absent another sample."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Why NYT might agree to limit chat searches&lt;/h2&gt;
&lt;p&gt;It's unclear if the NYT will agree to limit the sample as part of this week's settlement conference. But the NYT may be motivated to settle, as the newspaper has recently strongly opposed Microsoft's requests to compel NYT reporters' privileged logs from its internal alternative to ChatGPT, a service called ChatExplorer.&lt;/p&gt;
&lt;p&gt;In its defense, NYT has argued that Microsoft's request is too broad—demanding more than 80,000 logs, including logs from journalists and NYT lawyers "who have nothing to do with this case." If that defense sounds like OpenAI's arguments over ChatGPT logs to you, don't worry, the NYT explains why the two requests for chat samples are supposedly very different.&lt;/p&gt;
&lt;p&gt;According to the NYT, its request for ChatGPT logs properly seeks "direct evidence of copyright infringement," while Microsoft "does not need" to access ChatExplorer data, which allegedly might only be used to "support its substantial non-infringing uses and fair use defenses."&lt;/p&gt;
&lt;p&gt;Since the NYT has already provided evidence that shows that its journalists use "the accused products" for "transformative purposes" in service of Microsoft's defenses—and Microsoft failed to tailor its request to certain employees or search terms—the newspaper has argued that Microsoft's request would needlessly pull in privileged logs of 58 NYT reporters and lawyers without furthering those arguments.&lt;/p&gt;
&lt;p&gt;It's possible that the NYT's defense is strong enough to give news plaintiffs leverage in the settlement that could come this week over ChatGPT logs. Recognizing that possibility could be the reason OpenAI CEO Sam Altman recently floated the idea of "AI privilege," where any chats between users and chatbots are considered confidential, VentureBeat reported.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/08/openai-offers-20-million-user-chats-in-chatgpt-lawsuit-nyt-wants-120-million/</guid><pubDate>Tue, 05 Aug 2025 17:55:43 +0000</pubDate></item></channel></rss>