<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 03 Dec 2025 06:35:49 +0000</lastBuildDate><item><title>New control system teaches soft robots the art of staying safe (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-csail-Contact-Aware.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-8adfd4f6-7fff-d047-727d-cc029b78b3ad"&gt;Imagine having a continuum soft robotic arm bend around a bunch of grapes or broccoli, adjusting its grip in real time as it lifts the object. Unlike traditional rigid robots that generally aim to avoid contact with the environment as much as possible and stay far away from humans for safety reasons, this arm senses subtle forces, stretching and flexing in ways that mimic more of the compliance of a human hand. Its every motion is calculated to avoid excessive force while achieving the task efficiently. In MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Laboratory for Information and Decisions Systems (LIDS) labs, these seemingly simple movements are the culmination of complex mathematics, careful engineering, and a vision for robots that can safely interact with humans and delicate objects.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots, with their deformable bodies, promise a future where machines move more seamlessly alongside people, assist in caregiving, or handle delicate items in industrial settings. Yet that very flexibility makes them difficult to control. Small bends or twists can produce unpredictable forces, raising the risk of damage or injury. This motivates the need for safe control strategies for soft robots.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Inspired by advances in safe control and formal methods for rigid robots, we aim to adapt these ideas to soft robotics — modeling their complex behavior and embracing, rather than avoiding, contact — to enable higher-performance designs (e.g., greater payload and precision) without sacrificing safety or embodied intelligence,” says lead senior author and MIT Assistant Professor Gioele Zardini, who is a principal investigator in LIDS and the Department of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society (IDSS). “This vision is shared by recent and parallel work from other groups.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Safety first&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team developed a new framework that blends nonlinear control theory (controlling systems that involve highly complex dynamics) with advanced physical modeling techniques and efficient real-time optimization to produce what they call “contact-aware safety.” At the heart of the approach are high-order control barrier functions (HOCBFs) and high-order control Lyapunov functions (HOCLFs). HOCBFs define safe operating boundaries, ensuring the robot doesn’t exert unsafe forces. HOCLFs guide the robot efficiently toward its task objectives, balancing safety with performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“Essentially, we’re teaching the robot to know its own limits when interacting with the environment while still achieving its goals,” says MIT Department of Mechanical Engineering PhD student Kiwan Wong, the lead author of a new paper describing the framework. “The approach involves some complex derivation of soft robot dynamics, contact models, and control constraints, but the specification of control objectives and safety barriers is rather straightforward for the practitioner, and the outcomes are very tangible, as you see the robot moving smoothly, reacting to contact, and never causing unsafe situations.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Compared with traditional kinematic CBFs — where forward-invariant safe sets are hard to specify — the HOCBF framework simplifies barrier design, and its optimization formulation accounts for system dynamics (e.g., inertia), ensuring the soft robot stops early enough to avoid unsafe contact forces,” says Worcester Polytechnic Institute Assistant Professor and former CSAIL postdoc Wei Xiao.&lt;/p&gt;&lt;p dir="ltr"&gt;“Since soft robots emerged, the field has highlighted their embodied intelligence and greater inherent safety relative to rigid robots, thanks to passive material and structural compliance. Yet their “cognitive” intelligence — especially safety systems — has lagged behind that of rigid serial-link manipulators,” says co-lead author Maximilian Stölzle, a research intern at Disney Research and formerly a Delft University of Technology PhD student and visiting researcher at MIT LIDS and CSAIL. “This work helps close that gap by adapting proven algorithms to soft robots and tailoring them for safe contact and soft-continuum dynamics.”&lt;/p&gt;&lt;p dir="ltr"&gt;The LIDS and CSAIL team tested the system on a series of experiments designed to challenge the robot’s safety and adaptability. In one test, the arm pressed gently against a compliant surface, maintaining a precise force without overshooting. In another, it traced the contours of a curved object, adjusting its grip to avoid slippage. In yet another demonstration, the robot manipulated fragile items alongside a human operator, reacting in real time to unexpected nudges or shifts. “These experiments show that our framework is able to generalize to diverse tasks and objectives, and the robot can sense, adapt, and act in complex scenarios while always respecting clearly defined safety limits,” says Zardini.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots with contact-aware safety could be a real value-add in high-stakes places, of course. In health care, they could assist in surgeries, providing precise manipulation while reducing risk to patients. In industry, they might handle fragile goods without constant supervision. In domestic settings, robots could help with chores or caregiving tasks, interacting safely with children or the elderly — a key step toward making soft robots reliable partners in real-world environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots have incredible potential,” says co-lead senior author Daniela Rus, director of CSAIL and a professor in the Department of Electrical Engineering and Computer Science. “But ensuring safety and encoding motion tasks via relatively simple objectives has always been a central challenge. We wanted to create a system where the robot can remain flexible and responsive while mathematically guaranteeing it won’t exceed safe force limits.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Combining soft robot models, differentiable simulation, and control theory&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Underlying the control strategy is a differentiable implementation of something called the Piecewise Cosserat-Segment (PCS) dynamics model, which predicts how a soft robot deforms and where forces accumulate. This model allows the system to anticipate how the robot’s body will respond to actuation and complex interactions with the environment. “The aspect that I most like about this work is the blend of integration of new and old tools coming from different fields like advanced soft robot models, differentiable simulation, Lyapunov theory, convex optimization, and injury-severity–based safety constraints. All of this is nicely blended into a real-time controller fully grounded in first principles,” says co-author Cosimo Della Santina, who is an associate professor at Delft University of Technology.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Complementing this is the Differentiable Conservative Separating Axis Theorem (DCSAT), which estimates distances between the soft robot and obstacles in the environment that can be approximated with a chain of convex polygons in a differentiable manner. “Earlier differentiable distance metrics for convex polygons either couldn’t compute penetration depth — essential for estimating contact forces — or yielded non-conservative estimates that could compromise safety,” says Wong. “Instead, the DCSAT metric returns strictly conservative, and therefore safe, estimates while simultaneously allowing for fast and differentiable computation.” Together, PCS and DCSAT give the robot a predictive sense of its environment for more proactive, safe interactions.&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, the team plans to extend their methods to three-dimensional soft robots and explore integration with learning-based strategies. By combining contact-aware safety with adaptive learning, soft robots could handle even more complex, unpredictable environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This is what makes our work exciting,” says Rus. “You can see the robot behaving in a human-like, careful manner, but behind that grace is a rigorous control framework ensuring it never oversteps its bounds.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots are generally safer to interact with than rigid-bodied robots by design, due to the compliance and energy-absorbing properties of their bodies,” says University of Michigan Assistant Professor Daniel Bruder, who wasn’t involved in the research. “However, as soft robots become faster, stronger, and more capable, that may no longer be enough to ensure safety. This work takes a crucial step towards ensuring soft robots can operate safely by offering a method to limit contact forces across their entire bodies.”&lt;/p&gt;&lt;p&gt;The team’s work was supported, in part, by The Hong Kong Jockey Club Scholarships, the European Union’s Horizon Europe Program, Cultuurfonds Wetenschapsbeurzen, and the Rudge (1948) and Nancy Allen Chair. Their work was published earlier this month in the Institute of Electrical and Electronics Engineers’ &lt;em&gt;Robotics and Automation Letters&lt;/em&gt;.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202511/mit-csail-Contact-Aware.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-8adfd4f6-7fff-d047-727d-cc029b78b3ad"&gt;Imagine having a continuum soft robotic arm bend around a bunch of grapes or broccoli, adjusting its grip in real time as it lifts the object. Unlike traditional rigid robots that generally aim to avoid contact with the environment as much as possible and stay far away from humans for safety reasons, this arm senses subtle forces, stretching and flexing in ways that mimic more of the compliance of a human hand. Its every motion is calculated to avoid excessive force while achieving the task efficiently. In MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and Laboratory for Information and Decisions Systems (LIDS) labs, these seemingly simple movements are the culmination of complex mathematics, careful engineering, and a vision for robots that can safely interact with humans and delicate objects.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots, with their deformable bodies, promise a future where machines move more seamlessly alongside people, assist in caregiving, or handle delicate items in industrial settings. Yet that very flexibility makes them difficult to control. Small bends or twists can produce unpredictable forces, raising the risk of damage or injury. This motivates the need for safe control strategies for soft robots.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Inspired by advances in safe control and formal methods for rigid robots, we aim to adapt these ideas to soft robotics — modeling their complex behavior and embracing, rather than avoiding, contact — to enable higher-performance designs (e.g., greater payload and precision) without sacrificing safety or embodied intelligence,” says lead senior author and MIT Assistant Professor Gioele Zardini, who is a principal investigator in LIDS and the Department of Civil and Environmental Engineering, and an affiliate faculty with the Institute for Data, Systems, and Society (IDSS). “This vision is shared by recent and parallel work from other groups.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Safety first&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;The team developed a new framework that blends nonlinear control theory (controlling systems that involve highly complex dynamics) with advanced physical modeling techniques and efficient real-time optimization to produce what they call “contact-aware safety.” At the heart of the approach are high-order control barrier functions (HOCBFs) and high-order control Lyapunov functions (HOCLFs). HOCBFs define safe operating boundaries, ensuring the robot doesn’t exert unsafe forces. HOCLFs guide the robot efficiently toward its task objectives, balancing safety with performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“Essentially, we’re teaching the robot to know its own limits when interacting with the environment while still achieving its goals,” says MIT Department of Mechanical Engineering PhD student Kiwan Wong, the lead author of a new paper describing the framework. “The approach involves some complex derivation of soft robot dynamics, contact models, and control constraints, but the specification of control objectives and safety barriers is rather straightforward for the practitioner, and the outcomes are very tangible, as you see the robot moving smoothly, reacting to contact, and never causing unsafe situations.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Compared with traditional kinematic CBFs — where forward-invariant safe sets are hard to specify — the HOCBF framework simplifies barrier design, and its optimization formulation accounts for system dynamics (e.g., inertia), ensuring the soft robot stops early enough to avoid unsafe contact forces,” says Worcester Polytechnic Institute Assistant Professor and former CSAIL postdoc Wei Xiao.&lt;/p&gt;&lt;p dir="ltr"&gt;“Since soft robots emerged, the field has highlighted their embodied intelligence and greater inherent safety relative to rigid robots, thanks to passive material and structural compliance. Yet their “cognitive” intelligence — especially safety systems — has lagged behind that of rigid serial-link manipulators,” says co-lead author Maximilian Stölzle, a research intern at Disney Research and formerly a Delft University of Technology PhD student and visiting researcher at MIT LIDS and CSAIL. “This work helps close that gap by adapting proven algorithms to soft robots and tailoring them for safe contact and soft-continuum dynamics.”&lt;/p&gt;&lt;p dir="ltr"&gt;The LIDS and CSAIL team tested the system on a series of experiments designed to challenge the robot’s safety and adaptability. In one test, the arm pressed gently against a compliant surface, maintaining a precise force without overshooting. In another, it traced the contours of a curved object, adjusting its grip to avoid slippage. In yet another demonstration, the robot manipulated fragile items alongside a human operator, reacting in real time to unexpected nudges or shifts. “These experiments show that our framework is able to generalize to diverse tasks and objectives, and the robot can sense, adapt, and act in complex scenarios while always respecting clearly defined safety limits,” says Zardini.&lt;/p&gt;&lt;p dir="ltr"&gt;Soft robots with contact-aware safety could be a real value-add in high-stakes places, of course. In health care, they could assist in surgeries, providing precise manipulation while reducing risk to patients. In industry, they might handle fragile goods without constant supervision. In domestic settings, robots could help with chores or caregiving tasks, interacting safely with children or the elderly — a key step toward making soft robots reliable partners in real-world environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots have incredible potential,” says co-lead senior author Daniela Rus, director of CSAIL and a professor in the Department of Electrical Engineering and Computer Science. “But ensuring safety and encoding motion tasks via relatively simple objectives has always been a central challenge. We wanted to create a system where the robot can remain flexible and responsive while mathematically guaranteeing it won’t exceed safe force limits.”&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Combining soft robot models, differentiable simulation, and control theory&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;Underlying the control strategy is a differentiable implementation of something called the Piecewise Cosserat-Segment (PCS) dynamics model, which predicts how a soft robot deforms and where forces accumulate. This model allows the system to anticipate how the robot’s body will respond to actuation and complex interactions with the environment. “The aspect that I most like about this work is the blend of integration of new and old tools coming from different fields like advanced soft robot models, differentiable simulation, Lyapunov theory, convex optimization, and injury-severity–based safety constraints. All of this is nicely blended into a real-time controller fully grounded in first principles,” says co-author Cosimo Della Santina, who is an associate professor at Delft University of Technology.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;Complementing this is the Differentiable Conservative Separating Axis Theorem (DCSAT), which estimates distances between the soft robot and obstacles in the environment that can be approximated with a chain of convex polygons in a differentiable manner. “Earlier differentiable distance metrics for convex polygons either couldn’t compute penetration depth — essential for estimating contact forces — or yielded non-conservative estimates that could compromise safety,” says Wong. “Instead, the DCSAT metric returns strictly conservative, and therefore safe, estimates while simultaneously allowing for fast and differentiable computation.” Together, PCS and DCSAT give the robot a predictive sense of its environment for more proactive, safe interactions.&lt;/p&gt;&lt;p dir="ltr"&gt;Looking ahead, the team plans to extend their methods to three-dimensional soft robots and explore integration with learning-based strategies. By combining contact-aware safety with adaptive learning, soft robots could handle even more complex, unpredictable environments.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;“This is what makes our work exciting,” says Rus. “You can see the robot behaving in a human-like, careful manner, but behind that grace is a rigorous control framework ensuring it never oversteps its bounds.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Soft robots are generally safer to interact with than rigid-bodied robots by design, due to the compliance and energy-absorbing properties of their bodies,” says University of Michigan Assistant Professor Daniel Bruder, who wasn’t involved in the research. “However, as soft robots become faster, stronger, and more capable, that may no longer be enough to ensure safety. This work takes a crucial step towards ensuring soft robots can operate safely by offering a method to limit contact forces across their entire bodies.”&lt;/p&gt;&lt;p&gt;The team’s work was supported, in part, by The Hong Kong Jockey Club Scholarships, the European Union’s Horizon Europe Program, Cultuurfonds Wetenschapsbeurzen, and the Rudge (1948) and Nancy Allen Chair. Their work was published earlier this month in the Institute of Electrical and Electronics Engineers’ &lt;em&gt;Robotics and Automation Letters&lt;/em&gt;.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-control-system-teaches-soft-robots-art-staying-safe-1202</guid><pubDate>Tue, 02 Dec 2025 19:00:00 +0000</pubDate></item><item><title>Android 16 adds AI notification summaries, new customization options, and more (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/android-16-adds-ai-notification-summaries-new-customization-options-and-more/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it is releasing a slew of Android 16 updates, along with new general Android and accessibility features. The rollout of the new Android 16 features, which are first coming to Pixel devices, marks a new chapter in how Android updates are delivered, as the company is moving from a single yearly update to more frequent releases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Android 16 is adding AI-powered notification summaries that condense long messages and group chats into quick, glanceable overviews. A new “Notification organizer” will automatically group and silence lower-priority notifications, such as promotions, news, and social alerts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The update also brings more ways to customize devices, with users getting access to custom icon shapes, themed icons, and the option to automatically darken light apps, even those that don’t have their own native dark theme.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, there’s a new Parental Controls option within Android Settings that allows parents to set screen time limits, create downtime schedules, control app usage, and more for their children.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3071853" height="519" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-02-at-12.51.39-PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;These updates are starting to roll out with Android 16 on eligible Pixel devices starting Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also releasing several new Android features that aren’t specific to Android 16. A new beta feature called “Call Reason” allows users to flag calls to saved contacts as “urgent.” Recipients will see this on their incoming call screen and know it’s time-sensitive. If they miss the call, the “urgent” note will stay in their call history.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also launching “Expressive Captions” that display the full emotion of speech with tags like [sad] or [joyful], whether it’s a video message or a post on social media. The company says this will allow users to glean the full context of what’s being said when the sound is off.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant is making it easier to spot and exit unwanted group chats. If an unknown number invites a user to a group, they’ll get an alert that shows key information about the group. The user can then quickly choose to reply, leave the chat, or block and report the number.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Pinned tabs in Chrome now work the same way as on desktop, which means pinned pages stay saved at the front of the browser, letting users pick up where they left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3071855" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/2.-Expressive-Captions.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also updating Circle to Search, its feature that&amp;nbsp;allows users to&amp;nbsp;search from anywhere on their phone&amp;nbsp;by using gestures like circling, highlighting, scribbling, or tapping. Users can now analyze suspicious messages with the feature — after initiating Circle to Search, an AI Overview will appear indicating whether the message is likely a scam.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For accessibility updates, Google is enhancing its “Guided Frame” feature in the Pixel camera app. Previously, the feature has notified users about things like a face in the frame. Now, it will provide a more in-depth description, such as “one girl with a yellow T-shirt sits on the sofa and looks at the dog.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, users no longer need to physically tap their phone to start using Voice Access, which allows users to control their Android devices with voice commands. Now, users just need to say “Hey Google, start Voice Access” to begin controlling their phone hands-free.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also launching Fast Pair for hearing aids, starting with hearing aids from Demant, a Danish company that owns several major hearing aid brands, including Oticon, Sonic, and Bernafon.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Tuesday that it is releasing a slew of Android 16 updates, along with new general Android and accessibility features. The rollout of the new Android 16 features, which are first coming to Pixel devices, marks a new chapter in how Android updates are delivered, as the company is moving from a single yearly update to more frequent releases.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Android 16 is adding AI-powered notification summaries that condense long messages and group chats into quick, glanceable overviews. A new “Notification organizer” will automatically group and silence lower-priority notifications, such as promotions, news, and social alerts.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The update also brings more ways to customize devices, with users getting access to custom icon shapes, themed icons, and the option to automatically darken light apps, even those that don’t have their own native dark theme.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, there’s a new Parental Controls option within Android Settings that allows parents to set screen time limits, create downtime schedules, control app usage, and more for their children.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3071853" height="519" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-02-at-12.51.39-PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;These updates are starting to roll out with Android 16 on eligible Pixel devices starting Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also releasing several new Android features that aren’t specific to Android 16. A new beta feature called “Call Reason” allows users to flag calls to saved contacts as “urgent.” Recipients will see this on their incoming call screen and know it’s time-sensitive. If they miss the call, the “urgent” note will stay in their call history.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also launching “Expressive Captions” that display the full emotion of speech with tags like [sad] or [joyful], whether it’s a video message or a post on social media. The company says this will allow users to glean the full context of what’s being said when the sound is off.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The tech giant is making it easier to spot and exit unwanted group chats. If an unknown number invites a user to a group, they’ll get an alert that shows key information about the group. The user can then quickly choose to reply, leave the chat, or block and report the number.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In addition, Pinned tabs in Chrome now work the same way as on desktop, which means pinned pages stay saved at the front of the browser, letting users pick up where they left off.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3071855" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/2.-Expressive-Captions.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Google is also updating Circle to Search, its feature that&amp;nbsp;allows users to&amp;nbsp;search from anywhere on their phone&amp;nbsp;by using gestures like circling, highlighting, scribbling, or tapping. Users can now analyze suspicious messages with the feature — after initiating Circle to Search, an AI Overview will appear indicating whether the message is likely a scam.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For accessibility updates, Google is enhancing its “Guided Frame” feature in the Pixel camera app. Previously, the feature has notified users about things like a face in the frame. Now, it will provide a more in-depth description, such as “one girl with a yellow T-shirt sits on the sofa and looks at the dog.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, users no longer need to physically tap their phone to start using Voice Access, which allows users to control their Android devices with voice commands. Now, users just need to say “Hey Google, start Voice Access” to begin controlling their phone hands-free.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also launching Fast Pair for hearing aids, starting with hearing aids from Demant, a Danish company that owns several major hearing aid brands, including Oticon, Sonic, and Bernafon.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/android-16-adds-ai-notification-summaries-new-customization-options-and-more/</guid><pubDate>Tue, 02 Dec 2025 19:00:00 +0000</pubDate></item><item><title>Google announces second Android 16 release of 2025 is heading to Pixels (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/12/google-announces-second-android-16-release-of-2025-is-heading-to-pixels/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The update is rolling out to Pixels starting today.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Android 16 on a Pixel" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Android 16 on a Pixel" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A new Android 16 is rolling out. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is following through on its pledge to split Android versions into more frequent updates. We already had one Android 16 release this year, and now it’s time for the second. The new version is rolling out first on Google’s Pixel phones, featuring more icon customization, easier parental controls, and AI-powered notifications. Don’t be bummed if you aren’t first in line for the new Android 16—Google also has a raft of general improvements coming to the wider Android ecosystem.&lt;/p&gt;
&lt;h2&gt;Android 16, part 2&lt;/h2&gt;
&lt;p&gt;Since rolling out the first version of Android in 2008, Google has largely stuck to one major release per year. Android 16 changes things, moving from one monolithic release to two. Today’s OS update is the second part of the Android 16 era, but don’t expect major changes. As expected, the first release in June made more changes. Most of what we’ll see in the second update is geared toward Google’s Pixel phones, plus some less notable changes for developers.&lt;/p&gt;
&lt;p&gt;Google’s new AI features for notifications are probably the most important change. Android 16 will use AI for two notification tasks: summarizing and organizing. The OS will take long chat conversations and summarize the notifications with AI. Notification data is processed locally on the device and won’t be uploaded anywhere. In the notification shade, the collapsed notification line will feature a summary of the conversation rather than a snippet of one message. Expanding the notification will display the full text.&lt;/p&gt;
&lt;div style="margin-left: auto; margin-right: auto;"&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1920" id="video-2129874-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/1-Notification-summary.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;
&lt;p&gt;Google also says AI will help to reduce notification overload in Android 16, part 2. This will build on the notification grouping from the first Android 16 release by gathering lower-priority notifications and silencing them. These items will be organized into batches, like news and promotions, where they can be safely ignored until you want to take a closer look.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Material 3 Expressive came to Pixels earlier this year but not as part of the first Android 16 upgrade—Google’s relationship with Android versions is complicated these days. Regardless, Material 3 will get a bit more cohesive on Pixels following this update. Google will now apply Material theming to all icons on your device automatically, replacing legacy colored icons with theme-friendly versions. Similarly, dark mode will be supported across more apps, even if the devs haven’t added support. Google is also adding a few more icon shape options if you want to jazz up your home screen.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2129875 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Android 16 screens" class="fullwidth full" height="1920" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Android-16-2.jpg" width="2160" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;By way of functional changes, Google has added a more intuitive way of managing parental controls—you can just use the managed device directly. Parents will be able to set a PIN code for accessing features like screen time, app usage, and so on without grabbing a different device. If you want more options or control, the new on-device settings will also help you configure Google Family Link.&lt;/p&gt;
&lt;h2&gt;Android for all&lt;/h2&gt;
&lt;p&gt;No Pixel? No problem. Google has also bundled up a collection of app and system updates that will begin rolling out today for all supported Android devices.&lt;/p&gt;
&lt;p&gt;Chrome for Android is getting an update with tab pinning, mirroring a feature that has been in the desktop version since time immemorial. The Google Messages app is also taking care of some low-hanging fruit. When you’re invited to a group chat by a new number, the app will display group information and a one-tap option to leave and report the chat as spam.&lt;/p&gt;
&lt;p&gt;Google’s official dialer app comes on Pixels, but it’s also in the Play Store for anyone to download. If you and your contacts use Google Dialer, you’ll soon be able to place calls with a “reason.” You can flag a call as “Urgent” to indicate to the recipient that they shouldn’t send you to voicemail. The urgent label will also remain in the call history if they miss the call.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2129877 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="928" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/new-android-dec.jpg" width="909" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google also says it’s adding yet another AI-powered feature to stop scams. Circle to Search, which is available on most modern Android phones, lets you highlight anything you come across to check for scams. It’s unclear how accurate it will be, though. This feature plugs into AI Overviews to assess the risk and provide suggestions.&lt;/p&gt;
&lt;p&gt;Android devices are also getting a raft of new accessibility options. Those who use a mouse with AutoClick can now set custom hover times. You’ll also be able to launch TalkBack voice control in Gboard with a two-finger tap. Voice Access, which lets you control the phone UI by voice, is also getting easier to use. Rather than tapping things to launch it, you’ll be able to tell the phone’s Gemini assistant by voice to “start Voice Access.”&lt;/p&gt;
&lt;p&gt;Gemini is also making an appearance in Google’s Guided Frame camera feature, but this one is only for Pixels. Guided Frame helps those with low or no vision take photos by offering voice descriptions of what’s in the frame. With the new update, Guided Frame will use Gemini to summarize the shot’s content. This should allow for more detailed descriptions and hopefully not too many hallucinations.&lt;/p&gt;
&lt;p&gt;The widely available Android feature updates will roll out over the coming weeks. Pixel owners should begin getting update notifications for the new Android 16 build over a similar timeframe. There will also be manual update files on Google’s developer site. Non-Pixel phones will get the new Android 16 whenever OEMs get around to it, but there may not be much overlap with the features Google has announced for Pixels.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The update is rolling out to Pixels starting today.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Android 16 on a Pixel" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Android 16 on a Pixel" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Android-16-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A new Android 16 is rolling out. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google is following through on its pledge to split Android versions into more frequent updates. We already had one Android 16 release this year, and now it’s time for the second. The new version is rolling out first on Google’s Pixel phones, featuring more icon customization, easier parental controls, and AI-powered notifications. Don’t be bummed if you aren’t first in line for the new Android 16—Google also has a raft of general improvements coming to the wider Android ecosystem.&lt;/p&gt;
&lt;h2&gt;Android 16, part 2&lt;/h2&gt;
&lt;p&gt;Since rolling out the first version of Android in 2008, Google has largely stuck to one major release per year. Android 16 changes things, moving from one monolithic release to two. Today’s OS update is the second part of the Android 16 era, but don’t expect major changes. As expected, the first release in June made more changes. Most of what we’ll see in the second update is geared toward Google’s Pixel phones, plus some less notable changes for developers.&lt;/p&gt;
&lt;p&gt;Google’s new AI features for notifications are probably the most important change. Android 16 will use AI for two notification tasks: summarizing and organizing. The OS will take long chat conversations and summarize the notifications with AI. Notification data is processed locally on the device and won’t be uploaded anywhere. In the notification shade, the collapsed notification line will feature a summary of the conversation rather than a snippet of one message. Expanding the notification will display the full text.&lt;/p&gt;
&lt;div style="margin-left: auto; margin-right: auto;"&gt;
&lt;figure class="video ars-wp-video"&gt;
  &lt;div class="wrapper ars-wp-video-wrapper relative"&gt;
    &lt;video class="wp-video-shortcode absolute w-full h-full object-cover left-0 top-0" controls="controls" height="1920" id="video-2129874-1" preload="metadata" width="1080"&gt;&lt;source src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/1-Notification-summary.mp4?_=1" type="video/mp4" /&gt;&lt;/video&gt;
  &lt;/div&gt;

  &lt;figcaption&gt;
    &lt;span class="icon caption-arrow icon-drop-indicator"&gt;&lt;/span&gt;
      &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;/div&gt;
&lt;p&gt;Google also says AI will help to reduce notification overload in Android 16, part 2. This will build on the notification grouping from the first Android 16 release by gathering lower-priority notifications and silencing them. These items will be organized into batches, like news and promotions, where they can be safely ignored until you want to take a closer look.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Material 3 Expressive came to Pixels earlier this year but not as part of the first Android 16 upgrade—Google’s relationship with Android versions is complicated these days. Regardless, Material 3 will get a bit more cohesive on Pixels following this update. Google will now apply Material theming to all icons on your device automatically, replacing legacy colored icons with theme-friendly versions. Similarly, dark mode will be supported across more apps, even if the devs haven’t added support. Google is also adding a few more icon shape options if you want to jazz up your home screen.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2129875 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Android 16 screens" class="fullwidth full" height="1920" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/Android-16-2.jpg" width="2160" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;By way of functional changes, Google has added a more intuitive way of managing parental controls—you can just use the managed device directly. Parents will be able to set a PIN code for accessing features like screen time, app usage, and so on without grabbing a different device. If you want more options or control, the new on-device settings will also help you configure Google Family Link.&lt;/p&gt;
&lt;h2&gt;Android for all&lt;/h2&gt;
&lt;p&gt;No Pixel? No problem. Google has also bundled up a collection of app and system updates that will begin rolling out today for all supported Android devices.&lt;/p&gt;
&lt;p&gt;Chrome for Android is getting an update with tab pinning, mirroring a feature that has been in the desktop version since time immemorial. The Google Messages app is also taking care of some low-hanging fruit. When you’re invited to a group chat by a new number, the app will display group information and a one-tap option to leave and report the chat as spam.&lt;/p&gt;
&lt;p&gt;Google’s official dialer app comes on Pixels, but it’s also in the Play Store for anyone to download. If you and your contacts use Google Dialer, you’ll soon be able to place calls with a “reason.” You can flag a call as “Urgent” to indicate to the recipient that they shouldn’t send you to voicemail. The urgent label will also remain in the call history if they miss the call.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2129877 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="fullwidth full" height="928" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/new-android-dec.jpg" width="909" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Google also says it’s adding yet another AI-powered feature to stop scams. Circle to Search, which is available on most modern Android phones, lets you highlight anything you come across to check for scams. It’s unclear how accurate it will be, though. This feature plugs into AI Overviews to assess the risk and provide suggestions.&lt;/p&gt;
&lt;p&gt;Android devices are also getting a raft of new accessibility options. Those who use a mouse with AutoClick can now set custom hover times. You’ll also be able to launch TalkBack voice control in Gboard with a two-finger tap. Voice Access, which lets you control the phone UI by voice, is also getting easier to use. Rather than tapping things to launch it, you’ll be able to tell the phone’s Gemini assistant by voice to “start Voice Access.”&lt;/p&gt;
&lt;p&gt;Gemini is also making an appearance in Google’s Guided Frame camera feature, but this one is only for Pixels. Guided Frame helps those with low or no vision take photos by offering voice descriptions of what’s in the frame. With the new update, Guided Frame will use Gemini to summarize the shot’s content. This should allow for more detailed descriptions and hopefully not too many hallucinations.&lt;/p&gt;
&lt;p&gt;The widely available Android feature updates will roll out over the coming weeks. Pixel owners should begin getting update notifications for the new Android 16 build over a similar timeframe. There will also be manual update files on Google’s developer site. Non-Pixel phones will get the new Android 16 whenever OEMs get around to it, but there may not be much overlap with the features Google has announced for Pixels.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/12/google-announces-second-android-16-release-of-2025-is-heading-to-pixels/</guid><pubDate>Tue, 02 Dec 2025 19:11:47 +0000</pubDate></item><item><title>Amazon previews 3 AI agents, including ‘Kiro’ that can code on its own for days (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/amazon-previews-3-ai-agents-including-kiro-that-can-code-on-its-own-for-days/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-1065679054.jpg?resize=1200,849" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services on Tuesday announced three new AI agents it calls “frontier agents,” including one designed to learn how you like to work and then operate on its own for days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each of these agents handle different tasks such as writing code, security processes like code reviews, and automating DevOps tasks such as preventing incidents when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps the biggest and most interesting claim by AWS is its promise that the frontier agent called “Kiro autonomous agent” can work on its own for days at a time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kiro is a software coding agent based on AWS’s existing AI coding tool Kiro, which was announced in July. While that existing tool could be used for vibe coding (which is really just prototyping), it was intended to produce operational code, or software that would be pushed live. To make reliable code, the AI must follow a company’s software-coding specifications. Kiro does that through a concept called “spec-driven development.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Kiro codes, it has the human instruct, confirm, or correct its assumptions, thereby creating specifications. The Kiro autonomous agent watches how the team works in various tools by scanning existing code, among other training means. And then, AWS says, it can work independently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You simply assign a complex task from the backlog and it independently figures out how to get that work done,” AWS CEO Matt Garman promised when introducing the new product during his keynote at AWS re:Invent on Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It actually learns how you like to work, and it continues to deepen its understanding of your code and your products and the standards that your team follows over time,” he said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says Kiro maintains “persistent context across sessions.” In other words, it doesn’t run out of memory and forget what it was supposed to do.  It can therefore be handed tasks and work on its own for hours or days, Amazon promises, with minimal human intervention. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Garman described a task like updating a bit of critical code used by 15 bits of corporate software. Instead of assigning and verifying each update, Kiro can be assigned to fix all 15 in one prompt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To complete the automation of coding tasks, the cloud provider developed AWS Security Agent, an agent that works independently to identify security problems as code is written, tests it after the fact, and then offers suggested fixes. The DevOps Agent rounds out the trio, automatically testing the new code for performance issues, or compatibility with other software, hardware, or cloud settings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To be sure, Amazon’s agents aren’t the first to claim long work windows. For instance OpenAI said last month that GPT‑5.1-Codex-Max, its agentic coding model, is designed for long runs, too, up to 24 hours.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also not totally clear that the biggest hurdle to agentic adoption is the context window (aka the ability to work continuously without stalling out). LLMs still have hallucination and accuracy issues that turn developers into “babysitters,” they say.  So developers often want to assign short tasks and verify quickly before moving on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, before agents can become like co-workers, context windows must grow bigger. Amazon’s tech is another big step in that direction. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/02/GettyImages-1065679054.jpg?resize=1200,849" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services on Tuesday announced three new AI agents it calls “frontier agents,” including one designed to learn how you like to work and then operate on its own for days.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each of these agents handle different tasks such as writing code, security processes like code reviews, and automating DevOps tasks such as preventing incidents when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Perhaps the biggest and most interesting claim by AWS is its promise that the frontier agent called “Kiro autonomous agent” can work on its own for days at a time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kiro is a software coding agent based on AWS’s existing AI coding tool Kiro, which was announced in July. While that existing tool could be used for vibe coding (which is really just prototyping), it was intended to produce operational code, or software that would be pushed live. To make reliable code, the AI must follow a company’s software-coding specifications. Kiro does that through a concept called “spec-driven development.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Kiro codes, it has the human instruct, confirm, or correct its assumptions, thereby creating specifications. The Kiro autonomous agent watches how the team works in various tools by scanning existing code, among other training means. And then, AWS says, it can work independently.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You simply assign a complex task from the backlog and it independently figures out how to get that work done,” AWS CEO Matt Garman promised when introducing the new product during his keynote at AWS re:Invent on Tuesday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It actually learns how you like to work, and it continues to deepen its understanding of your code and your products and the standards that your team follows over time,” he said.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon says Kiro maintains “persistent context across sessions.” In other words, it doesn’t run out of memory and forget what it was supposed to do.  It can therefore be handed tasks and work on its own for hours or days, Amazon promises, with minimal human intervention. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Garman described a task like updating a bit of critical code used by 15 bits of corporate software. Instead of assigning and verifying each update, Kiro can be assigned to fix all 15 in one prompt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To complete the automation of coding tasks, the cloud provider developed AWS Security Agent, an agent that works independently to identify security problems as code is written, tests it after the fact, and then offers suggested fixes. The DevOps Agent rounds out the trio, automatically testing the new code for performance issues, or compatibility with other software, hardware, or cloud settings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To be sure, Amazon’s agents aren’t the first to claim long work windows. For instance OpenAI said last month that GPT‑5.1-Codex-Max, its agentic coding model, is designed for long runs, too, up to 24 hours.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also not totally clear that the biggest hurdle to agentic adoption is the context window (aka the ability to work continuously without stalling out). LLMs still have hallucination and accuracy issues that turn developers into “babysitters,” they say.  So developers often want to assign short tasks and verify quickly before moving on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, before agents can become like co-workers, context windows must grow bigger. Amazon’s tech is another big step in that direction. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/amazon-previews-3-ai-agents-including-kiro-that-can-code-on-its-own-for-days/</guid><pubDate>Tue, 02 Dec 2025 22:18:01 +0000</pubDate></item><item><title>Google tests merging AI Overviews with AI Mode (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/google-tests-merging-ai-overviews-with-ai-mode/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/google-ai-mode-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As OpenAI goes into “Code Red” over competitive pressures, Google announced it has begun testing a new feature that merges its AI Overviews with AI Mode in Search. That means that users who are provided with the now common AI-generated snapshot of key information on a topic or question above their search results can choose to go deeper by asking follow-up questions in a conversational interface.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google calls this conversational feature AI Mode. It launched to U.S. users this May, and to global users this August, allowing for back-and-forth chats with Google’s Gemini AI, in an experience similar to ChatGPT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, accessing the experience so far has required you to think ahead about what type of question you were preparing to search for. If it were a more traditional search query, or one where you could expect to get a quick answer, you’d likely stick with typing into the search box as usual.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if you expected to ask more questions or explore a topic in more detail, you’d have to click over to the AI Mode tab to start chatting with the AI instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google now wants to test whether or not it makes sense to differentiate the two experiences. After all, the process of information seeking can often lead to a desire to learn more. You may have thought you were starting a simple query, only to find yourself delving deeper into the topic.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;(1/2) Today we’re starting to test a new way to seamlessly go deeper in AI Mode directly from the Search results page on mobile, globally.&lt;/p&gt;&lt;p&gt;This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you… pic.twitter.com/mcCS7oT2FI&lt;/p&gt;— Robby Stein (@rmstein) December 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new test, announced on Monday, Google says users will be able to “seamlessly go deeper” in AI Mode directly from the Search results page. While the test is rolling out to users globally, it’s only available on mobile devices for the time being. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout comes alongside a push inside Google’s AI rival, OpenAI, which is now delaying other products to focus on improving the chatbox experience. Thanks in part to the release of Gemini’s Nano Banana image model and other Gemini improvements, Gemini has grown to over 650 million monthly users as of November. Merging the conversational mode with AI Overviews, which has 2 billion monthly users, could give Gemini an edge in consumer adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notes VP of Product for Google Search Robby Stein, in a post on X, “You shouldn’t have to think about where or how to ask your question.” Instead, he explained, users will continue to get an AI Overview as a helpful starting point, but will then be able to ask conversational follow-up questions in AI Mode from the same screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you need,” Stein wrote. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/google-ai-mode-techcrunch.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As OpenAI goes into “Code Red” over competitive pressures, Google announced it has begun testing a new feature that merges its AI Overviews with AI Mode in Search. That means that users who are provided with the now common AI-generated snapshot of key information on a topic or question above their search results can choose to go deeper by asking follow-up questions in a conversational interface.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google calls this conversational feature AI Mode. It launched to U.S. users this May, and to global users this August, allowing for back-and-forth chats with Google’s Gemini AI, in an experience similar to ChatGPT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, accessing the experience so far has required you to think ahead about what type of question you were preparing to search for. If it were a more traditional search query, or one where you could expect to get a quick answer, you’d likely stick with typing into the search box as usual.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But if you expected to ask more questions or explore a topic in more detail, you’d have to click over to the AI Mode tab to start chatting with the AI instead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google now wants to test whether or not it makes sense to differentiate the two experiences. After all, the process of information seeking can often lead to a desire to learn more. You may have thought you were starting a simple query, only to find yourself delving deeper into the topic.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;(1/2) Today we’re starting to test a new way to seamlessly go deeper in AI Mode directly from the Search results page on mobile, globally.&lt;/p&gt;&lt;p&gt;This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you… pic.twitter.com/mcCS7oT2FI&lt;/p&gt;— Robby Stein (@rmstein) December 1, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With the new test, announced on Monday, Google says users will be able to “seamlessly go deeper” in AI Mode directly from the Search results page. While the test is rolling out to users globally, it’s only available on mobile devices for the time being. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The rollout comes alongside a push inside Google’s AI rival, OpenAI, which is now delaying other products to focus on improving the chatbox experience. Thanks in part to the release of Gemini’s Nano Banana image model and other Gemini improvements, Gemini has grown to over 650 million monthly users as of November. Merging the conversational mode with AI Overviews, which has 2 billion monthly users, could give Gemini an edge in consumer adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Notes VP of Product for Google Search Robby Stein, in a post on X, “You shouldn’t have to think about where or how to ask your question.” Instead, he explained, users will continue to get an AI Overview as a helpful starting point, but will then be able to ask conversational follow-up questions in AI Mode from the same screen.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This brings us closer to our vision for Search: just ask whatever’s on your mind – no matter how long or complex – and find exactly what you need,” Stein wrote. &lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/google-tests-merging-ai-overviews-with-ai-mode/</guid><pubDate>Tue, 02 Dec 2025 22:26:34 +0000</pubDate></item><item><title>OpenAI CEO declares “code red” as Gemini gains 200 million users in 3 months (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/12/openai-ceo-declares-code-red-as-gemini-gains-200-million-users-in-3-months/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Three years after Google sounded alarm bells over ChatGPT, the tables have turned.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anadolu via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The shoe is most certainly on the other foot. On Monday, OpenAI CEO Sam Altman reportedly declared a “code red” at the company to improve ChatGPT, delaying advertising plans and other products in the process, &amp;nbsp;The Information reported based on a leaked internal memo. The move follows Google’s release of its Gemini 3 model last month, which has outperformed ChatGPT on some industry benchmark tests and sparked high-profile praise on social media.&lt;/p&gt;
&lt;p&gt;In the memo, Altman wrote, “We are at a critical time for ChatGPT.” The company will push back work on advertising integration, AI agents for health and shopping, and a personal assistant feature called Pulse. Altman encouraged temporary team transfers and established daily calls for employees responsible for enhancing the chatbot.&lt;/p&gt;
&lt;p&gt;The directive creates an odd symmetry with events from December 2022, when Google management declared its own “code red” internal emergency after ChatGPT launched and rapidly gained in popularity. At the time, Google CEO Sundar Pichai reassigned teams across the company to develop AI prototypes and products to compete with OpenAI’s chatbot. Now, three years later, the AI industry is in a very different place.&lt;/p&gt;
&lt;p&gt;Google released Gemini 3 in mid-November, and the model quickly topped the LMArena leaderboard, a crowdsourced vibemarking site that allows users to compare two AI models and select the one with outputs that please them most. The launch has been accompanied by measured praise from some and bombastic hype from others. Salesforce CEO Marc Benioff wrote Sunday on X that he was switching to Gemini 3 after using ChatGPT daily for three years. “I’m not going back,” Benioff wrote. “The leap is insane.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to buzz about Gemini on social media, Google is quickly catching up to ChatGPT in user numbers. ChatGPT has more than 800 million weekly users, according to OpenAI, while Google’s Gemini app has grown from 450 million monthly active users in July to 650 million in October, according to Business Insider.&lt;/p&gt;
&lt;h2&gt;Financial stakes run high&lt;/h2&gt;
&lt;p&gt;Not everyone views OpenAI’s “code red” as a genuine alarm. Reuters columnist Robert Cyran wrote on Tuesday that OpenAI’s announcement added “to the impression that OpenAI is trying to do too much at once with technology that still requires a great deal of development and funding.” On the same day Altman’s memo circulated, OpenAI announced an ownership stake in a Thrive Capital venture and a collaboration with Accenture. “The only thing bigger than the company’s attention deficit is its appetite for capital,” Cyran wrote.&lt;/p&gt;
&lt;p&gt;In fact, OpenAI faces an unusual competitive disadvantage: Unlike Google, which subsidizes its AI ventures through search advertising revenue, OpenAI does not turn a profit and relies on fundraising to survive. According to The Information, the company, now valued at around $500 billion, has committed more than $1 trillion in financial obligations to cloud computing providers and chipmakers that supply the computing power needed to train and run its AI models.&lt;/p&gt;
&lt;p&gt;But the tech industry never stands still, and things can change quickly. Altman’s memo also reportedly stated that OpenAI plans to release a new simulated reasoning model next week that may beat Gemini 3 in internal evaluations. In AI, the back-and-forth cycle of one-upmanship is expected to continue as long as the dollars keep flowing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Three years after Google sounded alarm bells over ChatGPT, the tables have turned.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-640x360.jpg" width="640" /&gt;
                  &lt;img alt="OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/12/aaltman_crop-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      OpenAI CEO Sam Altman delivers a speech with video at the SK AI Summit 2025 at COEX in Seoul, South Korea on November 3, 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Anadolu via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The shoe is most certainly on the other foot. On Monday, OpenAI CEO Sam Altman reportedly declared a “code red” at the company to improve ChatGPT, delaying advertising plans and other products in the process, &amp;nbsp;The Information reported based on a leaked internal memo. The move follows Google’s release of its Gemini 3 model last month, which has outperformed ChatGPT on some industry benchmark tests and sparked high-profile praise on social media.&lt;/p&gt;
&lt;p&gt;In the memo, Altman wrote, “We are at a critical time for ChatGPT.” The company will push back work on advertising integration, AI agents for health and shopping, and a personal assistant feature called Pulse. Altman encouraged temporary team transfers and established daily calls for employees responsible for enhancing the chatbot.&lt;/p&gt;
&lt;p&gt;The directive creates an odd symmetry with events from December 2022, when Google management declared its own “code red” internal emergency after ChatGPT launched and rapidly gained in popularity. At the time, Google CEO Sundar Pichai reassigned teams across the company to develop AI prototypes and products to compete with OpenAI’s chatbot. Now, three years later, the AI industry is in a very different place.&lt;/p&gt;
&lt;p&gt;Google released Gemini 3 in mid-November, and the model quickly topped the LMArena leaderboard, a crowdsourced vibemarking site that allows users to compare two AI models and select the one with outputs that please them most. The launch has been accompanied by measured praise from some and bombastic hype from others. Salesforce CEO Marc Benioff wrote Sunday on X that he was switching to Gemini 3 after using ChatGPT daily for three years. “I’m not going back,” Benioff wrote. “The leap is insane.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to buzz about Gemini on social media, Google is quickly catching up to ChatGPT in user numbers. ChatGPT has more than 800 million weekly users, according to OpenAI, while Google’s Gemini app has grown from 450 million monthly active users in July to 650 million in October, according to Business Insider.&lt;/p&gt;
&lt;h2&gt;Financial stakes run high&lt;/h2&gt;
&lt;p&gt;Not everyone views OpenAI’s “code red” as a genuine alarm. Reuters columnist Robert Cyran wrote on Tuesday that OpenAI’s announcement added “to the impression that OpenAI is trying to do too much at once with technology that still requires a great deal of development and funding.” On the same day Altman’s memo circulated, OpenAI announced an ownership stake in a Thrive Capital venture and a collaboration with Accenture. “The only thing bigger than the company’s attention deficit is its appetite for capital,” Cyran wrote.&lt;/p&gt;
&lt;p&gt;In fact, OpenAI faces an unusual competitive disadvantage: Unlike Google, which subsidizes its AI ventures through search advertising revenue, OpenAI does not turn a profit and relies on fundraising to survive. According to The Information, the company, now valued at around $500 billion, has committed more than $1 trillion in financial obligations to cloud computing providers and chipmakers that supply the computing power needed to train and run its AI models.&lt;/p&gt;
&lt;p&gt;But the tech industry never stands still, and things can change quickly. Altman’s memo also reportedly stated that OpenAI plans to release a new simulated reasoning model next week that may beat Gemini 3 in internal evaluations. In AI, the back-and-forth cycle of one-upmanship is expected to continue as long as the dollars keep flowing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/12/openai-ceo-declares-code-red-as-gemini-gains-200-million-users-in-3-months/</guid><pubDate>Tue, 02 Dec 2025 22:42:09 +0000</pubDate></item><item><title>Amazon challenges competitors with on-premises Nvidia ‘AI Factories’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/amazon-challenges-competitors-with-on-premises-nvidia-ai-factories/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Amazon-AWS-AI-Factory.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced a new product Tuesday called “AI Factories” that allows big corporations and governments to run its AI systems in their own data centers. Or as AWS puts it: Customers supply the power and the data center, and AWS plunks in the AI system, manages it, and can tie it into other AWS cloud services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is to cater to companies and governments concerned with data sovereignty, or absolute control over their data so it can’t wind up in a competitor’s or foreign adversary’s hands. An on-prem AI Factory means not sending their data to a model maker and not even sharing the hardware.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If that product name sounds familiar, it should. That’s what Nvidia calls its hardware systems that are chock-full of tools needed to run AI, from its GPU chips to its networking tech. This AWS AI Factory is, in fact, a collaboration with Nvidia, both companies say. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, the AWS Factory will use a combination of AWS and Nvidia technology. Companies that deploy these systems can opt for Nvidia’s latest Blackwell GPUs or Amazon’s new Trainium3 chip. It uses AWS’ homegrown networking, storage, databases, and security and can tap into Amazon Bedrock — the AI model selection and management service, and AWS SageMaker AI, the model building and training tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, AWS is far from the only giant cloud provider installing Nvidia AI Factories. In October, Microsoft showed off its first of many-to-come AI Factories rolling out into its global data centers to run OpenAI workloads. Microsoft didn’t announce at the time that these extreme machines would be available for private clouds. Instead, Microsoft highlighted how it was leaning on a host of Nvidia AI Factory data center tech to build and connect its new “AI Superfactories,” aka new state-of-the-art data centers being built in Wisconsin and Georgia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Microsoft also outlined the data centers and cloud services that would be built in local countries to address the data sovereignty issue. To be fair, its options also include “Azure Local,” Microsoft’s own managed hardware that could be installed on customer sites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, it is a bit ironic that AI is causing the biggest cloud providers to invest so heavily in corporate private data centers and hybrid clouds like it’s 2009 all over again.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Amazon-AWS-AI-Factory.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon announced a new product Tuesday called “AI Factories” that allows big corporations and governments to run its AI systems in their own data centers. Or as AWS puts it: Customers supply the power and the data center, and AWS plunks in the AI system, manages it, and can tie it into other AWS cloud services.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is to cater to companies and governments concerned with data sovereignty, or absolute control over their data so it can’t wind up in a competitor’s or foreign adversary’s hands. An on-prem AI Factory means not sending their data to a model maker and not even sharing the hardware.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;If that product name sounds familiar, it should. That’s what Nvidia calls its hardware systems that are chock-full of tools needed to run AI, from its GPU chips to its networking tech. This AWS AI Factory is, in fact, a collaboration with Nvidia, both companies say. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, the AWS Factory will use a combination of AWS and Nvidia technology. Companies that deploy these systems can opt for Nvidia’s latest Blackwell GPUs or Amazon’s new Trainium3 chip. It uses AWS’ homegrown networking, storage, databases, and security and can tap into Amazon Bedrock — the AI model selection and management service, and AWS SageMaker AI, the model building and training tool.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Interestingly, AWS is far from the only giant cloud provider installing Nvidia AI Factories. In October, Microsoft showed off its first of many-to-come AI Factories rolling out into its global data centers to run OpenAI workloads. Microsoft didn’t announce at the time that these extreme machines would be available for private clouds. Instead, Microsoft highlighted how it was leaning on a host of Nvidia AI Factory data center tech to build and connect its new “AI Superfactories,” aka new state-of-the-art data centers being built in Wisconsin and Georgia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last month, Microsoft also outlined the data centers and cloud services that would be built in local countries to address the data sovereignty issue. To be fair, its options also include “Azure Local,” Microsoft’s own managed hardware that could be installed on customer sites.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, it is a bit ironic that AI is causing the biggest cloud providers to invest so heavily in corporate private data centers and hybrid clouds like it’s 2009 all over again.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/amazon-challenges-competitors-with-on-premises-nvidia-ai-factories/</guid><pubDate>Wed, 03 Dec 2025 00:43:37 +0000</pubDate></item><item><title>All the biggest news from AWS’ big tech show re:Invent 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/02/all-the-biggest-news-from-aws-big-tech-show-reinvent-2025/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2179195367.jpg?resize=1200,746" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ annual tech conference AWS re:Invent has wrapped up its first official day of programming and has already delivered an endless stream of product news. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The unsurprising theme is AI for the enterprise, although this year it’s all about upgrades that give its customers greater control to customize AI agents — including one that AWS claims can learn from you and then work independently for days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the announcements that got our attention. TechCrunch will continue to update this article through the end of AWS re:Invent, so be sure to check back.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-training-chip-and-nvidia-compatibility"&gt;An AI training chip and Nvidia compatibility&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-expanded-agentcore-capabilities"&gt;Expanded AgentCore capabilities&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-nonstop-ai-agent-worker-bee"&gt;A nonstop AI agent worker bee&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such&amp;nbsp;as preventing incidents&amp;nbsp;when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-new-nova-models-and-services"&gt;New Nova models and services&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lyft-s-argument-for-ai-agents"&gt;Lyft’s argument for AI agents&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-factory-for-the-private-data-center"&gt;An AI Factory for the private data center&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-2179195367.jpg?resize=1200,746" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services’ annual tech conference AWS re:Invent has wrapped up its first official day of programming and has already delivered an endless stream of product news. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The unsurprising theme is AI for the enterprise, although this year it’s all about upgrades that give its customers greater control to customize AI agents — including one that AWS claims can learn from you and then work independently for days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AWS re:Invent 2025, which runs through December 5, started with a keynote from AWS CEO Matt Garman, who leaned into the idea that AI agents can unlock the “true value” of AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI assistants are starting to give way to AI agents that can perform tasks and automate on your behalf,” he said during the December 2 keynote. “This is where we’re starting to see material business returns from your AI investments.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While AI agent news promises to be a persistent presence throughout AWS re:Invent 2025, there were other announcements, too. Here is a roundup of the announcements that got our attention. TechCrunch will continue to update this article through the end of AWS re:Invent, so be sure to check back.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-training-chip-and-nvidia-compatibility"&gt;An AI training chip and Nvidia compatibility&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS introduced a new version of its AI training chip called Trainium3 along with an AI system called UltraServer that runs it. The TL;DR: This upgraded chip comes with some impressive specs, including a promise of up to 4x performance gains for both AI training and inference while lowering energy use by 40%.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also provided a teaser. The company already has Trainium4 in development, which will be able to work with Nvidia’s chips.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 class="wp-block-heading" id="h-expanded-agentcore-capabilities"&gt;Expanded AgentCore capabilities&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS announced new features in its AgentCore AI agent building platform. One feature of note is Policy in AgentCore, which gives developers the ability to more easily set boundaries for AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also announced that agents will now be able to log and remember things about their users. Plus it announced that it will help its customers evaluate agents through 13 prebuilt evaluation systems.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-a-nonstop-ai-agent-worker-bee"&gt;A nonstop AI agent worker bee&lt;/h2&gt;

&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AWS announced three new AI agents (there is that term again) called “Frontier agents,” including one called “Kiro autonomous agent” that writes code and is designed to learn how a team likes to work so it can operate largely on its own for hours or days.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Another of these new agents handles security processes like code reviews, and the third does DevOps tasks such&amp;nbsp;as preventing incidents&amp;nbsp;when pushing new code live. Preview versions of the agents are available now.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-new-nova-models-and-services"&gt;New Nova models and services&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;AWS is rolling out four new AI models within its Nova AI model family — three of which are text generating and one that can create text and images.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company also announced a new service called Nova Forge that allows AWS cloud customers to access pre-trained, mid-trained, or post-trained models that they can then top off by training on their own proprietary data. AWS’s big pitch is flexibility and customization.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lyft-s-argument-for-ai-agents"&gt;Lyft’s argument for AI agents&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The ride-hailing company was among many AWS customers that piped up during the event to share their success stories and evidence of how products affected their business. Lyft is using Anthropic’s Claude model via Amazon Bedrock to create an AI agent that handles driver and rider questions and issues. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company said this AI agent has reduced average resolution time by 87%. Lyft also said it has seen a 70% increase in driver usage of the AI agent this year. &lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-an-ai-factory-for-the-private-data-center"&gt;An AI Factory for the private data center&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon also announced “AI Factories” that allow big corporations and governments to run AWS AI systems in their own data centers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The system was designed in partnership with Nvidia and includes both Nvidia’s tech and AWS’s. While companies that use it can stock it with Nvidia GPUs, they can also opt for Amazon’s newest homegrown AI chip, the Trainium3. The system is Amazon’s way of addressing data sovereignty, or the need of governments and many companies to control their data and not share it, even to use AI.&lt;/p&gt;



[embedded content]

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Check out the latest reveals on everything from agentic AI and cloud infrastructure to security and much more from the flagship Amazon Web Services event in Las Vegas. This video is brought to you in partnership with AWS.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/02/all-the-biggest-news-from-aws-big-tech-show-reinvent-2025/</guid><pubDate>Wed, 03 Dec 2025 01:06:25 +0000</pubDate></item></channel></rss>