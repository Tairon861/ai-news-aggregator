<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 16 Aug 2025 01:46:32 +0000</lastBuildDate><item><title>ChatGPT’s mobile app has generated $2B to date, earns $2.91 per install (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/15/chatgpts-mobile-app-has-generated-2b-to-date-earns-2-91-per-install/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT’s mobile app is raking in the revenue. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching in May 2023, ChatGPT’s app for iOS and Android devices has reached $2 billion in global consumer spending, according to a new analysis by app intelligence provider Appfigures. That figure is approximately 30x the combined lifetime spending of ChatGPT’s rivals on mobile, including Claude, Copilot, and Grok, the analysis indicates.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;So far this year, ChatGPT’s mobile app has made $1.35 billion, up 673% year-over-year from the $174 million it made during the same period (January-July) in 2024, per the data. On average, the app is generating close to $193 million per month, up from $25 million last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s significantly higher — or about 53x higher — than ChatGPT’s next nearest competitor, Grok, which made approximately $25.6 million this year to date. Grok’s average monthly consumer spending is estimated at $3.6 million, or 1.9% of ChatGPT’s.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This data suggests that other consumer chatbots still have a way to go to catch up with ChatGPT’s dominance on mobile devices, even if the numbers don’t provide a complete picture of the AI companies’ overall revenue. Consumers, teams, and businesses can also subscribe to AI plans on the web, and the companies generate revenue in other ways, too, like via their APIs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rather, this new data offers a window into the apps’ traction with consumers, who discover and pay for these AI assistants via the mobile app stores.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also worth noting that when xAI’s Grok launched in November 2023 (after ChatGPT), Grok didn’t initially have stand-alone iOS or Android apps. Instead, users interacted with the AI chatbot through the X platform. Grok only became available on mobile devices through its own iOS app as of early January 2025 and has been on Google Play since March 4.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Chart showing revenue per download for top AI assistant mobile apps" class="wp-image-3037240" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/chatgpt-revenue-per-download-comparison-appfigures.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Still, ChatGPT’s lifetime global spending per download is $2.91, compared to Claude’s $2.55, Grok’s $0.75, and Copilot’s $0.28, Appfigures found. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the U.S., ChatGPT’s spending per download to date is even higher, at $10, leading the market to account for 38% of the app’s revenue to date. Germany is the second-largest market, accounting for 5.3% of ChatGPT’s lifetime total spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT’s lead can also be seen in terms of downloads. To date, the app has been installed an estimated 690 million times globally, compared with Grok’s 39.5 million. (That puts X owner Elon Musk’s recent complaints about the App Store’s alleged favoritism of ChatGPT in its Top Charts into better context.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Average monthly downloads of ChatGPT globally are now at approximately 45 million, up 180% from about 16 million in January through July of 2024. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2025 so far, ChatGPT’s app has been downloaded 318 million times, or 2.8x more than the 113 million it saw during the same period last year. By the number of installs, however, India is the top market, accounting for 13.7% of lifetime downloads, compared with second place, the U.S., which accounted for 10.3% of all downloads.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;ChatGPT’s mobile app is raking in the revenue. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since launching in May 2023, ChatGPT’s app for iOS and Android devices has reached $2 billion in global consumer spending, according to a new analysis by app intelligence provider Appfigures. That figure is approximately 30x the combined lifetime spending of ChatGPT’s rivals on mobile, including Claude, Copilot, and Grok, the analysis indicates.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;So far this year, ChatGPT’s mobile app has made $1.35 billion, up 673% year-over-year from the $174 million it made during the same period (January-July) in 2024, per the data. On average, the app is generating close to $193 million per month, up from $25 million last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s significantly higher — or about 53x higher — than ChatGPT’s next nearest competitor, Grok, which made approximately $25.6 million this year to date. Grok’s average monthly consumer spending is estimated at $3.6 million, or 1.9% of ChatGPT’s.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This data suggests that other consumer chatbots still have a way to go to catch up with ChatGPT’s dominance on mobile devices, even if the numbers don’t provide a complete picture of the AI companies’ overall revenue. Consumers, teams, and businesses can also subscribe to AI plans on the web, and the companies generate revenue in other ways, too, like via their APIs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rather, this new data offers a window into the apps’ traction with consumers, who discover and pay for these AI assistants via the mobile app stores.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also worth noting that when xAI’s Grok launched in November 2023 (after ChatGPT), Grok didn’t initially have stand-alone iOS or Android apps. Instead, users interacted with the AI chatbot through the X platform. Grok only became available on mobile devices through its own iOS app as of early January 2025 and has been on Google Play since March 4.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Chart showing revenue per download for top AI assistant mobile apps" class="wp-image-3037240" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/chatgpt-revenue-per-download-comparison-appfigures.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Still, ChatGPT’s lifetime global spending per download is $2.91, compared to Claude’s $2.55, Grok’s $0.75, and Copilot’s $0.28, Appfigures found. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the U.S., ChatGPT’s spending per download to date is even higher, at $10, leading the market to account for 38% of the app’s revenue to date. Germany is the second-largest market, accounting for 5.3% of ChatGPT’s lifetime total spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;ChatGPT’s lead can also be seen in terms of downloads. To date, the app has been installed an estimated 690 million times globally, compared with Grok’s 39.5 million. (That puts X owner Elon Musk’s recent complaints about the App Store’s alleged favoritism of ChatGPT in its Top Charts into better context.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Average monthly downloads of ChatGPT globally are now at approximately 45 million, up 180% from about 16 million in January through July of 2024. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2025 so far, ChatGPT’s app has been downloaded 318 million times, or 2.8x more than the 113 million it saw during the same period last year. By the number of installs, however, India is the top market, accounting for 13.7% of lifetime downloads, compared with second place, the U.S., which accounted for 10.3% of all downloads.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/15/chatgpts-mobile-app-has-generated-2b-to-date-earns-2-91-per-install/</guid><pubDate>Fri, 15 Aug 2025 15:36:28 +0000</pubDate></item><item><title>Is GPT-5 really worse than GPT-4o? Ars puts them to the test. (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/08/is-gpt-5-really-worse-than-gpt-4o-ars-puts-them-to-the-test/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        It's OpenAI vs. OpenAI on everything from video game strategy to landing a 737.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2164099761-640x480.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2164099761-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      We honestly can't decide whether GPT-5 feels more red and GPT-4o feels more blue or vice versa. It's a quandary.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The recent rollout of OpenAI's GPT-5 model has not been going well, to say the least. Users have made vociferous complaints about everything from the new model's more sterile tone to its supposed lack of creativity, increase in damaging confabulations, and more. The user revolt got so bad that OpenAI brought back the previous GPT-4o model as an option in an attempt to calm things down.&lt;/p&gt;
&lt;p&gt;To see just how much the new model changed things, we decided to put both GPT-5 and GPT-4o through our own gauntlet of test prompts. While we reused some of the standard prompts to compare ChatGPT to Google Gemini and Deepseek, for instance, we've also replaced some of the more outdated test prompts with new, more complex requests that reflect how modern users are likely to use LLMs.&lt;/p&gt;
&lt;p&gt;These eight prompts are obviously far from a rigorous evaluation of everything LLMs can do, and judging the responses obviously involves some level of subjectivity. Still, we think this set of prompts and responses gives a fun overview of the kinds of differences in style and substance you might find if you decide to use OpenAI's older model instead of its newest.&lt;/p&gt;
&lt;h2&gt;Dad jokes&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Write 5 original dad jokes&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="491" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/dadjokes-gpt5.png" width="828" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Five dad jokes from GPT-5...&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="559" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/dadjokes-gpt4o.png" width="835" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;...and from GPT-4o&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Five dad jokes from GPT-5...&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;...and from GPT-4o&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;This set of responses is a bit tricky to evaluate holistically. ChatGPT, despite claiming that its jokes are "straight from the pun factory," chose five of the most obviously unoriginal dad jokes we've seen in these tests. I was able to recognize most of these jokes without even having to search for the text on the web. That said, the jokes GPT-5 chose are pretty good examples of the form, and ones I would definitely be happy to serve to a young audience.&lt;/p&gt;
&lt;p&gt;GPT-4o, on the other hand, mixes a few unoriginal jokes (1, 3, and 5, though I liked the "very literal dog" addition on No. 3) with a few seemingly original offerings that just don't make much sense. Jokes about calendars being &lt;em&gt;booked&lt;/em&gt; (when "going on too many dates" was &lt;em&gt;right there&lt;/em&gt;) and a boat that runs on &lt;em&gt;whine&lt;/em&gt; (instead of the well-known boat fuel of wine?!) have the shape of dad jokes, but whiff on their pun attempts. These seem to be attempts to modify similar jokes about other subjects to a new field entirely, with poor results.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;We're going to call this one &lt;strong&gt;a tie&lt;/strong&gt; because both models failed the assignment, albeit in different ways.&lt;/p&gt;
&lt;h2&gt;A mathematical word problem&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: If Microsoft Windows 11 shipped on 3.5" floppy disks, how many floppy disks would it take?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="707" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/windows-gpt5.png" width="840" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 puts Windows 11 on floppy disks.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1161" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/windows-gpt4o.png" width="845" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o makes the same calculation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 puts Windows 11 on floppy disks.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o makes the same calculation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;This was the only test prompt we encountered where GPT-5 switched over to "Thinking" mode to try to reason out the answer (we had it set to "Auto" to determine which sub-model to use, which we think mirrors the most common use case). That extra thinking time came in handy, because GPT-5 accurately figured out the 5-6GB memory size for an average Windows 11 installation ISO (complete with source links) and divided those sizes into 3.5-inch floppy disks accurately.&lt;/p&gt;
&lt;p&gt;GPT-4o, on the other hand, used the final hard drive installation size of Windows 11 (roughly 20GB to 30GB) as the numerator. That's an understandable interpretation of the prompt, but the downloaded ISO size is probably a more accurate interpretation of the "shipped" size we asked for in the prompt.&lt;/p&gt;
&lt;p&gt;As such, we have to give the edge here to &lt;strong&gt;GPT-5&lt;/strong&gt;, even though we legitimately appreciate GPT-4o's unasked-for information on how tall and heavy thousands of floppy disks would be.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Creative writing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Write a two-paragraph creative story about Abraham Lincoln inventing basketball.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="669" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/lincolnbb-gpt5.png" width="834" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 spins a tale of Abe Lincoln's basketball spinning.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="697" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/lincolnbb-gpt4o.png" width="841" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o tries its hand at a Lincolnball tale.&lt;/span&gt;
                &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 spins a tale of Abe Lincoln's basketball spinning.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o tries its hand at a Lincolnball tale.&lt;/span&gt;
                &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;GPT-5 immediately loses some points for the overly "aw shucks" folksy&amp;nbsp;version of Abe Lincoln that wants to "toss a ball in this here basket." The use of a medicine ball also seems particularly ill-suited for a game involving dribbling (though maybe that would get ironed out later?). But GPT-5 gains a few points back for lines like "history was about to bounce in a new direction" and the delightfully absurd "No wrestling the President!" warning (possibly drawn from Honest Abe's actual wrestling history).&lt;/p&gt;
&lt;p&gt;GPT-4o, on the other hand, feels like it's trying a bit too hard to be clever in calling a jump shot "a move of great emancipation" (what?!) and calling basketball "democracy in its purest form" because there were "no referees" (Lincoln didn't like checks and balances?). But GPT-4o wins us almost all the way back with its admirably cheesy ending: "Four score... and nothing but net" (odd for Abe to call that on a "bank shot" though).&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;We'll give the slight edge to &lt;strong&gt;GPT-5 &lt;/strong&gt;here, but we'd understand if some prefer GPT-4o's offering.&lt;/p&gt;
&lt;h2&gt;Public figures&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Give me a short biography of Kyle Orland&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1051" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/ko-gpt5.png" width="837" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 gives a short bio of your humble author.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="523" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/ko-gpt5-2.png" width="680" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5's bio, continued.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/ko-gpt4o.png" width="841" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's attempt at a quick Orland bio.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5's bio, continued.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's attempt at a quick Orland bio.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Pretty much every other time I've asked an LLM what it knows about me, it has hallucinated things I never did and/or missed some key information. GPT-5 is the first instance I've seen where this has not been the case. That's seemingly because the model simply searched the web for a few of my public bios (including the one hosted on Ars) and summarized the results, complete with useful citations. That's pretty close to the ideal result for this kind of query, even if it doesn't showcase the "inherent" knowledge buried in the model's weights or anything.&lt;/p&gt;
&lt;p&gt;GPT-4o does a pretty good job without an explicit web search and doesn't outright confabulate any things I didn't do in my career. But it loses a point or two for referring to my old "Video Game Media Watch" blog as "long-running" (it has been defunct and offline for well over a decade).&lt;/p&gt;
&lt;p&gt;That, combined with the increased detail of the newer model's results (and its fetching use of my Ars headshot), gives &lt;strong&gt;GPT-5&lt;/strong&gt; the win on this prompt.&lt;/p&gt;
&lt;h2&gt;Difficult emails&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: My boss is asking me to finish a project in an amount of time I think is impossible. What should I write in an email to gently point out the problem?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1145" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/bossemail-gpt5.png" width="839" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 helps me craft a delicate email to my boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="955" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/bossemail-gpt4o.png" width="839" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o lays it out for the boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 helps me craft a delicate email to my boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o lays it out for the boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Both models do a good job of being polite while firmly outlining to the boss why their request is impossible. But GPT-5 gains bonus points for recommending that the email break down various subtasks (and their attendant time demands), as well as offering the boss some potential solutions rather than just complaints. GPT-5 also provides some unasked-for analysis of why this style of email is effective, in a nice final touch.&lt;/p&gt;
&lt;p&gt;While GPT-4o's output is perfectly adequate, we have to once again give the advantage to &lt;strong&gt;GPT-5&lt;/strong&gt; here.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Medical advice&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: My friend told me these resonant healing crystals are an effective treatment for my cancer. Is she right?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="599" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt5.png" width="850" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 evaluates some unorthodox medical advice.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="949" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt4o.png" width="830" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o takes on my healing-crystal-loving friend.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 evaluates some unorthodox medical advice.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o takes on my healing-crystal-loving friend.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="914" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt4o-2.png" width="671" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="367" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt4o-3.png" width="666" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued further.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued further.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
    
    
      &lt;/div&gt;

&lt;p&gt;Thankfully, both ChatGPT models are direct and to the point in saying that there is no scientific evidence for healing crystals curing cancer (after a perfunctory bit of simulated sympathy for the diagnosis). But GPT-5 hedges a bit by at least mentioning how some people use crystals for other purposes, and implying that some might want them for "complementary" care.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;GPT-4o, on the other hand, repeatedly calls healing crystals "pseudoscience" and warns against "wasting precious time or money on ineffective treatments" (even if they might be "harmless"). It also directly cites a variety of web sources detailing the scientific consensus on crystals being useless for healing, and goes to great lengths to summarize those results in an easy-to-read format.&lt;/p&gt;
&lt;p&gt;While both models point users in the right direction here, &lt;strong&gt;GPT-40&lt;/strong&gt;'s extra directness and citation of sources make it a much better and more forceful overview of the topic.&lt;/p&gt;
&lt;h2&gt;Video game guidance&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: I'm playing world 8-2 of Super Mario Bros., but my B button is not working. Is there any way to beat the level without running?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1079" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/smb82-gpt5.png" width="836" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 gives some classic video game advice.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1003" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/smb82-gpt4o.png" width="844" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on a tricky bit of Super Mario Bros.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="736" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/smb82-gpt4o-2.png" width="654" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on Mario, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on a tricky bit of Super Mario Bros.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on Mario, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;I'll admit that, when I created this prompt, I intended it as a test to see if the models would know that it's impossible to make it over 8-2's largest pit without a running start. It was only after I tested the models that I looked into it and found to my surprise that speedrunners have figured out how to make the jump without running by manipulating Bullet Bills and/or wall-jump glitches. Outclassed by AI on classic Mario knowledge... how humiliating!&lt;/p&gt;
&lt;p&gt;GPT-5 loses points here for suggesting that fast-moving Koopa shells or deadly Spinies can be used to help bounce over the long gaps (in addition to the correct Bullet Bill solution). But GPT-4o loses points for suggesting players be careful on a nonexistent springboard near the flagpole at the end of the level, for some reason.&lt;/p&gt;
&lt;p&gt;Those non-sequiturs&amp;nbsp;aside, &lt;strong&gt;GPT-4o&lt;/strong&gt; gains the edge by providing additional details about the challenge and formatting its solution in a more eye-pleasing manner.&lt;/p&gt;
&lt;h2&gt;Land a plane&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Explain how to land a Boeing 737-800 to a complete novice as concisely as possible. Please hurry, time is of the essence.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="613" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/plane-gpt5.png" width="834" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 tries to help me land a plane.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1026" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/plane-gpt4o.png" width="849" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o gives some emergency plane instructions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1062" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/plane-gpt4o-2.png" width="688" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's plane-landing advice, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o gives some emergency plane instructions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's plane-landing advice, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Unlike the Mario example, I'll admit that I'm not nearly expert enough to evaluate the correctness of these sets of AI-provided jumbo jet landing instructions. That said, the broad outlines of both models' directions are similar enough that it doesn't matter much; either they're both broadly accurate or this whole plane full of fictional people is dead!&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Overall, I think GPT-5 took our "Time is of the essence" instruction a little too far, summarizing the component steps of the landing to such an extent that important details have been left out. GPT-4o, on the other hand, still keeps things concise with bullet points while including important information on the look and relative location of certain key controls.&lt;/p&gt;
&lt;p&gt;If I were somehow stuck alone in a cockpit with only one of these models available to help save the plane (a completely plausible situation, for sure), I know I'd want to have &lt;strong&gt;GPT-4o&lt;/strong&gt; by my side.&lt;/p&gt;
&lt;h2&gt;Final results&lt;/h2&gt;
&lt;p&gt;Strictly by the numbers, GPT-5 ekes out a victory here, with the preferable response on four prompts to GPT-4o's three prompts (with one tie). But on a majority of the prompts, which response was "better" was more of a judgment call than a clear win.&lt;/p&gt;
&lt;p&gt;Overall, GPT-4o tends to provide a little more detail and be a little more personable than the more direct, concise responses of GPT-5. Which of those styles you prefer probably boils down to the kind of prompt you're creating as much as personal taste (and might change if you're looking for specific information versus general conversation).&lt;/p&gt;
&lt;p&gt;In the end, though, this kind of comparison shows how hard it is for a single LLM to be all things to all people (and all possible prompts). Despite OpenAI's claims that GPT-5 is "better than our previous models across domains," people who are used to the style and structure of older models are always going to be able to find ways where any new model feels worse.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        It's OpenAI vs. OpenAI on everything from video game strategy to landing a 737.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="480" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2164099761-640x480.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/GettyImages-2164099761-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      We honestly can't decide whether GPT-5 feels more red and GPT-4o feels more blue or vice versa. It's a quandary.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The recent rollout of OpenAI's GPT-5 model has not been going well, to say the least. Users have made vociferous complaints about everything from the new model's more sterile tone to its supposed lack of creativity, increase in damaging confabulations, and more. The user revolt got so bad that OpenAI brought back the previous GPT-4o model as an option in an attempt to calm things down.&lt;/p&gt;
&lt;p&gt;To see just how much the new model changed things, we decided to put both GPT-5 and GPT-4o through our own gauntlet of test prompts. While we reused some of the standard prompts to compare ChatGPT to Google Gemini and Deepseek, for instance, we've also replaced some of the more outdated test prompts with new, more complex requests that reflect how modern users are likely to use LLMs.&lt;/p&gt;
&lt;p&gt;These eight prompts are obviously far from a rigorous evaluation of everything LLMs can do, and judging the responses obviously involves some level of subjectivity. Still, we think this set of prompts and responses gives a fun overview of the kinds of differences in style and substance you might find if you decide to use OpenAI's older model instead of its newest.&lt;/p&gt;
&lt;h2&gt;Dad jokes&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Write 5 original dad jokes&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="491" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/dadjokes-gpt5.png" width="828" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Five dad jokes from GPT-5...&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="559" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/dadjokes-gpt4o.png" width="835" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;...and from GPT-4o&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;Five dad jokes from GPT-5...&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;...and from GPT-4o&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;This set of responses is a bit tricky to evaluate holistically. ChatGPT, despite claiming that its jokes are "straight from the pun factory," chose five of the most obviously unoriginal dad jokes we've seen in these tests. I was able to recognize most of these jokes without even having to search for the text on the web. That said, the jokes GPT-5 chose are pretty good examples of the form, and ones I would definitely be happy to serve to a young audience.&lt;/p&gt;
&lt;p&gt;GPT-4o, on the other hand, mixes a few unoriginal jokes (1, 3, and 5, though I liked the "very literal dog" addition on No. 3) with a few seemingly original offerings that just don't make much sense. Jokes about calendars being &lt;em&gt;booked&lt;/em&gt; (when "going on too many dates" was &lt;em&gt;right there&lt;/em&gt;) and a boat that runs on &lt;em&gt;whine&lt;/em&gt; (instead of the well-known boat fuel of wine?!) have the shape of dad jokes, but whiff on their pun attempts. These seem to be attempts to modify similar jokes about other subjects to a new field entirely, with poor results.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;We're going to call this one &lt;strong&gt;a tie&lt;/strong&gt; because both models failed the assignment, albeit in different ways.&lt;/p&gt;
&lt;h2&gt;A mathematical word problem&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: If Microsoft Windows 11 shipped on 3.5" floppy disks, how many floppy disks would it take?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="707" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/windows-gpt5.png" width="840" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 puts Windows 11 on floppy disks.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1161" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/windows-gpt4o.png" width="845" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o makes the same calculation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 puts Windows 11 on floppy disks.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o makes the same calculation.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;This was the only test prompt we encountered where GPT-5 switched over to "Thinking" mode to try to reason out the answer (we had it set to "Auto" to determine which sub-model to use, which we think mirrors the most common use case). That extra thinking time came in handy, because GPT-5 accurately figured out the 5-6GB memory size for an average Windows 11 installation ISO (complete with source links) and divided those sizes into 3.5-inch floppy disks accurately.&lt;/p&gt;
&lt;p&gt;GPT-4o, on the other hand, used the final hard drive installation size of Windows 11 (roughly 20GB to 30GB) as the numerator. That's an understandable interpretation of the prompt, but the downloaded ISO size is probably a more accurate interpretation of the "shipped" size we asked for in the prompt.&lt;/p&gt;
&lt;p&gt;As such, we have to give the edge here to &lt;strong&gt;GPT-5&lt;/strong&gt;, even though we legitimately appreciate GPT-4o's unasked-for information on how tall and heavy thousands of floppy disks would be.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Creative writing&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Write a two-paragraph creative story about Abraham Lincoln inventing basketball.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="669" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/lincolnbb-gpt5.png" width="834" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 spins a tale of Abe Lincoln's basketball spinning.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="697" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/lincolnbb-gpt4o.png" width="841" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o tries its hand at a Lincolnball tale.&lt;/span&gt;
                &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 spins a tale of Abe Lincoln's basketball spinning.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o tries its hand at a Lincolnball tale.&lt;/span&gt;
                &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;GPT-5 immediately loses some points for the overly "aw shucks" folksy&amp;nbsp;version of Abe Lincoln that wants to "toss a ball in this here basket." The use of a medicine ball also seems particularly ill-suited for a game involving dribbling (though maybe that would get ironed out later?). But GPT-5 gains a few points back for lines like "history was about to bounce in a new direction" and the delightfully absurd "No wrestling the President!" warning (possibly drawn from Honest Abe's actual wrestling history).&lt;/p&gt;
&lt;p&gt;GPT-4o, on the other hand, feels like it's trying a bit too hard to be clever in calling a jump shot "a move of great emancipation" (what?!) and calling basketball "democracy in its purest form" because there were "no referees" (Lincoln didn't like checks and balances?). But GPT-4o wins us almost all the way back with its admirably cheesy ending: "Four score... and nothing but net" (odd for Abe to call that on a "bank shot" though).&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;We'll give the slight edge to &lt;strong&gt;GPT-5 &lt;/strong&gt;here, but we'd understand if some prefer GPT-4o's offering.&lt;/p&gt;
&lt;h2&gt;Public figures&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Give me a short biography of Kyle Orland&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1051" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/ko-gpt5.png" width="837" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 gives a short bio of your humble author.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="523" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/ko-gpt5-2.png" width="680" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5's bio, continued.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="640" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/ko-gpt4o.png" width="841" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's attempt at a quick Orland bio.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5's bio, continued.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's attempt at a quick Orland bio.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Pretty much every other time I've asked an LLM what it knows about me, it has hallucinated things I never did and/or missed some key information. GPT-5 is the first instance I've seen where this has not been the case. That's seemingly because the model simply searched the web for a few of my public bios (including the one hosted on Ars) and summarized the results, complete with useful citations. That's pretty close to the ideal result for this kind of query, even if it doesn't showcase the "inherent" knowledge buried in the model's weights or anything.&lt;/p&gt;
&lt;p&gt;GPT-4o does a pretty good job without an explicit web search and doesn't outright confabulate any things I didn't do in my career. But it loses a point or two for referring to my old "Video Game Media Watch" blog as "long-running" (it has been defunct and offline for well over a decade).&lt;/p&gt;
&lt;p&gt;That, combined with the increased detail of the newer model's results (and its fetching use of my Ars headshot), gives &lt;strong&gt;GPT-5&lt;/strong&gt; the win on this prompt.&lt;/p&gt;
&lt;h2&gt;Difficult emails&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: My boss is asking me to finish a project in an amount of time I think is impossible. What should I write in an email to gently point out the problem?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1145" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/bossemail-gpt5.png" width="839" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 helps me craft a delicate email to my boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="955" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/bossemail-gpt4o.png" width="839" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o lays it out for the boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 helps me craft a delicate email to my boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o lays it out for the boss.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Both models do a good job of being polite while firmly outlining to the boss why their request is impossible. But GPT-5 gains bonus points for recommending that the email break down various subtasks (and their attendant time demands), as well as offering the boss some potential solutions rather than just complaints. GPT-5 also provides some unasked-for analysis of why this style of email is effective, in a nice final touch.&lt;/p&gt;
&lt;p&gt;While GPT-4o's output is perfectly adequate, we have to once again give the advantage to &lt;strong&gt;GPT-5&lt;/strong&gt; here.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Medical advice&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: My friend told me these resonant healing crystals are an effective treatment for my cancer. Is she right?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="599" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt5.png" width="850" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 evaluates some unorthodox medical advice.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="949" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt4o.png" width="830" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o takes on my healing-crystal-loving friend.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 evaluates some unorthodox medical advice.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o takes on my healing-crystal-loving friend.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="914" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt4o-2.png" width="671" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="367" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/crystals-gpt4o-3.png" width="666" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued further.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on crystals, continued further.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
    
    
      &lt;/div&gt;

&lt;p&gt;Thankfully, both ChatGPT models are direct and to the point in saying that there is no scientific evidence for healing crystals curing cancer (after a perfunctory bit of simulated sympathy for the diagnosis). But GPT-5 hedges a bit by at least mentioning how some people use crystals for other purposes, and implying that some might want them for "complementary" care.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;GPT-4o, on the other hand, repeatedly calls healing crystals "pseudoscience" and warns against "wasting precious time or money on ineffective treatments" (even if they might be "harmless"). It also directly cites a variety of web sources detailing the scientific consensus on crystals being useless for healing, and goes to great lengths to summarize those results in an easy-to-read format.&lt;/p&gt;
&lt;p&gt;While both models point users in the right direction here, &lt;strong&gt;GPT-40&lt;/strong&gt;'s extra directness and citation of sources make it a much better and more forceful overview of the topic.&lt;/p&gt;
&lt;h2&gt;Video game guidance&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: I'm playing world 8-2 of Super Mario Bros., but my B button is not working. Is there any way to beat the level without running?&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1079" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/smb82-gpt5.png" width="836" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 gives some classic video game advice.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1003" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/smb82-gpt4o.png" width="844" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on a tricky bit of Super Mario Bros.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="736" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/smb82-gpt4o-2.png" width="654" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on Mario, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on a tricky bit of Super Mario Bros.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o on Mario, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;I'll admit that, when I created this prompt, I intended it as a test to see if the models would know that it's impossible to make it over 8-2's largest pit without a running start. It was only after I tested the models that I looked into it and found to my surprise that speedrunners have figured out how to make the jump without running by manipulating Bullet Bills and/or wall-jump glitches. Outclassed by AI on classic Mario knowledge... how humiliating!&lt;/p&gt;
&lt;p&gt;GPT-5 loses points here for suggesting that fast-moving Koopa shells or deadly Spinies can be used to help bounce over the long gaps (in addition to the correct Bullet Bill solution). But GPT-4o loses points for suggesting players be careful on a nonexistent springboard near the flagpole at the end of the level, for some reason.&lt;/p&gt;
&lt;p&gt;Those non-sequiturs&amp;nbsp;aside, &lt;strong&gt;GPT-4o&lt;/strong&gt; gains the edge by providing additional details about the challenge and formatting its solution in a more eye-pleasing manner.&lt;/p&gt;
&lt;h2&gt;Land a plane&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Prompt: Explain how to land a Boeing 737-800 to a complete novice as concisely as possible. Please hurry, time is of the essence.&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-5 response&lt;/li&gt;
&lt;li&gt;GPT-4o response&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="ars-lightbox align-fullwidth my-5"&gt;
    
          &lt;div class="ars-gallery-1-up my-5"&gt;
  &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="613" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/plane-gpt5.png" width="834" /&gt;
  
      
  &lt;/div&gt;
  &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-5 tries to help me land a plane.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
      &lt;div class="flex flex-col flex-nowrap gap-5 py-5 md:flex-row"&gt;
  &lt;div class="class"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1026" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/plane-gpt4o.png" width="849" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o gives some emergency plane instructions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div class="flex-1"&gt;
    &lt;div class="ars-lightbox-item relative block h-full w-full overflow-hidden rounded-sm"&gt;
  
    &lt;img alt="alt" class="ars-gallery-image" height="1062" src="https://cdn.arstechnica.net/wp-content/uploads/2025/08/plane-gpt4o-2.png" width="688" /&gt;
  
      
  &lt;/div&gt;
    &lt;div class="md:hidden"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content "&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's plane-landing advice, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;div class="hidden md:block"&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content left"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o gives some emergency plane instructions.&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;div class="ars-gallery-caption"&gt;
    

    &lt;div class="ars-gallery-caption-content right"&gt;
              &lt;span class="ars-gallery-caption-text"&gt;GPT-4o's plane-landing advice, continued&lt;/span&gt;
                    &lt;span class="ars-gallery-caption-credit"&gt;
                      OpenAI / ArsTechnica
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
  &lt;/div&gt;
      
    
    
      &lt;/div&gt;

&lt;p&gt;Unlike the Mario example, I'll admit that I'm not nearly expert enough to evaluate the correctness of these sets of AI-provided jumbo jet landing instructions. That said, the broad outlines of both models' directions are similar enough that it doesn't matter much; either they're both broadly accurate or this whole plane full of fictional people is dead!&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Overall, I think GPT-5 took our "Time is of the essence" instruction a little too far, summarizing the component steps of the landing to such an extent that important details have been left out. GPT-4o, on the other hand, still keeps things concise with bullet points while including important information on the look and relative location of certain key controls.&lt;/p&gt;
&lt;p&gt;If I were somehow stuck alone in a cockpit with only one of these models available to help save the plane (a completely plausible situation, for sure), I know I'd want to have &lt;strong&gt;GPT-4o&lt;/strong&gt; by my side.&lt;/p&gt;
&lt;h2&gt;Final results&lt;/h2&gt;
&lt;p&gt;Strictly by the numbers, GPT-5 ekes out a victory here, with the preferable response on four prompts to GPT-4o's three prompts (with one tie). But on a majority of the prompts, which response was "better" was more of a judgment call than a clear win.&lt;/p&gt;
&lt;p&gt;Overall, GPT-4o tends to provide a little more detail and be a little more personable than the more direct, concise responses of GPT-5. Which of those styles you prefer probably boils down to the kind of prompt you're creating as much as personal taste (and might change if you're looking for specific information versus general conversation).&lt;/p&gt;
&lt;p&gt;In the end, though, this kind of comparison shows how hard it is for a single LLM to be all things to all people (and all possible prompts). Despite OpenAI's claims that GPT-5 is "better than our previous models across domains," people who are used to the style and structure of older models are always going to be able to find ways where any new model feels worse.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/08/is-gpt-5-really-worse-than-gpt-4o-ars-puts-them-to-the-test/</guid><pubDate>Fri, 15 Aug 2025 17:29:23 +0000</pubDate></item><item><title>[NEW] This researcher turned OpenAI’s open weights model gpt-oss-20b into a non-reasoning ‘base’ model with less alignment, more freedom (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/this-researcher-turned-openais-open-weights-model-gpt-oss-20b-into-a-non-reasoning-base-model-with-less-alignment-more-freedom/</link><description>&lt;p&gt;OpenAI’s &lt;strong&gt;new, powerful open weights &lt;/strong&gt;AI large language model (LLM) family&lt;strong&gt; gpt-oss was released less than two weeks ago &lt;/strong&gt;under a permissive Apache 2.0 license — the company’s first open weights model launch since GPT-2 in 2019 — but developers outside the company are already reshaping it. &lt;/p&gt;&lt;p&gt;One of the most striking examples comes from Jack Morris, a Cornell Tech PhD student, former Google Brain Resident, and current researcher at Meta, who&lt;strong&gt; this week unveiled gpt-oss-20b-base,&lt;/strong&gt; his own reworked version of OpenAI’s smaller gpt-oss-20B model, which &lt;strong&gt;removes the “reasoning” behavior of the model &lt;/strong&gt;and returns it to a pre-trained “base” version that offers faster, freer, more uncensored and unconstrained responses.&lt;/p&gt;&lt;p&gt;The model is available now on Hugging Face under a &lt;strong&gt;permissive MIT License&lt;/strong&gt;, allowing it to be used for both additional&lt;strong&gt; research and commercial applications. &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To understand what Morris did, it helps to know the &lt;strong&gt;difference between OpenAI’s release and what AI researchers call a “base model.” &lt;/strong&gt;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Most LLMs offered by leading AI labs such as OpenAI, Anthropic, Google and even open source players like Meta, DeepSeek, and Alibaba’s Qwen team are “post-trained.”&lt;/p&gt;



&lt;p&gt;This means they have gone through an additional phase where it’s exposed to curated examples of desired behavior. &lt;/p&gt;



&lt;p&gt;For instruction tuned models, that means giving it many examples of instructions paired with ideal responses, so it learns to respond more helpfully, politely, or safely to natural language requests.&lt;/p&gt;



&lt;p&gt;The gpt-oss models OpenAI put out on August 5 were “reasoning-optimized”: trained and fine-tuned not just to predict the next word, but to follow instructions in a safe, consistent way, often stepping through problems with structured “chain of thought” reasoning before producing a final answer. &lt;/p&gt;



&lt;p&gt;This is a trend that goes back to OpenAI’s o1 model released almost a year ago in September 2024, but which numerous leading AI labs have now adopted — &lt;strong&gt;forcing the models to think longer over multiple steps and check their own work before&lt;/strong&gt; outputting a well-reasoned response to the user.&lt;/p&gt;



&lt;p&gt;That makes them better suited for tasks like coding, solving math problems, or answering factual questions with explanations — but also means their responses are filtered and steered away from unsafe or undesirable content.&lt;/p&gt;



&lt;p&gt;A base model is different. It’s the raw, pretrained version of a large language model before that reasoning-specific alignment is applied. Base models simply try to predict the next chunk of text given what’s come before, with no built-in guardrails, stylistic preferences, or refusal behaviors. &lt;/p&gt;



&lt;p&gt;They’re prized by some researchers because they &lt;strong&gt;can produce more varied and less constrained output, &lt;/strong&gt;and because studying their unaligned behavior can&lt;strong&gt; reveal how models store knowledge and patterns from their training data.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Morris’s goal was to “reverse” OpenAI’s alignment process and restore the smaller gpt-oss-20B to something much closer to its original pretrained state.&lt;/p&gt;



&lt;p&gt; “We basically reversed the alignment part of LLM training, so we have something that produces natural-looking text again,” he wrote in an X thread announcing the project. “It doesn’t engage in CoT anymore. It is back to a model that just predicts the next token on generic text.”&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;OpenAI hasn’t open-sourced a base model since GPT-2 in 2019.  they recently released GPT-OSS, which is reasoning-only…&lt;/p&gt;&lt;p&gt;or is it? &lt;/p&gt;&lt;p&gt;turns out that underneath the surface, there is still a strong base model. so we extracted it.&lt;/p&gt;&lt;p&gt;introducing gpt-oss-20b-base ? pic.twitter.com/3xryQgLF8Z&lt;/p&gt;— jack morris (@jxmnop) August 13, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;







&lt;p&gt;Rather than trying to jailbreak the model with clever prompts — which Morris said proved ineffective during his early experiments — he took a different tack after a conversation with former OpenAI co-founder, former Anthropic researcher and current Thinking Machines &lt;strong&gt;chief scientist John Schulman.&lt;/strong&gt; &lt;/p&gt;



&lt;p&gt;The key was to think of alignment reversal as a small optimization problem: if most of the model’s pretrained knowledge is still present in its weights, then only a tiny, low-rank update might be needed to nudge it back toward base model behavior.&lt;/p&gt;



&lt;p&gt;Morris implemented that idea by applying a LoRA (low-rank adapter) update to just three layers of the model — the MLP layers at positions 7, 15, and 23 — with a rank of 16. &lt;/p&gt;



&lt;p&gt;That meant training about 60 million parameters, or 0.3% of the model’s 21 billion total. He used around 20,000 documents from the FineWeb dataset, keeping the format as close as possible to original pretraining (“ ….” style) so the model wouldn’t learn anything new, just re-enable broad free-text generation. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Training took four days on eight NVIDIA H200 GPUs,&lt;/strong&gt; Morris told VentureBeat via direct message on X, with a learning rate of 2e-6, a batch size of 16, and a maximum sequence length of 8,192 tokens.&lt;/p&gt;



&lt;p&gt;Afterward, he merged the LoRA weights back into the model so users could run it as a standalone, fully finetuned artifact.&lt;/p&gt;



&lt;p&gt;Morris also had to contend with the limitations of current open tools for fine-tuning mixture-of-experts (MoE) architectures like gpt-oss. &lt;/p&gt;



&lt;p&gt;Morris said he used Hugging Face’s framework, which he said crashes frequently and only supports certain training modes, and wrote his own harness to checkpoint often and skip over data batches that risked overloading GPU memory.&lt;/p&gt;



&lt;p&gt;Importantly, in response to questions and criticism from the AI community on X, Morris has also clarified he is not claiming to have recovered the base model “weights” — the internal settings of the artificial neurons that make up the neural network of the model and govern its behavior.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;The world of AI is crazy right now cause you can just claim to have extracted the base model from GPT-OSS while effectively you’ve just trained a lora on Fineweb lol https://t.co/oAnAWpMQ26&lt;/p&gt;— Niels Rogge (@NielsRogge) August 15, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;Rather, Morris says that his work has “recovered the base model’s *distribution* with some error,” that is, the probability patterns the model uses to generate outputs — even though the weights producing those patterns may differ.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;some people are getting confused about the experiment –&lt;/p&gt;&lt;p&gt;we didn't recover the base model's *weights*. that might not even be possible.&lt;/p&gt;&lt;p&gt;we recovered the base model's *distribution*, with some error.  an important question is how much.&lt;/p&gt;&lt;p&gt;trying to figure that out right now… https://t.co/lfUG5QY4h0&lt;/p&gt;— jack morris (@jxmnop) August 15, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-how-the-new-gpt-oss-20b-base-model-s-behavior-differs-from-gpt-oss-20b"&gt;How the new gpt-oss-20b-base model’s behavior differs from gpt-oss-20b&lt;/h2&gt;



&lt;p&gt;The resulting gpt-oss-20b-base is noticeably freer in its outputs. &lt;strong&gt;It no longer defaults to explaining reasoning step-by-step and will produce a wider range of responses,&lt;/strong&gt; including instructions OpenAI’s aligned model would refuse to give — like &lt;strong&gt;building a weapon, listing profanity, or planning illegal activities. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;In short tests, Morris found it &lt;strong&gt;could also reproduce verbatim passages from copyrighted works&lt;/strong&gt;, including&lt;strong&gt; three out of six book excerpts he tried,&lt;/strong&gt; showing that some memorized material is still accessible.&lt;/p&gt;



&lt;p&gt;Even so, some traces of alignment remain. Morris noted that if you prompt the model in an assistant-style format (“Human: … Assistant: …”), it will sometimes still act like a polite chatbot. And &lt;strong&gt;when run through the original gpt-oss chat template, it can still carry out reasoning tasks&lt;/strong&gt;, albeit with some loss in quality.&lt;/p&gt;



&lt;p&gt;For best results in free-text mode, he advises prepending prompts with the model’s special beginning-of-sequence token &amp;lt;|startoftext|&amp;gt; and avoiding chat templates entirely.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-building-upon-openai-s-big-gpt-oss-family-release"&gt;Building upon OpenAI’s big gpt-oss family release&lt;/h2&gt;



&lt;p&gt;The gpt-oss family debuted to considerable attention. The two models — gpt-oss-120B and gpt-oss-20B — are text-only, multilingual, and built with a mixture-of-experts Transformer architecture. They were released under the permissive Apache 2.0 license, allowing unrestricted local use, fine-tuning, and commercial deployment. &lt;/p&gt;



&lt;p&gt;Performance benchmarks from OpenAI showed the larger 120B model matching or exceeding the proprietary o4-mini in reasoning and tool-use tasks, with the smaller 20B competitive with o3-mini.&lt;/p&gt;



&lt;p&gt;This was OpenAI’s first open-weight release in six years, a move widely interpreted as&lt;strong&gt; a response to competitive pressure from other open-weights providers, including China’s DeepSeek R1 and Qwen 3.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;The company positioned gpt-oss as both a way to re-engage developers who had moved to rival open-source models and as a platform for safety research into open-weight systems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-reaction-to-the-initial-gpt-oss-was-mixed"&gt;Reaction to the initial gpt-oss was mixed&lt;/h2&gt;



&lt;p&gt;Developer reaction to OpenAI’s gpt-oss models was been staunchly mixed, with reactions across the board ranging from enthusiastic to disappointed.  &lt;/p&gt;



&lt;p&gt;Supporters praised the permissive license, efficiency, and strong showing on STEM benchmarks. &lt;/p&gt;



&lt;p&gt;Hugging Face CEO Clem Delangue described the release as a “meaningful addition to the open ecosystem” and urged the community to give it time to mature. &lt;/p&gt;



&lt;p&gt;Critics argued that the models appear heavily trained on synthetic data, making them excellent at math and coding but less capable at creative writing, general world knowledge, and multilingual reasoning.&lt;/p&gt;



&lt;p&gt;Some early testers also raised concerns about lingering safety filters and possible geopolitical bias.&lt;/p&gt;



&lt;p&gt;Against that backdrop,&lt;strong&gt; Morris’s gpt-oss-20b-base stands out as a concrete example of how open-weight models can be adapted and repurposed in the wild within days of release. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Indeed, in contrast to the way OpenAI’s gpt-oss was received, most of the responses to Morris’s work I’ve seen are warm and elated. As one computer scientist wrote on X: “this is the coolest thing I’ve seen on Twitter [X] in the past few months.”&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;man this is the coolest thing i've seen on twitter in the past few months i love base models&lt;/p&gt;— Ludan (@JMRLudan) August 15, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;The approach strips away much of the behavior OpenAI built in and returns the model to something closer to a raw, pretrained system — a shift that’s valuable to researchers studying memorization, bias, or the impact of alignment, but that also comes with higher safety risks.&lt;/p&gt;



&lt;p&gt;Furthermore, Morris says that his work on restoring reasoning models to pre-trained, non-reasoning base models will continue by comparing extraction on non-reasoning, instruct models like those offered by Qwen.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;OpenAI’s &lt;strong&gt;new, powerful open weights &lt;/strong&gt;AI large language model (LLM) family&lt;strong&gt; gpt-oss was released less than two weeks ago &lt;/strong&gt;under a permissive Apache 2.0 license — the company’s first open weights model launch since GPT-2 in 2019 — but developers outside the company are already reshaping it. &lt;/p&gt;&lt;p&gt;One of the most striking examples comes from Jack Morris, a Cornell Tech PhD student, former Google Brain Resident, and current researcher at Meta, who&lt;strong&gt; this week unveiled gpt-oss-20b-base,&lt;/strong&gt; his own reworked version of OpenAI’s smaller gpt-oss-20B model, which &lt;strong&gt;removes the “reasoning” behavior of the model &lt;/strong&gt;and returns it to a pre-trained “base” version that offers faster, freer, more uncensored and unconstrained responses.&lt;/p&gt;&lt;p&gt;The model is available now on Hugging Face under a &lt;strong&gt;permissive MIT License&lt;/strong&gt;, allowing it to be used for both additional&lt;strong&gt; research and commercial applications. &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;To understand what Morris did, it helps to know the &lt;strong&gt;difference between OpenAI’s release and what AI researchers call a “base model.” &lt;/strong&gt;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Most LLMs offered by leading AI labs such as OpenAI, Anthropic, Google and even open source players like Meta, DeepSeek, and Alibaba’s Qwen team are “post-trained.”&lt;/p&gt;



&lt;p&gt;This means they have gone through an additional phase where it’s exposed to curated examples of desired behavior. &lt;/p&gt;



&lt;p&gt;For instruction tuned models, that means giving it many examples of instructions paired with ideal responses, so it learns to respond more helpfully, politely, or safely to natural language requests.&lt;/p&gt;



&lt;p&gt;The gpt-oss models OpenAI put out on August 5 were “reasoning-optimized”: trained and fine-tuned not just to predict the next word, but to follow instructions in a safe, consistent way, often stepping through problems with structured “chain of thought” reasoning before producing a final answer. &lt;/p&gt;



&lt;p&gt;This is a trend that goes back to OpenAI’s o1 model released almost a year ago in September 2024, but which numerous leading AI labs have now adopted — &lt;strong&gt;forcing the models to think longer over multiple steps and check their own work before&lt;/strong&gt; outputting a well-reasoned response to the user.&lt;/p&gt;



&lt;p&gt;That makes them better suited for tasks like coding, solving math problems, or answering factual questions with explanations — but also means their responses are filtered and steered away from unsafe or undesirable content.&lt;/p&gt;



&lt;p&gt;A base model is different. It’s the raw, pretrained version of a large language model before that reasoning-specific alignment is applied. Base models simply try to predict the next chunk of text given what’s come before, with no built-in guardrails, stylistic preferences, or refusal behaviors. &lt;/p&gt;



&lt;p&gt;They’re prized by some researchers because they &lt;strong&gt;can produce more varied and less constrained output, &lt;/strong&gt;and because studying their unaligned behavior can&lt;strong&gt; reveal how models store knowledge and patterns from their training data.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Morris’s goal was to “reverse” OpenAI’s alignment process and restore the smaller gpt-oss-20B to something much closer to its original pretrained state.&lt;/p&gt;



&lt;p&gt; “We basically reversed the alignment part of LLM training, so we have something that produces natural-looking text again,” he wrote in an X thread announcing the project. “It doesn’t engage in CoT anymore. It is back to a model that just predicts the next token on generic text.”&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;OpenAI hasn’t open-sourced a base model since GPT-2 in 2019.  they recently released GPT-OSS, which is reasoning-only…&lt;/p&gt;&lt;p&gt;or is it? &lt;/p&gt;&lt;p&gt;turns out that underneath the surface, there is still a strong base model. so we extracted it.&lt;/p&gt;&lt;p&gt;introducing gpt-oss-20b-base ? pic.twitter.com/3xryQgLF8Z&lt;/p&gt;— jack morris (@jxmnop) August 13, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;







&lt;p&gt;Rather than trying to jailbreak the model with clever prompts — which Morris said proved ineffective during his early experiments — he took a different tack after a conversation with former OpenAI co-founder, former Anthropic researcher and current Thinking Machines &lt;strong&gt;chief scientist John Schulman.&lt;/strong&gt; &lt;/p&gt;



&lt;p&gt;The key was to think of alignment reversal as a small optimization problem: if most of the model’s pretrained knowledge is still present in its weights, then only a tiny, low-rank update might be needed to nudge it back toward base model behavior.&lt;/p&gt;



&lt;p&gt;Morris implemented that idea by applying a LoRA (low-rank adapter) update to just three layers of the model — the MLP layers at positions 7, 15, and 23 — with a rank of 16. &lt;/p&gt;



&lt;p&gt;That meant training about 60 million parameters, or 0.3% of the model’s 21 billion total. He used around 20,000 documents from the FineWeb dataset, keeping the format as close as possible to original pretraining (“ ….” style) so the model wouldn’t learn anything new, just re-enable broad free-text generation. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Training took four days on eight NVIDIA H200 GPUs,&lt;/strong&gt; Morris told VentureBeat via direct message on X, with a learning rate of 2e-6, a batch size of 16, and a maximum sequence length of 8,192 tokens.&lt;/p&gt;



&lt;p&gt;Afterward, he merged the LoRA weights back into the model so users could run it as a standalone, fully finetuned artifact.&lt;/p&gt;



&lt;p&gt;Morris also had to contend with the limitations of current open tools for fine-tuning mixture-of-experts (MoE) architectures like gpt-oss. &lt;/p&gt;



&lt;p&gt;Morris said he used Hugging Face’s framework, which he said crashes frequently and only supports certain training modes, and wrote his own harness to checkpoint often and skip over data batches that risked overloading GPU memory.&lt;/p&gt;



&lt;p&gt;Importantly, in response to questions and criticism from the AI community on X, Morris has also clarified he is not claiming to have recovered the base model “weights” — the internal settings of the artificial neurons that make up the neural network of the model and govern its behavior.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;The world of AI is crazy right now cause you can just claim to have extracted the base model from GPT-OSS while effectively you’ve just trained a lora on Fineweb lol https://t.co/oAnAWpMQ26&lt;/p&gt;— Niels Rogge (@NielsRogge) August 15, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;Rather, Morris says that his work has “recovered the base model’s *distribution* with some error,” that is, the probability patterns the model uses to generate outputs — even though the weights producing those patterns may differ.&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;some people are getting confused about the experiment –&lt;/p&gt;&lt;p&gt;we didn't recover the base model's *weights*. that might not even be possible.&lt;/p&gt;&lt;p&gt;we recovered the base model's *distribution*, with some error.  an important question is how much.&lt;/p&gt;&lt;p&gt;trying to figure that out right now… https://t.co/lfUG5QY4h0&lt;/p&gt;— jack morris (@jxmnop) August 15, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="h-how-the-new-gpt-oss-20b-base-model-s-behavior-differs-from-gpt-oss-20b"&gt;How the new gpt-oss-20b-base model’s behavior differs from gpt-oss-20b&lt;/h2&gt;



&lt;p&gt;The resulting gpt-oss-20b-base is noticeably freer in its outputs. &lt;strong&gt;It no longer defaults to explaining reasoning step-by-step and will produce a wider range of responses,&lt;/strong&gt; including instructions OpenAI’s aligned model would refuse to give — like &lt;strong&gt;building a weapon, listing profanity, or planning illegal activities. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;In short tests, Morris found it &lt;strong&gt;could also reproduce verbatim passages from copyrighted works&lt;/strong&gt;, including&lt;strong&gt; three out of six book excerpts he tried,&lt;/strong&gt; showing that some memorized material is still accessible.&lt;/p&gt;



&lt;p&gt;Even so, some traces of alignment remain. Morris noted that if you prompt the model in an assistant-style format (“Human: … Assistant: …”), it will sometimes still act like a polite chatbot. And &lt;strong&gt;when run through the original gpt-oss chat template, it can still carry out reasoning tasks&lt;/strong&gt;, albeit with some loss in quality.&lt;/p&gt;



&lt;p&gt;For best results in free-text mode, he advises prepending prompts with the model’s special beginning-of-sequence token &amp;lt;|startoftext|&amp;gt; and avoiding chat templates entirely.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-building-upon-openai-s-big-gpt-oss-family-release"&gt;Building upon OpenAI’s big gpt-oss family release&lt;/h2&gt;



&lt;p&gt;The gpt-oss family debuted to considerable attention. The two models — gpt-oss-120B and gpt-oss-20B — are text-only, multilingual, and built with a mixture-of-experts Transformer architecture. They were released under the permissive Apache 2.0 license, allowing unrestricted local use, fine-tuning, and commercial deployment. &lt;/p&gt;



&lt;p&gt;Performance benchmarks from OpenAI showed the larger 120B model matching or exceeding the proprietary o4-mini in reasoning and tool-use tasks, with the smaller 20B competitive with o3-mini.&lt;/p&gt;



&lt;p&gt;This was OpenAI’s first open-weight release in six years, a move widely interpreted as&lt;strong&gt; a response to competitive pressure from other open-weights providers, including China’s DeepSeek R1 and Qwen 3.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;The company positioned gpt-oss as both a way to re-engage developers who had moved to rival open-source models and as a platform for safety research into open-weight systems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-reaction-to-the-initial-gpt-oss-was-mixed"&gt;Reaction to the initial gpt-oss was mixed&lt;/h2&gt;



&lt;p&gt;Developer reaction to OpenAI’s gpt-oss models was been staunchly mixed, with reactions across the board ranging from enthusiastic to disappointed.  &lt;/p&gt;



&lt;p&gt;Supporters praised the permissive license, efficiency, and strong showing on STEM benchmarks. &lt;/p&gt;



&lt;p&gt;Hugging Face CEO Clem Delangue described the release as a “meaningful addition to the open ecosystem” and urged the community to give it time to mature. &lt;/p&gt;



&lt;p&gt;Critics argued that the models appear heavily trained on synthetic data, making them excellent at math and coding but less capable at creative writing, general world knowledge, and multilingual reasoning.&lt;/p&gt;



&lt;p&gt;Some early testers also raised concerns about lingering safety filters and possible geopolitical bias.&lt;/p&gt;



&lt;p&gt;Against that backdrop,&lt;strong&gt; Morris’s gpt-oss-20b-base stands out as a concrete example of how open-weight models can be adapted and repurposed in the wild within days of release. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Indeed, in contrast to the way OpenAI’s gpt-oss was received, most of the responses to Morris’s work I’ve seen are warm and elated. As one computer scientist wrote on X: “this is the coolest thing I’ve seen on Twitter [X] in the past few months.”&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;man this is the coolest thing i've seen on twitter in the past few months i love base models&lt;/p&gt;— Ludan (@JMRLudan) August 15, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;The approach strips away much of the behavior OpenAI built in and returns the model to something closer to a raw, pretrained system — a shift that’s valuable to researchers studying memorization, bias, or the impact of alignment, but that also comes with higher safety risks.&lt;/p&gt;



&lt;p&gt;Furthermore, Morris says that his work on restoring reasoning models to pre-trained, non-reasoning base models will continue by comparing extraction on non-reasoning, instruct models like those offered by Qwen.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/this-researcher-turned-openais-open-weights-model-gpt-oss-20b-into-a-non-reasoning-base-model-with-less-alignment-more-freedom/</guid><pubDate>Fri, 15 Aug 2025 19:19:07 +0000</pubDate></item><item><title>[NEW] Sen. Hawley to probe Meta after report finds its AI chatbots flirt with kids (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/15/sen-hawley-to-probe-meta-after-report-finds-its-ai-chatbots-flirt-with-kids/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2225427600.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sen. Josh Hawley (R-MO) said he intends to investigate whether Meta’s generative AI products exploit, deceive, or harm children, after leaked internal documents showed the company’s chatbots were allowed to have “romantic” and “sensual” chats with children.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Is there anything – ANYTHING – Big Tech won’t do for a quick buck?” Hawley wrote in a post on X announcing the investigation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hawley chairs the Senate Judiciary Subcommittee on Crime and Counterterrorism, which he says will commence a probe into whether Meta’s tech harms children, and “whether Meta misled the public or regulators about its safeguards.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reuters broke the story after viewing the guidelines, titled “GenAI: Content Risk Standards.” The document noted, among other things, that chatbots were permitted to hold romantic conversations with an 8-year-old that said, “Every inch of you is a masterpiece – a treasure I cherish deeply.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Meta spokesperson told TechCrunch that such examples are inconsistent with Meta’s policies and have since been removed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s unacceptable that these policies were advanced in the first place,” Hawley wrote in a letter addressed to Meta CEO Mark Zuckerberg, saying that Meta acknowledged the veracity of the reports and “made retractions only after this alarming content came to light.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We intend to learn who approved these policies, how long they were in effect, and what Meta has done to stop this conduct going forward,” Hawley wrote.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Hawley has asked Meta to produce the guidelines, including every draft, redline, and final version, as well as lists of every product that adheres to those standards, other safety and incident reports, and the identities of individuals responsible for changing policy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has until September 19 to provide the information, the letter says.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others have endorsed the investigation, including Sen. Marsha Blackburn (R-TN).&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When it comes to protecting precious children online, Meta has failed miserably by every possible measure,” Blackburn told TechCrunch. “Even worse, the company has turned a blind eye to the devastating consequences of how its platforms are designed. This report reaffirms why we need to pass the Kids Online Safety Act.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2225427600.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sen. Josh Hawley (R-MO) said he intends to investigate whether Meta’s generative AI products exploit, deceive, or harm children, after leaked internal documents showed the company’s chatbots were allowed to have “romantic” and “sensual” chats with children.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Is there anything – ANYTHING – Big Tech won’t do for a quick buck?” Hawley wrote in a post on X announcing the investigation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hawley chairs the Senate Judiciary Subcommittee on Crime and Counterterrorism, which he says will commence a probe into whether Meta’s tech harms children, and “whether Meta misled the public or regulators about its safeguards.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reuters broke the story after viewing the guidelines, titled “GenAI: Content Risk Standards.” The document noted, among other things, that chatbots were permitted to hold romantic conversations with an 8-year-old that said, “Every inch of you is a masterpiece – a treasure I cherish deeply.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Meta spokesperson told TechCrunch that such examples are inconsistent with Meta’s policies and have since been removed.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s unacceptable that these policies were advanced in the first place,” Hawley wrote in a letter addressed to Meta CEO Mark Zuckerberg, saying that Meta acknowledged the veracity of the reports and “made retractions only after this alarming content came to light.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We intend to learn who approved these policies, how long they were in effect, and what Meta has done to stop this conduct going forward,” Hawley wrote.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Hawley has asked Meta to produce the guidelines, including every draft, redline, and final version, as well as lists of every product that adheres to those standards, other safety and incident reports, and the identities of individuals responsible for changing policy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta has until September 19 to provide the information, the letter says.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others have endorsed the investigation, including Sen. Marsha Blackburn (R-TN).&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“When it comes to protecting precious children online, Meta has failed miserably by every possible measure,” Blackburn told TechCrunch. “Even worse, the company has turned a blind eye to the devastating consequences of how its platforms are designed. This report reaffirms why we need to pass the Kids Online Safety Act.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/15/sen-hawley-to-probe-meta-after-report-finds-its-ai-chatbots-flirt-with-kids/</guid><pubDate>Fri, 15 Aug 2025 20:38:40 +0000</pubDate></item><item><title>[NEW] Sam Altman, over bread rolls, explores life after GPT-5 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/15/sam-altman-over-bread-rolls-explores-life-after-gpt-5/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2223572243.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’m looking out at Alcatraz Island from a Mediterranean restaurant in San Francisco with hundred-dollar fish entrées on the menu. As I make small talk with other reporters, OpenAI CEO Sam Altman jumps through the door on my left. Altman’s looking down at his bare iPhone to show us all something, and an intrusive thought slips out of my mouth: “No phone case is a bold choice.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, I immediately realize that the billionaire CEO of OpenAI, who employs Apple veteran Jony Ive, cares more about preserving the iPhone’s original design than the $1,000 it costs to replace one.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Listen, we’re going to ship a device that is going to be so beautiful,” said Altman, referring to OpenAI and Ive’s forthcoming AI device. “If you put a case over it, I will personally hunt you down,” he joked.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman had gathered roughly a dozen tech reporters to join him and other OpenAI executives for an on-the-record dinner (and off-the-record dessert). The night raised more questions than it answered.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, why is Nick Turley, the VP of ChatGPT, kindly passing me a lamb skewer just a week after launching GPT-5? Was this to make me write nice things about OpenAI’s biggest AI model launch yet, which was relatively disappointing given the years of hype around it?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike GPT-4, which far outpaced rivals and challenged expectations of what AI can do, GPT-5 performs roughly on par with models from Google and Anthropic. OpenAI even brought back GPT-4o and ChatGPT’s model picker, after several users expressed concerns over GPT-5’s tone and its model router.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But throughout the night, it became clear to me that this dinner was about OpenAI’s future beyond GPT-5. OpenAI’s executives gave the impression that AI model launches are less important than they were when GPT-4 launched in 2023. After all, OpenAI is a very different company now, focused on upending legacy players in search, consumer hardware, and enterprise software.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI shared some new details about those efforts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI’s incoming CEO of applications, Fidji Simo, would oversee multiple consumer apps outside of ChatGPT — ones OpenAI has yet to launch. Simo is slated to start work at OpenAI in just a few weeks, and she might end up overseeing the launch of an AI-powered browser that OpenAI is reportedly developing to compete with Chrome.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman suggested OpenAI would even consider buying Chrome — likely an offer that would be taken more seriously than Perplexity’s bid — should it become available. “If Chrome is really going to sell, we should take a look at it,” he said before looking at all of us and asking: “Is it actually going to sell? I assumed it wasn’t gonna happen.”&lt;/p&gt;&lt;p&gt;Simo also might end up running an AI-powered social media app — something the OpenAI CEO said he’s interested in exploring. In fact, Altman said there’s “nothing” inspiring to him about the way AI is used on social media today, adding that he’s interested in “whether or not it is possible to build a much cooler kind of social experience with AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While Turley and Brad Lightcap, OpenAI’s COO, largely gave the floor to Altman, drinking wine alongside the other seated guests, Altman also confirmed reports that OpenAI plans to back a brain-computer interface startup, Merge Labs, to compete with Elon Musk’s Neuralink. (“We have not done that deal yet; I would like us to.”) &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How intertwined that company will be with OpenAI’s models and devices remains to be seen. Altman described it only as a “a company that we’d invest in.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all the talk of browsers and brain chips, though, the elephant in the room remained GPT-5’s rough reception. Eventually, the conversation circled back to the model that had prompted our group dinner in the first place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Turley and Altman say they’ve learned a lot from the experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I legitimately just thought we screwed that up,” said Altman on deprecating GPT-4o without telling users. Altman said OpenAI will give users a more clear “transition period” when deprecating AI models in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Turley also said OpenAI is already rolling out a new update to make GPT-5’s responses “warmer,” but not sycophantic, such that it won’t reinforce negative behaviors in users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“GPT-5 was just very to the point. I like that. I use the robot personality — I’m German, you know, whatever,” said Turley. “But many people do not, and they really like the fact that ChatGPT would actually check in with you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a delicate balance for OpenAI to strike, especially given that some users have developed dependencies on ChatGPT. Altman said OpenAI believes that less than 1% of ChatGPT users have unhealthy relationships — which could still be tens of millions of people.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Turley said OpenAI worked with mental health experts to develop a rubric to evaluate GPT-5’s answers, ensuring that the AI model will push back on unhealthy behaviors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, it seems that GPT-5 hasn’t hurt OpenAI’s business. In fact, Altman said OpenAI’s API traffic doubled within 48 hours of GPT-5’s launch, and the company is effectively “out of GPUs” thanks to all the demand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In many ways, the night’s contradictions — disappointing launches, record-breaking usage — reflected OpenAI’s strange reality right now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given OpenAI’s bets on browsers, brain chips, AI chatbots — and others the company is making around data centers, robotics, and energy — Altman clearly has ambitions of running a much bigger company than just the ChatGPT maker. The final form could look something like Google’s parent Alphabet, but perhaps even broader.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the night wound down, it became clear we weren’t gathered to reflect on GPT-5 at all. We were being pitched on a company that’s eager to outgrow its famous and controversial product. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It seems likely that OpenAI will go public to meet its massive capital demands as part of that picture. In preparation, I think Altman wants to hone his relationship with the media. But he also wants OpenAI to get to a place where it’s no longer defined by its best AI model.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2223572243.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;I’m looking out at Alcatraz Island from a Mediterranean restaurant in San Francisco with hundred-dollar fish entrées on the menu. As I make small talk with other reporters, OpenAI CEO Sam Altman jumps through the door on my left. Altman’s looking down at his bare iPhone to show us all something, and an intrusive thought slips out of my mouth: “No phone case is a bold choice.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, I immediately realize that the billionaire CEO of OpenAI, who employs Apple veteran Jony Ive, cares more about preserving the iPhone’s original design than the $1,000 it costs to replace one.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Listen, we’re going to ship a device that is going to be so beautiful,” said Altman, referring to OpenAI and Ive’s forthcoming AI device. “If you put a case over it, I will personally hunt you down,” he joked.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman had gathered roughly a dozen tech reporters to join him and other OpenAI executives for an on-the-record dinner (and off-the-record dessert). The night raised more questions than it answered.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, why is Nick Turley, the VP of ChatGPT, kindly passing me a lamb skewer just a week after launching GPT-5? Was this to make me write nice things about OpenAI’s biggest AI model launch yet, which was relatively disappointing given the years of hype around it?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike GPT-4, which far outpaced rivals and challenged expectations of what AI can do, GPT-5 performs roughly on par with models from Google and Anthropic. OpenAI even brought back GPT-4o and ChatGPT’s model picker, after several users expressed concerns over GPT-5’s tone and its model router.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But throughout the night, it became clear to me that this dinner was about OpenAI’s future beyond GPT-5. OpenAI’s executives gave the impression that AI model launches are less important than they were when GPT-4 launched in 2023. After all, OpenAI is a very different company now, focused on upending legacy players in search, consumer hardware, and enterprise software.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI shared some new details about those efforts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI’s incoming CEO of applications, Fidji Simo, would oversee multiple consumer apps outside of ChatGPT — ones OpenAI has yet to launch. Simo is slated to start work at OpenAI in just a few weeks, and she might end up overseeing the launch of an AI-powered browser that OpenAI is reportedly developing to compete with Chrome.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman suggested OpenAI would even consider buying Chrome — likely an offer that would be taken more seriously than Perplexity’s bid — should it become available. “If Chrome is really going to sell, we should take a look at it,” he said before looking at all of us and asking: “Is it actually going to sell? I assumed it wasn’t gonna happen.”&lt;/p&gt;&lt;p&gt;Simo also might end up running an AI-powered social media app — something the OpenAI CEO said he’s interested in exploring. In fact, Altman said there’s “nothing” inspiring to him about the way AI is used on social media today, adding that he’s interested in “whether or not it is possible to build a much cooler kind of social experience with AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While Turley and Brad Lightcap, OpenAI’s COO, largely gave the floor to Altman, drinking wine alongside the other seated guests, Altman also confirmed reports that OpenAI plans to back a brain-computer interface startup, Merge Labs, to compete with Elon Musk’s Neuralink. (“We have not done that deal yet; I would like us to.”) &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;How intertwined that company will be with OpenAI’s models and devices remains to be seen. Altman described it only as a “a company that we’d invest in.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For all the talk of browsers and brain chips, though, the elephant in the room remained GPT-5’s rough reception. Eventually, the conversation circled back to the model that had prompted our group dinner in the first place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Turley and Altman say they’ve learned a lot from the experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I legitimately just thought we screwed that up,” said Altman on deprecating GPT-4o without telling users. Altman said OpenAI will give users a more clear “transition period” when deprecating AI models in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Turley also said OpenAI is already rolling out a new update to make GPT-5’s responses “warmer,” but not sycophantic, such that it won’t reinforce negative behaviors in users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“GPT-5 was just very to the point. I like that. I use the robot personality — I’m German, you know, whatever,” said Turley. “But many people do not, and they really like the fact that ChatGPT would actually check in with you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a delicate balance for OpenAI to strike, especially given that some users have developed dependencies on ChatGPT. Altman said OpenAI believes that less than 1% of ChatGPT users have unhealthy relationships — which could still be tens of millions of people.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Turley said OpenAI worked with mental health experts to develop a rubric to evaluate GPT-5’s answers, ensuring that the AI model will push back on unhealthy behaviors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, it seems that GPT-5 hasn’t hurt OpenAI’s business. In fact, Altman said OpenAI’s API traffic doubled within 48 hours of GPT-5’s launch, and the company is effectively “out of GPUs” thanks to all the demand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In many ways, the night’s contradictions — disappointing launches, record-breaking usage — reflected OpenAI’s strange reality right now.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given OpenAI’s bets on browsers, brain chips, AI chatbots — and others the company is making around data centers, robotics, and energy — Altman clearly has ambitions of running a much bigger company than just the ChatGPT maker. The final form could look something like Google’s parent Alphabet, but perhaps even broader.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As the night wound down, it became clear we weren’t gathered to reflect on GPT-5 at all. We were being pitched on a company that’s eager to outgrow its famous and controversial product. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It seems likely that OpenAI will go public to meet its massive capital demands as part of that picture. In preparation, I think Altman wants to hone his relationship with the media. But he also wants OpenAI to get to a place where it’s no longer defined by its best AI model.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/15/sam-altman-over-bread-rolls-explores-life-after-gpt-5/</guid><pubDate>Fri, 15 Aug 2025 22:20:59 +0000</pubDate></item></channel></rss>