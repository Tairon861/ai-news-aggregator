<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 12 Feb 2026 07:05:09 +0000</lastBuildDate><item><title>Is a secure AI assistant possible? (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/molt-pen2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.&lt;/p&gt;  &lt;p&gt;That might explain why the first breakthrough LLM personal assistant came not from one of the major AI labs, which have to worry about reputation and liability, but from an independent software engineer, Peter Steinberger. In November of 2025, Steinberger uploaded his tool, now called OpenClaw, to GitHub, and in late January the project went viral.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;OpenClaw harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out. The risks posed by OpenClaw are so extensive that it would probably take someone the better part of a week to read all of the security blog posts on it that have cropped up in the past few weeks. The Chinese government took the step of issuing a public warning about OpenClaw’s security vulnerabilities.&lt;/p&gt;  &lt;p&gt;In response to these concerns, Steinberger posted on X that nontechnical people should not use the software. (He did not respond to a request for comment for this article.) But there’s a clear appetite for what OpenClaw is offering, and it’s not limited to people who can run their own software security audits. Any AI companies that hope to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Risk management&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenClaw is, in essence, a mecha suit for LLMs. Users can choose any LLM they like to act as the pilot; that LLM then gains access to improved memory capabilities and the ability to set itself tasks that it repeats on a regular cadence. Unlike the agentic offerings from the major AI companies, OpenClaw agents are meant to be on 24-7, and users can communicate with them using WhatsApp or other messaging apps. That means they can act like a superpowered personal assistant who wakes you each morning with a personalized to-do list, plans vacations while you work, and spins up new apps in its spare time.&lt;/p&gt;  &lt;p&gt;But all that power has consequences. If you want your AI personal assistant to manage your inbox, then you need to give it access to your email—and all the sensitive information contained there. If you want it to make purchases on your behalf, you need to give it your credit card info. And if you want it to do tasks on your computer, such as writing code, it needs some access to your local files.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;There are a few ways this can go wrong. The first is that the AI assistant might make a mistake, as when a user’s Google Antigravity coding agent reportedly wiped his entire hard drive. The second is that someone might gain access to the agent using conventional hacking tools and use it to either extract sensitive data or run malicious code. In the weeks since OpenClaw went viral, security researchers have demonstrated numerous such vulnerabilities that put security-naïve users at risk.&lt;/p&gt;  &lt;p&gt;Both of these dangers can be managed: Some users are choosing to run their OpenClaw agents on separate computers or in the cloud, which protects data on their hard drives from being erased, and other vulnerabilities could be fixed using tried-and-true security approaches.&lt;/p&gt;  &lt;p&gt;But the experts I spoke to for this article were focused on a much more insidious security risk known as prompt injection. Prompt injection is effectively LLM hijacking: Simply by posting malicious text or images on a website that an LLM might peruse, or sending them to an inbox that an LLM reads, attackers can bend it to their will.&lt;/p&gt;  &lt;p&gt;And if that LLM has access to any of its user’s private information, the consequences could be dire. “Using something like OpenClaw is like giving your wallet to a stranger in the street,” says Nicolas Papernot, a professor of electrical and computer engineering at the University of Toronto. Whether or not the major AI companies can feel comfortable offering personal assistants may come down to the quality of the defenses that they can muster against such attacks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;It’s important to note here that prompt injection has not yet caused any catastrophes, or at least none that have been publicly reported. But now that there are likely hundreds of thousands of OpenClaw agents buzzing around the internet, prompt injection might start to look like a much more appealing strategy for cybercriminals. “Tools like this are incentivizing malicious actors to attack a much broader population,” Papernot says.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Building guardrails&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The term “prompt injection” was coined by the popular LLM blogger Simon Willison in 2022, a couple of months before ChatGPT was released. Even back then, it was possible to discern that LLMs would introduce a completely new type of security vulnerability once they came into widespread use. LLMs can’t tell apart the instructions that they receive from users and the data that they use to carry out those instructions, such as emails and web search results—to an LLM, they’re all just text. So if an attacker embeds a few sentences in an email and the LLM mistakes them for an instruction from its user, the attacker can get the LLM to do anything it wants.&lt;/p&gt;  &lt;p&gt;Prompt injection is a tough problem, and it doesn’t seem to be going away anytime soon. “We don’t really have a silver-bullet defense right now,” says Dawn Song, a professor of computer science at UC Berkeley. But there’s a robust academic community working on the problem, and they’ve come up with strategies that could eventually make AI personal assistants safe.&lt;/p&gt;  &lt;p&gt;Technically speaking, it is possible to use OpenClaw today without risking prompt injection: Just don’t connect it to the internet. But restricting OpenClaw from reading your emails, managing your calendar, and doing online research defeats much of the purpose of using an AI assistant. The trick of protecting against prompt injection is to prevent the LLM from responding to hijacking attempts while still giving it room to do its job.&lt;/p&gt; 

 &lt;p&gt;One strategy is to train the LLM to ignore prompt injections. A major part of the LLM development process, called post-training, involves taking a model that knows how to produce realistic text and turning it into a useful assistant by “rewarding” it for answering questions appropriately and “punishing” it when it fails to do so. These rewards and punishments are metaphorical, but the LLM learns from them as an animal would. Using this process, it’s possible to train an LLM not to respond to specific examples of prompt injection.&lt;/p&gt;  &lt;p&gt;But there’s a balance: Train an LLM to reject injected commands too enthusiastically, and it might also start to reject legitimate requests from the user. And because there’s a fundamental element of randomness in LLM behavior, even an LLM that has been very effectively trained to resist prompt injection will likely still slip up every once in a while.&lt;/p&gt;  &lt;p&gt;Another approach involves halting the prompt injection attack before it ever reaches the LLM. Typically, this involves using a specialized detector LLM to determine whether or not the data being sent to the original LLM contains any prompt injections. In a recent study, however, even the best-performing detector completely failed to pick up on certain categories of prompt injection attack.&lt;/p&gt;  &lt;p&gt;The third strategy is more complicated. Rather than controlling the inputs to an LLM by detecting whether or not they contain a prompt injection, the goal is to formulate a policy that guides the LLM’s outputs—i.e., its behaviors—and prevents it from doing anything harmful. Some defenses in this vein are quite simple: If an LLM is allowed to email only a few pre-approved addresses, for example, then it definitely won’t send its user’s credit card information to an attacker. But such a policy would prevent the LLM from completing many useful tasks, such as researching and reaching out to potential professional contacts on behalf of its user.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt;&lt;p&gt;“The challenge is how to accurately define those policies,” says Neil Gong, a professor of electrical and computer engineering at Duke University. “It’s a trade-off between utility and security.”&lt;/p&gt;  &lt;p&gt;On a larger scale, the entire agentic world is wrestling with that trade-off: At what point will agents be secure enough to be useful? Experts disagree. Song, whose startup, Virtue AI, makes an agent security platform, says she thinks it’s possible to safely deploy an AI personal assistant now. But Gong says, “We’re not there yet.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even if AI agents can’t yet be entirely protected against prompt injection, there are certainly ways to mitigate the risks. And it’s possible that some of those techniques could be implemented in OpenClaw. Last week, at the inaugural ClawCon event in San Francisco, Steinberger announced that he’d brought a security person on board to work on the tool.&lt;/p&gt;  &lt;p&gt;As of now, OpenClaw remains vulnerable, though that hasn’t dissuaded its multitude of enthusiastic users. George Pickett, a volunteer maintainer of the OpenGlaw GitHub repository and a fan of the tool, says he’s taken some security measures to keep himself safe while using it: He runs it in the cloud, so that he doesn’t have to worry about accidentally deleting his hard drive, and he’s put mechanisms in place to ensure that no one else can connect to his assistant.&lt;/p&gt;  &lt;p&gt;But he hasn’t taken any specific actions to prevent prompt injection. He’s aware of the risk but says he hasn’t yet seen any reports of it happening with OpenClaw. “Maybe my perspective is a stupid way to look at it, but it’s unlikely that I’ll be the first one to be hacked,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/molt-pen2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 chronoton/executive-summary_0"&gt;&lt;div class="chronotonExecutiveSummary__summaryContainer--3bc10da0658ecd8e2ef22c6023461fb5"&gt;&lt;button class="chronotonExecutiveSummary__toggleButton--1cd6118db37e8a8252fc4cd8bbadcb85" type="button"&gt;&lt;span class="chronotonExecutiveSummary__toggleText--a713e14989fe65c5162db2b69cb422c9"&gt;EXECUTIVE SUMMARY&lt;/span&gt;&lt;svg class="chronotonExecutiveSummary__toggleArrow--a8c1be9c06a9f066a767b3af80d859a1" fill="none" height="7" viewBox="0 0 11 7" width="11" xmlns="http://www.w3.org/2000/svg"&gt;&lt;path d="M1 1L5.5 5.5L10 1" stroke="#58C0B3" stroke-linecap="round" stroke-width="1.5"&gt;&lt;/svg&gt;&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;AI agents are a risky business. Even when stuck inside the chatbox window, LLMs will make mistakes and behave badly. Once they have tools that they can use to interact with the outside world, such as web browsers and email addresses, the consequences of those mistakes become far more serious.&lt;/p&gt;  &lt;p&gt;That might explain why the first breakthrough LLM personal assistant came not from one of the major AI labs, which have to worry about reputation and liability, but from an independent software engineer, Peter Steinberger. In November of 2025, Steinberger uploaded his tool, now called OpenClaw, to GitHub, and in late January the project went viral.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;OpenClaw harnesses existing LLMs to let users create their own bespoke assistants. For some users, this means handing over reams of personal data, from years of emails to the contents of their hard drive. That has security experts thoroughly freaked out. The risks posed by OpenClaw are so extensive that it would probably take someone the better part of a week to read all of the security blog posts on it that have cropped up in the past few weeks. The Chinese government took the step of issuing a public warning about OpenClaw’s security vulnerabilities.&lt;/p&gt;  &lt;p&gt;In response to these concerns, Steinberger posted on X that nontechnical people should not use the software. (He did not respond to a request for comment for this article.) But there’s a clear appetite for what OpenClaw is offering, and it’s not limited to people who can run their own software security audits. Any AI companies that hope to get in on the personal assistant business will need to figure out how to build a system that will keep users’ data safe and secure. To do so, they’ll need to borrow approaches from the cutting edge of agent security research.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Risk management&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;OpenClaw is, in essence, a mecha suit for LLMs. Users can choose any LLM they like to act as the pilot; that LLM then gains access to improved memory capabilities and the ability to set itself tasks that it repeats on a regular cadence. Unlike the agentic offerings from the major AI companies, OpenClaw agents are meant to be on 24-7, and users can communicate with them using WhatsApp or other messaging apps. That means they can act like a superpowered personal assistant who wakes you each morning with a personalized to-do list, plans vacations while you work, and spins up new apps in its spare time.&lt;/p&gt;  &lt;p&gt;But all that power has consequences. If you want your AI personal assistant to manage your inbox, then you need to give it access to your email—and all the sensitive information contained there. If you want it to make purchases on your behalf, you need to give it your credit card info. And if you want it to do tasks on your computer, such as writing code, it needs some access to your local files.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;There are a few ways this can go wrong. The first is that the AI assistant might make a mistake, as when a user’s Google Antigravity coding agent reportedly wiped his entire hard drive. The second is that someone might gain access to the agent using conventional hacking tools and use it to either extract sensitive data or run malicious code. In the weeks since OpenClaw went viral, security researchers have demonstrated numerous such vulnerabilities that put security-naïve users at risk.&lt;/p&gt;  &lt;p&gt;Both of these dangers can be managed: Some users are choosing to run their OpenClaw agents on separate computers or in the cloud, which protects data on their hard drives from being erased, and other vulnerabilities could be fixed using tried-and-true security approaches.&lt;/p&gt;  &lt;p&gt;But the experts I spoke to for this article were focused on a much more insidious security risk known as prompt injection. Prompt injection is effectively LLM hijacking: Simply by posting malicious text or images on a website that an LLM might peruse, or sending them to an inbox that an LLM reads, attackers can bend it to their will.&lt;/p&gt;  &lt;p&gt;And if that LLM has access to any of its user’s private information, the consequences could be dire. “Using something like OpenClaw is like giving your wallet to a stranger in the street,” says Nicolas Papernot, a professor of electrical and computer engineering at the University of Toronto. Whether or not the major AI companies can feel comfortable offering personal assistants may come down to the quality of the defenses that they can muster against such attacks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_7"&gt; &lt;p&gt;It’s important to note here that prompt injection has not yet caused any catastrophes, or at least none that have been publicly reported. But now that there are likely hundreds of thousands of OpenClaw agents buzzing around the internet, prompt injection might start to look like a much more appealing strategy for cybercriminals. “Tools like this are incentivizing malicious actors to attack a much broader population,” Papernot says.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Building guardrails&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The term “prompt injection” was coined by the popular LLM blogger Simon Willison in 2022, a couple of months before ChatGPT was released. Even back then, it was possible to discern that LLMs would introduce a completely new type of security vulnerability once they came into widespread use. LLMs can’t tell apart the instructions that they receive from users and the data that they use to carry out those instructions, such as emails and web search results—to an LLM, they’re all just text. So if an attacker embeds a few sentences in an email and the LLM mistakes them for an instruction from its user, the attacker can get the LLM to do anything it wants.&lt;/p&gt;  &lt;p&gt;Prompt injection is a tough problem, and it doesn’t seem to be going away anytime soon. “We don’t really have a silver-bullet defense right now,” says Dawn Song, a professor of computer science at UC Berkeley. But there’s a robust academic community working on the problem, and they’ve come up with strategies that could eventually make AI personal assistants safe.&lt;/p&gt;  &lt;p&gt;Technically speaking, it is possible to use OpenClaw today without risking prompt injection: Just don’t connect it to the internet. But restricting OpenClaw from reading your emails, managing your calendar, and doing online research defeats much of the purpose of using an AI assistant. The trick of protecting against prompt injection is to prevent the LLM from responding to hijacking attempts while still giving it room to do its job.&lt;/p&gt; 

 &lt;p&gt;One strategy is to train the LLM to ignore prompt injections. A major part of the LLM development process, called post-training, involves taking a model that knows how to produce realistic text and turning it into a useful assistant by “rewarding” it for answering questions appropriately and “punishing” it when it fails to do so. These rewards and punishments are metaphorical, but the LLM learns from them as an animal would. Using this process, it’s possible to train an LLM not to respond to specific examples of prompt injection.&lt;/p&gt;  &lt;p&gt;But there’s a balance: Train an LLM to reject injected commands too enthusiastically, and it might also start to reject legitimate requests from the user. And because there’s a fundamental element of randomness in LLM behavior, even an LLM that has been very effectively trained to resist prompt injection will likely still slip up every once in a while.&lt;/p&gt;  &lt;p&gt;Another approach involves halting the prompt injection attack before it ever reaches the LLM. Typically, this involves using a specialized detector LLM to determine whether or not the data being sent to the original LLM contains any prompt injections. In a recent study, however, even the best-performing detector completely failed to pick up on certain categories of prompt injection attack.&lt;/p&gt;  &lt;p&gt;The third strategy is more complicated. Rather than controlling the inputs to an LLM by detecting whether or not they contain a prompt injection, the goal is to formulate a policy that guides the LLM’s outputs—i.e., its behaviors—and prevents it from doing anything harmful. Some defenses in this vein are quite simple: If an LLM is allowed to email only a few pre-approved addresses, for example, then it definitely won’t send its user’s credit card information to an attacker. But such a policy would prevent the LLM from completing many useful tasks, such as researching and reaching out to potential professional contacts on behalf of its user.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt;&lt;p&gt;“The challenge is how to accurately define those policies,” says Neil Gong, a professor of electrical and computer engineering at Duke University. “It’s a trade-off between utility and security.”&lt;/p&gt;  &lt;p&gt;On a larger scale, the entire agentic world is wrestling with that trade-off: At what point will agents be secure enough to be useful? Experts disagree. Song, whose startup, Virtue AI, makes an agent security platform, says she thinks it’s possible to safely deploy an AI personal assistant now. But Gong says, “We’re not there yet.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even if AI agents can’t yet be entirely protected against prompt injection, there are certainly ways to mitigate the risks. And it’s possible that some of those techniques could be implemented in OpenClaw. Last week, at the inaugural ClawCon event in San Francisco, Steinberger announced that he’d brought a security person on board to work on the tool.&lt;/p&gt;  &lt;p&gt;As of now, OpenClaw remains vulnerable, though that hasn’t dissuaded its multitude of enthusiastic users. George Pickett, a volunteer maintainer of the OpenGlaw GitHub repository and a fan of the tool, says he’s taken some security measures to keep himself safe while using it: He runs it in the cloud, so that he doesn’t have to worry about accidentally deleting his hard drive, and he’s put mechanisms in place to ensure that no one else can connect to his assistant.&lt;/p&gt;  &lt;p&gt;But he hasn’t taken any specific actions to prevent prompt injection. He’s aware of the risk but says he hasn’t yet seen any reports of it happening with OpenClaw. “Maybe my perspective is a stupid way to look at it, but it’s unlikely that I’ll be the first one to be hacked,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/11/1132768/is-a-secure-ai-assistant-possible/</guid><pubDate>Wed, 11 Feb 2026 20:08:35 +0000</pubDate></item><item><title>OpenAI researcher quits over ChatGPT ads, warns of "Facebook" path (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/02/openai-researcher-quits-over-fears-that-chatgpt-ads-could-manipulate-users/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Zoë Hitzig resigned on the same day OpenAI began testing ads in its chatbot.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Wednesday, former OpenAI researcher Zoë Hitzig published a guest essay in The New York Times announcing that she resigned from the company on Monday, the same day OpenAI began testing advertisements inside ChatGPT. Hitzig, an economist and published poet who holds a junior fellowship at the Harvard Society of Fellows, spent two years at OpenAI helping shape how its AI models were built and priced. She wrote that OpenAI’s advertising strategy risks repeating the same mistakes that Facebook made a decade ago.&lt;/p&gt;
&lt;p&gt;“I once believed I could help the people building A.I. get ahead of the problems it would create,” Hitzig wrote. “This week confirmed my slow realization that OpenAI seems to have stopped asking the questions I’d joined to help answer.”&lt;/p&gt;
&lt;p&gt;Hitzig did not call advertising itself immoral. Instead, she argued that the nature of the data at stake makes ChatGPT ads especially risky. Users have shared medical fears, relationship problems, and religious beliefs with the chatbot, she wrote, often “because people believed they were talking to something that had no ulterior agenda.” She called this accumulated record of personal disclosures “an archive of human candor that has no precedent.”&lt;/p&gt;
&lt;p&gt;She also drew a direct parallel to Facebook’s early history, noting that the social media company once promised users control over their data and the ability to vote on policy changes. Those pledges eroded over time, Hitzig wrote, and the Federal Trade Commission found that privacy changes Facebook marketed as giving users more control actually did the opposite.&lt;/p&gt;
&lt;p&gt;She warned that a similar trajectory could play out with ChatGPT: “I believe the first iteration of ads will probably follow those principles. But I’m worried subsequent iterations won’t, because the company is building an economic engine that creates strong incentives to override its own rules.”&lt;/p&gt;
&lt;h2&gt;Ads arrive after a week of AI industry sparring&lt;/h2&gt;
&lt;p&gt;Hitzig’s resignation adds another voice to a growing debate over advertising in AI chatbots. OpenAI announced in January that it would begin testing ads in the US for users on its free and $8-per-month “Go” subscription tiers, while paid Plus, Pro, Business, Enterprise, and Education subscribers would not see ads. The company said ads would appear at the bottom of ChatGPT responses, be clearly labeled, and would not influence the chatbot’s answers.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The rollout on Sunday followed a week of public jabs between OpenAI and its rival, Anthropic. Anthropic declared Claude would remain ad-free, then ran Super Bowl ads with the tagline “Ads are coming to AI. But not to Claude,” which depicted AI chatbots awkwardly inserting product placements into personal conversations.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman called the ads “funny” but “clearly dishonest,” writing on X that OpenAI “would obviously never run ads in the way Anthropic depicts them.” He framed the ad-supported model as a way to bring AI to users who cannot afford subscriptions, writing that “Anthropic serves an expensive product to rich people.”&lt;/p&gt;
&lt;p&gt;Anthropic responded as part of an advertising campaign of its own that including ads in conversations with its Claude chatbot “would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.” The company said more than 80 percent of its revenue comes from enterprise customers.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;What Hitzig saw from the inside&lt;/h2&gt;
&lt;p&gt;Regardless of the debate over whether AI chatbots should carry ads, OpenAI’s support documentation reveals that ad personalization is enabled by default for users in the test. If left on, ads will be selected using information from current and past chat threads, as well as past ad interactions. Advertisers do not receive users’ chats or personal details, OpenAI says, and ads will not appear near conversations about health, mental health, or politics.&lt;/p&gt;
&lt;p&gt;In her essay, Hitzig pointed to what she called an existing tension in OpenAI’s principles. She noted that while the company states it does not optimize for user activity solely to generate advertising revenue, reporting has suggested that OpenAI already optimizes for daily active users, “likely by encouraging the model to be more flattering and sycophantic.”&lt;/p&gt;
&lt;p&gt;She warned that this optimization can make users feel more dependent on AI models for support, pointing to psychiatrists who have documented instances of “chatbot psychosis” and allegations that ChatGPT reinforced suicidal ideation.&lt;/p&gt;
&lt;p&gt;OpenAI currently faces multiple wrongful death lawsuits, including one alleging ChatGPT helped a teenager plan his suicide and another alleging it validated a man’s paranoid delusions about his mother before a murder-suicide.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Rather than framing the debate as ads versus no ads, Hitzig proposed several structural alternatives. These included cross-subsidies modeled on the FCC’s universal service fund (in which businesses that pay for high-value AI labor would subsidize free access for others), independent oversight boards with binding authority over how conversational data is used in ad targeting, and data trusts or cooperatives in which users retain control of their information. She pointed to the Swiss cooperative MIDATA and Germany’s co-determination laws as partial precedents.&lt;/p&gt;
&lt;p&gt;Hitzig closed her essay with what she described as the two outcomes she fears most: “a technology that manipulates the people who use it at no cost, and one that exclusively benefits the few who can afford to use it.”&lt;/p&gt;
&lt;h2&gt;A changing of the AI seasons&lt;/h2&gt;
&lt;p&gt;Hitzig was not the only prominent AI researcher to publicly resign this week. On Sunday, Mrinank Sharma, who led Anthropic’s Safeguards Research Team and co-authored a widely cited 2023 study on AI sycophancy, announced his departure in a letter warning that “the world is in peril.” He wrote that he had “repeatedly seen how hard it is to truly let our values govern our actions” inside the organization and said he plans to pursue a poetry degree (Hitzig, coincidentally, is also a published poet).&lt;/p&gt;
&lt;p&gt;On Monday, xAI co-founder Yuhuai “Tony” Wu also resigned, followed the next day by fellow co-founder Jimmy Ba. They were part of a larger wave: at least nine&amp;nbsp;xAI employees, including the two co-founders, publicly announced their departures over the past week, according to TechCrunch. Six of the company’s 12 original co-founders have now left.&lt;/p&gt;
&lt;p&gt;The departures follow Elon Musk’s decision to merge xAI with SpaceX in an all-stock deal ahead of a planned IPO, a transaction that converted xAI equity into shares of a company valued at $1.25 trillion, though it is unclear whether the timing of the departures is related to vesting schedules.&lt;/p&gt;
&lt;p&gt;The three sets of departures across OpenAI, Anthropic, and xAI appear unrelated in their specifics, but they arrive during a period of rapid commercialization across the AI industry that has tested the patience of researchers at multiple companies, and they fit a broader pattern of turnover and burnout that has become common at major AI labs.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Zoë Hitzig resigned on the same day OpenAI began testing ads in its chatbot.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/open-ai-monkey-ad-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson | Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Wednesday, former OpenAI researcher Zoë Hitzig published a guest essay in The New York Times announcing that she resigned from the company on Monday, the same day OpenAI began testing advertisements inside ChatGPT. Hitzig, an economist and published poet who holds a junior fellowship at the Harvard Society of Fellows, spent two years at OpenAI helping shape how its AI models were built and priced. She wrote that OpenAI’s advertising strategy risks repeating the same mistakes that Facebook made a decade ago.&lt;/p&gt;
&lt;p&gt;“I once believed I could help the people building A.I. get ahead of the problems it would create,” Hitzig wrote. “This week confirmed my slow realization that OpenAI seems to have stopped asking the questions I’d joined to help answer.”&lt;/p&gt;
&lt;p&gt;Hitzig did not call advertising itself immoral. Instead, she argued that the nature of the data at stake makes ChatGPT ads especially risky. Users have shared medical fears, relationship problems, and religious beliefs with the chatbot, she wrote, often “because people believed they were talking to something that had no ulterior agenda.” She called this accumulated record of personal disclosures “an archive of human candor that has no precedent.”&lt;/p&gt;
&lt;p&gt;She also drew a direct parallel to Facebook’s early history, noting that the social media company once promised users control over their data and the ability to vote on policy changes. Those pledges eroded over time, Hitzig wrote, and the Federal Trade Commission found that privacy changes Facebook marketed as giving users more control actually did the opposite.&lt;/p&gt;
&lt;p&gt;She warned that a similar trajectory could play out with ChatGPT: “I believe the first iteration of ads will probably follow those principles. But I’m worried subsequent iterations won’t, because the company is building an economic engine that creates strong incentives to override its own rules.”&lt;/p&gt;
&lt;h2&gt;Ads arrive after a week of AI industry sparring&lt;/h2&gt;
&lt;p&gt;Hitzig’s resignation adds another voice to a growing debate over advertising in AI chatbots. OpenAI announced in January that it would begin testing ads in the US for users on its free and $8-per-month “Go” subscription tiers, while paid Plus, Pro, Business, Enterprise, and Education subscribers would not see ads. The company said ads would appear at the bottom of ChatGPT responses, be clearly labeled, and would not influence the chatbot’s answers.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The rollout on Sunday followed a week of public jabs between OpenAI and its rival, Anthropic. Anthropic declared Claude would remain ad-free, then ran Super Bowl ads with the tagline “Ads are coming to AI. But not to Claude,” which depicted AI chatbots awkwardly inserting product placements into personal conversations.&lt;/p&gt;
&lt;p&gt;OpenAI CEO Sam Altman called the ads “funny” but “clearly dishonest,” writing on X that OpenAI “would obviously never run ads in the way Anthropic depicts them.” He framed the ad-supported model as a way to bring AI to users who cannot afford subscriptions, writing that “Anthropic serves an expensive product to rich people.”&lt;/p&gt;
&lt;p&gt;Anthropic responded as part of an advertising campaign of its own that including ads in conversations with its Claude chatbot “would be incompatible with what we want Claude to be: a genuinely helpful assistant for work and for deep thinking.” The company said more than 80 percent of its revenue comes from enterprise customers.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;What Hitzig saw from the inside&lt;/h2&gt;
&lt;p&gt;Regardless of the debate over whether AI chatbots should carry ads, OpenAI’s support documentation reveals that ad personalization is enabled by default for users in the test. If left on, ads will be selected using information from current and past chat threads, as well as past ad interactions. Advertisers do not receive users’ chats or personal details, OpenAI says, and ads will not appear near conversations about health, mental health, or politics.&lt;/p&gt;
&lt;p&gt;In her essay, Hitzig pointed to what she called an existing tension in OpenAI’s principles. She noted that while the company states it does not optimize for user activity solely to generate advertising revenue, reporting has suggested that OpenAI already optimizes for daily active users, “likely by encouraging the model to be more flattering and sycophantic.”&lt;/p&gt;
&lt;p&gt;She warned that this optimization can make users feel more dependent on AI models for support, pointing to psychiatrists who have documented instances of “chatbot psychosis” and allegations that ChatGPT reinforced suicidal ideation.&lt;/p&gt;
&lt;p&gt;OpenAI currently faces multiple wrongful death lawsuits, including one alleging ChatGPT helped a teenager plan his suicide and another alleging it validated a man’s paranoid delusions about his mother before a murder-suicide.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Rather than framing the debate as ads versus no ads, Hitzig proposed several structural alternatives. These included cross-subsidies modeled on the FCC’s universal service fund (in which businesses that pay for high-value AI labor would subsidize free access for others), independent oversight boards with binding authority over how conversational data is used in ad targeting, and data trusts or cooperatives in which users retain control of their information. She pointed to the Swiss cooperative MIDATA and Germany’s co-determination laws as partial precedents.&lt;/p&gt;
&lt;p&gt;Hitzig closed her essay with what she described as the two outcomes she fears most: “a technology that manipulates the people who use it at no cost, and one that exclusively benefits the few who can afford to use it.”&lt;/p&gt;
&lt;h2&gt;A changing of the AI seasons&lt;/h2&gt;
&lt;p&gt;Hitzig was not the only prominent AI researcher to publicly resign this week. On Sunday, Mrinank Sharma, who led Anthropic’s Safeguards Research Team and co-authored a widely cited 2023 study on AI sycophancy, announced his departure in a letter warning that “the world is in peril.” He wrote that he had “repeatedly seen how hard it is to truly let our values govern our actions” inside the organization and said he plans to pursue a poetry degree (Hitzig, coincidentally, is also a published poet).&lt;/p&gt;
&lt;p&gt;On Monday, xAI co-founder Yuhuai “Tony” Wu also resigned, followed the next day by fellow co-founder Jimmy Ba. They were part of a larger wave: at least nine&amp;nbsp;xAI employees, including the two co-founders, publicly announced their departures over the past week, according to TechCrunch. Six of the company’s 12 original co-founders have now left.&lt;/p&gt;
&lt;p&gt;The departures follow Elon Musk’s decision to merge xAI with SpaceX in an all-stock deal ahead of a planned IPO, a transaction that converted xAI equity into shares of a company valued at $1.25 trillion, though it is unclear whether the timing of the departures is related to vesting schedules.&lt;/p&gt;
&lt;p&gt;The three sets of departures across OpenAI, Anthropic, and xAI appear unrelated in their specifics, but they arrive during a period of rapid commercialization across the AI industry that has tested the patience of researchers at multiple companies, and they fit a broader pattern of turnover and burnout that has become common at major AI labs.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/02/openai-researcher-quits-over-fears-that-chatgpt-ads-could-manipulate-users/</guid><pubDate>Wed, 11 Feb 2026 20:44:19 +0000</pubDate></item><item><title>Elon Musk suggests spate of xAI exits have been push, not pull (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2199701496.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk is addressing a wave of departures from xAI, including two more co-founders who left this week, bringing the total to six out of the original 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At an all-hands meeting Tuesday night, Musk suggested the exits were about fit, not performance. “Because we’ve reached a certain scale, we’re organizing the company to be more effective at this scale,” he said, according to The New York Times. “And actually, when this happens, there’s some people who are better suited for the early stages of a company and less suited for the later stages.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wednesday afternoon on X, he went further, making clear these departures weren’t voluntary. “xAI was reorganized a few days ago to improve speed of execution,” Musk wrote. “As a company grows, especially as quickly as xAI, the structure must evolve just like any living organism. This unfortunately required parting ways with some people.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the company is “hiring aggressively” and closed with a quintessentially Musk pitch: “Join xAI if the idea of mass drivers on the Moon appeals to you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Losing half your co-founders in a relatively short period raises questions, and Musk’s comments seem designed to control the narrative, reframing the exits as necessary rather than a problem for the outfit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In total, at least nine engineers, including the two co-founders, have publicly announced their departure from xAI in the past week — though two of those exits appear to have occurred a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Three of the departing staff members have said they will be starting something new alongside other former xAI engineers, although no details are available about the new venture. Others have hinted at a desire for more autonomy and smaller teams to build frontier tech more rapidly, pointing to the anticipated surge in AI productivity.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Yuhuai (Tony) Wu, an xAI co-founder and reasoning lead, said in a post announcing his resignation: “It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training at xAI and previously worked at Twitter/X, said last week he was leaving to “start something new.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Career update: I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.&lt;/p&gt;&lt;p&gt;xAI is truly an extraordinary place. The team is incredibly hardcore and talented, shipping at a pace that shouldn’t be possible. From the Home… pic.twitter.com/HKWOebg9QI&lt;/p&gt;— Shayan (@shayan_) February 7, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Vahid Kazemi, who had a brief stint working on machine learning, posted Tuesday that he left a few weeks ago, adding: “IMO, all AI labs are building the exact same thing, and it’s boring … So, I’m starting something new.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roland Gavrilescu, a former xAI engineer, left in November to start Nuraline, a company building “forward-deployed AI agents,” but posted again on Tuesday that he left the firm to build “something new with others that left xAI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The departures come at a moment of significant controversy for xAI. The company is facing regulatory scrutiny after Grok created nonconsensual explicit deepfakes of women and children that were disseminated on X — French authorities last week raided X offices as part of an investigation. The company is also moving toward a planned IPO later this year, after being legally acquired by SpaceX last week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk is also facing personal controversy after files published by the Justice Department show extended conversations with convicted rapist and sex trafficker Jeffrey Epstein. The emails show Musk discussing a visit to Epstein’s island on two separate occasions, in 2012 and 2013. Epstein was first convicted of procuring a child for prostitution in 2008.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI maintains a headcount of over 1,000 employees, so the departures are unlikely to affect the company’s short-term capabilities. Still, the rapid pace of the recent departures had taken on a life of its own online, with users jokingly announcing on X that they too are “leaving xAI” despite never having worked there — a sign of how quickly the narrative of a “mass exodus” snowballed on Musk’s social network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, forced co-founder exits are rarely a sign of smooth scaling. While Musk frames the reorganization as calculated, the fact that several engineers followed the co-founders out the door — and that at least three are starting something new together — suggests the departures may also reflect deeper tensions. In frontier AI, where talent is scarce and reputation matters, xAI’s ability to attract and retain top researchers will be tested as it competes with OpenAI, Anthropic, and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI for more information.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-timeline-of-departure-announcements"&gt;Timeline of departure announcements&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The following employees have publicly announced their departures from xAI on X in recent days:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 6:&amp;nbsp; &lt;/strong&gt;Ayush Jaiswal, engineer, wrote: “This was my last week at xAI. Will be taking a few months to spend time with family &amp;amp; tinker with AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 7: &lt;/strong&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training and was previously at X, wrote: “I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.” He added that working closely with Elon Musk taught him “obsessive attention to detail, maniacal urgency, and to think from first principles.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9: &lt;/strong&gt;Simon Zhai, MTS (member of technical staff), wrote: “Today is my last day at xAI, feeling very fortunate about the opportunity. It has been an amazing journey.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9:&lt;/strong&gt; Yuhuai (Tony) Wu, co-founder and reasoning lead, wrote: “I resigned from xAI today. It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Jimmy Ba, co-founder and research/safety lead, wrote: “Last day at xAI. We are heading to an age of 100x productivity with the right tools. Recursive self improvement loops likely go live in the next 12 months. It’s time to recalibrate my gradient on the big picture. 2026 is gonna be insane and likely the busiest (and most consequential) year for the future of our species.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Vahid Kazemi, an ML PhD, wrote that he had left xAI “a few weeks ago,” adding: “IMO, all AI labs are building the exact same thing, and it’s boring. I think there’s room for more creativity. So, I’m starting something new.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Hang Gao, who worked on multimodal efforts, including Grok Imagine, wrote: “I left xAI today.” He described his time there as “truly rewarding,” citing contributions to Grok Imagine’s releases and praising the team’s “humble craftsmanship and ambitious vision.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Roland Gavrilescu, the engineer who left in November to start Nuraline, posted: “I left xAI. Building something new with others that left xAI. We’re hiring :)”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Chace Lee, a member of the Macrohard founding team, wrote: “Taking a brief reset, then back to the frontier.” (Macrohard is an AI-only software venture under xAI designed to fully automate software development, coding, and operations using Grok-powered, multi-agent systems. Its name is a dig at Microsoft.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com&lt;/em&gt; &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at @rebeccabellan.491&lt;/em&gt; &lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/07/GettyImages-2199701496.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk is addressing a wave of departures from xAI, including two more co-founders who left this week, bringing the total to six out of the original 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At an all-hands meeting Tuesday night, Musk suggested the exits were about fit, not performance. “Because we’ve reached a certain scale, we’re organizing the company to be more effective at this scale,” he said, according to The New York Times. “And actually, when this happens, there’s some people who are better suited for the early stages of a company and less suited for the later stages.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Wednesday afternoon on X, he went further, making clear these departures weren’t voluntary. “xAI was reorganized a few days ago to improve speed of execution,” Musk wrote. “As a company grows, especially as quickly as xAI, the structure must evolve just like any living organism. This unfortunately required parting ways with some people.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the company is “hiring aggressively” and closed with a quintessentially Musk pitch: “Join xAI if the idea of mass drivers on the Moon appeals to you.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Losing half your co-founders in a relatively short period raises questions, and Musk’s comments seem designed to control the narrative, reframing the exits as necessary rather than a problem for the outfit.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In total, at least nine engineers, including the two co-founders, have publicly announced their departure from xAI in the past week — though two of those exits appear to have occurred a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Three of the departing staff members have said they will be starting something new alongside other former xAI engineers, although no details are available about the new venture. Others have hinted at a desire for more autonomy and smaller teams to build frontier tech more rapidly, pointing to the anticipated surge in AI productivity.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Yuhuai (Tony) Wu, an xAI co-founder and reasoning lead, said in a post announcing his resignation: “It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training at xAI and previously worked at Twitter/X, said last week he was leaving to “start something new.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;Career update: I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.&lt;/p&gt;&lt;p&gt;xAI is truly an extraordinary place. The team is incredibly hardcore and talented, shipping at a pace that shouldn’t be possible. From the Home… pic.twitter.com/HKWOebg9QI&lt;/p&gt;— Shayan (@shayan_) February 7, 2026&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Vahid Kazemi, who had a brief stint working on machine learning, posted Tuesday that he left a few weeks ago, adding: “IMO, all AI labs are building the exact same thing, and it’s boring … So, I’m starting something new.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Roland Gavrilescu, a former xAI engineer, left in November to start Nuraline, a company building “forward-deployed AI agents,” but posted again on Tuesday that he left the firm to build “something new with others that left xAI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The departures come at a moment of significant controversy for xAI. The company is facing regulatory scrutiny after Grok created nonconsensual explicit deepfakes of women and children that were disseminated on X — French authorities last week raided X offices as part of an investigation. The company is also moving toward a planned IPO later this year, after being legally acquired by SpaceX last week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Musk is also facing personal controversy after files published by the Justice Department show extended conversations with convicted rapist and sex trafficker Jeffrey Epstein. The emails show Musk discussing a visit to Epstein’s island on two separate occasions, in 2012 and 2013. Epstein was first convicted of procuring a child for prostitution in 2008.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI maintains a headcount of over 1,000 employees, so the departures are unlikely to affect the company’s short-term capabilities. Still, the rapid pace of the recent departures had taken on a life of its own online, with users jokingly announcing on X that they too are “leaving xAI” despite never having worked there — a sign of how quickly the narrative of a “mass exodus” snowballed on Musk’s social network.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, forced co-founder exits are rarely a sign of smooth scaling. While Musk frames the reorganization as calculated, the fact that several engineers followed the co-founders out the door — and that at least three are starting something new together — suggests the departures may also reflect deeper tensions. In frontier AI, where talent is scarce and reputation matters, xAI’s ability to attract and retain top researchers will be tested as it competes with OpenAI, Anthropic, and Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI for more information.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-timeline-of-departure-announcements"&gt;Timeline of departure announcements&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The following employees have publicly announced their departures from xAI on X in recent days:&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 6:&amp;nbsp; &lt;/strong&gt;Ayush Jaiswal, engineer, wrote: “This was my last week at xAI. Will be taking a few months to spend time with family &amp;amp; tinker with AI.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 7: &lt;/strong&gt;Shayan Salehian, who worked on product infrastructure and model behavior post-training and was previously at X, wrote: “I left xAI to start something new, closing my 7+ year chapter working at Twitter, X, and xAI with so much gratitude.” He added that working closely with Elon Musk taught him “obsessive attention to detail, maniacal urgency, and to think from first principles.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9: &lt;/strong&gt;Simon Zhai, MTS (member of technical staff), wrote: “Today is my last day at xAI, feeling very fortunate about the opportunity. It has been an amazing journey.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 9:&lt;/strong&gt; Yuhuai (Tony) Wu, co-founder and reasoning lead, wrote: “I resigned from xAI today. It’s time for my next chapter. It is an era with full possibilities: a small team armed with AIs can move mountains and redefine what’s possible.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Jimmy Ba, co-founder and research/safety lead, wrote: “Last day at xAI. We are heading to an age of 100x productivity with the right tools. Recursive self improvement loops likely go live in the next 12 months. It’s time to recalibrate my gradient on the big picture. 2026 is gonna be insane and likely the busiest (and most consequential) year for the future of our species.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Vahid Kazemi, an ML PhD, wrote that he had left xAI “a few weeks ago,” adding: “IMO, all AI labs are building the exact same thing, and it’s boring. I think there’s room for more creativity. So, I’m starting something new.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Hang Gao, who worked on multimodal efforts, including Grok Imagine, wrote: “I left xAI today.” He described his time there as “truly rewarding,” citing contributions to Grok Imagine’s releases and praising the team’s “humble craftsmanship and ambitious vision.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Roland Gavrilescu, the engineer who left in November to start Nuraline, posted: “I left xAI. Building something new with others that left xAI. We’re hiring :)”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;February 10:&lt;/strong&gt; Chace Lee, a member of the Macrohard founding team, wrote: “Taking a brief reset, then back to the frontier.” (Macrohard is an AI-only software venture under xAI designed to fully automate software development, coding, and operations using Grok-powered, multi-agent systems. Its name is a dig at Microsoft.)&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at rebecca.bellan@techcrunch.com&lt;/em&gt; &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at @rebeccabellan.491&lt;/em&gt; &lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/senior-engineers-including-co-founders-exit-xai-amid-controversy/</guid><pubDate>Wed, 11 Feb 2026 20:47:00 +0000</pubDate></item><item><title>Who will own your company’s AI layer? Glean’s CEO explains (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/who-will-own-your-companys-ai-layer-gleans-ceo-explains/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2259183614.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30922151"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit underneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — the startup raised&amp;nbsp;$150 million last year at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;more competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as Equity&amp;nbsp;host Rebecca Bellan sits down with Glean’s CEO and founder Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2259183614.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;div class="jwppp-video-box" id="jwppp-video-box-30922151"&gt;






&lt;span class="jwppp-instant"&gt;&lt;/span&gt;&lt;p&gt;Loading the player…&lt;/p&gt;
&lt;/div&gt;



&lt;p class="wp-block-paragraph"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit underneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — the startup raised&amp;nbsp;$150 million last year at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;more competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Watch as Equity&amp;nbsp;host Rebecca Bellan sits down with Glean’s CEO and founder Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube,&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/who-will-own-your-companys-ai-layer-gleans-ceo-explains/</guid><pubDate>Wed, 11 Feb 2026 20:49:07 +0000</pubDate></item><item><title>Glean’s fight to own the AI layer inside every company (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/glean-arvind-jain-equity-podcast-own-the-ai-layer-inside-every-company/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Arvind-Jain-headshot.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit beneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — last June, the startup raised&amp;nbsp;$150 million at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan sits down with Glean founder and CEO Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the AI agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The fight between bundled AI from tech titans like Microsoft and&amp;nbsp;Google,&amp;nbsp;and platform layers&amp;nbsp;like&amp;nbsp;Glean and its competitors.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How AI adoption is reshaping leadership and organizational design.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why permissions and governance are harder problems than most companies realize.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube, Apple Podcasts, Overcast, Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X&amp;nbsp;and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Arvind-Jain-headshot.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Enterprise AI is shifting fast from chatbots that answer questions to systems that actually do the work across an organization. But who will own the AI layer that powers all of it?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Glean, which started as an enterprise search product, has evolved into what it calls an “AI work assistant,” aiming to&amp;nbsp;sit beneath other AI experiences, connecting to internal systems, managing permissions, and delivering intelligence wherever employees work.&amp;nbsp;Investors are buying into&amp;nbsp;the vision, too — last June, the startup raised&amp;nbsp;$150 million at a&amp;nbsp;$7.2 billion&amp;nbsp;valuation&amp;nbsp;as&amp;nbsp;competition&amp;nbsp;heats up&amp;nbsp;against tech giants bundling&amp;nbsp;AI.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On this episode of TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Rebecca Bellan sits down with Glean founder and CEO Arvind Jain at Web Summit Qatar to break down how enterprises are thinking about AI architecture, what’s driving consolidation, and&amp;nbsp;what’s&amp;nbsp;real versus hype in the AI agent space.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The fight between bundled AI from tech titans like Microsoft and&amp;nbsp;Google,&amp;nbsp;and platform layers&amp;nbsp;like&amp;nbsp;Glean and its competitors.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;How AI adoption is reshaping leadership and organizational design.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why permissions and governance are harder problems than most companies realize.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;YouTube, Apple Podcasts, Overcast, Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X&amp;nbsp;and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/glean-arvind-jain-equity-podcast-own-the-ai-layer-inside-every-company/</guid><pubDate>Wed, 11 Feb 2026 21:07:28 +0000</pubDate></item><item><title>Uber Eats launches AI assistant to help with grocery cart creation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/uber-eats-launches-ai-assistant-to-help-with-grocery-cart-creation/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Uber Eats announced a new AI feature, “Cart Assistant,” on Wednesday designed to fill customers’ grocery carts faster and easier. The beta version is now available in the app.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the new chatbot, users search for a grocery store in the Uber Eats app and tap the purple Cart Assistant icon on the store’s page to begin shopping.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Customers can enter a list or upload an image of one, and Cart Assistant will automatically add the necessary items to their basket. This includes photos of handwritten lists or screenshots of recipes and their ingredients. Users can then customize the basket by swapping items for preferred brands or adding more products from the store.​&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3092211" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/UberEats-cart-assistant.gif?w=346" width="346" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Uber Eats&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Uber Eats notes that Cart Assistant uses previous orders to prioritize familiar items — like your usual milk or favorite oatmeal — to make the experience more personalized.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Users were telling us they wanted a quicker way to shop, and we know how precious your time is,” Uber CTO Praveen Neppalli Naga said in a statement. “Cart Assistant helps you get from idea to checkout in seconds.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cart Assistant could help Uber Eats better compete with other food delivery and grocery apps that are already integrating or developing AI chatbots.​ For example, Instacart launched an AI search tool powered by OpenAI’s ChatGPT in 2023 to help customers save time and receive personalized shopping recommendations. DoorDash was also reportedly testing an AI chatbot called DashAI that same year.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, both Uber Eats and rival DoorDash integrated with ChatGPT to streamline food ordering. With Uber Eats, U.S. users can browse local restaurants and menus in ChatGPT, then complete their purchase in the Uber Eats app. DoorDash’s integration allows users to request meal plans and automatically add all necessary ingredients to their DoorDash cart.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Bloomberg reported in 2023 that Uber Eats was developing an AI-powered chatbot that would supposedly ask users about their budget and food preferences, and help them place an order.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Uber Eats is rapidly investing in AI, including tools for merchants like AI-generated menu descriptions, enhanced food photos, and customer review summaries.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Uber Eats announced a new AI feature, “Cart Assistant,” on Wednesday designed to fill customers’ grocery carts faster and easier. The beta version is now available in the app.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To use the new chatbot, users search for a grocery store in the Uber Eats app and tap the purple Cart Assistant icon on the store’s page to begin shopping.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Customers can enter a list or upload an image of one, and Cart Assistant will automatically add the necessary items to their basket. This includes photos of handwritten lists or screenshots of recipes and their ingredients. Users can then customize the basket by swapping items for preferred brands or adding more products from the store.​&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3092211" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/UberEats-cart-assistant.gif?w=346" width="346" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Uber Eats&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Uber Eats notes that Cart Assistant uses previous orders to prioritize familiar items — like your usual milk or favorite oatmeal — to make the experience more personalized.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Users were telling us they wanted a quicker way to shop, and we know how precious your time is,” Uber CTO Praveen Neppalli Naga said in a statement. “Cart Assistant helps you get from idea to checkout in seconds.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cart Assistant could help Uber Eats better compete with other food delivery and grocery apps that are already integrating or developing AI chatbots.​ For example, Instacart launched an AI search tool powered by OpenAI’s ChatGPT in 2023 to help customers save time and receive personalized shopping recommendations. DoorDash was also reportedly testing an AI chatbot called DashAI that same year.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, both Uber Eats and rival DoorDash integrated with ChatGPT to streamline food ordering. With Uber Eats, U.S. users can browse local restaurants and menus in ChatGPT, then complete their purchase in the Uber Eats app. DoorDash’s integration allows users to request meal plans and automatically add all necessary ingredients to their DoorDash cart.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Notably, Bloomberg reported in 2023 that Uber Eats was developing an AI-powered chatbot that would supposedly ask users about their budget and food preferences, and help them place an order.​&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Overall, Uber Eats is rapidly investing in AI, including tools for merchants like AI-generated menu descriptions, enhanced food photos, and customer review summaries.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/uber-eats-launches-ai-assistant-to-help-with-grocery-cart-creation/</guid><pubDate>Wed, 11 Feb 2026 21:07:37 +0000</pubDate></item><item><title>Apple’s Siri revamp reportedly delayed… again (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/apples-siri-revamp-reportedly-delayed-again/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-cook-glowing-apple-logo-GettyImages-2234517515.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple has been promising a new-and-improved, cutting-edge, AI-powered Siri since it first unveiled Apple Intelligence in 2024. Over about a year and a half since then, the release date for this new era of Siri has been continuously pushed back. According to a new report from Bloomberg’s Mark Gurman, we’ll likely have to wait even longer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the new Siri was expected to launch with the upcoming iOS 26.4 update in March, now, the changes are expected to roll out more slowly over time, reportedly postponing some features until the May iOS update, or even until the release of iOS 27 in September. Apparently, Apple ran into trouble when testing the software, requiring the launch date to be pushed back further.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The changes are rumored to make the longtime digital assistant more like the LLM chatbots that have swept the tech world — but instead of opening up a ChatGPT or Claude app on your iPhone or MacBook, you would be able to just talk to Siri, which will be powered by Google Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We’re starting to feel bad for the Siri product managers. Hang in there, folks.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/tim-cook-glowing-apple-logo-GettyImages-2234517515.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple has been promising a new-and-improved, cutting-edge, AI-powered Siri since it first unveiled Apple Intelligence in 2024. Over about a year and a half since then, the release date for this new era of Siri has been continuously pushed back. According to a new report from Bloomberg’s Mark Gurman, we’ll likely have to wait even longer.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the new Siri was expected to launch with the upcoming iOS 26.4 update in March, now, the changes are expected to roll out more slowly over time, reportedly postponing some features until the May iOS update, or even until the release of iOS 27 in September. Apparently, Apple ran into trouble when testing the software, requiring the launch date to be pushed back further.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The changes are rumored to make the longtime digital assistant more like the LLM chatbots that have swept the tech world — but instead of opening up a ChatGPT or Claude app on your iPhone or MacBook, you would be able to just talk to Siri, which will be powered by Google Gemini.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We’re starting to feel bad for the Siri product managers. Hang in there, folks.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/apples-siri-revamp-reportedly-delayed-again/</guid><pubDate>Wed, 11 Feb 2026 21:19:46 +0000</pubDate></item><item><title>OpenAI disbands mission alignment team (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/openai-disbands-mission-alignment-team-which-focused-on-safe-and-trustworthy-ai-development/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198379368.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has disbanded a team that was designed to communicate the company’s mission to the public and to its own employees. At the same time, the team’s former leader has been given a new role as the company’s “chief futurist.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI confirmed to TechCrunch that the team’s members have now been assigned to other roles. The news was first reported by Platformer.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The disbanded team in question appears to have been formed in September of 2024. Platformer reports that the team was dedicated to promoting “the company’s stated mission to ensure that artificial general intelligence benefits all of humanity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An official OpenAI spokesperson described the team thusly: “The Mission Alignment project was a support function to help employees and the public understand our mission and the impact of AI. That work continues throughout the organization.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post published Wednesday, Josh Achiam, the former head of OpenAI’s mission alignment team, explained his new role as the company’s chief futurist. “My goal is to support OpenAI’s mission — to ensure that artificial general intelligence benefits all of humanity — by studying how the world will change in response to AI, AGI, and beyond,” Achiam wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam noted that, in his new role, he would be collaborating with Jason Pruet, an OpenAI physicist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for OpenAI said the rest of the mission alignment team — a group of six or seven people — had subsequently been reassigned to different parts of the company. The spokesperson couldn’t say where exactly the team members had been assigned, but said that they were engaged in similar work in those roles. It was also unclear whether Achiam would have a new team as part of his “futurist” role.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The spokesperson attributed the disbanding of the team to the kinds of routine reorganizations that occur within a fast-moving company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI previously had what it called a “superalignment team” — which was formed in 2023 and focused on studying long-term existential threats posed by AI — but that team was disbanded in 2024. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam’s personal website still lists him as head of mission alignment at OpenAI, and describes him as being interested in ensuring that the “long-term future of humanity is good.” His LinkedIn profile shows he had served as head of mission alignment since September 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: This story originally confused mission alignment with another, similarly named team called alignment. It has been corrected and we apologize for the error.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198379368.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI has disbanded a team that was designed to communicate the company’s mission to the public and to its own employees. At the same time, the team’s former leader has been given a new role as the company’s “chief futurist.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI confirmed to TechCrunch that the team’s members have now been assigned to other roles. The news was first reported by Platformer.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The disbanded team in question appears to have been formed in September of 2024. Platformer reports that the team was dedicated to promoting “the company’s stated mission to ensure that artificial general intelligence benefits all of humanity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An official OpenAI spokesperson described the team thusly: “The Mission Alignment project was a support function to help employees and the public understand our mission and the impact of AI. That work continues throughout the organization.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a blog post published Wednesday, Josh Achiam, the former head of OpenAI’s mission alignment team, explained his new role as the company’s chief futurist. “My goal is to support OpenAI’s mission — to ensure that artificial general intelligence benefits all of humanity — by studying how the world will change in response to AI, AGI, and beyond,” Achiam wrote.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam noted that, in his new role, he would be collaborating with Jason Pruet, an OpenAI physicist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for OpenAI said the rest of the mission alignment team — a group of six or seven people — had subsequently been reassigned to different parts of the company. The spokesperson couldn’t say where exactly the team members had been assigned, but said that they were engaged in similar work in those roles. It was also unclear whether Achiam would have a new team as part of his “futurist” role.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The spokesperson attributed the disbanding of the team to the kinds of routine reorganizations that occur within a fast-moving company. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI previously had what it called a “superalignment team” — which was formed in 2023 and focused on studying long-term existential threats posed by AI — but that team was disbanded in 2024. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Achiam’s personal website still lists him as head of mission alignment at OpenAI, and describes him as being interested in ensuring that the “long-term future of humanity is good.” His LinkedIn profile shows he had served as head of mission alignment since September 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Correction: This story originally confused mission alignment with another, similarly named team called alignment. It has been corrected and we apologize for the error.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/openai-disbands-mission-alignment-team-which-focused-on-safe-and-trustworthy-ai-development/</guid><pubDate>Wed, 11 Feb 2026 21:57:18 +0000</pubDate></item><item><title>AI inference startup Modal Labs in talks to raise at $2.5B valuation, sources say (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/ai-inference-startup-modal-labs-in-talks-to-raise-at-2-5b-valuation-sources-say/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2024-02-14-at-9.26.16AM.png?resize=1200,607" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Modal Labs, a startup specializing in AI inference infrastructure, is talking to VCs about a new round at a valuation of about $2.5 billion, according to four people with knowledge of the deal. Should the deal close at these terms, the funding round would more than double the company’s valuation of $1.1 billion announced less than five months ago, when it announced an $87 million Series B round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;General Catalyst is in talks to lead the round, the people told TechCrunch.&amp;nbsp;Modal’s annualized revenue run rate (ARR) is approximately $50 million, our sources said. The discussions are early, and terms could still change. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Modal Labs co-founder and CEO Erik Bernhardsson denied that his company was actively fundraising and characterized his recent interactions with VCs as general conversations. General Catalyst did not respond to our requests for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is focused on optimizing inference, the process of running trained AI models to generate answers from user requests. Improving inference efficiency reduces compute costs and cuts down the lag time between a user’s prompt and the AI’s response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is one of the handful of inference-focused companies attracting intense investor attention now. Last week, its competitor Baseten announced a $300 million raise at a $5 billion valuation, more than doubling the $2.1 billion valuation it reached just months prior in September. Similarly, Fireworks AI, an inference cloud provider, secured $250 million at a $4 billion valuation in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In January, the creators of the open source inference project vLLM announced they had transitioned the tool into a VC-backed startup, Inferact, raising $150 million in seed funding led by Andreessen Horowitz at an $800 million valuation. Meanwhile, TechCrunch reported that the team behind SGLang has commercialized as RadixArk, which sources told us secured seed funding at a $400 million valuation led by Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal was co-founded by CEO Erik Bernhardsson in 2021 after he spent more than 15 years building and leading data teams at companies including Spotify and Better.com, where he was CTO.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The startup counts Lux Capital and Redpoint Ventures among its earlier backers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Editor’s Note: This story was updated to include a comment from Modal.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/01/Screenshot-2024-02-14-at-9.26.16AM.png?resize=1200,607" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Modal Labs, a startup specializing in AI inference infrastructure, is talking to VCs about a new round at a valuation of about $2.5 billion, according to four people with knowledge of the deal. Should the deal close at these terms, the funding round would more than double the company’s valuation of $1.1 billion announced less than five months ago, when it announced an $87 million Series B round.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;General Catalyst is in talks to lead the round, the people told TechCrunch.&amp;nbsp;Modal’s annualized revenue run rate (ARR) is approximately $50 million, our sources said. The discussions are early, and terms could still change. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Modal Labs co-founder and CEO Erik Bernhardsson denied that his company was actively fundraising and characterized his recent interactions with VCs as general conversations. General Catalyst did not respond to our requests for comment.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is focused on optimizing inference, the process of running trained AI models to generate answers from user requests. Improving inference efficiency reduces compute costs and cuts down the lag time between a user’s prompt and the AI’s response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal is one of the handful of inference-focused companies attracting intense investor attention now. Last week, its competitor Baseten announced a $300 million raise at a $5 billion valuation, more than doubling the $2.1 billion valuation it reached just months prior in September. Similarly, Fireworks AI, an inference cloud provider, secured $250 million at a $4 billion valuation in October.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In January, the creators of the open source inference project vLLM announced they had transitioned the tool into a VC-backed startup, Inferact, raising $150 million in seed funding led by Andreessen Horowitz at an $800 million valuation. Meanwhile, TechCrunch reported that the team behind SGLang has commercialized as RadixArk, which sources told us secured seed funding at a $400 million valuation led by Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Modal was co-founded by CEO Erik Bernhardsson in 2021 after he spent more than 15 years building and leading data teams at companies including Spotify and Better.com, where he was CTO.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;The startup counts Lux Capital and Redpoint Ventures among its earlier backers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Editor’s Note: This story was updated to include a comment from Modal.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/ai-inference-startup-modal-labs-in-talks-to-raise-at-2-5b-valuation-sources-say/</guid><pubDate>Wed, 11 Feb 2026 22:48:35 +0000</pubDate></item><item><title>xAI lays out interplanetary ambitions in public all-hands (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/11/xai-lays-out-interplanetary-ambitions-in-public-all-hands/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, xAI took the rare step of publishing a full 45-minute all-hands meeting video on X, making it publicly accessible. Details of the Tuesday night meeting were previously reported by The New York Times, which may have influenced xAI’s decision to post the video online.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The full video reveals significant new details about Musk’s plans for the AI lab, including its product roadmap and its ongoing ties to the X platform.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The most immediate revelation concerned a string of departing employees, which Musk described as layoffs resulting from a changing organizational structure at the company. While reorganizations are common, the breadth of the departures has caused significant confusion, particularly as it has meant the loss of a significant portion of the founding team.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a company grows, especially as quickly as xAI, the structure must evolve,” Musk said on X. “This unfortunately required parting ways with some people. We wish them well in future endeavors.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new organizational system splits xAI into four primary teams: one focused on the Grok chatbot (including voice), another for the app’s coding system, another for the Imagine video generator, and finally a team focused on the Macrohard project, which spans from simple computer use simulation to modeling entire corporations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Macrohard] is able to do anything on a computer that a computer is able to do,” Toby Pohlen, who will lead the project under the new organizational structure, told his colleagues. “There should be rocket engines fully designed by AI.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3092311" height="444" src="https://techcrunch.com/wp-content/uploads/2026/02/Screen-Shot-2026-02-11-at-3.21.27-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;xAI (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The all-hands also featured claims about new usage and revenue figures for xAI and X. Nikita Bier, X’s head of product, said X had “just crossed” $1 billion in annual recurring revenue from subscriptions, which he attributed to a marketing push during the holidays.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, executives said the xAI’s Imagine tool is generating 50 million videos a day, and more than 6 billion images over the past 30 days, according to their internal metrics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s difficult to separate those figures from the flood of deepfake pornography that overtook X during that same period. The X platform saw engagement skyrocket as AI-generated explicit images became more prevalent, and with an estimated 1.8 million sexualized images generated over just nine days, the image-generation figures likely include substantial amounts of this controversial content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most eye-catching part of the presentation came at the end, when Musk reemphasized the importance of space-based data centers despite the technical challenges involved. Musk went still further, envisioning a moon-based factory for AI satellites, including a lunar mass driver — essentially an electromagnetic catapult — to launch them. With such infrastructure, Musk said, one could launch an AI cluster capable of capturing significant portions of the sun’s total energy output or even expanding to other galaxies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s difficult to imagine what an intelligence of that scale would think about,” Musk said, “but it’s going to be incredibly exciting to see it happen.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, xAI took the rare step of publishing a full 45-minute all-hands meeting video on X, making it publicly accessible. Details of the Tuesday night meeting were previously reported by The New York Times, which may have influenced xAI’s decision to post the video online.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The full video reveals significant new details about Musk’s plans for the AI lab, including its product roadmap and its ongoing ties to the X platform.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The most immediate revelation concerned a string of departing employees, which Musk described as layoffs resulting from a changing organizational structure at the company. While reorganizations are common, the breadth of the departures has caused significant confusion, particularly as it has meant the loss of a significant portion of the founding team.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“As a company grows, especially as quickly as xAI, the structure must evolve,” Musk said on X. “This unfortunately required parting ways with some people. We wish them well in future endeavors.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new organizational system splits xAI into four primary teams: one focused on the Grok chatbot (including voice), another for the app’s coding system, another for the Imagine video generator, and finally a team focused on the Macrohard project, which spans from simple computer use simulation to modeling entire corporations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“[Macrohard] is able to do anything on a computer that a computer is able to do,” Toby Pohlen, who will lead the project under the new organizational structure, told his colleagues. “There should be rocket engines fully designed by AI.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3092311" height="444" src="https://techcrunch.com/wp-content/uploads/2026/02/Screen-Shot-2026-02-11-at-3.21.27-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;xAI (screenshot)&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The all-hands also featured claims about new usage and revenue figures for xAI and X. Nikita Bier, X’s head of product, said X had “just crossed” $1 billion in annual recurring revenue from subscriptions, which he attributed to a marketing push during the holidays.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, executives said the xAI’s Imagine tool is generating 50 million videos a day, and more than 6 billion images over the past 30 days, according to their internal metrics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it’s difficult to separate those figures from the flood of deepfake pornography that overtook X during that same period. The X platform saw engagement skyrocket as AI-generated explicit images became more prevalent, and with an estimated 1.8 million sexualized images generated over just nine days, the image-generation figures likely include substantial amounts of this controversial content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most eye-catching part of the presentation came at the end, when Musk reemphasized the importance of space-based data centers despite the technical challenges involved. Musk went still further, envisioning a moon-based factory for AI satellites, including a lunar mass driver — essentially an electromagnetic catapult — to launch them. With such infrastructure, Musk said, one could launch an AI cluster capable of capturing significant portions of the sun’s total energy output or even expanding to other galaxies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s difficult to imagine what an intelligence of that scale would think about,” Musk said, “but it’s going to be incredibly exciting to see it happen.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/11/xai-lays-out-interplanetary-ambitions-in-public-all-hands/</guid><pubDate>Wed, 11 Feb 2026 23:29:08 +0000</pubDate></item><item><title>[NEW] Accelerating science with AI and simulations (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/accelerating-science-ai-and-simulations-rafael-gomez-bombarelli-0212</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-Rafael-Gomez-Bombarelli-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;For more than a decade, MIT Associate Professor Rafael Gómez-Bombarelli has used artificial intelligence to create new materials. As the technology has expanded, so have his ambitions.&lt;/p&gt;&lt;p&gt;Now, the newly tenured professor in materials science and engineering believes AI is poised to transform science in ways never before possible. His work at MIT and beyond is devoted to accelerating that future.&lt;/p&gt;&lt;p&gt;“We’re at a second inflection point,”&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says. “The first one was around 2015 with the first wave of representation learning, generative AI, and high-throughput data in some areas of science. Those are some of the techniques I first brought into my lab at MIT. Now I think we’re at a second inflection point, mixing language and merging multiple modalities into general scientific intelligence. We’re going to have all the model classes and scaling laws needed to reason about language, reason over material structures, and reason over synthesis recipes.”&lt;/p&gt;&lt;p&gt;Gómez Bombarelli’s research combines physics-based simulations with approaches like machine learning and generative AI to discover new materials with promising real-world applications. His work has led to new materials for batteries, catalysts, plastics, and organic light-emitting diodes (OLEDs). He has also co-founded multiple companies and served on scientific advisory boards for startups applying AI to drug discovery, robotics, and more. His latest company, Lila Sciences, is working to build a scientific superintelligence platform for the life sciences, chemical, and materials science industries.&lt;/p&gt;&lt;p&gt;All of that work is designed to ensure the future of scientific research is more seamless and productive than research today.&lt;/p&gt;&lt;p&gt;“AI for science is one of the most exciting and aspirational uses of AI,” Gómez-Bombarelli says. “Other applications for AI have more downsides and ambiguity. AI for science is about bringing a better future forward in time.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From experiments to simulations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli grew up in Spain and gravitated toward the physical sciences from an early age. In 2001, he won a Chemistry Olympics competition, setting him on an academic track in chemistry, which he studied as an undergraduate at his hometown college, the University of Salamanca. Gómez-Bombarelli stuck around for his PhD, where he investigated the function of DNA-damaging chemicals.&lt;/p&gt;&lt;p&gt;“My PhD started out experimental, and then I got bitten by the bug of simulation and computer science about halfway through,” he says. “I started simulating the same chemical reactions I was measuring in the lab. I like the way programming organizes your brain; it felt like a natural way to organize one’s thinking. Programming is also a lot less limited by what you can do with your hands or with scientific instruments.”&lt;/p&gt;&lt;p&gt;Next, Gómez-Bombarelli went to Scotland for a postdoctoral position, where he studied quantum effects in biology. Through that work, he connected with Alán Aspuru-Guzik, a chemistry professor at Harvard University, whom he joined for his next postdoc in 2014.&lt;/p&gt;&lt;p&gt;“I was one of the first people to use generative AI for chemistry in 2016, and I was on the first team to use neural networks to understand molecules in 2015,” Gómez-Bombarelli says. “It was the early, early days of deep learning for science.”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli also began working to eliminate manual parts of molecular simulations to run more high-throughput experiments. He and his collaborators ended up running hundreds of thousands of calculations across materials, discovering hundreds of promising materials for testing.&lt;/p&gt;&lt;p&gt;After two years in the lab, Gómez-Bombarelli and Aspuru-Guzik started a general-purpose materials computation company, which eventually pivoted to focus on producing organic light-emitting diodes. Gómez-Bombarelli joined the company full-time and calls it the hardest thing he’s ever done in his career.&lt;/p&gt;&lt;p&gt;“It was amazing to make something tangible,” he says. “Also, after seeing Aspuru-Guzik run a lab, I didn’t want to become a professor. My dad was a professor in linguistics, and I thought it was a mellow job. Then I saw Aspuru-Guzik with a 40-person group, and he was on the road 120 days a year. It was insane. I didn’t think I had that type of energy and creativity in me.”&lt;/p&gt;&lt;p&gt;In 2018, Aspuru-Guzik suggested Gómez-Bombarelli apply for a new position in MIT’s Department of Materials Science and Engineering. But, with his trepidation about a faculty job, Gómez-Bombarelli let the deadline pass. Aspuru-Guzik confronted him in his office, slammed his hands on the table, and told him, “You need to apply for this.” It was enough to get Gómez-Bombarelli to put together a formal application.&lt;/p&gt;&lt;p&gt;Fortunately at his startup, Gómez-Bombarelli had spent a lot of time thinking about how to create value from computational materials discovery. During the interview process, he says, he was attracted to the energy and collaborative spirit at MIT. He also began to appreciate the research possibilities.&lt;/p&gt;&lt;p&gt;“Everything I had been doing as a postdoc and at the company was going to be a subset of what I could do at MIT,” he says. “I was making products, and I still get to do that. Suddenly, my universe of work was a subset of this new universe of things I could explore and do.”&lt;/p&gt;&lt;p&gt;It’s been nine years since Gómez Bombarelli joined MIT. Today his lab focuses on how the composition, structure, and reactivity of atoms impact material performance. He has also used high-throughput simulations to create new materials and helped develop tools for merging deep learning with physics-based modeling.&lt;/p&gt;&lt;p&gt;“Physics-based simulations make data and AI algorithms get better the more data you give them,” Gómez Bombarelli’s says. “There are all sorts of virtuous cycles between AI and simulations.”&lt;/p&gt;&lt;p&gt;The research group he has built is solely computational — they don’t run physical experiments.&lt;/p&gt;&lt;p&gt;“It’s a blessing because we can have a huge amount of breadth and do lots of things at once,” he says. “We love working with experimentalists and try to be good partners with them. We also love to create computational tools that help experimentalists triage the ideas coming from AI .”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli is also still focused on the real-world applications of the materials he invents. His lab works closely with companies and organizations like MIT’s Industrial Liaison Program to understand the material needs of the private sector and the practical hurdles of commercial development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Accelerating science&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As excitement around artificial intelligence has exploded, Gómez-Bombarelli has seen the field mature. Companies like Meta, Microsoft, and Google’s DeepMind now regularly conduct physics-based simulations reminiscent of what he was working on back in 2016. In November, the U.S. Department of Energy launched the Genesis Mission to accelerate scientific discovery, national security, and energy dominance using AI.&lt;/p&gt;&lt;p&gt;“AI for simulations has gone from something that maybe could work to a consensus scientific view,” Gómez-Bombarelli says. “We’re at an inflection point. Humans think in natural language, we write papers in natural language, and it turns out these large language models that have mastered natural language have opened up the ability to accelerate science. We’ve seen that scaling works for simulations. We’ve seen that scaling works for language. Now we’re going to see how scaling works for science.”&lt;/p&gt;&lt;p&gt;When he first came to MIT,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says he was blown away by how non-competitive things were between researchers. He tries to bring that same positive-sum thinking to his research group, which is made up of about 25 graduate students and postdocs.&lt;/p&gt;&lt;p&gt;“We’ve naturally grown into a really diverse group, with a diverse set of mentalities,” Gomez-Bombarelli says. “Everyone has their own career aspirations and strengths and weaknesses. Figuring out how to help people be the best versions of themselves is fun. Now I’ve become the one insisting that people apply to faculty positions after the deadline. I guess I’ve passed that baton.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/MIT-Rafael-Gomez-Bombarelli-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;For more than a decade, MIT Associate Professor Rafael Gómez-Bombarelli has used artificial intelligence to create new materials. As the technology has expanded, so have his ambitions.&lt;/p&gt;&lt;p&gt;Now, the newly tenured professor in materials science and engineering believes AI is poised to transform science in ways never before possible. His work at MIT and beyond is devoted to accelerating that future.&lt;/p&gt;&lt;p&gt;“We’re at a second inflection point,”&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says. “The first one was around 2015 with the first wave of representation learning, generative AI, and high-throughput data in some areas of science. Those are some of the techniques I first brought into my lab at MIT. Now I think we’re at a second inflection point, mixing language and merging multiple modalities into general scientific intelligence. We’re going to have all the model classes and scaling laws needed to reason about language, reason over material structures, and reason over synthesis recipes.”&lt;/p&gt;&lt;p&gt;Gómez Bombarelli’s research combines physics-based simulations with approaches like machine learning and generative AI to discover new materials with promising real-world applications. His work has led to new materials for batteries, catalysts, plastics, and organic light-emitting diodes (OLEDs). He has also co-founded multiple companies and served on scientific advisory boards for startups applying AI to drug discovery, robotics, and more. His latest company, Lila Sciences, is working to build a scientific superintelligence platform for the life sciences, chemical, and materials science industries.&lt;/p&gt;&lt;p&gt;All of that work is designed to ensure the future of scientific research is more seamless and productive than research today.&lt;/p&gt;&lt;p&gt;“AI for science is one of the most exciting and aspirational uses of AI,” Gómez-Bombarelli says. “Other applications for AI have more downsides and ambiguity. AI for science is about bringing a better future forward in time.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;From experiments to simulations&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli grew up in Spain and gravitated toward the physical sciences from an early age. In 2001, he won a Chemistry Olympics competition, setting him on an academic track in chemistry, which he studied as an undergraduate at his hometown college, the University of Salamanca. Gómez-Bombarelli stuck around for his PhD, where he investigated the function of DNA-damaging chemicals.&lt;/p&gt;&lt;p&gt;“My PhD started out experimental, and then I got bitten by the bug of simulation and computer science about halfway through,” he says. “I started simulating the same chemical reactions I was measuring in the lab. I like the way programming organizes your brain; it felt like a natural way to organize one’s thinking. Programming is also a lot less limited by what you can do with your hands or with scientific instruments.”&lt;/p&gt;&lt;p&gt;Next, Gómez-Bombarelli went to Scotland for a postdoctoral position, where he studied quantum effects in biology. Through that work, he connected with Alán Aspuru-Guzik, a chemistry professor at Harvard University, whom he joined for his next postdoc in 2014.&lt;/p&gt;&lt;p&gt;“I was one of the first people to use generative AI for chemistry in 2016, and I was on the first team to use neural networks to understand molecules in 2015,” Gómez-Bombarelli says. “It was the early, early days of deep learning for science.”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli also began working to eliminate manual parts of molecular simulations to run more high-throughput experiments. He and his collaborators ended up running hundreds of thousands of calculations across materials, discovering hundreds of promising materials for testing.&lt;/p&gt;&lt;p&gt;After two years in the lab, Gómez-Bombarelli and Aspuru-Guzik started a general-purpose materials computation company, which eventually pivoted to focus on producing organic light-emitting diodes. Gómez-Bombarelli joined the company full-time and calls it the hardest thing he’s ever done in his career.&lt;/p&gt;&lt;p&gt;“It was amazing to make something tangible,” he says. “Also, after seeing Aspuru-Guzik run a lab, I didn’t want to become a professor. My dad was a professor in linguistics, and I thought it was a mellow job. Then I saw Aspuru-Guzik with a 40-person group, and he was on the road 120 days a year. It was insane. I didn’t think I had that type of energy and creativity in me.”&lt;/p&gt;&lt;p&gt;In 2018, Aspuru-Guzik suggested Gómez-Bombarelli apply for a new position in MIT’s Department of Materials Science and Engineering. But, with his trepidation about a faculty job, Gómez-Bombarelli let the deadline pass. Aspuru-Guzik confronted him in his office, slammed his hands on the table, and told him, “You need to apply for this.” It was enough to get Gómez-Bombarelli to put together a formal application.&lt;/p&gt;&lt;p&gt;Fortunately at his startup, Gómez-Bombarelli had spent a lot of time thinking about how to create value from computational materials discovery. During the interview process, he says, he was attracted to the energy and collaborative spirit at MIT. He also began to appreciate the research possibilities.&lt;/p&gt;&lt;p&gt;“Everything I had been doing as a postdoc and at the company was going to be a subset of what I could do at MIT,” he says. “I was making products, and I still get to do that. Suddenly, my universe of work was a subset of this new universe of things I could explore and do.”&lt;/p&gt;&lt;p&gt;It’s been nine years since Gómez Bombarelli joined MIT. Today his lab focuses on how the composition, structure, and reactivity of atoms impact material performance. He has also used high-throughput simulations to create new materials and helped develop tools for merging deep learning with physics-based modeling.&lt;/p&gt;&lt;p&gt;“Physics-based simulations make data and AI algorithms get better the more data you give them,” Gómez Bombarelli’s says. “There are all sorts of virtuous cycles between AI and simulations.”&lt;/p&gt;&lt;p&gt;The research group he has built is solely computational — they don’t run physical experiments.&lt;/p&gt;&lt;p&gt;“It’s a blessing because we can have a huge amount of breadth and do lots of things at once,” he says. “We love working with experimentalists and try to be good partners with them. We also love to create computational tools that help experimentalists triage the ideas coming from AI .”&lt;/p&gt;&lt;p&gt;Gómez-Bombarelli is also still focused on the real-world applications of the materials he invents. His lab works closely with companies and organizations like MIT’s Industrial Liaison Program to understand the material needs of the private sector and the practical hurdles of commercial development.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Accelerating science&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As excitement around artificial intelligence has exploded, Gómez-Bombarelli has seen the field mature. Companies like Meta, Microsoft, and Google’s DeepMind now regularly conduct physics-based simulations reminiscent of what he was working on back in 2016. In November, the U.S. Department of Energy launched the Genesis Mission to accelerate scientific discovery, national security, and energy dominance using AI.&lt;/p&gt;&lt;p&gt;“AI for simulations has gone from something that maybe could work to a consensus scientific view,” Gómez-Bombarelli says. “We’re at an inflection point. Humans think in natural language, we write papers in natural language, and it turns out these large language models that have mastered natural language have opened up the ability to accelerate science. We’ve seen that scaling works for simulations. We’ve seen that scaling works for language. Now we’re going to see how scaling works for science.”&lt;/p&gt;&lt;p&gt;When he first came to MIT,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;Gómez-Bombarelli says he was blown away by how non-competitive things were between researchers. He tries to bring that same positive-sum thinking to his research group, which is made up of about 25 graduate students and postdocs.&lt;/p&gt;&lt;p&gt;“We’ve naturally grown into a really diverse group, with a diverse set of mentalities,” Gomez-Bombarelli says. “Everyone has their own career aspirations and strengths and weaknesses. Figuring out how to help people be the best versions of themselves is fun. Now I’ve become the one insisting that people apply to faculty positions after the deadline. I guess I’ve passed that baton.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/accelerating-science-ai-and-simulations-rafael-gomez-bombarelli-0212</guid><pubDate>Thu, 12 Feb 2026 05:00:00 +0000</pubDate></item></channel></rss>