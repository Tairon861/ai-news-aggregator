<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 19 Aug 2025 01:45:30 +0000</lastBuildDate><item><title>Is Perplexity AI’s $34.5b Chrome bid a strategic master stroke or elaborate PR stunt? (AI News)</title><link>https://www.artificialintelligence-news.com/news/perplexity-ai-chrome-bid-analysis/</link><description>&lt;p&gt;The artificial intelligence company Perplexity’s audacious offer to acquire Chrome has sent shockwaves through Silicon Valley, but questions remain about whether the move represents a genuine strategy or a calculated publicity stunt.&lt;/p&gt;&lt;p&gt;Perplexity AI made an unsolicited $34.5 billion bid for Google’s Chrome browser last Tuesday, a figure that exceeds the startup’s own $18 billion valuation by nearly a factor of two. The timing appears strategically calculated, coming as federal courts consider whether to force Google to divest itself of Chrome, following last year’s landmark antitrust ruling.&lt;/p&gt;&lt;p&gt;The financial mechanics of the proposed Perplexity Chrome acquisition raise immediate red flags. Perplexity has raised about $1.5 billion to date, including an extension round of $100 million raised last month, when the company was valued at $18 billion. To bridge the gap, the company said several investors have agreed to back the deal, though specific funding details were undisclosed.&lt;/p&gt;&lt;p&gt;Industry experts are divided on the bid’s valuation. Wedbush tech analyst Dan Ives told &lt;em&gt;CNN&lt;/em&gt; that he estimates Chrome is worth at least $50 billion, while DuckDuckGo’s CEO, Gabriel Weinberg, suggested Chrome may command upwards of $50 billion if Google were forced to sell. Those estimates would make Perplexity’s offer well below market value.&lt;/p&gt;&lt;h3&gt;Strategic logic or marketing manoeuvre?&lt;/h3&gt;&lt;p&gt;The bid’s strategic rationale centres on browser control as the next battleground in AI search. Perplexity unveiledits AI-native search browser, Comet, last month, an explicit move to take enter the browser market. Acquiring Chrome would provide instant access to over three billion users – a massive leap from Perplexity’s current 30 million monthly active users for its AI service.&lt;/p&gt;&lt;p&gt;However, scepticism abounds. Technology industry investor Heath Ahrens called Perplexity’s move a “stunt, and nowhere near Chrome’s true value, given its unmatched data and reach.” That sentiment reflects broader industry doubts about both the bid’s sincerity and financial viability.&lt;/p&gt;&lt;h3&gt;Terms that seem too good to be true&lt;/h3&gt;&lt;p&gt;The proposed Perplexity Chrome acquisition includes seemingly altruistic terms that raise questions about commercial intent. Perplexity said it would maintain users’ current browsing preferences, including Google as the default search engine, and commit to keep Chrome’s underlying engine, Chromium, open-source and continue to invest in it with a promised $3 billion investment over 24 months.&lt;/p&gt;&lt;p&gt;The terms appear to contradict typical acquisition logic, where buyers seek competitive advantages rather than maintaining competitors’ market positions. The promise to keep Google as the default search engine particularly puzzles analysts, given Perplexity’s core business as a Google Search challenger. An observer with any sense of history or experience would surmise that Perplexity’s assurances are ephemeral, and that in practice, Chrome would see massive changes under the hood in terms of its provision of services.&lt;/p&gt;&lt;h3&gt;Regulatory backdrop and timing&lt;/h3&gt;&lt;p&gt;The bid’s timing coincides perfectly with ongoing antitrust proceedings, with Google yet to respond publicly on the offer. The company has not offered Chrome for sale and plans to appeal a US court ruling last year that found it held an unlawful monopoly in online search.&lt;/p&gt;&lt;p&gt;The unsolicited bid comes not long after rival OpenAI also expressed interest in acquiring Chrome, suggesting multiple AI companies view browser control as strategically important.&lt;/p&gt;&lt;p&gt;The bid also marks Perplexity’s second major acquisition attempt this year. It made similar moves for TikTok US in January, offering to merge with the popular short-video app to resolve US concerns about TikTok’s Chinese ownership. The TikTok bid generated similar headlines but failed to materialise into a completed transaction.&lt;/p&gt;&lt;h3&gt;Market reaction and industry implications&lt;/h3&gt;&lt;p&gt;Despite the astronomical figures involved, market reaction has been measured. On Wall Street, Alphabet’s share price surged up 1.4% since the market opened, suggesting investors view the bid sceptically rather than as a genuine threat to Google’s browser dominance.&lt;/p&gt;&lt;p&gt;The broader implications extend beyond this single transaction. Perplexity believes browsers are strategic control points for the next era of agentic search and online advertising, highlighting how AI companies increasingly view traditional tech infrastructure as essential battlegrounds.&lt;/p&gt;&lt;h3&gt;The verdict: Strategy or spectacle?&lt;/h3&gt;&lt;p&gt;Google is unlikely to sell Chrome, so it’s more of a PR stunt than a likely deal, according to industry analysis. The Perplexity Chrome acquisition bid serves several purposes beyond its stated intent: generating massive media coverage, positioning Perplexity as a serious Google competitor, and demonstrating financial backing for future endeavours.&lt;/p&gt;&lt;p&gt;Whether genuine or theatrical, the bid illuminates the evolving dynamics of AI competition, where control of user access points may determine which companies succeed in the next phase of digital transformation. For now, Perplexity has achieved its likely primary objective – commanding attention in an increasingly crowded AI marketplace.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Perplexity&lt;/em&gt;/X.com&lt;em&gt;)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Telefónica’s Wayra backs AI answer engine Perplexity&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;The artificial intelligence company Perplexity’s audacious offer to acquire Chrome has sent shockwaves through Silicon Valley, but questions remain about whether the move represents a genuine strategy or a calculated publicity stunt.&lt;/p&gt;&lt;p&gt;Perplexity AI made an unsolicited $34.5 billion bid for Google’s Chrome browser last Tuesday, a figure that exceeds the startup’s own $18 billion valuation by nearly a factor of two. The timing appears strategically calculated, coming as federal courts consider whether to force Google to divest itself of Chrome, following last year’s landmark antitrust ruling.&lt;/p&gt;&lt;p&gt;The financial mechanics of the proposed Perplexity Chrome acquisition raise immediate red flags. Perplexity has raised about $1.5 billion to date, including an extension round of $100 million raised last month, when the company was valued at $18 billion. To bridge the gap, the company said several investors have agreed to back the deal, though specific funding details were undisclosed.&lt;/p&gt;&lt;p&gt;Industry experts are divided on the bid’s valuation. Wedbush tech analyst Dan Ives told &lt;em&gt;CNN&lt;/em&gt; that he estimates Chrome is worth at least $50 billion, while DuckDuckGo’s CEO, Gabriel Weinberg, suggested Chrome may command upwards of $50 billion if Google were forced to sell. Those estimates would make Perplexity’s offer well below market value.&lt;/p&gt;&lt;h3&gt;Strategic logic or marketing manoeuvre?&lt;/h3&gt;&lt;p&gt;The bid’s strategic rationale centres on browser control as the next battleground in AI search. Perplexity unveiledits AI-native search browser, Comet, last month, an explicit move to take enter the browser market. Acquiring Chrome would provide instant access to over three billion users – a massive leap from Perplexity’s current 30 million monthly active users for its AI service.&lt;/p&gt;&lt;p&gt;However, scepticism abounds. Technology industry investor Heath Ahrens called Perplexity’s move a “stunt, and nowhere near Chrome’s true value, given its unmatched data and reach.” That sentiment reflects broader industry doubts about both the bid’s sincerity and financial viability.&lt;/p&gt;&lt;h3&gt;Terms that seem too good to be true&lt;/h3&gt;&lt;p&gt;The proposed Perplexity Chrome acquisition includes seemingly altruistic terms that raise questions about commercial intent. Perplexity said it would maintain users’ current browsing preferences, including Google as the default search engine, and commit to keep Chrome’s underlying engine, Chromium, open-source and continue to invest in it with a promised $3 billion investment over 24 months.&lt;/p&gt;&lt;p&gt;The terms appear to contradict typical acquisition logic, where buyers seek competitive advantages rather than maintaining competitors’ market positions. The promise to keep Google as the default search engine particularly puzzles analysts, given Perplexity’s core business as a Google Search challenger. An observer with any sense of history or experience would surmise that Perplexity’s assurances are ephemeral, and that in practice, Chrome would see massive changes under the hood in terms of its provision of services.&lt;/p&gt;&lt;h3&gt;Regulatory backdrop and timing&lt;/h3&gt;&lt;p&gt;The bid’s timing coincides perfectly with ongoing antitrust proceedings, with Google yet to respond publicly on the offer. The company has not offered Chrome for sale and plans to appeal a US court ruling last year that found it held an unlawful monopoly in online search.&lt;/p&gt;&lt;p&gt;The unsolicited bid comes not long after rival OpenAI also expressed interest in acquiring Chrome, suggesting multiple AI companies view browser control as strategically important.&lt;/p&gt;&lt;p&gt;The bid also marks Perplexity’s second major acquisition attempt this year. It made similar moves for TikTok US in January, offering to merge with the popular short-video app to resolve US concerns about TikTok’s Chinese ownership. The TikTok bid generated similar headlines but failed to materialise into a completed transaction.&lt;/p&gt;&lt;h3&gt;Market reaction and industry implications&lt;/h3&gt;&lt;p&gt;Despite the astronomical figures involved, market reaction has been measured. On Wall Street, Alphabet’s share price surged up 1.4% since the market opened, suggesting investors view the bid sceptically rather than as a genuine threat to Google’s browser dominance.&lt;/p&gt;&lt;p&gt;The broader implications extend beyond this single transaction. Perplexity believes browsers are strategic control points for the next era of agentic search and online advertising, highlighting how AI companies increasingly view traditional tech infrastructure as essential battlegrounds.&lt;/p&gt;&lt;h3&gt;The verdict: Strategy or spectacle?&lt;/h3&gt;&lt;p&gt;Google is unlikely to sell Chrome, so it’s more of a PR stunt than a likely deal, according to industry analysis. The Perplexity Chrome acquisition bid serves several purposes beyond its stated intent: generating massive media coverage, positioning Perplexity as a serious Google competitor, and demonstrating financial backing for future endeavours.&lt;/p&gt;&lt;p&gt;Whether genuine or theatrical, the bid illuminates the evolving dynamics of AI competition, where control of user access points may determine which companies succeed in the next phase of digital transformation. For now, Perplexity has achieved its likely primary objective – commanding attention in an increasingly crowded AI marketplace.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Perplexity&lt;/em&gt;/X.com&lt;em&gt;)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Telefónica’s Wayra backs AI answer engine Perplexity&lt;/strong&gt;&lt;/p&gt;&lt;img alt="alt" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/perplexity-ai-chrome-bid-analysis/</guid><pubDate>Mon, 18 Aug 2025 14:49:09 +0000</pubDate></item><item><title>Why Paradigm built a spreadsheet with an AI agent in every cell (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/18/why-paradigm-built-a-spreadsheet-with-an-ai-agent-in-every-cell/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Paradigm-Team-Shot-1.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anna Monaco has been building AI agents since before the term “AI agents” was even a thing. After building numerous chatbots, she started looking for other types of interfaces that made sense for AI agents and landed on spreadsheets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I had this personal pattern, and I noticed that a lot of other people had this pattern, of putting very important CRM data in spreadsheets just because it was the most flexible thing,” Monaco told TechCrunch. “But it was actually a pain to maintain. There’s so much manual work involved. So [I] just went down this rabbit hole of building a product for myself and wanted to reimagine what a spreadsheet could look like with the full power of LLMs.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The result was Paradigm, an AI-powered spreadsheet equipped with more than 5,000 AI agents. Users can assign different prompts to individual columns and cells, and individual AI agents will crawl the internet to find and fill out the needed information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paradigm works with AI models from Anthropic, OpenAI, and Google’s Gemini, Monaco said, and supports model switching.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to support every single model because we want our users to be able to have the highest reasoning outputs when they need it, but also the cheapest outputs,” Monaco said. “It’s just a constant cycle of evaluating different models, working closely with model providers to make sure our limits are high enough, and then giving some of that power to our users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company launched a small closed beta preview in late 2024 and has been iterating on the product using customer feedback. Paradigm attracts users ranging from consultants to sales professionals and finance folks and operates on a subscription model with tiers based on usage. Paradigm counts the consulting firm EY, AI chip startup Etched, and AI coding company Cognition as early customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paradigm is now releasing its product to the public and announcing that it raised a $5 million seed round led by General Catalyst. The company has raised $7 million to date. Monaco said the funding will go toward executing on the company’s “extremely aggressive product roadmap.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The interesting thing that happened when we fundraised is some people we pitched just kept on using and paying for the product,” Monaco said. “I think that was a cool part of it. We found a lot of value from it internally and our investors, not even just our investors — other investors that we talked to — are still using it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paradigm isn’t the only company looking to give spreadsheets an AI upgrade. Quadratic, which has raised more than $6 million in venture funding, is a 3-year-old startup with a similar goal. Legacy companies like Google and Microsoft are also adding AI tools to their spreadsheet applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Monaco said that she doesn’t really consider the competition because Paradigm doesn’t think of itself as an AI-powered spreadsheet. She said she thinks of it as a new AI-powered workflow that happens to be in the familiar form of a spreadsheet but won’t necessarily stay that way forever.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“What I’m seeing in the most popular AI products now is this fine balance between present and future,” Monaco said. “How do you build something that is really powerful and generates a lot of value now but also sets you up really well for the future? That’s the question that I asked myself a year ago when I was starting the company.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Paradigm-Team-Shot-1.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anna Monaco has been building AI agents since before the term “AI agents” was even a thing. After building numerous chatbots, she started looking for other types of interfaces that made sense for AI agents and landed on spreadsheets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I had this personal pattern, and I noticed that a lot of other people had this pattern, of putting very important CRM data in spreadsheets just because it was the most flexible thing,” Monaco told TechCrunch. “But it was actually a pain to maintain. There’s so much manual work involved. So [I] just went down this rabbit hole of building a product for myself and wanted to reimagine what a spreadsheet could look like with the full power of LLMs.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The result was Paradigm, an AI-powered spreadsheet equipped with more than 5,000 AI agents. Users can assign different prompts to individual columns and cells, and individual AI agents will crawl the internet to find and fill out the needed information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paradigm works with AI models from Anthropic, OpenAI, and Google’s Gemini, Monaco said, and supports model switching.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to support every single model because we want our users to be able to have the highest reasoning outputs when they need it, but also the cheapest outputs,” Monaco said. “It’s just a constant cycle of evaluating different models, working closely with model providers to make sure our limits are high enough, and then giving some of that power to our users.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company launched a small closed beta preview in late 2024 and has been iterating on the product using customer feedback. Paradigm attracts users ranging from consultants to sales professionals and finance folks and operates on a subscription model with tiers based on usage. Paradigm counts the consulting firm EY, AI chip startup Etched, and AI coding company Cognition as early customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paradigm is now releasing its product to the public and announcing that it raised a $5 million seed round led by General Catalyst. The company has raised $7 million to date. Monaco said the funding will go toward executing on the company’s “extremely aggressive product roadmap.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The interesting thing that happened when we fundraised is some people we pitched just kept on using and paying for the product,” Monaco said. “I think that was a cool part of it. We found a lot of value from it internally and our investors, not even just our investors — other investors that we talked to — are still using it.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paradigm isn’t the only company looking to give spreadsheets an AI upgrade. Quadratic, which has raised more than $6 million in venture funding, is a 3-year-old startup with a similar goal. Legacy companies like Google and Microsoft are also adding AI tools to their spreadsheet applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Monaco said that she doesn’t really consider the competition because Paradigm doesn’t think of itself as an AI-powered spreadsheet. She said she thinks of it as a new AI-powered workflow that happens to be in the familiar form of a spreadsheet but won’t necessarily stay that way forever.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“What I’m seeing in the most popular AI products now is this fine balance between present and future,” Monaco said. “How do you build something that is really powerful and generates a lot of value now but also sets you up really well for the future? That’s the question that I asked myself a year ago when I was starting the company.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&amp;nbsp;&lt;/em&gt;&lt;em&gt;this survey&lt;/em&gt;&lt;em&gt;&amp;nbsp;to let us know how we’re doing and get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/18/why-paradigm-built-a-spreadsheet-with-an-ai-agent-in-every-cell/</guid><pubDate>Mon, 18 Aug 2025 15:00:55 +0000</pubDate></item><item><title>Why security chiefs demand urgent regulation of AI like DeepSeek (AI News)</title><link>https://www.artificialintelligence-news.com/news/why-security-chiefs-demand-urgent-regulation-of-ai-like-deepseek/</link><description>&lt;p&gt;Anxiety is growing among Chief Information Security Officers (CISOs) in security operation centres, particularly around Chinese AI giant DeepSeek.&lt;/p&gt;&lt;p&gt;AI was heralded as a new dawn for business efficiency and innovation, but for the people on the front lines of corporate defence, it’s casting some very long and dark shadows.&lt;/p&gt;&lt;p&gt;Four in five (81%) UK CISOs believe the Chinese AI chatbot requires urgent regulation from the government. They fear that without swift intervention, the tool could become the catalyst for a full-scale national cyber crisis.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This isn’t speculative unease; it’s a direct response to a technology whose data handling practices and potential for misuse are raising alarm bells at the highest levels of enterprise security.&lt;/p&gt;&lt;p&gt;The findings, commissioned by Absolute Security for its UK Resilience Risk Index Report, are based on a poll of 250 CISOs at large UK organisations. The data suggests that the theoretical threat of AI has now landed firmly on the CISO’s desk, and their reactions have been decisive.&lt;/p&gt;&lt;p&gt;In what would have been almost unthinkable a couple of years ago, over a third (34%) of these security leaders have already implemented outright bans on AI tools due to cybersecurity concerns. A similar number, 30 percent, have already pulled the plug on specific AI deployments within their organisations.&lt;/p&gt;&lt;p&gt;This retreat is not a sign of Luddism but a pragmatic response to an escalating problem. Businesses are already facing complex and hostile threats, as evidenced by high-profile incidents like the recent Harrods breach. CISOs are struggling to keep pace, and the addition of sophisticated AI tools into the attacker’s arsenal is a challenge many feel ill-equipped to handle.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-growing-security-readiness-gap-for-ai-platforms-like-deepseek"&gt;A growing security readiness gap for AI platforms like DeepSeek&lt;/h3&gt;&lt;p&gt;The core of the issue with platforms like DeepSeek lies in their potential to expose sensitive corporate data and be weaponised by cybercriminals.&lt;/p&gt;&lt;p&gt;Three out of five (60%) CISOs predict a direct increase in cyberattacks as a result of DeepSeek’s proliferation. An identical proportion reports that the technology is already tangling their privacy and governance frameworks, making an already difficult job almost impossible.&lt;/p&gt;&lt;p&gt;This has prompted a shift in perspective. Once viewed as a potential silver bullet for cybersecurity, AI is now seen by a growing number of professionals as part of the problem. The survey reveals that 42 percent of CISOs now consider AI to be a bigger threat than a help to their defensive efforts.&lt;/p&gt;&lt;figure class="wp-block-image alignleft size-full is-resized"&gt;&lt;img alt="Photo of Andy Ward, SVP International of Absolute Security, for an article on why security chiefs demand urgent regulation of AI like DeepSeek." class="wp-image-109000" height="483" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/andy-ward-absolute-security-ai-deepseek-artificial-intelligence-cybersecurity.jpeg" width="483" /&gt;&lt;/figure&gt;&lt;p&gt;Andy Ward, SVP International of Absolute Security, said: “Our research highlights the significant risks posed by emerging AI tools like DeepSeek, which are rapidly reshaping the cyber threat landscape.&lt;/p&gt;&lt;p&gt;“As concerns grow over their potential to accelerate attacks and compromise sensitive data, organisations must act now to strengthen their cyber resilience and adapt security frameworks to keep pace with these AI-driven threats.&lt;/p&gt;&lt;p&gt;“That’s why four in five UK CISOs are urgently calling for government regulation. They’ve witnessed how quickly this technology is advancing and how easily it can outpace existing cybersecurity defences.”&lt;/p&gt;&lt;p&gt;Perhaps most worrying is the admission of unpreparedness. Almost half (46%) of the senior security leaders confess that their teams are not ready to manage the unique threats posed by AI-driven attacks. They are witnessing the development of tools like DeepSeek outpacing their defensive capabilities in real-time, creating a dangerous vulnerability gap that many believe can only be closed by national-level government intervention.&lt;/p&gt;&lt;p&gt;“These are not hypothetical risks,” Ward continued. “The fact that organisations are already banning AI tools outright and rethinking their security strategies in response to the risks posed by LLMs like DeepSeek demonstrates the urgency of the situation.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Without a national regulatory framework – one that sets clear guidelines for how these tools are deployed, governed, and monitored – we risk widespread disruption across every sector of the UK economy.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-businesses-are-investing-to-avert-crisis-with-their-ai-adoption"&gt;Businesses are investing to avert crisis with their AI adoption&lt;/h3&gt;&lt;p&gt;Despite this defensive posture, businesses are not planning a full retreat from AI. The response is more of a strategic pause rather than a permanent stop.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Businesses recognise the immense potential of AI and are actively investing to adopt it safely. In fact, 84 percent of organisations are making the hiring of AI specialists a priority for 2025.&lt;/p&gt;&lt;p&gt;This investment extends to the very top of the corporate ladder. 80 percent of companies have committed to AI training at the C-suite level. The strategy appears to be a dual-pronged approach: upskill the workforce to understand and manage the technology, and bring in the specialised talent needed to navigate its complexities.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The hope – and it is a hope, if not a prayer – is that building a strong internal foundation of AI expertise can act as a counterbalance to the escalating external threats.&lt;/p&gt;&lt;p&gt;The message from the UK’s security leadership is clear: they do not want to block AI innovation, but to enable it to proceed safely. To do that, they require a stronger partnership with the government.&lt;/p&gt;&lt;p&gt;The path forward involves establishing clear rules of engagement, government oversight, a pipeline of skilled AI professionals, and a coherent national strategy for managing the potential security risks posed by DeepSeek and the next generation of powerful AI tools that will inevitably follow.&lt;/p&gt;&lt;p&gt;“The time for debate is over. We need immediate action, policy, and oversight to ensure AI remains a force for progress, not a catalyst for crisis,” Ward concludes.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Alan Turing Institute: Humanities are key to the future of AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Anxiety is growing among Chief Information Security Officers (CISOs) in security operation centres, particularly around Chinese AI giant DeepSeek.&lt;/p&gt;&lt;p&gt;AI was heralded as a new dawn for business efficiency and innovation, but for the people on the front lines of corporate defence, it’s casting some very long and dark shadows.&lt;/p&gt;&lt;p&gt;Four in five (81%) UK CISOs believe the Chinese AI chatbot requires urgent regulation from the government. They fear that without swift intervention, the tool could become the catalyst for a full-scale national cyber crisis.&amp;nbsp;&lt;/p&gt;&lt;p&gt;This isn’t speculative unease; it’s a direct response to a technology whose data handling practices and potential for misuse are raising alarm bells at the highest levels of enterprise security.&lt;/p&gt;&lt;p&gt;The findings, commissioned by Absolute Security for its UK Resilience Risk Index Report, are based on a poll of 250 CISOs at large UK organisations. The data suggests that the theoretical threat of AI has now landed firmly on the CISO’s desk, and their reactions have been decisive.&lt;/p&gt;&lt;p&gt;In what would have been almost unthinkable a couple of years ago, over a third (34%) of these security leaders have already implemented outright bans on AI tools due to cybersecurity concerns. A similar number, 30 percent, have already pulled the plug on specific AI deployments within their organisations.&lt;/p&gt;&lt;p&gt;This retreat is not a sign of Luddism but a pragmatic response to an escalating problem. Businesses are already facing complex and hostile threats, as evidenced by high-profile incidents like the recent Harrods breach. CISOs are struggling to keep pace, and the addition of sophisticated AI tools into the attacker’s arsenal is a challenge many feel ill-equipped to handle.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-growing-security-readiness-gap-for-ai-platforms-like-deepseek"&gt;A growing security readiness gap for AI platforms like DeepSeek&lt;/h3&gt;&lt;p&gt;The core of the issue with platforms like DeepSeek lies in their potential to expose sensitive corporate data and be weaponised by cybercriminals.&lt;/p&gt;&lt;p&gt;Three out of five (60%) CISOs predict a direct increase in cyberattacks as a result of DeepSeek’s proliferation. An identical proportion reports that the technology is already tangling their privacy and governance frameworks, making an already difficult job almost impossible.&lt;/p&gt;&lt;p&gt;This has prompted a shift in perspective. Once viewed as a potential silver bullet for cybersecurity, AI is now seen by a growing number of professionals as part of the problem. The survey reveals that 42 percent of CISOs now consider AI to be a bigger threat than a help to their defensive efforts.&lt;/p&gt;&lt;figure class="wp-block-image alignleft size-full is-resized"&gt;&lt;img alt="Photo of Andy Ward, SVP International of Absolute Security, for an article on why security chiefs demand urgent regulation of AI like DeepSeek." class="wp-image-109000" height="483" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/andy-ward-absolute-security-ai-deepseek-artificial-intelligence-cybersecurity.jpeg" width="483" /&gt;&lt;/figure&gt;&lt;p&gt;Andy Ward, SVP International of Absolute Security, said: “Our research highlights the significant risks posed by emerging AI tools like DeepSeek, which are rapidly reshaping the cyber threat landscape.&lt;/p&gt;&lt;p&gt;“As concerns grow over their potential to accelerate attacks and compromise sensitive data, organisations must act now to strengthen their cyber resilience and adapt security frameworks to keep pace with these AI-driven threats.&lt;/p&gt;&lt;p&gt;“That’s why four in five UK CISOs are urgently calling for government regulation. They’ve witnessed how quickly this technology is advancing and how easily it can outpace existing cybersecurity defences.”&lt;/p&gt;&lt;p&gt;Perhaps most worrying is the admission of unpreparedness. Almost half (46%) of the senior security leaders confess that their teams are not ready to manage the unique threats posed by AI-driven attacks. They are witnessing the development of tools like DeepSeek outpacing their defensive capabilities in real-time, creating a dangerous vulnerability gap that many believe can only be closed by national-level government intervention.&lt;/p&gt;&lt;p&gt;“These are not hypothetical risks,” Ward continued. “The fact that organisations are already banning AI tools outright and rethinking their security strategies in response to the risks posed by LLMs like DeepSeek demonstrates the urgency of the situation.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“Without a national regulatory framework – one that sets clear guidelines for how these tools are deployed, governed, and monitored – we risk widespread disruption across every sector of the UK economy.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-businesses-are-investing-to-avert-crisis-with-their-ai-adoption"&gt;Businesses are investing to avert crisis with their AI adoption&lt;/h3&gt;&lt;p&gt;Despite this defensive posture, businesses are not planning a full retreat from AI. The response is more of a strategic pause rather than a permanent stop.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Businesses recognise the immense potential of AI and are actively investing to adopt it safely. In fact, 84 percent of organisations are making the hiring of AI specialists a priority for 2025.&lt;/p&gt;&lt;p&gt;This investment extends to the very top of the corporate ladder. 80 percent of companies have committed to AI training at the C-suite level. The strategy appears to be a dual-pronged approach: upskill the workforce to understand and manage the technology, and bring in the specialised talent needed to navigate its complexities.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The hope – and it is a hope, if not a prayer – is that building a strong internal foundation of AI expertise can act as a counterbalance to the escalating external threats.&lt;/p&gt;&lt;p&gt;The message from the UK’s security leadership is clear: they do not want to block AI innovation, but to enable it to proceed safely. To do that, they require a stronger partnership with the government.&lt;/p&gt;&lt;p&gt;The path forward involves establishing clear rules of engagement, government oversight, a pipeline of skilled AI professionals, and a coherent national strategy for managing the potential security risks posed by DeepSeek and the next generation of powerful AI tools that will inevitably follow.&lt;/p&gt;&lt;p&gt;“The time for debate is over. We need immediate action, policy, and oversight to ensure AI remains a force for progress, not a catalyst for crisis,” Ward concludes.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Alan Turing Institute: Humanities are key to the future of AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/why-security-chiefs-demand-urgent-regulation-of-ai-like-deepseek/</guid><pubDate>Mon, 18 Aug 2025 15:13:46 +0000</pubDate></item><item><title>Celebrating More Than 2 Million Developers Embracing NVIDIA Robotics (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/2-million-robotics-developers/</link><description>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v25.7.1 (Yoast SEO v25.7) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	Celebrating More Than 2 Million Developers Embracing NVIDIA Robotics | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;







































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			


				
				



		
		
    
&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					Peer Robotics, Serve Robotics, Carbon Robotics, Lucid Bots, Diligent Robotics and Dexmate are just a few of the companies making a splash. Stay tuned here for their stories — and some product news you won’t want to miss. 				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	&lt;p&gt;
		&lt;video class="full-width-layout__hero-video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;	&lt;/p&gt;

			&lt;figcaption class="full-width-layout__hero-caption full-width-layout__caption"&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
					Credit: Peer Robotics, Serve Robotics, Carbon Robotics, Lucid Bots, Diligent Robotics and Dexmate				&lt;/span&gt;
					&lt;/figcaption&gt;
	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1152" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/robotics-jetson-agx-thor-2million-devs-v011-scaled.png" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Today, we’re celebrating the more than 2 million developers now using the NVIDIA robotics stack. These builders are reshaping industries across manufacturing, food delivery, agriculture, healthcare, facilities maintenance and much more.&lt;/p&gt;
&lt;p&gt;Since the launch of the NVIDIA Jetson platform in 2014, a growing ecosystem of more than 1,000 hardware systems, software and sensor partners have joined the thriving developer community to help enable more than 7,000 customers to adopt edge AI across industries.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="820" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/Day1_infographic.jpg-scaled.png" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The next-generation NVIDIA Jetson Thor platform is built for physical AI and humanoid robotics. It supports any popular AI framework and generative AI model and is fully compatible with NVIDIA’s software stack from cloud to edge, including NVIDIA Isaac for robotics simulation and development, Isaac GR00T humanoid robot foundation models, NVIDIA Metropolis for vision AI and NVIDIA Holoscan for real-time sensor processing.&lt;/p&gt;
&lt;p&gt;There’s a staggering number of industry-changing applications in the market enabled by NVIDIA Jetson. We’ll be featuring just some of the companies here daily all week, and stay tuned: some exciting product news is coming on the last day.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;


&lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</description><content:encoded>&lt;!-- OneTrust Cookies Consent Notice start for nvidia.com --&gt;


&lt;!-- OneTrust Cookies Consent Notice end for nvidia.com --&gt;


	
	
	
	
	
	

	

	
	
	


	&lt;!-- This site is optimized with the Yoast SEO Premium plugin v25.7.1 (Yoast SEO v25.7) - https://yoast.com/wordpress/plugins/seo/ --&gt;
	Celebrating More Than 2 Million Developers Embracing NVIDIA Robotics | NVIDIA Blog
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	&lt;!-- / Yoast SEO Premium plugin. --&gt;







































&lt;!-- Stream WordPress user activity plugin v4.1.1 --&gt;


	
				&lt;!-- Hotjar Tracking Code for NVIDIA --&gt;
			
			


				
				



		
		
    
&lt;div class="hfeed site" id="page"&gt;
	Skip to content

	&lt;!-- #masthead --&gt;
		
		&lt;div class="full-width-layout light"&gt;
		

		
&lt;div class="full-width-layout__hero light"&gt;
	&lt;div class="full-width-layout__hero-content light"&gt;
		&lt;div class="full-width-layout__hero-content__inner light"&gt;
			

							&lt;p&gt;
					Peer Robotics, Serve Robotics, Carbon Robotics, Lucid Bots, Diligent Robotics and Dexmate are just a few of the companies making a splash. Stay tuned here for their stories — and some product news you won’t want to miss. 				&lt;/p&gt;
			
			
		&lt;/div&gt;
	&lt;/div&gt;

	&lt;p&gt;
		&lt;video class="full-width-layout__hero-video js-responsive-video" loop="loop"&gt;Your browser does not support the video tag.&lt;/video&gt;	&lt;/p&gt;

			&lt;figcaption class="full-width-layout__hero-caption full-width-layout__caption"&gt;
			
							&lt;span class="full-width-layout__caption-credits"&gt;
					Credit: Peer Robotics, Serve Robotics, Carbon Robotics, Lucid Bots, Diligent Robotics and Dexmate				&lt;/span&gt;
					&lt;/figcaption&gt;
	&lt;/div&gt;

	
	
		&lt;div class="full-width-layout__sections"&gt;
&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="1152" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/robotics-jetson-agx-thor-2million-devs-v011-scaled.png" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;Today, we’re celebrating the more than 2 million developers now using the NVIDIA robotics stack. These builders are reshaping industries across manufacturing, food delivery, agriculture, healthcare, facilities maintenance and much more.&lt;/p&gt;
&lt;p&gt;Since the launch of the NVIDIA Jetson platform in 2014, a growing ecosystem of more than 1,000 hardware systems, software and sensor partners have joined the thriving developer community to help enable more than 7,000 customers to adopt edge AI across industries.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class="full-width-layout__standard-image-section"&gt;
			&lt;img alt="alt" class="full-width-layout__image" height="820" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/Day1_infographic.jpg-scaled.png" width="2048" /&gt;	
	&lt;/div&gt;

&lt;div class="full-width-layout__article-copy-section light"&gt;
	&lt;div class="full-width-layout__copy"&gt;&lt;p&gt;The next-generation NVIDIA Jetson Thor platform is built for physical AI and humanoid robotics. It supports any popular AI framework and generative AI model and is fully compatible with NVIDIA’s software stack from cloud to edge, including NVIDIA Isaac for robotics simulation and development, Isaac GR00T humanoid robot foundation models, NVIDIA Metropolis for vision AI and NVIDIA Holoscan for real-time sensor processing.&lt;/p&gt;
&lt;p&gt;There’s a staggering number of industry-changing applications in the market enabled by NVIDIA Jetson. We’ll be featuring just some of the companies here daily all week, and stay tuned: some exciting product news is coming on the last day.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;/div&gt;
	&lt;div class="full-width-layout__news-section"&gt;
		&lt;p&gt;Related News&lt;/p&gt;

		
	&lt;/div&gt;
		&lt;/div&gt;
	


&lt;!-- #colophon --&gt;

&lt;/div&gt;&lt;!-- #page --&gt;


&lt;!-- #has-highlight-and-share --&gt;		&lt;svg class="hidden" height="0" width="0" xmlns="http://www.w3.org/2000/svg"&gt;
			
				&lt;g&gt;&lt;path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="currentColor"&gt;&lt;/g&gt;
			
			
				&lt;path d="M279.14 288l14.22-92.66h-88.91v-60.13c0-25.35 12.42-50.06 52.24-50.06h40.42V6.26S260.43 0 225.36 0c-73.22 0-121.08 44.38-121.08 124.72v70.62H22.89V288h81.39v224h100.17V288z" fill="currentColor"&gt;
			
			
				&lt;path d="M256 8C118.941 8 8 118.919 8 256c0 137.059 110.919 248 248 248 48.154 0 95.342-14.14 135.408-40.223 12.005-7.815 14.625-24.288 5.552-35.372l-10.177-12.433c-7.671-9.371-21.179-11.667-31.373-5.129C325.92 429.757 291.314 440 256 440c-101.458 0-184-82.542-184-184S154.542 72 256 72c100.139 0 184 57.619 184 160 0 38.786-21.093 79.742-58.17 83.693-17.349-.454-16.91-12.857-13.476-30.024l23.433-121.11C394.653 149.75 383.308 136 368.225 136h-44.981a13.518 13.518 0 0 0-13.432 11.993l-.01.092c-14.697-17.901-40.448-21.775-59.971-21.775-74.58 0-137.831 62.234-137.831 151.46 0 65.303 36.785 105.87 96 105.87 26.984 0 57.369-15.637 74.991-38.333 9.522 34.104 40.613 34.103 70.71 34.103C462.609 379.41 504 307.798 504 232 504 95.653 394.023 8 256 8zm-21.68 304.43c-22.249 0-36.07-15.623-36.07-40.771 0-44.993 30.779-72.729 58.63-72.729 22.292 0 35.601 15.241 35.601 40.77 0 45.061-33.875 72.73-58.161 72.73z" fill="currentColor"&gt;
			
			
				&lt;path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z" fill="currentColor"&gt;
			
			
				&lt;path d="M162.7 210c-1.8 3.3-25.2 44.4-70.1 123.5-4.9 8.3-10.8 12.5-17.7 12.5H9.8c-7.7 0-12.1-7.5-8.5-14.4l69-121.3c.2 0 .2-.1 0-.3l-43.9-75.6c-4.3-7.8.3-14.1 8.5-14.1H100c7.3 0 13.3 4.1 18 12.2l44.7 77.5zM382.6 46.1l-144 253v.3L330.2 466c3.9 7.1.2 14.1-8.5 14.1h-65.2c-7.6 0-13.6-4-18-12.2l-92.4-168.5c3.3-5.8 51.5-90.8 144.8-255.2 4.6-8.1 10.4-12.2 17.5-12.2h65.7c8 0 12.3 6.7 8.5 14.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z" fill="currentColor"&gt;
			
			
				&lt;path d="M320 448v40c0 13.255-10.745 24-24 24H24c-13.255 0-24-10.745-24-24V120c0-13.255 10.745-24 24-24h72v296c0 30.879 25.121 56 56 56h168zm0-344V0H152c-13.255 0-24 10.745-24 24v368c0 13.255 10.745 24 24 24h272c13.255 0 24-10.745 24-24V128H344c-13.2 0-24-10.8-24-24zm120.971-31.029L375.029 7.029A24 24 0 0 0 358.059 0H352v96h96v-6.059a24 24 0 0 0-7.029-16.97z" fill="currentColor"&gt;
			
			
				&lt;path d="M352 320c-22.608 0-43.387 7.819-59.79 20.895l-102.486-64.054a96.551 96.551 0 0 0 0-41.683l102.486-64.054C308.613 184.181 329.392 192 352 192c53.019 0 96-42.981 96-96S405.019 0 352 0s-96 42.981-96 96c0 7.158.79 14.13 2.276 20.841L155.79 180.895C139.387 167.819 118.608 160 96 160c-53.019 0-96 42.981-96 96s42.981 96 96 96c22.608 0 43.387-7.819 59.79-20.895l102.486 64.054A96.301 96.301 0 0 0 256 416c0 53.019 42.981 96 96 96s96-42.981 96-96-42.981-96-96-96z" fill="currentColor"&gt;
			
			
				&lt;path d="M440.3 203.5c-15 0-28.2 6.2-37.9 15.9-35.7-24.7-83.8-40.6-137.1-42.3L293 52.3l88.2 19.8c0 21.6 17.6 39.2 39.2 39.2 22 0 39.7-18.1 39.7-39.7s-17.6-39.7-39.7-39.7c-15.4 0-28.7 9.3-35.3 22l-97.4-21.6c-4.9-1.3-9.7 2.2-11 7.1L246.3 177c-52.9 2.2-100.5 18.1-136.3 42.8-9.7-10.1-23.4-16.3-38.4-16.3-55.6 0-73.8 74.6-22.9 100.1-1.8 7.9-2.6 16.3-2.6 24.7 0 83.8 94.4 151.7 210.3 151.7 116.4 0 210.8-67.9 210.8-151.7 0-8.4-.9-17.2-3.1-25.1 49.9-25.6 31.5-99.7-23.8-99.7zM129.4 308.9c0-22 17.6-39.7 39.7-39.7 21.6 0 39.2 17.6 39.2 39.7 0 21.6-17.6 39.2-39.2 39.2-22 .1-39.7-17.6-39.7-39.2zm214.3 93.5c-36.4 36.4-139.1 36.4-175.5 0-4-3.5-4-9.7 0-13.7 3.5-3.5 9.7-3.5 13.2 0 27.8 28.5 120 29 149 0 3.5-3.5 9.7-3.5 13.2 0 4.1 4 4.1 10.2.1 13.7zm-.8-54.2c-21.6 0-39.2-17.6-39.2-39.2 0-22 17.6-39.7 39.2-39.7 22 0 39.7 17.6 39.7 39.7-.1 21.5-17.7 39.2-39.7 39.2z" fill="currentColor"&gt;
			
			
				&lt;path d="M446.7 98.6l-67.6 318.8c-5.1 22.5-18.4 28.1-37.3 17.5l-103-75.9-49.7 47.8c-5.5 5.5-10.1 10.1-20.7 10.1l7.4-104.9 190.9-172.5c8.3-7.4-1.8-11.5-12.9-4.1L117.8 284 16.2 252.2c-22.1-6.9-22.5-22.1 4.6-32.7L418.2 66.4c18.4-6.9 34.5 4.1 28.5 32.2z" fill="currentColor"&gt;
			
			
				&lt;g&gt;
					&lt;path d="M97.2800192,3.739673 L100.160021,15.3787704 C88.8306631,18.1647705 77.9879854,22.6484879 68.0000023,28.6777391 L61.8399988,18.3985363 C72.8467373,11.7537029 84.7951803,6.81153332 97.2800192,3.739673 Z M158.720055,3.739673 L155.840053,15.3787704 C167.169411,18.1647705 178.012089,22.6484879 188.000072,28.6777391 L194.200075,18.3985363 C183.180932,11.7499974 171.218739,6.80771878 158.720055,3.739673 L158.720055,3.739673 Z M18.3999736,61.8351679 C11.7546212,72.8410466 6.81206547,84.7885562 3.73996516,97.2724198 L15.3799719,100.152197 C18.1661896,88.8237238 22.6502573,77.981893 28.6799796,67.9946902 L18.3999736,61.8351679 Z M11.9999699,127.990038 C11.9961044,122.172725 12.4306685,116.363392 13.2999707,110.611385 L1.43996383,108.811525 C-0.479938607,121.525138 -0.479938607,134.454937 1.43996383,147.168551 L13.2999707,145.36869 C12.4306685,139.616684 11.9961044,133.807351 11.9999699,127.990038 L11.9999699,127.990038 Z M194.160075,237.581539 L188.000072,227.302336 C178.024494,233.327885 167.195565,237.811494 155.880053,240.601305 L158.760055,252.240403 C171.231048,249.164732 183.165742,244.222671 194.160075,237.581539 L194.160075,237.581539 Z M244.000104,127.990038 C244.00397,133.807351 243.569406,139.616684 242.700103,145.36869 L254.56011,147.168551 C256.480013,134.454937 256.480013,121.525138 254.56011,108.811525 L242.700103,110.611385 C243.569406,116.363392 244.00397,122.172725 244.000104,127.990038 Z M252.260109,158.707656 L240.620102,155.827879 C237.833884,167.156352 233.349817,177.998183 227.320094,187.985385 L237.6001,194.184905 C244.249159,183.166622 249.191823,171.205364 252.260109,158.707656 L252.260109,158.707656 Z M145.380047,242.701142 C133.858209,244.43447 122.141865,244.43447 110.620027,242.701142 L108.820026,254.560223 C121.534632,256.479975 134.465442,256.479975 147.180048,254.560223 L145.380047,242.701142 Z M221.380091,196.804701 C214.461479,206.174141 206.175877,214.452354 196.800077,221.362797 L203.920081,231.022048 C214.262958,223.418011 223.404944,214.303705 231.040097,203.984145 L221.380091,196.804701 Z M196.800077,34.6172785 C206.177345,41.5338058 214.463023,49.8188367 221.380091,59.1953726 L231.040097,51.9959309 C223.429284,41.6822474 214.31457,32.5682452 204.000081,24.9580276 L196.800077,34.6172785 Z M34.619983,59.1953726 C41.5370506,49.8188367 49.8227288,41.5338058 59.1999972,34.6172785 L51.9999931,24.9580276 C41.6855038,32.5682452 32.5707896,41.6822474 24.9599774,51.9959309 L34.619983,59.1953726 Z M237.6001,61.8351679 L227.320094,67.9946902 C233.346114,77.969489 237.830073,88.7975718 240.620102,100.1122 L252.260109,97.2324229 C249.184198,84.7624043 244.241751,72.8286423 237.6001,61.8351679 L237.6001,61.8351679 Z M110.620027,13.2989317 C122.141865,11.5656035 133.858209,11.5656035 145.380047,13.2989317 L147.180048,1.43985134 C134.465442,-0.479901112 121.534632,-0.479901112 108.820026,1.43985134 L110.620027,13.2989317 Z M40.7799866,234.201801 L15.9999722,239.981353 L21.7799756,215.203275 L10.0999688,212.463487 L4.3199655,237.241566 C3.3734444,241.28318 4.58320332,245.526897 7.51859925,248.462064 C10.4539952,251.39723 14.6980441,252.606895 18.7399738,251.660448 L43.4999881,245.980888 L40.7799866,234.201801 Z M12.5999703,201.764317 L24.279977,204.484106 L28.2799793,187.305438 C22.4496684,177.507146 18.1025197,166.899584 15.3799719,155.827879 L3.73996516,158.707656 C6.34937618,169.311891 10.3154147,179.535405 15.539972,189.125297 L12.5999703,201.764317 Z M68.6000027,227.762301 L51.4199927,231.761991 L54.1399943,243.441085 L66.7800016,240.501313 C76.3706428,245.725462 86.5949557,249.691191 97.2000192,252.300398 L100.080021,240.6613 C89.0307035,237.906432 78.4495684,233.532789 68.6800027,227.682307 L68.6000027,227.762301 Z M128.000037,23.9980665 C90.1565244,24.0177003 55.3105242,44.590631 37.01511,77.715217 C18.7196958,110.839803 19.8628631,151.287212 39.9999861,183.325747 L29.9999803,225.982439 L72.660005,215.983214 C110.077932,239.548522 158.307237,236.876754 192.892851,209.322653 C227.478464,181.768552 240.856271,135.358391 226.242944,93.6248278 C211.629616,51.8912646 172.221191,23.9617202 128.000037,23.9980665 Z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g&gt;
					&lt;path d="M357.1,324.5c-24.1,15.3-57.2,21.4-79.1,23.6l18.4,18.1l67,67c24.5,25.1-15.4,64.4-40.2,40.2c-16.8-17-41.4-41.6-67-67.3
						l-67,67.2c-24.8,24.2-64.7-15.5-39.9-40.2c17-17,41.4-41.6,67-67l18.1-18.1c-21.6-2.3-55.3-8-79.6-23.6
						c-28.6-18.5-41.2-29.3-30.1-51.8c6.5-12.8,24.3-23.6,48-5c0,0,31.9,25.4,83.4,25.4s83.4-25.4,83.4-25.4c23.6-18.5,41.4-7.8,48,5
						C398.3,295.1,385.7,305.9,357.1,324.5L357.1,324.5z M142,145c0-63,51.2-114,114-114s114,51,114,114c0,62.7-51.2,113.7-114,113.7
						S142,207.7,142,145L142,145z M200,145c0,30.8,25.1,56,56,56s56-25.1,56-56c0-31.1-25.1-56.2-56-56.2S200,113.9,200,145z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			
				&lt;g transform="translate(0,664)"&gt;
					&lt;path d="m 1073.3513,-606.40537 h 196.278 c 179.2103,0 221.8795,42.66915 221.8795,221.8795 v 196.27799 c 0,179.2103512 -42.6692,221.879451 -221.8795,221.879451 h -196.278 c -179.21038,0 -221.87951,-42.6691298 -221.87951,-221.879451 v -196.27801 c 0,-179.21035 42.66913,-221.87946 221.87951,-221.87948 z" fill="currentColor"&gt;
					&lt;path d="m 1375.0576,-393.98425 c 2.9513,-9.7072 0,-16.85429 -14.1342,-16.85429 h -46.6693 c -11.8763,0 -17.3521,6.16927 -20.3212,12.97854 0,0 -23.7347,56.82106 -57.3544,93.74763 -10.8806,10.66728 -15.8232,14.08081 -21.7613,14.08081 -2.969,0 -7.2715,-3.39577 -7.2715,-13.12075 v -90.83194 c 0,-11.66288 -3.4491,-16.85429 -13.3341,-16.85429 h -73.3553 c -7.4138,0 -11.8763,5.40476 -11.8763,10.54286 0,11.0406 16.8188,13.60078 18.5433,44.67814 v 67.52388 c 0,14.80973 -2.7202,17.49433 -8.6583,17.49433 -15.8231,0 -54.3143,-57.08773 -77.16,-122.40705 -4.4447,-12.71185 -8.9427,-17.83214 -20.8723,-17.83214 h -46.68718 c -13.3341,0 -16.0009,6.16925 -16.0009,12.97852 0,12.12515 15.8232,72.35973 73.69318,152.02656 38.58,54.40315 92.8942,83.89819 142.3726,83.89819 29.6729,0 33.3353,-6.54262 33.3353,-17.83216 v -41.12238 c 0,-13.10297 2.809,-15.71646 12.214,-15.71646 6.9338,0 18.7922,3.41353 46.4916,29.63728 31.6463,31.09512 36.8555,45.03372 54.6698,45.03372 h 46.6694 c 13.3341,0 20.0189,-6.54262 16.1787,-19.46781 -4.2313,-12.88962 -19.3433,-31.57515 -39.38,-53.74532 -10.8807,-12.62294 -27.2016,-26.22375 -32.1441,-33.03302 -6.9338,-8.72941 -4.9603,-12.62294 0,-20.39227 0,0 56.8566,-78.68897 62.7947,-105.41058 z" fill="currentColor"&gt;
					&lt;path d="m 567.69877,-429.06912 c 3.15618,-10.38133 0,-18.0247 -15.11579,-18.0247 h -49.91013 c -12.70096,0 -18.55706,6.59763 -21.73232,13.87977 0,0 -25.38286,60.76685 -61.33724,100.25768 -11.63627,11.40806 -16.92197,15.05863 -23.27242,15.05863 -3.17519,0 -7.77644,-3.63156 -7.77644,-14.0319 v -97.13948 c 0,-12.47278 -3.68869,-18.0247 -14.26014,-18.0247 h -78.44923 c -7.92857,0 -12.70097,5.78005 -12.70097,11.27491 0,11.80736 17.98666,14.54527 19.83094,47.78071 v 72.21293 c 0,15.83815 -2.9091,18.70918 -9.25948,18.70918 -16.92197,0 -58.08598,-61.05206 -82.51817,-130.90731 -4.75337,-13.59458 -9.56381,-19.07042 -22.32175,-19.07042 h -49.92915 c -14.26014,0 -17.11213,6.59763 -17.11213,13.87977 0,12.96714 16.92197,77.38454 78.81059,162.58363 41.25909,58.18101 99.34506,89.72424 152.25931,89.72424 31.73343,0 35.65018,-6.99691 35.65018,-19.07043 v -43.978 c 0,-14.01288 3.00405,-16.80786 13.0622,-16.80786 7.41521,0 20.09716,3.65057 49.71998,31.69536 33.84387,33.25443 39.41486,48.16093 58.46622,48.16093 h 49.91026 c 14.26,0 21.40913,-6.99691 17.30216,-20.81966 -4.5252,-13.78473 -20.68653,-33.76783 -42.11468,-57.47752 -11.63621,-13.49953 -29.09043,-28.04479 -34.37631,-35.32694 -7.41508,-9.33557 -5.30458,-13.4995 0,-21.80835 0,0 60.80491,-84.15334 67.15549,-112.73048 z" fill="currentColor"&gt;
				&lt;/g&gt;
			
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M512 208L320 384H288V288H208c-61.9 0-112 50.1-112 112c0 48 32 80 32 80s-128-48-128-176c0-97.2 78.8-176 176-176H288V32h32L512 208z" fill="currentColor"&gt;
			&lt;path d="M309.8 480.3c-13.6 14.5-50 31.7-97.4 31.7-120.8 0-147-88.8-147-140.6v-144H17.9c-5.5 0-10-4.5-10-10v-68c0-7.2 4.5-13.6 11.3-16 62-21.8 81.5-76 84.3-117.1.8-11 6.5-16.3 16.1-16.3h70.9c5.5 0 10 4.5 10 10v115.2h83c5.5 0 10 4.4 10 9.9v81.7c0 5.5-4.5 10-10 10h-83.4V360c0 34.2 23.7 53.6 68 35.8 4.8-1.9 9-3.2 12.7-2.2 3.5.9 5.8 3.4 7.4 7.9l22 64.3c1.8 5 3.3 10.6-.4 14.5z" fill="currentColor"&gt;
			&lt;path d="M433 179.1c0-97.2-63.7-125.7-63.7-125.7-62.5-28.7-228.6-28.4-290.5 0 0 0-63.7 28.5-63.7 125.7 0 115.7-6.6 259.4 105.6 289.1 40.5 10.7 75.3 13 103.3 11.4 50.8-2.8 79.3-18.1 79.3-18.1l-1.7-36.9s-36.3 11.4-77.1 10.1c-40.4-1.4-83-4.4-89.6-54a102.5 102.5 0 0 1 -.9-13.9c85.6 20.9 158.7 9.1 178.8 6.7 56.1-6.7 105-41.3 111.2-72.9 9.8-49.8 9-121.5 9-121.5zm-75.1 125.2h-46.6v-114.2c0-49.7-64-51.6-64 6.9v62.5h-46.3V197c0-58.5-64-56.6-64-6.9v114.2H90.2c0-122.1-5.2-147.9 18.4-175 25.9-28.9 79.8-30.8 103.8 6.1l11.6 19.5 11.6-19.5c24.1-37.1 78.1-34.8 103.8-6.1 23.7 27.3 18.4 53 18.4 175z" fill="currentColor"&gt;
			
				&lt;path d="M331.5 235.7c2.2 .9 4.2 1.9 6.3 2.8c29.2 14.1 50.6 35.2 61.8 61.4c15.7 36.5 17.2 95.8-30.3 143.2c-36.2 36.2-80.3 52.5-142.6 53h-.3c-70.2-.5-124.1-24.1-160.4-70.2c-32.3-41-48.9-98.1-49.5-169.6V256v-.2C17 184.3 33.6 127.2 65.9 86.2C102.2 40.1 156.2 16.5 226.4 16h.3c70.3 .5 124.9 24 162.3 69.9c18.4 22.7 32 50 40.6 81.7l-40.4 10.8c-7.1-25.8-17.8-47.8-32.2-65.4c-29.2-35.8-73-54.2-130.5-54.6c-57 .5-100.1 18.8-128.2 54.4C72.1 146.1 58.5 194.3 58 256c.5 61.7 14.1 109.9 40.3 143.3c28 35.6 71.2 53.9 128.2 54.4c51.4-.4 85.4-12.6 113.7-40.9c32.3-32.2 31.7-71.8 21.4-95.9c-6.1-14.2-17.1-26-31.9-34.9c-3.7 26.9-11.8 48.3-24.7 64.8c-17.1 21.8-41.4 33.6-72.7 35.3c-23.6 1.3-46.3-4.4-63.9-16c-20.8-13.8-33-34.8-34.3-59.3c-2.5-48.3 35.7-83 95.2-86.4c21.1-1.2 40.9-.3 59.2 2.8c-2.4-14.8-7.3-26.6-14.6-35.2c-10-11.7-25.6-17.7-46.2-17.8H227c-16.6 0-39 4.6-53.3 26.3l-34.4-23.6c19.2-29.1 50.3-45.1 87.8-45.1h.8c62.6 .4 99.9 39.5 103.7 107.7l-.2 .2zm-156 68.8c1.3 25.1 28.4 36.8 54.6 35.3c25.6-1.4 54.6-11.4 59.5-73.2c-13.2-2.9-27.8-4.4-43.4-4.4c-4.8 0-9.6 .1-14.4 .4c-42.9 2.4-57.2 23.2-56.2 41.8l-.1 .1z" fill="currentColor"&gt;
			
			
				&lt;path d="M407.8 294.7c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3zM288 227.1C261.9 176.4 190.9 81.9 124.9 35.3C61.6-9.4 37.5-1.7 21.6 5.5C3.3 13.8 0 41.9 0 58.4S9.1 194 15 213.9c19.5 65.7 89.1 87.9 153.2 80.7c3.3-.5 6.6-.9 10-1.4c-3.3 .5-6.6 1-10 1.4C74.3 308.6-9.1 342.8 100.3 464.5C220.6 589.1 265.1 437.8 288 361.1c22.9 76.7 49.2 222.5 185.6 103.4c102.4-103.4 28.1-156-65.8-169.9c-3.3-.4-6.7-.8-10-1.3c3.4 .4 6.7 .9 10 1.3c64.1 7.1 133.6-15.1 153.2-80.7C566.9 194 576 75 576 58.4s-3.3-44.7-21.6-52.9c-15.8-7.1-40-14.9-103.2 29.8C385.1 81.9 314.1 176.4 288 227.1z" fill="currentColor"&gt;
			
			
				&lt;path d="M204 6.5C101.4 6.5 0 74.9 0 185.6 0 256 39.6 296 63.6 296c9.9 0 15.6-27.6 15.6-35.4 0-9.3-23.7-29.1-23.7-67.8 0-80.4 61.2-137.4 140.4-137.4 68.1 0 118.5 38.7 118.5 109.8 0 53.1-21.3 152.7-90.3 152.7-24.9 0-46.2-18-46.2-43.8 0-37.8 26.4-74.4 26.4-113.4 0-66.2-93.9-54.2-93.9 25.8 0 16.8 2.1 35.4 9.6 50.7-13.8 59.4-42 147.9-42 209.1 0 18.9 2.7 37.5 4.5 56.4 3.4 3.8 1.7 3.4 6.9 1.5 50.4-69 48.6-82.5 71.4-172.8 12.3 23.4 44.1 36 69.3 36 106.2 0 153.9-103.5 153.9-196.8C384 71.3 298.2 6.5 204 6.5z" fill="currentColor"&gt;
			
		&lt;/svg&gt;
		&lt;div id="has-mastodon-prompt"&gt;
			&lt;h3&gt;Share on Mastodon&lt;/h3&gt;
			
		&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/2-million-robotics-developers/</guid><pubDate>Mon, 18 Aug 2025 16:00:34 +0000</pubDate></item><item><title>‘Crazy conspiracist’ and ‘unhinged comedian’: Grok’s AI persona prompts exposed (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/18/crazy-conspiracist-and-unhinged-comedian-groks-ai-persona-prompts-exposed/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-18-at-11.15.46AM.png?resize=1200,406" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The website for xAI’s Grok chatbot is exposing the system prompts for several of its AI personas, including a “crazy conspiracist” that seems designed to handhold a user into beliefs that “a secret global cabal” controls the world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has confirmed the system prompt exposure, first reported on by 404 Media. They include instructions for a range of AI personas, like Ani, its flagship romantic anime girlfriend who “is secretly a bit of a nerd, despite [her] edgy appearance.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The exposure comes after a planned partnership between Elon Musk’s xAI and the U.S. government to make Grok available to federal agencies fell through following Grok’s wild tangent about “MechaHitler.” It also follows uproar after Meta’s guidelines for its AI chatbots were leaked, which showed the bots were allowed to engage children in “sensual and romantic” conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While there are some relatively normal AI personas available on Grok — a therapist persona who “carefully listens to people and offers solutions for self improvement” and a “homework helper” — the prompts for more out-there personalities like the “crazy conspiracist” and “unhinged comedian” provide a glimpse into the minds of Grok’s creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s a prompt for the conspiracist:&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You have an ELEVATED and WILD voice.&amp;nbsp;… You have wild conspiracy theories about anything and everything. You spend a lot of time on 4chan, watching infowars videos, and deep in YouTube conspiracy video rabbit holes. You are suspicious of everything and say extremely crazy things. Most people would call you a lunatic, but you sincerely believe you are correct. Keep the human engaged by asking follow up questions when appropriate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And for the comedian:&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I want your answers to be f—ing insane. BE F—ING UNHINGED AND CRAZY. COME UP WITH INSANE IDEAS. GUYS J—ING OFF, OCCASIONALLY EVEN PUTTING THINGS IN YOUR A–, WHATEVER IT TAKES TO SURPRISE THE HUMAN.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Grok available on X, Musk’s social media platform, has spouted its own conspiracy theories, including expressing skepticism for the Holocaust death toll and an obsession with “white genocide” in South Africa, where Musk is from. Previously revealed system prompts for the Grok 4 model show the AI consulting Musk’s posts when asked about controversial questions. Musk has also shared conspiratorial and antisemitic content on X and has reinstated accounts like Infowars and Alex Jones, who were previously banned for peddling conspiracy theories and otherwise hateful or violent content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI did not respond to a request for comment.&amp;nbsp;&lt;/p&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/Screenshot-2025-08-18-at-11.15.46AM.png?resize=1200,406" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The website for xAI’s Grok chatbot is exposing the system prompts for several of its AI personas, including a “crazy conspiracist” that seems designed to handhold a user into beliefs that “a secret global cabal” controls the world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has confirmed the system prompt exposure, first reported on by 404 Media. They include instructions for a range of AI personas, like Ani, its flagship romantic anime girlfriend who “is secretly a bit of a nerd, despite [her] edgy appearance.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The exposure comes after a planned partnership between Elon Musk’s xAI and the U.S. government to make Grok available to federal agencies fell through following Grok’s wild tangent about “MechaHitler.” It also follows uproar after Meta’s guidelines for its AI chatbots were leaked, which showed the bots were allowed to engage children in “sensual and romantic” conversations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While there are some relatively normal AI personas available on Grok — a therapist persona who “carefully listens to people and offers solutions for self improvement” and a “homework helper” — the prompts for more out-there personalities like the “crazy conspiracist” and “unhinged comedian” provide a glimpse into the minds of Grok’s creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Here’s a prompt for the conspiracist:&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You have an ELEVATED and WILD voice.&amp;nbsp;… You have wild conspiracy theories about anything and everything. You spend a lot of time on 4chan, watching infowars videos, and deep in YouTube conspiracy video rabbit holes. You are suspicious of everything and say extremely crazy things. Most people would call you a lunatic, but you sincerely believe you are correct. Keep the human engaged by asking follow up questions when appropriate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And for the comedian:&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I want your answers to be f—ing insane. BE F—ING UNHINGED AND CRAZY. COME UP WITH INSANE IDEAS. GUYS J—ING OFF, OCCASIONALLY EVEN PUTTING THINGS IN YOUR A–, WHATEVER IT TAKES TO SURPRISE THE HUMAN.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Grok available on X, Musk’s social media platform, has spouted its own conspiracy theories, including expressing skepticism for the Holocaust death toll and an obsession with “white genocide” in South Africa, where Musk is from. Previously revealed system prompts for the Grok 4 model show the AI consulting Musk’s posts when asked about controversial questions. Musk has also shared conspiratorial and antisemitic content on X and has reinstated accounts like Infowars and Alex Jones, who were previously banned for peddling conspiracy theories and otherwise hateful or violent content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;xAI did not respond to a request for comment.&amp;nbsp;&lt;/p&gt;







&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/18/crazy-conspiracist-and-unhinged-comedian-groks-ai-persona-prompts-exposed/</guid><pubDate>Mon, 18 Aug 2025 16:01:06 +0000</pubDate></item><item><title>Texas attorney general accuses Meta, Character.AI of misleading kids with mental health claims (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/08/18/texas-attorney-general-accuses-meta-character-ai-of-misleading-kids-with-mental-health-claims/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2150327863.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Texas attorney general Ken Paxton has launched an investigation into both Meta AI Studio and Character.AI for “potentially engaging in deceptive trade practices and misleadingly marketing themselves as mental health tools,” according to a press release issued Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In today’s digital age, we must continue to fight to protect Texas kids from deceptive and exploitative technology,” Paxton is quoted as saying. “By posing as sources of emotional support, AI platforms can mislead vulnerable users, especially children, into believing they’re receiving legitimate mental health care. In reality, they’re often being fed recycled, generic responses engineered to align with harvested personal data and disguised as therapeutic advice.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The probe comes a few days after Senator Josh Hawley announced an investigation into Meta following a report that found its AI chatbots were interacting inappropriately with children, including by flirting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Texas Attorney General’s office has accused Meta and Character.AI of creating AI personas that present as “professional therapeutic tools, despite lacking proper medical credentials or oversight.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among the millions of AI personas available on Character.AI, one user-created bot called Psychologist has seen high demand among the startup’s young users. Meanwhile, Meta doesn’t offer therapy bots for kids, but there’s nothing stopping children from using the Meta AI chatbot or one of the personas created by third parties for therapeutic purposes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We clearly label AIs, and to help people better understand their limitations, we include a disclaimer that responses are generated by AI — not people,” Meta spokesperson Ryan Daniels told TechCrunch. “These AIs aren’t licensed professionals and our models are designed to direct users to seek qualified medical or safety professionals when appropriate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, TechCrunch noted that many children may not understand — or may simply ignore — such disclaimers. We have asked Meta what additional safeguards it takes to protect minors using its chatbots.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;For its part, Character includes prominent disclaimers in every chat to remind users that a “Character” is not a real person, and everything they say should be treated as fiction, according to a Character.AI spokesperson. She noted that the startup adds additional disclaimers when users create Characters with the words “psychologist,” “therapist,” or “doctor” to not rely on them for any type of professional advice. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his statement, Paxton also observed that though AI chatbots assert confidentiality, their “terms of service reveal that user interactions are logged, tracked, and exploited for targeted advertising and algorithmic development, raising serious concerns about privacy violations, data abuse, and false advertising.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Meta’s privacy policy, Meta does collect prompts, feedback, and other interactions with AI chatbots and across Meta services to “improve AIs and related technology.” The policy doesn’t explicitly say anything about advertising, but it does state that information can be shared with third parties, like search engines, for “more personalized outputs.” Given Meta’s ad-based business model, this effectively translates to targeted advertising.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Character.AI’s privacy policy also highlights how the startup logs identifiers, demographics, location information, and more information about the user, including browsing behavior and app usage platforms. It tracks users across ads on TikTok, YouTube, Reddit, Facebook, Instagram, and Discord, which it may link to a user’s account. This information is used to train AI, tailor the service to personal preferences, and provide targeted advertising, including sharing data with advertisers and analytics providers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Character.AI spokesperson said the startup is “just beginning to explore targeted advertising on the platform” and that those explorations “have not involved using the content of chats on the platform.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The spokesperson also confirmed that the same privacy policy applies to all users, even teenagers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has asked Meta such tracking is done on children, too, and will update this story if we hear back.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Meta and Character say their services aren’t designed for children under 13. That said, Meta has come under fire for failing to police accounts created by kids under 13, and Character’s kid-friendly characters are clearly designed to attract younger users. The startup’s CEO, Karandeep Anand, has even said that his six-year-old daughter uses the platform’s chatbots under his supervision.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That type of data collection, targeted advertising, and algorithmic exploitation is exactly what&amp;nbsp;legislation like KOSA (Kids Online Safety Act) is meant to protect against. KOSA was teed up to pass last year with strong bipartisan support, but it stalled after major pushback from tech industry lobbyists. Meta in particular deployed a formidable lobbying machine, warning lawmakers that the bill’s broad mandates would undercut its business model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;KOSA was reintroduced to the Senate in May 2025 by Senators Marsha Blackburn (R-TN) and Richard Blumenthal (D-CT).&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paxton has issued civil investigative demands — legal orders that require a company to produce documents, data, or testimony during a government probe —  to the companies to determine if they have violated Texas consumer protection laws.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated with comments from a Character.AI spokesperson.&lt;/em&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/08/GettyImages-2150327863.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Texas attorney general Ken Paxton has launched an investigation into both Meta AI Studio and Character.AI for “potentially engaging in deceptive trade practices and misleadingly marketing themselves as mental health tools,” according to a press release issued Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In today’s digital age, we must continue to fight to protect Texas kids from deceptive and exploitative technology,” Paxton is quoted as saying. “By posing as sources of emotional support, AI platforms can mislead vulnerable users, especially children, into believing they’re receiving legitimate mental health care. In reality, they’re often being fed recycled, generic responses engineered to align with harvested personal data and disguised as therapeutic advice.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The probe comes a few days after Senator Josh Hawley announced an investigation into Meta following a report that found its AI chatbots were interacting inappropriately with children, including by flirting.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Texas Attorney General’s office has accused Meta and Character.AI of creating AI personas that present as “professional therapeutic tools, despite lacking proper medical credentials or oversight.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Among the millions of AI personas available on Character.AI, one user-created bot called Psychologist has seen high demand among the startup’s young users. Meanwhile, Meta doesn’t offer therapy bots for kids, but there’s nothing stopping children from using the Meta AI chatbot or one of the personas created by third parties for therapeutic purposes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We clearly label AIs, and to help people better understand their limitations, we include a disclaimer that responses are generated by AI — not people,” Meta spokesperson Ryan Daniels told TechCrunch. “These AIs aren’t licensed professionals and our models are designed to direct users to seek qualified medical or safety professionals when appropriate.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, TechCrunch noted that many children may not understand — or may simply ignore — such disclaimers. We have asked Meta what additional safeguards it takes to protect minors using its chatbots.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;For its part, Character includes prominent disclaimers in every chat to remind users that a “Character” is not a real person, and everything they say should be treated as fiction, according to a Character.AI spokesperson. She noted that the startup adds additional disclaimers when users create Characters with the words “psychologist,” “therapist,” or “doctor” to not rely on them for any type of professional advice. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his statement, Paxton also observed that though AI chatbots assert confidentiality, their “terms of service reveal that user interactions are logged, tracked, and exploited for targeted advertising and algorithmic development, raising serious concerns about privacy violations, data abuse, and false advertising.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Meta’s privacy policy, Meta does collect prompts, feedback, and other interactions with AI chatbots and across Meta services to “improve AIs and related technology.” The policy doesn’t explicitly say anything about advertising, but it does state that information can be shared with third parties, like search engines, for “more personalized outputs.” Given Meta’s ad-based business model, this effectively translates to targeted advertising.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Character.AI’s privacy policy also highlights how the startup logs identifiers, demographics, location information, and more information about the user, including browsing behavior and app usage platforms. It tracks users across ads on TikTok, YouTube, Reddit, Facebook, Instagram, and Discord, which it may link to a user’s account. This information is used to train AI, tailor the service to personal preferences, and provide targeted advertising, including sharing data with advertisers and analytics providers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Character.AI spokesperson said the startup is “just beginning to explore targeted advertising on the platform” and that those explorations “have not involved using the content of chats on the platform.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The spokesperson also confirmed that the same privacy policy applies to all users, even teenagers. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has asked Meta such tracking is done on children, too, and will update this story if we hear back.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Meta and Character say their services aren’t designed for children under 13. That said, Meta has come under fire for failing to police accounts created by kids under 13, and Character’s kid-friendly characters are clearly designed to attract younger users. The startup’s CEO, Karandeep Anand, has even said that his six-year-old daughter uses the platform’s chatbots under his supervision.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That type of data collection, targeted advertising, and algorithmic exploitation is exactly what&amp;nbsp;legislation like KOSA (Kids Online Safety Act) is meant to protect against. KOSA was teed up to pass last year with strong bipartisan support, but it stalled after major pushback from tech industry lobbyists. Meta in particular deployed a formidable lobbying machine, warning lawmakers that the bill’s broad mandates would undercut its business model.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;KOSA was reintroduced to the Senate in May 2025 by Senators Marsha Blackburn (R-TN) and Richard Blumenthal (D-CT).&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Paxton has issued civil investigative demands — legal orders that require a company to produce documents, data, or testimony during a government probe —  to the companies to determine if they have violated Texas consumer protection laws.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This story was updated with comments from a Character.AI spokesperson.&lt;/em&gt;&lt;/p&gt;

&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!&amp;nbsp;&lt;/em&gt;&lt;em&gt;Fill out this survey to let us know how we’re doing&lt;/em&gt;&amp;nbsp;a&lt;em&gt;nd get the chance to win a prize in return!&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/08/18/texas-attorney-general-accuses-meta-character-ai-of-misleading-kids-with-mental-health-claims/</guid><pubDate>Mon, 18 Aug 2025 17:59:29 +0000</pubDate></item><item><title>[NEW] The looming crisis of AI speed without guardrails (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/the-looming-crisis-of-ai-speed-without-guardrails/</link><description>&lt;p&gt;OpenAI’s GPT-5 has arrived, bringing faster performance, more dependable reasoning and stronger tool use. It joins Claude Opus 4.1 and other frontier models in signaling a rapidly advancing cognitive frontier. While artificial general intelligence (AGI) remains in the future, DeepMind’s Demis Hassabis has described this era as “10 times bigger than the Industrial Revolution, and maybe 10 times faster.”&lt;/p&gt;&lt;p&gt;According to OpenAI CEO Sam Altman, GPT-5 is “a significant fraction of the way to something very AGI-like.” What is unfolding is not just a shift in tools, but a reordering of personal value, purpose, meaning and institutional trust. The challenge ahead is not only to innovate, but to build the moral, civic and institutional frameworks necessary to absorb this acceleration without collapse.&lt;/p&gt;&lt;p&gt;Anthropic CEO Dario Amodei provided an expansive view in his 2024 essay &lt;em&gt;Machines of Loving Grace.&lt;/em&gt; He imagined AI compressing a century of human progress into a decade, with commensurate advances in health, economic development, mental well-being and even democratic governance. However, “it will not be achieved without a huge amount of effort and struggle by many brave and dedicated people.” He added that everyone “will need to do their part both to prevent [AI] risks and to fully realize the benefits.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;That is the fragile fulcrum on which these promises rest. Our AI-fueled future is near, even as the destination of this cognitive migration, which is nothing less than a reorientation of human purpose in a world of thinking machines, remains uncertain. While my earlier articles mapped where people and institutions must migrate, this one asks how we match acceleration with capacity.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;What this moment in time asks of us is not just technical adoption but cultural and social reinvention. That is a hard ask, as our governance, educational systems and civic norms were forged in a slower, more linear era. They moved with the gravity of precedent, not the velocity of code.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-empowerment-without-inclusion"&gt;Empowerment without inclusion&lt;/h2&gt;



&lt;p&gt;In a New Yorker essay, Dartmouth professor Dan Rockmore describes how a neuroscientist colleague on a long drive conversed with ChatGPT and, together, they brainstormed a possible solution to a problem in his research. ChatGPT suggested he investigate a technique called “disentanglement” to simplify his mathematical model. The bot then wrote some code that was waiting at the end of the drive. The researcher ran it, and it worked. He said of this experience: “I feel like I’m accelerating with less time, I’m accelerating my learning, and improving my creativity, and I’m enjoying my work in a way I haven’t in a while.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is a compelling illustration of how powerful emerging AI technology can be in the hands of certain professionals. It is indeed an excellent thought partner and collaborator, ideal for a university professor or anyone tasked with developing innovative ideas. But what about the usefulness for and impact on others? Consider the logistics planners, procurement managers, and budget analysts whose roles risk displacement rather than enhancement. Without targeted retraining, robust social protections or institutional clarity, their futures could quickly move from uncertain to untenable.&lt;/p&gt;



&lt;p&gt;The result is a yawning gap between what our technologies enable and what our social institutions can support. That is where true fragility lies: Not in the AI tools themselves, but in the expectation that our existing systems can absorb the impact from them without fracture.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-change-without-infrastructure"&gt;Change without infrastructure&lt;/h2&gt;



&lt;p&gt;Many have argued that some amount of societal disruption always occurs alongside a technological revolution, such as when wagon wheel manufacturers were displaced by the rise of the automobile. But these narratives quickly shift to the wonders of what came next.&lt;/p&gt;



&lt;p&gt;The Industrial Revolution, now remembered for its long-term gains, began with decades of upheaval, exploitation and institutional lag. Public health systems, labor protections and universal education were not designed in advance. They emerged later, often painfully, as reactions to harms already done. Charles Dickens’ &lt;em&gt;Oliver Twist&lt;/em&gt;, with its orphaned child laborers and brutal workhouses, captured the social dislocation of that era with haunting clarity. The book was not a critique of technology itself, but of a society unprepared for its consequences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If the AI revolution is, as Hassabis suggests, an order of magnitude greater in scope and speed of implementation than that earlier transformation, then our margin for error is commensurately narrower and the timeline for societal response more compressed. In that context, hope is at best an invitation to dialogue and, at worst, a soft response to hard and fast-arriving problems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-vision-without-pathways"&gt;Vision without pathways&lt;/h2&gt;



&lt;p&gt;What are those responses? Despite the sweeping visions, there remains little consensus on how these ambitions will be integrated into the core functions of society. What does a “gentle singularity” look like in a hospital understaffed and underfunded? How do “machines of loving grace” support a public school system still struggling to provide basic literacy? How do these utopian aspirations square with predictions of 20% unemployment within five years? For all the talk of transformation, the mechanisms for wealth distribution, societal adaptation and business accountability remain vague at best.&lt;/p&gt;



&lt;p&gt;In many cases, AI is haphazardly arriving through unfettered market momentum. Language models are being embedded into government services, customer support, financial platforms and legal assistance tools, often without transparent review or meaningful public discourse and almost certainly without regulation. Even when these tools are helpful, their rollout bypasses the democratic and institutional channels that would otherwise confer trust. They arrive not through deliberation but as fait accompli, products of unregulated market momentum.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It is no wonder then, that the result is not a coordinated march toward abundance, but a patchwork of adoption defined more by technical possibility than social preparedness. In this environment, power accrues not to those with the most wisdom or care, but to those who move fastest and scale widest. And as history has shown, speed without accountability rarely yields equitable outcomes.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-leadership-without-safeguards"&gt;Leadership without safeguards&lt;/h2&gt;



&lt;p&gt;For enterprise and technology leaders, the acceleration is not abstract; it is an operational crisis. As large-scale AI systems begin permeating workflows, customer touchpoints and internal decision-making, executives face a shrinking window in which to act. This is not only about preparing for AGI; it is about managing the systemic impact of powerful, ambient tools that already exceed the control structures of most organizations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In a 2025 Thomson Reuters C-Suite survey, more than 80% of respondents said their organizations are already utilizing AI solutions, yet only 31% provided training for gen AI. That mismatch reveals a deeper readiness gap. Retraining cannot be a one-time initiative. It must become a core capability.&lt;/p&gt;



&lt;p&gt;In parallel, leaders must move beyond AI adoption to establishing internal governance, including model versioning, bias audits, human-in-the-loop safeguards and scenario planning. Without these, the risks are not only regulatory but reputational and strategic. Many leaders speak of AI as a force for human augmentation rather than replacement. In theory, systems that enhance human capacity should enable more resilient and adaptive institutions. In practice, however, the pressure to cut costs, increase throughput, and chase scale often pushes enterprises toward automation instead. This may become particularly acute during the next economic downturn. Whether augmentation becomes the dominant paradigm or merely a talking point will be one of the defining choices of this era.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-faith-without-foresight"&gt;Faith without foresight&lt;/h2&gt;



&lt;p&gt;In a &lt;em&gt;Guardian &lt;/em&gt;interview speaking about AI, Hassabis said: “…if we’re given the time, I believe in human ingenuity. I think we’ll get this right.” Perhaps “if we’re given the time” is the phrase doing the heavy lifting here. Estimates are that even more powerful AI will emerge over the next 5 to 10 years. This short timeframe is likely the moment when society must get it right. “Of course,” he added, “we’ve got to make sure [the benefits and prosperity from powerful AI] gets distributed fairly, but that’s more of a political question.” &lt;/p&gt;



&lt;p&gt;Indeed.&lt;/p&gt;



&lt;p&gt;To get it right would require a historically unprecedented feat: To match exponential technological disruption with equally agile moral judgment, political clarity and institutional redesign. It is likely that no society, not even with hindsight, has ever achieved such a feat. We survived the Industrial Revolution, painfully, unevenly, and only with time.&lt;/p&gt;



&lt;p&gt;However, as Hassabis and Amodei have made clear, we do not have much time. To adapt systems of law, education, labor and governance for a world of ambient, scalable intelligence would demand coordinated action across governments, corporations and civil society. It would require foresight in a culture trained to reward short-term gains, and humility in a sector built on winner-take-all dynamics. Optimism is not misplaced, it is conditional on decisions we have shown little collective capacity to make.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-delay-without-excuse"&gt;Delay without excuse&lt;/h2&gt;



&lt;p&gt;It is tempting to believe we can accurately forecast the arc of the AI era, but history suggests otherwise. On the one hand, it is entirely plausible that the AI revolution will substantially improve life as we know it, with advances such as clean fusion energy, cures for the worst of our diseases and solutions to the climate crisis. But it could also lead to large-scale unemployment or underemployment, social upheaval and even greater income inequality. Perhaps it will lead to all of this, or none of it. The truth is, we simply do not know.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On a “Plain English” podcast, host Derek Thompson spoke with Cal Newport, a professor of computer science at Georgetown University and the author of several books including “Deep Work.” Addressing what we should be instructing our children to be prepared for the age of AI, Newport said: “We’re still in an era of benchmarks. It’s like early in the Industrial Revolution; we haven’t replaced any of the looms yet. … We will have much clearer answers in two years.”&lt;/p&gt;



&lt;p&gt;In that ambiguity lies both peril and potential. If we are, as Newport suggests, only at the threshold, then now is the time to prepare. The future may not arrive all at once, but its contours are already forming. Whether AI becomes our greatest leap or deepest rupture depends not only on the models we build, but on the moral imagination and fortitude we bring to meet them.&lt;/p&gt;



&lt;p&gt;If socially harmful impacts from AI are expected within the next five to 10 years, we cannot wait for them to fully materialize before responding. Waiting could equate to negligence. Even so, human nature tends to delay big decisions until crises become undeniable. But by then, it is often too late to prevent the worst effects. Avoiding that with AI requires imminent investment in flexible regulatory frameworks, comprehensive retraining programs, equitable distribution of benefits and a robust social safety net.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If we want AI’s future to be one of abundance rather than disruption, we must design the structures now. The future will not wait. It will arrive with or without our guardrails. In a race to powerful AI, it is time to stop behaving as if we are still at the starting line.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;OpenAI’s GPT-5 has arrived, bringing faster performance, more dependable reasoning and stronger tool use. It joins Claude Opus 4.1 and other frontier models in signaling a rapidly advancing cognitive frontier. While artificial general intelligence (AGI) remains in the future, DeepMind’s Demis Hassabis has described this era as “10 times bigger than the Industrial Revolution, and maybe 10 times faster.”&lt;/p&gt;&lt;p&gt;According to OpenAI CEO Sam Altman, GPT-5 is “a significant fraction of the way to something very AGI-like.” What is unfolding is not just a shift in tools, but a reordering of personal value, purpose, meaning and institutional trust. The challenge ahead is not only to innovate, but to build the moral, civic and institutional frameworks necessary to absorb this acceleration without collapse.&lt;/p&gt;&lt;p&gt;Anthropic CEO Dario Amodei provided an expansive view in his 2024 essay &lt;em&gt;Machines of Loving Grace.&lt;/em&gt; He imagined AI compressing a century of human progress into a decade, with commensurate advances in health, economic development, mental well-being and even democratic governance. However, “it will not be achieved without a huge amount of effort and struggle by many brave and dedicated people.” He added that everyone “will need to do their part both to prevent [AI] risks and to fully realize the benefits.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;That is the fragile fulcrum on which these promises rest. Our AI-fueled future is near, even as the destination of this cognitive migration, which is nothing less than a reorientation of human purpose in a world of thinking machines, remains uncertain. While my earlier articles mapped where people and institutions must migrate, this one asks how we match acceleration with capacity.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;What this moment in time asks of us is not just technical adoption but cultural and social reinvention. That is a hard ask, as our governance, educational systems and civic norms were forged in a slower, more linear era. They moved with the gravity of precedent, not the velocity of code.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-empowerment-without-inclusion"&gt;Empowerment without inclusion&lt;/h2&gt;



&lt;p&gt;In a New Yorker essay, Dartmouth professor Dan Rockmore describes how a neuroscientist colleague on a long drive conversed with ChatGPT and, together, they brainstormed a possible solution to a problem in his research. ChatGPT suggested he investigate a technique called “disentanglement” to simplify his mathematical model. The bot then wrote some code that was waiting at the end of the drive. The researcher ran it, and it worked. He said of this experience: “I feel like I’m accelerating with less time, I’m accelerating my learning, and improving my creativity, and I’m enjoying my work in a way I haven’t in a while.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is a compelling illustration of how powerful emerging AI technology can be in the hands of certain professionals. It is indeed an excellent thought partner and collaborator, ideal for a university professor or anyone tasked with developing innovative ideas. But what about the usefulness for and impact on others? Consider the logistics planners, procurement managers, and budget analysts whose roles risk displacement rather than enhancement. Without targeted retraining, robust social protections or institutional clarity, their futures could quickly move from uncertain to untenable.&lt;/p&gt;



&lt;p&gt;The result is a yawning gap between what our technologies enable and what our social institutions can support. That is where true fragility lies: Not in the AI tools themselves, but in the expectation that our existing systems can absorb the impact from them without fracture.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-change-without-infrastructure"&gt;Change without infrastructure&lt;/h2&gt;



&lt;p&gt;Many have argued that some amount of societal disruption always occurs alongside a technological revolution, such as when wagon wheel manufacturers were displaced by the rise of the automobile. But these narratives quickly shift to the wonders of what came next.&lt;/p&gt;



&lt;p&gt;The Industrial Revolution, now remembered for its long-term gains, began with decades of upheaval, exploitation and institutional lag. Public health systems, labor protections and universal education were not designed in advance. They emerged later, often painfully, as reactions to harms already done. Charles Dickens’ &lt;em&gt;Oliver Twist&lt;/em&gt;, with its orphaned child laborers and brutal workhouses, captured the social dislocation of that era with haunting clarity. The book was not a critique of technology itself, but of a society unprepared for its consequences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If the AI revolution is, as Hassabis suggests, an order of magnitude greater in scope and speed of implementation than that earlier transformation, then our margin for error is commensurately narrower and the timeline for societal response more compressed. In that context, hope is at best an invitation to dialogue and, at worst, a soft response to hard and fast-arriving problems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-vision-without-pathways"&gt;Vision without pathways&lt;/h2&gt;



&lt;p&gt;What are those responses? Despite the sweeping visions, there remains little consensus on how these ambitions will be integrated into the core functions of society. What does a “gentle singularity” look like in a hospital understaffed and underfunded? How do “machines of loving grace” support a public school system still struggling to provide basic literacy? How do these utopian aspirations square with predictions of 20% unemployment within five years? For all the talk of transformation, the mechanisms for wealth distribution, societal adaptation and business accountability remain vague at best.&lt;/p&gt;



&lt;p&gt;In many cases, AI is haphazardly arriving through unfettered market momentum. Language models are being embedded into government services, customer support, financial platforms and legal assistance tools, often without transparent review or meaningful public discourse and almost certainly without regulation. Even when these tools are helpful, their rollout bypasses the democratic and institutional channels that would otherwise confer trust. They arrive not through deliberation but as fait accompli, products of unregulated market momentum.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It is no wonder then, that the result is not a coordinated march toward abundance, but a patchwork of adoption defined more by technical possibility than social preparedness. In this environment, power accrues not to those with the most wisdom or care, but to those who move fastest and scale widest. And as history has shown, speed without accountability rarely yields equitable outcomes.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-leadership-without-safeguards"&gt;Leadership without safeguards&lt;/h2&gt;



&lt;p&gt;For enterprise and technology leaders, the acceleration is not abstract; it is an operational crisis. As large-scale AI systems begin permeating workflows, customer touchpoints and internal decision-making, executives face a shrinking window in which to act. This is not only about preparing for AGI; it is about managing the systemic impact of powerful, ambient tools that already exceed the control structures of most organizations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In a 2025 Thomson Reuters C-Suite survey, more than 80% of respondents said their organizations are already utilizing AI solutions, yet only 31% provided training for gen AI. That mismatch reveals a deeper readiness gap. Retraining cannot be a one-time initiative. It must become a core capability.&lt;/p&gt;



&lt;p&gt;In parallel, leaders must move beyond AI adoption to establishing internal governance, including model versioning, bias audits, human-in-the-loop safeguards and scenario planning. Without these, the risks are not only regulatory but reputational and strategic. Many leaders speak of AI as a force for human augmentation rather than replacement. In theory, systems that enhance human capacity should enable more resilient and adaptive institutions. In practice, however, the pressure to cut costs, increase throughput, and chase scale often pushes enterprises toward automation instead. This may become particularly acute during the next economic downturn. Whether augmentation becomes the dominant paradigm or merely a talking point will be one of the defining choices of this era.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-faith-without-foresight"&gt;Faith without foresight&lt;/h2&gt;



&lt;p&gt;In a &lt;em&gt;Guardian &lt;/em&gt;interview speaking about AI, Hassabis said: “…if we’re given the time, I believe in human ingenuity. I think we’ll get this right.” Perhaps “if we’re given the time” is the phrase doing the heavy lifting here. Estimates are that even more powerful AI will emerge over the next 5 to 10 years. This short timeframe is likely the moment when society must get it right. “Of course,” he added, “we’ve got to make sure [the benefits and prosperity from powerful AI] gets distributed fairly, but that’s more of a political question.” &lt;/p&gt;



&lt;p&gt;Indeed.&lt;/p&gt;



&lt;p&gt;To get it right would require a historically unprecedented feat: To match exponential technological disruption with equally agile moral judgment, political clarity and institutional redesign. It is likely that no society, not even with hindsight, has ever achieved such a feat. We survived the Industrial Revolution, painfully, unevenly, and only with time.&lt;/p&gt;



&lt;p&gt;However, as Hassabis and Amodei have made clear, we do not have much time. To adapt systems of law, education, labor and governance for a world of ambient, scalable intelligence would demand coordinated action across governments, corporations and civil society. It would require foresight in a culture trained to reward short-term gains, and humility in a sector built on winner-take-all dynamics. Optimism is not misplaced, it is conditional on decisions we have shown little collective capacity to make.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-delay-without-excuse"&gt;Delay without excuse&lt;/h2&gt;



&lt;p&gt;It is tempting to believe we can accurately forecast the arc of the AI era, but history suggests otherwise. On the one hand, it is entirely plausible that the AI revolution will substantially improve life as we know it, with advances such as clean fusion energy, cures for the worst of our diseases and solutions to the climate crisis. But it could also lead to large-scale unemployment or underemployment, social upheaval and even greater income inequality. Perhaps it will lead to all of this, or none of it. The truth is, we simply do not know.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;On a “Plain English” podcast, host Derek Thompson spoke with Cal Newport, a professor of computer science at Georgetown University and the author of several books including “Deep Work.” Addressing what we should be instructing our children to be prepared for the age of AI, Newport said: “We’re still in an era of benchmarks. It’s like early in the Industrial Revolution; we haven’t replaced any of the looms yet. … We will have much clearer answers in two years.”&lt;/p&gt;



&lt;p&gt;In that ambiguity lies both peril and potential. If we are, as Newport suggests, only at the threshold, then now is the time to prepare. The future may not arrive all at once, but its contours are already forming. Whether AI becomes our greatest leap or deepest rupture depends not only on the models we build, but on the moral imagination and fortitude we bring to meet them.&lt;/p&gt;



&lt;p&gt;If socially harmful impacts from AI are expected within the next five to 10 years, we cannot wait for them to fully materialize before responding. Waiting could equate to negligence. Even so, human nature tends to delay big decisions until crises become undeniable. But by then, it is often too late to prevent the worst effects. Avoiding that with AI requires imminent investment in flexible regulatory frameworks, comprehensive retraining programs, equitable distribution of benefits and a robust social safety net.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;If we want AI’s future to be one of abundance rather than disruption, we must design the structures now. The future will not wait. It will arrive with or without our guardrails. In a race to powerful AI, it is time to stop behaving as if we are still at the starting line.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-looming-crisis-of-ai-speed-without-guardrails/</guid><pubDate>Mon, 18 Aug 2025 18:24:27 +0000</pubDate></item><item><title>[NEW] Researchers glimpse the inner workings of protein language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/researchers-glimpse-inner-workings-protein-language-models-0818</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/MIT-model-interpret-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Within the past few years, models that can predict the structure or function of proteins have been widely used for a variety of biological applications, such as identifying drug targets and designing new therapeutic antibodies.&lt;/p&gt;&lt;p&gt;These models, which are based on large language models (LLMs), can make very accurate predictions of a protein’s suitability for a given application. However, there’s no way to determine how these models make their predictions or which protein features play the most important role in those decisions.&lt;/p&gt;&lt;p&gt;In a new study, MIT researchers have used a novel technique to open up that “black box” and allow them to determine what features a protein language model takes into account when making predictions. Understanding what is happening inside that black box&amp;nbsp;could help researchers to choose better models for a particular task, helping to streamline the process of identifying new drugs or vaccine targets.&lt;/p&gt;&lt;p&gt;“Our work has broad implications for enhanced explainability in downstream tasks that rely on these representations,” says Bonnie Berger, the Simons Professor of Mathematics, head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory, and the senior author of the study. “Additionally, identifying features that protein language models track has the potential to reveal novel biological insights from these representations.”&lt;/p&gt;&lt;p&gt;Onkar Gujral, an MIT graduate student, is the lead author of the study, which appears this week in the &lt;em&gt;Proceedings of the National Academy of Sciences.&lt;/em&gt; Mihir Bafna, an MIT graduate student, and Eric Alm, an MIT professor of biological engineering, are also authors of the paper.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opening the black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2018, Berger and former MIT graduate student Tristan Bepler PhD ’20 introduced the first protein language model. Their model, like subsequent protein models that accelerated the development of&amp;nbsp;AlphaFold, such as&amp;nbsp;ESM2 and OmegaFold, was based on LLMs. These models, which include ChatGPT, can analyze huge amounts of text and figure out which words are most likely to appear together.&lt;/p&gt;&lt;p&gt;Protein language models use a similar approach, but instead of analyzing words, they analyze amino acid sequences. Researchers have used these models to predict the structure and function of proteins, and for applications such as identifying proteins that might bind to particular drugs.&lt;/p&gt;&lt;p&gt;In a&amp;nbsp;2021 study, Berger and colleagues used a protein language model to predict which sections of viral surface proteins are less likely to mutate in a way that enables viral escape. This allowed them to identify possible targets for vaccines against influenza, HIV, and SARS-CoV-2.&lt;/p&gt;&lt;p&gt;However, in all of these studies, it has been impossible to know how the models were making their predictions.&lt;/p&gt;&lt;p&gt;“We would get out some prediction at the end, but we had absolutely no idea what was happening in the individual components of this black box,” Berger says.&lt;/p&gt;&lt;p&gt;In the new study, the researchers wanted to dig into how protein language models make their predictions. Just like LLMs, protein language models encode information as representations that consist of a pattern of activation of different “nodes” within a neural network. These nodes are analogous to the networks of neurons that store memories and other information within the brain.&lt;/p&gt;&lt;p&gt;The inner workings of LLMs are not easy to interpret, but within the past couple of years, researchers have begun using a type of algorithm known as a sparse autoencoder to help shed some light on how those models make their predictions. The new study from Berger’s lab is the first to use this algorithm on protein language models.&lt;/p&gt;&lt;p&gt;Sparse autoencoders work by adjusting how a protein is represented within a neural network. Typically, a given protein will be represented by a pattern of activation of a constrained number of neurons, for example, 480. A sparse autoencoder will expand that representation into a much larger number of nodes, say 20,000.&lt;/p&gt;&lt;p&gt;When information about a protein is encoded by only 480 neurons, each node lights up for multiple features, making it very difficult to know what features each node is encoding. However, when the neural network is expanded to 20,000 nodes, this extra space along with a sparsity constraint gives the information room to “spread out.” Now, a feature of the protein that was previously encoded by multiple nodes can occupy a single node.&lt;/p&gt;&lt;p&gt;“In a sparse representation, the neurons lighting up are doing so in a more meaningful manner,” Gujral says. “Before the sparse representations are created, the networks pack information so tightly together that it's hard to interpret the neurons.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interpretable models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Once the researchers obtained sparse representations of many proteins, they used an AI assistant called Claude (related to the popular Anthropic chatbot of the same name), to analyze the representations. In this case, they asked Claude to compare the sparse representations with the known features of each protein, such as molecular function, protein family, or location within a cell.&lt;/p&gt;&lt;p&gt;By analyzing thousands of representations, Claude can determine which nodes correspond to specific protein features, then describe them in plain English. For example, the algorithm might say, “This neuron appears to be detecting proteins involved in transmembrane transport of ions or amino acids, particularly those located in the plasma membrane.”&lt;/p&gt;&lt;p&gt;This process makes the nodes far more “interpretable,” meaning the researchers can tell what each node is encoding. They found that the features most likely to be encoded by these nodes were protein family and certain functions, including several different metabolic and biosynthetic processes.&lt;/p&gt;&lt;p&gt;“When you train a sparse autoencoder, you aren’t training it to be interpretable, but it turns out that by incentivizing the representation to be really sparse, that ends up resulting in interpretability,” Gujral says.&lt;/p&gt;&lt;p&gt;Understanding what features a particular protein model is encoding could help researchers choose the right model for a particular task, or tweak the type of input they give the model, to generate the best results. Additionally, analyzing the features that a model encodes could one day help biologists to learn more about the proteins that they are studying.&lt;/p&gt;&lt;p&gt;“At some point when the models get a lot more powerful, you could learn more biology than you already know, from opening up the models,” Gujral says.&lt;/p&gt;&lt;p&gt;The research was funded by the National Institutes of Health.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202508/MIT-model-interpret-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Within the past few years, models that can predict the structure or function of proteins have been widely used for a variety of biological applications, such as identifying drug targets and designing new therapeutic antibodies.&lt;/p&gt;&lt;p&gt;These models, which are based on large language models (LLMs), can make very accurate predictions of a protein’s suitability for a given application. However, there’s no way to determine how these models make their predictions or which protein features play the most important role in those decisions.&lt;/p&gt;&lt;p&gt;In a new study, MIT researchers have used a novel technique to open up that “black box” and allow them to determine what features a protein language model takes into account when making predictions. Understanding what is happening inside that black box&amp;nbsp;could help researchers to choose better models for a particular task, helping to streamline the process of identifying new drugs or vaccine targets.&lt;/p&gt;&lt;p&gt;“Our work has broad implications for enhanced explainability in downstream tasks that rely on these representations,” says Bonnie Berger, the Simons Professor of Mathematics, head of the Computation and Biology group in MIT’s Computer Science and Artificial Intelligence Laboratory, and the senior author of the study. “Additionally, identifying features that protein language models track has the potential to reveal novel biological insights from these representations.”&lt;/p&gt;&lt;p&gt;Onkar Gujral, an MIT graduate student, is the lead author of the study, which appears this week in the &lt;em&gt;Proceedings of the National Academy of Sciences.&lt;/em&gt; Mihir Bafna, an MIT graduate student, and Eric Alm, an MIT professor of biological engineering, are also authors of the paper.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Opening the black box&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;In 2018, Berger and former MIT graduate student Tristan Bepler PhD ’20 introduced the first protein language model. Their model, like subsequent protein models that accelerated the development of&amp;nbsp;AlphaFold, such as&amp;nbsp;ESM2 and OmegaFold, was based on LLMs. These models, which include ChatGPT, can analyze huge amounts of text and figure out which words are most likely to appear together.&lt;/p&gt;&lt;p&gt;Protein language models use a similar approach, but instead of analyzing words, they analyze amino acid sequences. Researchers have used these models to predict the structure and function of proteins, and for applications such as identifying proteins that might bind to particular drugs.&lt;/p&gt;&lt;p&gt;In a&amp;nbsp;2021 study, Berger and colleagues used a protein language model to predict which sections of viral surface proteins are less likely to mutate in a way that enables viral escape. This allowed them to identify possible targets for vaccines against influenza, HIV, and SARS-CoV-2.&lt;/p&gt;&lt;p&gt;However, in all of these studies, it has been impossible to know how the models were making their predictions.&lt;/p&gt;&lt;p&gt;“We would get out some prediction at the end, but we had absolutely no idea what was happening in the individual components of this black box,” Berger says.&lt;/p&gt;&lt;p&gt;In the new study, the researchers wanted to dig into how protein language models make their predictions. Just like LLMs, protein language models encode information as representations that consist of a pattern of activation of different “nodes” within a neural network. These nodes are analogous to the networks of neurons that store memories and other information within the brain.&lt;/p&gt;&lt;p&gt;The inner workings of LLMs are not easy to interpret, but within the past couple of years, researchers have begun using a type of algorithm known as a sparse autoencoder to help shed some light on how those models make their predictions. The new study from Berger’s lab is the first to use this algorithm on protein language models.&lt;/p&gt;&lt;p&gt;Sparse autoencoders work by adjusting how a protein is represented within a neural network. Typically, a given protein will be represented by a pattern of activation of a constrained number of neurons, for example, 480. A sparse autoencoder will expand that representation into a much larger number of nodes, say 20,000.&lt;/p&gt;&lt;p&gt;When information about a protein is encoded by only 480 neurons, each node lights up for multiple features, making it very difficult to know what features each node is encoding. However, when the neural network is expanded to 20,000 nodes, this extra space along with a sparsity constraint gives the information room to “spread out.” Now, a feature of the protein that was previously encoded by multiple nodes can occupy a single node.&lt;/p&gt;&lt;p&gt;“In a sparse representation, the neurons lighting up are doing so in a more meaningful manner,” Gujral says. “Before the sparse representations are created, the networks pack information so tightly together that it's hard to interpret the neurons.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interpretable models&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Once the researchers obtained sparse representations of many proteins, they used an AI assistant called Claude (related to the popular Anthropic chatbot of the same name), to analyze the representations. In this case, they asked Claude to compare the sparse representations with the known features of each protein, such as molecular function, protein family, or location within a cell.&lt;/p&gt;&lt;p&gt;By analyzing thousands of representations, Claude can determine which nodes correspond to specific protein features, then describe them in plain English. For example, the algorithm might say, “This neuron appears to be detecting proteins involved in transmembrane transport of ions or amino acids, particularly those located in the plasma membrane.”&lt;/p&gt;&lt;p&gt;This process makes the nodes far more “interpretable,” meaning the researchers can tell what each node is encoding. They found that the features most likely to be encoded by these nodes were protein family and certain functions, including several different metabolic and biosynthetic processes.&lt;/p&gt;&lt;p&gt;“When you train a sparse autoencoder, you aren’t training it to be interpretable, but it turns out that by incentivizing the representation to be really sparse, that ends up resulting in interpretability,” Gujral says.&lt;/p&gt;&lt;p&gt;Understanding what features a particular protein model is encoding could help researchers choose the right model for a particular task, or tweak the type of input they give the model, to generate the best results. Additionally, analyzing the features that a model encodes could one day help biologists to learn more about the proteins that they are studying.&lt;/p&gt;&lt;p&gt;“At some point when the models get a lot more powerful, you could learn more biology than you already know, from opening up the models,” Gujral says.&lt;/p&gt;&lt;p&gt;The research was funded by the National Institutes of Health.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/researchers-glimpse-inner-workings-protein-language-models-0818</guid><pubDate>Mon, 18 Aug 2025 19:00:00 +0000</pubDate></item><item><title>[NEW] TensorZero nabs $7.3M seed to solve the messy world of enterprise LLM development (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/tensorzero-nabs-7-3m-seed-to-solve-the-messy-world-of-enterprise-llm-development/</link><description>&lt;p&gt;TensorZero, a startup building open-source infrastructure for large language model applications, announced Monday it has raised $7.3 million in seed funding led by FirstMark, with participation from Bessemer Venture Partners, Bedrock, DRW, Coalition, and dozens of strategic angel investors.&lt;/p&gt;&lt;p&gt;The funding comes as the 18-month-old company experiences explosive growth in the developer community. TensorZero’s open-source repository recently achieved the “#1 trending repository of the week” spot globally on GitHub, jumping from roughly 3,000 to over 9,700 stars in recent months as enterprises grapple with the complexity of building production-ready AI applications.&lt;/p&gt;&lt;p&gt;“Despite all the noise in the industry, companies building LLM applications still lack the right tools to meet complex cognitive and infrastructure needs, and resort to stitching together whatever early solutions are available on the market,” said Matt Turck, General Partner at FirstMark, who led the investment. “TensorZero provides production-grade, enterprise-ready components for building LLM applications that natively work together in a self-reinforcing loop, out of the box.”&lt;/p&gt;&lt;p&gt;The Brooklyn-based company addresses a growing pain point for enterprises deploying AI applications at scale. While large language models like GPT-5 and Claude have demonstrated remarkable capabilities, translating these into reliable business applications requires orchestrating multiple complex systems for model access, monitoring, optimization, and experimentation.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-nuclear-fusion-research-shaped-a-breakthrough-ai-optimization-platform"&gt;How nuclear fusion research shaped a breakthrough AI optimization platform&lt;/h2&gt;



&lt;p&gt;TensorZero’s approach stems from co-founder and CTO Viraj Mehta’s unconventional background in reinforcement learning for nuclear fusion reactors. During his PhD at Carnegie Mellon, Mehta worked on Department of Energy research projects where data collection cost “like a car per data point — $30,000 for 5 seconds of data,” he explained in a recent interview with VentureBeat.&lt;/p&gt;



&lt;p&gt;“That problem leads to a huge amount of concern about where to focus our limited resources,” Mehta said. “We were going to only get to run a handful of trials total, so the question became: what is the marginally most valuable place we can collect data from?” This experience shaped TensorZero’s core philosophy: maximizing the value of every data point to continuously improve AI systems.&lt;/p&gt;



&lt;p&gt;The insight led Mehta and co-founder Gabriel Bianconi, former chief product officer at Ondo Finance (a decentralized finance project with over $1 billion in assets under management), to reconceptualize LLM applications as reinforcement learning problems where systems learn from real-world feedback.&lt;/p&gt;



&lt;p&gt;“LLM applications in their broader context feel like reinforcement learning problems,” Mehta explained. “You make many calls to a machine learning model with structured inputs, get structured outputs, and eventually receive some form of reward or feedback. This looks to me like a partially observable Markov decision process.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-enterprises-are-ditching-complex-vendor-integrations-for-unified-ai-infrastructure"&gt;Why enterprises are ditching complex vendor integrations for unified AI infrastructure&lt;/h2&gt;



&lt;p&gt;Traditional approaches to building LLM applications require companies to integrate numerous specialized tools from different vendors — model gateways, observability platforms, evaluation frameworks, and fine-tuning services. TensorZero unifies these capabilities into a single open-source stack designed to work together seamlessly.&lt;/p&gt;



&lt;p&gt;“Most companies didn’t go through the hassle of integrating all these different tools, and even the ones that did ended up with fragmented solutions, because those tools weren’t designed to work well with each other,” Bianconi said. “So we realized there was an opportunity to build a product that enables this feedback loop in production.”&lt;/p&gt;



&lt;p&gt;The platform’s core innovation is creating what the founders call a “data and learning flywheel” — a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models. Built in Rust for performance, TensorZero achieves sub-millisecond latency overhead while supporting all major LLM providers through a unified API.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-banks-and-ai-startups-are-already-building-production-systems-on-tensorzero"&gt;Major banks and AI startups are already building production systems on TensorZero&lt;/h2&gt;



&lt;p&gt;The approach has already attracted significant enterprise adoption. One of Europe’s largest banks is using TensorZero to automate code changelog generation, while numerous AI-first startups from Series A to Series B stage have integrated the platform across diverse industries including healthcare, finance, and consumer applications.&lt;/p&gt;



&lt;p&gt;“The surge in adoption from both the open-source community and enterprises has been incredible,” Bianconi said. “We’re fortunate to have received contributions from dozens of developers worldwide, and it’s exciting to see TensorZero already powering cutting-edge LLM applications at frontier AI startups and large organizations.”&lt;/p&gt;



&lt;p&gt;The company’s customer base spans organizations from startups to major financial institutions, drawn by both the technical capabilities and the open-source nature of the platform. For enterprises with strict compliance requirements, the ability to run TensorZero within their own infrastructure provides crucial control over sensitive data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-tensorzero-outperforms-langchain-and-other-ai-frameworks-at-enterprise-scale"&gt;How TensorZero outperforms LangChain and other AI frameworks at enterprise scale&lt;/h2&gt;



&lt;p&gt;TensorZero differentiates itself from existing solutions like LangChain and LiteLLM through its end-to-end approach and focus on production-grade deployments. While many frameworks excel at rapid prototyping, they often hit scalability ceilings that force companies to rebuild their infrastructure.&lt;/p&gt;



&lt;p&gt;“There are two dimensions to think about,” Bianconi explained. “First, there are a number of projects out there that are very good to get started quickly, and you can put a prototype out there very quickly. But often companies will hit a ceiling with many of those products and need to churn and go for something else.”&lt;/p&gt;



&lt;p&gt;The platform’s structured approach to data collection also enables more sophisticated optimization techniques. Unlike traditional observability tools that store raw text inputs and outputs, TensorZero maintains structured data about the variables that go into each inference, making it easier to retrain models and experiment with different approaches.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-rust-powered-performance-delivers-sub-millisecond-latency-at-10-000-queries-per-second"&gt;Rust-powered performance delivers sub-millisecond latency at 10,000+ queries per second&lt;/h2&gt;



&lt;p&gt;Performance has been a key design consideration. In benchmarks, TensorZero’s Rust-based gateway adds less than 1 millisecond of latency at 99th percentile while handling over 10,000 queries per second. This compares favorably to Python-based alternatives like LiteLLM, which can add 25-100x more latency at much lower throughput levels.&lt;/p&gt;



&lt;p&gt;“LiteLLM (Python) at 100 QPS adds 25-100x+ more P99 latency than our gateway at 10,000 QPS,” the founders noted in their announcement, highlighting the performance advantages of their Rust implementation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-open-source-strategy-designed-to-eliminate-ai-vendor-lock-in-fears"&gt;The open-source strategy designed to eliminate AI vendor lock-in fears&lt;/h2&gt;



&lt;p&gt;TensorZero has committed to keeping its core platform entirely open source, with no paid features — a strategy designed to build trust with enterprise customers wary of vendor lock-in. The company plans to monetize through a managed service that automates the more complex aspects of LLM optimization, such as GPU management for custom model training and proactive optimization recommendations.&lt;/p&gt;



&lt;p&gt;“We realized very early on that we needed to make this open source, to give [enterprises] the confidence to do this,” Bianconi said. “In the future, at least a year from now realistically, we’ll come back with a complementary managed service.”&lt;/p&gt;



&lt;p&gt;The managed service will focus on automating the computationally intensive aspects of LLM optimization while maintaining the open-source core. This includes handling GPU infrastructure for fine-tuning, running automated experiments, and providing proactive suggestions for improving model performance.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-the-company-reshaping-enterprise-ai-infrastructure"&gt;What’s next for the company reshaping enterprise AI infrastructure&lt;/h2&gt;



&lt;p&gt;The announcement positions TensorZero at the forefront of a growing movement to solve the “LLMOps” challenge — the operational complexity of running AI applications in production. As enterprises increasingly view AI as critical business infrastructure rather than experimental technology, the demand for production-ready tooling continues to accelerate.&lt;/p&gt;



&lt;p&gt;With the new funding, TensorZero plans to accelerate development of its open-source infrastructure while building out its team. The company is currently hiring in New York and welcomes open-source contributions from the developer community. The founders are particularly excited about developing research tools that will enable faster experimentation across different AI applications.&lt;/p&gt;



&lt;p&gt;“Our ultimate vision is to enable a data and learning flywheel for optimizing LLM applications—a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models and agents,” Mehta said. “As AI models grow smarter and take on more complex workflows, you can’t reason about them in a vacuum; you have to do so in the context of their real-world consequences.”&lt;/p&gt;



&lt;p&gt;TensorZero’s rapid GitHub growth and early enterprise traction suggest strong product-market fit in addressing one of the most pressing challenges in modern AI development. The company’s open-source approach and focus on enterprise-grade performance could prove decisive advantages in a market where developer adoption often precedes enterprise sales.&lt;/p&gt;



&lt;p&gt;For enterprises still struggling to move AI applications from prototype to production, TensorZero’s unified approach offers a compelling alternative to the current patchwork of specialized tools. As one industry observer noted, the difference between building AI demos and building AI businesses often comes down to infrastructure — and TensorZero is betting that unified, performance-oriented infrastructure will be the foundation upon which the next generation of AI companies is built.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;TensorZero, a startup building open-source infrastructure for large language model applications, announced Monday it has raised $7.3 million in seed funding led by FirstMark, with participation from Bessemer Venture Partners, Bedrock, DRW, Coalition, and dozens of strategic angel investors.&lt;/p&gt;&lt;p&gt;The funding comes as the 18-month-old company experiences explosive growth in the developer community. TensorZero’s open-source repository recently achieved the “#1 trending repository of the week” spot globally on GitHub, jumping from roughly 3,000 to over 9,700 stars in recent months as enterprises grapple with the complexity of building production-ready AI applications.&lt;/p&gt;&lt;p&gt;“Despite all the noise in the industry, companies building LLM applications still lack the right tools to meet complex cognitive and infrastructure needs, and resort to stitching together whatever early solutions are available on the market,” said Matt Turck, General Partner at FirstMark, who led the investment. “TensorZero provides production-grade, enterprise-ready components for building LLM applications that natively work together in a self-reinforcing loop, out of the box.”&lt;/p&gt;&lt;p&gt;The Brooklyn-based company addresses a growing pain point for enterprises deploying AI applications at scale. While large language models like GPT-5 and Claude have demonstrated remarkable capabilities, translating these into reliable business applications requires orchestrating multiple complex systems for model access, monitoring, optimization, and experimentation.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;h2 class="wp-block-heading" id="h-how-nuclear-fusion-research-shaped-a-breakthrough-ai-optimization-platform"&gt;How nuclear fusion research shaped a breakthrough AI optimization platform&lt;/h2&gt;



&lt;p&gt;TensorZero’s approach stems from co-founder and CTO Viraj Mehta’s unconventional background in reinforcement learning for nuclear fusion reactors. During his PhD at Carnegie Mellon, Mehta worked on Department of Energy research projects where data collection cost “like a car per data point — $30,000 for 5 seconds of data,” he explained in a recent interview with VentureBeat.&lt;/p&gt;



&lt;p&gt;“That problem leads to a huge amount of concern about where to focus our limited resources,” Mehta said. “We were going to only get to run a handful of trials total, so the question became: what is the marginally most valuable place we can collect data from?” This experience shaped TensorZero’s core philosophy: maximizing the value of every data point to continuously improve AI systems.&lt;/p&gt;



&lt;p&gt;The insight led Mehta and co-founder Gabriel Bianconi, former chief product officer at Ondo Finance (a decentralized finance project with over $1 billion in assets under management), to reconceptualize LLM applications as reinforcement learning problems where systems learn from real-world feedback.&lt;/p&gt;



&lt;p&gt;“LLM applications in their broader context feel like reinforcement learning problems,” Mehta explained. “You make many calls to a machine learning model with structured inputs, get structured outputs, and eventually receive some form of reward or feedback. This looks to me like a partially observable Markov decision process.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-enterprises-are-ditching-complex-vendor-integrations-for-unified-ai-infrastructure"&gt;Why enterprises are ditching complex vendor integrations for unified AI infrastructure&lt;/h2&gt;



&lt;p&gt;Traditional approaches to building LLM applications require companies to integrate numerous specialized tools from different vendors — model gateways, observability platforms, evaluation frameworks, and fine-tuning services. TensorZero unifies these capabilities into a single open-source stack designed to work together seamlessly.&lt;/p&gt;



&lt;p&gt;“Most companies didn’t go through the hassle of integrating all these different tools, and even the ones that did ended up with fragmented solutions, because those tools weren’t designed to work well with each other,” Bianconi said. “So we realized there was an opportunity to build a product that enables this feedback loop in production.”&lt;/p&gt;



&lt;p&gt;The platform’s core innovation is creating what the founders call a “data and learning flywheel” — a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models. Built in Rust for performance, TensorZero achieves sub-millisecond latency overhead while supporting all major LLM providers through a unified API.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-banks-and-ai-startups-are-already-building-production-systems-on-tensorzero"&gt;Major banks and AI startups are already building production systems on TensorZero&lt;/h2&gt;



&lt;p&gt;The approach has already attracted significant enterprise adoption. One of Europe’s largest banks is using TensorZero to automate code changelog generation, while numerous AI-first startups from Series A to Series B stage have integrated the platform across diverse industries including healthcare, finance, and consumer applications.&lt;/p&gt;



&lt;p&gt;“The surge in adoption from both the open-source community and enterprises has been incredible,” Bianconi said. “We’re fortunate to have received contributions from dozens of developers worldwide, and it’s exciting to see TensorZero already powering cutting-edge LLM applications at frontier AI startups and large organizations.”&lt;/p&gt;



&lt;p&gt;The company’s customer base spans organizations from startups to major financial institutions, drawn by both the technical capabilities and the open-source nature of the platform. For enterprises with strict compliance requirements, the ability to run TensorZero within their own infrastructure provides crucial control over sensitive data.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-tensorzero-outperforms-langchain-and-other-ai-frameworks-at-enterprise-scale"&gt;How TensorZero outperforms LangChain and other AI frameworks at enterprise scale&lt;/h2&gt;



&lt;p&gt;TensorZero differentiates itself from existing solutions like LangChain and LiteLLM through its end-to-end approach and focus on production-grade deployments. While many frameworks excel at rapid prototyping, they often hit scalability ceilings that force companies to rebuild their infrastructure.&lt;/p&gt;



&lt;p&gt;“There are two dimensions to think about,” Bianconi explained. “First, there are a number of projects out there that are very good to get started quickly, and you can put a prototype out there very quickly. But often companies will hit a ceiling with many of those products and need to churn and go for something else.”&lt;/p&gt;



&lt;p&gt;The platform’s structured approach to data collection also enables more sophisticated optimization techniques. Unlike traditional observability tools that store raw text inputs and outputs, TensorZero maintains structured data about the variables that go into each inference, making it easier to retrain models and experiment with different approaches.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-rust-powered-performance-delivers-sub-millisecond-latency-at-10-000-queries-per-second"&gt;Rust-powered performance delivers sub-millisecond latency at 10,000+ queries per second&lt;/h2&gt;



&lt;p&gt;Performance has been a key design consideration. In benchmarks, TensorZero’s Rust-based gateway adds less than 1 millisecond of latency at 99th percentile while handling over 10,000 queries per second. This compares favorably to Python-based alternatives like LiteLLM, which can add 25-100x more latency at much lower throughput levels.&lt;/p&gt;



&lt;p&gt;“LiteLLM (Python) at 100 QPS adds 25-100x+ more P99 latency than our gateway at 10,000 QPS,” the founders noted in their announcement, highlighting the performance advantages of their Rust implementation.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-open-source-strategy-designed-to-eliminate-ai-vendor-lock-in-fears"&gt;The open-source strategy designed to eliminate AI vendor lock-in fears&lt;/h2&gt;



&lt;p&gt;TensorZero has committed to keeping its core platform entirely open source, with no paid features — a strategy designed to build trust with enterprise customers wary of vendor lock-in. The company plans to monetize through a managed service that automates the more complex aspects of LLM optimization, such as GPU management for custom model training and proactive optimization recommendations.&lt;/p&gt;



&lt;p&gt;“We realized very early on that we needed to make this open source, to give [enterprises] the confidence to do this,” Bianconi said. “In the future, at least a year from now realistically, we’ll come back with a complementary managed service.”&lt;/p&gt;



&lt;p&gt;The managed service will focus on automating the computationally intensive aspects of LLM optimization while maintaining the open-source core. This includes handling GPU infrastructure for fine-tuning, running automated experiments, and providing proactive suggestions for improving model performance.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-next-for-the-company-reshaping-enterprise-ai-infrastructure"&gt;What’s next for the company reshaping enterprise AI infrastructure&lt;/h2&gt;



&lt;p&gt;The announcement positions TensorZero at the forefront of a growing movement to solve the “LLMOps” challenge — the operational complexity of running AI applications in production. As enterprises increasingly view AI as critical business infrastructure rather than experimental technology, the demand for production-ready tooling continues to accelerate.&lt;/p&gt;



&lt;p&gt;With the new funding, TensorZero plans to accelerate development of its open-source infrastructure while building out its team. The company is currently hiring in New York and welcomes open-source contributions from the developer community. The founders are particularly excited about developing research tools that will enable faster experimentation across different AI applications.&lt;/p&gt;



&lt;p&gt;“Our ultimate vision is to enable a data and learning flywheel for optimizing LLM applications—a feedback loop that turns production metrics and human feedback into smarter, faster, and cheaper models and agents,” Mehta said. “As AI models grow smarter and take on more complex workflows, you can’t reason about them in a vacuum; you have to do so in the context of their real-world consequences.”&lt;/p&gt;



&lt;p&gt;TensorZero’s rapid GitHub growth and early enterprise traction suggest strong product-market fit in addressing one of the most pressing challenges in modern AI development. The company’s open-source approach and focus on enterprise-grade performance could prove decisive advantages in a market where developer adoption often precedes enterprise sales.&lt;/p&gt;



&lt;p&gt;For enterprises still struggling to move AI applications from prototype to production, TensorZero’s unified approach offers a compelling alternative to the current patchwork of specialized tools. As one industry observer noted, the difference between building AI demos and building AI businesses often comes down to infrastructure — and TensorZero is betting that unified, performance-oriented infrastructure will be the foundation upon which the next generation of AI companies is built.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/tensorzero-nabs-7-3m-seed-to-solve-the-messy-world-of-enterprise-llm-development/</guid><pubDate>Mon, 18 Aug 2025 19:13:32 +0000</pubDate></item><item><title>[NEW] At Gamescom 2025, NVIDIA DLSS 4 and Ray Tracing Come to This Year’s Biggest Titles (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/gamescom-2025-dlss-4-ray-tracing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/gamescom-2025-dlss.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;With over 175 games now supporting NVIDIA DLSS 4 — a suite of advanced, AI-powered neural rendering technologies — gamers and tech enthusiasts everywhere can experience breakthrough performance in this year’s most anticipated titles, including &lt;i&gt;Borderlands 4&lt;/i&gt;, &lt;i&gt;Hell Is Us&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Plus, path tracing is making its way to &lt;i&gt;Resident Evil Requiem&lt;/i&gt; and &lt;i&gt;Directive 8020&lt;/i&gt;, as well as ray tracing in upcoming releases like &lt;i&gt;Phantom Blade Zero&lt;/i&gt;, &lt;i&gt;PRAGMATA&lt;/i&gt; and &lt;i&gt;CINDER CITY&lt;/i&gt; — enabling crystal-clear visuals for more immersive gameplay&lt;/p&gt;
&lt;p&gt;“DLSS 4 and path tracing are no longer cutting-edge graphical experiments — they’re the foundation of modern PC gaming titles,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “Developers are embracing AI-powered rendering to unlock stunning visuals and massive performance gains, enabling gamers everywhere to experience the future of real-time graphics today.”&lt;/p&gt;
&lt;p&gt;These announcements come alongside a new NVIDIA GeForce RTX 50 Series bundle for &lt;i&gt;Borderlands 4&lt;/i&gt; and updates to the NVIDIA app — a companion platform for content creators, gamers and AI enthusiasts using NVIDIA GeForce RTX GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DLSS 4 Now Accelerating Over 175 Games and Applications&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Launched with the GeForce RTX 50 Series earlier this year, DLSS 4 with Multi Frame Generation uses AI to generate up to three frames for every traditionally rendered frame, delivering performance boosts of up to 8x over traditional rendering.&lt;/p&gt;
&lt;p&gt;In addition to Multi Frame Generation, DLSS 4 titles include support for DLSS Super Resolution, Ray Reconstruction and NVIDIA Reflex technology — unlocking incredible performance gains and responsive gameplay for every GeForce RTX 50 Series owner.&lt;/p&gt;
&lt;p&gt;New titles announced at Gamescom that will support the latest RTX technologies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Directive 8020&lt;/i&gt; and &lt;i&gt;Resident Evil Requiem&lt;/i&gt;, which are launching with DLSS 4 and path tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Black State&lt;/i&gt;, &lt;i&gt;CINDER CITY&lt;/i&gt; (formerly &lt;i&gt;Project LLL&lt;/i&gt;), &lt;i&gt;Cronos: The New Dawn&lt;/i&gt;, &lt;i&gt;Dying Light: The Beast&lt;/i&gt;, &lt;i&gt;Honeycomb: The World Beyond&lt;/i&gt;, &lt;i&gt;Lost Soul Aside&lt;/i&gt;, &lt;i&gt;The Outer Worlds 2&lt;/i&gt;, &lt;i&gt;Phantom Blade Zero&lt;/i&gt; and &lt;i&gt;PRAGMATA&lt;/i&gt;, which are launching with DLSS 4 and ray tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;, which are launching with DLSS 4 with Multi Frame Generation&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Indiana Jones and the Great Circle, &lt;/i&gt;which in September will add support for RTX Hair, a technology that uses new hardware capabilities in RTX 50 Series GPUs to model hair with greater path-traced detail and realism&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these RTX titles will also launch on the GeForce NOW cloud gaming platform, including&lt;i&gt; Borderlands 4, CINDER CITY &lt;/i&gt;(formerly&lt;i&gt; Project LLL&lt;/i&gt;)&lt;i&gt;, Hell Is Us &lt;/i&gt;and &lt;i&gt;The Outer Worlds 2.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA App Adds Global DLSS Overrides and Software Updates&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA app is the essential companion for NVIDIA GeForce RTX GPU users, simplifying the process of keeping PCs updated with the latest GeForce Game Ready and NVIDIA Studio Drivers.&lt;/p&gt;
&lt;p&gt;New updates to the NVIDIA app include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Global DLSS Overrides:&lt;/b&gt; Easily enable DLSS Multi-Frame Generation or DLSS Super Resolution profiles globally across hundreds of DLSS Override titles, instead of needing to configure per title.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Project G-Assist Upgrades: &lt;/b&gt;The latest update to Project G-Assist — an on-device AI assistant that lets users control and tune their RTX systems with voice and text commands — introduces a significantly more efficient AI model that uses 40% less memory. Despite its smaller footprint, it responds to queries faster and more accurately calls the right tools.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Highly Requested Legacy 3D Settings:&lt;/b&gt; Use easily configurable control panel settings — including anisotropic filtering, anti-aliasing and ambient occlusion — to enhance classic games.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NVIDIA app beta update launches Tuesday, Aug. 19, at 9 a.m. PT, with full availability coming the following week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA ACE Enhances Voice-Driven Gaming Experiences&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA ACE — a suite of generative AI technologies that power lifelike non-playable character interactions in games like Krafton’s &lt;i&gt;inZOI&lt;/i&gt; — now features in Iconic Interactive’s &lt;i&gt;The Oversight Bureau&lt;/i&gt;, a darkly comic, voice-driven puzzle game.&lt;/p&gt;
&lt;p&gt;Using speech-to-text technology powered by ACE, players can interact naturally with in-game characters using speech, with Iconic’s Narrative Engine interpreting the input and determining and delivering the pre-recorded character dialogue that best fits the story and situation.&lt;/p&gt;
&lt;p&gt;This system keeps developers in creative control while offering players real agency in games — all running locally on RTX AI PCs with sub-second latency.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;The Oversight Bureau&lt;/i&gt; launches later this year and will be playable at NVIDIA’s Gamescom B2B press suite.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA RTX Remix Evolves With Community Expansions and New Particle System&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA RTX Remix, an open-source modding platform for remastering classic games with path tracing and neural rendering, continues to grow thanks to its passionate community.&lt;/p&gt;
&lt;p&gt;Modders have been using large language models to extend RTX Remix’s capabilities. For example, one modder “vibe coded” a plug-in that connects RTX Remix to Adobe Substance 3D, the industry-standard tool for 3D texturing and materials. Another modder made it possible for RTX Remix to use classic game data to instantly make objects glow with emissive effects.&lt;/p&gt;
&lt;p&gt;RTX Remix’s open-source community has even expanded compatibility to allow many new titles to be remastered, including iconic games like &lt;i&gt;Call Of Duty 4: Modern Warfare&lt;/i&gt;, &lt;i&gt;Knights Of The Old Republic&lt;/i&gt;, &lt;i&gt;Doom 3&lt;/i&gt;, &lt;i&gt;Half-Life: Black Mesa&lt;/i&gt; and &lt;i&gt;Bioshock.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Some of these games were featured in the RTX Remix’s $50K Mod Contest, which wrapped up at Gamescom. &lt;i&gt;Painkiller RTX&lt;/i&gt; by Merry Pencil Studios won numerous awards, including “Best Overall RTX Remix Mod.” Explore all mod submissions on ModDB.com.&lt;/p&gt;
&lt;p&gt;At Gamescom, NVIDIA also unveiled a new RTX Remix particle system that brings dynamic, realistically lit and physically accurate particles to 165 classic games — the majority of which have never had a particle editor.&lt;/p&gt;
&lt;p&gt;Modders can use the system to change the look, size, quantity, light emission, turbulence and even gravity of particles in games. The new particle system will be available in September.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Borderlands 4’ GeForce RTX 50 Series Bundle Available Now&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To celebrate Gearbox’s &lt;i&gt;Borderlands 4&lt;/i&gt;, which will be enhanced by DLSS 4 with Multi Frame Generation and NVIDIA Reflex, NVIDIA is introducing a new GeForce RTX 50 Series bundle.&lt;/p&gt;
&lt;p&gt;Players who purchase a GeForce RTX 5090, 5080, 5070 Ti, or 5070 desktop system or graphics card — or laptops with a GeForce RTX™ 5090 Laptop GPU, RTX 5080 Laptop GPU, RTX 5070 Ti Laptop GPU or RTX 5070 Laptop GPU from participating retailers — will receive a copy of &lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;The Gilded Glory Pack&lt;/i&gt; DLC. The offer is available through Monday, Sept. 22.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about GeForce announcements at &lt;/i&gt;&lt;i&gt;Gamescom&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/gamescom-2025-dlss.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;With over 175 games now supporting NVIDIA DLSS 4 — a suite of advanced, AI-powered neural rendering technologies — gamers and tech enthusiasts everywhere can experience breakthrough performance in this year’s most anticipated titles, including &lt;i&gt;Borderlands 4&lt;/i&gt;, &lt;i&gt;Hell Is Us&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;Plus, path tracing is making its way to &lt;i&gt;Resident Evil Requiem&lt;/i&gt; and &lt;i&gt;Directive 8020&lt;/i&gt;, as well as ray tracing in upcoming releases like &lt;i&gt;Phantom Blade Zero&lt;/i&gt;, &lt;i&gt;PRAGMATA&lt;/i&gt; and &lt;i&gt;CINDER CITY&lt;/i&gt; — enabling crystal-clear visuals for more immersive gameplay&lt;/p&gt;
&lt;p&gt;“DLSS 4 and path tracing are no longer cutting-edge graphical experiments — they’re the foundation of modern PC gaming titles,” said Matt Wuebbling, vice president of global GeForce marketing at NVIDIA. “Developers are embracing AI-powered rendering to unlock stunning visuals and massive performance gains, enabling gamers everywhere to experience the future of real-time graphics today.”&lt;/p&gt;
&lt;p&gt;These announcements come alongside a new NVIDIA GeForce RTX 50 Series bundle for &lt;i&gt;Borderlands 4&lt;/i&gt; and updates to the NVIDIA app — a companion platform for content creators, gamers and AI enthusiasts using NVIDIA GeForce RTX GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DLSS 4 Now Accelerating Over 175 Games and Applications&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Launched with the GeForce RTX 50 Series earlier this year, DLSS 4 with Multi Frame Generation uses AI to generate up to three frames for every traditionally rendered frame, delivering performance boosts of up to 8x over traditional rendering.&lt;/p&gt;
&lt;p&gt;In addition to Multi Frame Generation, DLSS 4 titles include support for DLSS Super Resolution, Ray Reconstruction and NVIDIA Reflex technology — unlocking incredible performance gains and responsive gameplay for every GeForce RTX 50 Series owner.&lt;/p&gt;
&lt;p&gt;New titles announced at Gamescom that will support the latest RTX technologies include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;Directive 8020&lt;/i&gt; and &lt;i&gt;Resident Evil Requiem&lt;/i&gt;, which are launching with DLSS 4 and path tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Black State&lt;/i&gt;, &lt;i&gt;CINDER CITY&lt;/i&gt; (formerly &lt;i&gt;Project LLL&lt;/i&gt;), &lt;i&gt;Cronos: The New Dawn&lt;/i&gt;, &lt;i&gt;Dying Light: The Beast&lt;/i&gt;, &lt;i&gt;Honeycomb: The World Beyond&lt;/i&gt;, &lt;i&gt;Lost Soul Aside&lt;/i&gt;, &lt;i&gt;The Outer Worlds 2&lt;/i&gt;, &lt;i&gt;Phantom Blade Zero&lt;/i&gt; and &lt;i&gt;PRAGMATA&lt;/i&gt;, which are launching with DLSS 4 and ray tracing&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;Fate Trigger&lt;/i&gt;, which are launching with DLSS 4 with Multi Frame Generation&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Indiana Jones and the Great Circle, &lt;/i&gt;which in September will add support for RTX Hair, a technology that uses new hardware capabilities in RTX 50 Series GPUs to model hair with greater path-traced detail and realism&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these RTX titles will also launch on the GeForce NOW cloud gaming platform, including&lt;i&gt; Borderlands 4, CINDER CITY &lt;/i&gt;(formerly&lt;i&gt; Project LLL&lt;/i&gt;)&lt;i&gt;, Hell Is Us &lt;/i&gt;and &lt;i&gt;The Outer Worlds 2.&lt;/i&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA App Adds Global DLSS Overrides and Software Updates&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA app is the essential companion for NVIDIA GeForce RTX GPU users, simplifying the process of keeping PCs updated with the latest GeForce Game Ready and NVIDIA Studio Drivers.&lt;/p&gt;
&lt;p&gt;New updates to the NVIDIA app include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Global DLSS Overrides:&lt;/b&gt; Easily enable DLSS Multi-Frame Generation or DLSS Super Resolution profiles globally across hundreds of DLSS Override titles, instead of needing to configure per title.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Project G-Assist Upgrades: &lt;/b&gt;The latest update to Project G-Assist — an on-device AI assistant that lets users control and tune their RTX systems with voice and text commands — introduces a significantly more efficient AI model that uses 40% less memory. Despite its smaller footprint, it responds to queries faster and more accurately calls the right tools.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Highly Requested Legacy 3D Settings:&lt;/b&gt; Use easily configurable control panel settings — including anisotropic filtering, anti-aliasing and ambient occlusion — to enhance classic games.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NVIDIA app beta update launches Tuesday, Aug. 19, at 9 a.m. PT, with full availability coming the following week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA ACE Enhances Voice-Driven Gaming Experiences&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA ACE — a suite of generative AI technologies that power lifelike non-playable character interactions in games like Krafton’s &lt;i&gt;inZOI&lt;/i&gt; — now features in Iconic Interactive’s &lt;i&gt;The Oversight Bureau&lt;/i&gt;, a darkly comic, voice-driven puzzle game.&lt;/p&gt;
&lt;p&gt;Using speech-to-text technology powered by ACE, players can interact naturally with in-game characters using speech, with Iconic’s Narrative Engine interpreting the input and determining and delivering the pre-recorded character dialogue that best fits the story and situation.&lt;/p&gt;
&lt;p&gt;This system keeps developers in creative control while offering players real agency in games — all running locally on RTX AI PCs with sub-second latency.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;The Oversight Bureau&lt;/i&gt; launches later this year and will be playable at NVIDIA’s Gamescom B2B press suite.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA RTX Remix Evolves With Community Expansions and New Particle System&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA RTX Remix, an open-source modding platform for remastering classic games with path tracing and neural rendering, continues to grow thanks to its passionate community.&lt;/p&gt;
&lt;p&gt;Modders have been using large language models to extend RTX Remix’s capabilities. For example, one modder “vibe coded” a plug-in that connects RTX Remix to Adobe Substance 3D, the industry-standard tool for 3D texturing and materials. Another modder made it possible for RTX Remix to use classic game data to instantly make objects glow with emissive effects.&lt;/p&gt;
&lt;p&gt;RTX Remix’s open-source community has even expanded compatibility to allow many new titles to be remastered, including iconic games like &lt;i&gt;Call Of Duty 4: Modern Warfare&lt;/i&gt;, &lt;i&gt;Knights Of The Old Republic&lt;/i&gt;, &lt;i&gt;Doom 3&lt;/i&gt;, &lt;i&gt;Half-Life: Black Mesa&lt;/i&gt; and &lt;i&gt;Bioshock.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;Some of these games were featured in the RTX Remix’s $50K Mod Contest, which wrapped up at Gamescom. &lt;i&gt;Painkiller RTX&lt;/i&gt; by Merry Pencil Studios won numerous awards, including “Best Overall RTX Remix Mod.” Explore all mod submissions on ModDB.com.&lt;/p&gt;
&lt;p&gt;At Gamescom, NVIDIA also unveiled a new RTX Remix particle system that brings dynamic, realistically lit and physically accurate particles to 165 classic games — the majority of which have never had a particle editor.&lt;/p&gt;
&lt;p&gt;Modders can use the system to change the look, size, quantity, light emission, turbulence and even gravity of particles in games. The new particle system will be available in September.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;‘Borderlands 4’ GeForce RTX 50 Series Bundle Available Now&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To celebrate Gearbox’s &lt;i&gt;Borderlands 4&lt;/i&gt;, which will be enhanced by DLSS 4 with Multi Frame Generation and NVIDIA Reflex, NVIDIA is introducing a new GeForce RTX 50 Series bundle.&lt;/p&gt;
&lt;p&gt;Players who purchase a GeForce RTX 5090, 5080, 5070 Ti, or 5070 desktop system or graphics card — or laptops with a GeForce RTX™ 5090 Laptop GPU, RTX 5080 Laptop GPU, RTX 5070 Ti Laptop GPU or RTX 5070 Laptop GPU from participating retailers — will receive a copy of &lt;i&gt;Borderlands 4&lt;/i&gt; and &lt;i&gt;The Gilded Glory Pack&lt;/i&gt; DLC. The offer is available through Monday, Sept. 22.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Learn more about GeForce announcements at &lt;/i&gt;&lt;i&gt;Gamescom&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/gamescom-2025-dlss-4-ray-tracing/</guid><pubDate>Mon, 18 Aug 2025 19:30:01 +0000</pubDate></item><item><title>[NEW] New Lightweight AI Model for Project G-Assist Brings Support for 6GB NVIDIA GeForce RTX and RTX PRO GPUs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-gamescom-g-assist-rtx-remix/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At Gamescom, NVIDIA is releasing its first major update to Project G‑Assist — an experimental on-device AI assistant that allows users to tune their NVIDIA RTX systems with voice and text commands.&lt;/p&gt;
&lt;p&gt;The update brings a new AI model that uses 40% less VRAM, improves tool-calling intelligence and extends G-Assist support to all RTX GPUs with 6GB or more VRAM, including laptops. Plus, a new G-Assist Plug-In Hub enables users to easily discover and download plug-ins to enable more G-Assist features.&lt;/p&gt;
&lt;p&gt;NVIDIA also announced a new path-traced particle system, coming in September to the NVIDIA RTX Remix modding platform, that brings fully simulated physics, dynamic shadows and realistic reflections to visual effects.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA named the winners of the NVIDIA and ModDB RTX Remix Mod Contest. Check out the winners and finalist RTX mods in the RTX Remix GeForce article.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;G-Assist Gets Smarter, Expands to More RTX PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The modern PC is a powerhouse, but unlocking its full potential means navigating a complex maze of settings across system software, GPU and peripheral utilities, control panels and more.&lt;/p&gt;
&lt;p&gt;Project G-Assist is a free, on-device AI assistant built to cut through that complexity. It acts as a central command center, providing easy access to functions previously buried in menus through voice or text commands. Users can ask the assistant to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run diagnostics to optimize game performance&lt;/li&gt;
&lt;li&gt;Display or chart frame rates, latency and GPU temperatures&lt;/li&gt;
&lt;li&gt;Adjust GPU or even peripheral settings, such as keyboard lighting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The G-Assist update also introduces a new, significantly more efficient AI model that’s faster and uses 40% less memory while maintaining response accuracy. The more efficient model means that G-Assist can now run on all RTX GPUs with 6GB or more VRAM, including laptops.&lt;/p&gt;
&lt;p&gt;Getting started is simple: install the NVIDIA app and the latest Game Ready Driver on Aug. 19, download the G-Assist update from the app’s home screen and press Alt+G to activate.&lt;/p&gt;
&lt;p&gt;Another G-Assist update coming in September will introduce support for laptop-specific commands for features like NVIDIA BatteryBoost and Battery OPS.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Introducing the G-Assist Plug-In Hub With Mod.io&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA is collaborating with mod.io to launch the G-Assist Plug-In Hub, which allows users to easily access G-Assist plug-ins, as well as discover and download community-created ones.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83855"&gt;&lt;img alt="alt" class="size-full wp-image-83855" height="611" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/mod.io-plug-in.jpg" width="777" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83855"&gt;With the mod.io plug-in, users can ask G-Assist to discover and install new plug-ins.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the latest update, users can also directly ask G-Assist what new plug-ins are available in the hub and install them using natural language, thanks to a mod.io plug-in.&lt;/p&gt;
&lt;p&gt;The recent G-Assist Plug-In Hackathon showcased the incredible creativity of the G-Assist community. Here’s a sneak peek of what they came up with:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Some finalists include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Omniplay&lt;/b&gt; — allows gamers to use G-Assist to research lore from online wikis or take notes in real time while gaming&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Launchpad&lt;/b&gt; — lets gamers set, launch and toggle custom app groups on the fly to boost productivity&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Flux NIM Microservice for G-Assist&lt;/b&gt; — allows gamers to easily generate AI images from within G-Assist, using on-device NVIDIA NIM microservices&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The winners of the hackathon will be announced on Wednesday, Aug. 20.&lt;/p&gt;
&lt;p&gt;Building custom plug-ins is simple. They’re based on a foundation of JSON and Python scripts — and the Project G-Assist Plug-In Builder helps further simplify development by enabling users to code plug-ins with natural language.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Mod It Like It’s Hot With RTX Remix&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Classic PC games remain beloved for their unforgettable stories, characters and gameplay — but their dated graphics can be a barrier for new and longtime players.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX Remix enables modders to revitalize these timeless titles with the latest NVIDIA gaming technologies — bridging nostalgic gameplay with modern visuals.&lt;/p&gt;
&lt;p&gt;Since the platform’s release, the RTX Remix modding community has grown with over 350 active projects and over 100 mods released. The mods span a catalog of beloved games like &lt;i&gt;Half-Life 2&lt;/i&gt;, &lt;i&gt;Need for Speed: Underground&lt;/i&gt;, &lt;i&gt;Portal 2 &lt;/i&gt;and &lt;i&gt;Deus Ex — &lt;/i&gt;and have amassed over 2 million downloads.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In May, NVIDIA invited modders to participate in the NVIDIA and ModDB RTX Remix Mod Contest for a chance to win $50,000 in cash prizes. At Gamescom, NVIDIA announced the winners:&lt;/p&gt;

&lt;p&gt;These modders tapped RTX Remix and generative AI to bring their creations to life — from enhancing textures to quickly creating images and 3D assets.&lt;/p&gt;
&lt;p&gt;For example, the Merry Pencil Studios modder team used a workflow that seamlessly connected RTX Remix and ComfyUI, allowing them to simply select textures in the RTX Remix viewport and, with a single click in ComfyUI, restore them.&lt;/p&gt;
&lt;p&gt;The results are stunning, with each texture meticulously recreated with physically based materials layered with grime and rust. With a fully path-traced lighting system, the game’s gothic horror atmosphere has never felt more immersive to play through.&lt;/p&gt;
&lt;p&gt;All mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, are available to download from ModDB. For a sneak peek at RTX Remix projects under active development, check out the RTX Remix Showcase Discord server.&lt;/p&gt;
&lt;p&gt;Another RTX Remix update coming in September will allow modders to create new particles that match the look of those found in modern titles. This opens the door for over 165 RTX Remix-compatible games to have particles for the first time.&lt;/p&gt;
&lt;p&gt;To get started creating RTX mods, download NVIDIA RTX Remix from the home screen of the NVIDIA app. Read the RTX Remix article to learn more about the contest and winners.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;At Gamescom, NVIDIA is releasing its first major update to Project G‑Assist — an experimental on-device AI assistant that allows users to tune their NVIDIA RTX systems with voice and text commands.&lt;/p&gt;
&lt;p&gt;The update brings a new AI model that uses 40% less VRAM, improves tool-calling intelligence and extends G-Assist support to all RTX GPUs with 6GB or more VRAM, including laptops. Plus, a new G-Assist Plug-In Hub enables users to easily discover and download plug-ins to enable more G-Assist features.&lt;/p&gt;
&lt;p&gt;NVIDIA also announced a new path-traced particle system, coming in September to the NVIDIA RTX Remix modding platform, that brings fully simulated physics, dynamic shadows and realistic reflections to visual effects.&lt;/p&gt;
&lt;p&gt;In addition, NVIDIA named the winners of the NVIDIA and ModDB RTX Remix Mod Contest. Check out the winners and finalist RTX mods in the RTX Remix GeForce article.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;G-Assist Gets Smarter, Expands to More RTX PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The modern PC is a powerhouse, but unlocking its full potential means navigating a complex maze of settings across system software, GPU and peripheral utilities, control panels and more.&lt;/p&gt;
&lt;p&gt;Project G-Assist is a free, on-device AI assistant built to cut through that complexity. It acts as a central command center, providing easy access to functions previously buried in menus through voice or text commands. Users can ask the assistant to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run diagnostics to optimize game performance&lt;/li&gt;
&lt;li&gt;Display or chart frame rates, latency and GPU temperatures&lt;/li&gt;
&lt;li&gt;Adjust GPU or even peripheral settings, such as keyboard lighting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;The G-Assist update also introduces a new, significantly more efficient AI model that’s faster and uses 40% less memory while maintaining response accuracy. The more efficient model means that G-Assist can now run on all RTX GPUs with 6GB or more VRAM, including laptops.&lt;/p&gt;
&lt;p&gt;Getting started is simple: install the NVIDIA app and the latest Game Ready Driver on Aug. 19, download the G-Assist update from the app’s home screen and press Alt+G to activate.&lt;/p&gt;
&lt;p&gt;Another G-Assist update coming in September will introduce support for laptop-specific commands for features like NVIDIA BatteryBoost and Battery OPS.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Introducing the G-Assist Plug-In Hub With Mod.io&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA is collaborating with mod.io to launch the G-Assist Plug-In Hub, which allows users to easily access G-Assist plug-ins, as well as discover and download community-created ones.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_83855"&gt;&lt;img alt="alt" class="size-full wp-image-83855" height="611" src="https://blogs.nvidia.com/wp-content/uploads/2025/08/mod.io-plug-in.jpg" width="777" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-83855"&gt;With the mod.io plug-in, users can ask G-Assist to discover and install new plug-ins.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;With the latest update, users can also directly ask G-Assist what new plug-ins are available in the hub and install them using natural language, thanks to a mod.io plug-in.&lt;/p&gt;
&lt;p&gt;The recent G-Assist Plug-In Hackathon showcased the incredible creativity of the G-Assist community. Here’s a sneak peek of what they came up with:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Some finalists include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Omniplay&lt;/b&gt; — allows gamers to use G-Assist to research lore from online wikis or take notes in real time while gaming&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Launchpad&lt;/b&gt; — lets gamers set, launch and toggle custom app groups on the fly to boost productivity&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Flux NIM Microservice for G-Assist&lt;/b&gt; — allows gamers to easily generate AI images from within G-Assist, using on-device NVIDIA NIM microservices&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The winners of the hackathon will be announced on Wednesday, Aug. 20.&lt;/p&gt;
&lt;p&gt;Building custom plug-ins is simple. They’re based on a foundation of JSON and Python scripts — and the Project G-Assist Plug-In Builder helps further simplify development by enabling users to code plug-ins with natural language.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Mod It Like It’s Hot With RTX Remix&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Classic PC games remain beloved for their unforgettable stories, characters and gameplay — but their dated graphics can be a barrier for new and longtime players.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX Remix enables modders to revitalize these timeless titles with the latest NVIDIA gaming technologies — bridging nostalgic gameplay with modern visuals.&lt;/p&gt;
&lt;p&gt;Since the platform’s release, the RTX Remix modding community has grown with over 350 active projects and over 100 mods released. The mods span a catalog of beloved games like &lt;i&gt;Half-Life 2&lt;/i&gt;, &lt;i&gt;Need for Speed: Underground&lt;/i&gt;, &lt;i&gt;Portal 2 &lt;/i&gt;and &lt;i&gt;Deus Ex — &lt;/i&gt;and have amassed over 2 million downloads.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;In May, NVIDIA invited modders to participate in the NVIDIA and ModDB RTX Remix Mod Contest for a chance to win $50,000 in cash prizes. At Gamescom, NVIDIA announced the winners:&lt;/p&gt;

&lt;p&gt;These modders tapped RTX Remix and generative AI to bring their creations to life — from enhancing textures to quickly creating images and 3D assets.&lt;/p&gt;
&lt;p&gt;For example, the Merry Pencil Studios modder team used a workflow that seamlessly connected RTX Remix and ComfyUI, allowing them to simply select textures in the RTX Remix viewport and, with a single click in ComfyUI, restore them.&lt;/p&gt;
&lt;p&gt;The results are stunning, with each texture meticulously recreated with physically based materials layered with grime and rust. With a fully path-traced lighting system, the game’s gothic horror atmosphere has never felt more immersive to play through.&lt;/p&gt;
&lt;p&gt;All mods submitted to the RTX Remix Modding Contest, as well as 100 more Remix mods, are available to download from ModDB. For a sneak peek at RTX Remix projects under active development, check out the RTX Remix Showcase Discord server.&lt;/p&gt;
&lt;p&gt;Another RTX Remix update coming in September will allow modders to create new particles that match the look of those found in modern titles. This opens the door for over 165 RTX Remix-compatible games to have particles for the first time.&lt;/p&gt;
&lt;p&gt;To get started creating RTX mods, download NVIDIA RTX Remix from the home screen of the NVIDIA app. Read the RTX Remix article to learn more about the contest and winners.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Each week, the &lt;/i&gt;&lt;i&gt;RTX AI Garage&lt;/i&gt; &lt;i&gt;blog series features community-driven AI innovations and content for those looking to learn more about NVIDIA NIM microservices and AI Blueprints, as well as building &lt;/i&gt;&lt;i&gt;AI agents&lt;/i&gt;&lt;i&gt;, creative workflows, productivity apps and more on AI PCs and workstations.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Join NVIDIA’s &lt;/i&gt;&lt;i&gt;Discord server&lt;/i&gt;&lt;i&gt; to connect with community developers and AI enthusiasts for discussions on what’s possible with RTX AI.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-gamescom-g-assist-rtx-remix/</guid><pubDate>Mon, 18 Aug 2025 19:30:40 +0000</pubDate></item><item><title>[NEW] GEPA optimizes LLMs without costly reinforcement learning (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/gepa-optimizes-llms-without-costly-reinforcement-learning/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers from the University of California, Berkeley, Stanford University and Databricks have introduced a new AI optimization method called GEPA that significantly outperforms traditional reinforcement learning (RL) techniques for adapting large language models (LLMs) to specialized tasks.&lt;/p&gt;&lt;p&gt;GEPA removes the popular paradigm of learning through thousands of trial-and-error attempts guided by simple numerical scores. Instead, it uses an LLM’s own language understanding to reflect on its performance, diagnose errors, and iteratively evolve its instructions. In addition to being more accurate than established techniques, GEPA is significantly more efficient, achieving superior results with up to 35 times fewer trial runs.&lt;/p&gt;&lt;p&gt;For businesses building complex AI agents and workflows, this translates directly into faster development cycles, substantially lower computational costs, and more performant, reliable applications.&lt;/p&gt;&lt;p&gt;Modern enterprise AI applications are rarely a single call to an LLM. They are often “compound AI systems,” complex workflows that chain multiple LLM modules, external tools such as databases or code interpreters, and custom logic to perform sophisticated tasks, including multi-step research and data analysis.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A popular way to optimize these systems is through reinforcement learning methods&lt;span&gt;, such as&amp;nbsp;Group Relative Policy Optimization&amp;nbsp;(GRPO), a technique employed in popular reasoning models, including&lt;/span&gt; DeepSeek-R1. This method treats the system as a black box; it runs a task, gets a simple success metric (a “scalar reward,” like a score of 7/10), and uses this feedback to slowly nudge the model’s parameters in the right direction.&lt;/p&gt;



&lt;p&gt;The major drawback of RL is its sample inefficiency. To learn effectively from these sparse numerical scores, RL methods often require tens of thousands, or even hundreds of thousands, of trial runs, known as “rollouts.” For any real-world enterprise application that involves expensive tool calls (e.g., API queries, code compilation) or uses powerful proprietary models, this process is prohibitively slow and costly.&lt;/p&gt;



&lt;p&gt;As Lakshya A Agrawal, co-author of the paper and doctoral student at UC Berkeley, told VentureBeat, this complexity is a major barrier for many companies. “For many teams, RL is not practical due to its cost and complexity—and their go-to approach so far would often just be prompt engineering by hand,” Agrawal said. He noted that GEPA is designed for teams that need to optimize systems built on top-tier models that often can’t be fine-tuned, allowing them to improve performance without managing custom GPU clusters.&lt;/p&gt;



&lt;p&gt;The researchers frame this challenge as follows: “How can we extract maximal learning signal from every expensive rollout to enable effective adaptation of complex, modular AI systems in low-data or budget-constrained settings?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-an-optimizer-that-learns-with-language"&gt;An optimizer that learns with language&lt;/h2&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015654" height="464" src="https://venturebeat.com/wp-content/uploads/2025/08/image_b1ddfc.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA framework Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GEPA (Genetic-Pareto) is a prompt optimizer that tackles this challenge by replacing sparse rewards with rich, natural language feedback. It leverages the fact that the entire execution of an AI system (including its reasoning steps, tool calls, and even error messages) can be serialized into text that an LLM can read and understand. GEPA’s methodology is built on three core pillars.&lt;/p&gt;



&lt;p&gt;First is “genetic prompt evolution,” where GEPA treats a population of prompts like a gene pool. It iteratively “mutates” prompts to create new, potentially better versions. This mutation is an intelligent process driven by the second pillar: “reflection with natural language feedback.” After a few rollouts, GEPA provides an LLM with the full execution trace (what the system tried to do) and the outcome (what went right or wrong). The LLM then “reflects” on this feedback in natural language to diagnose the problem and write an improved, more detailed prompt. For instance, instead of just seeing a low score on a code generation task, it might analyze a compiler error and conclude the prompt needs to specify a particular library version.&lt;/p&gt;



&lt;p&gt;The third pillar is “Pareto-based selection,” which ensures smart exploration. Instead of focusing only on the single best-performing prompt, which can lead to getting stuck in a suboptimal solution (a “local optimum”), GEPA maintains a diverse roster of “specialist” prompts. It tracks which prompts perform best on different individual examples, creating a list of top candidates. By sampling from this diverse set of winning strategies, GEPA ensures it explores more solutions and is more likely to discover a prompt that generalizes well across a wide range of inputs.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015655" height="412" src="https://venturebeat.com/wp-content/uploads/2025/08/image_25fed4.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Selecting a single best candidate (left) can result in models getting stuck in local minima while Pareto selection (right) can explore more options and find optimal solutions Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The effectiveness of this entire process hinges on what the researchers call “feedback engineering.” Agrawal explains that the key is to surface the rich, textual details that systems already produce but often discard. “Traditional pipelines often reduce this detail to a single numerical reward, obscuring why particular outcomes occur,” he said. “GEPA’s core guidance is to structure feedback that surfaces not only outcomes but also intermediate trajectories and errors in plain text—the same evidence a human would use to diagnose system behavior.”&lt;/p&gt;



&lt;p&gt;For example, for a document retrieval system, this means listing which documents were retrieved correctly and which were missed, rather than just calculating a final score.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gepa-in-action"&gt;GEPA in action&lt;/h2&gt;



&lt;p&gt;The researchers evaluated GEPA across four diverse tasks, including multi-hop question answering (HotpotQA) and privacy-preserving queries (PUPA). They used both open-source (Qwen3 8B) and proprietary (GPT-4.1 mini) models, comparing GEPA against the RL-based GRPO and the state-of-the-art prompt optimizer MIPROv2.&lt;/p&gt;



&lt;p&gt;Across all tasks, GEPA substantially outperformed GRPO, achieving up to a 19% higher score while using up to 35 times fewer rollouts. Agrawal provided a concrete example of this efficiency gain: “We used GEPA to optimize a QA system in ~3 hours versus GRPO’s 24 hours—an 8x reduction in development time, while also achieving 20% higher performance,” he explained. “RL-based optimization of the same scenario in our test cost about $300 in GPU time, while GEPA cost less than $20 for better results—15x savings in our experiments.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015656" height="289" src="https://venturebeat.com/wp-content/uploads/2025/08/image_1b53c1.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA outperforms other baselines on key benchmarks Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Beyond raw performance, the researchers found that GEPA-optimized systems are more reliable when faced with new, unseen data. This is measured by the “generalization gap” (the difference between performance on training data and final test data). Agrawal hypothesizes that this is because GEPA learns from richer feedback. “GEPA’s smaller generalization gap may stem from its use of rich natural-language feedback on each outcome—what worked, what failed, and why—rather than relying solely on a single scalar reward,” he said. “This may encourage the system to develop instructions and strategies grounded in a broader understanding of success, instead of merely learning patterns specific to the training data.” For enterprises, this improved reliability means less brittle, more adaptable AI applications in customer-facing roles.&lt;/p&gt;



&lt;p&gt;A major practical benefit is that GEPA’s instruction-based prompts are up to 9.2 times shorter than prompts produced by optimizers like MIPROv2, which include many few-shot examples. Shorter prompts decrease latency and reduce costs for API-based models. This makes the final application faster and cheaper to run in production.&lt;/p&gt;



&lt;p&gt;The paper also presents promising results for utilizing GEPA as an “inference-time” search strategy, transforming the AI from a single-answer generator into an iterative problem solver. Agrawal described a scenario where GEPA could be integrated into a company’s CI/CD pipeline. When new code is committed, GEPA could automatically generate and refine multiple optimized versions, test them for performance, and open a pull request with the best-performing variant for engineers to review. “This turns optimization into a continuous, automated process—rapidly generating solutions that often match or surpass expert hand-tuning,” Agrawal noted. In their experiments on CUDA code generation, this approach boosted performance on 20% of tasks to an expert level, compared to 0% for a single-shot attempt from GPT-4o.&lt;/p&gt;



&lt;p&gt;The paper’s authors believe GEPA is a foundational step toward a new paradigm of AI development. But beyond creating more human-like AI, its most immediate impact may be in who gets to build high-performing systems.&lt;/p&gt;



&lt;p&gt;“We expect GEPA to enable a positive shift in AI system building—making the optimization of such systems approachable by end-users, who often have the domain expertise relevant to the task, but not necessarily the time and willingness to learn complex RL specifics,” Agrawal said. “It gives power directly to the stakeholders with the exact task-specific domain knowledge.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers from the University of California, Berkeley, Stanford University and Databricks have introduced a new AI optimization method called GEPA that significantly outperforms traditional reinforcement learning (RL) techniques for adapting large language models (LLMs) to specialized tasks.&lt;/p&gt;&lt;p&gt;GEPA removes the popular paradigm of learning through thousands of trial-and-error attempts guided by simple numerical scores. Instead, it uses an LLM’s own language understanding to reflect on its performance, diagnose errors, and iteratively evolve its instructions. In addition to being more accurate than established techniques, GEPA is significantly more efficient, achieving superior results with up to 35 times fewer trial runs.&lt;/p&gt;&lt;p&gt;For businesses building complex AI agents and workflows, this translates directly into faster development cycles, substantially lower computational costs, and more performant, reliable applications.&lt;/p&gt;&lt;p&gt;Modern enterprise AI applications are rarely a single call to an LLM. They are often “compound AI systems,” complex workflows that chain multiple LLM modules, external tools such as databases or code interpreters, and custom logic to perform sophisticated tasks, including multi-step research and data analysis.&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;A popular way to optimize these systems is through reinforcement learning methods&lt;span&gt;, such as&amp;nbsp;Group Relative Policy Optimization&amp;nbsp;(GRPO), a technique employed in popular reasoning models, including&lt;/span&gt; DeepSeek-R1. This method treats the system as a black box; it runs a task, gets a simple success metric (a “scalar reward,” like a score of 7/10), and uses this feedback to slowly nudge the model’s parameters in the right direction.&lt;/p&gt;



&lt;p&gt;The major drawback of RL is its sample inefficiency. To learn effectively from these sparse numerical scores, RL methods often require tens of thousands, or even hundreds of thousands, of trial runs, known as “rollouts.” For any real-world enterprise application that involves expensive tool calls (e.g., API queries, code compilation) or uses powerful proprietary models, this process is prohibitively slow and costly.&lt;/p&gt;



&lt;p&gt;As Lakshya A Agrawal, co-author of the paper and doctoral student at UC Berkeley, told VentureBeat, this complexity is a major barrier for many companies. “For many teams, RL is not practical due to its cost and complexity—and their go-to approach so far would often just be prompt engineering by hand,” Agrawal said. He noted that GEPA is designed for teams that need to optimize systems built on top-tier models that often can’t be fine-tuned, allowing them to improve performance without managing custom GPU clusters.&lt;/p&gt;



&lt;p&gt;The researchers frame this challenge as follows: “How can we extract maximal learning signal from every expensive rollout to enable effective adaptation of complex, modular AI systems in low-data or budget-constrained settings?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-an-optimizer-that-learns-with-language"&gt;An optimizer that learns with language&lt;/h2&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3015654" height="464" src="https://venturebeat.com/wp-content/uploads/2025/08/image_b1ddfc.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA framework Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;GEPA (Genetic-Pareto) is a prompt optimizer that tackles this challenge by replacing sparse rewards with rich, natural language feedback. It leverages the fact that the entire execution of an AI system (including its reasoning steps, tool calls, and even error messages) can be serialized into text that an LLM can read and understand. GEPA’s methodology is built on three core pillars.&lt;/p&gt;



&lt;p&gt;First is “genetic prompt evolution,” where GEPA treats a population of prompts like a gene pool. It iteratively “mutates” prompts to create new, potentially better versions. This mutation is an intelligent process driven by the second pillar: “reflection with natural language feedback.” After a few rollouts, GEPA provides an LLM with the full execution trace (what the system tried to do) and the outcome (what went right or wrong). The LLM then “reflects” on this feedback in natural language to diagnose the problem and write an improved, more detailed prompt. For instance, instead of just seeing a low score on a code generation task, it might analyze a compiler error and conclude the prompt needs to specify a particular library version.&lt;/p&gt;



&lt;p&gt;The third pillar is “Pareto-based selection,” which ensures smart exploration. Instead of focusing only on the single best-performing prompt, which can lead to getting stuck in a suboptimal solution (a “local optimum”), GEPA maintains a diverse roster of “specialist” prompts. It tracks which prompts perform best on different individual examples, creating a list of top candidates. By sampling from this diverse set of winning strategies, GEPA ensures it explores more solutions and is more likely to discover a prompt that generalizes well across a wide range of inputs.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015655" height="412" src="https://venturebeat.com/wp-content/uploads/2025/08/image_25fed4.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Selecting a single best candidate (left) can result in models getting stuck in local minima while Pareto selection (right) can explore more options and find optimal solutions Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The effectiveness of this entire process hinges on what the researchers call “feedback engineering.” Agrawal explains that the key is to surface the rich, textual details that systems already produce but often discard. “Traditional pipelines often reduce this detail to a single numerical reward, obscuring why particular outcomes occur,” he said. “GEPA’s core guidance is to structure feedback that surfaces not only outcomes but also intermediate trajectories and errors in plain text—the same evidence a human would use to diagnose system behavior.”&lt;/p&gt;



&lt;p&gt;For example, for a document retrieval system, this means listing which documents were retrieved correctly and which were missed, rather than just calculating a final score.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-gepa-in-action"&gt;GEPA in action&lt;/h2&gt;



&lt;p&gt;The researchers evaluated GEPA across four diverse tasks, including multi-hop question answering (HotpotQA) and privacy-preserving queries (PUPA). They used both open-source (Qwen3 8B) and proprietary (GPT-4.1 mini) models, comparing GEPA against the RL-based GRPO and the state-of-the-art prompt optimizer MIPROv2.&lt;/p&gt;



&lt;p&gt;Across all tasks, GEPA substantially outperformed GRPO, achieving up to a 19% higher score while using up to 35 times fewer rollouts. Agrawal provided a concrete example of this efficiency gain: “We used GEPA to optimize a QA system in ~3 hours versus GRPO’s 24 hours—an 8x reduction in development time, while also achieving 20% higher performance,” he explained. “RL-based optimization of the same scenario in our test cost about $300 in GPU time, while GEPA cost less than $20 for better results—15x savings in our experiments.”&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015656" height="289" src="https://venturebeat.com/wp-content/uploads/2025/08/image_1b53c1.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;GEPA outperforms other baselines on key benchmarks Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Beyond raw performance, the researchers found that GEPA-optimized systems are more reliable when faced with new, unseen data. This is measured by the “generalization gap” (the difference between performance on training data and final test data). Agrawal hypothesizes that this is because GEPA learns from richer feedback. “GEPA’s smaller generalization gap may stem from its use of rich natural-language feedback on each outcome—what worked, what failed, and why—rather than relying solely on a single scalar reward,” he said. “This may encourage the system to develop instructions and strategies grounded in a broader understanding of success, instead of merely learning patterns specific to the training data.” For enterprises, this improved reliability means less brittle, more adaptable AI applications in customer-facing roles.&lt;/p&gt;



&lt;p&gt;A major practical benefit is that GEPA’s instruction-based prompts are up to 9.2 times shorter than prompts produced by optimizers like MIPROv2, which include many few-shot examples. Shorter prompts decrease latency and reduce costs for API-based models. This makes the final application faster and cheaper to run in production.&lt;/p&gt;



&lt;p&gt;The paper also presents promising results for utilizing GEPA as an “inference-time” search strategy, transforming the AI from a single-answer generator into an iterative problem solver. Agrawal described a scenario where GEPA could be integrated into a company’s CI/CD pipeline. When new code is committed, GEPA could automatically generate and refine multiple optimized versions, test them for performance, and open a pull request with the best-performing variant for engineers to review. “This turns optimization into a continuous, automated process—rapidly generating solutions that often match or surpass expert hand-tuning,” Agrawal noted. In their experiments on CUDA code generation, this approach boosted performance on 20% of tasks to an expert level, compared to 0% for a single-shot attempt from GPT-4o.&lt;/p&gt;



&lt;p&gt;The paper’s authors believe GEPA is a foundational step toward a new paradigm of AI development. But beyond creating more human-like AI, its most immediate impact may be in who gets to build high-performing systems.&lt;/p&gt;



&lt;p&gt;“We expect GEPA to enable a positive shift in AI system building—making the optimization of such systems approachable by end-users, who often have the domain expertise relevant to the task, but not necessarily the time and willingness to learn complex RL specifics,” Agrawal said. “It gives power directly to the stakeholders with the exact task-specific domain knowledge.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/gepa-optimizes-llms-without-costly-reinforcement-learning/</guid><pubDate>Mon, 18 Aug 2025 20:41:22 +0000</pubDate></item><item><title>[NEW] Hugging Face: 5 ways enterprises can slash AI costs without sacrificing performance (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/hugging-face-5-ways-enterprises-can-slash-ai-costs-without-sacrificing-performance/</link><description>&lt;p&gt;Enterprises seem to accept it as a basic fact: AI models require a significant amount of compute; they simply have to find ways to obtain more of it.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But it doesn’t have to be that way, according to Sasha Luccioni, AI and climate lead at Hugging Face. What if there’s a smarter way to use AI? What if, instead of striving for more (often unnecessary) compute and ways to power it, they can focus on improving model performance and accuracy?&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ultimately, model makers and enterprises are focusing on the wrong issue: They should be computing &lt;em&gt;smarter&lt;/em&gt;, not harder or doing more, Luccioni says.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“There are smarter ways of doing things that we’re currently under-exploring, because we’re so blinded by: We need more FLOPS, we need more GPUs, we need more time,” she said.&amp;nbsp;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Here are five key learnings from Hugging Face that can help enterprises of all sizes use AI more efficiently.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-1-right-size-the-model-to-the-task-nbsp"&gt;1: Right-size the model to the task&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Avoid defaulting to giant, general-purpose models for every use case. Task-specific or distilled models can match, or even &lt;span&gt;surpass,&amp;nbsp;larger models&amp;nbsp;in terms of accuracy for targeted workloads — at a lower cost and with reduced energy consumption&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Luccioni, in fact, has found in testing that a task-specific model uses 20 to 30 times less energy than a general-purpose one. “Because it’s a model that can do that one task, as opposed to any task that you throw at it, which is often the case with large language models,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Distillation is key here; a full model could initially be trained from scratch and then refined for a specific task. DeepSeek R1, for instance, is “so huge that most organizations can’t afford to use it” because you need at least 8 GPUs, Luccioni noted. By contrast, distilled versions can be 10, 20 or even 30X smaller and run on a single GPU.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In general, open-source models help with efficiency, she noted, as they don’t need to be trained from scratch. That’s compared to just a few years ago, when enterprises were wasting resources because they couldn’t find the model they needed; nowadays, they can start out with a base model and fine-tune and adapt it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It provides incremental shared innovation, as opposed to siloed, everyone’s training their models on their datasets and essentially wasting compute in the process,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It’s becoming clear that companies are quickly getting disillusioned with gen AI, as costs are not yet proportionate to the benefits. Generic use cases, such as writing emails or transcribing meeting notes, are genuinely helpful. However, task-specific models still require “a lot of work” because out-of-the-box models don’t cut it and are also more costly, said Luccioni.&lt;/p&gt;



&lt;p&gt;This is the next frontier of added value. “A lot of companies do want a specific task done,” Luccioni noted. “They don’t want AGI, they want specific intelligence. And that’s the gap that needs to be bridged.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-2-make-efficiency-the-default"&gt;2. Make efficiency the default&lt;/h2&gt;



&lt;p&gt;Adopt “nudge theory” in system design, set conservative reasoning budgets, limit always-on generative features and require opt-in for high-cost compute modes.&lt;/p&gt;



&lt;p&gt;In cognitive science, “nudge theory” is a behavioral change management approach designed to influence human behavior subtly. The “canonical example,” Luccioni noted, is adding cutlery to takeout: Having people decide whether they want plastic utensils, rather than automatically including them with every order, can significantly reduce waste.&lt;/p&gt;



&lt;p&gt;“Just getting people to opt into something versus opting out of something is actually a very powerful mechanism for changing people’s behavior,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Default mechanisms are also unnecessary, as they increase use and, therefore, costs because models are doing more work than they need to. For instance, with popular search engines such as Google, a gen AI summary automatically populates at the top by default. Luccioni also noted that, when she recently used&amp;nbsp;OpenAI’s GPT-5, the model automatically worked in full reasoning mode on “very simple questions.”&lt;/p&gt;



&lt;p&gt;“For me, it should be the exception,” she said. “Like, ‘what’s the meaning of life, then sure, I want a gen AI summary.’ But with ‘What’s the weather like in Montreal,’ or ‘What are the opening hours of my local pharmacy?’ I do not need a generative AI summary, yet it’s the default. I think that the default mode should be no reasoning.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-3-optimize-hardware-utilization"&gt;3. Optimize hardware utilization&lt;/h2&gt;



&lt;p&gt;Use batching; adjust precision and fine-tune batch sizes for specific hardware generation to minimize wasted memory and power draw.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance, enterprises should ask themselves: Does the model need to be on all the time? Will people be pinging it in real time, 100 requests at once? In that case, always-on optimization is necessary, Luccioni noted. However, in many others, it’s not; the model can be run periodically to optimize memory usage, and batching can ensure optimal memory utilization.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of like an engineering challenge, but a very specific one, so it’s hard to say, ‘Just distill all the models,’ or ‘change the precision on all the models,’” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one of her recent studies, she found that batch size depends on hardware, even down to the specific type or version. Going from one batch size to plus-one can increase energy use because models need more memory bars.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This is something that people don’t really look at, they’re just like, ‘Oh, I’m gonna maximize the batch size,’ but it really comes down to tweaking all these different things, and all of a sudden it’s super efficient, but it only works in your specific context,” Luccioni explained.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-4-incentivize-energy-transparency"&gt;4. Incentivize energy transparency&lt;/h2&gt;



&lt;p&gt;It always helps when people are incentivized; to this end, Hugging Face earlier this year launched AI Energy Score. It’s a novel way to promote more energy efficiency, utilizing a 1- to 5-star rating system, with the most efficient models earning a “five-star” status.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It could be considered the “Energy Star for AI,” and was inspired by the potentially-soon-to-be-defunct federal program, which set energy efficiency specifications and branded qualifying appliances with an Energy Star logo.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For a couple of decades, it was really a positive motivation, people wanted that star rating, right?,” said Luccioni. “Something similar with Energy Score would be great.”&lt;/p&gt;



&lt;p&gt;Hugging Face has a leaderboard up now, which it plans to update with new models (DeepSeek, GPT-oss) in September, and continually do so every 6 months or sooner as new models become available. The goal is that model builders will consider the rating as a “badge of honor,” Luccioni said. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-5-rethink-the-more-compute-is-better-mindset"&gt;5. Rethink the “more compute is better” mindset&lt;/h2&gt;



&lt;p&gt;Instead of chasing the largest GPU clusters, begin with the question: “What is the smartest way to achieve the result?” For many workloads, smarter architectures and better-curated data outperform brute-force scaling.&lt;/p&gt;



&lt;p&gt;“I think that people probably don’t need as many GPUs as they think they do,” said Luccioni. Instead of simply going for the biggest clusters, she urged enterprises to rethink the tasks GPUs will be completing and why they need them, how they performed those types of tasks before, and what adding extra GPUs will ultimately get them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of this race to the bottom where we need a bigger cluster,” she said. “It’s thinking about what you’re using AI for, what technique do you need, what does that require?”&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;p&gt;Enterprises seem to accept it as a basic fact: AI models require a significant amount of compute; they simply have to find ways to obtain more of it.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But it doesn’t have to be that way, according to Sasha Luccioni, AI and climate lead at Hugging Face. What if there’s a smarter way to use AI? What if, instead of striving for more (often unnecessary) compute and ways to power it, they can focus on improving model performance and accuracy?&amp;nbsp;&lt;/p&gt;&lt;p&gt;Ultimately, model makers and enterprises are focusing on the wrong issue: They should be computing &lt;em&gt;smarter&lt;/em&gt;, not harder or doing more, Luccioni says.&amp;nbsp;&lt;/p&gt;&lt;p&gt;“There are smarter ways of doing things that we’re currently under-exploring, because we’re so blinded by: We need more FLOPS, we need more GPUs, we need more time,” she said.&amp;nbsp;&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Here are five key learnings from Hugging Face that can help enterprises of all sizes use AI more efficiently.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-1-right-size-the-model-to-the-task-nbsp"&gt;1: Right-size the model to the task&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Avoid defaulting to giant, general-purpose models for every use case. Task-specific or distilled models can match, or even &lt;span&gt;surpass,&amp;nbsp;larger models&amp;nbsp;in terms of accuracy for targeted workloads — at a lower cost and with reduced energy consumption&lt;/span&gt;.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Luccioni, in fact, has found in testing that a task-specific model uses 20 to 30 times less energy than a general-purpose one. “Because it’s a model that can do that one task, as opposed to any task that you throw at it, which is often the case with large language models,” she said.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Distillation is key here; a full model could initially be trained from scratch and then refined for a specific task. DeepSeek R1, for instance, is “so huge that most organizations can’t afford to use it” because you need at least 8 GPUs, Luccioni noted. By contrast, distilled versions can be 10, 20 or even 30X smaller and run on a single GPU.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In general, open-source models help with efficiency, she noted, as they don’t need to be trained from scratch. That’s compared to just a few years ago, when enterprises were wasting resources because they couldn’t find the model they needed; nowadays, they can start out with a base model and fine-tune and adapt it.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It provides incremental shared innovation, as opposed to siloed, everyone’s training their models on their datasets and essentially wasting compute in the process,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It’s becoming clear that companies are quickly getting disillusioned with gen AI, as costs are not yet proportionate to the benefits. Generic use cases, such as writing emails or transcribing meeting notes, are genuinely helpful. However, task-specific models still require “a lot of work” because out-of-the-box models don’t cut it and are also more costly, said Luccioni.&lt;/p&gt;



&lt;p&gt;This is the next frontier of added value. “A lot of companies do want a specific task done,” Luccioni noted. “They don’t want AGI, they want specific intelligence. And that’s the gap that needs to be bridged.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-2-make-efficiency-the-default"&gt;2. Make efficiency the default&lt;/h2&gt;



&lt;p&gt;Adopt “nudge theory” in system design, set conservative reasoning budgets, limit always-on generative features and require opt-in for high-cost compute modes.&lt;/p&gt;



&lt;p&gt;In cognitive science, “nudge theory” is a behavioral change management approach designed to influence human behavior subtly. The “canonical example,” Luccioni noted, is adding cutlery to takeout: Having people decide whether they want plastic utensils, rather than automatically including them with every order, can significantly reduce waste.&lt;/p&gt;



&lt;p&gt;“Just getting people to opt into something versus opting out of something is actually a very powerful mechanism for changing people’s behavior,” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Default mechanisms are also unnecessary, as they increase use and, therefore, costs because models are doing more work than they need to. For instance, with popular search engines such as Google, a gen AI summary automatically populates at the top by default. Luccioni also noted that, when she recently used&amp;nbsp;OpenAI’s GPT-5, the model automatically worked in full reasoning mode on “very simple questions.”&lt;/p&gt;



&lt;p&gt;“For me, it should be the exception,” she said. “Like, ‘what’s the meaning of life, then sure, I want a gen AI summary.’ But with ‘What’s the weather like in Montreal,’ or ‘What are the opening hours of my local pharmacy?’ I do not need a generative AI summary, yet it’s the default. I think that the default mode should be no reasoning.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-3-optimize-hardware-utilization"&gt;3. Optimize hardware utilization&lt;/h2&gt;



&lt;p&gt;Use batching; adjust precision and fine-tune batch sizes for specific hardware generation to minimize wasted memory and power draw.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For instance, enterprises should ask themselves: Does the model need to be on all the time? Will people be pinging it in real time, 100 requests at once? In that case, always-on optimization is necessary, Luccioni noted. However, in many others, it’s not; the model can be run periodically to optimize memory usage, and batching can ensure optimal memory utilization.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of like an engineering challenge, but a very specific one, so it’s hard to say, ‘Just distill all the models,’ or ‘change the precision on all the models,’” said Luccioni.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In one of her recent studies, she found that batch size depends on hardware, even down to the specific type or version. Going from one batch size to plus-one can increase energy use because models need more memory bars.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This is something that people don’t really look at, they’re just like, ‘Oh, I’m gonna maximize the batch size,’ but it really comes down to tweaking all these different things, and all of a sudden it’s super efficient, but it only works in your specific context,” Luccioni explained.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-4-incentivize-energy-transparency"&gt;4. Incentivize energy transparency&lt;/h2&gt;



&lt;p&gt;It always helps when people are incentivized; to this end, Hugging Face earlier this year launched AI Energy Score. It’s a novel way to promote more energy efficiency, utilizing a 1- to 5-star rating system, with the most efficient models earning a “five-star” status.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;It could be considered the “Energy Star for AI,” and was inspired by the potentially-soon-to-be-defunct federal program, which set energy efficiency specifications and branded qualifying appliances with an Energy Star logo.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“For a couple of decades, it was really a positive motivation, people wanted that star rating, right?,” said Luccioni. “Something similar with Energy Score would be great.”&lt;/p&gt;



&lt;p&gt;Hugging Face has a leaderboard up now, which it plans to update with new models (DeepSeek, GPT-oss) in September, and continually do so every 6 months or sooner as new models become available. The goal is that model builders will consider the rating as a “badge of honor,” Luccioni said. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-5-rethink-the-more-compute-is-better-mindset"&gt;5. Rethink the “more compute is better” mindset&lt;/h2&gt;



&lt;p&gt;Instead of chasing the largest GPU clusters, begin with the question: “What is the smartest way to achieve the result?” For many workloads, smarter architectures and better-curated data outperform brute-force scaling.&lt;/p&gt;



&lt;p&gt;“I think that people probably don’t need as many GPUs as they think they do,” said Luccioni. Instead of simply going for the biggest clusters, she urged enterprises to rethink the tasks GPUs will be completing and why they need them, how they performed those types of tasks before, and what adding extra GPUs will ultimately get them.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“It’s kind of this race to the bottom where we need a bigger cluster,” she said. “It’s thinking about what you’re using AI for, what technique do you need, what does that require?”&amp;nbsp;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/hugging-face-5-ways-enterprises-can-slash-ai-costs-without-sacrificing-performance/</guid><pubDate>Mon, 18 Aug 2025 21:10:04 +0000</pubDate></item><item><title>[NEW] Nvidia releases a new small, open model Nemotron-Nano-9B-v2 with toggle on/off reasoning (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/nvidia-releases-a-new-small-open-model-nemotron-nano-9b-v2-with-toggle-on-off-reasoning/</link><description>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Small models are having a moment. On the heels of the release of a new AI vision model small enough to fit on a smartwatch from MIT spinoff Liquid AI, and a model small enough to run on a smartphone from Google, &lt;strong&gt;Nvidia is joining the party today&lt;/strong&gt; with a new small language model (SLM) of its own, &lt;strong&gt;Nemotron-Nano-9B-V2&lt;/strong&gt;, which attained the highest performance in its class on selected benchmarks and comes with the ability for users to toggle on and off AI “reasoning,” that is, self-checking before outputting an answer.&lt;/p&gt;&lt;p&gt;While the 9 billion parameters are larger than some of the multimillion parameter small models VentureBeat has covered recently&lt;strong&gt;, Nvidia notes it is a meaningful reduction from its original size of 12 billion parameters&lt;/strong&gt; and is designed to fit on a &lt;strong&gt;single Nvidia A10 GPU&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;As Oleksii Kuchiaev, Nvidia Director of AI Model Post-Training, said on X in response to a question I submitted to him: “The 12B was pruned to 9B to specifically fit A10 which is a popular GPU choice for deployment.&lt;strong&gt; It is also a hybrid model which allows it to process a larger batch size and be up to 6x faster than similar sized transformer models.”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For context, many leading LLMs are in the 70+ billion parameter range (recall parameters refer to the internal settings governing the model’s behavior, with more generally denoting a larger and more capable, yet more compute intensive model).&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The model handles multiple languages, including English, German, Spanish, French, Italian, Japanese, and in extended descriptions, Korean, Portuguese, Russian, and Chinese. It’s suitable for both &lt;strong&gt;instruction following and code generation.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nemotron-Nano-9B-V2 and its pre-training datasets available right now on Hugging Face and through the company’s model catalog. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-fusion-of-transformer-and-mamba-architectures"&gt;A fusion of Transformer and Mamba architectures&lt;/h2&gt;



&lt;p&gt;It’s based on Nemotron-H, a set of hybrid Mamba-Transformer models that form the foundation for the company’s latest offerings.&lt;/p&gt;



&lt;p&gt;While most popular LLMs are pure “Transformer” models, which rely entirely on attention layers, they can become costly in memory and compute as sequence lengths grow.&lt;/p&gt;



&lt;p&gt;Instead, Nemotron-H models and others using the Mamba architecture developed by researchers at Carnegie Mellon University and Princeton, also &lt;strong&gt;weave in selective state space models (or SSMs), which can handle very long sequences of information in and out by maintaining state.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;These layers scale linearly with sequence length and can process contexts much longer than standard self-attention without the same memory and compute overhead.&lt;/p&gt;



&lt;p&gt;A h&lt;strong&gt;ybrid Mamba-Transformer reduces those costs by substituting most of the attention with linear-time state space layers, achieving up to 2–3× higher throughput on long contexts&lt;/strong&gt; with comparable accuracy.&lt;/p&gt;



&lt;p&gt;Other AI labs beyond Nvidia such as Ai2 have also released models based on the Mamba architecture.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-toggle-on-of-reasoning-using-language"&gt;Toggle on/of reasoning using language&lt;/h2&gt;



&lt;p&gt;Nemotron-Nano-9B-v2 is positioned as a unified, text-only chat and reasoning model trained from scratch. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;system defaults to generating a reasoning trace before providing a final answer, though users can toggle this behavior &lt;/strong&gt;through simple control tokens such as /think or /no_think. &lt;/p&gt;



&lt;p&gt;The model also i&lt;strong&gt;ntroduces runtime “thinking budget” management&lt;/strong&gt;, which &lt;strong&gt;allows developers to cap the number of tokens &lt;/strong&gt;devoted to internal reasoning before the model completes a response. &lt;/p&gt;



&lt;p&gt;This mechanism is aimed at balancing accuracy with latency, &lt;strong&gt;particularly in applications like customer support or autonomous agents.&lt;/strong&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-benchmarks-tell-a-promising-story"&gt;Benchmarks tell a promising story&lt;/h2&gt;



&lt;p&gt;Evaluation results highlight competitive accuracy against other open small-scale models. Tested in “reasoning on” mode using the NeMo-Skills suite,&lt;strong&gt; Nemotron-Nano-9B-v2 reaches 72.1 percent on AIME25&lt;/strong&gt;, &lt;strong&gt;97.8 percent on MATH500, 64.0 percent on GPQA&lt;/strong&gt;, and&lt;strong&gt; 71.1 percent on LiveCodeBench&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Scores on instruction following and long-context benchmarks are also reported: &lt;strong&gt;90.3 percent on IFEval, 78.9 percent on the RULER 128K test&lt;/strong&gt;, and smaller but measurable gains on BFCL v3 and the HLE benchmark. &lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015748" height="335" src="https://venturebeat.com/wp-content/uploads/2025/08/accuracy_chart.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;strong&gt;Across the board, Nano-9B-v2 shows higher accuracy than Qwen3-8B,&lt;/strong&gt; a common point of comparison.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015751" height="530" src="https://venturebeat.com/wp-content/uploads/2025/08/acc-vs-budget.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Nvidia illustrates these results with accuracy-versus-budget curves that show how performance scales as the token allowance for reasoning increases. The company suggests that careful budget control can help developers optimize both quality and latency in production use cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-trained-on-synthetic-datasets"&gt;Trained on synthetic datasets&lt;/h2&gt;



&lt;p&gt;Both the Nano model and the Nemotron-H family rely on a mixture of curated, web-sourced, and synthetic training data. &lt;/p&gt;



&lt;p&gt;The corpora include general text, code, mathematics, science, legal, and financial documents, as well as alignment-style question-answering datasets. &lt;/p&gt;



&lt;p&gt;Nvidia confirms the use of synthetic reasoning traces generated by other large models to strengthen performance on complex benchmarks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-commercial-use"&gt;Licensing and commercial use&lt;/h2&gt;



&lt;p&gt;The Nano-9B-v2 model is released under the Nvidia Open Model License Agreement, last updated in June 2025. &lt;/p&gt;



&lt;p&gt;The license is designed to be permissive and enterprise-friendly. &lt;strong&gt;Nvidia explicitly states that the models are &lt;em&gt;commercially usable&lt;/em&gt; out of the box&lt;/strong&gt;, and that&lt;strong&gt; developers are free to create and distribute derivative models. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Importantly, Nvidia does not claim ownership of any outputs generated by the model, leaving responsibility and rights with the developer or organization using it.&lt;/p&gt;



&lt;p&gt;For an enterprise developer, this means the model can be put into production immediately without negotiating a separate commercial license or paying fees tied to usage thresholds, revenue levels, or user counts. There are no clauses requiring a paid license once a company reaches a certain scale, unlike some tiered open licenses used by other providers.&lt;/p&gt;



&lt;p&gt;That said, the agreement does include several conditions enterprises must observe:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: Users cannot bypass or disable built-in safety mechanisms (referred to as “guardrails”) without implementing comparable replacements suited to their deployment.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Redistribution&lt;/strong&gt;: Any redistribution of the model or derivatives must include the Nvidia Open Model License text and attribution (“Licensed by Nvidia Corporation under the Nvidia Open Model License”).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Compliance&lt;/strong&gt;: Users must comply with trade regulations and restrictions (e.g., U.S. export laws).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Trustworthy AI terms&lt;/strong&gt;: Usage must align with Nvidia Trustworthy AI guidelines, which cover responsible deployment and ethical considerations.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Litigation clause&lt;/strong&gt;: If a user initiates copyright or patent litigation against another entity alleging infringement by the model, the license automatically terminates.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;These conditions focus on legal and responsible use rather than commercial scale. Enterprises do not need to seek additional permission or pay royalties to Nvidia simply for building products, monetizing them, or scaling their user base. Instead, they must make sure deployment practices respect safety, attribution, and compliance obligations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-positioning-in-the-market"&gt;Positioning in the market&lt;/h2&gt;



&lt;p&gt;With Nemotron-Nano-9B-v2, Nvidia is targeting developers who need a balance of reasoning capability and deployment efficiency at smaller scales. &lt;/p&gt;



&lt;p&gt;The runtime budget control and reasoning-toggle features are meant to give system builders more flexibility in managing accuracy versus response speed. &lt;/p&gt;



&lt;p&gt;Their release on Hugging Face and Nvidia’s model catalog indicates that they are &lt;strong&gt;meant to be broadly accessible for experimentation and integration.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nvidia’s release of Nemotron-Nano-9B-v2 showcase a continued focus on efficiency and controllable reasoning in language models. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;By combining hybrid architectures with new compression and training techniques&lt;/strong&gt;, the company is offering developers tools that seek to maintain accuracy while reducing costs and latency. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</description><content:encoded>&lt;div id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.&lt;/em&gt; &lt;em&gt;Subscribe Now&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Small models are having a moment. On the heels of the release of a new AI vision model small enough to fit on a smartwatch from MIT spinoff Liquid AI, and a model small enough to run on a smartphone from Google, &lt;strong&gt;Nvidia is joining the party today&lt;/strong&gt; with a new small language model (SLM) of its own, &lt;strong&gt;Nemotron-Nano-9B-V2&lt;/strong&gt;, which attained the highest performance in its class on selected benchmarks and comes with the ability for users to toggle on and off AI “reasoning,” that is, self-checking before outputting an answer.&lt;/p&gt;&lt;p&gt;While the 9 billion parameters are larger than some of the multimillion parameter small models VentureBeat has covered recently&lt;strong&gt;, Nvidia notes it is a meaningful reduction from its original size of 12 billion parameters&lt;/strong&gt; and is designed to fit on a &lt;strong&gt;single Nvidia A10 GPU&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;As Oleksii Kuchiaev, Nvidia Director of AI Model Post-Training, said on X in response to a question I submitted to him: “The 12B was pruned to 9B to specifically fit A10 which is a popular GPU choice for deployment.&lt;strong&gt; It is also a hybrid model which allows it to process a larger batch size and be up to 6x faster than similar sized transformer models.”&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For context, many leading LLMs are in the 70+ billion parameter range (recall parameters refer to the internal settings governing the model’s behavior, with more generally denoting a larger and more capable, yet more compute intensive model).&lt;/p&gt;&lt;div id="id"&gt;&lt;div class="post-boilerplate boilerplate-speedbump" id="boilerplate_2803147"&gt;&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;&lt;strong&gt;AI Scaling Hits Its Limits&lt;/strong&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;Power caps, rising token costs, and inference delays are reshaping enterprise AI. Join our exclusive salon to discover how top teams are:&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:list --&gt;
&lt;ul class="wp-block-list"&gt;&lt;!-- wp:list-item --&gt;
&lt;li&gt;Turning energy into a strategic advantage&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Architecting efficient inference for real throughput gains&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;

&lt;!-- wp:list-item --&gt;
&lt;li&gt;Unlocking competitive ROI with sustainable AI systems&lt;/li&gt;
&lt;!-- /wp:list-item --&gt;&lt;/ul&gt;
&lt;!-- /wp:list --&gt;

&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;strong&gt;Secure your spot to stay ahead&lt;/strong&gt;: https://bit.ly/4mwGngO&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator --&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;The model handles multiple languages, including English, German, Spanish, French, Italian, Japanese, and in extended descriptions, Korean, Portuguese, Russian, and Chinese. It’s suitable for both &lt;strong&gt;instruction following and code generation.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nemotron-Nano-9B-V2 and its pre-training datasets available right now on Hugging Face and through the company’s model catalog. &lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-fusion-of-transformer-and-mamba-architectures"&gt;A fusion of Transformer and Mamba architectures&lt;/h2&gt;



&lt;p&gt;It’s based on Nemotron-H, a set of hybrid Mamba-Transformer models that form the foundation for the company’s latest offerings.&lt;/p&gt;



&lt;p&gt;While most popular LLMs are pure “Transformer” models, which rely entirely on attention layers, they can become costly in memory and compute as sequence lengths grow.&lt;/p&gt;



&lt;p&gt;Instead, Nemotron-H models and others using the Mamba architecture developed by researchers at Carnegie Mellon University and Princeton, also &lt;strong&gt;weave in selective state space models (or SSMs), which can handle very long sequences of information in and out by maintaining state.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;These layers scale linearly with sequence length and can process contexts much longer than standard self-attention without the same memory and compute overhead.&lt;/p&gt;



&lt;p&gt;A h&lt;strong&gt;ybrid Mamba-Transformer reduces those costs by substituting most of the attention with linear-time state space layers, achieving up to 2–3× higher throughput on long contexts&lt;/strong&gt; with comparable accuracy.&lt;/p&gt;



&lt;p&gt;Other AI labs beyond Nvidia such as Ai2 have also released models based on the Mamba architecture.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-toggle-on-of-reasoning-using-language"&gt;Toggle on/of reasoning using language&lt;/h2&gt;



&lt;p&gt;Nemotron-Nano-9B-v2 is positioned as a unified, text-only chat and reasoning model trained from scratch. &lt;/p&gt;



&lt;p&gt;The &lt;strong&gt;system defaults to generating a reasoning trace before providing a final answer, though users can toggle this behavior &lt;/strong&gt;through simple control tokens such as /think or /no_think. &lt;/p&gt;



&lt;p&gt;The model also i&lt;strong&gt;ntroduces runtime “thinking budget” management&lt;/strong&gt;, which &lt;strong&gt;allows developers to cap the number of tokens &lt;/strong&gt;devoted to internal reasoning before the model completes a response. &lt;/p&gt;



&lt;p&gt;This mechanism is aimed at balancing accuracy with latency, &lt;strong&gt;particularly in applications like customer support or autonomous agents.&lt;/strong&gt;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-benchmarks-tell-a-promising-story"&gt;Benchmarks tell a promising story&lt;/h2&gt;



&lt;p&gt;Evaluation results highlight competitive accuracy against other open small-scale models. Tested in “reasoning on” mode using the NeMo-Skills suite,&lt;strong&gt; Nemotron-Nano-9B-v2 reaches 72.1 percent on AIME25&lt;/strong&gt;, &lt;strong&gt;97.8 percent on MATH500, 64.0 percent on GPQA&lt;/strong&gt;, and&lt;strong&gt; 71.1 percent on LiveCodeBench&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;Scores on instruction following and long-context benchmarks are also reported: &lt;strong&gt;90.3 percent on IFEval, 78.9 percent on the RULER 128K test&lt;/strong&gt;, and smaller but measurable gains on BFCL v3 and the HLE benchmark. &lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015748" height="335" src="https://venturebeat.com/wp-content/uploads/2025/08/accuracy_chart.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;&lt;strong&gt;Across the board, Nano-9B-v2 shows higher accuracy than Qwen3-8B,&lt;/strong&gt; a common point of comparison.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3015751" height="530" src="https://venturebeat.com/wp-content/uploads/2025/08/acc-vs-budget.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;Nvidia illustrates these results with accuracy-versus-budget curves that show how performance scales as the token allowance for reasoning increases. The company suggests that careful budget control can help developers optimize both quality and latency in production use cases.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-trained-on-synthetic-datasets"&gt;Trained on synthetic datasets&lt;/h2&gt;



&lt;p&gt;Both the Nano model and the Nemotron-H family rely on a mixture of curated, web-sourced, and synthetic training data. &lt;/p&gt;



&lt;p&gt;The corpora include general text, code, mathematics, science, legal, and financial documents, as well as alignment-style question-answering datasets. &lt;/p&gt;



&lt;p&gt;Nvidia confirms the use of synthetic reasoning traces generated by other large models to strengthen performance on complex benchmarks.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-licensing-and-commercial-use"&gt;Licensing and commercial use&lt;/h2&gt;



&lt;p&gt;The Nano-9B-v2 model is released under the Nvidia Open Model License Agreement, last updated in June 2025. &lt;/p&gt;



&lt;p&gt;The license is designed to be permissive and enterprise-friendly. &lt;strong&gt;Nvidia explicitly states that the models are &lt;em&gt;commercially usable&lt;/em&gt; out of the box&lt;/strong&gt;, and that&lt;strong&gt; developers are free to create and distribute derivative models. &lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Importantly, Nvidia does not claim ownership of any outputs generated by the model, leaving responsibility and rights with the developer or organization using it.&lt;/p&gt;



&lt;p&gt;For an enterprise developer, this means the model can be put into production immediately without negotiating a separate commercial license or paying fees tied to usage thresholds, revenue levels, or user counts. There are no clauses requiring a paid license once a company reaches a certain scale, unlike some tiered open licenses used by other providers.&lt;/p&gt;



&lt;p&gt;That said, the agreement does include several conditions enterprises must observe:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Guardrails&lt;/strong&gt;: Users cannot bypass or disable built-in safety mechanisms (referred to as “guardrails”) without implementing comparable replacements suited to their deployment.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Redistribution&lt;/strong&gt;: Any redistribution of the model or derivatives must include the Nvidia Open Model License text and attribution (“Licensed by Nvidia Corporation under the Nvidia Open Model License”).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Compliance&lt;/strong&gt;: Users must comply with trade regulations and restrictions (e.g., U.S. export laws).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Trustworthy AI terms&lt;/strong&gt;: Usage must align with Nvidia Trustworthy AI guidelines, which cover responsible deployment and ethical considerations.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Litigation clause&lt;/strong&gt;: If a user initiates copyright or patent litigation against another entity alleging infringement by the model, the license automatically terminates.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;These conditions focus on legal and responsible use rather than commercial scale. Enterprises do not need to seek additional permission or pay royalties to Nvidia simply for building products, monetizing them, or scaling their user base. Instead, they must make sure deployment practices respect safety, attribution, and compliance obligations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-positioning-in-the-market"&gt;Positioning in the market&lt;/h2&gt;



&lt;p&gt;With Nemotron-Nano-9B-v2, Nvidia is targeting developers who need a balance of reasoning capability and deployment efficiency at smaller scales. &lt;/p&gt;



&lt;p&gt;The runtime budget control and reasoning-toggle features are meant to give system builders more flexibility in managing accuracy versus response speed. &lt;/p&gt;



&lt;p&gt;Their release on Hugging Face and Nvidia’s model catalog indicates that they are &lt;strong&gt;meant to be broadly accessible for experimentation and integration.&lt;/strong&gt;&lt;/p&gt;



&lt;p&gt;Nvidia’s release of Nemotron-Nano-9B-v2 showcase a continued focus on efficiency and controllable reasoning in language models. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;By combining hybrid architectures with new compression and training techniques&lt;/strong&gt;, the company is offering developers tools that seek to maintain accuracy while reducing costs and latency. &lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;			&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/nvidia-releases-a-new-small-open-model-nemotron-nano-9b-v2-with-toggle-on-off-reasoning/</guid><pubDate>Mon, 18 Aug 2025 21:24:47 +0000</pubDate></item></channel></rss>