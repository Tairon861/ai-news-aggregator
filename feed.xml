<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 14 Dec 2025 06:33:40 +0000</lastBuildDate><item><title>AI data center boom could be bad news for other infrastructure projects (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/13/ai-data-center-boom-could-be-bad-news-for-other-infrastructure-projects/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2152222109.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Improvements to roads, bridges, and other infrastructure could take a hit as data center construction accelerates, according to Bloomberg.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2025, state and local governments reportedly sold a record amount of debt for the second year in a row, with strategists predicting another $600 billion in sales next year. Most of that money is expected to fund infrastructure projects.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meanwhile, Census Bureau data reportedly shows that private spending on data center construction was running at annualized run rate of more than $41 billion&amp;nbsp;— roughly the same as state and local government spending on transportation construction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All these projects are likely to compete for construction workers just as the industry faces labor shortages from retirements and President Donald Trump’s immigration crackdown.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Andrew Anagnost, CEO of architecture and design software maker Autodesk, told Bloomberg there’s “absolutely no doubt” that data center construction “sucks resources from other projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I guarantee you a lot of those [infrastructure] projects are not going to move as fast as people want,” he said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/GettyImages-2152222109.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Improvements to roads, bridges, and other infrastructure could take a hit as data center construction accelerates, according to Bloomberg.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2025, state and local governments reportedly sold a record amount of debt for the second year in a row, with strategists predicting another $600 billion in sales next year. Most of that money is expected to fund infrastructure projects.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meanwhile, Census Bureau data reportedly shows that private spending on data center construction was running at annualized run rate of more than $41 billion&amp;nbsp;— roughly the same as state and local government spending on transportation construction.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;All these projects are likely to compete for construction workers just as the industry faces labor shortages from retirements and President Donald Trump’s immigration crackdown.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Andrew Anagnost, CEO of architecture and design software maker Autodesk, told Bloomberg there’s “absolutely no doubt” that data center construction “sucks resources from other projects.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I guarantee you a lot of those [infrastructure] projects are not going to move as fast as people want,” he said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/13/ai-data-center-boom-could-be-bad-news-for-other-infrastructure-projects/</guid><pubDate>Sat, 13 Dec 2025 19:37:47 +0000</pubDate></item><item><title>Why most enterprise AI coding pilots underperform (Hint: It's not the model) (AI | VentureBeat)</title><link>https://venturebeat.com/ai/why-most-enterprise-ai-coding-pilots-underperform-hint-its-not-the-model</link><description>[unable to retrieve full-text content]&lt;p&gt;Gen AI in software engineering has moved well beyond autocomplete. The emerging frontier is &lt;a href="https://venturebeat.com/ai/why-ai-coding-agents-arent-production-ready-brittle-context-windows-broken"&gt;agentic coding&lt;/a&gt;: AI systems capable of planning changes, executing them across multiple steps and iterating based on feedback. Yet despite the excitement around “AI agents that code,” most enterprise deployments underperform. The limiting factor is no longer the model. It’s &lt;i&gt;context&lt;/i&gt;: The structure, history and intent surrounding the code being changed. In other words, enterprises are now facing a systems design problem: They have not yet engineered the environment these agents operate in.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The shift from assistance to agency&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The past year has seen a rapid evolution from assistive coding tools to agentic workflows. Research has begun to formalize what agentic behavior means in practice: The ability to reason across design, testing, execution and validation rather than generate isolated snippets. Work such as &lt;a href="https://arxiv.org/abs/2503.14269?utm_source=chatgpt.com"&gt;&lt;u&gt;dynamic action re-sampling&lt;/u&gt;&lt;/a&gt; shows that allowing agents to branch, reconsider and revise their own decisions significantly improves outcomes in large, interdependent codebases. At the platform level, providers like GitHub are now building dedicated agent orchestration environments, such as &lt;a href="https://venturebeat.com/ai/githubs-agent-hq-aims-to-solve-enterprises-biggest-ai-coding-problem-too?utm_source=chatgpt.com"&gt;&lt;u&gt;Copilot Agent and Agent HQ&lt;/u&gt;&lt;/a&gt;, to support multi-agent collaboration inside real enterprise pipelines.&lt;/p&gt;&lt;p&gt;But early field results tell a cautionary story. When organizations introduce agentic tools without addressing workflow and environment, productivity can decline. A randomized control study this year showed that developers who used AI assistance in unchanged workflows completed tasks more slowly, largely due to verification, rework and confusion around intent. The lesson is straightforward: Autonomy without orchestration rarely yields efficiency.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why context engineering is the real unlock&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In every unsuccessful deployment I’ve observed, the failure stemmed from context. When agents lack a structured understanding of a codebase, specifically its relevant modules, dependency graph, test harness, architectural conventions and change history. They often generate output that appears correct but is disconnected from reality. Too much information overwhelms the agent; too little forces it to guess. The goal is not to feed the model more tokens. The goal is to determine what should be visible to the agent, when and in what form.&lt;/p&gt;&lt;p&gt;The teams seeing meaningful gains treat context as an engineering surface. They create tooling to snapshot, compact and version the &lt;a href="https://venturebeat.com/ai/ontology-is-the-real-guardrail-how-to-stop-ai-agents-from-misunderstanding"&gt;agent’s working memory&lt;/a&gt;: What is persisted across turns, what is discarded, what is summarized and what is linked instead of inlined. They design deliberation steps rather than prompting sessions. They make the specification a first-class artifact, something reviewable, testable and owned, not a transient chat history. This shift aligns with a broader trend some researchers describe as “specs becoming the new source of truth.”&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Workflow must change alongside tooling&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;But context alone isn’t enough. Enterprises must re-architect the workflows around these agents. As &lt;a href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/one-year-of-agentic-ai-six-lessons-from-the-people-doing-the-work?utm_source=chatgpt.com"&gt;&lt;u&gt;McKinsey’s 2025 report &lt;/u&gt;&lt;i&gt;&lt;u&gt;“One Year of Agentic AI”&lt;/u&gt;&lt;/i&gt;&lt;/a&gt; noted, productivity gains arise not from layering AI onto existing processes but from rethinking the process itself. When teams simply drop an agent into an unaltered workflow, they invite friction: Engineers spend more time verifying AI-written code than they would have spent writing it themselves. The agents can only amplify what’s already structured: Well-tested, modular codebases with clear ownership and documentation. Without those foundations, autonomy becomes chaos.&lt;/p&gt;&lt;p&gt;Security and governance, too, demand a shift in mindset. AI-generated code introduces new forms of risk: Unvetted dependencies, subtle license violations and undocumented modules that escape peer review. Mature teams are beginning to integrate agentic activity directly into their &lt;a href="https://venturebeat.com/ai/why-observable-ai-is-the-missing-sre-layer-enterprises-need-for-reliable"&gt;CI/CD pipelines&lt;/a&gt;, treating agents as autonomous contributors whose work must pass the same static analysis, audit logging and approval gates as any human developer. GitHub’s own documentation highlights this trajectory, positioning Copilot Agents not as replacements for engineers but as orchestrated participants in secure, reviewable workflows. The goal isn’t to let an AI “write everything,” but to ensure that when it acts, it does so inside defined guardrails.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What enterprise decision-makers should focus on now&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For technical leaders, the path forward starts with readiness rather than hype. Monoliths with sparse tests rarely yield net gains; agents thrive where tests are authoritative and can drive iterative refinement. This is exactly the loop &lt;a href="https://www.anthropic.com/research/building-effective-agents?utm_source=chatgpt.com"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; calls out for coding agents. Pilots in tightly scoped domains (test generation, legacy modernization, isolated refactors); treat each deployment as an experiment with explicit metrics (defect escape rate, PR cycle time, change failure rate, security findings burned down). As your usage grows, treat agents as data infrastructure: Every plan, context snapshot, action log and test run is data that composes into a searchable memory of engineering intent, and a durable competitive advantage.&lt;/p&gt;&lt;p&gt;Under the hood, agentic coding is less a tooling problem than a data problem. Every context snapshot, test iteration and code revision becomes a form of structured data that must be stored, indexed and reused. As these agents proliferate, enterprises will find themselves managing an entirely new data layer: One that captures not just what was built, but how it was reasoned about. This shift turns engineering logs into a knowledge graph of intent, decision-making and validation. In time, the organizations that can search and replay this contextual memory will outpace those who still treat code as static text.&lt;/p&gt;&lt;p&gt;The coming year will likely determine whether agentic coding becomes a cornerstone of enterprise development or another inflated promise. The difference will hinge on context engineering: How intelligently teams design the informational substrate their agents rely on. The winners will be those who see autonomy not as magic, but as an extension of disciplined systems design:Clear workflows, measurable feedback, and rigorous governance.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Bottom line&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Platforms are converging on orchestration and guardrails, and research keeps improving context control at inference time. The winners over the next 12 to 24 months won’t be the teams with the flashiest model; they’ll be the ones that engineer context as an asset and treat workflow as the product. Do that, and autonomy compounds. Skip it, and the review queue does.&lt;/p&gt;&lt;p&gt;Context + agent = leverage. Skip the first half, and the rest collapses.&lt;/p&gt;&lt;p&gt;&lt;i&gt;Dhyey Mavani is accelerating generative AI at LinkedIn.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Read more from our &lt;/i&gt;&lt;a href="https://venturebeat.com/datadecisionmakers"&gt;&lt;i&gt;guest writers&lt;/i&gt;&lt;/a&gt;&lt;i&gt;. Or, consider submitting a post of your own! See our &lt;/i&gt;&lt;a href="https://venturebeat.com/guest-posts"&gt;&lt;i&gt;guidelines here&lt;/i&gt;&lt;/a&gt;&lt;i&gt;. &lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Gen AI in software engineering has moved well beyond autocomplete. The emerging frontier is &lt;a href="https://venturebeat.com/ai/why-ai-coding-agents-arent-production-ready-brittle-context-windows-broken"&gt;agentic coding&lt;/a&gt;: AI systems capable of planning changes, executing them across multiple steps and iterating based on feedback. Yet despite the excitement around “AI agents that code,” most enterprise deployments underperform. The limiting factor is no longer the model. It’s &lt;i&gt;context&lt;/i&gt;: The structure, history and intent surrounding the code being changed. In other words, enterprises are now facing a systems design problem: They have not yet engineered the environment these agents operate in.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The shift from assistance to agency&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The past year has seen a rapid evolution from assistive coding tools to agentic workflows. Research has begun to formalize what agentic behavior means in practice: The ability to reason across design, testing, execution and validation rather than generate isolated snippets. Work such as &lt;a href="https://arxiv.org/abs/2503.14269?utm_source=chatgpt.com"&gt;&lt;u&gt;dynamic action re-sampling&lt;/u&gt;&lt;/a&gt; shows that allowing agents to branch, reconsider and revise their own decisions significantly improves outcomes in large, interdependent codebases. At the platform level, providers like GitHub are now building dedicated agent orchestration environments, such as &lt;a href="https://venturebeat.com/ai/githubs-agent-hq-aims-to-solve-enterprises-biggest-ai-coding-problem-too?utm_source=chatgpt.com"&gt;&lt;u&gt;Copilot Agent and Agent HQ&lt;/u&gt;&lt;/a&gt;, to support multi-agent collaboration inside real enterprise pipelines.&lt;/p&gt;&lt;p&gt;But early field results tell a cautionary story. When organizations introduce agentic tools without addressing workflow and environment, productivity can decline. A randomized control study this year showed that developers who used AI assistance in unchanged workflows completed tasks more slowly, largely due to verification, rework and confusion around intent. The lesson is straightforward: Autonomy without orchestration rarely yields efficiency.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why context engineering is the real unlock&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In every unsuccessful deployment I’ve observed, the failure stemmed from context. When agents lack a structured understanding of a codebase, specifically its relevant modules, dependency graph, test harness, architectural conventions and change history. They often generate output that appears correct but is disconnected from reality. Too much information overwhelms the agent; too little forces it to guess. The goal is not to feed the model more tokens. The goal is to determine what should be visible to the agent, when and in what form.&lt;/p&gt;&lt;p&gt;The teams seeing meaningful gains treat context as an engineering surface. They create tooling to snapshot, compact and version the &lt;a href="https://venturebeat.com/ai/ontology-is-the-real-guardrail-how-to-stop-ai-agents-from-misunderstanding"&gt;agent’s working memory&lt;/a&gt;: What is persisted across turns, what is discarded, what is summarized and what is linked instead of inlined. They design deliberation steps rather than prompting sessions. They make the specification a first-class artifact, something reviewable, testable and owned, not a transient chat history. This shift aligns with a broader trend some researchers describe as “specs becoming the new source of truth.”&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Workflow must change alongside tooling&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;But context alone isn’t enough. Enterprises must re-architect the workflows around these agents. As &lt;a href="https://www.mckinsey.com/capabilities/quantumblack/our-insights/one-year-of-agentic-ai-six-lessons-from-the-people-doing-the-work?utm_source=chatgpt.com"&gt;&lt;u&gt;McKinsey’s 2025 report &lt;/u&gt;&lt;i&gt;&lt;u&gt;“One Year of Agentic AI”&lt;/u&gt;&lt;/i&gt;&lt;/a&gt; noted, productivity gains arise not from layering AI onto existing processes but from rethinking the process itself. When teams simply drop an agent into an unaltered workflow, they invite friction: Engineers spend more time verifying AI-written code than they would have spent writing it themselves. The agents can only amplify what’s already structured: Well-tested, modular codebases with clear ownership and documentation. Without those foundations, autonomy becomes chaos.&lt;/p&gt;&lt;p&gt;Security and governance, too, demand a shift in mindset. AI-generated code introduces new forms of risk: Unvetted dependencies, subtle license violations and undocumented modules that escape peer review. Mature teams are beginning to integrate agentic activity directly into their &lt;a href="https://venturebeat.com/ai/why-observable-ai-is-the-missing-sre-layer-enterprises-need-for-reliable"&gt;CI/CD pipelines&lt;/a&gt;, treating agents as autonomous contributors whose work must pass the same static analysis, audit logging and approval gates as any human developer. GitHub’s own documentation highlights this trajectory, positioning Copilot Agents not as replacements for engineers but as orchestrated participants in secure, reviewable workflows. The goal isn’t to let an AI “write everything,” but to ensure that when it acts, it does so inside defined guardrails.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What enterprise decision-makers should focus on now&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For technical leaders, the path forward starts with readiness rather than hype. Monoliths with sparse tests rarely yield net gains; agents thrive where tests are authoritative and can drive iterative refinement. This is exactly the loop &lt;a href="https://www.anthropic.com/research/building-effective-agents?utm_source=chatgpt.com"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; calls out for coding agents. Pilots in tightly scoped domains (test generation, legacy modernization, isolated refactors); treat each deployment as an experiment with explicit metrics (defect escape rate, PR cycle time, change failure rate, security findings burned down). As your usage grows, treat agents as data infrastructure: Every plan, context snapshot, action log and test run is data that composes into a searchable memory of engineering intent, and a durable competitive advantage.&lt;/p&gt;&lt;p&gt;Under the hood, agentic coding is less a tooling problem than a data problem. Every context snapshot, test iteration and code revision becomes a form of structured data that must be stored, indexed and reused. As these agents proliferate, enterprises will find themselves managing an entirely new data layer: One that captures not just what was built, but how it was reasoned about. This shift turns engineering logs into a knowledge graph of intent, decision-making and validation. In time, the organizations that can search and replay this contextual memory will outpace those who still treat code as static text.&lt;/p&gt;&lt;p&gt;The coming year will likely determine whether agentic coding becomes a cornerstone of enterprise development or another inflated promise. The difference will hinge on context engineering: How intelligently teams design the informational substrate their agents rely on. The winners will be those who see autonomy not as magic, but as an extension of disciplined systems design:Clear workflows, measurable feedback, and rigorous governance.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Bottom line&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Platforms are converging on orchestration and guardrails, and research keeps improving context control at inference time. The winners over the next 12 to 24 months won’t be the teams with the flashiest model; they’ll be the ones that engineer context as an asset and treat workflow as the product. Do that, and autonomy compounds. Skip it, and the review queue does.&lt;/p&gt;&lt;p&gt;Context + agent = leverage. Skip the first half, and the rest collapses.&lt;/p&gt;&lt;p&gt;&lt;i&gt;Dhyey Mavani is accelerating generative AI at LinkedIn.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;&lt;i&gt;Read more from our &lt;/i&gt;&lt;a href="https://venturebeat.com/datadecisionmakers"&gt;&lt;i&gt;guest writers&lt;/i&gt;&lt;/a&gt;&lt;i&gt;. Or, consider submitting a post of your own! See our &lt;/i&gt;&lt;a href="https://venturebeat.com/guest-posts"&gt;&lt;i&gt;guidelines here&lt;/i&gt;&lt;/a&gt;&lt;i&gt;. &lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/why-most-enterprise-ai-coding-pilots-underperform-hint-its-not-the-model</guid><pubDate>Sat, 13 Dec 2025 20:00:00 +0000</pubDate></item></channel></rss>