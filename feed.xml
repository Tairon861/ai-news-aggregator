<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 06 Feb 2026 06:59:06 +0000</lastBuildDate><item><title> ()</title><link>https://www.wired.com/feed/category/artificial-intelligence/rss</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.wired.com/feed/category/artificial-intelligence/rss</guid></item><item><title>OpenAI launches new agentic coding model only minutes after Anthropic drops its own (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/openai-launches-new-agentic-coding-model-only-minutes-after-anthropic-drops-its-own/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2224158119.jpg?resize=1200,837" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Monday, OpenAI launched Codex, an agentic coding tool marketed to software developers. Today, OpenAI also launched a new model designed to turbo-charge Codex: GPT-5.3 Codex.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says that the model transforms Codex&amp;nbsp;from an agent that can merely “write and review code” to one that can do “nearly anything developers and professionals do on a computer, expanding who can build software and how work gets done.” Having tested its new model against a number of performance benchmarks, OpenAI claims that it can create “highly functional complex games and apps from scratch over the course of days.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that GPT-5.3 Codex is also 25% faster than its previous model (GPT-5.2) and that it was the company’s first model that “was instrumental in creating itself,” meaning that the company’s staff used early versions of the program to debug itself and evaluate how it was performing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new model release notably follows hot on the heels of a new agentic coding model released by its competitor, Anthropic. Indeed, OpenAI and Anthropic had originally planned to release their two agentic coding tools at the exact same time: 10 a.m. PST. However, not long before the original release time, Anthropic moved its release date up by 15 minutes, slightly besting OpenAI in the race to publicize the models.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/GettyImages-2224158119.jpg?resize=1200,837" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Monday, OpenAI launched Codex, an agentic coding tool marketed to software developers. Today, OpenAI also launched a new model designed to turbo-charge Codex: GPT-5.3 Codex.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says that the model transforms Codex&amp;nbsp;from an agent that can merely “write and review code” to one that can do “nearly anything developers and professionals do on a computer, expanding who can build software and how work gets done.” Having tested its new model against a number of performance benchmarks, OpenAI claims that it can create “highly functional complex games and apps from scratch over the course of days.” &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI says that GPT-5.3 Codex is also 25% faster than its previous model (GPT-5.2) and that it was the company’s first model that “was instrumental in creating itself,” meaning that the company’s staff used early versions of the program to debug itself and evaluate how it was performing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new model release notably follows hot on the heels of a new agentic coding model released by its competitor, Anthropic. Indeed, OpenAI and Anthropic had originally planned to release their two agentic coding tools at the exact same time: 10 a.m. PST. However, not long before the original release time, Anthropic moved its release date up by 15 minutes, slightly besting OpenAI in the race to publicize the models.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/openai-launches-new-agentic-coding-model-only-minutes-after-anthropic-drops-its-own/</guid><pubDate>Thu, 05 Feb 2026 20:01:39 +0000</pubDate></item><item><title>Helping AI agents search to get the best results out of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2026/helping-ai-agents-search-to-get-best-results-from-llms-0205</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/encompass2-mit-00_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;Whether you’re a scientist brainstorming research ideas or a CEO hoping to automate a task in human resources or finance, you’ll find that artificial intelligence tools are becoming the assistants you didn’t know you needed. In particular,&amp;nbsp;many professionals are tapping into the talents of semi-autonomous software systems called AI agents, which can call on AI at specific points to solve problems and complete tasks.&lt;/p&gt;&lt;p&gt;AI agents are particularly effective when they use large language models (LLMs) because those systems are powerful, efficient, and adaptable. One way to program such technology is by describing in code what you want your system to do (the “workflow”), including when it should use an LLM. If you were a software company trying to revamp your old codebase to use a more modern programming language for better optimizations and safety, you might build a system that uses an LLM to translate the codebase one file at a time, testing each file as you go.&lt;/p&gt;&lt;p&gt;But what happens when LLMs make mistakes? You’ll want the agent to backtrack to make another attempt, incorporating lessons it learned from previous mistakes. Coding this up can take as much effort as implementing the original agent; if your system for translating a codebase contained thousands of lines of code, then you’d be making thousands of lines of code changes or additions to support the logic for backtracking when LLMs make mistakes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;To save programmers time and effort, researchers with MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Asari AI have developed a framework called “EnCompass.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With EnCompass, you no longer have to make these changes yourself. Instead, when EnCompass runs your program, it automatically backtracks if LLMs make mistakes. EnCompass can also make clones of the program runtime to make multiple attempts in parallel in search of the best solution. In full generality, EnCompass searches over the different possible paths your agent could take as a result of the different possible outputs of all the LLM calls, looking for the path where the LLM finds the best solution.&lt;/p&gt;&lt;p&gt;Then, all you have to do is to annotate the locations where you may want to backtrack or clone the program runtime, as well as record any information that may be useful to the strategy used to search over the different possible execution paths of your agent (the search strategy). You can then separately specify the search strategy — you could either use one that EnCompass provides out of the box or, if desired, implement your own custom search strategy.&lt;/p&gt;&lt;p dir="ltr"&gt;“With EnCompass, we’ve separated the search strategy from the underlying workflow of an AI agent,” says lead author Zhening Li ’25, MEng ’25, who is an MIT electrical engineering and computer science (EECS) PhD student, CSAIL researcher, and research consultant at Asari AI. “Our framework lets programmers easily experiment with different search strategies to find the one that makes the AI agent perform the best.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;EnCompass was used for agents implemented as Python programs that call LLMs, where it demonstrated noticeable code savings. EnCompass reduced coding effort for implementing search by up to 80 percent across agents, such as an agent for translating code repositories and for discovering transformation rules of digital grids. In the future, EnCompass could enable agents to tackle large-scale tasks, including managing massive code libraries, designing and carrying out science experiments, and creating blueprints for rockets and other hardware.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Branching out&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;When programming your agent, you mark particular operations — such as calls to an LLM — where results may vary. These annotations are called “branchpoints.” If you imagine your agent program as generating a single plot line of a story, then adding branchpoints turns the story into a choose-your-own-adventure story game, where branchpoints are locations where the plot branches into multiple future plot lines.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;You can then specify the strategy that EnCompass uses to navigate that story game, in search of the best possible ending to the story. This can include launching parallel threads of execution or backtracking to a previous branchpoint when you get stuck in a dead end.&lt;/p&gt;&lt;p&gt;Users can also plug-and-play a few common search strategies provided by EnCompass out of the box, or define their own custom strategy. For example, you could opt for Monte Carlo tree search, which builds a search tree by balancing exploration and exploitation, or beam search, which keeps the best few outputs from every step. EnCompass makes it easy to experiment with different approaches to find the best strategy to maximize the likelihood of successfully completing your task.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The coding efficiency of EnCompass&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;So just how code-efficient is EnCompass for adding search to agent programs? According to researchers’ findings, the framework drastically cut down how much programmers needed to add to their agent programs to add search, helping them experiment with different strategies to find the one that performs the best.&lt;/p&gt;&lt;p&gt;For example, the researchers applied EnCompass to an agent that translates a repository of code from the Java programming language, which is commonly used to program apps and enterprise software, to Python. They found that implementing search with EnCompass — mainly involving adding branchpoint annotations and annotations that record how well each step did — required 348 fewer lines of code (about 82 percent) than implementing it by hand. They also demonstrated how EnCompass enabled them to easily try out different search strategies, identifying the best strategy to be a two-level beam search algorithm, achieving an accuracy boost of 15 to 40 percent across five different repositories at a search budget of 16 times the LLM calls made by the agent without search.&lt;/p&gt;&lt;p dir="ltr"&gt;“As LLMs become a more integral part of everyday software, it becomes more important to understand how to efficiently build software that leverages their strengths and works around their limitations,” says co-author Armando Solar-Lezama, who is an MIT professor of EECS and CSAIL principal investigator. “EnCompass is an important step in that direction.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers add that EnCompass targets agents where a program specifies the steps of the high-level workflow; the current iteration of their framework is less applicable to agents that are entirely controlled by an LLM. “In those agents, instead of having a program that specifies the steps and then using an LLM to carry out those steps, the LLM itself decides everything,” says Li. “There is no underlying programmatic workflow, so you can execute inference-time search on whatever the LLM invents on the fly. In this case, there’s less need for a tool like EnCompass that modifies how a program executes with search and backtracking.”&lt;/p&gt;&lt;p dir="ltr"&gt;Li and his colleagues plan to extend EnCompass to more general search frameworks for AI agents. They also plan to test their system on more complex tasks to refine it for real-world uses, including at companies. What’s more, they’re evaluating how well EnCompass helps agents work with humans on tasks like brainstorming hardware designs or translating much larger code libraries. For now, EnCompass is a powerful building block that enables humans to tinker with AI agents more easily, improving their performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“EnCompass arrives at a timely moment, as AI-driven agents and search-based techniques are beginning to reshape workflows in software engineering,” says Carnegie Mellon University Professor Yiming Yang, who wasn’t involved in the research. “By cleanly separating an agent’s programming logic from its inference-time search strategy, the framework offers a principled way to explore how structured search can enhance code generation, translation, and analysis. This abstraction provides a solid foundation for more systematic and reliable search-driven approaches to software development.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Li and Solar-Lezama wrote the paper with two Asari AI researchers: Caltech Professor Yisong Yue, an advisor at the company; and senior author Stephan Zheng, who is the founder and CEO. Their work was supported by Asari AI.&lt;/p&gt;&lt;p&gt;The team’s work was presented at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202602/encompass2-mit-00_0.png" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;Whether you’re a scientist brainstorming research ideas or a CEO hoping to automate a task in human resources or finance, you’ll find that artificial intelligence tools are becoming the assistants you didn’t know you needed. In particular,&amp;nbsp;many professionals are tapping into the talents of semi-autonomous software systems called AI agents, which can call on AI at specific points to solve problems and complete tasks.&lt;/p&gt;&lt;p&gt;AI agents are particularly effective when they use large language models (LLMs) because those systems are powerful, efficient, and adaptable. One way to program such technology is by describing in code what you want your system to do (the “workflow”), including when it should use an LLM. If you were a software company trying to revamp your old codebase to use a more modern programming language for better optimizations and safety, you might build a system that uses an LLM to translate the codebase one file at a time, testing each file as you go.&lt;/p&gt;&lt;p&gt;But what happens when LLMs make mistakes? You’ll want the agent to backtrack to make another attempt, incorporating lessons it learned from previous mistakes. Coding this up can take as much effort as implementing the original agent; if your system for translating a codebase contained thousands of lines of code, then you’d be making thousands of lines of code changes or additions to support the logic for backtracking when LLMs make mistakes.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr" id="docs-internal-guid-e7f34161-7fff-b8ec-7efe-e9bf6135715c"&gt;To save programmers time and effort, researchers with MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and Asari AI have developed a framework called “EnCompass.”&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;With EnCompass, you no longer have to make these changes yourself. Instead, when EnCompass runs your program, it automatically backtracks if LLMs make mistakes. EnCompass can also make clones of the program runtime to make multiple attempts in parallel in search of the best solution. In full generality, EnCompass searches over the different possible paths your agent could take as a result of the different possible outputs of all the LLM calls, looking for the path where the LLM finds the best solution.&lt;/p&gt;&lt;p&gt;Then, all you have to do is to annotate the locations where you may want to backtrack or clone the program runtime, as well as record any information that may be useful to the strategy used to search over the different possible execution paths of your agent (the search strategy). You can then separately specify the search strategy — you could either use one that EnCompass provides out of the box or, if desired, implement your own custom search strategy.&lt;/p&gt;&lt;p dir="ltr"&gt;“With EnCompass, we’ve separated the search strategy from the underlying workflow of an AI agent,” says lead author Zhening Li ’25, MEng ’25, who is an MIT electrical engineering and computer science (EECS) PhD student, CSAIL researcher, and research consultant at Asari AI. “Our framework lets programmers easily experiment with different search strategies to find the one that makes the AI agent perform the best.”&amp;nbsp;&lt;/p&gt;&lt;p&gt;EnCompass was used for agents implemented as Python programs that call LLMs, where it demonstrated noticeable code savings. EnCompass reduced coding effort for implementing search by up to 80 percent across agents, such as an agent for translating code repositories and for discovering transformation rules of digital grids. In the future, EnCompass could enable agents to tackle large-scale tasks, including managing massive code libraries, designing and carrying out science experiments, and creating blueprints for rockets and other hardware.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;Branching out&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;When programming your agent, you mark particular operations — such as calls to an LLM — where results may vary. These annotations are called “branchpoints.” If you imagine your agent program as generating a single plot line of a story, then adding branchpoints turns the story into a choose-your-own-adventure story game, where branchpoints are locations where the plot branches into multiple future plot lines.&amp;nbsp;&lt;/p&gt;&lt;p dir="ltr"&gt;You can then specify the strategy that EnCompass uses to navigate that story game, in search of the best possible ending to the story. This can include launching parallel threads of execution or backtracking to a previous branchpoint when you get stuck in a dead end.&lt;/p&gt;&lt;p&gt;Users can also plug-and-play a few common search strategies provided by EnCompass out of the box, or define their own custom strategy. For example, you could opt for Monte Carlo tree search, which builds a search tree by balancing exploration and exploitation, or beam search, which keeps the best few outputs from every step. EnCompass makes it easy to experiment with different approaches to find the best strategy to maximize the likelihood of successfully completing your task.&lt;/p&gt;&lt;p dir="ltr"&gt;&lt;strong&gt;The coding efficiency of EnCompass&lt;/strong&gt;&lt;/p&gt;&lt;p dir="ltr"&gt;So just how code-efficient is EnCompass for adding search to agent programs? According to researchers’ findings, the framework drastically cut down how much programmers needed to add to their agent programs to add search, helping them experiment with different strategies to find the one that performs the best.&lt;/p&gt;&lt;p&gt;For example, the researchers applied EnCompass to an agent that translates a repository of code from the Java programming language, which is commonly used to program apps and enterprise software, to Python. They found that implementing search with EnCompass — mainly involving adding branchpoint annotations and annotations that record how well each step did — required 348 fewer lines of code (about 82 percent) than implementing it by hand. They also demonstrated how EnCompass enabled them to easily try out different search strategies, identifying the best strategy to be a two-level beam search algorithm, achieving an accuracy boost of 15 to 40 percent across five different repositories at a search budget of 16 times the LLM calls made by the agent without search.&lt;/p&gt;&lt;p dir="ltr"&gt;“As LLMs become a more integral part of everyday software, it becomes more important to understand how to efficiently build software that leverages their strengths and works around their limitations,” says co-author Armando Solar-Lezama, who is an MIT professor of EECS and CSAIL principal investigator. “EnCompass is an important step in that direction.”&lt;/p&gt;&lt;p dir="ltr"&gt;The researchers add that EnCompass targets agents where a program specifies the steps of the high-level workflow; the current iteration of their framework is less applicable to agents that are entirely controlled by an LLM. “In those agents, instead of having a program that specifies the steps and then using an LLM to carry out those steps, the LLM itself decides everything,” says Li. “There is no underlying programmatic workflow, so you can execute inference-time search on whatever the LLM invents on the fly. In this case, there’s less need for a tool like EnCompass that modifies how a program executes with search and backtracking.”&lt;/p&gt;&lt;p dir="ltr"&gt;Li and his colleagues plan to extend EnCompass to more general search frameworks for AI agents. They also plan to test their system on more complex tasks to refine it for real-world uses, including at companies. What’s more, they’re evaluating how well EnCompass helps agents work with humans on tasks like brainstorming hardware designs or translating much larger code libraries. For now, EnCompass is a powerful building block that enables humans to tinker with AI agents more easily, improving their performance.&lt;/p&gt;&lt;p dir="ltr"&gt;“EnCompass arrives at a timely moment, as AI-driven agents and search-based techniques are beginning to reshape workflows in software engineering,” says Carnegie Mellon University Professor Yiming Yang, who wasn’t involved in the research. “By cleanly separating an agent’s programming logic from its inference-time search strategy, the framework offers a principled way to explore how structured search can enhance code generation, translation, and analysis. This abstraction provides a solid foundation for more systematic and reliable search-driven approaches to software development.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Li and Solar-Lezama wrote the paper with two Asari AI researchers: Caltech Professor Yisong Yue, an advisor at the company; and senior author Stephan Zheng, who is the founder and CEO. Their work was supported by Asari AI.&lt;/p&gt;&lt;p&gt;The team’s work was presented at the Conference on Neural Information Processing Systems (NeurIPS) in December.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2026/helping-ai-agents-search-to-get-best-results-from-llms-0205</guid><pubDate>Thu, 05 Feb 2026 21:30:00 +0000</pubDate></item><item><title>With GPT-5.3-Codex, OpenAI pitches Codex for more than just writing code (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/02/with-gpt-5-3-codex-openai-pitches-codex-for-more-than-just-writing-code/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The emphasis is on “mid-turn steering and frequent progress updates.”
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-1152x648-1770052639.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Codex macOS app.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Today, OpenAI announced GPT-5.3-Codex, a new version of its frontier coding model that will be available via the command line, IDE extension, web interface, and the new macOS desktop app. (No API access yet, but it’s coming.)&lt;/p&gt;
&lt;p&gt;GPT-5.3-Codex outperforms GPT-5.2-Codex and GPT-5.2 in SWE-Bench Pro, Terminal-Bench 2.0, and other benchmarks, according to the company’s testing.&lt;/p&gt;
&lt;p&gt;There are already a few headlines out there saying “Codex built itself,” but let’s reality-check that, as that’s an overstatement. The domains OpenAI described using it for here are similar to the ones you see in some other enterprise software development firms now: managing deployments, debugging, and handling test results and evaluations. There is no claim here that GPT-5.3-Codex built itself.&lt;/p&gt;
&lt;p&gt;Instead, OpenAI says GPT-5.3-Codex was “instrumental in creating itself.” You can read more about what that means in the company’s blog post.&lt;/p&gt;
&lt;p&gt;But that’s part of the pitch with this model update—OpenAI is trying to position Codex as a tool that does more than generate lines of code. The goal is to make it useful for “all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more.” There’s also an emphasis on steering the model mid-task and frequent status updates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s worth mentioning that the general-purpose ChatGPT model is still version numbered 5.2. With Codex moving to 5.3, OpenAI may be planning a similar update for ChatGPT in the near future, but nothing has been announced yet.&lt;/p&gt;
&lt;p&gt;There are no announced changes to limits or pricing with this model, but OpenAI says that, as of this update, models will run 25 percent faster for Codex users, “thanks to improvements in our infrastructure and inference stack.”&lt;/p&gt;
&lt;p&gt;OpenAI also notes that “what’s next” for Codex is “moving beyond writing code to using it as a tool to operate a computer and get real work done end to end.” That’s something that people have already been using these tools for via MCP and other methods, and which Anthropic started rolling out with Claude Cowork a few weeks back.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        The emphasis is on “mid-turn steering and frequent progress updates.”
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-640x360.jpg" width="640" /&gt;
                  &lt;img alt="A screenshot of a simple panel, with conversations listed on the left" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/Codex-dark-1152x648-1770052639.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Codex macOS app.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          OpenAI

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Today, OpenAI announced GPT-5.3-Codex, a new version of its frontier coding model that will be available via the command line, IDE extension, web interface, and the new macOS desktop app. (No API access yet, but it’s coming.)&lt;/p&gt;
&lt;p&gt;GPT-5.3-Codex outperforms GPT-5.2-Codex and GPT-5.2 in SWE-Bench Pro, Terminal-Bench 2.0, and other benchmarks, according to the company’s testing.&lt;/p&gt;
&lt;p&gt;There are already a few headlines out there saying “Codex built itself,” but let’s reality-check that, as that’s an overstatement. The domains OpenAI described using it for here are similar to the ones you see in some other enterprise software development firms now: managing deployments, debugging, and handling test results and evaluations. There is no claim here that GPT-5.3-Codex built itself.&lt;/p&gt;
&lt;p&gt;Instead, OpenAI says GPT-5.3-Codex was “instrumental in creating itself.” You can read more about what that means in the company’s blog post.&lt;/p&gt;
&lt;p&gt;But that’s part of the pitch with this model update—OpenAI is trying to position Codex as a tool that does more than generate lines of code. The goal is to make it useful for “all of the work in the software lifecycle—debugging, deploying, monitoring, writing PRDs, editing copy, user research, tests, metrics, and more.” There’s also an emphasis on steering the model mid-task and frequent status updates.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;It’s worth mentioning that the general-purpose ChatGPT model is still version numbered 5.2. With Codex moving to 5.3, OpenAI may be planning a similar update for ChatGPT in the near future, but nothing has been announced yet.&lt;/p&gt;
&lt;p&gt;There are no announced changes to limits or pricing with this model, but OpenAI says that, as of this update, models will run 25 percent faster for Codex users, “thanks to improvements in our infrastructure and inference stack.”&lt;/p&gt;
&lt;p&gt;OpenAI also notes that “what’s next” for Codex is “moving beyond writing code to using it as a tool to operate a computer and get real work done end to end.” That’s something that people have already been using these tools for via MCP and other methods, and which Anthropic started rolling out with Claude Cowork a few weeks back.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/02/with-gpt-5-3-codex-openai-pitches-codex-for-more-than-just-writing-code/</guid><pubDate>Thu, 05 Feb 2026 21:47:06 +0000</pubDate></item><item><title>Amazon and Google are winning the AI capex race — but what’s the prize? (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/amazon-and-google-are-winning-the-ai-capex-race-but-whats-the-prize/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2215577882.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sometimes, it can seem like the AI industry is racing to see who can spend the most money on data centers. Whoever builds the most data centers will have the most compute, the thinking goes, and thus be able to build the best AI products, which will guarantee victory in the years to come. There are limits to this way of thinking — traditionally, businesses eventually succeed by making &lt;em&gt;more&lt;/em&gt; money and spending &lt;em&gt;less&lt;/em&gt; — but it’s proven remarkably persuasive for large tech companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that is the game, Amazon does seem to be winning. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company announced in its earnings on Thursday that it projects $200 billion in capital expenditures throughout 2026, across “AI, chips, robotics, and low earth orbit satellites.” That’s up from the $131.8 billion in capex in 2025. It’s tempting to attribute the whole capex budget to AI. But unlike most of its competitors, Amazon has a significant physical plant, some of which is being converted for use by expensive robots, so the non-AI expenses aren’t so easy to wave away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is close behind. In its earnings on Wednesday, the company projected between $175 billion and $185 billion in capital expenditures for 2026, up from $91.4 billion the previous year. It’s significantly more than the company spent on fixed assets last year, and significantly more than most of its competitors are spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta, which reported last week, projected $115 billion to $135 billion in capex spending for 2026, while Oracle (once the poster child for AI infrastructure) projects a measly $50 billion. Microsoft doesn’t have an official projection for 2026 yet, but the most recent quarterly figure was $37.5 billion, which pencils out to roughly $150 billion, assuming it keeps up. It’s a notable increase, and one that has led to investor pressure on CEO Satya Nadella — but it still puts the company in third place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From within the tech world, the logic here is simple. The revolutionary potential of AI is going to turn high-end compute into the scarce resource of the future, and only companies that control their own supply will survive. But while Google, Amazon, Microsoft, Meta, Oracle, and others are frantically prepping for the compute desert of the future, their investors aren’t convinced. Each company saw its stock price plummet as investors balked at the hundreds of billions of dollars being committed, and companies with higher spends tended to drop more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucially, this isn’t just a problem for companies like Meta that haven’t figured out their AI product strategy yet. It’s everyone — even companies like Microsoft and Amazon with a robust cloud business and a straightforward take on how to make money in the AI era. The numbers are simply too high for investor comfort.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Investor sentiment isn’t everything — and in this case, it may not do much to change the industry’s mind. If you believe AI is about to change everything (and the argument is pretty compelling at this point), you’d be a fool to change course just because Wall Street got jumpy. But going forward, Big Tech companies will be under a lot of pressure to downplay how expensive their AI ambitions really are.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/GettyImages-2215577882.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sometimes, it can seem like the AI industry is racing to see who can spend the most money on data centers. Whoever builds the most data centers will have the most compute, the thinking goes, and thus be able to build the best AI products, which will guarantee victory in the years to come. There are limits to this way of thinking — traditionally, businesses eventually succeed by making &lt;em&gt;more&lt;/em&gt; money and spending &lt;em&gt;less&lt;/em&gt; — but it’s proven remarkably persuasive for large tech companies.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If that is the game, Amazon does seem to be winning. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company announced in its earnings on Thursday that it projects $200 billion in capital expenditures throughout 2026, across “AI, chips, robotics, and low earth orbit satellites.” That’s up from the $131.8 billion in capex in 2025. It’s tempting to attribute the whole capex budget to AI. But unlike most of its competitors, Amazon has a significant physical plant, some of which is being converted for use by expensive robots, so the non-AI expenses aren’t so easy to wave away.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is close behind. In its earnings on Wednesday, the company projected between $175 billion and $185 billion in capital expenditures for 2026, up from $91.4 billion the previous year. It’s significantly more than the company spent on fixed assets last year, and significantly more than most of its competitors are spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta, which reported last week, projected $115 billion to $135 billion in capex spending for 2026, while Oracle (once the poster child for AI infrastructure) projects a measly $50 billion. Microsoft doesn’t have an official projection for 2026 yet, but the most recent quarterly figure was $37.5 billion, which pencils out to roughly $150 billion, assuming it keeps up. It’s a notable increase, and one that has led to investor pressure on CEO Satya Nadella — but it still puts the company in third place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From within the tech world, the logic here is simple. The revolutionary potential of AI is going to turn high-end compute into the scarce resource of the future, and only companies that control their own supply will survive. But while Google, Amazon, Microsoft, Meta, Oracle, and others are frantically prepping for the compute desert of the future, their investors aren’t convinced. Each company saw its stock price plummet as investors balked at the hundreds of billions of dollars being committed, and companies with higher spends tended to drop more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Crucially, this isn’t just a problem for companies like Meta that haven’t figured out their AI product strategy yet. It’s everyone — even companies like Microsoft and Amazon with a robust cloud business and a straightforward take on how to make money in the AI era. The numbers are simply too high for investor comfort.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Investor sentiment isn’t everything — and in this case, it may not do much to change the industry’s mind. If you believe AI is about to change everything (and the argument is pretty compelling at this point), you’d be a fool to change course just because Wall Street got jumpy. But going forward, Big Tech companies will be under a lot of pressure to downplay how expensive their AI ambitions really are.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/amazon-and-google-are-winning-the-ai-capex-race-but-whats-the-prize/</guid><pubDate>Thu, 05 Feb 2026 22:43:11 +0000</pubDate></item><item><title>AI companies want you to stop chatting with bots and start managing them (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/02/ai-companies-want-you-to-stop-chatting-with-bots-and-start-managing-them/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Claude Opus 4.6 and OpenAI Frontier pitch a future of supervising AI agents.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Business people supervising a robot work" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Business people supervising a robot work" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          demaerre via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Thursday, Anthropic and OpenAI shipped products built around the same idea: instead of chatting with a single AI assistant, users should be managing teams of AI agents that divide up work and run in parallel. The simultaneous releases are part of a gradual shift across the industry, from AI as a conversation partner to AI as a delegated workforce, and they arrive during a week when that very concept reportedly helped wipe $285 billion off software stocks.&lt;/p&gt;
&lt;p&gt;Whether that supervisory model works in practice remains an open question. Current AI agents still require heavy human intervention to catch errors, and no independent evaluation has confirmed that these multi-agent tools reliably outperform a single developer working alone.&lt;/p&gt;
&lt;p&gt;Even so, the companies are going all-in on agents. Anthropic’s contribution is Claude Opus 4.6, a new version of its most capable AI model, paired with a feature called “agent teams” in Claude Code. Agent teams let developers spin up multiple AI agents that split a task into independent pieces, coordinate autonomously, and run concurrently.&lt;/p&gt;
&lt;p&gt;In practice, agent teams look like a split-screen terminal environment: A developer can jump between subagents using Shift+Up/Down, take over any one directly, and watch the others keep working. Anthropic describes the feature as best suited for “tasks that split into independent, read-heavy work like codebase reviews.” It is available as a research preview.&lt;/p&gt;
&lt;p&gt;OpenAI, meanwhile, released Frontier, an enterprise platform it describes as a way to “hire AI co-workers who take on many of the tasks people already do on a computer.” Frontier assigns each AI agent its own identity, permissions, and memory, and it connects to existing business systems such as CRMs, ticketing tools, and data warehouses. “What we’re fundamentally doing is basically transitioning agents into true AI co-workers,” Barret Zoph, OpenAI’s general manager of business-to-business, told CNBC.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Despite the hype about these agents being co-workers, from our experience, these agents tend to work best if you think of them as tools that amplify existing skills, not as the autonomous co-workers the marketing language implies. They can produce impressive drafts fast but still require constant human course-correction.&lt;/p&gt;
&lt;p&gt;The Frontier launch came just three days after OpenAI released a new macOS desktop app for Codex, its AI coding tool, which OpenAI executives described as a “command center for agents.” The Codex app lets developers run multiple agent threads in parallel, each working on an isolated copy of a codebase via Git worktrees.&lt;/p&gt;
&lt;p&gt;OpenAI also released GPT-5.3-Codex on Thursday, a new AI model that powers the Codex app. OpenAI claims that the Codex team used early versions of GPT-5.3-Codex to debug the model’s own training run, manage its deployment, and diagnose test results, similar to what OpenAI told Ars Technica in a December interview.&lt;/p&gt;
&lt;p&gt;“Our team was blown away by how much Codex was able to accelerate its own development,” the company wrote. On Terminal-Bench 2.0, the agentic coding benchmark, GPT-5.3-Codex scored 77.3%, which exceeds Anthropic’s just-released Opus 4.6 by about 12 percentage points.&lt;/p&gt;
&lt;p&gt;The common thread across all of these products is a shift in the user’s role. Rather than merely typing a prompt and waiting for a single response, the developer or knowledge worker becomes more like a supervisor, dispatching tasks, monitoring progress, and stepping in when an agent needs direction.&lt;/p&gt;
&lt;p&gt;In this vision, developers and knowledge workers effectively become middle managers of AI. That is, not writing the code or doing the analysis themselves, but delegating tasks, reviewing output, and hoping the agents underneath them don’t quietly break things. Whether that will come to pass (or if it’s actually a good idea) is still widely debated.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;A new model under the Claude hood&lt;/h2&gt;
&lt;p&gt;Opus 4.6 is a substantial update to Anthropic’s flagship model. It succeeds Claude Opus 4.5, which Anthropic released in November. In a first for the Opus model family, it supports a context window of up to 1 million tokens (in beta), which means it can process much larger bodies of text or code in a single session.&lt;/p&gt;
&lt;p&gt;On benchmarks, Anthropic says Opus 4.6 tops OpenAI’s GPT-5.2 (an earlier model than the one released today) and Google’s Gemini 3 Pro across several evaluations, including Terminal-Bench 2.0 (an agentic coding test), Humanity’s Last Exam (a multidisciplinary reasoning test), and BrowseComp (a test of finding hard-to-locate information online)&lt;/p&gt;
&lt;p&gt;Although it should be noted that OpenAI’s GPT-5.3-Codex, released the same day, seemingly reclaimed the lead on Terminal-Bench. On ARC AGI 2, which attempts to test the ability to solve problems that are easy for humans but hard for AI models, Opus 4.6 scored 68.8 percent, compared to 37.6 percent for Opus 4.5, 54.2 percent for GPT-5.2, and 45.1 percent for Gemini 3 Pro.&lt;/p&gt;
&lt;p&gt;As always, take AI benchmarks with a grain of salt, since objectively measuring AI model capabilities is a relatively new and unsettled science.&lt;/p&gt;
&lt;p&gt;Anthropic also said that on a long-context retrieval benchmark called MRCR v2, Opus 4.6 scored 76 percent on the 1 million-token variant, compared to 18.5 percent for its Sonnet 4.5 model. That gap matters for the agent teams use case, since agents working across large codebases need to track information across hundreds of thousands of tokens without losing the thread.&lt;/p&gt;
&lt;p&gt;Pricing for the API stays the same as Opus 4.5 at $5 per million input tokens and $25 per million output tokens, with a premium rate of $10/$37.50 for prompts that exceed 200,000 tokens. Opus 4.6 is available on claude.ai, the Claude API, and all major cloud platforms.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The market fallout outside&lt;/h2&gt;
&lt;p&gt;These releases occurred during a week of exceptional volatility for software stocks. On January 30, Anthropic released 11 open source plugins for Cowork, its agentic productivity tool that launched on January 12. Cowork itself is a general-purpose tool that gives Claude access to local folders for work tasks, but the plugins extended it into specific professional domains: legal contract review, non-disclosure agreement triage, compliance workflows, financial analysis, sales, and marketing.&lt;/p&gt;
&lt;p&gt;By Tuesday, investors reportedly reacted to the release by erasing roughly $285 billion in market value across software, financial services, and asset management stocks. A Goldman Sachs basket of US software stocks fell 6 percent that day, its steepest single-session decline since April’s tariff-driven sell-off. Thomson Reuters led the rout with an 18 percent drop, and the pain spread to European and Asian markets.&lt;/p&gt;
&lt;p&gt;The purported fear among investors centers on AI model companies packaging complete workflows that compete with established software-as-a-service (SaaS) vendors, even if the verdict is still out on whether these tools can achieve those tasks.&lt;/p&gt;
&lt;p&gt;OpenAI’s Frontier might deepen that concern: its stated design lets AI agents log in to applications, execute tasks, and manage work with minimal human involvement, which Fortune described as a bid to become “the operating system of the enterprise.” OpenAI CEO of Applications Fidji Simo pushed back on the idea that Frontier replaces existing software, telling reporters, “Frontier is really a recognition that we’re not going to build everything ourselves.”&lt;/p&gt;
&lt;p&gt;Whether these co-working apps actually live up to their billing or not, the convergence is hard to miss. Anthropic’s Scott White, the company’s head of product for enterprise, gave the practice a name that is likely to roll a few eyes. “Everybody has seen this transformation happen with software engineering in the last year and a half, where vibe coding started to exist as a concept, and people could now do things with their ideas,” White told CNBC. “I think that we are now transitioning almost into vibe working.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Claude Opus 4.6 and OpenAI Frontier pitch a future of supervising AI agents.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Business people supervising a robot work" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Business people supervising a robot work" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/05/lazy_workers-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          demaerre via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;On Thursday, Anthropic and OpenAI shipped products built around the same idea: instead of chatting with a single AI assistant, users should be managing teams of AI agents that divide up work and run in parallel. The simultaneous releases are part of a gradual shift across the industry, from AI as a conversation partner to AI as a delegated workforce, and they arrive during a week when that very concept reportedly helped wipe $285 billion off software stocks.&lt;/p&gt;
&lt;p&gt;Whether that supervisory model works in practice remains an open question. Current AI agents still require heavy human intervention to catch errors, and no independent evaluation has confirmed that these multi-agent tools reliably outperform a single developer working alone.&lt;/p&gt;
&lt;p&gt;Even so, the companies are going all-in on agents. Anthropic’s contribution is Claude Opus 4.6, a new version of its most capable AI model, paired with a feature called “agent teams” in Claude Code. Agent teams let developers spin up multiple AI agents that split a task into independent pieces, coordinate autonomously, and run concurrently.&lt;/p&gt;
&lt;p&gt;In practice, agent teams look like a split-screen terminal environment: A developer can jump between subagents using Shift+Up/Down, take over any one directly, and watch the others keep working. Anthropic describes the feature as best suited for “tasks that split into independent, read-heavy work like codebase reviews.” It is available as a research preview.&lt;/p&gt;
&lt;p&gt;OpenAI, meanwhile, released Frontier, an enterprise platform it describes as a way to “hire AI co-workers who take on many of the tasks people already do on a computer.” Frontier assigns each AI agent its own identity, permissions, and memory, and it connects to existing business systems such as CRMs, ticketing tools, and data warehouses. “What we’re fundamentally doing is basically transitioning agents into true AI co-workers,” Barret Zoph, OpenAI’s general manager of business-to-business, told CNBC.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Despite the hype about these agents being co-workers, from our experience, these agents tend to work best if you think of them as tools that amplify existing skills, not as the autonomous co-workers the marketing language implies. They can produce impressive drafts fast but still require constant human course-correction.&lt;/p&gt;
&lt;p&gt;The Frontier launch came just three days after OpenAI released a new macOS desktop app for Codex, its AI coding tool, which OpenAI executives described as a “command center for agents.” The Codex app lets developers run multiple agent threads in parallel, each working on an isolated copy of a codebase via Git worktrees.&lt;/p&gt;
&lt;p&gt;OpenAI also released GPT-5.3-Codex on Thursday, a new AI model that powers the Codex app. OpenAI claims that the Codex team used early versions of GPT-5.3-Codex to debug the model’s own training run, manage its deployment, and diagnose test results, similar to what OpenAI told Ars Technica in a December interview.&lt;/p&gt;
&lt;p&gt;“Our team was blown away by how much Codex was able to accelerate its own development,” the company wrote. On Terminal-Bench 2.0, the agentic coding benchmark, GPT-5.3-Codex scored 77.3%, which exceeds Anthropic’s just-released Opus 4.6 by about 12 percentage points.&lt;/p&gt;
&lt;p&gt;The common thread across all of these products is a shift in the user’s role. Rather than merely typing a prompt and waiting for a single response, the developer or knowledge worker becomes more like a supervisor, dispatching tasks, monitoring progress, and stepping in when an agent needs direction.&lt;/p&gt;
&lt;p&gt;In this vision, developers and knowledge workers effectively become middle managers of AI. That is, not writing the code or doing the analysis themselves, but delegating tasks, reviewing output, and hoping the agents underneath them don’t quietly break things. Whether that will come to pass (or if it’s actually a good idea) is still widely debated.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;A new model under the Claude hood&lt;/h2&gt;
&lt;p&gt;Opus 4.6 is a substantial update to Anthropic’s flagship model. It succeeds Claude Opus 4.5, which Anthropic released in November. In a first for the Opus model family, it supports a context window of up to 1 million tokens (in beta), which means it can process much larger bodies of text or code in a single session.&lt;/p&gt;
&lt;p&gt;On benchmarks, Anthropic says Opus 4.6 tops OpenAI’s GPT-5.2 (an earlier model than the one released today) and Google’s Gemini 3 Pro across several evaluations, including Terminal-Bench 2.0 (an agentic coding test), Humanity’s Last Exam (a multidisciplinary reasoning test), and BrowseComp (a test of finding hard-to-locate information online)&lt;/p&gt;
&lt;p&gt;Although it should be noted that OpenAI’s GPT-5.3-Codex, released the same day, seemingly reclaimed the lead on Terminal-Bench. On ARC AGI 2, which attempts to test the ability to solve problems that are easy for humans but hard for AI models, Opus 4.6 scored 68.8 percent, compared to 37.6 percent for Opus 4.5, 54.2 percent for GPT-5.2, and 45.1 percent for Gemini 3 Pro.&lt;/p&gt;
&lt;p&gt;As always, take AI benchmarks with a grain of salt, since objectively measuring AI model capabilities is a relatively new and unsettled science.&lt;/p&gt;
&lt;p&gt;Anthropic also said that on a long-context retrieval benchmark called MRCR v2, Opus 4.6 scored 76 percent on the 1 million-token variant, compared to 18.5 percent for its Sonnet 4.5 model. That gap matters for the agent teams use case, since agents working across large codebases need to track information across hundreds of thousands of tokens without losing the thread.&lt;/p&gt;
&lt;p&gt;Pricing for the API stays the same as Opus 4.5 at $5 per million input tokens and $25 per million output tokens, with a premium rate of $10/$37.50 for prompts that exceed 200,000 tokens. Opus 4.6 is available on claude.ai, the Claude API, and all major cloud platforms.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The market fallout outside&lt;/h2&gt;
&lt;p&gt;These releases occurred during a week of exceptional volatility for software stocks. On January 30, Anthropic released 11 open source plugins for Cowork, its agentic productivity tool that launched on January 12. Cowork itself is a general-purpose tool that gives Claude access to local folders for work tasks, but the plugins extended it into specific professional domains: legal contract review, non-disclosure agreement triage, compliance workflows, financial analysis, sales, and marketing.&lt;/p&gt;
&lt;p&gt;By Tuesday, investors reportedly reacted to the release by erasing roughly $285 billion in market value across software, financial services, and asset management stocks. A Goldman Sachs basket of US software stocks fell 6 percent that day, its steepest single-session decline since April’s tariff-driven sell-off. Thomson Reuters led the rout with an 18 percent drop, and the pain spread to European and Asian markets.&lt;/p&gt;
&lt;p&gt;The purported fear among investors centers on AI model companies packaging complete workflows that compete with established software-as-a-service (SaaS) vendors, even if the verdict is still out on whether these tools can achieve those tasks.&lt;/p&gt;
&lt;p&gt;OpenAI’s Frontier might deepen that concern: its stated design lets AI agents log in to applications, execute tasks, and manage work with minimal human involvement, which Fortune described as a bid to become “the operating system of the enterprise.” OpenAI CEO of Applications Fidji Simo pushed back on the idea that Frontier replaces existing software, telling reporters, “Frontier is really a recognition that we’re not going to build everything ourselves.”&lt;/p&gt;
&lt;p&gt;Whether these co-working apps actually live up to their billing or not, the convergence is hard to miss. Anthropic’s Scott White, the company’s head of product for enterprise, gave the practice a name that is likely to roll a few eyes. “Everybody has seen this transformation happen with software engineering in the last year and a half, where vibe coding started to exist as a concept, and people could now do things with their ideas,” White told CNBC. “I think that we are now transitioning almost into vibe working.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/02/ai-companies-want-you-to-stop-chatting-with-bots-and-start-managing-them/</guid><pubDate>Thu, 05 Feb 2026 22:47:54 +0000</pubDate></item><item><title>AWS revenue continues to soar as cloud demand remains high (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/aws-revenue-continues-to-soar-as-cloud-demand-remains-high/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/IMG_4752.jpg?resize=1200,797" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services ended 2025 with its strongest quarterly growth rate in more than three years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company reported Thursday that its cloud service business recorded $35.6 billion in revenue in the fourth quarter of 2025. This figure marks a 24% year-on-year increase and the business segment’s largest growth rate in 13 quarters. Annual revenue run rate for the business segment is $142 billion, according to Amazon. The cloud service also saw an increase in its operating income from $12.5 billion in the fourth quarter compared to $10.6 billion in the same period in 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s very different having 24% year-over-year growth on $142 billion annualized run rate than to have a higher percentage growth on a meaningfully smaller base, which is the case with our competitors,” Amazon CEO Andy Jassy said during the company’s fourth-quarter earnings call. “We continue to add more incremental revenue and capacity than others, and extend our leadership position.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That fourth-quarter growth was fueled by new agreements with Salesforce, BlackRock, Perplexity, and the U.S. Air Force, among other companies and government entities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“More of the top 500 U.S. startups use AWS as their primary cloud provider than the next two providers combined,” Jassy said. “We’re adding significant easy to core computing capacity each day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also added more than a gigawatt of power to its data center network in the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said AWS still sees a fair amount of its business coming from enterprises that want to move infrastructure from on-premise to the cloud. AWS is, of course, also seeing a boost from the AI boom, and Jassy credited AWS’s top-to-bottom AI stack functionality.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We consistently see customers wanting to run their AI workloads where the rest of their applications and data are,” Jassy said. “We’re also seeing that as customers run large AI workloads on AWS, they’re adding to their core AWS footprint as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS made up 16.6% of Amazon’s overall $213.4 billion revenue in the fourth quarter. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS’s success wasn’t enough to appease Amazon investors, however. Amazon shares fell 10% in after-hours trading after investors reacted to the company’s plan to boost capital expenditures and missed Wall Street’s expectations on earnings per share.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/IMG_4752.jpg?resize=1200,797" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon Web Services ended 2025 with its strongest quarterly growth rate in more than three years.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company reported Thursday that its cloud service business recorded $35.6 billion in revenue in the fourth quarter of 2025. This figure marks a 24% year-on-year increase and the business segment’s largest growth rate in 13 quarters. Annual revenue run rate for the business segment is $142 billion, according to Amazon. The cloud service also saw an increase in its operating income from $12.5 billion in the fourth quarter compared to $10.6 billion in the same period in 2024.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“It’s very different having 24% year-over-year growth on $142 billion annualized run rate than to have a higher percentage growth on a meaningfully smaller base, which is the case with our competitors,” Amazon CEO Andy Jassy said during the company’s fourth-quarter earnings call. “We continue to add more incremental revenue and capacity than others, and extend our leadership position.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That fourth-quarter growth was fueled by new agreements with Salesforce, BlackRock, Perplexity, and the U.S. Air Force, among other companies and government entities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“More of the top 500 U.S. startups use AWS as their primary cloud provider than the next two providers combined,” Jassy said. “We’re adding significant easy to core computing capacity each day.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS also added more than a gigawatt of power to its data center network in the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said AWS still sees a fair amount of its business coming from enterprises that want to move infrastructure from on-premise to the cloud. AWS is, of course, also seeing a boost from the AI boom, and Jassy credited AWS’s top-to-bottom AI stack functionality.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We consistently see customers wanting to run their AI workloads where the rest of their applications and data are,” Jassy said. “We’re also seeing that as customers run large AI workloads on AWS, they’re adding to their core AWS footprint as well.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS made up 16.6% of Amazon’s overall $213.4 billion revenue in the fourth quarter. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AWS’s success wasn’t enough to appease Amazon investors, however. Amazon shares fell 10% in after-hours trading after investors reacted to the company’s plan to boost capital expenditures and missed Wall Street’s expectations on earnings per share.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/aws-revenue-continues-to-soar-as-cloud-demand-remains-high/</guid><pubDate>Thu, 05 Feb 2026 23:11:37 +0000</pubDate></item><item><title>Reddit looks to AI search as its next big opportunity (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/reddit-looks-to-ai-search-as-its-next-big-opportunity/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/reddit-ipo-v2.webp?resize=1200,674" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reddit suggested on Thursday that its AI-powered search engine could be the next big opportunity for its business — not just in terms of product, but also as a revenue driver impacting its bottom line. During the company’s fourth-quarter earnings call on Thursday, it offered an update on its plans to merge traditional and AI search together and hinted that although search is not yet monetized, “it’s an enormous market and opportunity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In particular, the company believes that generative AI search will be “better for most queries.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There’s a type of query we’re, I think, particularly good at — I would argue, the best on the internet — which is questions that have no answers, where the answer actually is multiple perspectives from lots of people,” said Reddit CEO Steve Huffman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Traditional search, meanwhile, is more like navigation — it’s a way to find the right link to a topic or subreddit. But LLMs can be good at this, too, if not better, he said. “So that’s the direction we’re going.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The exec also noted that weekly active users for search over the past year grew 30% from 60 million users to 80 million users. Meanwhile, the weekly active users for the AI-powered Reddit Answers grew from 1 million in the first quarter of 2025 to 15 million by the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re seeing a lot of growth there, and I think there’s a lot of potential too,” Huffman added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reddit said it’s working to modernize the AI answers interface by making its responses more media-rich, and pilots of this are already underway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also thinking about how it can position itself when it’s not just a social site, but a place people come for answers. Reddit told investors on the call that it’s doing away with the distinction between logged-in and logged-out users starting in Q3 2026, as it will aim to personalize the site — using AI and machine learning — and make it relevant to whoever shows up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced in 2025 it was planning to combine its AI search feature, Reddit Answers, with its traditional search engine to improve the experience for end users. In the fourth quarter, Reddit said it had made “significant progress” in unifying its core search and its AI feature. It also released five new languages on Reddit Answers and is piloting dynamic agents along with search results that include “media beyond text.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Reddit sees value in its AI answers, it’s not been keeping that to itself. The company’s content licensing business, which allows other companies to train their AI models on its data, is growing, too. That business revenue is reported as part of Reddit’s “other” revenues (i.e., its non-ad revenue). This “other” revenue increased by 8% year-over-year to reach $36 million in Q4 and was up 22% to reach $140 million for 2025.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/05/reddit-ipo-v2.webp?resize=1200,674" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Reddit suggested on Thursday that its AI-powered search engine could be the next big opportunity for its business — not just in terms of product, but also as a revenue driver impacting its bottom line. During the company’s fourth-quarter earnings call on Thursday, it offered an update on its plans to merge traditional and AI search together and hinted that although search is not yet monetized, “it’s an enormous market and opportunity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In particular, the company believes that generative AI search will be “better for most queries.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“There’s a type of query we’re, I think, particularly good at — I would argue, the best on the internet — which is questions that have no answers, where the answer actually is multiple perspectives from lots of people,” said Reddit CEO Steve Huffman.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Traditional search, meanwhile, is more like navigation — it’s a way to find the right link to a topic or subreddit. But LLMs can be good at this, too, if not better, he said. “So that’s the direction we’re going.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The exec also noted that weekly active users for search over the past year grew 30% from 60 million users to 80 million users. Meanwhile, the weekly active users for the AI-powered Reddit Answers grew from 1 million in the first quarter of 2025 to 15 million by the fourth quarter.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re seeing a lot of growth there, and I think there’s a lot of potential too,” Huffman added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Reddit said it’s working to modernize the AI answers interface by making its responses more media-rich, and pilots of this are already underway.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is also thinking about how it can position itself when it’s not just a social site, but a place people come for answers. Reddit told investors on the call that it’s doing away with the distinction between logged-in and logged-out users starting in Q3 2026, as it will aim to personalize the site — using AI and machine learning — and make it relevant to whoever shows up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company announced in 2025 it was planning to combine its AI search feature, Reddit Answers, with its traditional search engine to improve the experience for end users. In the fourth quarter, Reddit said it had made “significant progress” in unifying its core search and its AI feature. It also released five new languages on Reddit Answers and is piloting dynamic agents along with search results that include “media beyond text.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Reddit sees value in its AI answers, it’s not been keeping that to itself. The company’s content licensing business, which allows other companies to train their AI models on its data, is growing, too. That business revenue is reported as part of Reddit’s “other” revenues (i.e., its non-ad revenue). This “other” revenue increased by 8% year-over-year to reach $36 million in Q4 and was up 22% to reach $140 million for 2025.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/reddit-looks-to-ai-search-as-its-next-big-opportunity/</guid><pubDate>Thu, 05 Feb 2026 23:20:27 +0000</pubDate></item><item><title>Sapiom raises $15M to help AI agents buy their own tech tools (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/05/sapiom-raises-15m-to-help-ai-agents-buy-their-own-tech-tools/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Ilan_Zerbib-3269-067-p.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People without coding backgrounds are discovering that they can build their own custom apps using vibe coding — solutions like Lovable that turn plain-language descriptions into working code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these prompt-to-code tools can help create nice prototypes, launching them into full-scale production (as this reporter recently discovered) can be tricky without figuring out how to connect the application with external tech services, such as those that can send text messages via SMS, email, and process Stripe payments.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ilan Zerbib, who spent five years as Shopify’s director of engineering for payments, is building a solution that could eliminate these back-end infrastructure headaches for nontechnical creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last summer, Zerbib launched Sapiom, a San Francisco startup developing the financial layer that allows AI agents to securely purchase and access software, APIs, data, and compute — essentially creating a payment system that lets AI automatically buy the services it needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Every time an AI agent connects to an external tool like Twilio for SMS, it requires authentication and a micro-payment. Sapiom’s goal is to make this whole process seamless, letting the AI agent decide what to buy and when without human intervention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the future, apps are going to consume services which require payments.&lt;strong&gt; &lt;/strong&gt;Right now, there’s no easy way for agents to actually access all of that,” said Amit Kumar, a partner at Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kumar has met with dozens of startups in the AI payments space, but he believes Zerbib’s focus on the financial layer for enterprises, rather than consumers, is what’s truly needed to make AI agents work. That’s why Accel is leading Sapiom’s $15 million seed round, with participation from Okta Ventures, Gradient Ventures, Array Ventures, Menlo Ventures, Anthropic, and Coinbase Ventures.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“If you really think about it, every API call is a payment. Every time you send a text message, it’s a payment. Every time you spin up a server for AWS, it’s a payment,” Kumar told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While it’s still early days for Sapiom, the startup hopes that its infrastructure solution will be adopted by vibe-coding companies and other companies creating AI agents that will eventually be tasked with doing many things on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, anyone who has vibe-coded an app with SMS capabilities won’t have to manually sign up for Twilio, add a credit card, and copy an API key into their code. Instead, Sapiom handles all of that in the background, and the person building the micro-app will be charged for Twilio’s services as a pass-through fee by Lovable, Bolt, or another vibe-coding platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While Sapiom is currently focused on B2B solutions, its technology could eventually empower personal AI agents to handle consumer transactions. The expectation is that individuals will one day trust agents to make independent financial decisions, such as ordering an Uber or shopping on Amazon. While that future is exciting, Zerbib believes that AI won’t magically make people buy more things, which is why he’s focusing on creating financial layers for businesses instead.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Ilan_Zerbib-3269-067-p.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;People without coding backgrounds are discovering that they can build their own custom apps using vibe coding — solutions like Lovable that turn plain-language descriptions into working code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While these prompt-to-code tools can help create nice prototypes, launching them into full-scale production (as this reporter recently discovered) can be tricky without figuring out how to connect the application with external tech services, such as those that can send text messages via SMS, email, and process Stripe payments.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Ilan Zerbib, who spent five years as Shopify’s director of engineering for payments, is building a solution that could eliminate these back-end infrastructure headaches for nontechnical creators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last summer, Zerbib launched Sapiom, a San Francisco startup developing the financial layer that allows AI agents to securely purchase and access software, APIs, data, and compute — essentially creating a payment system that lets AI automatically buy the services it needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Every time an AI agent connects to an external tool like Twilio for SMS, it requires authentication and a micro-payment. Sapiom’s goal is to make this whole process seamless, letting the AI agent decide what to buy and when without human intervention.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the future, apps are going to consume services which require payments.&lt;strong&gt; &lt;/strong&gt;Right now, there’s no easy way for agents to actually access all of that,” said Amit Kumar, a partner at Accel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kumar has met with dozens of startups in the AI payments space, but he believes Zerbib’s focus on the financial layer for enterprises, rather than consumers, is what’s truly needed to make AI agents work. That’s why Accel is leading Sapiom’s $15 million seed round, with participation from Okta Ventures, Gradient Ventures, Array Ventures, Menlo Ventures, Anthropic, and Coinbase Ventures.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 23, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“If you really think about it, every API call is a payment. Every time you send a text message, it’s a payment. Every time you spin up a server for AWS, it’s a payment,” Kumar told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While it’s still early days for Sapiom, the startup hopes that its infrastructure solution will be adopted by vibe-coding companies and other companies creating AI agents that will eventually be tasked with doing many things on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, anyone who has vibe-coded an app with SMS capabilities won’t have to manually sign up for Twilio, add a credit card, and copy an API key into their code. Instead, Sapiom handles all of that in the background, and the person building the micro-app will be charged for Twilio’s services as a pass-through fee by Lovable, Bolt, or another vibe-coding platform.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While Sapiom is currently focused on B2B solutions, its technology could eventually empower personal AI agents to handle consumer transactions. The expectation is that individuals will one day trust agents to make independent financial decisions, such as ordering an Uber or shopping on Amazon. While that future is exciting, Zerbib believes that AI won’t magically make people buy more things, which is why he’s focusing on creating financial layers for businesses instead.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/05/sapiom-raises-15m-to-help-ai-agents-buy-their-own-tech-tools/</guid><pubDate>Thu, 05 Feb 2026 23:53:42 +0000</pubDate></item></channel></rss>