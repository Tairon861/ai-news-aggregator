<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 15 Dec 2025 18:36:04 +0000</lastBuildDate><item><title>CEOs still betting big on AI: Strategy vs. return on investment in 2026 (AI News)</title><link>https://www.artificialintelligence-news.com/news/ceos-still-betting-on-ai-strategy-vs-return-on-investment-in-2026/</link><description>&lt;p&gt;Enterprise leaders are pressing ahead with artificial intelligence, even as some early results remain uneven. Reporting from the &lt;em&gt;Wall Street Journal&lt;/em&gt; and &lt;em&gt;Reuters&lt;/em&gt; shows that most CEOs expect AI spending to keep rising through 2026, despite difficulty tying those investments to clear, enterprise-wide returns.&lt;/p&gt;&lt;p&gt;The tension highlights where many organisations now sit in their AI journey. The technology has moved beyond trials and proofs of concept, but it has yet to settle into a reliable source of value. Companies are operating in an in-between phase, where ambition, execution, and expectations are all under strain at the same time.&lt;/p&gt;&lt;h3&gt;Spending continues, even as returns lag&lt;/h3&gt;&lt;p&gt;AI budgets have climbed steadily in large enterprises over the past two years. Competitive pressure, board oversight, and fear of being left behind have all played a role. At the same time, executives are more open about the limits they are seeing. Gains often show up in pockets rather than in the business, pilots fail to spread, and the cost of connecting AI systems to existing tools keeps rising.&lt;/p&gt;&lt;p&gt;A &lt;em&gt;Wall Street Journal&lt;/em&gt; survey of senior executives found that most CEOs see AI as central to long-term competitiveness, even if short-term benefits are hard to measure. For many, AI no longer feels optional. It is treated as a capability that must be developed over time, rather than a project that can be paused if results disappoint.&lt;/p&gt;&lt;p&gt;That view helps explain why spending remains steady. Leaders worry that cutting back now could weaken their position later, especially as rivals improve how they use the technology.&lt;/p&gt;&lt;h3&gt;Why pilots struggle to scale&lt;/h3&gt;&lt;p&gt;One of the main barriers to stronger returns is the jump from experimentation to day-to-day use. Many organisations have launched AI pilots in different teams, often without shared rules or coordination. While these efforts can generate insight and interest, few translate into changes that affect the wider business.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Reuters&lt;/em&gt; has reported that companies trying to scale AI frequently run into issues with data quality, system links, security controls, and regulatory requirements. The problems are not only technical, but reflect how work is organised. Responsibility is often split in teams, ownership is unclear, and decisions slow down once projects touch legal, risk, and IT functions.&lt;/p&gt;&lt;p&gt;The result is a pattern of heavy spending on trials, with limited progress toward systems that are embedded in core operations.&lt;/p&gt;&lt;h3&gt;Infrastructure costs reshape the equation&lt;/h3&gt;&lt;p&gt;The cost of infrastructure is also weighing on AI returns. Training and running models demands large amounts of computing power, storage, and energy. Cloud bills can rise quickly as use grows, while building on-site systems requires upfront investment and long planning cycles. Executives cited by &lt;em&gt;Reuters&lt;/em&gt; have warned that infrastructure costs can outpace the benefits delivered by AI tools, particularly in the early stages. This has led to tough choices: whether to centralise AI resources or leave teams to experiment on their own; whether to build in-house systems or rely on vendors; and how much waste is acceptable while capabilities are still forming.&lt;/p&gt;&lt;p&gt;In practice, these decisions are shaping AI strategy as much as model performance or use-case selection.&lt;/p&gt;&lt;h3&gt;AI governance moves to the centre of CEO decision-making&lt;/h3&gt;&lt;p&gt;As AI spending increases, so does scrutiny. Boards, regulators, and internal audit teams are asking harder questions. In response, many organisations are tightening control. Decision rights are shifting toward central teams, AI councils are becoming more common, and projects are being linked more closely to business priorities.&lt;/p&gt;&lt;p&gt;The&lt;em&gt; Wall Street Journal&lt;/em&gt; reports that companies are moving away from loosely connected experiments toward clearer goals, measures, and timelines. This can slow progress, but it reflects a growing belief that AI should be managed with the same discipline as other major investments.&lt;/p&gt;&lt;p&gt;The shift marks a change in how AI is treated. It is no longer a side effort or a curiosity but is being brought into existing operating and risk structures.&lt;/p&gt;&lt;h3&gt;Expectations are being reset, not abandoned&lt;/h3&gt;&lt;p&gt;Importantly, the persistence of AI spending does not signal blind optimism. Instead, it reflects a reset in expectations. CEOs are learning that AI rarely delivers immediate, sweeping returns. Value tends to emerge gradually, as organisations adjust workflows, retrain staff, and refine data foundations.&lt;/p&gt;&lt;p&gt;Rather than abandoning AI initiatives, many enterprises are narrowing their focus. They are prioritising fewer use cases, demanding clearer ownership, and aligning projects more closely with business outcomes. The re-calibration may reduce short-term excitement, but it improves the likelihood of sustainable returns.&lt;/p&gt;&lt;h3&gt;What CEO AI strategy signals for 2026 planning&lt;/h3&gt;&lt;p&gt;For organisations shaping their plans for 2026, the message for every CEO is not to retreat from AI, but to pursue it with more care as AI strategies mature. Ownership, governance, and realistic timelines matter more than headline spending levels or bold claims.&lt;/p&gt;&lt;p&gt;Those most likely to benefit are treating AI as a long-term shift in how the organisation works, not a quick route to growth. In the next phase, advantage will depend less on how much is spent and more on how well AI fits into everyday operations.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Ambre Estève)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Enterprise leaders are pressing ahead with artificial intelligence, even as some early results remain uneven. Reporting from the &lt;em&gt;Wall Street Journal&lt;/em&gt; and &lt;em&gt;Reuters&lt;/em&gt; shows that most CEOs expect AI spending to keep rising through 2026, despite difficulty tying those investments to clear, enterprise-wide returns.&lt;/p&gt;&lt;p&gt;The tension highlights where many organisations now sit in their AI journey. The technology has moved beyond trials and proofs of concept, but it has yet to settle into a reliable source of value. Companies are operating in an in-between phase, where ambition, execution, and expectations are all under strain at the same time.&lt;/p&gt;&lt;h3&gt;Spending continues, even as returns lag&lt;/h3&gt;&lt;p&gt;AI budgets have climbed steadily in large enterprises over the past two years. Competitive pressure, board oversight, and fear of being left behind have all played a role. At the same time, executives are more open about the limits they are seeing. Gains often show up in pockets rather than in the business, pilots fail to spread, and the cost of connecting AI systems to existing tools keeps rising.&lt;/p&gt;&lt;p&gt;A &lt;em&gt;Wall Street Journal&lt;/em&gt; survey of senior executives found that most CEOs see AI as central to long-term competitiveness, even if short-term benefits are hard to measure. For many, AI no longer feels optional. It is treated as a capability that must be developed over time, rather than a project that can be paused if results disappoint.&lt;/p&gt;&lt;p&gt;That view helps explain why spending remains steady. Leaders worry that cutting back now could weaken their position later, especially as rivals improve how they use the technology.&lt;/p&gt;&lt;h3&gt;Why pilots struggle to scale&lt;/h3&gt;&lt;p&gt;One of the main barriers to stronger returns is the jump from experimentation to day-to-day use. Many organisations have launched AI pilots in different teams, often without shared rules or coordination. While these efforts can generate insight and interest, few translate into changes that affect the wider business.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Reuters&lt;/em&gt; has reported that companies trying to scale AI frequently run into issues with data quality, system links, security controls, and regulatory requirements. The problems are not only technical, but reflect how work is organised. Responsibility is often split in teams, ownership is unclear, and decisions slow down once projects touch legal, risk, and IT functions.&lt;/p&gt;&lt;p&gt;The result is a pattern of heavy spending on trials, with limited progress toward systems that are embedded in core operations.&lt;/p&gt;&lt;h3&gt;Infrastructure costs reshape the equation&lt;/h3&gt;&lt;p&gt;The cost of infrastructure is also weighing on AI returns. Training and running models demands large amounts of computing power, storage, and energy. Cloud bills can rise quickly as use grows, while building on-site systems requires upfront investment and long planning cycles. Executives cited by &lt;em&gt;Reuters&lt;/em&gt; have warned that infrastructure costs can outpace the benefits delivered by AI tools, particularly in the early stages. This has led to tough choices: whether to centralise AI resources or leave teams to experiment on their own; whether to build in-house systems or rely on vendors; and how much waste is acceptable while capabilities are still forming.&lt;/p&gt;&lt;p&gt;In practice, these decisions are shaping AI strategy as much as model performance or use-case selection.&lt;/p&gt;&lt;h3&gt;AI governance moves to the centre of CEO decision-making&lt;/h3&gt;&lt;p&gt;As AI spending increases, so does scrutiny. Boards, regulators, and internal audit teams are asking harder questions. In response, many organisations are tightening control. Decision rights are shifting toward central teams, AI councils are becoming more common, and projects are being linked more closely to business priorities.&lt;/p&gt;&lt;p&gt;The&lt;em&gt; Wall Street Journal&lt;/em&gt; reports that companies are moving away from loosely connected experiments toward clearer goals, measures, and timelines. This can slow progress, but it reflects a growing belief that AI should be managed with the same discipline as other major investments.&lt;/p&gt;&lt;p&gt;The shift marks a change in how AI is treated. It is no longer a side effort or a curiosity but is being brought into existing operating and risk structures.&lt;/p&gt;&lt;h3&gt;Expectations are being reset, not abandoned&lt;/h3&gt;&lt;p&gt;Importantly, the persistence of AI spending does not signal blind optimism. Instead, it reflects a reset in expectations. CEOs are learning that AI rarely delivers immediate, sweeping returns. Value tends to emerge gradually, as organisations adjust workflows, retrain staff, and refine data foundations.&lt;/p&gt;&lt;p&gt;Rather than abandoning AI initiatives, many enterprises are narrowing their focus. They are prioritising fewer use cases, demanding clearer ownership, and aligning projects more closely with business outcomes. The re-calibration may reduce short-term excitement, but it improves the likelihood of sustainable returns.&lt;/p&gt;&lt;h3&gt;What CEO AI strategy signals for 2026 planning&lt;/h3&gt;&lt;p&gt;For organisations shaping their plans for 2026, the message for every CEO is not to retreat from AI, but to pursue it with more care as AI strategies mature. Ownership, governance, and realistic timelines matter more than headline spending levels or bold claims.&lt;/p&gt;&lt;p&gt;Those most likely to benefit are treating AI as a long-term shift in how the organisation works, not a quick route to growth. In the next phase, advantage will depend less on how much is spent and more on how well AI fits into everyday operations.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Ambre Estève)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ceos-still-betting-on-ai-strategy-vs-return-on-investment-in-2026/</guid><pubDate>Mon, 15 Dec 2025 09:00:00 +0000</pubDate></item><item><title>AI coding is now everywhere. But not everyone is convinced. (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1128352/rise-of-ai-coding-developers-2026/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;br /&gt;Depending who you ask, AI-powered coding is either giving software developers an unprecedented productivity boost or churning out masses of poorly designed code that saps their attention and sets software projects up for serious long term-maintenance problems.&lt;/p&gt;  &lt;p&gt;The problem is right now, it’s not easy to know which is true.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;As tech giants pour billions into large language models (LLMs), coding has been touted as the technology’s killer app. Both Microsoft CEO Satya Nadella and Google CEO Sundar Pichai have claimed that around a quarter of their companies’ code is now AI-generated. And in March, Anthropic’s CEO, Dario Amodei, predicted that within six months 90% of all code would be written by AI. It’s an appealing and obvious use case. Code is a form of language, we need lots of it, and it’s expensive to produce manually. It’s also easy to tell if it works—run a program and it’s immediately evident whether it’s functional.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Executives enamored with the potential to break through human bottlenecks are pushing engineers to lean into an AI-powered future. But after speaking to more than 30 developers, technology executives, analysts, and researchers, &lt;em&gt;MIT Technology Review&lt;/em&gt; found that the picture is not as straightforward as it might seem.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For some developers on the front lines, initial enthusiasm is waning as they bump up against the technology’s limitations. And as a growing body of research suggests that the claimed productivity gains may be illusory, some are questioning whether the emperor is wearing any clothes.&lt;/p&gt; 
 &lt;p&gt;The pace of progress is complicating the picture, though. A steady drumbeat of new model releases mean these tools’ capabilities and quirks are constantly evolving. And their utility often depends on the tasks they are applied to and the organizational structures built around them. All of this leaves developers navigating confusing gaps between expectation and reality.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Is it the best of times or the worst of times (to channel Dickens) for AI coding? Maybe both.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;A fast-moving field&lt;/h3&gt;  &lt;p&gt;It’s hard to avoid AI coding tools these days. There are a dizzying array of products available, both from model developers like Anthropic, OpenAI, and Google and from companies like Cursor and Windsurf, which wrap these models in polished code-editing software. And according to Stack Overflow’s 2025 Developer Survey, they’re being adopted rapidly, with 65% of developers now using them at least weekly.&lt;/p&gt;  &lt;p&gt;AI coding tools first emerged around 2016 but were supercharged with the arrival of LLMs. Early versions functioned as little more than autocomplete for programmers, suggesting what to type next. Today they can analyze entire code bases, edit across files, fix bugs, and even generate documentation explaining how the code works. All this is guided through natural-language prompts via a chat interface.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“Agents”—autonomous LLM-powered coding tools that can take a high-level plan and build entire programs independently—represent the latest frontier in AI coding. This leap was enabled by the latest reasoning models, which can tackle complex problems step by step and, crucially, access external tools to complete tasks. “This is how the model is able to code, as opposed to just talk about coding,” says Boris Cherny, head of Claude Code, Anthropic’s coding agent.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;These agents have made impressive progress on software engineering benchmarks—standardized tests that measure model performance. When OpenAI introduced the SWE-bench Verified benchmark in August 2024, offering a way to evaluate agents’ success at fixing real bugs in open-source repositories, the top model solved just 33% of issues. A year later, leading models consistently score above 70%.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In February, Andrej Karpathy, a founding member of OpenAI and former director of AI at Tesla, coined the term “vibe coding”—meaning an approach where people describe software in natural language and let AI write, refine, and debug the code. Social media abounds with developers who have bought into this vision, claiming massive productivity boosts.&lt;/p&gt;  &lt;p&gt;But while some developers and companies report such productivity gains, the hard evidence is more mixed. Early studies from GitHub, Google, and Microsoft—all vendors of AI tools—found developers completing tasks 20% to 55% faster. But a September report from the consultancy Bain &amp;amp; Company described real-world savings as “unremarkable.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Data from the developer analytics firm GitClear shows that most engineers are producing roughly 10% more durable code—code that isn’t deleted or rewritten within weeks—since 2022, likely thanks to AI. But that gain has come with sharp declines in several measures of code quality. Stack Overflow’s survey also found trust and positive sentiment toward AI tools falling significantly for the first time. And most provocatively, a July study by the nonprofit research organization Model Evaluation &amp;amp; Threat Research (METR) showed that while experienced developers believed AI made them 20% faster, objective tests showed they were actually 19% slower.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Growing disillusionment&lt;/h3&gt;  &lt;p&gt;For Mike Judge, principal developer at the software consultancy Substantial, the METR study struck a nerve. He was an enthusiastic early adopter of AI tools, but over time he grew frustrated with their limitations and the modest boost they brought to his productivity. “I was complaining to people because I was like, ‘It’s helping me but I can’t figure out how to make it really help me a lot,’” he says. “I kept feeling like the AI was really dumb, but maybe I could trick it into being smart if I found the right magic incantation.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;When asked by a friend, Judge had estimated the tools were providing a roughly 25% speedup. So when he saw similar estimates attributed to developers in the METR study he decided to test his own. For six weeks, he guessed how long a task would take, flipped a coin to decide whether to use AI or code manually, and timed himself. To his surprise, AI slowed him down by an median of 21%—mirroring the METR results.&lt;/p&gt;  &lt;p&gt;This got Judge crunching the numbers. If these tools were really speeding developers up, he reasoned, you should see a massive boom in new apps, website registrations, video games, and projects on GitHub. He spent hours and several hundred dollars analyzing all the publicly available data and found flat lines everywhere.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;“Shouldn’t this be going up and to the right?” says Judge. “Where’s the hockey stick on any of these graphs? I thought everybody was so extraordinarily productive.” The obvious conclusion, he says, is that AI tools provide little productivity boost for most developers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Developers interviewed by &lt;em&gt;MIT Technology Review&lt;/em&gt; generally agree on where AI tools excel: producing “boilerplate code” (reusable chunks of code repeated in multiple places with little modification), writing tests, fixing bugs, and explaining unfamiliar code to new developers. Several noted that AI helps overcome the “blank page problem” by offering an imperfect first stab to get a developer’s creative juices flowing. It can also let nontechnical colleagues quickly prototype software features, easing the load on already overworked engineers.&lt;/p&gt;  &lt;p&gt;These tasks can be tedious, and developers are typically&amp;nbsp; glad to hand them off. But they represent only a small part of an experienced engineer’s workload. For the more complex problems where engineers really earn their bread, many developers told MIT Technology Review, the tools face significant hurdles.&lt;/p&gt;  &lt;p&gt;Perhaps the biggest problem is that LLMs can hold only a limited amount of information in their “context window”—essentially their working memory. This means they struggle to parse large code bases and are prone to forgetting what they’re doing on longer tasks. “It gets really nearsighted—it’ll only look at the thing that’s right in front of it,” says Judge. “And if you tell it to do a dozen things, it’ll do 11 of them and just forget that last one.”&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1129585" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_1-1-v2.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;DEREK BRAHNEY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;LLMs’ myopia can lead to headaches for human coders. While an LLM-generated response to a problem may work in isolation, software is made up of hundreds of interconnected modules. If these aren’t built with consideration for other parts of the software, it can quickly lead to a tangled, inconsistent code base that’s hard for humans to parse and, more important, to maintain.&lt;/p&gt;  &lt;p&gt;Developers have traditionally addressed this by following conventions—loosely defined coding guidelines that differ widely between projects and teams. “AI has this overwhelming tendency to not understand what the existing conventions are within a repository,” says Bill Harding, the CEO of GitClear. “And so it is very likely to come up with its own slightly different version of how to solve a problem.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;The models also just get things wrong. Like all LLMs, coding models are prone to “hallucinating”—it’s an issue built into how they work. But because the code they output looks so polished, errors can be difficult to detect, says James Liu, director of software engineering at the advertising technology company Mediaocean. Put all these flaws together, and using these tools can feel a lot like pulling a lever on a one-armed bandit. “Some projects you get a 20x improvement in terms of speed or efficiency,” says Liu. “On other things, it just falls flat on its face, and you spend all this time trying to coax it into granting you the wish that you wanted and it’s just not going to.”&lt;/p&gt;  &lt;p&gt;Judge suspects this is why engineers often overestimate productivity gains. “You remember the jackpots. You don’t remember sitting there plugging tokens into the slot machine for two hours,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;p&gt;And it can be particularly pernicious if the developer is unfamiliar with the task. Judge remembers getting AI to help set up a Microsoft cloud service called an Azure Functions, which he’d never used before. He thought it would take about two hours, but nine hours later he threw in the towel. “It kept leading me down these rabbit holes and I didn’t know enough about the topic to be able to tell it ‘Hey, this is nonsensical,’” he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The debt begins to mount up&lt;/h3&gt;  &lt;p&gt;Developers&amp;nbsp;constantly make trade-offs between speed of development and the maintainability of their code—creating what’s known as “technical debt,” says Geoffrey G. Parker, professor of engineering innovation at Dartmouth College. Each shortcut adds complexity and makes the code base harder to manage, accruing “interest” that must eventually be repaid by restructuring the code. As this debt piles up, adding new features and maintaining the software becomes slower and more difficult.&lt;/p&gt; 
 &lt;p&gt;Accumulating technical debt is inevitable in most projects, but AI tools make it much easier for time-pressured engineers to cut corners, says GitClear’s Harding. And GitClear’s data suggests this is happening at scale. Since 2020, the company has seen a significant rise in the amount of copy-pasted code—an indicator that developers are reusing more code snippets, most likely based on AI suggestions—and an even bigger decline in the amount of code moved from one place to another, which happens when developers clean up their code base.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_20"&gt; &lt;p&gt;And as models improve, the code they produce is becoming increasingly verbose and complex, says Tariq Shaukat, CEO of Sonar, which makes tools for checking code quality. This is driving down the number of obvious bugs and security vulnerabilities, he says, but at the cost of increasing the number of “code smells”—harder-to-pinpoint flaws that lead to maintenance problems and technical debt.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Recent research by Sonar found that these make up more than 90% of the issues found in code generated by leading AI models. “Issues that are easy to spot are disappearing, and what’s left are much more complex issues that take a while to find,” says Shaukat. “That’s what worries us about this space at the moment. You’re almost being lulled into a false sense of security.”&lt;/p&gt;  &lt;p&gt;If AI tools make it increasingly difficult to maintain code, that could have significant security implications, says Jessica Ji, a security researcher at Georgetown University. “The harder it is to update things and fix things, the more likely a code base or any given chunk of code is to become insecure over time,” says Ji.&lt;/p&gt;  &lt;p&gt;There are also more specific security concerns, she says. Researchers have discovered a worrying class of hallucinations where models reference nonexistent software packages in their code. Attackers can exploit this by creating packages with those names that harbor vulnerabilities, which the model or developer may then unwittingly incorporate into software.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;LLMs are also vulnerable to “data-poisoning attacks,” where hackers seed the publicly available data sets models train on with data that alters the model’s behavior in undesirable ways, such as generating insecure code when triggered by specific phrases. In October, research by Anthropic found that as few as 250 malicious documents can introduce this kind of back door into an LLM regardless of its size.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;h3 class="wp-block-heading"&gt;The converted&lt;/h3&gt;  &lt;p&gt;Despite these issues, though, there’s probably no turning back. “Odds are that writing every line of code on a keyboard by hand—those days are quickly slipping behind us,” says Kyle Daigle, chief operating officer at the Microsoft-owned code-hosting platform GitHub, which produces a popular AI-powered tool called Copilot (not to be confused with the Microsoft product of the same name).&lt;/p&gt;  &lt;p&gt;The Stack Overflow report found that despite growing distrust in the technology, usage has increased rapidly and consistently over the past three years. Erin Yepis, a senior analyst at Stack Overflow, says this suggests that engineers are taking advantage of the tools with a clear-eyed view of the risks. The report also found that frequent users tend to be more enthusiastic and more than half of developers are not using the latest coding agents, perhaps explaining why many remain underwhelmed by the technology.&lt;/p&gt;  &lt;p&gt;Those latest tools can be a revelation. Trevor Dilley, CTO at the software development agency Twenty20 Ideas, says he had found some value in AI editors’ autocomplete functions, but when he tried anything more complex it would “fail catastrophically.” Then in March, while on vacation with his family, he set the newly released Claude Code to work on one of his hobby projects. It completed a four-hour task in two minutes, and the code was better than what he would have written.&lt;/p&gt;  &lt;p&gt;“I was like, Whoa,” he says. “That, for me, was the moment, really. There’s no going back from here.” Dilley has since cofounded a startup called DevSwarm, which is creating software that can marshal multiple agents to work in parallel on a piece of software.&lt;/p&gt; 
 &lt;p&gt;The challenge, says Armin Ronacher, a prominent open-source developer, is that the learning curve for these tools is shallow but long. Until March he’d remained unimpressed by AI tools, but after leaving his job at the software company Sentry in April to launch a startup, he started experimenting with agents. “I basically spent a lot of months doing nothing but this,” he says. “Now, 90% of the code that I write is AI-generated.”&lt;/p&gt;  &lt;p&gt;Getting to that point involved extensive trial and error, to figure out which problems tend to trip the tools up and which they can handle efficiently. Today’s models can tackle most coding tasks with the right guardrails, says Ronacher, but these can be very task and project specific.&lt;/p&gt;  &lt;p&gt;To get the most out of these tools, developers must surrender control over individual lines of code and focus on the overall software architecture, says Nico Westerdale, chief technology officer at the veterinary staffing company IndeVets. He recently built a data science platform 100,000 lines of code long almost exclusively by prompting models rather than writing the code himself.&lt;/p&gt;  &lt;p&gt;Westerdale’s process starts with an extended conversation with the modelagent to develop a detailed plan for what to build and how. He then guides it through each step. It rarely gets things right on the first try and needs constant wrangling, but if you force it to stick to well-defined design patterns, the models can produce high-quality, easily maintainable code, says Westerdale. He reviews every line, and the code is as good as anything he’s ever produced, he says: “I’ve just found it absolutely revolutionary,. It’s also frustrating, difficult, a different way of thinking, and we’re only just getting used to it.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;But while individual developers are learning how to use these tools effectively, getting consistent results across a large engineering team is significantly harder. AI tools amplify both the good and bad aspects of your engineering culture, says Ryan J. Salva, senior director of product management at Google. With strong processes, clear coding patterns, and well-defined best practices, these tools can shine.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1129593" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_1-3-webv3.jpg?w=1999" width="1999" /&gt;&lt;div class="image-credit"&gt;DEREK BRAHNEY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;But if your development process is disorganized, they’ll only magnify the problems. It’s also essential to codify that institutional knowledge so the models can draw on it effectively. “A lot of work needs to be done to help build up context and get the tribal knowledge out of our heads,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt; &lt;p&gt;The cryptocurrency exchange Coinbase has been vocal about its adoption of AI tools. CEO Brian Armstrong made headlines in August when he revealed that the company had fired staff unwilling to adopt AI tools. But Coinbase’s head of platform, Rob Witoff, tells &lt;em&gt;MIT Technology Review&lt;/em&gt; that while they’ve seen massive productivity gains in some areas, the impact has been patchy. For simpler tasks like restructuring the code base and writing tests, AI-powered workflows have achieved speedups of up to 90%. But gains are more modest for other tasks, and the disruption caused by overhauling existing processes often counteracts the increased coding speed, says Witoff.&lt;/p&gt;  &lt;p&gt;One factor is that AI tools let junior developers produce far more code,. As in almost all engineering teams, this code has to be reviewed by others, normally more senior developers, to catch bugs and ensure it meets quality standards. But the sheer volume of code now being churned out i whichs quickly saturatinges the ability of midlevel staff to review changes. “This is the cycle we’re going through almost every month, where we automate a new thing lower down in the stack, which brings more pressure higher up in the stack,” he says. “Then we’re looking at applying automation to that higher-up piece.”&lt;/p&gt;  &lt;p&gt;Developers also spend only 20% to 40% of their time coding, says Jue Wang, a partner at Bain, so even a significant speedup there often translates to more modest overall gains. Developers spend the rest of their time analyzing software problems and dealing with customer feedback, product strategy, and administrative tasks. To get significant efficiency boosts, companies may need to apply generative AI to all these other processes too, says Jue, and that is still in the works.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Rapid evolution&lt;/h3&gt;  &lt;p&gt;Programming with agents is a dramatic departure from previous working practices, though, so it’s not surprising companies are facing some teething issues. These are also very new products that are changing by the day. “Every couple months the model improves, and there’s a big step change in the model’s coding capabilities and you have to get recalibrated,” says Anthropic’s Cherny.&lt;/p&gt;  &lt;p&gt;For example, in June Anthropic introduced a built-in planning mode to Claude; it has since been replicated by other providers. In October, the company also enabled Claude to ask users questions when it needs more context or faces multiple possible solutions, which Cherny says helps it avoid the tendency to simply assume which path is the best way forward.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_29"&gt; &lt;p&gt;Most significant, Anthropic has added features that make Claude better at managing its own context. When it nears the limits of its working memory, it summarizes key details and uses them to start a new context window, effectively giving it an “infinite” one, says Cherny. Claude can also invoke sub-agents to work on smaller tasks, so it no longer has to hold all aspects of the project in its own head. The company claims that its latest model, Claude 4.5 Sonnet, can now code autonomously for more than 30 hours without major performance degradation.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_31"&gt; &lt;p&gt;Novel approaches to software development could also sidestep coding agents’ other flaws. MIT professor Max Tegmark has introduced something he calls “vericoding,” which could allow agents to produce entirely bug-free code from a natural-language description. It builds on an approach known as “formal verification,” where developers create a mathematical model of their software that can prove incontrovertibly that it functions correctly. This approach is used in high-stakes areas like flight-control systems and cryptographic libraries, but it remains costly and time-consuming, limiting its broader use.&lt;/p&gt;  &lt;p&gt;Rapid improvements in LLMs’ mathematical capabilities have opened up the tantalizing possibility of models that produce not only software but the mathematical proof that it’s bug free, says Tegmark. “You just give the specification, and the AI comes back with provably correct code,” he says. “You don’t have to touch the code. You don’t even have to ever look at the code.”&lt;/p&gt;  &lt;p&gt;When tested on about 2,000 vericoding problems in Dafny—a language designed for formal verification—the best LLMs solved over 60%, according to non-peer-reviewed research by Tegmark’s group. This was achieved with off-the-shelf LLMs, and Tegmark expects that training specifically for vericoding could improve scores rapidly.&lt;/p&gt;  &lt;p&gt;And counterintuitively, Tthe speed at which AI generates code could actuallylso ease maintainability concerns. Alex Worden, principal engineer at the business software giant Intuit, notes that maintenance is often difficult because engineers reuse components across projects, creating a tangle of dependencies where one change triggers cascading effects across the code base. Reusing code used to save developers time, but in a world where AI can produce hundreds of lines of code in seconds, that imperative has gone, says Worden.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_33"&gt; &lt;p&gt;Instead, he advocates for “disposable code,” where each component is generated independently by AI without regard for whether it follows design patterns or conventions. They are then connected via APIs—sets of rules that let components request information or services from each other. Each component’s inner workings are not dependent on other parts of the code base, making it possible to rip them out and replace them without wider impact, says Worden.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The industry is still concerned about humans maintaining AI-generated code,” he says. “I question how long humans will look at or care about code.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A narrowing talent pipeline&lt;/h3&gt;  &lt;p&gt;For the foreseeable future, though, humans will still need to understand and maintain the code that underpins their projects. And one of the most pernicious side effects of AI tools may be a shrinking pool of people capable of doing so.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Early evidence suggests that fears around the job-destroying effects of AI may be justified. A recent Stanford University study found that employment among software developers aged 22 to 25 fell nearly 20% between 2022 and 2025, coinciding with the rise of AI-powered coding tools.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_35"&gt;&lt;p&gt;Experienced developers could face difficulties too. Luciano Nooijen, an engineer at the video-game infrastructure developer Companion Group, used AI tools heavily in his day job, where they were provided for free. But when he began a side project without access to those tools, he found himself struggling with tasks that previously came naturally. “I was feeling so stupid because things that used to be instinct became manual, sometimes even cumbersome,” says Nooijen.&lt;/p&gt;  &lt;p&gt;Just as athletes still perform basic drills, he thinks the only way to maintain an instinct for coding is to regularly practice the grunt work. That’s why he’s largely abandoned AI tools, though he admits that deeper motivations are also at play.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Part of the reason Nooijen and other developers MIT Technology Review spoke to are pushing back against AI tools is a sense that they are hollowing out the parts of their jobs that they love. “I got into software engineering because I like working with computers. I like making machines do things that I want,” Nooijen says. “It’s just not fun sitting there with my work being done for me.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;br /&gt;Depending who you ask, AI-powered coding is either giving software developers an unprecedented productivity boost or churning out masses of poorly designed code that saps their attention and sets software projects up for serious long term-maintenance problems.&lt;/p&gt;  &lt;p&gt;The problem is right now, it’s not easy to know which is true.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;As tech giants pour billions into large language models (LLMs), coding has been touted as the technology’s killer app. Both Microsoft CEO Satya Nadella and Google CEO Sundar Pichai have claimed that around a quarter of their companies’ code is now AI-generated. And in March, Anthropic’s CEO, Dario Amodei, predicted that within six months 90% of all code would be written by AI. It’s an appealing and obvious use case. Code is a form of language, we need lots of it, and it’s expensive to produce manually. It’s also easy to tell if it works—run a program and it’s immediately evident whether it’s functional.&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Executives enamored with the potential to break through human bottlenecks are pushing engineers to lean into an AI-powered future. But after speaking to more than 30 developers, technology executives, analysts, and researchers, &lt;em&gt;MIT Technology Review&lt;/em&gt; found that the picture is not as straightforward as it might seem.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For some developers on the front lines, initial enthusiasm is waning as they bump up against the technology’s limitations. And as a growing body of research suggests that the claimed productivity gains may be illusory, some are questioning whether the emperor is wearing any clothes.&lt;/p&gt; 
 &lt;p&gt;The pace of progress is complicating the picture, though. A steady drumbeat of new model releases mean these tools’ capabilities and quirks are constantly evolving. And their utility often depends on the tasks they are applied to and the organizational structures built around them. All of this leaves developers navigating confusing gaps between expectation and reality.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Is it the best of times or the worst of times (to channel Dickens) for AI coding? Maybe both.&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;A fast-moving field&lt;/h3&gt;  &lt;p&gt;It’s hard to avoid AI coding tools these days. There are a dizzying array of products available, both from model developers like Anthropic, OpenAI, and Google and from companies like Cursor and Windsurf, which wrap these models in polished code-editing software. And according to Stack Overflow’s 2025 Developer Survey, they’re being adopted rapidly, with 65% of developers now using them at least weekly.&lt;/p&gt;  &lt;p&gt;AI coding tools first emerged around 2016 but were supercharged with the arrival of LLMs. Early versions functioned as little more than autocomplete for programmers, suggesting what to type next. Today they can analyze entire code bases, edit across files, fix bugs, and even generate documentation explaining how the code works. All this is guided through natural-language prompts via a chat interface.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“Agents”—autonomous LLM-powered coding tools that can take a high-level plan and build entire programs independently—represent the latest frontier in AI coding. This leap was enabled by the latest reasoning models, which can tackle complex problems step by step and, crucially, access external tools to complete tasks. “This is how the model is able to code, as opposed to just talk about coding,” says Boris Cherny, head of Claude Code, Anthropic’s coding agent.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;These agents have made impressive progress on software engineering benchmarks—standardized tests that measure model performance. When OpenAI introduced the SWE-bench Verified benchmark in August 2024, offering a way to evaluate agents’ success at fixing real bugs in open-source repositories, the top model solved just 33% of issues. A year later, leading models consistently score above 70%.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In February, Andrej Karpathy, a founding member of OpenAI and former director of AI at Tesla, coined the term “vibe coding”—meaning an approach where people describe software in natural language and let AI write, refine, and debug the code. Social media abounds with developers who have bought into this vision, claiming massive productivity boosts.&lt;/p&gt;  &lt;p&gt;But while some developers and companies report such productivity gains, the hard evidence is more mixed. Early studies from GitHub, Google, and Microsoft—all vendors of AI tools—found developers completing tasks 20% to 55% faster. But a September report from the consultancy Bain &amp;amp; Company described real-world savings as “unremarkable.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Data from the developer analytics firm GitClear shows that most engineers are producing roughly 10% more durable code—code that isn’t deleted or rewritten within weeks—since 2022, likely thanks to AI. But that gain has come with sharp declines in several measures of code quality. Stack Overflow’s survey also found trust and positive sentiment toward AI tools falling significantly for the first time. And most provocatively, a July study by the nonprofit research organization Model Evaluation &amp;amp; Threat Research (METR) showed that while experienced developers believed AI made them 20% faster, objective tests showed they were actually 19% slower.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Growing disillusionment&lt;/h3&gt;  &lt;p&gt;For Mike Judge, principal developer at the software consultancy Substantial, the METR study struck a nerve. He was an enthusiastic early adopter of AI tools, but over time he grew frustrated with their limitations and the modest boost they brought to his productivity. “I was complaining to people because I was like, ‘It’s helping me but I can’t figure out how to make it really help me a lot,’” he says. “I kept feeling like the AI was really dumb, but maybe I could trick it into being smart if I found the right magic incantation.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;When asked by a friend, Judge had estimated the tools were providing a roughly 25% speedup. So when he saw similar estimates attributed to developers in the METR study he decided to test his own. For six weeks, he guessed how long a task would take, flipped a coin to decide whether to use AI or code manually, and timed himself. To his surprise, AI slowed him down by an median of 21%—mirroring the METR results.&lt;/p&gt;  &lt;p&gt;This got Judge crunching the numbers. If these tools were really speeding developers up, he reasoned, you should see a massive boom in new apps, website registrations, video games, and projects on GitHub. He spent hours and several hundred dollars analyzing all the publicly available data and found flat lines everywhere.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;“Shouldn’t this be going up and to the right?” says Judge. “Where’s the hockey stick on any of these graphs? I thought everybody was so extraordinarily productive.” The obvious conclusion, he says, is that AI tools provide little productivity boost for most developers.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Developers interviewed by &lt;em&gt;MIT Technology Review&lt;/em&gt; generally agree on where AI tools excel: producing “boilerplate code” (reusable chunks of code repeated in multiple places with little modification), writing tests, fixing bugs, and explaining unfamiliar code to new developers. Several noted that AI helps overcome the “blank page problem” by offering an imperfect first stab to get a developer’s creative juices flowing. It can also let nontechnical colleagues quickly prototype software features, easing the load on already overworked engineers.&lt;/p&gt;  &lt;p&gt;These tasks can be tedious, and developers are typically&amp;nbsp; glad to hand them off. But they represent only a small part of an experienced engineer’s workload. For the more complex problems where engineers really earn their bread, many developers told MIT Technology Review, the tools face significant hurdles.&lt;/p&gt;  &lt;p&gt;Perhaps the biggest problem is that LLMs can hold only a limited amount of information in their “context window”—essentially their working memory. This means they struggle to parse large code bases and are prone to forgetting what they’re doing on longer tasks. “It gets really nearsighted—it’ll only look at the thing that’s right in front of it,” says Judge. “And if you tell it to do a dozen things, it’ll do 11 of them and just forget that last one.”&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1129585" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_1-1-v2.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;DEREK BRAHNEY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;LLMs’ myopia can lead to headaches for human coders. While an LLM-generated response to a problem may work in isolation, software is made up of hundreds of interconnected modules. If these aren’t built with consideration for other parts of the software, it can quickly lead to a tangled, inconsistent code base that’s hard for humans to parse and, more important, to maintain.&lt;/p&gt;  &lt;p&gt;Developers have traditionally addressed this by following conventions—loosely defined coding guidelines that differ widely between projects and teams. “AI has this overwhelming tendency to not understand what the existing conventions are within a repository,” says Bill Harding, the CEO of GitClear. “And so it is very likely to come up with its own slightly different version of how to solve a problem.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;The models also just get things wrong. Like all LLMs, coding models are prone to “hallucinating”—it’s an issue built into how they work. But because the code they output looks so polished, errors can be difficult to detect, says James Liu, director of software engineering at the advertising technology company Mediaocean. Put all these flaws together, and using these tools can feel a lot like pulling a lever on a one-armed bandit. “Some projects you get a 20x improvement in terms of speed or efficiency,” says Liu. “On other things, it just falls flat on its face, and you spend all this time trying to coax it into granting you the wish that you wanted and it’s just not going to.”&lt;/p&gt;  &lt;p&gt;Judge suspects this is why engineers often overestimate productivity gains. “You remember the jackpots. You don’t remember sitting there plugging tokens into the slot machine for two hours,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;p&gt;And it can be particularly pernicious if the developer is unfamiliar with the task. Judge remembers getting AI to help set up a Microsoft cloud service called an Azure Functions, which he’d never used before. He thought it would take about two hours, but nine hours later he threw in the towel. “It kept leading me down these rabbit holes and I didn’t know enough about the topic to be able to tell it ‘Hey, this is nonsensical,’” he says.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The debt begins to mount up&lt;/h3&gt;  &lt;p&gt;Developers&amp;nbsp;constantly make trade-offs between speed of development and the maintainability of their code—creating what’s known as “technical debt,” says Geoffrey G. Parker, professor of engineering innovation at Dartmouth College. Each shortcut adds complexity and makes the code base harder to manage, accruing “interest” that must eventually be repaid by restructuring the code. As this debt piles up, adding new features and maintaining the software becomes slower and more difficult.&lt;/p&gt; 
 &lt;p&gt;Accumulating technical debt is inevitable in most projects, but AI tools make it much easier for time-pressured engineers to cut corners, says GitClear’s Harding. And GitClear’s data suggests this is happening at scale. Since 2020, the company has seen a significant rise in the amount of copy-pasted code—an indicator that developers are reusing more code snippets, most likely based on AI suggestions—and an even bigger decline in the amount of code moved from one place to another, which happens when developers clean up their code base.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_20"&gt; &lt;p&gt;And as models improve, the code they produce is becoming increasingly verbose and complex, says Tariq Shaukat, CEO of Sonar, which makes tools for checking code quality. This is driving down the number of obvious bugs and security vulnerabilities, he says, but at the cost of increasing the number of “code smells”—harder-to-pinpoint flaws that lead to maintenance problems and technical debt.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Recent research by Sonar found that these make up more than 90% of the issues found in code generated by leading AI models. “Issues that are easy to spot are disappearing, and what’s left are much more complex issues that take a while to find,” says Shaukat. “That’s what worries us about this space at the moment. You’re almost being lulled into a false sense of security.”&lt;/p&gt;  &lt;p&gt;If AI tools make it increasingly difficult to maintain code, that could have significant security implications, says Jessica Ji, a security researcher at Georgetown University. “The harder it is to update things and fix things, the more likely a code base or any given chunk of code is to become insecure over time,” says Ji.&lt;/p&gt;  &lt;p&gt;There are also more specific security concerns, she says. Researchers have discovered a worrying class of hallucinations where models reference nonexistent software packages in their code. Attackers can exploit this by creating packages with those names that harbor vulnerabilities, which the model or developer may then unwittingly incorporate into software.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;LLMs are also vulnerable to “data-poisoning attacks,” where hackers seed the publicly available data sets models train on with data that alters the model’s behavior in undesirable ways, such as generating insecure code when triggered by specific phrases. In October, research by Anthropic found that as few as 250 malicious documents can introduce this kind of back door into an LLM regardless of its size.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;h3 class="wp-block-heading"&gt;The converted&lt;/h3&gt;  &lt;p&gt;Despite these issues, though, there’s probably no turning back. “Odds are that writing every line of code on a keyboard by hand—those days are quickly slipping behind us,” says Kyle Daigle, chief operating officer at the Microsoft-owned code-hosting platform GitHub, which produces a popular AI-powered tool called Copilot (not to be confused with the Microsoft product of the same name).&lt;/p&gt;  &lt;p&gt;The Stack Overflow report found that despite growing distrust in the technology, usage has increased rapidly and consistently over the past three years. Erin Yepis, a senior analyst at Stack Overflow, says this suggests that engineers are taking advantage of the tools with a clear-eyed view of the risks. The report also found that frequent users tend to be more enthusiastic and more than half of developers are not using the latest coding agents, perhaps explaining why many remain underwhelmed by the technology.&lt;/p&gt;  &lt;p&gt;Those latest tools can be a revelation. Trevor Dilley, CTO at the software development agency Twenty20 Ideas, says he had found some value in AI editors’ autocomplete functions, but when he tried anything more complex it would “fail catastrophically.” Then in March, while on vacation with his family, he set the newly released Claude Code to work on one of his hobby projects. It completed a four-hour task in two minutes, and the code was better than what he would have written.&lt;/p&gt;  &lt;p&gt;“I was like, Whoa,” he says. “That, for me, was the moment, really. There’s no going back from here.” Dilley has since cofounded a startup called DevSwarm, which is creating software that can marshal multiple agents to work in parallel on a piece of software.&lt;/p&gt; 
 &lt;p&gt;The challenge, says Armin Ronacher, a prominent open-source developer, is that the learning curve for these tools is shallow but long. Until March he’d remained unimpressed by AI tools, but after leaving his job at the software company Sentry in April to launch a startup, he started experimenting with agents. “I basically spent a lot of months doing nothing but this,” he says. “Now, 90% of the code that I write is AI-generated.”&lt;/p&gt;  &lt;p&gt;Getting to that point involved extensive trial and error, to figure out which problems tend to trip the tools up and which they can handle efficiently. Today’s models can tackle most coding tasks with the right guardrails, says Ronacher, but these can be very task and project specific.&lt;/p&gt;  &lt;p&gt;To get the most out of these tools, developers must surrender control over individual lines of code and focus on the overall software architecture, says Nico Westerdale, chief technology officer at the veterinary staffing company IndeVets. He recently built a data science platform 100,000 lines of code long almost exclusively by prompting models rather than writing the code himself.&lt;/p&gt;  &lt;p&gt;Westerdale’s process starts with an extended conversation with the modelagent to develop a detailed plan for what to build and how. He then guides it through each step. It rarely gets things right on the first try and needs constant wrangling, but if you force it to stick to well-defined design patterns, the models can produce high-quality, easily maintainable code, says Westerdale. He reviews every line, and the code is as good as anything he’s ever produced, he says: “I’ve just found it absolutely revolutionary,. It’s also frustrating, difficult, a different way of thinking, and we’re only just getting used to it.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;But while individual developers are learning how to use these tools effectively, getting consistent results across a large engineering team is significantly harder. AI tools amplify both the good and bad aspects of your engineering culture, says Ryan J. Salva, senior director of product management at Google. With strong processes, clear coding patterns, and well-defined best practices, these tools can shine.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1129593" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_1-3-webv3.jpg?w=1999" width="1999" /&gt;&lt;div class="image-credit"&gt;DEREK BRAHNEY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;But if your development process is disorganized, they’ll only magnify the problems. It’s also essential to codify that institutional knowledge so the models can draw on it effectively. “A lot of work needs to be done to help build up context and get the tribal knowledge out of our heads,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt; &lt;p&gt;The cryptocurrency exchange Coinbase has been vocal about its adoption of AI tools. CEO Brian Armstrong made headlines in August when he revealed that the company had fired staff unwilling to adopt AI tools. But Coinbase’s head of platform, Rob Witoff, tells &lt;em&gt;MIT Technology Review&lt;/em&gt; that while they’ve seen massive productivity gains in some areas, the impact has been patchy. For simpler tasks like restructuring the code base and writing tests, AI-powered workflows have achieved speedups of up to 90%. But gains are more modest for other tasks, and the disruption caused by overhauling existing processes often counteracts the increased coding speed, says Witoff.&lt;/p&gt;  &lt;p&gt;One factor is that AI tools let junior developers produce far more code,. As in almost all engineering teams, this code has to be reviewed by others, normally more senior developers, to catch bugs and ensure it meets quality standards. But the sheer volume of code now being churned out i whichs quickly saturatinges the ability of midlevel staff to review changes. “This is the cycle we’re going through almost every month, where we automate a new thing lower down in the stack, which brings more pressure higher up in the stack,” he says. “Then we’re looking at applying automation to that higher-up piece.”&lt;/p&gt;  &lt;p&gt;Developers also spend only 20% to 40% of their time coding, says Jue Wang, a partner at Bain, so even a significant speedup there often translates to more modest overall gains. Developers spend the rest of their time analyzing software problems and dealing with customer feedback, product strategy, and administrative tasks. To get significant efficiency boosts, companies may need to apply generative AI to all these other processes too, says Jue, and that is still in the works.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Rapid evolution&lt;/h3&gt;  &lt;p&gt;Programming with agents is a dramatic departure from previous working practices, though, so it’s not surprising companies are facing some teething issues. These are also very new products that are changing by the day. “Every couple months the model improves, and there’s a big step change in the model’s coding capabilities and you have to get recalibrated,” says Anthropic’s Cherny.&lt;/p&gt;  &lt;p&gt;For example, in June Anthropic introduced a built-in planning mode to Claude; it has since been replicated by other providers. In October, the company also enabled Claude to ask users questions when it needs more context or faces multiple possible solutions, which Cherny says helps it avoid the tendency to simply assume which path is the best way forward.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_29"&gt; &lt;p&gt;Most significant, Anthropic has added features that make Claude better at managing its own context. When it nears the limits of its working memory, it summarizes key details and uses them to start a new context window, effectively giving it an “infinite” one, says Cherny. Claude can also invoke sub-agents to work on smaller tasks, so it no longer has to hold all aspects of the project in its own head. The company claims that its latest model, Claude 4.5 Sonnet, can now code autonomously for more than 30 hours without major performance degradation.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_31"&gt; &lt;p&gt;Novel approaches to software development could also sidestep coding agents’ other flaws. MIT professor Max Tegmark has introduced something he calls “vericoding,” which could allow agents to produce entirely bug-free code from a natural-language description. It builds on an approach known as “formal verification,” where developers create a mathematical model of their software that can prove incontrovertibly that it functions correctly. This approach is used in high-stakes areas like flight-control systems and cryptographic libraries, but it remains costly and time-consuming, limiting its broader use.&lt;/p&gt;  &lt;p&gt;Rapid improvements in LLMs’ mathematical capabilities have opened up the tantalizing possibility of models that produce not only software but the mathematical proof that it’s bug free, says Tegmark. “You just give the specification, and the AI comes back with provably correct code,” he says. “You don’t have to touch the code. You don’t even have to ever look at the code.”&lt;/p&gt;  &lt;p&gt;When tested on about 2,000 vericoding problems in Dafny—a language designed for formal verification—the best LLMs solved over 60%, according to non-peer-reviewed research by Tegmark’s group. This was achieved with off-the-shelf LLMs, and Tegmark expects that training specifically for vericoding could improve scores rapidly.&lt;/p&gt;  &lt;p&gt;And counterintuitively, Tthe speed at which AI generates code could actuallylso ease maintainability concerns. Alex Worden, principal engineer at the business software giant Intuit, notes that maintenance is often difficult because engineers reuse components across projects, creating a tangle of dependencies where one change triggers cascading effects across the code base. Reusing code used to save developers time, but in a world where AI can produce hundreds of lines of code in seconds, that imperative has gone, says Worden.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_33"&gt; &lt;p&gt;Instead, he advocates for “disposable code,” where each component is generated independently by AI without regard for whether it follows design patterns or conventions. They are then connected via APIs—sets of rules that let components request information or services from each other. Each component’s inner workings are not dependent on other parts of the code base, making it possible to rip them out and replace them without wider impact, says Worden.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The industry is still concerned about humans maintaining AI-generated code,” he says. “I question how long humans will look at or care about code.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A narrowing talent pipeline&lt;/h3&gt;  &lt;p&gt;For the foreseeable future, though, humans will still need to understand and maintain the code that underpins their projects. And one of the most pernicious side effects of AI tools may be a shrinking pool of people capable of doing so.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Early evidence suggests that fears around the job-destroying effects of AI may be justified. A recent Stanford University study found that employment among software developers aged 22 to 25 fell nearly 20% between 2022 and 2025, coinciding with the rise of AI-powered coding tools.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_35"&gt;&lt;p&gt;Experienced developers could face difficulties too. Luciano Nooijen, an engineer at the video-game infrastructure developer Companion Group, used AI tools heavily in his day job, where they were provided for free. But when he began a side project without access to those tools, he found himself struggling with tasks that previously came naturally. “I was feeling so stupid because things that used to be instinct became manual, sometimes even cumbersome,” says Nooijen.&lt;/p&gt;  &lt;p&gt;Just as athletes still perform basic drills, he thinks the only way to maintain an instinct for coding is to regularly practice the grunt work. That’s why he’s largely abandoned AI tools, though he admits that deeper motivations are also at play.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Part of the reason Nooijen and other developers MIT Technology Review spoke to are pushing back against AI tools is a sense that they are hollowing out the parts of their jobs that they love. “I got into software engineering because I like working with computers. I like making machines do things that I want,” Nooijen says. “It’s just not fun sitting there with my work being done for me.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1128352/rise-of-ai-coding-developers-2026/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>Deep-learning model predicts how fruit flies form, cell by cell (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-CellPredict-01-press.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During early development, tissues and organs begin to bloom through the shifting, splitting, and growing of many thousands of cells.&lt;/p&gt;&lt;p&gt;A team of MIT engineers has now developed a way to predict, minute by minute, how individual cells will fold, divide, and rearrange during a fruit fly’s earliest stage of growth. The new method may one day be applied to predict the development of more complex tissues, organs, and organisms. It could also help scientists identify cell patterns that correspond to early-onset diseases, such as asthma and cancer.&lt;/p&gt;&lt;p&gt;In a study appearing today in the journal &lt;em&gt;Nature Methods&lt;/em&gt;, the team presents a new deep-learning model that learns, then predicts, how certain geometric properties of individual cells will change as a fruit fly develops. The model records and tracks properties such as a cell’s position, and whether it is touching a neighboring cell at a given moment.&lt;/p&gt;&lt;p&gt;The team applied the model to videos of developing fruit fly embryos, each of which starts as a cluster of about 5,000 cells. They found the model could predict, with 90 percent accuracy, how each of the 5,000 cells would fold, shift, and rearrange, minute by minute, during the first hour of development, as the embryo morphs from a smooth, uniform shape into more defined structures and features.&lt;/p&gt;&lt;p&gt;“This very initial phase is known as gastrulation, which takes place over roughly one hour, when individual cells are rearranging on a time scale of minutes,” says study author Ming Guo, associate professor of mechanical engineering at MIT. “By accurately modeling this early period, we can start to uncover how local cell interactions give rise to global tissues and organisms.”&lt;/p&gt;&lt;p&gt;The researchers hope to apply the model to predict the cell-by-cell development in other species, such zebrafish and mice. Then, they can begin to identify patterns that are common across species. The team also envisions that the method could be used to discern early patterns of disease, such as in asthma. Lung tissue in people with asthma looks markedly different from healthy lung tissue. How asthma-prone tissue initially develops is an unknown process that the team’s new method could potentially reveal.&lt;/p&gt;&lt;p&gt;“Asthmatic tissues show different cell dynamics when imaged live,” says co-author and MIT graduate student Haiqian Yang. “We envision that our model could capture these subtle dynamical differences and provide a more comprehensive representation of tissue behavior, potentially improving diagnostics or drug-screening assays.”&lt;/p&gt;&lt;p&gt;The study’s co-authors are Markus Buehler, the&amp;nbsp;McAfee Professor of Engineering in MIT’s Department of Civil and Environmental Engineering; George Roy and Tomer Stern of the University of Michigan; and Anh Nguyen and Dapeng Bi of Northeastern University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Points and foams&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Scientists typically model how an embryo develops in one of two ways: as a point cloud, where each point represents an individual cell as point that moves over time; or as a “foam,” which represents individual cells as bubbles that shift and slide against each other, similar to the bubbles in shaving foam.&lt;/p&gt;&lt;p&gt;Rather than choose between the two approaches, Guo and Yang embraced both.&lt;/p&gt;&lt;p&gt;“There’s a debate about whether to model as a point cloud or a foam,” Yang says. “But both of them are essentially different ways of modeling the same underlying graph, which is an elegant way to represent living tissues. By combining these as one graph, we can highlight more structural information, like how cells are connected to each other as they rearrange over time.”&lt;/p&gt;&lt;p&gt;At the heart of the new model is a “dual-graph” structure that represents a developing embryo as both moving points and bubbles. Through this dual representation, the researchers hoped to capture more detailed geometric properties of individual cells, such as the location of a cell’s nucleus, whether a cell is touching a neighboring cell, and whether it is folding or dividing at a given moment in time.&lt;/p&gt;&lt;p&gt;As a proof of principle, the team trained the new model to “learn” how individual cells change over time during fruit fly gastrulation.&lt;/p&gt;&lt;p&gt;“The overall shape of the fruit fly at this stage is roughly an ellipsoid, but there are gigantic dynamics going on at the surface during gastrulation,” Guo says. “It goes from entirely smooth to forming a number of folds at different angles. And we want to predict all of those dynamics, moment to moment, and cell by cell.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Where and when&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For their new study, the researchers applied the new model to high-quality videos of fruit fly gastrulation taken by their collaborators at the University of Michigan. The videos are one-hour recordings of developing fruit flies, taken at single-cell resolution. What’s more, the videos contain labels of individual cells’ edges and nuclei — data that are incredibly detailed and difficult to come by.&lt;/p&gt;&lt;p&gt;“These videos are of extremely high quality,” Yang says. “This data is very rare, where you get submicron resolution of the whole 3D volume at a pretty fast frame rate.”&lt;/p&gt;&lt;p&gt;The team trained the new model with data from three of four fruit fly embryo videos, such that the model might “learn” how individual cells interact and change as an embryo develops. They then tested the model on an entirely new fruit fly video, and found that it was able to predict with high accuracy how most of the embryo’s 5,000 cells changed from minute to minute.&lt;/p&gt;&lt;p&gt;Specifically, the model could predict properties of individual cells, such as whether they will fold, divide, or continue sharing an edge with a neighboring cell, with about 90 percent accuracy.&lt;/p&gt;&lt;p&gt;“We end up predicting not only whether these things will happen, but also when,” Guo says. “For instance, will this cell detach from this cell seven minutes from now, or eight? We can tell when that will happen.”&lt;/p&gt;&lt;p&gt;The team believes that, in principle, the new model, and the dual-graph approach, should be able to predict the cell-by-cell development of other multiceullar systems, such as more complex species, and even some human tissues and organs. The limiting factor is the availability of high-quality video data.&lt;/p&gt;&lt;p&gt;“From the model perspective, I think it’s ready,” Guo says. “The real bottleneck is the data. If we have good quality data of specific tissues, the model could be directly applied to predict the development of many more structures.”&lt;/p&gt;&lt;p&gt;This work is supported, in part, by the U.S. National Institutes of Health.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-CellPredict-01-press.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During early development, tissues and organs begin to bloom through the shifting, splitting, and growing of many thousands of cells.&lt;/p&gt;&lt;p&gt;A team of MIT engineers has now developed a way to predict, minute by minute, how individual cells will fold, divide, and rearrange during a fruit fly’s earliest stage of growth. The new method may one day be applied to predict the development of more complex tissues, organs, and organisms. It could also help scientists identify cell patterns that correspond to early-onset diseases, such as asthma and cancer.&lt;/p&gt;&lt;p&gt;In a study appearing today in the journal &lt;em&gt;Nature Methods&lt;/em&gt;, the team presents a new deep-learning model that learns, then predicts, how certain geometric properties of individual cells will change as a fruit fly develops. The model records and tracks properties such as a cell’s position, and whether it is touching a neighboring cell at a given moment.&lt;/p&gt;&lt;p&gt;The team applied the model to videos of developing fruit fly embryos, each of which starts as a cluster of about 5,000 cells. They found the model could predict, with 90 percent accuracy, how each of the 5,000 cells would fold, shift, and rearrange, minute by minute, during the first hour of development, as the embryo morphs from a smooth, uniform shape into more defined structures and features.&lt;/p&gt;&lt;p&gt;“This very initial phase is known as gastrulation, which takes place over roughly one hour, when individual cells are rearranging on a time scale of minutes,” says study author Ming Guo, associate professor of mechanical engineering at MIT. “By accurately modeling this early period, we can start to uncover how local cell interactions give rise to global tissues and organisms.”&lt;/p&gt;&lt;p&gt;The researchers hope to apply the model to predict the cell-by-cell development in other species, such zebrafish and mice. Then, they can begin to identify patterns that are common across species. The team also envisions that the method could be used to discern early patterns of disease, such as in asthma. Lung tissue in people with asthma looks markedly different from healthy lung tissue. How asthma-prone tissue initially develops is an unknown process that the team’s new method could potentially reveal.&lt;/p&gt;&lt;p&gt;“Asthmatic tissues show different cell dynamics when imaged live,” says co-author and MIT graduate student Haiqian Yang. “We envision that our model could capture these subtle dynamical differences and provide a more comprehensive representation of tissue behavior, potentially improving diagnostics or drug-screening assays.”&lt;/p&gt;&lt;p&gt;The study’s co-authors are Markus Buehler, the&amp;nbsp;McAfee Professor of Engineering in MIT’s Department of Civil and Environmental Engineering; George Roy and Tomer Stern of the University of Michigan; and Anh Nguyen and Dapeng Bi of Northeastern University.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Points and foams&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Scientists typically model how an embryo develops in one of two ways: as a point cloud, where each point represents an individual cell as point that moves over time; or as a “foam,” which represents individual cells as bubbles that shift and slide against each other, similar to the bubbles in shaving foam.&lt;/p&gt;&lt;p&gt;Rather than choose between the two approaches, Guo and Yang embraced both.&lt;/p&gt;&lt;p&gt;“There’s a debate about whether to model as a point cloud or a foam,” Yang says. “But both of them are essentially different ways of modeling the same underlying graph, which is an elegant way to represent living tissues. By combining these as one graph, we can highlight more structural information, like how cells are connected to each other as they rearrange over time.”&lt;/p&gt;&lt;p&gt;At the heart of the new model is a “dual-graph” structure that represents a developing embryo as both moving points and bubbles. Through this dual representation, the researchers hoped to capture more detailed geometric properties of individual cells, such as the location of a cell’s nucleus, whether a cell is touching a neighboring cell, and whether it is folding or dividing at a given moment in time.&lt;/p&gt;&lt;p&gt;As a proof of principle, the team trained the new model to “learn” how individual cells change over time during fruit fly gastrulation.&lt;/p&gt;&lt;p&gt;“The overall shape of the fruit fly at this stage is roughly an ellipsoid, but there are gigantic dynamics going on at the surface during gastrulation,” Guo says. “It goes from entirely smooth to forming a number of folds at different angles. And we want to predict all of those dynamics, moment to moment, and cell by cell.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Where and when&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;For their new study, the researchers applied the new model to high-quality videos of fruit fly gastrulation taken by their collaborators at the University of Michigan. The videos are one-hour recordings of developing fruit flies, taken at single-cell resolution. What’s more, the videos contain labels of individual cells’ edges and nuclei — data that are incredibly detailed and difficult to come by.&lt;/p&gt;&lt;p&gt;“These videos are of extremely high quality,” Yang says. “This data is very rare, where you get submicron resolution of the whole 3D volume at a pretty fast frame rate.”&lt;/p&gt;&lt;p&gt;The team trained the new model with data from three of four fruit fly embryo videos, such that the model might “learn” how individual cells interact and change as an embryo develops. They then tested the model on an entirely new fruit fly video, and found that it was able to predict with high accuracy how most of the embryo’s 5,000 cells changed from minute to minute.&lt;/p&gt;&lt;p&gt;Specifically, the model could predict properties of individual cells, such as whether they will fold, divide, or continue sharing an edge with a neighboring cell, with about 90 percent accuracy.&lt;/p&gt;&lt;p&gt;“We end up predicting not only whether these things will happen, but also when,” Guo says. “For instance, will this cell detach from this cell seven minutes from now, or eight? We can tell when that will happen.”&lt;/p&gt;&lt;p&gt;The team believes that, in principle, the new model, and the dual-graph approach, should be able to predict the cell-by-cell development of other multiceullar systems, such as more complex species, and even some human tissues and organs. The limiting factor is the availability of high-quality video data.&lt;/p&gt;&lt;p&gt;“From the model perspective, I think it’s ready,” Guo says. “The real bottleneck is the data. If we have good quality data of specific tissues, the model could be directly applied to predict the development of many more structures.”&lt;/p&gt;&lt;p&gt;This work is supported, in part, by the U.S. National Institutes of Health.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>AI materials discovery now needs to move into the real world (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129210/ai-materials-science-discovery-startups-investment/</link><description>&lt;div class="cst-block "&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;The microwave-size instrument at Lila Sciences in Cambridge, Massachusetts, doesn’t look all that different from others that I’ve seen in state-of-the-art materials labs. Inside its vacuum chamber, the machine zaps a palette of different elements to create vaporized particles, which then fly through the chamber and land to create a thin film, using a technique called sputtering. What sets this instrument apart is that artificial intelligence is running the experiment; an AI agent, trained on vast amounts of scientific literature and data, has determined the recipe and is varying the combination of elements.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Later, a person will walk the samples, each containing multiple potential catalysts, over to a different part of the lab for testing. Another AI agent will scan and interpret the data, using it to suggest another round of experiments to try to optimize the materials’ performance.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;For now, a human scientist keeps a close eye on the experiments and will approve the next steps on the basis of the AI’s suggestions and the test results. But the startup is convinced this AI-controlled machine is a peek into the future of materials discovery—one in which autonomous labs could make it far cheaper and faster to come up with novel and useful compounds.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Flush with hundreds of millions of dollars in new funding, Lila Sciences is one of AI’s latest unicorns. The company is on a larger mission to use AI-run autonomous labs for scientific discovery—the goal is to achieve what it calls scientific superintelligence. But I’m here this morning to learn specifically about the discovery of new materials.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129431" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/658-NYT-Lila-421.jpg?w=2000" width="2000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lila Sciences’ John Gregoire (background) and Rafael Gómez-Bombarelli watch as an AI-guided sputtering instrument makes samples of thin-film alloys.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CODY O’LOUGHLIN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;We desperately need better materials to solve our problems. We’ll need improved electrodes and other parts for more powerful batteries; compounds to more cheaply suck carbon dioxide out of the air; and better catalysts to make green hydrogen and other clean fuels and chemicals. And we will likely need novel materials like higher-temperature superconductors, improved magnets, and different types of semiconductors for a next generation of breakthroughs in everything from quantum computing to fusion power to AI hardware.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But materials science has not had many commercial wins in the last few decades. In part because of its complexity and the lack of successes, the field has become something of an innovation backwater, overshadowed by the more glamorous—and lucrative—search for new drugs and insights into biology.&lt;/p&gt;  &lt;p&gt;The idea of using AI for materials discovery is not exactly new, but it got a huge boost in 2020 when DeepMind showed that its AlphaFold2 model could accurately predict the three-dimensional structure of proteins. Then, in 2022, came the success and popularity of ChatGPT. The hope that similar AI models using deep learning could aid in doing science captivated tech insiders. Why not use our new generative AI capabilities to search the vast chemical landscape and help simulate atomic structures, pointing the way to new substances with amazing properties?&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“Simulations can be super powerful for framing problems and understanding what is worth testing in the lab. But there’s zero problems we can ever solve in the real world with simulation alone.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;John Gregoire, Lila Sciences, chief autonomous science officer&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;Researchers touted an AI model that had reportedly discovered “millions of new materials.” The money began pouring in, funding a host of startups. But so far there has been no “eureka” moment, no ChatGPT-like breakthrough—no discovery of new miracle materials or even slightly better ones.&lt;/p&gt;  &lt;p&gt;The startups that want to find useful new compounds face a common bottleneck: By far the most time-consuming and expensive step in materials discovery is not imagining new structures but making them in the real world. Before trying to synthesize a material, you don’t know if, in fact, it can be made and is stable, and many of its properties remain unknown until you test it in the lab.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“Simulations can be super powerful for kind of framing problems and understanding what is worth testing in the lab,” says John Gregoire, Lila Sciences’ chief autonomous science officer. “But there’s zero problems we can ever solve in the real world with simulation alone.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Startups like Lila Sciences have staked their strategies on using AI to transform experimentation and are building labs that use agents to plan, run, and interpret the results of experiments to synthesize new materials. Automation in laboratories already exists. But the idea is to have AI agents take it to the next level by directing autonomous labs, where their tasks could include designing experiments and controlling the robotics used to shuffle samples around. And, most important, companies want to use AI to vacuum up and analyze the vast amount of data produced by such experiments in the search for clues to better materials.&lt;/p&gt;  &lt;p&gt;If they succeed, these companies could shorten the discovery process from decades to a few years or less, helping uncover new materials and optimize existing ones. But it’s a gamble. Even though AI is already taking over many laboratory chores and tasks, finding new—and useful—materials on its own is another matter entirely.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Innovation backwater&lt;/h3&gt;  &lt;p&gt;I have been reporting about materials discovery for nearly 40 years, and to be honest, there have been only a few memorable commercial breakthroughs, such as lithium-­ion batteries, over that time. There have been plenty of scientific advances to write about, from perovskite solar cells to graphene transistors to metal-­organic frameworks (MOFs), materials based on an intriguing type of molecular architecture that recently won its inventors a Nobel Prize. But few of those advances—including MOFs—have made it far out of the lab. Others, like quantum dots, have found some commercial uses, but in general, the kinds of life-changing inventions created in earlier decades have been lacking.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Blame the amount of time (typically 20 years or more) and the hundreds of millions of dollars it takes to make, test, optimize, and manufacture a new material—and the industry’s lack of interest in spending that kind of time and money in low-margin commodity markets. Or maybe we’ve just run out of ideas for making stuff.&lt;/p&gt;  &lt;p&gt;The need to both speed up that process and find new ideas is the reason researchers have turned to AI. For decades, scientists have used computers to design potential materials, calculating where to place atoms to form structures that are stable and have predictable characteristics. It’s worked—but only kind of. Advances in AI have made that computational modeling far faster and have promised the ability to quickly explore a vast number of possible structures. Google DeepMind, Meta, and Microsoft have all launched efforts to bring AI tools to the problem of designing new materials.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But the limitations that have always plagued computational modeling of new materials remain. With many types of materials, such as crystals, useful characteristics often can’t be predicted solely by calculating atomic structures.&lt;/p&gt;  &lt;p&gt;To uncover and optimize those properties, you need to make something real. Or as Rafael Gómez-Bombarelli, one of Lila’s cofounders and an MIT professor of materials science, puts it: “Structure helps us think about the problem, but it’s neither necessary nor sufficient for real materials problems.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Perhaps no advance exemplified the gap between the virtual and physical worlds more than DeepMind’s announcement in late 2023 that it had used deep learning to discover “millions of new materials,” including 380,000 crystals that it declared “the most stable, making them promising candidates for experimental synthesis.” In technical terms, the arrangement of atoms represented a minimum energy state where they were content to stay put. This was “an order-of-magnitude expansion in stable materials known to humanity,” the DeepMind researchers proclaimed.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;To the AI community, it appeared to be the breakthrough everyone had been waiting for. The DeepMind research not only offered a gold mine of possible new materials, it also created powerful new computational methods for predicting a large number of structures.&lt;/p&gt;  &lt;p&gt;But some materials scientists had a far different reaction. After closer scrutiny, researchers at the University of California, Santa Barbara, said they’d found “scant evidence for compounds that fulfill the trifecta of novelty, credibility, and utility.” In fact, the scientists reported, they didn’t find any truly novel compounds among the ones they looked at; some were merely “trivial” variations of known ones. The scientists appeared particularly peeved that the potential compounds were labeled materials. They wrote: “We would respectfully suggest that the work does not report any new materials but reports a list of proposed compounds. In our view, a compound can be called a material when it exhibits some functionality and, therefore, has potential utility.”&lt;/p&gt;  &lt;p&gt;Some of the imagined crystals simply defied the conditions of the real world. To do computations on so many possible structures, DeepMind researchers simulated them at absolute zero, where atoms are well ordered; they vibrate a bit but don’t move around. At higher temperatures—the kind that would exist in the lab or anywhere in the world—the atoms fly about in complex ways, often creating more disorderly crystal structures. A number of the so-called novel materials predicted by DeepMind appeared to be well-ordered versions of disordered ones that were already known.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;More generally, the DeepMind paper was simply another reminder of how challenging it is to capture physical realities in virtual simulations—at least for now. Because of the limitations of computational power, researchers typically perform calculations on relatively few atoms. Yet many desirable properties are determined by the microstructure of the materials—at a scale much larger than the atomic world. And some effects, like high-temperature superconductivity or even the catalysis that is key to many common industrial processes, are far too complex or poorly understood to be explained by atomic simulations alone.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A common language&lt;/h3&gt;  &lt;p&gt;Even so, there are signs that the divide between simulations and experimental work is beginning to narrow. DeepMind, for one, says that since the release of the 2023 paper it has been working with scientists in labs around the world to synthesize AI-identified compounds and has achieved some success. Meanwhile, a number of the startups entering the space are looking to combine computational and experimental expertise in one organization.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;One such startup is Periodic Labs, cofounded by Ekin Dogus Cubuk, a physicist who led the scientific team that generated the 2023 DeepMind headlines, and by Liam Fedus, a co-creator of ChatGPT at OpenAI. Despite its founders’ background in computational modeling and AI software, the company is building much of its materials discovery strategy around synthesis done in automated labs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The vision behind the startup is to link these different fields of expertise by using large language models that are trained on scientific literature and able to learn from ongoing experiments. An LLM might suggest the recipe and conditions to make a compound; it can also interpret test data and feed additional suggestions to the startup’s chemists and physicists. In this strategy, simulations might suggest possible material candidates, but they are also used to help explain the experimental results and suggest possible structural tweaks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;The grand prize would be a room-temperature superconductor, a material that could transform computing and electricity but that has eluded scientists for decades.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Periodic Labs, like Lila Sciences, has ambitions beyond designing and making new materials. It wants to “create an AI scientist”—specifically, one adept at the physical sciences. “LLMs have gotten quite good at distilling chemistry information, physics information,” says Cubuk, “and now we’re trying to make it more advanced by teaching it how to do science—for example, doing simulations, doing experiments, doing theoretical modeling.”&lt;/p&gt;  &lt;p&gt;The approach, like that of Lila Sciences, is based on the expectation that a better understanding of the science behind materials and their synthesis will lead to clues that could help researchers find a broad range of new ones. One target for Periodic Labs is materials whose properties are defined by quantum effects, such as new types of magnets. The grand prize would be a room-temperature superconductor, a material that could transform computing and electricity but that has eluded scientists for decades.&lt;/p&gt; 
 &lt;p&gt;Superconductors are materials in which electricity flows without any resistance and, thus, without producing heat. So far, the best of these materials become superconducting only at relatively low temperatures and require significant cooling. If they can be made to work at or close to room temperature, they could lead to far more efficient power grids, new types of quantum computers, and even more practical high-speed magnetic-levitation trains.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129432" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/658-NYT-Lila-434.jpg?w=2000" width="2000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lila staff scientist Natalie Page (right), Gómez- Bombarelli, and Gregoire inspect thin-film samples after they come out of the sputtering machine and before they undergo testing.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CODY O’LOUGHLIN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The failure to find a room-­temperature superconductor is one of the great disappointments in materials science over the last few decades. I was there when President Reagan spoke about the technology in 1987, during the peak hype over newly made ceramics that became superconducting at the relatively balmy temperature of 93 Kelvin (that’s −292&amp;nbsp;°F), enthusing that they “bring us to the threshold of a new age.” There was a sense of optimism among the scientists and businesspeople in that packed ballroom at the Washington Hilton as Reagan anticipated “a host of benefits, not least among them a reduced dependence on foreign oil, a cleaner environment, and a stronger national economy.” In retrospect, it might have been one of the last times that we pinned our economic and technical aspirations on a breakthrough in materials.&lt;/p&gt; 
 &lt;p&gt;The promised new age never came. Scientists still have not found a material that becomes superconducting at room temperatures, or anywhere close, under normal conditions.&amp;nbsp;The best existing superconductors are brittle and tend to make lousy wires.&lt;/p&gt;  &lt;p&gt;One of the reasons that finding higher-­temperature superconductors has been so difficult is that no theory explains the effect at relatively high temperatures—or can predict it simply from the placement of atoms in the structure. It will ultimately fall to lab scientists to synthesize any interesting candidates, test them, and search the resulting data for clues to understanding the still puzzling phenomenon. Doing so, says Cubuk, is one of the top priorities of Periodic Labs.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;AI in charge&lt;/h3&gt;  &lt;p&gt;It can take a researcher a year or more to make a crystal structure for the first time. Then there are typically years of further work to test its properties and figure out how to make the larger quantities needed for a commercial product.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_20"&gt; &lt;p&gt;Startups like Lila Sciences and Periodic Labs are pinning their hopes largely on the prospect that AI-directed experiments can slash those times. One reason for the optimism is that many labs have already incorporated a lot of automation, for everything from preparing samples to shuttling test items around. Researchers routinely use robotic arms, software, automated versions of microscopes and other analytical instruments, and mechanized tools for manipulating lab equipment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;p&gt;The automation allows, among other things, for high-throughput synthesis, in which multiple samples with various combinations of ingredients are rapidly created and screened in large batches, greatly speeding up the experiments.&lt;/p&gt;  &lt;p&gt;The idea is that using AI to plan and run such automated synthesis can make it far more systematic and efficient. AI agents, which can collect and analyze far more data than any human possibly could, can use real-time information to vary the ingredients and synthesis conditions until they get a sample with the optimal properties. Such AI-directed labs could do far more experiments than a person and could be far smarter than existing systems for high-throughput synthesis.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But so-called self-driving labs for materials are still a work in progress.&lt;/p&gt;    &lt;p&gt;Many types of materials require solid-­state synthesis, a set of processes that are far more difficult to automate than the liquid-­handling activities that are commonplace in making drugs. You need to prepare and mix powders of multiple inorganic ingredients in the right combination for making, say, a catalyst and then decide how to process the sample to create the desired structure—for example, identifying the right temperature and pressure at which to carry out the synthesis. Even determining what you’ve made can be tricky.&lt;/p&gt; 
 &lt;p&gt;In 2023, the A-Lab at Lawrence Berkeley National Laboratory claimed to be the first fully automated lab to use inorganic powders as starting ingredients. Subsequently, scientists reported that the autonomous lab had used robotics and AI to synthesize and test 41 novel materials, including some predicted in the DeepMind database. Some critics questioned the novelty of what was produced and complained that the automated analysis of the materials was not up to experimental standards, but the Berkeley researchers defended the effort as simply a demonstration of the autonomous system’s potential.&lt;/p&gt;  &lt;p&gt;“How it works today and how we envision it are still somewhat different. There’s just a lot of tool building that needs to be done,” says Gerbrand Ceder, the principal scientist behind the A-Lab.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI agents are already getting good at doing many laboratory chores, from preparing recipes to interpreting some kinds of test data—finding, for example, patterns in a micrograph that might be hidden to the human eye. But Ceder is hoping the technology could soon “capture human decision-making,” analyzing ongoing experiments to make strategic choices on what to do next. For example, his group is working on an improved synthesis agent that would better incorporate what he calls scientists’ “diffused” knowledge—the kind gained from extensive training and experience. “I imagine a world where people build agents around their expertise, and then there’s sort of an uber-model that puts it together,” he says. “The uber-model essentially needs to know what agents it can call on and what they know, or what their expertise is.”&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“In one field that I work in, solid-state batteries, there are 50 papers published every day. And that is just one field that I work in. The A I revolution is about finally gathering all the scientific data we have.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Gerbrand Ceder, principal scientist, A-Lab&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;One of the strengths of AI agents is their ability to devour vast amounts of scientific literature. “In one field that I work in, solid-­state batteries, there are 50 papers published every day. And that is just one field that I work in,” says Ceder. It’s impossible for anyone to keep up. “The AI revolution is about finally gathering all the scientific data we have,” he says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;Last summer, Ceder became the chief science officer at an AI materials discovery startup called Radical AI and took a sabbatical from the University of California, Berkeley, to help set up its self-driving labs in New York City. A slide deck shows the portfolio of different AI agents and generative models meant to help realize Ceder’s vision. If you look closely, you can spot an LLM called the “orchestrator”—it’s what CEO Joseph Krause calls the “head honcho.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;New hope&lt;/h3&gt;  &lt;p&gt;So far, despite the hype around the use of AI to discover new materials and the growing momentum—and money—behind the field, there still has not been a convincing big win. There is no example like the 2016 victory of DeepMind’s AlphaGo over a Go world champion. Or like AlphaFold’s achievement in mastering one of biomedicine’s hardest and most time-consuming chores, predicting 3D structures of proteins.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt; &lt;p&gt;The field of materials discovery is still waiting for its moment. It could come if AI agents can dramatically speed the design or synthesis of practical materials, similar to but better than what we have today. Or maybe the moment will be the discovery of a truly novel one, such as a room-­temperature superconductor.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A hexagonal window in the side of a black box" class="wp-image-1129433" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/658-NYT-Lila-453.jpg?w=2247" width="2247" /&gt;&lt;figcaption class="wp-element-caption"&gt;A small window provides a view of the inside workings of Lila’s sputtering instrument.The startup uses the machine to create a wide variety of experimental samples, including potential materials that could be useful for coatings and catalysts.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CODY O’LOUGHLIN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;With or without such a breakthrough moment, startups face the challenge of trying to turn their scientific achievements into useful materials. The task is particularly difficult because any new materials would likely have to be commercialized in an industry dominated by large incumbents that are not particularly prone to risk-taking.&lt;/p&gt;  &lt;p&gt;Susan Schofer, a tech investor and partner at the venture capital firm SOSV, is cautiously optimistic about the field. But Schofer, who spent several years in the mid-2000s as a catalyst researcher at one of the first startups using automation and high-throughput screening for materials discovery (it didn’t survive), wants to see some evidence that the technology can translate into commercial successes when she evaluates startups to invest in.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In particular, she wants to see evidence that the AI startups are already “finding something new, that’s different, and know how they are going to iterate from there.” And she wants to see a business model that captures the value of new materials. She says, “I think the ideal would be: I got a spec from the industry. I know what their problem is. We’ve defined it. Now we’re going to go build it. Now we have a new material that we can sell, that we have scaled up enough that we’ve proven it. And then we partner somehow to manufacture it, but we get revenue off selling the material.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_29"&gt; &lt;p&gt;Schofer says that while she gets the vision of trying to redefine science, she’d advise startups to “show us how you’re going to get there.” She adds, “Let’s see the first steps.”&lt;/p&gt;  &lt;p&gt;Demonstrating those first steps could be essential in enticing large existing materials companies to embrace AI technologies more fully. Corporate researchers in the industry have been burned before—by the promise over the decades that increasingly powerful computers will magically design new materials; by combinatorial chemistry, a fad that raced through materials R&amp;amp;D labs in the early 2000s with little tangible result; and by the promise that synthetic biology would make our next generation of chemicals and materials.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_31"&gt; &lt;p&gt;More recently, the materials community has been blanketed by a new hype cycle around AI. Some of that hype was fueled by the 2023 DeepMind announcement of the discovery of “millions of new materials,” a claim that, in retrospect, clearly overpromised. And it was further fueled when an MIT economics student posted a paper in late 2024 claiming that a large, unnamed corporate R&amp;amp;D lab had used AI to efficiently invent a slew of new materials. AI, it seemed, was already revolutionizing the industry.&lt;/p&gt;  &lt;p&gt;A few months later, the MIT economics department concluded that “the paper should be withdrawn from public discourse.” Two prominent MIT economists who are acknowledged in a footnote in the paper added that they had “no confidence in the provenance, reliability or validity of the data and the veracity of the research.”&lt;/p&gt;  &lt;p&gt;Can AI move beyond the hype and false hopes and truly transform materials discovery? Maybe. There is ample evidence that it’s changing how materials scientists work, providing them—if nothing else—with useful lab tools. Researchers are increasingly using LLMs to query the scientific literature and spot patterns in experimental data.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But it’s still early days in turning those AI tools into actual materials discoveries. The use of AI to run autonomous labs, in particular, is just getting underway; making and testing stuff takes time and lots of money. The morning I visited Lila Sciences, its labs were largely empty, and it’s now preparing to move into a much larger space a few miles away. Periodic Labs is just beginning to set up its lab in San Francisco. It’s starting with manual synthesis guided by AI predictions; its robotic high-throughput lab will come soon. Radical AI reports that its lab is almost fully autonomous but plans to soon move to a larger space.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_33"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129434" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/20250921_PERIODIC_LABS_0111.jpg?w=2393" width="2393" /&gt;&lt;figcaption class="wp-element-caption"&gt;Prominent AI researchers Liam Fedus (left) and Ekin Dogus Cubuk are the cofounders of Periodic Labs. The San Francisco–based startup aims to build an AI scientist that’s adept at the physical sciences.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JASON HENRY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;When I talk to the scientific founders of these startups, I hear a renewed excitement about a field that long operated in the shadows of drug discovery and genomic medicine. For one thing, there is the money. “You see this enormous enthusiasm to put AI and materials together,” says Ceder. “I’ve never seen this much money flow into materials.”&lt;/p&gt;  &lt;p&gt;Reviving the materials industry is a challenge that goes beyond scientific advances, however. It means selling companies on a whole new way of doing R&amp;amp;D.&lt;/p&gt;  &lt;p&gt;But the startups benefit from a huge dose of confidence borrowed from the rest of the AI industry. And maybe that, after years of playing it safe, is just what the materials business needs.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="cst-block "&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;The microwave-size instrument at Lila Sciences in Cambridge, Massachusetts, doesn’t look all that different from others that I’ve seen in state-of-the-art materials labs. Inside its vacuum chamber, the machine zaps a palette of different elements to create vaporized particles, which then fly through the chamber and land to create a thin film, using a technique called sputtering. What sets this instrument apart is that artificial intelligence is running the experiment; an AI agent, trained on vast amounts of scientific literature and data, has determined the recipe and is varying the combination of elements.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Later, a person will walk the samples, each containing multiple potential catalysts, over to a different part of the lab for testing. Another AI agent will scan and interpret the data, using it to suggest another round of experiments to try to optimize the materials’ performance.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;For now, a human scientist keeps a close eye on the experiments and will approve the next steps on the basis of the AI’s suggestions and the test results. But the startup is convinced this AI-controlled machine is a peek into the future of materials discovery—one in which autonomous labs could make it far cheaper and faster to come up with novel and useful compounds.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt; &lt;p&gt;Flush with hundreds of millions of dollars in new funding, Lila Sciences is one of AI’s latest unicorns. The company is on a larger mission to use AI-run autonomous labs for scientific discovery—the goal is to achieve what it calls scientific superintelligence. But I’m here this morning to learn specifically about the discovery of new materials.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129431" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/658-NYT-Lila-421.jpg?w=2000" width="2000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lila Sciences’ John Gregoire (background) and Rafael Gómez-Bombarelli watch as an AI-guided sputtering instrument makes samples of thin-film alloys.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CODY O’LOUGHLIN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;We desperately need better materials to solve our problems. We’ll need improved electrodes and other parts for more powerful batteries; compounds to more cheaply suck carbon dioxide out of the air; and better catalysts to make green hydrogen and other clean fuels and chemicals. And we will likely need novel materials like higher-temperature superconductors, improved magnets, and different types of semiconductors for a next generation of breakthroughs in everything from quantum computing to fusion power to AI hardware.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But materials science has not had many commercial wins in the last few decades. In part because of its complexity and the lack of successes, the field has become something of an innovation backwater, overshadowed by the more glamorous—and lucrative—search for new drugs and insights into biology.&lt;/p&gt;  &lt;p&gt;The idea of using AI for materials discovery is not exactly new, but it got a huge boost in 2020 when DeepMind showed that its AlphaFold2 model could accurately predict the three-dimensional structure of proteins. Then, in 2022, came the success and popularity of ChatGPT. The hope that similar AI models using deep learning could aid in doing science captivated tech insiders. Why not use our new generative AI capabilities to search the vast chemical landscape and help simulate atomic structures, pointing the way to new substances with amazing properties?&lt;/p&gt; 
 &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“Simulations can be super powerful for framing problems and understanding what is worth testing in the lab. But there’s zero problems we can ever solve in the real world with simulation alone.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;John Gregoire, Lila Sciences, chief autonomous science officer&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;Researchers touted an AI model that had reportedly discovered “millions of new materials.” The money began pouring in, funding a host of startups. But so far there has been no “eureka” moment, no ChatGPT-like breakthrough—no discovery of new miracle materials or even slightly better ones.&lt;/p&gt;  &lt;p&gt;The startups that want to find useful new compounds face a common bottleneck: By far the most time-consuming and expensive step in materials discovery is not imagining new structures but making them in the real world. Before trying to synthesize a material, you don’t know if, in fact, it can be made and is stable, and many of its properties remain unknown until you test it in the lab.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;“Simulations can be super powerful for kind of framing problems and understanding what is worth testing in the lab,” says John Gregoire, Lila Sciences’ chief autonomous science officer. “But there’s zero problems we can ever solve in the real world with simulation alone.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Startups like Lila Sciences have staked their strategies on using AI to transform experimentation and are building labs that use agents to plan, run, and interpret the results of experiments to synthesize new materials. Automation in laboratories already exists. But the idea is to have AI agents take it to the next level by directing autonomous labs, where their tasks could include designing experiments and controlling the robotics used to shuffle samples around. And, most important, companies want to use AI to vacuum up and analyze the vast amount of data produced by such experiments in the search for clues to better materials.&lt;/p&gt;  &lt;p&gt;If they succeed, these companies could shorten the discovery process from decades to a few years or less, helping uncover new materials and optimize existing ones. But it’s a gamble. Even though AI is already taking over many laboratory chores and tasks, finding new—and useful—materials on its own is another matter entirely.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Innovation backwater&lt;/h3&gt;  &lt;p&gt;I have been reporting about materials discovery for nearly 40 years, and to be honest, there have been only a few memorable commercial breakthroughs, such as lithium-­ion batteries, over that time. There have been plenty of scientific advances to write about, from perovskite solar cells to graphene transistors to metal-­organic frameworks (MOFs), materials based on an intriguing type of molecular architecture that recently won its inventors a Nobel Prize. But few of those advances—including MOFs—have made it far out of the lab. Others, like quantum dots, have found some commercial uses, but in general, the kinds of life-changing inventions created in earlier decades have been lacking.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Blame the amount of time (typically 20 years or more) and the hundreds of millions of dollars it takes to make, test, optimize, and manufacture a new material—and the industry’s lack of interest in spending that kind of time and money in low-margin commodity markets. Or maybe we’ve just run out of ideas for making stuff.&lt;/p&gt;  &lt;p&gt;The need to both speed up that process and find new ideas is the reason researchers have turned to AI. For decades, scientists have used computers to design potential materials, calculating where to place atoms to form structures that are stable and have predictable characteristics. It’s worked—but only kind of. Advances in AI have made that computational modeling far faster and have promised the ability to quickly explore a vast number of possible structures. Google DeepMind, Meta, and Microsoft have all launched efforts to bring AI tools to the problem of designing new materials.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But the limitations that have always plagued computational modeling of new materials remain. With many types of materials, such as crystals, useful characteristics often can’t be predicted solely by calculating atomic structures.&lt;/p&gt;  &lt;p&gt;To uncover and optimize those properties, you need to make something real. Or as Rafael Gómez-Bombarelli, one of Lila’s cofounders and an MIT professor of materials science, puts it: “Structure helps us think about the problem, but it’s neither necessary nor sufficient for real materials problems.”&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Perhaps no advance exemplified the gap between the virtual and physical worlds more than DeepMind’s announcement in late 2023 that it had used deep learning to discover “millions of new materials,” including 380,000 crystals that it declared “the most stable, making them promising candidates for experimental synthesis.” In technical terms, the arrangement of atoms represented a minimum energy state where they were content to stay put. This was “an order-of-magnitude expansion in stable materials known to humanity,” the DeepMind researchers proclaimed.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;To the AI community, it appeared to be the breakthrough everyone had been waiting for. The DeepMind research not only offered a gold mine of possible new materials, it also created powerful new computational methods for predicting a large number of structures.&lt;/p&gt;  &lt;p&gt;But some materials scientists had a far different reaction. After closer scrutiny, researchers at the University of California, Santa Barbara, said they’d found “scant evidence for compounds that fulfill the trifecta of novelty, credibility, and utility.” In fact, the scientists reported, they didn’t find any truly novel compounds among the ones they looked at; some were merely “trivial” variations of known ones. The scientists appeared particularly peeved that the potential compounds were labeled materials. They wrote: “We would respectfully suggest that the work does not report any new materials but reports a list of proposed compounds. In our view, a compound can be called a material when it exhibits some functionality and, therefore, has potential utility.”&lt;/p&gt;  &lt;p&gt;Some of the imagined crystals simply defied the conditions of the real world. To do computations on so many possible structures, DeepMind researchers simulated them at absolute zero, where atoms are well ordered; they vibrate a bit but don’t move around. At higher temperatures—the kind that would exist in the lab or anywhere in the world—the atoms fly about in complex ways, often creating more disorderly crystal structures. A number of the so-called novel materials predicted by DeepMind appeared to be well-ordered versions of disordered ones that were already known.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;More generally, the DeepMind paper was simply another reminder of how challenging it is to capture physical realities in virtual simulations—at least for now. Because of the limitations of computational power, researchers typically perform calculations on relatively few atoms. Yet many desirable properties are determined by the microstructure of the materials—at a scale much larger than the atomic world. And some effects, like high-temperature superconductivity or even the catalysis that is key to many common industrial processes, are far too complex or poorly understood to be explained by atomic simulations alone.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A common language&lt;/h3&gt;  &lt;p&gt;Even so, there are signs that the divide between simulations and experimental work is beginning to narrow. DeepMind, for one, says that since the release of the 2023 paper it has been working with scientists in labs around the world to synthesize AI-identified compounds and has achieved some success. Meanwhile, a number of the startups entering the space are looking to combine computational and experimental expertise in one organization.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;One such startup is Periodic Labs, cofounded by Ekin Dogus Cubuk, a physicist who led the scientific team that generated the 2023 DeepMind headlines, and by Liam Fedus, a co-creator of ChatGPT at OpenAI. Despite its founders’ background in computational modeling and AI software, the company is building much of its materials discovery strategy around synthesis done in automated labs.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The vision behind the startup is to link these different fields of expertise by using large language models that are trained on scientific literature and able to learn from ongoing experiments. An LLM might suggest the recipe and conditions to make a compound; it can also interpret test data and feed additional suggestions to the startup’s chemists and physicists. In this strategy, simulations might suggest possible material candidates, but they are also used to help explain the experimental results and suggest possible structural tweaks.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;The grand prize would be a room-temperature superconductor, a material that could transform computing and electricity but that has eluded scientists for decades.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;Periodic Labs, like Lila Sciences, has ambitions beyond designing and making new materials. It wants to “create an AI scientist”—specifically, one adept at the physical sciences. “LLMs have gotten quite good at distilling chemistry information, physics information,” says Cubuk, “and now we’re trying to make it more advanced by teaching it how to do science—for example, doing simulations, doing experiments, doing theoretical modeling.”&lt;/p&gt;  &lt;p&gt;The approach, like that of Lila Sciences, is based on the expectation that a better understanding of the science behind materials and their synthesis will lead to clues that could help researchers find a broad range of new ones. One target for Periodic Labs is materials whose properties are defined by quantum effects, such as new types of magnets. The grand prize would be a room-temperature superconductor, a material that could transform computing and electricity but that has eluded scientists for decades.&lt;/p&gt; 
 &lt;p&gt;Superconductors are materials in which electricity flows without any resistance and, thus, without producing heat. So far, the best of these materials become superconducting only at relatively low temperatures and require significant cooling. If they can be made to work at or close to room temperature, they could lead to far more efficient power grids, new types of quantum computers, and even more practical high-speed magnetic-levitation trains.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129432" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/658-NYT-Lila-434.jpg?w=2000" width="2000" /&gt;&lt;figcaption class="wp-element-caption"&gt;Lila staff scientist Natalie Page (right), Gómez- Bombarelli, and Gregoire inspect thin-film samples after they come out of the sputtering machine and before they undergo testing.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CODY O’LOUGHLIN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The failure to find a room-­temperature superconductor is one of the great disappointments in materials science over the last few decades. I was there when President Reagan spoke about the technology in 1987, during the peak hype over newly made ceramics that became superconducting at the relatively balmy temperature of 93 Kelvin (that’s −292&amp;nbsp;°F), enthusing that they “bring us to the threshold of a new age.” There was a sense of optimism among the scientists and businesspeople in that packed ballroom at the Washington Hilton as Reagan anticipated “a host of benefits, not least among them a reduced dependence on foreign oil, a cleaner environment, and a stronger national economy.” In retrospect, it might have been one of the last times that we pinned our economic and technical aspirations on a breakthrough in materials.&lt;/p&gt; 
 &lt;p&gt;The promised new age never came. Scientists still have not found a material that becomes superconducting at room temperatures, or anywhere close, under normal conditions.&amp;nbsp;The best existing superconductors are brittle and tend to make lousy wires.&lt;/p&gt;  &lt;p&gt;One of the reasons that finding higher-­temperature superconductors has been so difficult is that no theory explains the effect at relatively high temperatures—or can predict it simply from the placement of atoms in the structure. It will ultimately fall to lab scientists to synthesize any interesting candidates, test them, and search the resulting data for clues to understanding the still puzzling phenomenon. Doing so, says Cubuk, is one of the top priorities of Periodic Labs.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;AI in charge&lt;/h3&gt;  &lt;p&gt;It can take a researcher a year or more to make a crystal structure for the first time. Then there are typically years of further work to test its properties and figure out how to make the larger quantities needed for a commercial product.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_20"&gt; &lt;p&gt;Startups like Lila Sciences and Periodic Labs are pinning their hopes largely on the prospect that AI-directed experiments can slash those times. One reason for the optimism is that many labs have already incorporated a lot of automation, for everything from preparing samples to shuttling test items around. Researchers routinely use robotic arms, software, automated versions of microscopes and other analytical instruments, and mechanized tools for manipulating lab equipment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;p&gt;The automation allows, among other things, for high-throughput synthesis, in which multiple samples with various combinations of ingredients are rapidly created and screened in large batches, greatly speeding up the experiments.&lt;/p&gt;  &lt;p&gt;The idea is that using AI to plan and run such automated synthesis can make it far more systematic and efficient. AI agents, which can collect and analyze far more data than any human possibly could, can use real-time information to vary the ingredients and synthesis conditions until they get a sample with the optimal properties. Such AI-directed labs could do far more experiments than a person and could be far smarter than existing systems for high-throughput synthesis.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But so-called self-driving labs for materials are still a work in progress.&lt;/p&gt;    &lt;p&gt;Many types of materials require solid-­state synthesis, a set of processes that are far more difficult to automate than the liquid-­handling activities that are commonplace in making drugs. You need to prepare and mix powders of multiple inorganic ingredients in the right combination for making, say, a catalyst and then decide how to process the sample to create the desired structure—for example, identifying the right temperature and pressure at which to carry out the synthesis. Even determining what you’ve made can be tricky.&lt;/p&gt; 
 &lt;p&gt;In 2023, the A-Lab at Lawrence Berkeley National Laboratory claimed to be the first fully automated lab to use inorganic powders as starting ingredients. Subsequently, scientists reported that the autonomous lab had used robotics and AI to synthesize and test 41 novel materials, including some predicted in the DeepMind database. Some critics questioned the novelty of what was produced and complained that the automated analysis of the materials was not up to experimental standards, but the Berkeley researchers defended the effort as simply a demonstration of the autonomous system’s potential.&lt;/p&gt;  &lt;p&gt;“How it works today and how we envision it are still somewhat different. There’s just a lot of tool building that needs to be done,” says Gerbrand Ceder, the principal scientist behind the A-Lab.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;AI agents are already getting good at doing many laboratory chores, from preparing recipes to interpreting some kinds of test data—finding, for example, patterns in a micrograph that might be hidden to the human eye. But Ceder is hoping the technology could soon “capture human decision-making,” analyzing ongoing experiments to make strategic choices on what to do next. For example, his group is working on an improved synthesis agent that would better incorporate what he calls scientists’ “diffused” knowledge—the kind gained from extensive training and experience. “I imagine a world where people build agents around their expertise, and then there’s sort of an uber-model that puts it together,” he says. “The uber-model essentially needs to know what agents it can call on and what they know, or what their expertise is.”&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;“In one field that I work in, solid-state batteries, there are 50 papers published every day. And that is just one field that I work in. The A I revolution is about finally gathering all the scientific data we have.”&lt;/strong&gt;&lt;/p&gt; &lt;cite&gt;Gerbrand Ceder, principal scientist, A-Lab&lt;/cite&gt;&lt;/blockquote&gt;  &lt;p&gt;One of the strengths of AI agents is their ability to devour vast amounts of scientific literature. “In one field that I work in, solid-­state batteries, there are 50 papers published every day. And that is just one field that I work in,” says Ceder. It’s impossible for anyone to keep up. “The AI revolution is about finally gathering all the scientific data we have,” he says.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;Last summer, Ceder became the chief science officer at an AI materials discovery startup called Radical AI and took a sabbatical from the University of California, Berkeley, to help set up its self-driving labs in New York City. A slide deck shows the portfolio of different AI agents and generative models meant to help realize Ceder’s vision. If you look closely, you can spot an LLM called the “orchestrator”—it’s what CEO Joseph Krause calls the “head honcho.”&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;New hope&lt;/h3&gt;  &lt;p&gt;So far, despite the hype around the use of AI to discover new materials and the growing momentum—and money—behind the field, there still has not been a convincing big win. There is no example like the 2016 victory of DeepMind’s AlphaGo over a Go world champion. Or like AlphaFold’s achievement in mastering one of biomedicine’s hardest and most time-consuming chores, predicting 3D structures of proteins.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt; &lt;p&gt;The field of materials discovery is still waiting for its moment. It could come if AI agents can dramatically speed the design or synthesis of practical materials, similar to but better than what we have today. Or maybe the moment will be the discovery of a truly novel one, such as a room-­temperature superconductor.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="A hexagonal window in the side of a black box" class="wp-image-1129433" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/658-NYT-Lila-453.jpg?w=2247" width="2247" /&gt;&lt;figcaption class="wp-element-caption"&gt;A small window provides a view of the inside workings of Lila’s sputtering instrument.The startup uses the machine to create a wide variety of experimental samples, including potential materials that could be useful for coatings and catalysts.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;CODY O’LOUGHLIN&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;With or without such a breakthrough moment, startups face the challenge of trying to turn their scientific achievements into useful materials. The task is particularly difficult because any new materials would likely have to be commercialized in an industry dominated by large incumbents that are not particularly prone to risk-taking.&lt;/p&gt;  &lt;p&gt;Susan Schofer, a tech investor and partner at the venture capital firm SOSV, is cautiously optimistic about the field. But Schofer, who spent several years in the mid-2000s as a catalyst researcher at one of the first startups using automation and high-throughput screening for materials discovery (it didn’t survive), wants to see some evidence that the technology can translate into commercial successes when she evaluates startups to invest in.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In particular, she wants to see evidence that the AI startups are already “finding something new, that’s different, and know how they are going to iterate from there.” And she wants to see a business model that captures the value of new materials. She says, “I think the ideal would be: I got a spec from the industry. I know what their problem is. We’ve defined it. Now we’re going to go build it. Now we have a new material that we can sell, that we have scaled up enough that we’ve proven it. And then we partner somehow to manufacture it, but we get revenue off selling the material.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_29"&gt; &lt;p&gt;Schofer says that while she gets the vision of trying to redefine science, she’d advise startups to “show us how you’re going to get there.” She adds, “Let’s see the first steps.”&lt;/p&gt;  &lt;p&gt;Demonstrating those first steps could be essential in enticing large existing materials companies to embrace AI technologies more fully. Corporate researchers in the industry have been burned before—by the promise over the decades that increasingly powerful computers will magically design new materials; by combinatorial chemistry, a fad that raced through materials R&amp;amp;D labs in the early 2000s with little tangible result; and by the promise that synthetic biology would make our next generation of chemicals and materials.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_31"&gt; &lt;p&gt;More recently, the materials community has been blanketed by a new hype cycle around AI. Some of that hype was fueled by the 2023 DeepMind announcement of the discovery of “millions of new materials,” a claim that, in retrospect, clearly overpromised. And it was further fueled when an MIT economics student posted a paper in late 2024 claiming that a large, unnamed corporate R&amp;amp;D lab had used AI to efficiently invent a slew of new materials. AI, it seemed, was already revolutionizing the industry.&lt;/p&gt;  &lt;p&gt;A few months later, the MIT economics department concluded that “the paper should be withdrawn from public discourse.” Two prominent MIT economists who are acknowledged in a footnote in the paper added that they had “no confidence in the provenance, reliability or validity of the data and the veracity of the research.”&lt;/p&gt;  &lt;p&gt;Can AI move beyond the hype and false hopes and truly transform materials discovery? Maybe. There is ample evidence that it’s changing how materials scientists work, providing them—if nothing else—with useful lab tools. Researchers are increasingly using LLMs to query the scientific literature and spot patterns in experimental data.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But it’s still early days in turning those AI tools into actual materials discoveries. The use of AI to run autonomous labs, in particular, is just getting underway; making and testing stuff takes time and lots of money. The morning I visited Lila Sciences, its labs were largely empty, and it’s now preparing to move into a much larger space a few miles away. Periodic Labs is just beginning to set up its lab in San Francisco. It’s starting with manual synthesis guided by AI predictions; its robotic high-throughput lab will come soon. Radical AI reports that its lab is almost fully autonomous but plans to soon move to a larger space.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_33"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129434" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/20250921_PERIODIC_LABS_0111.jpg?w=2393" width="2393" /&gt;&lt;figcaption class="wp-element-caption"&gt;Prominent AI researchers Liam Fedus (left) and Ekin Dogus Cubuk are the cofounders of Periodic Labs. The San Francisco–based startup aims to build an AI scientist that’s adept at the physical sciences.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;JASON HENRY&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;When I talk to the scientific founders of these startups, I hear a renewed excitement about a field that long operated in the shadows of drug discovery and genomic medicine. For one thing, there is the money. “You see this enormous enthusiasm to put AI and materials together,” says Ceder. “I’ve never seen this much money flow into materials.”&lt;/p&gt;  &lt;p&gt;Reviving the materials industry is a challenge that goes beyond scientific advances, however. It means selling companies on a whole new way of doing R&amp;amp;D.&lt;/p&gt;  &lt;p&gt;But the startups benefit from a huge dose of confidence borrowed from the rest of the AI industry. And maybe that, after years of playing it safe, is just what the materials business needs.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129210/ai-materials-science-discovery-startups-investment/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>What even is the AI bubble? (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129183/what-even-is-the-ai-bubble/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-7-super.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt; Explains: Let our writers untangle the complex, messy world of technology to help you &lt;/em&gt;understand&lt;em&gt; what’s coming next. You can read &lt;/em&gt;&lt;em&gt;more from the series here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In July, a widely cited MIT study claimed that 95% of organizations that invested in generative AI were getting “zero return.” Tech stocks briefly plunged. While the study itself was more nuanced than the headlines, for many it still felt like the first hard data point confirming what skeptics had muttered for months: Hype around AI might be outpacing reality.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Then, in August, OpenAI CEO Sam Altman said what everyone in Silicon Valley had been whispering. “Are we in a phase where investors as a whole are overexcited about AI?” he said during a press dinner I attended. “My opinion is yes.”&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt; 
 &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;He compared the current moment to the dot-com bubble. “When bubbles happen, smart people get overexcited about a kernel of truth,” he explained. “Tech was really important. The internet was a really big deal. People got overexcited.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;With those comments, it was off to the races. The next day’s stock market dip was attributed to the sentiment he shared. The question “Are we in an AI bubble?” became inescapable.&lt;/p&gt; 
 &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Who thinks it is a bubble?&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;The short answer: Lots of people. But not everyone agrees on who or what is overinflated. Tech leaders are using this moment of fear to take shots at their rivals and position themselves as clear winners on the other side. How they describe the bubble depends on where their company sits.&lt;/p&gt;  &lt;p&gt;When I asked Meta CEO Mark Zuckerberg about the AI bubble in September, he ran through the historical analogies of past bubbles—railroads, fiber for the internet, the dot-com boom—and noted that in each case, “the infrastructure gets built out, people take on too much debt, and then you hit some blip ... and then a lot of the companies end up going out of business.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But Zuckerberg’s prescription wasn’t for Meta to pump the brakes. It was to keep spending: “If we end up misspending a couple of hundred billion dollars, I think that that is going to be very unfortunate, obviously. But I’d say the risk is higher on the other side.”&lt;/p&gt;  &lt;p&gt;Bret Taylor, the chairman of OpenAI and CEO of the AI startup Sierra, uses a mental model from the late ’90s to help navigate this AI bubble. “I think the closest analogue to this AI wave is the dot-com boom or bubble, depending on your level of pessimism,” he recently told me. Back then, he explained, everyone knew e-commerce was going to be big, but there was a massive difference between Buy.com and Amazon. Taylor and others have been trying to position themselves as today’s Amazon.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Still others are arguing that the pain will be widespread. Google CEO Sundar Pichai told the BBC this month that there’s “some irrationality” in the current boom. Asked whether Google would be immune to a bubble bursting, he warned, “I think no company is going to be immune, including us."&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;What’s inflating the bubble?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;Companies are raising enormous sums of money and seeing unprecedented valuations. Much of that money, in turn, is going toward the buildout of massive data centers—on which both private companies like OpenAI and Elon Musk’s xAI and public ones such as Meta and Google are spending heavily. OpenAI has pledged that it will spend $500 billion to build AI data centers, more than 15 times what was spent on the Manhattan Project.&lt;/p&gt;  &lt;p&gt;This eye-popping spending on AI data centers isn’t entirely detached from reality. The leaders of the top AI companies all stress that they’re bottlenecked by their limited access to computing power. You hear it constantly when you talk to them. Startups can’t get the GPU allocations they need. Hyperscalers are rationing compute, saving it for their best customers.&lt;/p&gt;  &lt;p&gt;If today’s AI market is as brutally supply-constrained as tech leaders claim, perhaps aggressive infrastructure buildouts are warranted. But some of the numbers are too large to comprehend. Sam Altman has told employees that OpenAI’s moonshot goal is to build 250 gigawatts of computing capacity by 2033, roughly equaling India’s total national electricity demand. Such a plan would cost more than $12 trillion by today’s standards.&lt;/p&gt; 

 &lt;p&gt;“I do think there’s real execution risk,” OpenAI president and cofounder Greg Brockman recently told me about the company’s aggressive infrastructure goals. “Everything we say about the future, we see that it’s a possibility. It is not a certainty, but I don’t think the uncertainty comes from scientific questions. It’s a lot of hard work.”&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Who is exposed, and who is to blame?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;It depends on who you ask. During the August press dinner, where he made his market-moving comments, Altman was blunt about where he sees the excess. He said it’s “insane” that some AI startups with “three people and an idea” are receiving funding at such high valuations. “That’s not rational behavior,” he said. “Someone’s gonna get burned there, I think.” As Safe Superintelligence cofounder (and former OpenAI chief scientist and cofounder) Ilya Sutskever put it on a recent podcast: Silicon Valley has “more companies than ideas.”&lt;/p&gt;  &lt;p&gt;Demis Hassabis, the CEO of Google DeepMind, offered a similar diagnosis when I spoke with him in November. “It feels like there’s obviously a bubble in the private market,” he said. “You look at seed rounds with just nothing being tens of billions of dollars. That seems a little unsustainable."&lt;/p&gt;  &lt;p&gt;Anthropic CEO Dario Amodei also struck at his competition during the&lt;em&gt; New York Times &lt;/em&gt;DealBook Summit in early December. He said he feels confident about the technology itself but worries about how others are behaving on the business side: “On the economic side, I have my concerns where, even if the technology fulfills all its promises, I think there are players in the ecosystem who, if they just make a timing error, they just get it off by a little bit, bad things could happen."&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;He stopped short of naming Sam Altman and OpenAI, but the implication was clear. “There are some players who are YOLOing,” he said. “Let’s say you’re a person who just kind of constitutionally wants to YOLO things or just likes big numbers. Then you may turn the dial too far.”&lt;/p&gt;  &lt;p&gt;Amodei also flagged “circular deals,” or the increasingly common arrangements where chip suppliers like Nvidia invest in AI companies that then turn around and spend those funds on their chips. Anthropic has done some of these, he said, though “not at the same scale as some other players.” (OpenAI is at the center of a number of such deals, as are Nvidia, CoreWeave, and a roster of other players.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The danger, he explained, comes when the numbers get too big: “If you start stacking these where they get to huge amounts of money, and you’re saying, ’By 2027 or 2028 I need to make $200 billion a year,’ then yeah, you can overextend yourself.”&lt;/p&gt;  &lt;p&gt;Zuckerberg shared a similar message at an internal employee Q&amp;amp;A session after Meta’s last earnings call. He noted that unprofitable startups like OpenAI and Anthropic risk bankruptcy if they misjudge the timing of their investments, but Meta has the advantage of strong cash flow, he reassured staff.&lt;/p&gt; 
 &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;How could a bubble burst?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;My conversations with tech executives and investors suggest that the bubble will be most likely to pop if overfunded startups can’t turn a profit or grow into their lofty valuations. This bubble could last longer than than past ones, given that private markets aren’t traded on public markets and therefore move more slowly, but the ripple effects will still be profound when the end comes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If companies making grand commitments to data center buildouts no longer have the revenue growth to support them, the headline deals that have propped up the stock market come into question. Anthropic’s Amodei illustrated the problem during his DealBook Summit appearance, where he said the multi-year data center commitments he has to make combine with the company’s rapid, unpredictable revenue growth rate to create a “cone of uncertainty” about how much to spend.&lt;/p&gt; 
 &lt;p&gt;The two most prominent private players in AI, OpenAI and Anthropic, have yet to turn a profit. A recent Deutsche Bank chart put the situation in stark historical context. Amazon burned through $3 billion before becoming profitable. Tesla, around $4 billion. Uber, $30 billion. OpenAI is projected to burn through $140 billion by 2029, while Anthropic is expected to burn $20 billion by 2027.&lt;/p&gt;  &lt;p&gt;Consultants at Bain estimate that the wave of AI infrastructure spending will require $2 trillion in annual AI revenue by 2030 just to justify the investment. That’s more than the combined 2024 revenue of Amazon, Apple, Alphabet, Microsoft, Meta, and Nvidia. When I talk to leaders of these large tech companies, they all agree that their sprawling businesses can absorb an expensive miscalculation about the returns from their AI infrastructure buildouts. It’s all the other companies that are either highly leveraged with debt or just unprofitable—even OpenAI and Anthropic—that they worry about.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;Still, given the level of spending on AI, it still needs a viable business model beyond subscriptions, which won’t be able to&amp;nbsp; drive profits from billions of people’s eyeballs like the ad-driven businesses that have defined the last 20 years of the internet. Even the largest tech companies know they need to ship the world-changing agents they keep hyping: AI that can fully replace coworkers and complete tasks in the real world. &lt;/p&gt;  &lt;p&gt;For now, investors are mostly buying into the hype of the powerful AI systems that these data center buildouts will supposedly unlock in the future. At some point the biggest spenders, like OpenAI, will need to show investors that the money spent on the infrastructure buildout was worth it.&lt;/p&gt;  &lt;p&gt;There’s also still a lot of uncertainty about the technical direction that AI is heading in. LLMs are expected to remain critical to more advanced AI systems, but industry leaders can’t seem to agree on which additional breakthroughs are needed to achieve artificial general intelligence, or AGI. Some are betting on new kinds of AI that can understand the physical world, while others are focused on training AI to learn in a general way, like a human. In other words, what if all this unprecedented spending turns out to have been backing the wrong horse?&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;The question now&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;What makes this moment surreal is the honesty. The same people pouring billions into AI will openly tell you it might all come crashing down.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Taylor framed it as two truths existing at once. “I think it is both true that AI will transform the economy,” he told me, “and I think we’re also in a bubble, and a lot of people will lose a lot of money. I think both are absolutely true at the same time."&lt;/p&gt;  &lt;p&gt;He compared it to the internet. Webvan failed, but Instacart succeeded years later with essentially the same idea. If you were an Amazon shareholder from its IPO to now, you’re looking pretty good. If you were a Webvan shareholder, you probably feel differently.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"When the dust settles and you see who the winners are, society benefits from those inventions,” Amazon founder Jeff Bezos said in October. “This is real. The benefit to society from AI is going to be gigantic.”&lt;/p&gt;  &lt;p&gt;Goldman Sachs says the AI boom now looks the way tech stocks did in 1997, several years before the dot-com bubble actually burst. The bank flagged five warning signs seen in the late 1990s that investors should watch now: peak investment spending, falling corporate profits, rising corporate debt, Fed rate cuts, and widening credit spreads. We’re probably not at 1999 levels yet. But the imbalances are building fast. Michael Burry, who famously called the 2008 housing bubble collapse (as seen in the film &lt;em&gt;The Big Short&lt;/em&gt;), recently compared the AI boom to the 1990s dot-com bubble too.&lt;/p&gt;  &lt;p&gt;Maybe AI will save us from our own irrational exuberance. But for now, we’re living in an in-between moment when everyone knows what’s coming but keeps blowing more air into the balloon anyway. As Altman put it that night at dinner: “Someone is going to lose a phenomenal amount of money. We don’t know who.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Alex Heath is the author of &lt;/em&gt;Sources&lt;em&gt;, a newsletter about the AI race, and the cohost of &lt;/em&gt;ACCESS&lt;em&gt;, a podcast about the tech industry’s inside conversations. Previously, he was deputy editor at &lt;/em&gt;The Verge&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-7-super.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt; Explains: Let our writers untangle the complex, messy world of technology to help you &lt;/em&gt;understand&lt;em&gt; what’s coming next. You can read &lt;/em&gt;&lt;em&gt;more from the series here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In July, a widely cited MIT study claimed that 95% of organizations that invested in generative AI were getting “zero return.” Tech stocks briefly plunged. While the study itself was more nuanced than the headlines, for many it still felt like the first hard data point confirming what skeptics had muttered for months: Hype around AI might be outpacing reality.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Then, in August, OpenAI CEO Sam Altman said what everyone in Silicon Valley had been whispering. “Are we in a phase where investors as a whole are overexcited about AI?” he said during a press dinner I attended. “My opinion is yes.”&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt; 
 &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;He compared the current moment to the dot-com bubble. “When bubbles happen, smart people get overexcited about a kernel of truth,” he explained. “Tech was really important. The internet was a really big deal. People got overexcited.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;With those comments, it was off to the races. The next day’s stock market dip was attributed to the sentiment he shared. The question “Are we in an AI bubble?” became inescapable.&lt;/p&gt; 
 &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Who thinks it is a bubble?&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;The short answer: Lots of people. But not everyone agrees on who or what is overinflated. Tech leaders are using this moment of fear to take shots at their rivals and position themselves as clear winners on the other side. How they describe the bubble depends on where their company sits.&lt;/p&gt;  &lt;p&gt;When I asked Meta CEO Mark Zuckerberg about the AI bubble in September, he ran through the historical analogies of past bubbles—railroads, fiber for the internet, the dot-com boom—and noted that in each case, “the infrastructure gets built out, people take on too much debt, and then you hit some blip ... and then a lot of the companies end up going out of business.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But Zuckerberg’s prescription wasn’t for Meta to pump the brakes. It was to keep spending: “If we end up misspending a couple of hundred billion dollars, I think that that is going to be very unfortunate, obviously. But I’d say the risk is higher on the other side.”&lt;/p&gt;  &lt;p&gt;Bret Taylor, the chairman of OpenAI and CEO of the AI startup Sierra, uses a mental model from the late ’90s to help navigate this AI bubble. “I think the closest analogue to this AI wave is the dot-com boom or bubble, depending on your level of pessimism,” he recently told me. Back then, he explained, everyone knew e-commerce was going to be big, but there was a massive difference between Buy.com and Amazon. Taylor and others have been trying to position themselves as today’s Amazon.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Still others are arguing that the pain will be widespread. Google CEO Sundar Pichai told the BBC this month that there’s “some irrationality” in the current boom. Asked whether Google would be immune to a bubble bursting, he warned, “I think no company is going to be immune, including us."&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;What’s inflating the bubble?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;Companies are raising enormous sums of money and seeing unprecedented valuations. Much of that money, in turn, is going toward the buildout of massive data centers—on which both private companies like OpenAI and Elon Musk’s xAI and public ones such as Meta and Google are spending heavily. OpenAI has pledged that it will spend $500 billion to build AI data centers, more than 15 times what was spent on the Manhattan Project.&lt;/p&gt;  &lt;p&gt;This eye-popping spending on AI data centers isn’t entirely detached from reality. The leaders of the top AI companies all stress that they’re bottlenecked by their limited access to computing power. You hear it constantly when you talk to them. Startups can’t get the GPU allocations they need. Hyperscalers are rationing compute, saving it for their best customers.&lt;/p&gt;  &lt;p&gt;If today’s AI market is as brutally supply-constrained as tech leaders claim, perhaps aggressive infrastructure buildouts are warranted. But some of the numbers are too large to comprehend. Sam Altman has told employees that OpenAI’s moonshot goal is to build 250 gigawatts of computing capacity by 2033, roughly equaling India’s total national electricity demand. Such a plan would cost more than $12 trillion by today’s standards.&lt;/p&gt; 

 &lt;p&gt;“I do think there’s real execution risk,” OpenAI president and cofounder Greg Brockman recently told me about the company’s aggressive infrastructure goals. “Everything we say about the future, we see that it’s a possibility. It is not a certainty, but I don’t think the uncertainty comes from scientific questions. It’s a lot of hard work.”&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Who is exposed, and who is to blame?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;It depends on who you ask. During the August press dinner, where he made his market-moving comments, Altman was blunt about where he sees the excess. He said it’s “insane” that some AI startups with “three people and an idea” are receiving funding at such high valuations. “That’s not rational behavior,” he said. “Someone’s gonna get burned there, I think.” As Safe Superintelligence cofounder (and former OpenAI chief scientist and cofounder) Ilya Sutskever put it on a recent podcast: Silicon Valley has “more companies than ideas.”&lt;/p&gt;  &lt;p&gt;Demis Hassabis, the CEO of Google DeepMind, offered a similar diagnosis when I spoke with him in November. “It feels like there’s obviously a bubble in the private market,” he said. “You look at seed rounds with just nothing being tens of billions of dollars. That seems a little unsustainable."&lt;/p&gt;  &lt;p&gt;Anthropic CEO Dario Amodei also struck at his competition during the&lt;em&gt; New York Times &lt;/em&gt;DealBook Summit in early December. He said he feels confident about the technology itself but worries about how others are behaving on the business side: “On the economic side, I have my concerns where, even if the technology fulfills all its promises, I think there are players in the ecosystem who, if they just make a timing error, they just get it off by a little bit, bad things could happen."&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_9"&gt; &lt;p&gt;He stopped short of naming Sam Altman and OpenAI, but the implication was clear. “There are some players who are YOLOing,” he said. “Let’s say you’re a person who just kind of constitutionally wants to YOLO things or just likes big numbers. Then you may turn the dial too far.”&lt;/p&gt;  &lt;p&gt;Amodei also flagged “circular deals,” or the increasingly common arrangements where chip suppliers like Nvidia invest in AI companies that then turn around and spend those funds on their chips. Anthropic has done some of these, he said, though “not at the same scale as some other players.” (OpenAI is at the center of a number of such deals, as are Nvidia, CoreWeave, and a roster of other players.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The danger, he explained, comes when the numbers get too big: “If you start stacking these where they get to huge amounts of money, and you’re saying, ’By 2027 or 2028 I need to make $200 billion a year,’ then yeah, you can overextend yourself.”&lt;/p&gt;  &lt;p&gt;Zuckerberg shared a similar message at an internal employee Q&amp;amp;A session after Meta’s last earnings call. He noted that unprofitable startups like OpenAI and Anthropic risk bankruptcy if they misjudge the timing of their investments, but Meta has the advantage of strong cash flow, he reassured staff.&lt;/p&gt; 
 &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;How could a bubble burst?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;My conversations with tech executives and investors suggest that the bubble will be most likely to pop if overfunded startups can’t turn a profit or grow into their lofty valuations. This bubble could last longer than than past ones, given that private markets aren’t traded on public markets and therefore move more slowly, but the ripple effects will still be profound when the end comes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If companies making grand commitments to data center buildouts no longer have the revenue growth to support them, the headline deals that have propped up the stock market come into question. Anthropic’s Amodei illustrated the problem during his DealBook Summit appearance, where he said the multi-year data center commitments he has to make combine with the company’s rapid, unpredictable revenue growth rate to create a “cone of uncertainty” about how much to spend.&lt;/p&gt; 
 &lt;p&gt;The two most prominent private players in AI, OpenAI and Anthropic, have yet to turn a profit. A recent Deutsche Bank chart put the situation in stark historical context. Amazon burned through $3 billion before becoming profitable. Tesla, around $4 billion. Uber, $30 billion. OpenAI is projected to burn through $140 billion by 2029, while Anthropic is expected to burn $20 billion by 2027.&lt;/p&gt;  &lt;p&gt;Consultants at Bain estimate that the wave of AI infrastructure spending will require $2 trillion in annual AI revenue by 2030 just to justify the investment. That’s more than the combined 2024 revenue of Amazon, Apple, Alphabet, Microsoft, Meta, and Nvidia. When I talk to leaders of these large tech companies, they all agree that their sprawling businesses can absorb an expensive miscalculation about the returns from their AI infrastructure buildouts. It’s all the other companies that are either highly leveraged with debt or just unprofitable—even OpenAI and Anthropic—that they worry about.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_11"&gt;&lt;p&gt;Still, given the level of spending on AI, it still needs a viable business model beyond subscriptions, which won’t be able to&amp;nbsp; drive profits from billions of people’s eyeballs like the ad-driven businesses that have defined the last 20 years of the internet. Even the largest tech companies know they need to ship the world-changing agents they keep hyping: AI that can fully replace coworkers and complete tasks in the real world. &lt;/p&gt;  &lt;p&gt;For now, investors are mostly buying into the hype of the powerful AI systems that these data center buildouts will supposedly unlock in the future. At some point the biggest spenders, like OpenAI, will need to show investors that the money spent on the infrastructure buildout was worth it.&lt;/p&gt;  &lt;p&gt;There’s also still a lot of uncertainty about the technical direction that AI is heading in. LLMs are expected to remain critical to more advanced AI systems, but industry leaders can’t seem to agree on which additional breakthroughs are needed to achieve artificial general intelligence, or AGI. Some are betting on new kinds of AI that can understand the physical world, while others are focused on training AI to learn in a general way, like a human. In other words, what if all this unprecedented spending turns out to have been backing the wrong horse?&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;The question now&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;What makes this moment surreal is the honesty. The same people pouring billions into AI will openly tell you it might all come crashing down.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Taylor framed it as two truths existing at once. “I think it is both true that AI will transform the economy,” he told me, “and I think we’re also in a bubble, and a lot of people will lose a lot of money. I think both are absolutely true at the same time."&lt;/p&gt;  &lt;p&gt;He compared it to the internet. Webvan failed, but Instacart succeeded years later with essentially the same idea. If you were an Amazon shareholder from its IPO to now, you’re looking pretty good. If you were a Webvan shareholder, you probably feel differently.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"When the dust settles and you see who the winners are, society benefits from those inventions,” Amazon founder Jeff Bezos said in October. “This is real. The benefit to society from AI is going to be gigantic.”&lt;/p&gt;  &lt;p&gt;Goldman Sachs says the AI boom now looks the way tech stocks did in 1997, several years before the dot-com bubble actually burst. The bank flagged five warning signs seen in the late 1990s that investors should watch now: peak investment spending, falling corporate profits, rising corporate debt, Fed rate cuts, and widening credit spreads. We’re probably not at 1999 levels yet. But the imbalances are building fast. Michael Burry, who famously called the 2008 housing bubble collapse (as seen in the film &lt;em&gt;The Big Short&lt;/em&gt;), recently compared the AI boom to the 1990s dot-com bubble too.&lt;/p&gt;  &lt;p&gt;Maybe AI will save us from our own irrational exuberance. But for now, we’re living in an in-between moment when everyone knows what’s coming but keeps blowing more air into the balloon anyway. As Altman put it that night at dinner: “Someone is going to lose a phenomenal amount of money. We don’t know who.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Alex Heath is the author of &lt;/em&gt;Sources&lt;em&gt;, a newsletter about the AI race, and the cohost of &lt;/em&gt;ACCESS&lt;em&gt;, a podcast about the tech industry’s inside conversations. Previously, he was deputy editor at &lt;/em&gt;The Verge&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129183/what-even-is-the-ai-bubble/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>AI might not be coming for lawyers’ jobs anytime soon (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129181/ai-might-not-be-coming-for-lawyers-jobs-anytime-soon/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;When the generative AI boom took off in 2022, Rudi Miller and her law school classmates were suddenly gripped with anxiety. “Before graduating, there was discussion about what the job market would look like for us if AI became adopted,” she recalls.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So when it came time to choose a speciality, Miller—now a junior associate at the law firm Orrick—decided to become a litigator, the kind of lawyer who represents clients in court. She hoped the courtroom would be the last human stage. “Judges haven’t allowed ChatGPT-enabled robots to argue in court yet,” she says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;She had reason to be worried. The artificial-intelligence job apocalypse seemed to be coming for lawyers. In March 2023, researchers reported that GPT-4 had smashed the Uniform Bar Exam. That same month, an industry report predicted that 44% of legal work could be automated.&lt;strong&gt; &lt;/strong&gt;The legal tech industry entered a boom as law firms began adopting generative AI to mine mountains of documents and draft contracts, work ordinarily done by junior associates. Last month, the law firm Clifford Chance axed 10% of its staff in London, citing increased use of AI as a reason.&lt;/p&gt; 
 &lt;p&gt;But for all the hype, LLMs are still far from thinking like lawyers—let alone replacing them. The models continue to hallucinate case citations, struggle to navigate gray areas of the law and reason about novel questions, and stumble when they attempt to synthesize information scattered across statutes, regulations, and court cases. And there are deeper institutional reasons to think the models could struggle to supplant legal jobs. While AI is reshaping the grunt work of the profession, the end of lawyers may not be arriving anytime soon.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The big experiment&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The legal industry has long been defined by long hours and grueling workloads, so the promise of superhuman efficiency is appealing. Law firms are experimenting with general-purpose tools like ChatGPT and Microsoft Copilot and specialized legal tools like Harvey and Thomson Reuters’ CoCounsel, with some building their own in-house tools on top of frontier models. They’re rolling out AI boot camps and letting associates bill hundreds of hours to AI experimentation. As of 2024, 47.8% of attorneys at law firms employing 500 or more lawyers used AI, according to the American Bar Association.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But lawyers say that LLMs are a long way from reasoning well enough to replace them. Lucas Hale, a junior associate at McDermott Will &amp;amp; Schulte, has been embracing AI for many routine chores. He uses Relativity to sift through long documents and Microsoft Copilot for drafting legal citations. But when he turns to ChatGPT with a complex legal question, he finds the chatbot spewing hallucinations, rambling off topic, or drawing a blank.&lt;/p&gt;  &lt;p&gt;“In the case where we have a very narrow question or a question of first impression for the court,” he says, referring to a novel legal question that a court has never decided before, “that’s the kind of thinking that the tool can’t do.”&lt;/p&gt;  &lt;p&gt;Much of Lucas’s work involves creatively applying the law to new fact patterns. “Right now, I don’t think very much of the work that litigators do, at least not the work that I do, can be outsourced to an AI utility,” he says.&lt;/p&gt;  &lt;p&gt;Allison Douglis, a senior associate at Jenner &amp;amp; Block, uses an LLM to kick off her legal research. But the tools only take her so far. “When it comes to actually fleshing out and developing an argument as a litigator, I don’t think they’re there,” she says. She has watched the models hallucinate case citations and fumble through ambiguous areas of the law.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“Right now, I would much rather work with a junior associate than an AI tool,” she says. “Unless they get extraordinarily good very quickly, I can’t imagine that changing in the near future.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Beyond the bar&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The legal industry has seemed ripe for an AI takeover ever since ChatGPT’s triumph on the bar exam. But passing a standardized test isn’t the same as practicing law. The exam tests whether people can memorize legal rules and apply them to hypothetical situations—not whether they can exercise strategic judgment in complicated realities or craft arguments in uncharted legal territory. And models can be trained to ace benchmarks without genuinely improving their reasoning.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;But new benchmarks are aiming to better measure the models’ ability to do legal work in the real world. The Professional Reasoning Benchmark, published by ScaleAI in November, evaluated leading LLMs on legal and financial tasks designed by professionals in the field. The study found that the models have critical gaps in their reliability for professional adoption, with the best-performing model scoring only 37% on the most difficult legal problems, meaning it met just over a third of possible points on the evaluation criteria. The models frequently made inaccurate legal judgments, and if they did reach correct conclusions, they did so through incomplete or opaque reasoning processes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The tools actually are not there to basically substitute [for] your lawyer,” says Afra Feyza Akyurek, the lead author of the paper. “Even though a lot of people think that LLMs have a good grasp of the law, it’s still lagging behind.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The paper builds on other benchmarks measuring the models’ performance on economically valuable work. The AI Productivity Index, published by the data firm Mercor in September and updated in December, found that the models have “substantial limitations” in performing legal work. The best-performing model scored 77.9% on legal tasks, meaning it satisfied roughly four out of five evaluation criteria. A model with such a score might generate substantial economic value in some industries, but in fields where errors are costly, it may not be useful at all, the early version of the study noted.&lt;strong&gt; &lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Professional benchmarks are a big step forward in evaluating the LLMs’ real-world capabilities, but they may still not capture what lawyers actually do. “These questions, although more challenging than those in past benchmarks, still don’t fully reflect the kinds of subjective, extremely challenging questions lawyers tackle in real life,” says Jon Choi, a law professor at the University of Washington School of Law, who coauthored a study on legal benchmarks in 2023.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Unlike math or coding, in which LLMs have made significant progress, legal reasoning may be challenging for the models to learn. The law deals with messy real-world problems, riddled with ambiguity and subjectivity, that often have no right answer, says Choi. Making matters worse, a lot of legal work isn’t recorded in ways that can be used to train the models, he says. When it is, documents can span hundreds of pages, scattered across statutes, regulations, and court cases that exist in a complex hierarchy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But a more fundamental limitation might be that LLMs are simply not trained to think like lawyers. “The reasoning models still don’t fully reason about problems like we humans do,” says Julian Nyarko, a law professor at Stanford Law School.&lt;strong&gt; &lt;/strong&gt;The models may lack a mental model of the world—the ability to simulate a scenario and predict what will happen—and that capability could be at the heart of complex legal reasoning, he says. It’s possible that the current paradigm of LLMs trained on next-word prediction gets us only so far.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The jobs remain&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Despite early signs that AI is beginning to affect entry-level workers, labor statistics have yet to show that lawyers are being displaced. 93.4% of law school graduates in 2024 were employed within 10 months of graduation—the highest rate on record—according to the National Association for Law Placement. The number of graduates working in law firms rose by 13% from 2023 to 2024.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;For now, law firms are slow to shrink their ranks. “We’re not reducing headcounts at this point,” said Amy Ross, the chief of attorney talent at the law firm Ropes &amp;amp; Gray.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even looking ahead, the effects could be incremental. “I will expect some impact on the legal profession’s labor market, but not major,” says Mert Demirer, an economist at MIT. “AI is going to be very useful in terms of information discovery and summary,” he says, but for complex legal tasks, “the law’s low risk tolerance, plus the current capabilities of AI, are going to make that case less automatable at this point.” Capabilities may evolve over time, but that’s a big unknown.&lt;/p&gt;  &lt;p&gt;It’s not just that the models themselves are not ready to replace junior lawyers. Institutional barriers may also shape how AI is deployed. Higher productivity reduces billable hours, challenging the dominant business model of law firms. Liability looms large for lawyers, and clients may still want a human on the hook. Regulations could also constrain how lawyers use the technology.&lt;/p&gt; 
 &lt;p&gt;Still, as AI takes on some associate work, law firms may need to reinvent their training system. “When junior work dries up, you have to have a more formal way of teaching than hoping that an apprenticeship works,” says Ethan Mollick, a management professor at the Wharton School of the University of Pennsylvania.&lt;/p&gt;  &lt;p&gt;Zach Couger, a junior associate at McDermott Will &amp;amp; Schulte, leans on ChatGPT to comb through piles of contracts he once slogged through by hand. He can’t imagine going back to doing the job himself, but he wonders what he’s missing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I’m worried that I’m not getting the same reps that senior attorneys got,” he says, referring to the repetitive training that has long defined the early experiences of lawyers.&lt;strong&gt; &lt;/strong&gt;“On the other hand, it is very nice to have a semi–knowledge expert to just ask questions to that’s not a partner who’s also very busy.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even though an AI job apocalypse looks distant, the uncertainty sticks with him. Lately, Couger finds himself staying up late, wondering if he could be part of the last class of associates at big law firms: “I may be the last plane out.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;When the generative AI boom took off in 2022, Rudi Miller and her law school classmates were suddenly gripped with anxiety. “Before graduating, there was discussion about what the job market would look like for us if AI became adopted,” she recalls.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So when it came time to choose a speciality, Miller—now a junior associate at the law firm Orrick—decided to become a litigator, the kind of lawyer who represents clients in court. She hoped the courtroom would be the last human stage. “Judges haven’t allowed ChatGPT-enabled robots to argue in court yet,” she says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;She had reason to be worried. The artificial-intelligence job apocalypse seemed to be coming for lawyers. In March 2023, researchers reported that GPT-4 had smashed the Uniform Bar Exam. That same month, an industry report predicted that 44% of legal work could be automated.&lt;strong&gt; &lt;/strong&gt;The legal tech industry entered a boom as law firms began adopting generative AI to mine mountains of documents and draft contracts, work ordinarily done by junior associates. Last month, the law firm Clifford Chance axed 10% of its staff in London, citing increased use of AI as a reason.&lt;/p&gt; 
 &lt;p&gt;But for all the hype, LLMs are still far from thinking like lawyers—let alone replacing them. The models continue to hallucinate case citations, struggle to navigate gray areas of the law and reason about novel questions, and stumble when they attempt to synthesize information scattered across statutes, regulations, and court cases. And there are deeper institutional reasons to think the models could struggle to supplant legal jobs. While AI is reshaping the grunt work of the profession, the end of lawyers may not be arriving anytime soon.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The big experiment&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The legal industry has long been defined by long hours and grueling workloads, so the promise of superhuman efficiency is appealing. Law firms are experimenting with general-purpose tools like ChatGPT and Microsoft Copilot and specialized legal tools like Harvey and Thomson Reuters’ CoCounsel, with some building their own in-house tools on top of frontier models. They’re rolling out AI boot camps and letting associates bill hundreds of hours to AI experimentation. As of 2024, 47.8% of attorneys at law firms employing 500 or more lawyers used AI, according to the American Bar Association.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But lawyers say that LLMs are a long way from reasoning well enough to replace them. Lucas Hale, a junior associate at McDermott Will &amp;amp; Schulte, has been embracing AI for many routine chores. He uses Relativity to sift through long documents and Microsoft Copilot for drafting legal citations. But when he turns to ChatGPT with a complex legal question, he finds the chatbot spewing hallucinations, rambling off topic, or drawing a blank.&lt;/p&gt;  &lt;p&gt;“In the case where we have a very narrow question or a question of first impression for the court,” he says, referring to a novel legal question that a court has never decided before, “that’s the kind of thinking that the tool can’t do.”&lt;/p&gt;  &lt;p&gt;Much of Lucas’s work involves creatively applying the law to new fact patterns. “Right now, I don’t think very much of the work that litigators do, at least not the work that I do, can be outsourced to an AI utility,” he says.&lt;/p&gt;  &lt;p&gt;Allison Douglis, a senior associate at Jenner &amp;amp; Block, uses an LLM to kick off her legal research. But the tools only take her so far. “When it comes to actually fleshing out and developing an argument as a litigator, I don’t think they’re there,” she says. She has watched the models hallucinate case citations and fumble through ambiguous areas of the law.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;“Right now, I would much rather work with a junior associate than an AI tool,” she says. “Unless they get extraordinarily good very quickly, I can’t imagine that changing in the near future.”&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Beyond the bar&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;The legal industry has seemed ripe for an AI takeover ever since ChatGPT’s triumph on the bar exam. But passing a standardized test isn’t the same as practicing law. The exam tests whether people can memorize legal rules and apply them to hypothetical situations—not whether they can exercise strategic judgment in complicated realities or craft arguments in uncharted legal territory. And models can be trained to ace benchmarks without genuinely improving their reasoning.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;But new benchmarks are aiming to better measure the models’ ability to do legal work in the real world. The Professional Reasoning Benchmark, published by ScaleAI in November, evaluated leading LLMs on legal and financial tasks designed by professionals in the field. The study found that the models have critical gaps in their reliability for professional adoption, with the best-performing model scoring only 37% on the most difficult legal problems, meaning it met just over a third of possible points on the evaluation criteria. The models frequently made inaccurate legal judgments, and if they did reach correct conclusions, they did so through incomplete or opaque reasoning processes.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“The tools actually are not there to basically substitute [for] your lawyer,” says Afra Feyza Akyurek, the lead author of the paper. “Even though a lot of people think that LLMs have a good grasp of the law, it’s still lagging behind.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The paper builds on other benchmarks measuring the models’ performance on economically valuable work. The AI Productivity Index, published by the data firm Mercor in September and updated in December, found that the models have “substantial limitations” in performing legal work. The best-performing model scored 77.9% on legal tasks, meaning it satisfied roughly four out of five evaluation criteria. A model with such a score might generate substantial economic value in some industries, but in fields where errors are costly, it may not be useful at all, the early version of the study noted.&lt;strong&gt; &lt;/strong&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Professional benchmarks are a big step forward in evaluating the LLMs’ real-world capabilities, but they may still not capture what lawyers actually do. “These questions, although more challenging than those in past benchmarks, still don’t fully reflect the kinds of subjective, extremely challenging questions lawyers tackle in real life,” says Jon Choi, a law professor at the University of Washington School of Law, who coauthored a study on legal benchmarks in 2023.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Unlike math or coding, in which LLMs have made significant progress, legal reasoning may be challenging for the models to learn. The law deals with messy real-world problems, riddled with ambiguity and subjectivity, that often have no right answer, says Choi. Making matters worse, a lot of legal work isn’t recorded in ways that can be used to train the models, he says. When it is, documents can span hundreds of pages, scattered across statutes, regulations, and court cases that exist in a complex hierarchy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But a more fundamental limitation might be that LLMs are simply not trained to think like lawyers. “The reasoning models still don’t fully reason about problems like we humans do,” says Julian Nyarko, a law professor at Stanford Law School.&lt;strong&gt; &lt;/strong&gt;The models may lack a mental model of the world—the ability to simulate a scenario and predict what will happen—and that capability could be at the heart of complex legal reasoning, he says. It’s possible that the current paradigm of LLMs trained on next-word prediction gets us only so far.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The jobs remain&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Despite early signs that AI is beginning to affect entry-level workers, labor statistics have yet to show that lawyers are being displaced. 93.4% of law school graduates in 2024 were employed within 10 months of graduation—the highest rate on record—according to the National Association for Law Placement. The number of graduates working in law firms rose by 13% from 2023 to 2024.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;For now, law firms are slow to shrink their ranks. “We’re not reducing headcounts at this point,” said Amy Ross, the chief of attorney talent at the law firm Ropes &amp;amp; Gray.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even looking ahead, the effects could be incremental. “I will expect some impact on the legal profession’s labor market, but not major,” says Mert Demirer, an economist at MIT. “AI is going to be very useful in terms of information discovery and summary,” he says, but for complex legal tasks, “the law’s low risk tolerance, plus the current capabilities of AI, are going to make that case less automatable at this point.” Capabilities may evolve over time, but that’s a big unknown.&lt;/p&gt;  &lt;p&gt;It’s not just that the models themselves are not ready to replace junior lawyers. Institutional barriers may also shape how AI is deployed. Higher productivity reduces billable hours, challenging the dominant business model of law firms. Liability looms large for lawyers, and clients may still want a human on the hook. Regulations could also constrain how lawyers use the technology.&lt;/p&gt; 
 &lt;p&gt;Still, as AI takes on some associate work, law firms may need to reinvent their training system. “When junior work dries up, you have to have a more formal way of teaching than hoping that an apprenticeship works,” says Ethan Mollick, a management professor at the Wharton School of the University of Pennsylvania.&lt;/p&gt;  &lt;p&gt;Zach Couger, a junior associate at McDermott Will &amp;amp; Schulte, leans on ChatGPT to comb through piles of contracts he once slogged through by hand. He can’t imagine going back to doing the job himself, but he wonders what he’s missing.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“I’m worried that I’m not getting the same reps that senior attorneys got,” he says, referring to the repetitive training that has long defined the early experiences of lawyers.&lt;strong&gt; &lt;/strong&gt;“On the other hand, it is very nice to have a semi–knowledge expert to just ask questions to that’s not a partner who’s also very busy.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even though an AI job apocalypse looks distant, the uncertainty sticks with him. Lately, Couger finds himself staying up late, wondering if he could be part of the last class of associates at big law firms: “I may be the last plane out.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129181/ai-might-not-be-coming-for-lawyers-jobs-anytime-soon/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>Generative AI hype distracts us from AI’s more important breakthroughs (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129179/generative-ai-hype-distracts-us-from-ais-more-important-breakthroughs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;On April 28, 2022, at a highly anticipated concert in Spokane, Washington, the musician Paul McCartney astonished his audience with a groundbreaking application of AI: He began to perform with a lifelike depiction of his long-deceased musical partner, John Lennon.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using recent advances in audio and video processing, engineers had taken the pair’s final performance (London, 1969), separated Lennon’s voice and image from the original mix and restored them with lifelike clarity.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction &lt;/strong&gt;package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;For years, researchers like me had taught machines to “see” and “hear” in order to make such a moment possible. As McCartney and Lennon appeared to reunite across time and space, the arena fell silent; many in the crowd began to cry. As an AI scientist and lifelong Beatles fan, I felt profound gratitude that we could experience this truly life-changing moment.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Later that year, the world was captivated by another major breakthrough: AI conversation. For the first time in history, systems capable of generating new, contextually relevant comments in real time, on virtually any subject, were widely accessible owing to the release of ChatGPT. Billions of people were suddenly able to &lt;em&gt;interact with&lt;/em&gt; AI. This ignited the public’s imagination about what AI could be, bringing an explosion of creative ideas, hopes, and fears.&lt;/p&gt;  &lt;p&gt;Having done my PhD on AI language generation (long considered niche), I was thrilled we had come this far. But the awe I felt was rivaled by my growing rage at the flood of media takes and self-appointed experts insisting that generative AI could do things it simply can’t, and warning that anyone who didn’t adopt it would be left behind.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This kind of hype has contributed to a frenzy of misunderstandings about what AI actually is and what it can and cannot do. Crucially, generative AI is a seductive distraction from the type of AI that is most likely to make your life better, or even save it: Predictive AI. In contrast to AI designed for generative tasks, &lt;em&gt;predictive &lt;/em&gt;AI involves tasks with a finite, known set of answers; the system just has to process information to say which answer is right. A basic example is plant recognition: Point your phone camera at a plant and learn that it’s a Western sword fern. &lt;em&gt;Generative&lt;/em&gt;&lt;strong&gt; &lt;/strong&gt;tasks, in contrast, have no finite set of correct answers: The system must blend snippets of information it’s been trained on to create, for example, a novel picture of a fern.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The generative AI technology involved in chatbots, face-swaps, and synthetic video makes for stunning demos, driving clicks and sales as viewers run wild with ideas that superhuman AI will be capable of bringing us abundance or extinction. Yet predictive AI has quietly been improving weather prediction and food safety, enabling higher-quality music production, helping to organize photos, and accurately predicting the fastest driving routes. We incorporate predictive AI into our everyday lives without evening thinking about it, a testament to its indispensable utility.&lt;/p&gt; 
 &lt;p&gt;To get a sense of the immense progress on predictive AI and its future potential, we can look at the trajectory of the past 20 years. In 2005, we couldn’t get AI to tell the difference between a person and a pencil. By 2013, AI still couldn’t reliably detect a bird in a photo, and the difference between a pedestrian and a Coke bottle was massively confounding (this is how I learned that bottles do &lt;em&gt;kind of&lt;/em&gt; look like people, if people had no heads). The thought of deploying these systems in the real world was the stuff of science fiction.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet over the past 10 years, predictive AI has not only nailed bird detection down to the specific species; it has rapidly improved life-critical medical services like identifying problematic lesions and heart arrhythmia. Because of this technology, seismologists can predict earthquakes and meteorologists can predict flooding more reliably than ever before. Accuracy has skyrocketed for consumer-facing tech that detects and classifies everything from what song you’re thinking of when you hum a tune to which objects to avoid while you’re driving—making self-driving cars a reality.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;In the very near future, we should be able to accurately detect tumors and forecast hurricanes long before they can hurt anyone, realizing the lifelong hopes of people all over the world. That might not be as flashy as generating your own Studio Ghibli–ish film, but it’s definitely hype-worthy.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Predictive AI systems have also been shown to be incredibly useful when they leverage certain generative techniques within a constrained set of options. Systems of this type are diverse, spanning everything from outfit visualization to cross-language translation. Soon, predictive-generative hybrid systems will make it possible to clone your own voice speaking another language in real time, an extraordinary aid for travel (with serious impersonation risks). There’s considerable room for growth here, but generative AI delivers real value when anchored by strong predictive methods.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;To understand the difference between these two broad classes of AI, imagine yourself as an AI system tasked with showing someone what a cat looks like. You could adopt a generative approach, cutting and pasting small fragments from various cat images (potentially from sources that object) to construct a seemingly perfect depiction. The ability of modern generative AI to produce such a flawless collage is what makes it so astonishing. &lt;/p&gt;  &lt;p&gt;Alternatively, you could take the predictive approach: Simply locate and point to an existing picture of a cat. That method is much less glamorous but more energy-efficient and more likely to be accurate, and it properly acknowledges the original source. Generative AI is designed to create things that &lt;em&gt;look&lt;/em&gt; real; predictive AI identifies what &lt;em&gt;is&lt;/em&gt; real. A misunderstanding that generative systems are &lt;em&gt;retrieving&lt;/em&gt; things when they are actually &lt;em&gt;creating&lt;/em&gt; them has led to grave consequences when text is involved, requiring the withdrawal of legal rulings and the retraction of scientific articles.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Driving this confusion is a tendency for people to hype AI without making it clear what kind of AI they’re talking about (I reckon many don’t know). It’s very easy to equate “AI” with generative AI, or even just language-generating AI, and assume that all other capabilities fall out from there. That fallacy makes a ton of sense: The term literally references “intelligence,” and our human understanding of what “intelligence” might be is often mediated by the use of language. (Spoiler: No one actually knows what intelligence is.) But the phrase “artificial intelligence” was intentionally designed in the 1950s to inspire awe and allude to something humanlike. Today, it just refers to a set of disparate technologies for processing digital data. Some of my friends find it helpful to call it “mathy maths” instead.&lt;/p&gt;  &lt;p&gt;The bias toward treating generative AI as the most powerful and real form of AI is troubling given that it consumes considerably more energy than predictive AI systems. It also means using existing human work in AI products against the original creators’ wishes and replacing human jobs with AI systems whose capabilities their work made possible in the first place—without compensation. AI can be amazingly powerful, but that doesn’t mean creators should be ripped off.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Watching this unfold as an AI developer within the tech industry, I’ve drawn important lessons for next steps. The widespread appeal of AI is clearly linked to the intuitive nature of conversation-based interactions. But this method of engagement currently overuses generative methods where predictive ones would suffice, resulting in an awkward situation that’s confusing for users while imposing heavy costs in energy consumption, exploitation, and job displacement.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We have witnessed just a &lt;em&gt;glimpse&lt;/em&gt; of AI's full potential: The current excitement around AI reflects what it could be, not what it is. Generation-based approaches strain resources while still falling short on representation, accuracy, and the wishes of people whose work is folded into the system.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;If we can shift the spotlight from the hype around generative technologies to the predictive advances already transforming daily life, we can build AI that is genuinely useful, equitable, and sustainable. The systems that help doctors catch diseases earlier, help scientists forecast disasters sooner, and help everyday people navigate their lives more safely are the ones poised to deliver the greatest impact.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The future of beneficial AI will not be defined by the flashiest demos but by the quiet, rigorous progress that makes technology trustworthy. And if we build on that foundation—pairing predictive strength with more mature data practices and intuitive natural-language interfaces—AI can finally start living up to the promise that many people perceive today.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dr. Margaret Mitchell is a computer science researcher and chief ethics scientist at AI startup Hugging Face. She has worked in the technology industry for 15 years, and has published over 100 papers on natural language generation, assistive technology, computer vision, and AI ethics. Her work has received numerous awards and has been implemented by multiple technology companies.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-3.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;On April 28, 2022, at a highly anticipated concert in Spokane, Washington, the musician Paul McCartney astonished his audience with a groundbreaking application of AI: He began to perform with a lifelike depiction of his long-deceased musical partner, John Lennon.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Using recent advances in audio and video processing, engineers had taken the pair’s final performance (London, 1969), separated Lennon’s voice and image from the original mix and restored them with lifelike clarity.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction &lt;/strong&gt;package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;For years, researchers like me had taught machines to “see” and “hear” in order to make such a moment possible. As McCartney and Lennon appeared to reunite across time and space, the arena fell silent; many in the crowd began to cry. As an AI scientist and lifelong Beatles fan, I felt profound gratitude that we could experience this truly life-changing moment.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Later that year, the world was captivated by another major breakthrough: AI conversation. For the first time in history, systems capable of generating new, contextually relevant comments in real time, on virtually any subject, were widely accessible owing to the release of ChatGPT. Billions of people were suddenly able to &lt;em&gt;interact with&lt;/em&gt; AI. This ignited the public’s imagination about what AI could be, bringing an explosion of creative ideas, hopes, and fears.&lt;/p&gt;  &lt;p&gt;Having done my PhD on AI language generation (long considered niche), I was thrilled we had come this far. But the awe I felt was rivaled by my growing rage at the flood of media takes and self-appointed experts insisting that generative AI could do things it simply can’t, and warning that anyone who didn’t adopt it would be left behind.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;This kind of hype has contributed to a frenzy of misunderstandings about what AI actually is and what it can and cannot do. Crucially, generative AI is a seductive distraction from the type of AI that is most likely to make your life better, or even save it: Predictive AI. In contrast to AI designed for generative tasks, &lt;em&gt;predictive &lt;/em&gt;AI involves tasks with a finite, known set of answers; the system just has to process information to say which answer is right. A basic example is plant recognition: Point your phone camera at a plant and learn that it’s a Western sword fern. &lt;em&gt;Generative&lt;/em&gt;&lt;strong&gt; &lt;/strong&gt;tasks, in contrast, have no finite set of correct answers: The system must blend snippets of information it’s been trained on to create, for example, a novel picture of a fern.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The generative AI technology involved in chatbots, face-swaps, and synthetic video makes for stunning demos, driving clicks and sales as viewers run wild with ideas that superhuman AI will be capable of bringing us abundance or extinction. Yet predictive AI has quietly been improving weather prediction and food safety, enabling higher-quality music production, helping to organize photos, and accurately predicting the fastest driving routes. We incorporate predictive AI into our everyday lives without evening thinking about it, a testament to its indispensable utility.&lt;/p&gt; 
 &lt;p&gt;To get a sense of the immense progress on predictive AI and its future potential, we can look at the trajectory of the past 20 years. In 2005, we couldn’t get AI to tell the difference between a person and a pencil. By 2013, AI still couldn’t reliably detect a bird in a photo, and the difference between a pedestrian and a Coke bottle was massively confounding (this is how I learned that bottles do &lt;em&gt;kind of&lt;/em&gt; look like people, if people had no heads). The thought of deploying these systems in the real world was the stuff of science fiction.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Yet over the past 10 years, predictive AI has not only nailed bird detection down to the specific species; it has rapidly improved life-critical medical services like identifying problematic lesions and heart arrhythmia. Because of this technology, seismologists can predict earthquakes and meteorologists can predict flooding more reliably than ever before. Accuracy has skyrocketed for consumer-facing tech that detects and classifies everything from what song you’re thinking of when you hum a tune to which objects to avoid while you’re driving—making self-driving cars a reality.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;In the very near future, we should be able to accurately detect tumors and forecast hurricanes long before they can hurt anyone, realizing the lifelong hopes of people all over the world. That might not be as flashy as generating your own Studio Ghibli–ish film, but it’s definitely hype-worthy.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Predictive AI systems have also been shown to be incredibly useful when they leverage certain generative techniques within a constrained set of options. Systems of this type are diverse, spanning everything from outfit visualization to cross-language translation. Soon, predictive-generative hybrid systems will make it possible to clone your own voice speaking another language in real time, an extraordinary aid for travel (with serious impersonation risks). There’s considerable room for growth here, but generative AI delivers real value when anchored by strong predictive methods.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;To understand the difference between these two broad classes of AI, imagine yourself as an AI system tasked with showing someone what a cat looks like. You could adopt a generative approach, cutting and pasting small fragments from various cat images (potentially from sources that object) to construct a seemingly perfect depiction. The ability of modern generative AI to produce such a flawless collage is what makes it so astonishing. &lt;/p&gt;  &lt;p&gt;Alternatively, you could take the predictive approach: Simply locate and point to an existing picture of a cat. That method is much less glamorous but more energy-efficient and more likely to be accurate, and it properly acknowledges the original source. Generative AI is designed to create things that &lt;em&gt;look&lt;/em&gt; real; predictive AI identifies what &lt;em&gt;is&lt;/em&gt; real. A misunderstanding that generative systems are &lt;em&gt;retrieving&lt;/em&gt; things when they are actually &lt;em&gt;creating&lt;/em&gt; them has led to grave consequences when text is involved, requiring the withdrawal of legal rulings and the retraction of scientific articles.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Driving this confusion is a tendency for people to hype AI without making it clear what kind of AI they’re talking about (I reckon many don’t know). It’s very easy to equate “AI” with generative AI, or even just language-generating AI, and assume that all other capabilities fall out from there. That fallacy makes a ton of sense: The term literally references “intelligence,” and our human understanding of what “intelligence” might be is often mediated by the use of language. (Spoiler: No one actually knows what intelligence is.) But the phrase “artificial intelligence” was intentionally designed in the 1950s to inspire awe and allude to something humanlike. Today, it just refers to a set of disparate technologies for processing digital data. Some of my friends find it helpful to call it “mathy maths” instead.&lt;/p&gt;  &lt;p&gt;The bias toward treating generative AI as the most powerful and real form of AI is troubling given that it consumes considerably more energy than predictive AI systems. It also means using existing human work in AI products against the original creators’ wishes and replacing human jobs with AI systems whose capabilities their work made possible in the first place—without compensation. AI can be amazingly powerful, but that doesn’t mean creators should be ripped off.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Watching this unfold as an AI developer within the tech industry, I’ve drawn important lessons for next steps. The widespread appeal of AI is clearly linked to the intuitive nature of conversation-based interactions. But this method of engagement currently overuses generative methods where predictive ones would suffice, resulting in an awkward situation that’s confusing for users while imposing heavy costs in energy consumption, exploitation, and job displacement.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;We have witnessed just a &lt;em&gt;glimpse&lt;/em&gt; of AI's full potential: The current excitement around AI reflects what it could be, not what it is. Generation-based approaches strain resources while still falling short on representation, accuracy, and the wishes of people whose work is folded into the system.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;If we can shift the spotlight from the hype around generative technologies to the predictive advances already transforming daily life, we can build AI that is genuinely useful, equitable, and sustainable. The systems that help doctors catch diseases earlier, help scientists forecast disasters sooner, and help everyday people navigate their lives more safely are the ones poised to deliver the greatest impact.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The future of beneficial AI will not be defined by the flashiest demos but by the quiet, rigorous progress that makes technology trustworthy. And if we build on that foundation—pairing predictive strength with more mature data practices and intuitive natural-language interfaces—AI can finally start living up to the promise that many people perceive today.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dr. Margaret Mitchell is a computer science researcher and chief ethics scientist at AI startup Hugging Face. She has worked in the technology industry for 15 years, and has published over 100 papers on natural language generation, assistive technology, computer vision, and AI ethics. Her work has received numerous awards and has been implemented by multiple technology companies.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129179/generative-ai-hype-distracts-us-from-ais-more-important-breakthroughs/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>The great AI hype correction of 2025 (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129174/the-great-ai-hype-correction-of-2025/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Some disillusionment was inevitable. When OpenAI released a free web app called ChatGPT in late 2022, it changed the course of an entire industry—and several world economies. Millions of people started talking to their computers, and their computers started talking back. We were enchanted, and we expected more.&lt;/p&gt;  &lt;p&gt;We got it. Technology companies scrambled to stay ahead, putting out rival products that outdid one another with each new release: voice, images, video. With nonstop one-upmanship, AI companies have presented each new product drop as a major breakthrough, reinforcing a widespread faith that this technology would just keep getting better. Boosters told us that progress was exponential. They posted charts plotting how far we’d come since last year’s models: Look how the line goes up! Generative AI could do anything, it seemed.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Well, 2025 has been a year of reckoning.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;For a start, the heads of the top AI companies made promises they couldn’t keep. They told us that generative AI would replace the white-collar workforce, bring about an age of abundance, make scientific discoveries, and help find new cures for disease. FOMO across the world’s economies, at least in the Global North, made CEOs tear up their playbooks and try to get in on the action.&lt;/p&gt;  &lt;p&gt;That’s when the shine started to come off. Though the technology may have been billed as a universal multitool that could revamp outdated business processes and cut costs, a number of studies published this year suggest that firms are failing to make the AI pixie dust work its magic. Surveys and trackers from a range of sources, including the US Census Bureau and Stanford University, have found that business uptake of AI tools is stalling. And when the tools do get tried out, many projects stay stuck in the pilot stage. Without broad buy-in across the economy it is not clear how the big AI companies will ever recoup the incredible amounts they've already spent in this race.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;At the same time, updates to the core technology are no longer the step changes they once were.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;The highest-profile example of this was the botched launch of GPT-5 in August. Here was OpenAI, the firm that had ignited (and to a large extent sustained) the current boom, set to release a brand-new generation of its technology. OpenAI had been hyping GPT-5 for months: “PhD-level expert in anything,” CEO Sam Altman crowed. On another occasion Altman posted, without comment, an image of the Death Star from &lt;em&gt;Star Wars&lt;/em&gt;, which OpenAI stans took to be a symbol of ultimate power: Coming soon! Expectations were huge.&lt;/p&gt; 
 &lt;p&gt;And yet, when it landed, GPT-5 seemed to be—more of the same? What followed was the biggest vibe shift since ChatGPT first appeared three years ago. “The era of boundary-breaking advancements is over,” Yannic Kilcher, an AI researcher and popular YouTuber, announced in a video posted two days after GPT-5 came out: “AGI is not coming. It seems very much that we’re in the Samsung Galaxy era of LLMs.”&lt;/p&gt;  &lt;p&gt;A lot of people (me included) have made the analogy with phones. For a decade or so, smartphones were the most exciting consumer tech in the world. Today, new products drop from Apple or Samsung with little fanfare. While superfans pore over small upgrades, to most people this year’s iPhone now looks and feels a lot like last year’s iPhone. Is that where we are with generative AI? And is it a problem? Sure, smartphones have become the new normal. But they changed the way the world works, too.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;To be clear, the last few years have been filled with genuine “Wow” moments, from the stunning leaps in the quality of video generation models to the problem-solving chops of so-called reasoning models to the world-class competition wins of the latest coding and math models. But this remarkable technology is only a few years old, and in many ways it is still experimental. Its successes come with big caveats.&lt;/p&gt;  &lt;p&gt;Perhaps we need to readjust our expectations.&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;The big reset&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;Let’s be careful here: The pendulum from hype to anti-hype can swing too far. It would be rash to dismiss this technology just because it has been oversold. The knee-jerk response when AI fails to live up to its hype is to say that progress has hit a wall. But that misunderstands how research and innovation in tech work. Progress has always moved in fits and starts. There are ways over, around, and under walls.&lt;/p&gt;  &lt;p&gt;Take a step back from the GPT-5 launch. It came hot on the heels of a series of remarkable models that OpenAI had shipped in the previous months, including o1 and o3 (first-of-their-kind reasoning models that introduced the industry to a whole new paradigm) and Sora 2, which raised the bar for video generation once again. That doesn’t sound like hitting a wall to me.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;AI is really good! Look at Nano Banana Pro, the new image generation model from Google DeepMind that can turn a book chapter into an infographic, and much more. It’s just there—for free—on your phone.&lt;/p&gt;  &lt;p&gt;And yet you can’t help but wonder: When the wow factor is gone, what’s left? How will we view this technology a year or five from now? Will we think it was worth the colossal costs, both financial and environmental?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;With that in mind, here are four ways to think about the state of AI at the end of 2025: The start of a much-needed hype correction.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;01: &lt;strong&gt;LLMs are not everything&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In some ways, it is the hype around large language models, not AI as a whole, that needs correcting. It has become obvious that LLMs are not the doorway to artificial general intelligence, or AGI, a hypothetical technology that some insist will one day be able to do any (cognitive) task a human can.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Even an AGI evangelist like Ilya Sutskever, chief scientist and cofounder at the AI startup Safe Superintelligence and former chief scientist and cofounder at OpenAI, now highlights the limitations of LLMs, a technology he had a huge hand in creating. LLMs are very good at learning how to do a lot of specific tasks, but they do not seem to learn the principles behind those tasks, Sutskever said in an interview with Dwarkesh Patel in November.&lt;/p&gt;  &lt;p&gt;It’s the difference between learning how to solve a thousand different algebra problems and learning how to solve &lt;em&gt;any&lt;/em&gt; algebra problem. “The thing which I think is the most fundamental is that these models somehow just generalize dramatically worse than people,” Sutskever said.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;It’s easy to imagine that LLMs can do anything because their use of language is so compelling. It is astonishing how well this technology can mimic the way people write and speak. And we are hardwired to see intelligence in things that behave in certain ways—whether it’s there or not. In other words, we have built machines with humanlike behavior and cannot resist seeing a humanlike mind behind them.&lt;/p&gt;  &lt;p&gt;That’s understandable. LLMs have been part of mainstream life for only a few years. But in that time, marketers have preyed on our shaky sense of what the technology can really do, pumping up expectations and turbocharging the hype. As we live with this technology and come to understand it better, those expectations should fall back down to earth.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;0&lt;strong&gt;2&lt;/strong&gt;: &lt;strong&gt; AI is not a quick fix to all your problems&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In July, researchers at MIT published a study that became a tentpole talking point in the disillusionment camp. The headline result was that a whopping 95% of businesses that had tried using AI had found zero value in it.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The general thrust of that claim was echoed by other research, too. In November,&amp;nbsp;a study by researchers at Upwork, a company that runs an online marketplace for freelancers, found that agents powered by top LLMs from OpenAI, Google DeepMind, and Anthropic failed to complete many straightforward workplace tasks by themselves.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;This is miles off Altman’s prediction: “We believe that, in 2025, we may see the first AI agents ‘join the workforce’ and materially change the output of companies,” he wrote on his personal blog in January.&lt;/p&gt;  &lt;p&gt;But what gets missed in that MIT study is that the researchers’ measure of success was pretty narrow. That 95% failure rate accounts for companies that had tried to implement bespoke AI systems but had not yet scaled them beyond the pilot stage after six months. It shouldn’t be too surprising that a lot of experiments with experimental technology don’t pan out straight away.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;p&gt;That number also does not include the use of LLMs by employees outside of official pilots. The MIT researchers found that around 90% of the companies they surveyed had a kind of AI shadow economy where workers were using personal chatbot accounts. But the value of that shadow economy was not measured.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When the Upwork study looked at how well agents completed tasks together with people who knew what they were doing, success rates shot up. The takeaway seems to be that a lot of people are figuring out for themselves how AI might help them with their jobs.&lt;/p&gt; 
 &lt;p&gt;That fits with something the AI researcher and influencer (and coiner of the term “vibe coding”) Andrej Karpathy has noted: Chatbots are better than the average human at a lot of different things (think of giving legal advice, fixing bugs, doing high school math), but they are not better than an expert human. Karpathy suggests this may be why chatbots have proved popular with individual consumers, helping non-experts with everyday questions and tasks, but they have not upended the economy, which would require outperforming skilled employees at their jobs.&lt;/p&gt;  &lt;p&gt;That may change. For now, don’t be surprised that AI has not (yet) had the impact on jobs that boosters said it would. AI is not a quick fix, and it cannot replace humans. But there’s a lot to play for. The ways in which AI could be integrated into everyday workflows and business pipelines are still being tried out.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;0&lt;strong&gt;3: Are we in a bubble? (If so, what kind of bubble?)&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;If AI is a bubble, is it like the subprime mortgage bubble of 2008 or the internet bubble of 2000? Because there’s a big difference.&lt;/p&gt;  &lt;p&gt;The subprime bubble wiped out a big part of the economy, because when it burst it left nothing behind except debt and overvalued real estate. The dot-com bubble wiped out a lot of companies, which sent ripples across the world, but it left behind the infant internet—an international network of cables and a handful of startups, like Google and Amazon, that became the tech giants of today.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then again, maybe we’re in a bubble unlike either of those. After all, there’s no real business model for LLMs right now. We don’t yet know what the killer app will be, or if there will even be one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And many economists are concerned about the unprecedented amounts of money being sunk into the infrastructure required to build capacity and serve the projected demand. But what if that demand doesn’t materialize? Add to that the weird circularity of many of those deals—with Nvidia paying OpenAI to pay Nvidia, and so on—and it’s no surprise everybody’s got a different take on what’s coming.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_21"&gt; &lt;p&gt;Some investors remain sanguine. In an interview with the &lt;em&gt;Technology Business Programming Network&lt;/em&gt; podcast in November, Glenn Hutchins, cofounder of Silver Lake Partners, a major international private equity firm, gave a few reasons not to worry. “Every one of these data centers—almost all of them—has a solvent counterparty that is contracted to take all the output they’re built to suit,” he said. In other words, it’s not a case of “Build it and they’ll come”—the customers are already locked in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And, he pointed out, one of the biggest of those solvent counterparties is Microsoft. “Microsoft has the world’s best credit rating,” Hutchins said. “If you sign a deal with Microsoft to take the output from your data center, Satya is good for it.”&lt;/p&gt;  &lt;p&gt;Many CEOs will be looking back at the dot-com bubble and trying to learn its lessons. Here’s one way to see it: The companies that went bust back then didn’t have the money to last the distance. Those that survived the crash thrived.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;p&gt;With that lesson in mind, AI companies today are trying to pay their way through what may or may not be a bubble. Stay in the race; don’t get left behind. Even so, it’s a desperate gamble.&lt;/p&gt; 
 &lt;p&gt;But there’s another lesson too. Companies that might look like sideshows can turn into unicorns fast. Take Synthesia, which makes avatar generation tools for businesses. Nathan Benaich, cofounder of the VC firm Air Street Capital, admits that when he first heard about the company a few years ago, back when fear of deepfakes was rife, he wasn’t sure what its tech was for and thought there was no market for it.&lt;/p&gt;  &lt;p&gt;“We didn’t know who would pay for lip-synching and voice cloning,” he says. “Turns out there’s a lot of people who wanted to pay for it.” Synthesia now has around 55,000 corporate customers and brings in around $150 million a year. In October, the company was valued at $4 billion.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;04: ChatGPT was not the beginning, and it won’t be the end&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;ChatGPT was the culmination of a decade’s worth of progress in deep learning, the technology that underpins all of modern AI. The seeds of deep learning itself were planted in the 1980s. The field as a whole goes back at least to the 1950s. If progress is measured against that backdrop, generative AI has barely got going.&lt;/p&gt;  &lt;p&gt;Meanwhile, research is at a fever pitch. There are more high-quality submissions to the world’s major AI conferences than ever before. This year, organizers of some of those conferences resorted to turning down papers that reviewers had already approved, just to manage numbers. (At the same time, preprint servers like arXiv have been flooded with AI-generated research slop.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;“It’s back to the age of research again,” Sutskever said in that Dwarkesh interview, talking about the current bottleneck with LLMs. That’s not a setback; that’s the start of something new.&lt;/p&gt;  &lt;p&gt;“There’s always a lot of hype beasts,” says Benaich. But he thinks there’s an upside to that: Hype attracts the money and talent needed to make real progress. “You know, it was only like two or three years ago that the people who built these models were basically research nerds that just happened on something that kind of worked,” he says. “Now everybody who’s good at anything in technology is working on this.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt;&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Where do we go from here?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;The relentless hype hasn’t come just from companies drumming up business for their vastly expensive new technologies. There’s a large cohort of people—inside and outside the industry—who want to believe in the promise of machines that can read, write, and &lt;em&gt;think&lt;/em&gt;. It’s a wild decades-old dream.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But the hype was never sustainable—and that’s a good thing. We now have a chance to reset expectations and see this technology for what it really is—assess its true capabilities, understand its flaws, and take the time to learn how to apply it in valuable (and beneficial) ways. “We’re still trying to figure out how to invoke certain behaviors from this insanely high-dimensional black box of information and skills,” says Benaich.&lt;/p&gt;  &lt;p&gt;This hype correction was long overdue. But know that AI isn’t going anywhere. We don’t even fully understand what we’ve built so far, let alone what’s coming next.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-1.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Some disillusionment was inevitable. When OpenAI released a free web app called ChatGPT in late 2022, it changed the course of an entire industry—and several world economies. Millions of people started talking to their computers, and their computers started talking back. We were enchanted, and we expected more.&lt;/p&gt;  &lt;p&gt;We got it. Technology companies scrambled to stay ahead, putting out rival products that outdid one another with each new release: voice, images, video. With nonstop one-upmanship, AI companies have presented each new product drop as a major breakthrough, reinforcing a widespread faith that this technology would just keep getting better. Boosters told us that progress was exponential. They posted charts plotting how far we’d come since last year’s models: Look how the line goes up! Generative AI could do anything, it seemed.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Well, 2025 has been a year of reckoning.&amp;nbsp;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;For a start, the heads of the top AI companies made promises they couldn’t keep. They told us that generative AI would replace the white-collar workforce, bring about an age of abundance, make scientific discoveries, and help find new cures for disease. FOMO across the world’s economies, at least in the Global North, made CEOs tear up their playbooks and try to get in on the action.&lt;/p&gt;  &lt;p&gt;That’s when the shine started to come off. Though the technology may have been billed as a universal multitool that could revamp outdated business processes and cut costs, a number of studies published this year suggest that firms are failing to make the AI pixie dust work its magic. Surveys and trackers from a range of sources, including the US Census Bureau and Stanford University, have found that business uptake of AI tools is stalling. And when the tools do get tried out, many projects stay stuck in the pilot stage. Without broad buy-in across the economy it is not clear how the big AI companies will ever recoup the incredible amounts they've already spent in this race.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;At the same time, updates to the core technology are no longer the step changes they once were.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;The highest-profile example of this was the botched launch of GPT-5 in August. Here was OpenAI, the firm that had ignited (and to a large extent sustained) the current boom, set to release a brand-new generation of its technology. OpenAI had been hyping GPT-5 for months: “PhD-level expert in anything,” CEO Sam Altman crowed. On another occasion Altman posted, without comment, an image of the Death Star from &lt;em&gt;Star Wars&lt;/em&gt;, which OpenAI stans took to be a symbol of ultimate power: Coming soon! Expectations were huge.&lt;/p&gt; 
 &lt;p&gt;And yet, when it landed, GPT-5 seemed to be—more of the same? What followed was the biggest vibe shift since ChatGPT first appeared three years ago. “The era of boundary-breaking advancements is over,” Yannic Kilcher, an AI researcher and popular YouTuber, announced in a video posted two days after GPT-5 came out: “AGI is not coming. It seems very much that we’re in the Samsung Galaxy era of LLMs.”&lt;/p&gt;  &lt;p&gt;A lot of people (me included) have made the analogy with phones. For a decade or so, smartphones were the most exciting consumer tech in the world. Today, new products drop from Apple or Samsung with little fanfare. While superfans pore over small upgrades, to most people this year’s iPhone now looks and feels a lot like last year’s iPhone. Is that where we are with generative AI? And is it a problem? Sure, smartphones have become the new normal. But they changed the way the world works, too.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;To be clear, the last few years have been filled with genuine “Wow” moments, from the stunning leaps in the quality of video generation models to the problem-solving chops of so-called reasoning models to the world-class competition wins of the latest coding and math models. But this remarkable technology is only a few years old, and in many ways it is still experimental. Its successes come with big caveats.&lt;/p&gt;  &lt;p&gt;Perhaps we need to readjust our expectations.&lt;/p&gt;  &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;The big reset&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;Let’s be careful here: The pendulum from hype to anti-hype can swing too far. It would be rash to dismiss this technology just because it has been oversold. The knee-jerk response when AI fails to live up to its hype is to say that progress has hit a wall. But that misunderstands how research and innovation in tech work. Progress has always moved in fits and starts. There are ways over, around, and under walls.&lt;/p&gt;  &lt;p&gt;Take a step back from the GPT-5 launch. It came hot on the heels of a series of remarkable models that OpenAI had shipped in the previous months, including o1 and o3 (first-of-their-kind reasoning models that introduced the industry to a whole new paradigm) and Sora 2, which raised the bar for video generation once again. That doesn’t sound like hitting a wall to me.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;AI is really good! Look at Nano Banana Pro, the new image generation model from Google DeepMind that can turn a book chapter into an infographic, and much more. It’s just there—for free—on your phone.&lt;/p&gt;  &lt;p&gt;And yet you can’t help but wonder: When the wow factor is gone, what’s left? How will we view this technology a year or five from now? Will we think it was worth the colossal costs, both financial and environmental?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;With that in mind, here are four ways to think about the state of AI at the end of 2025: The start of a much-needed hype correction.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;01: &lt;strong&gt;LLMs are not everything&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In some ways, it is the hype around large language models, not AI as a whole, that needs correcting. It has become obvious that LLMs are not the doorway to artificial general intelligence, or AGI, a hypothetical technology that some insist will one day be able to do any (cognitive) task a human can.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Even an AGI evangelist like Ilya Sutskever, chief scientist and cofounder at the AI startup Safe Superintelligence and former chief scientist and cofounder at OpenAI, now highlights the limitations of LLMs, a technology he had a huge hand in creating. LLMs are very good at learning how to do a lot of specific tasks, but they do not seem to learn the principles behind those tasks, Sutskever said in an interview with Dwarkesh Patel in November.&lt;/p&gt;  &lt;p&gt;It’s the difference between learning how to solve a thousand different algebra problems and learning how to solve &lt;em&gt;any&lt;/em&gt; algebra problem. “The thing which I think is the most fundamental is that these models somehow just generalize dramatically worse than people,” Sutskever said.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;It’s easy to imagine that LLMs can do anything because their use of language is so compelling. It is astonishing how well this technology can mimic the way people write and speak. And we are hardwired to see intelligence in things that behave in certain ways—whether it’s there or not. In other words, we have built machines with humanlike behavior and cannot resist seeing a humanlike mind behind them.&lt;/p&gt;  &lt;p&gt;That’s understandable. LLMs have been part of mainstream life for only a few years. But in that time, marketers have preyed on our shaky sense of what the technology can really do, pumping up expectations and turbocharging the hype. As we live with this technology and come to understand it better, those expectations should fall back down to earth.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;0&lt;strong&gt;2&lt;/strong&gt;: &lt;strong&gt; AI is not a quick fix to all your problems&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In July, researchers at MIT published a study that became a tentpole talking point in the disillusionment camp. The headline result was that a whopping 95% of businesses that had tried using AI had found zero value in it.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The general thrust of that claim was echoed by other research, too. In November,&amp;nbsp;a study by researchers at Upwork, a company that runs an online marketplace for freelancers, found that agents powered by top LLMs from OpenAI, Google DeepMind, and Anthropic failed to complete many straightforward workplace tasks by themselves.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;This is miles off Altman’s prediction: “We believe that, in 2025, we may see the first AI agents ‘join the workforce’ and materially change the output of companies,” he wrote on his personal blog in January.&lt;/p&gt;  &lt;p&gt;But what gets missed in that MIT study is that the researchers’ measure of success was pretty narrow. That 95% failure rate accounts for companies that had tried to implement bespoke AI systems but had not yet scaled them beyond the pilot stage after six months. It shouldn’t be too surprising that a lot of experiments with experimental technology don’t pan out straight away.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt; &lt;p&gt;That number also does not include the use of LLMs by employees outside of official pilots. The MIT researchers found that around 90% of the companies they surveyed had a kind of AI shadow economy where workers were using personal chatbot accounts. But the value of that shadow economy was not measured.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;When the Upwork study looked at how well agents completed tasks together with people who knew what they were doing, success rates shot up. The takeaway seems to be that a lot of people are figuring out for themselves how AI might help them with their jobs.&lt;/p&gt; 
 &lt;p&gt;That fits with something the AI researcher and influencer (and coiner of the term “vibe coding”) Andrej Karpathy has noted: Chatbots are better than the average human at a lot of different things (think of giving legal advice, fixing bugs, doing high school math), but they are not better than an expert human. Karpathy suggests this may be why chatbots have proved popular with individual consumers, helping non-experts with everyday questions and tasks, but they have not upended the economy, which would require outperforming skilled employees at their jobs.&lt;/p&gt;  &lt;p&gt;That may change. For now, don’t be surprised that AI has not (yet) had the impact on jobs that boosters said it would. AI is not a quick fix, and it cannot replace humans. But there’s a lot to play for. The ways in which AI could be integrated into everyday workflows and business pipelines are still being tried out.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;0&lt;strong&gt;3: Are we in a bubble? (If so, what kind of bubble?)&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;If AI is a bubble, is it like the subprime mortgage bubble of 2008 or the internet bubble of 2000? Because there’s a big difference.&lt;/p&gt;  &lt;p&gt;The subprime bubble wiped out a big part of the economy, because when it burst it left nothing behind except debt and overvalued real estate. The dot-com bubble wiped out a lot of companies, which sent ripples across the world, but it left behind the infant internet—an international network of cables and a handful of startups, like Google and Amazon, that became the tech giants of today.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then again, maybe we’re in a bubble unlike either of those. After all, there’s no real business model for LLMs right now. We don’t yet know what the killer app will be, or if there will even be one.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And many economists are concerned about the unprecedented amounts of money being sunk into the infrastructure required to build capacity and serve the projected demand. But what if that demand doesn’t materialize? Add to that the weird circularity of many of those deals—with Nvidia paying OpenAI to pay Nvidia, and so on—and it’s no surprise everybody’s got a different take on what’s coming.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_21"&gt; &lt;p&gt;Some investors remain sanguine. In an interview with the &lt;em&gt;Technology Business Programming Network&lt;/em&gt; podcast in November, Glenn Hutchins, cofounder of Silver Lake Partners, a major international private equity firm, gave a few reasons not to worry. “Every one of these data centers—almost all of them—has a solvent counterparty that is contracted to take all the output they’re built to suit,” he said. In other words, it’s not a case of “Build it and they’ll come”—the customers are already locked in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And, he pointed out, one of the biggest of those solvent counterparties is Microsoft. “Microsoft has the world’s best credit rating,” Hutchins said. “If you sign a deal with Microsoft to take the output from your data center, Satya is good for it.”&lt;/p&gt;  &lt;p&gt;Many CEOs will be looking back at the dot-com bubble and trying to learn its lessons. Here’s one way to see it: The companies that went bust back then didn’t have the money to last the distance. Those that survived the crash thrived.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;p&gt;With that lesson in mind, AI companies today are trying to pay their way through what may or may not be a bubble. Stay in the race; don’t get left behind. Even so, it’s a desperate gamble.&lt;/p&gt; 
 &lt;p&gt;But there’s another lesson too. Companies that might look like sideshows can turn into unicorns fast. Take Synthesia, which makes avatar generation tools for businesses. Nathan Benaich, cofounder of the VC firm Air Street Capital, admits that when he first heard about the company a few years ago, back when fear of deepfakes was rife, he wasn’t sure what its tech was for and thought there was no market for it.&lt;/p&gt;  &lt;p&gt;“We didn’t know who would pay for lip-synching and voice cloning,” he says. “Turns out there’s a lot of people who wanted to pay for it.” Synthesia now has around 55,000 corporate customers and brings in around $150 million a year. In October, the company was valued at $4 billion.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;04: ChatGPT was not the beginning, and it won’t be the end&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;ChatGPT was the culmination of a decade’s worth of progress in deep learning, the technology that underpins all of modern AI. The seeds of deep learning itself were planted in the 1980s. The field as a whole goes back at least to the 1950s. If progress is measured against that backdrop, generative AI has barely got going.&lt;/p&gt;  &lt;p&gt;Meanwhile, research is at a fever pitch. There are more high-quality submissions to the world’s major AI conferences than ever before. This year, organizers of some of those conferences resorted to turning down papers that reviewers had already approved, just to manage numbers. (At the same time, preprint servers like arXiv have been flooded with AI-generated research slop.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;“It’s back to the age of research again,” Sutskever said in that Dwarkesh interview, talking about the current bottleneck with LLMs. That’s not a setback; that’s the start of something new.&lt;/p&gt;  &lt;p&gt;“There’s always a lot of hype beasts,” says Benaich. But he thinks there’s an upside to that: Hype attracts the money and talent needed to make real progress. “You know, it was only like two or three years ago that the people who built these models were basically research nerds that just happened on something that kind of worked,” he says. “Now everybody who’s good at anything in technology is working on this.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt;&lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Where do we go from here?&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;The relentless hype hasn’t come just from companies drumming up business for their vastly expensive new technologies. There’s a large cohort of people—inside and outside the industry—who want to believe in the promise of machines that can read, write, and &lt;em&gt;think&lt;/em&gt;. It’s a wild decades-old dream.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But the hype was never sustainable—and that’s a good thing. We now have a chance to reset expectations and see this technology for what it really is—assess its true capabilities, understand its flaws, and take the time to learn how to apply it in valuable (and beneficial) ways. “We’re still trying to figure out how to invoke certain behaviors from this insanely high-dimensional black box of information and skills,” says Benaich.&lt;/p&gt;  &lt;p&gt;This hype correction was long overdue. But know that AI isn’t going anywhere. We don’t even fully understand what we’ve built so far, let alone what’s coming next.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129174/the-great-ai-hype-correction-of-2025/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>The AI doomers feel undeterred (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129171/the-ai-doomers-feel-undeterred/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-4_167ce7.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;It’s a weird time to be an AI doomer.&lt;/p&gt;  &lt;p&gt;This small but influential community of researchers, scientists, and policy experts believes, in the simplest terms, that AI could get so good it could be bad—very, very bad—for humanity. Though many of these people would be more likely to describe themselves as advocates for AI safety than as literal doomsayers, they warn that AI poses an existential risk to humanity. They argue that absent more regulation, the industry could hurtle toward systems it can’t control. They commonly expect such systems to follow the creation of artificial general intelligence (AGI), a slippery concept generally understood as technology that can do whatever humans can do, and better.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Though this is far from a universally shared perspective in the AI field, the doomer crowd has had some notable success over the past several years: helping shape AI policy coming from the Biden administration, organizing prominent calls for international “red lines” to prevent AI risks, and getting a bigger (and more influential) megaphone as some of its adherents win science’s most prestigious awards.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But a number of developments over the past six months have put them on the back foot. Talk of an AI bubble has overwhelmed the discourse as tech companies continue to invest in multiple Manhattan Projects’ worth of data centers without any certainty that future demand will match what they’re building.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;And then there was the August release of OpenAI’s latest foundation model, GPT-5, which proved something of a letdown. Maybe that was inevitable, since it was the most hyped AI release of all time; OpenAI CEO Sam Altman had boasted that GPT-5 felt “like a PhD-level expert” in every topic and told the podcaster Theo Von that the model was so good, it had made him feel “useless relative to the AI.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Many expected GPT-5 to be a big step toward AGI, but whatever progress the model may have made was overshadowed by a string of technical bugs and the company’s mystifying, quickly reversed decision to shut off access to every old OpenAI model without warning. And while the new model achieved state-of-the-art benchmark scores, many people felt, perhaps unfairly, that in day-to-day use GPT-5 was a step backward.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;All this would seem to threaten some of the very foundations of the doomers’ case. In turn, a competing camp of AI accelerationists, who fear AI is actually not moving fast enough and that the industry is constantly at risk of being smothered by overregulation, is seeing a fresh chance to change how we approach AI safety (or, maybe more accurately, how we don’t).&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;This is particularly true of the industry types who’ve decamped to Washington: “The Doomer narratives were wrong,” declared David Sacks, the longtime venture capitalist turned Trump administration AI czar. “This notion of imminent AGI has been a distraction and harmful and now effectively proven wrong,” echoed the White House’s senior policy advisor for AI and tech investor Sriram Krishnan. (Sacks and Krishnan did not reply to requests for comment.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;(There is, of course, another camp in the AI safety debate: the group of researchers and advocates commonly associated with the label “AI ethics.” Though they also favor regulation, they tend to think the speed of AI progress has been overstated and have often written off AGI as a sci-fi story or a scam that distracts us from the technology’s immediate threats. But any potential doomer demise wouldn’t exactly give them the same opening the accelerationists are seeing.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;So where does this leave the doomers?&lt;strong&gt; &lt;/strong&gt;As part of our Hype Correction package, we decided to ask some of the movement’s biggest names to see if the recent setbacks and general vibe shift had altered their views. Are they frustrated that policymakers no longer seem to heed their threats? Are they quietly adjusting their timelines for the apocalypse?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Recent interviews with 20 people who study or advocate AI safety and governance—including Nobel Prize winner Geoffrey Hinton, Turing Prize winner Yoshua Bengio, and high-profile experts like former OpenAI board member Helen Toner—reveal that rather than feeling chastened or lost in the wilderness, they’re still deeply committed to their cause, believing that AGI remains not just possible but incredibly dangerous. &lt;/p&gt;  &lt;p&gt;At the same time, they seem to be grappling with a near contradiction. While they’re somewhat relieved that recent developments suggest AGI is further out than they previously thought (“Thank God we have more time,” says AI researcher Jeffrey Ladish), they also feel angry that people in power are not taking them seriously enough (Daniel Kokotajlo, lead author of a cautionary forecast called “AI 2027,” calls the Sacks and Krishnan tweets “deranged and/or dishonest”).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Broadly speaking, these experts see the talk of an AI bubble as no more than a speed bump, and disappointment in GPT-5 as more distracting than illuminating. They still generally favor more robust regulation and worry that progress on policy—the implementation of the EU AI Act; the passage of the first major American AI safety bill, California’s SB 53; and new interest in AGI risk from some members of Congress—has become vulnerable as Washington overreacts to what doomers see as short-term failures to live up to the hype.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Some were also eager to correct what they see as the most persistent misconceptions about the doomer world. Though their critics routinely mock them for predicting that AGI is right around the corner, they claim that’s never been an essential part of their case: It “isn’t about imminence,” says Berkeley professor Stuart Russell, the author of &lt;em&gt;Human Compatible&lt;/em&gt;&lt;em&gt;: Artificial Intelligence and the Problem of Control&lt;/em&gt;. Most people I spoke with say their timelines to dangerous systems have actually &lt;em&gt;lengthened&lt;/em&gt; slightly in the last year—an important change given how quickly the policy and technical landscapes can shift.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-pullquote alignleft has-text-align-left"&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;“If someone said there’s a four-mile-diameter asteroid that’s going to hit the Earth in 2067, we wouldn’t say, 'Remind me in 2066 and we’ll think about it.'”&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;  &lt;p&gt;Many of them, in fact, emphasize the importance of changing timelines. And even if they are &lt;em&gt;just a tad&lt;/em&gt; longer now, Toner tells me that one big-picture story of the ChatGPT era is the dramatic compression of these estimates &lt;em&gt;across&lt;/em&gt; the AI world. For a long while, she says, AGI was expected in many decades. Now, for the most part, the predicted arrival is sometime in the next few years to 20 years. So even if we have a little bit more time, she (and many of her peers) continue to see AI safety as incredibly, vitally urgent. She tells me that if AGI were possible anytime in even the next 30 years, “It’s a huge fucking deal. We should have a lot of people working on this.”&lt;/p&gt;  &lt;p&gt;So despite the precarious moment doomers find themselves in, their bottom line remains that no matter when AGI is coming (and, again, they say it’s very likely coming), the world is far from ready.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Maybe you agree. Or maybe you may think this future is far from guaranteed. Or that it’s the stuff of science fiction. You may even think AGI is a great big conspiracy theory. You’re not alone, of course—this topic is polarizing. But whatever you think about the doomer mindset, there’s no getting around the fact that certain people in this world have a lot of influence. So here are some of the most prominent people in the space, reflecting on this moment in their own words.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Interviews have been edited and condensed for length and clarity.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;h3 class="wp-block-heading"&gt;The Nobel laureate who’s not sure what’s coming&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Geoffrey Hinton, winner of the Turing Award and the Nobel Prize in physics for pioneering deep learning&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;The biggest change in the last few years is that there are people who are hard to dismiss who are saying this stuff is dangerous. Like, [former Google CEO] Eric Schmidt, for example, really recognized this stuff could be really dangerous. He and I were in China recently talking to someone on the Politburo, the party secretary of Shanghai, to make sure he really understood—and he did. I think in China, the leadership understands AI and its dangers much better because many of them are engineers.&lt;/p&gt;  &lt;p&gt;I’ve been focused on the longer-term threat: When AIs get more intelligent than us, can we really expect that humans will remain in control or even relevant? But I don’t think anything is inevitable. There’s huge uncertainty on everything. We’ve never been here before. Anybody who’s confident they know what’s going to happen seems silly to me. I think this is very unlikely but maybe it’ll turn out that all the people saying AI is way overhyped are correct. Maybe it’ll turn out that we can’t get much further than the current chatbots—we hit a wall due to limited data. I don’t believe that. I think that’s unlikely, but it’s possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I also don’t believe people like Eliezer Yudkowsky, who say if anybody builds it, we’re all going to die. We don’t know that.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But if you go on the balance of the evidence, I think it’s fair to say that most experts who know a lot about AI believe it’s very probable that we’ll have superintelligence within the next 20 years. [Google DeepMind CEO] Demis Hassabis says maybe 10 years. Even [prominent AI skeptic] Gary Marcus would probably say, “Well, if you guys make a hybrid system with good old-fashioned symbolic logic … maybe that’ll be superintelligent.” &lt;em&gt;[Editor’s note: In September, Marcus &lt;/em&gt;&lt;em&gt;predicted&lt;/em&gt;&lt;em&gt; AGI would arrive between 2033 and 2040.]&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;And I don’t think anybody believes progress will stall at AGI. I think more or less everybody believes a few years after AGI, we’ll have superintelligence, because the AGI will be better than us at building AI.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;So while I think it’s clear that the winds are getting more difficult, simultaneously, people are putting in many more resources [into developing advanced AI]. I think progress will continue just because there’s many more resources going in.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The deep learning pioneer who wishes he’d seen the risks sooner&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Yoshua Bengio, winner of the Turing Award, chair of the &lt;/em&gt;&lt;em&gt;International AI Safety Report&lt;/em&gt;&lt;em&gt;, and founder of &lt;/em&gt;&lt;em&gt;LawZero&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;Some people thought that GPT-5 meant we had hit a wall, but that isn’t quite what you see in the scientific data and trends.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;There have been people overselling the idea that &lt;em&gt;AGI is tomorrow morning&lt;/em&gt;, which commercially could make sense. But if you look at the various benchmarks, GPT-5 is just where you would expect the models at that point in time to be. By the way, it’s not just GPT-5, it’s Claude and Google models, too.&lt;strong&gt; &lt;/strong&gt;In some areas where AI systems weren’t very good, like Humanity’s Last Exam or FrontierMath, they’re getting much better scores now than they were at the beginning of the year.&lt;/p&gt;  &lt;p&gt;At the same time, the overall landscape for AI governance and safety is not good. There’s a strong force pushing against regulation. It’s like climate change. We can put our head in the sand and hope it’s going to be fine, but it doesn’t really deal with the issue.&lt;/p&gt; 
 &lt;p&gt;The biggest disconnect with policymakers is a misunderstanding of the scale of change that is likely to happen if the trend of AI progress continues. A lot of people in business and governments simply think of AI as just another technology that’s going to be economically very powerful. They don’t understand how much it might change the world if trends continue, and we approach human-level AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Like many people, I had been blinding myself to the potential risks to some extent. I should have seen it coming much earlier. But it’s human. You’re excited about your work and you want to see the good side of it. That makes us a little bit biased in not really paying attention to the bad things that could happen.&lt;/p&gt; 
 &lt;p&gt;Even a small chance—like 1% or 0.1%—of creating an accident where billions of people die is not acceptable.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The AI veteran who believes AI is progressing—but not fast enough to prevent the bubble from bursting&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Stuart Russell, distinguished professor of computer science, University of California, Berkeley, and author of &lt;/em&gt;Human Compatible&lt;/h5&gt;  &lt;p&gt;I hope the idea that talking about existential risk makes you a “doomer” or is “science fiction” comes to be seen as fringe, given that most leading AI researchers and most leading AI CEOs take it seriously.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There have been claims that AI could never pass a Turing test, or you could never have a system that uses natural language fluently, or one that could parallel-park a car. All these claims just end up getting disproved by progress.&lt;/p&gt;  &lt;p&gt;People are spending trillions of dollars to make superhuman AI happen. I think they need some new ideas, but there’s a significant chance they will come up with them, because many significant new ideas have happened in the last few years.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_19"&gt; &lt;p&gt;My fairly consistent estimate for the last 12 months has been that there’s a 75% chance that those breakthroughs are not going to happen in time to rescue the industry from the bursting of the bubble. Because the investments are consistent with a prediction that we’re going to have much better AI that will deliver much more value to real customers. But if those predictions don’t come true, then there’ll be a lot of blood on the floor in the stock markets.&lt;/p&gt;  &lt;p&gt;However, the safety case isn’t about imminence. It’s about the fact that we still don’t have a solution to the control problem. If someone said there’s a four-mile-diameter asteroid that’s going to hit the Earth in 2067, we wouldn’t say, “Remind me in 2066 and we’ll think about it.” We don’t know how long it takes to develop the technology needed to control superintelligent AI.&lt;/p&gt;  &lt;p&gt;Looking at precedents, the acceptable level of risk for a nuclear plant melting down is about one in a million per year. Extinction is much worse than that. So maybe set the acceptable risk at one in a billion. But the companies are saying it’s something like one in five. They don’t know how to make it acceptable. And that’s a problem.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The professor trying to set the narrative straight on AI safety&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;David Krueger&lt;/em&gt;&lt;em&gt;, assistant professor in machine learning at the University of Montreal and Yoshua Bengio’s Mila Institute, and founder of &lt;/em&gt;&lt;em&gt;Evitable&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;I think people definitely overcorrected in their response to GPT-5. But there was hype. My recollection was that there were multiple statements from CEOs at various levels of explicitness who basically said that by the end of 2025, we’re going to have an automated drop-in replacement remote worker. But it seems like it’s been underwhelming, with agents just not really being there yet.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_21"&gt; &lt;p&gt;I’ve been surprised how much these narratives predicting AGI in 2027 capture the public attention. When 2027 comes around, if things still look pretty normal, I think people are going to feel like the whole worldview has been falsified. And it’s really annoying how often when I’m talking to people about AI safety, they assume that I think we have really short timelines to dangerous systems, or that I think LLMs or deep learning are going to give us AGI. They ascribe all these extra assumptions to me that aren’t necessary to make the case.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’d expect we need decades for the international coordination problem. So even if dangerous AI is decades off, it’s already urgent. That point seems really lost on a lot of people. There’s this idea of “Let’s wait until we have a really dangerous system and then start governing it.” Man, that is way too late.&lt;/p&gt;  &lt;p&gt;I still think people in the safety community tend to work behind the scenes, with people in power, not really with civil society. It gives ammunition to people who say it’s all just a scam or insider lobbying. That’s not to say that there’s no truth to these narratives, but the underlying risk is still real. We need more public awareness and a broad base of support to have an effective response.&lt;/p&gt;  &lt;p&gt;If you actually believe there’s a 10% chance of doom in the next 10 years—which I think a reasonable person should, if they take a close look—then the first thing you think is: “Why are we doing this? This is crazy.” That’s just a very reasonable response once you buy the premise.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;h3 class="wp-block-heading"&gt;The governance expert worried about AI safety’s credibility&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Helen Toner, acting executive director of Georgetown University’s &lt;/em&gt;&lt;em&gt;Center for Security and Emerging Technology&lt;/em&gt;&lt;em&gt; and former OpenAI board member&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;When I got into the space, AI safety was more of a set of philosophical ideas. Today, it’s a thriving set of subfields of machine learning, filling in the gulf between some of the more “out there” concerns about AI scheming, deception, or power-seeking and real concrete systems we can test and play with.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-pullquote alignleft has-text-align-left"&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;“I worry that some aggressive AGI timeline estimates from some AI safety people are setting them up for a boy-who-cried-wolf moment.”&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;  &lt;p&gt;AI governance is improving slowly. If we have lots of time to adapt and governance can keep improving slowly, I feel not bad. If we don’t have much time, then we’re probably moving too slow.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;I think GPT-5 is generally seen as a disappointment in DC. There’s a pretty polarized conversation around: Are we going to have AGI and superintelligence in the next few years? Or is AI actually just totally all hype and useless and a bubble? The pendulum had maybe swung too far toward “We’re going to have super-capable systems very, very soon.” And so now it’s swinging back toward “It’s all hype.”&lt;/p&gt;  &lt;p&gt;I worry that some aggressive AGI timeline estimates from some AI safety people are setting them up for a boy-who-cried-wolf moment. When the predictions about AGI coming in 2027 don’t come true, people will say, “Look at all these people who made fools of themselves. You should never listen to them again.” That’s not the intellectually honest response,&lt;strong&gt; &lt;/strong&gt;if maybe they later changed their mind, or their take was that they only thought it&amp;nbsp;was 20 percent likely and they thought that was still worth paying attention to. I think that shouldn't be disqualifying for people to listen to you later,&lt;strong&gt; &lt;/strong&gt;but I do worry it will be a big credibility hit. And that’s applying to people who are very concerned about AI safety and never said anything about very short timelines.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The AI security researcher who now believes AGI is further out—and is grateful&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Jeffrey Ladish, executive director at &lt;/em&gt;&lt;em&gt;Palisade Research&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;In the last year, two big things updated my AGI timelines.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;First, the lack of high-quality data turned out to be a bigger problem than I expected.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Second, the first “reasoning” model, OpenAI’s o1 in September 2024, showed reinforcement learning scaling was more effective than I thought it would be. And then months later, you see the o1 to o3 scale-up and you see pretty crazy impressive performance in math and coding and science—domains where it’s easier to sort of verify the results. But while we’re seeing continued progress, it could have been much faster.&lt;/p&gt;  &lt;p&gt;All of this bumps up my median estimate to the start of fully automated AI research and development from three years to maybe five or six years. But those are kind of made up numbers. It’s hard. I want to caveat all this with, like, “Man, it’s just really hard to do forecasting here.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt; &lt;p&gt;Thank God we have more time. We have a possibly very brief window of opportunity to really try to understand these systems before they are capable and strategic enough to pose a real threat to our ability to control them.&lt;/p&gt;  &lt;p&gt;But it’s scary to see people think that we’re not making progress anymore when that’s clearly not true. I just know it’s not true because I use the models. One of the downsides of the way AI is progressing is that how fast it’s moving is becoming less legible to normal people.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now, this is not true in some domains—like, look at Sora 2. It is so obvious to anyone who looks at it that Sora 2 is vastly better than what came before. But if you ask GPT-4 and GPT-5 why the sky is blue, they’ll give you basically the same answer. It is the correct answer. It’s already saturated the ability to tell you why the sky is blue. So the people who I expect to most understand AI progress right now are the people who are actually building with AIs or using AIs on very difficult scientific problems.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The AGI forecaster who saw the critics coming&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Daniel Kokotajlo, executive director of the &lt;/em&gt;&lt;em&gt;AI Futures Project&lt;/em&gt;&lt;em&gt;; an &lt;/em&gt;&lt;em&gt;OpenAI whistleblower&lt;/em&gt;&lt;em&gt;; and lead author of “&lt;/em&gt;&lt;em&gt;AI 2027&lt;/em&gt;&lt;em&gt;,” a vivid scenario where—starting in 2027—AIs progress from “superhuman coders” to “wildly superintelligent” systems in the span of months&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;AI policy seems to be getting worse, like the “Pro-AI” super PAC [launched earlier this year by executives from OpenAI and Andreessen Horowitz to lobby for a deregulatory agenda], and the deranged and/or dishonest tweets from Sriram Krishnan and David Sacks. AI safety research is progressing at the usual pace, which is excitingly rapid compared to most fields, but slow compared to how fast it needs to be.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_30"&gt;&lt;p&gt;We said on the first page of “AI 2027” that our timelines were somewhat longer than 2027. So even when we launched AI 2027, we expected there to be a bunch of critics in 2028 triumphantly saying we’ve been discredited, like the tweets from Sacks and Krishnan. But we thought, and continue to think, that the intelligence explosion will probably happen sometime in the next five to 10 years, and that when it does, people will remember our scenario and realize it was closer to the truth than anything else available in 2025.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Predicting the future is hard, but it’s valuable to try; people should aim to communicate their uncertainty about the future in a way that is specific and falsifiable. This is what we’ve done and very few others have done. Our critics mostly haven’t made predictions of their own and often exaggerate and mischaracterize our views. They say our timelines are shorter than they are or ever were, or they say we are more confident than we are or were.&lt;/p&gt;  &lt;p&gt;I feel pretty good about having longer timelines to AGI. It feels like I just got a better prognosis from my doctor. The situation is still basically the same, though.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Garrison Lovely&lt;/em&gt;&lt;em&gt; is a freelance journalist and the author of &lt;/em&gt;Obsolete&lt;em&gt;, an &lt;/em&gt;&lt;em&gt;online publication&lt;/em&gt;&lt;em&gt; and forthcoming &lt;/em&gt;&lt;em&gt;book&lt;/em&gt;&lt;em&gt; on the discourse, economics, and geopolitics of the race to build machine superintelligence (out spring 2026). His writing on AI has appeared in the&lt;/em&gt; New York Times&lt;em&gt;, &lt;/em&gt;Nature&lt;em&gt;, &lt;/em&gt;Bloomberg&lt;em&gt;,&lt;/em&gt; Time&lt;em&gt;, the&lt;/em&gt; Guardian&lt;em&gt;, &lt;/em&gt;The Verge&lt;em&gt;, and elsewhere.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-4_167ce7.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;It’s a weird time to be an AI doomer.&lt;/p&gt;  &lt;p&gt;This small but influential community of researchers, scientists, and policy experts believes, in the simplest terms, that AI could get so good it could be bad—very, very bad—for humanity. Though many of these people would be more likely to describe themselves as advocates for AI safety than as literal doomsayers, they warn that AI poses an existential risk to humanity. They argue that absent more regulation, the industry could hurtle toward systems it can’t control. They commonly expect such systems to follow the creation of artificial general intelligence (AGI), a slippery concept generally understood as technology that can do whatever humans can do, and better.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Though this is far from a universally shared perspective in the AI field, the doomer crowd has had some notable success over the past several years: helping shape AI policy coming from the Biden administration, organizing prominent calls for international “red lines” to prevent AI risks, and getting a bigger (and more influential) megaphone as some of its adherents win science’s most prestigious awards.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But a number of developments over the past six months have put them on the back foot. Talk of an AI bubble has overwhelmed the discourse as tech companies continue to invest in multiple Manhattan Projects’ worth of data centers without any certainty that future demand will match what they’re building.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;And then there was the August release of OpenAI’s latest foundation model, GPT-5, which proved something of a letdown. Maybe that was inevitable, since it was the most hyped AI release of all time; OpenAI CEO Sam Altman had boasted that GPT-5 felt “like a PhD-level expert” in every topic and told the podcaster Theo Von that the model was so good, it had made him feel “useless relative to the AI.”&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Many expected GPT-5 to be a big step toward AGI, but whatever progress the model may have made was overshadowed by a string of technical bugs and the company’s mystifying, quickly reversed decision to shut off access to every old OpenAI model without warning. And while the new model achieved state-of-the-art benchmark scores, many people felt, perhaps unfairly, that in day-to-day use GPT-5 was a step backward.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;All this would seem to threaten some of the very foundations of the doomers’ case. In turn, a competing camp of AI accelerationists, who fear AI is actually not moving fast enough and that the industry is constantly at risk of being smothered by overregulation, is seeing a fresh chance to change how we approach AI safety (or, maybe more accurately, how we don’t).&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;This is particularly true of the industry types who’ve decamped to Washington: “The Doomer narratives were wrong,” declared David Sacks, the longtime venture capitalist turned Trump administration AI czar. “This notion of imminent AGI has been a distraction and harmful and now effectively proven wrong,” echoed the White House’s senior policy advisor for AI and tech investor Sriram Krishnan. (Sacks and Krishnan did not reply to requests for comment.)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;(There is, of course, another camp in the AI safety debate: the group of researchers and advocates commonly associated with the label “AI ethics.” Though they also favor regulation, they tend to think the speed of AI progress has been overstated and have often written off AGI as a sci-fi story or a scam that distracts us from the technology’s immediate threats. But any potential doomer demise wouldn’t exactly give them the same opening the accelerationists are seeing.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;So where does this leave the doomers?&lt;strong&gt; &lt;/strong&gt;As part of our Hype Correction package, we decided to ask some of the movement’s biggest names to see if the recent setbacks and general vibe shift had altered their views. Are they frustrated that policymakers no longer seem to heed their threats? Are they quietly adjusting their timelines for the apocalypse?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Recent interviews with 20 people who study or advocate AI safety and governance—including Nobel Prize winner Geoffrey Hinton, Turing Prize winner Yoshua Bengio, and high-profile experts like former OpenAI board member Helen Toner—reveal that rather than feeling chastened or lost in the wilderness, they’re still deeply committed to their cause, believing that AGI remains not just possible but incredibly dangerous. &lt;/p&gt;  &lt;p&gt;At the same time, they seem to be grappling with a near contradiction. While they’re somewhat relieved that recent developments suggest AGI is further out than they previously thought (“Thank God we have more time,” says AI researcher Jeffrey Ladish), they also feel angry that people in power are not taking them seriously enough (Daniel Kokotajlo, lead author of a cautionary forecast called “AI 2027,” calls the Sacks and Krishnan tweets “deranged and/or dishonest”).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Broadly speaking, these experts see the talk of an AI bubble as no more than a speed bump, and disappointment in GPT-5 as more distracting than illuminating. They still generally favor more robust regulation and worry that progress on policy—the implementation of the EU AI Act; the passage of the first major American AI safety bill, California’s SB 53; and new interest in AGI risk from some members of Congress—has become vulnerable as Washington overreacts to what doomers see as short-term failures to live up to the hype.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Some were also eager to correct what they see as the most persistent misconceptions about the doomer world. Though their critics routinely mock them for predicting that AGI is right around the corner, they claim that’s never been an essential part of their case: It “isn’t about imminence,” says Berkeley professor Stuart Russell, the author of &lt;em&gt;Human Compatible&lt;/em&gt;&lt;em&gt;: Artificial Intelligence and the Problem of Control&lt;/em&gt;. Most people I spoke with say their timelines to dangerous systems have actually &lt;em&gt;lengthened&lt;/em&gt; slightly in the last year—an important change given how quickly the policy and technical landscapes can shift.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-pullquote alignleft has-text-align-left"&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;“If someone said there’s a four-mile-diameter asteroid that’s going to hit the Earth in 2067, we wouldn’t say, 'Remind me in 2066 and we’ll think about it.'”&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;  &lt;p&gt;Many of them, in fact, emphasize the importance of changing timelines. And even if they are &lt;em&gt;just a tad&lt;/em&gt; longer now, Toner tells me that one big-picture story of the ChatGPT era is the dramatic compression of these estimates &lt;em&gt;across&lt;/em&gt; the AI world. For a long while, she says, AGI was expected in many decades. Now, for the most part, the predicted arrival is sometime in the next few years to 20 years. So even if we have a little bit more time, she (and many of her peers) continue to see AI safety as incredibly, vitally urgent. She tells me that if AGI were possible anytime in even the next 30 years, “It’s a huge fucking deal. We should have a lot of people working on this.”&lt;/p&gt;  &lt;p&gt;So despite the precarious moment doomers find themselves in, their bottom line remains that no matter when AGI is coming (and, again, they say it’s very likely coming), the world is far from ready.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Maybe you agree. Or maybe you may think this future is far from guaranteed. Or that it’s the stuff of science fiction. You may even think AGI is a great big conspiracy theory. You’re not alone, of course—this topic is polarizing. But whatever you think about the doomer mindset, there’s no getting around the fact that certain people in this world have a lot of influence. So here are some of the most prominent people in the space, reflecting on this moment in their own words.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Interviews have been edited and condensed for length and clarity.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;h3 class="wp-block-heading"&gt;The Nobel laureate who’s not sure what’s coming&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Geoffrey Hinton, winner of the Turing Award and the Nobel Prize in physics for pioneering deep learning&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;The biggest change in the last few years is that there are people who are hard to dismiss who are saying this stuff is dangerous. Like, [former Google CEO] Eric Schmidt, for example, really recognized this stuff could be really dangerous. He and I were in China recently talking to someone on the Politburo, the party secretary of Shanghai, to make sure he really understood—and he did. I think in China, the leadership understands AI and its dangers much better because many of them are engineers.&lt;/p&gt;  &lt;p&gt;I’ve been focused on the longer-term threat: When AIs get more intelligent than us, can we really expect that humans will remain in control or even relevant? But I don’t think anything is inevitable. There’s huge uncertainty on everything. We’ve never been here before. Anybody who’s confident they know what’s going to happen seems silly to me. I think this is very unlikely but maybe it’ll turn out that all the people saying AI is way overhyped are correct. Maybe it’ll turn out that we can’t get much further than the current chatbots—we hit a wall due to limited data. I don’t believe that. I think that’s unlikely, but it’s possible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I also don’t believe people like Eliezer Yudkowsky, who say if anybody builds it, we’re all going to die. We don’t know that.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But if you go on the balance of the evidence, I think it’s fair to say that most experts who know a lot about AI believe it’s very probable that we’ll have superintelligence within the next 20 years. [Google DeepMind CEO] Demis Hassabis says maybe 10 years. Even [prominent AI skeptic] Gary Marcus would probably say, “Well, if you guys make a hybrid system with good old-fashioned symbolic logic … maybe that’ll be superintelligent.” &lt;em&gt;[Editor’s note: In September, Marcus &lt;/em&gt;&lt;em&gt;predicted&lt;/em&gt;&lt;em&gt; AGI would arrive between 2033 and 2040.]&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;And I don’t think anybody believes progress will stall at AGI. I think more or less everybody believes a few years after AGI, we’ll have superintelligence, because the AGI will be better than us at building AI.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;So while I think it’s clear that the winds are getting more difficult, simultaneously, people are putting in many more resources [into developing advanced AI]. I think progress will continue just because there’s many more resources going in.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The deep learning pioneer who wishes he’d seen the risks sooner&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Yoshua Bengio, winner of the Turing Award, chair of the &lt;/em&gt;&lt;em&gt;International AI Safety Report&lt;/em&gt;&lt;em&gt;, and founder of &lt;/em&gt;&lt;em&gt;LawZero&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;Some people thought that GPT-5 meant we had hit a wall, but that isn’t quite what you see in the scientific data and trends.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;There have been people overselling the idea that &lt;em&gt;AGI is tomorrow morning&lt;/em&gt;, which commercially could make sense. But if you look at the various benchmarks, GPT-5 is just where you would expect the models at that point in time to be. By the way, it’s not just GPT-5, it’s Claude and Google models, too.&lt;strong&gt; &lt;/strong&gt;In some areas where AI systems weren’t very good, like Humanity’s Last Exam or FrontierMath, they’re getting much better scores now than they were at the beginning of the year.&lt;/p&gt;  &lt;p&gt;At the same time, the overall landscape for AI governance and safety is not good. There’s a strong force pushing against regulation. It’s like climate change. We can put our head in the sand and hope it’s going to be fine, but it doesn’t really deal with the issue.&lt;/p&gt; 
 &lt;p&gt;The biggest disconnect with policymakers is a misunderstanding of the scale of change that is likely to happen if the trend of AI progress continues. A lot of people in business and governments simply think of AI as just another technology that’s going to be economically very powerful. They don’t understand how much it might change the world if trends continue, and we approach human-level AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Like many people, I had been blinding myself to the potential risks to some extent. I should have seen it coming much earlier. But it’s human. You’re excited about your work and you want to see the good side of it. That makes us a little bit biased in not really paying attention to the bad things that could happen.&lt;/p&gt; 
 &lt;p&gt;Even a small chance—like 1% or 0.1%—of creating an accident where billions of people die is not acceptable.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The AI veteran who believes AI is progressing—but not fast enough to prevent the bubble from bursting&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Stuart Russell, distinguished professor of computer science, University of California, Berkeley, and author of &lt;/em&gt;Human Compatible&lt;/h5&gt;  &lt;p&gt;I hope the idea that talking about existential risk makes you a “doomer” or is “science fiction” comes to be seen as fringe, given that most leading AI researchers and most leading AI CEOs take it seriously.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There have been claims that AI could never pass a Turing test, or you could never have a system that uses natural language fluently, or one that could parallel-park a car. All these claims just end up getting disproved by progress.&lt;/p&gt;  &lt;p&gt;People are spending trillions of dollars to make superhuman AI happen. I think they need some new ideas, but there’s a significant chance they will come up with them, because many significant new ideas have happened in the last few years.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_19"&gt; &lt;p&gt;My fairly consistent estimate for the last 12 months has been that there’s a 75% chance that those breakthroughs are not going to happen in time to rescue the industry from the bursting of the bubble. Because the investments are consistent with a prediction that we’re going to have much better AI that will deliver much more value to real customers. But if those predictions don’t come true, then there’ll be a lot of blood on the floor in the stock markets.&lt;/p&gt;  &lt;p&gt;However, the safety case isn’t about imminence. It’s about the fact that we still don’t have a solution to the control problem. If someone said there’s a four-mile-diameter asteroid that’s going to hit the Earth in 2067, we wouldn’t say, “Remind me in 2066 and we’ll think about it.” We don’t know how long it takes to develop the technology needed to control superintelligent AI.&lt;/p&gt;  &lt;p&gt;Looking at precedents, the acceptable level of risk for a nuclear plant melting down is about one in a million per year. Extinction is much worse than that. So maybe set the acceptable risk at one in a billion. But the companies are saying it’s something like one in five. They don’t know how to make it acceptable. And that’s a problem.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The professor trying to set the narrative straight on AI safety&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;David Krueger&lt;/em&gt;&lt;em&gt;, assistant professor in machine learning at the University of Montreal and Yoshua Bengio’s Mila Institute, and founder of &lt;/em&gt;&lt;em&gt;Evitable&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;I think people definitely overcorrected in their response to GPT-5. But there was hype. My recollection was that there were multiple statements from CEOs at various levels of explicitness who basically said that by the end of 2025, we’re going to have an automated drop-in replacement remote worker. But it seems like it’s been underwhelming, with agents just not really being there yet.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_21"&gt; &lt;p&gt;I’ve been surprised how much these narratives predicting AGI in 2027 capture the public attention. When 2027 comes around, if things still look pretty normal, I think people are going to feel like the whole worldview has been falsified. And it’s really annoying how often when I’m talking to people about AI safety, they assume that I think we have really short timelines to dangerous systems, or that I think LLMs or deep learning are going to give us AGI. They ascribe all these extra assumptions to me that aren’t necessary to make the case.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I’d expect we need decades for the international coordination problem. So even if dangerous AI is decades off, it’s already urgent. That point seems really lost on a lot of people. There’s this idea of “Let’s wait until we have a really dangerous system and then start governing it.” Man, that is way too late.&lt;/p&gt;  &lt;p&gt;I still think people in the safety community tend to work behind the scenes, with people in power, not really with civil society. It gives ammunition to people who say it’s all just a scam or insider lobbying. That’s not to say that there’s no truth to these narratives, but the underlying risk is still real. We need more public awareness and a broad base of support to have an effective response.&lt;/p&gt;  &lt;p&gt;If you actually believe there’s a 10% chance of doom in the next 10 years—which I think a reasonable person should, if they take a close look—then the first thing you think is: “Why are we doing this? This is crazy.” That’s just a very reasonable response once you buy the premise.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_23"&gt; &lt;h3 class="wp-block-heading"&gt;The governance expert worried about AI safety’s credibility&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Helen Toner, acting executive director of Georgetown University’s &lt;/em&gt;&lt;em&gt;Center for Security and Emerging Technology&lt;/em&gt;&lt;em&gt; and former OpenAI board member&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;When I got into the space, AI safety was more of a set of philosophical ideas. Today, it’s a thriving set of subfields of machine learning, filling in the gulf between some of the more “out there” concerns about AI scheming, deception, or power-seeking and real concrete systems we can test and play with.&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-pullquote alignleft has-text-align-left"&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;“I worry that some aggressive AGI timeline estimates from some AI safety people are setting them up for a boy-who-cried-wolf moment.”&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;/figure&gt;  &lt;p&gt;AI governance is improving slowly. If we have lots of time to adapt and governance can keep improving slowly, I feel not bad. If we don’t have much time, then we’re probably moving too slow.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_25"&gt; &lt;p&gt;I think GPT-5 is generally seen as a disappointment in DC. There’s a pretty polarized conversation around: Are we going to have AGI and superintelligence in the next few years? Or is AI actually just totally all hype and useless and a bubble? The pendulum had maybe swung too far toward “We’re going to have super-capable systems very, very soon.” And so now it’s swinging back toward “It’s all hype.”&lt;/p&gt;  &lt;p&gt;I worry that some aggressive AGI timeline estimates from some AI safety people are setting them up for a boy-who-cried-wolf moment. When the predictions about AGI coming in 2027 don’t come true, people will say, “Look at all these people who made fools of themselves. You should never listen to them again.” That’s not the intellectually honest response,&lt;strong&gt; &lt;/strong&gt;if maybe they later changed their mind, or their take was that they only thought it&amp;nbsp;was 20 percent likely and they thought that was still worth paying attention to. I think that shouldn't be disqualifying for people to listen to you later,&lt;strong&gt; &lt;/strong&gt;but I do worry it will be a big credibility hit. And that’s applying to people who are very concerned about AI safety and never said anything about very short timelines.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The AI security researcher who now believes AGI is further out—and is grateful&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Jeffrey Ladish, executive director at &lt;/em&gt;&lt;em&gt;Palisade Research&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;In the last year, two big things updated my AGI timelines.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;First, the lack of high-quality data turned out to be a bigger problem than I expected.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Second, the first “reasoning” model, OpenAI’s o1 in September 2024, showed reinforcement learning scaling was more effective than I thought it would be. And then months later, you see the o1 to o3 scale-up and you see pretty crazy impressive performance in math and coding and science—domains where it’s easier to sort of verify the results. But while we’re seeing continued progress, it could have been much faster.&lt;/p&gt;  &lt;p&gt;All of this bumps up my median estimate to the start of fully automated AI research and development from three years to maybe five or six years. But those are kind of made up numbers. It’s hard. I want to caveat all this with, like, “Man, it’s just really hard to do forecasting here.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_27"&gt; &lt;p&gt;Thank God we have more time. We have a possibly very brief window of opportunity to really try to understand these systems before they are capable and strategic enough to pose a real threat to our ability to control them.&lt;/p&gt;  &lt;p&gt;But it’s scary to see people think that we’re not making progress anymore when that’s clearly not true. I just know it’s not true because I use the models. One of the downsides of the way AI is progressing is that how fast it’s moving is becoming less legible to normal people.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now, this is not true in some domains—like, look at Sora 2. It is so obvious to anyone who looks at it that Sora 2 is vastly better than what came before. But if you ask GPT-4 and GPT-5 why the sky is blue, they’ll give you basically the same answer. It is the correct answer. It’s already saturated the ability to tell you why the sky is blue. So the people who I expect to most understand AI progress right now are the people who are actually building with AIs or using AIs on very difficult scientific problems.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;The AGI forecaster who saw the critics coming&lt;/h3&gt;  &lt;h5 class="wp-block-heading"&gt;&lt;em&gt;Daniel Kokotajlo, executive director of the &lt;/em&gt;&lt;em&gt;AI Futures Project&lt;/em&gt;&lt;em&gt;; an &lt;/em&gt;&lt;em&gt;OpenAI whistleblower&lt;/em&gt;&lt;em&gt;; and lead author of “&lt;/em&gt;&lt;em&gt;AI 2027&lt;/em&gt;&lt;em&gt;,” a vivid scenario where—starting in 2027—AIs progress from “superhuman coders” to “wildly superintelligent” systems in the span of months&lt;/em&gt;&lt;/h5&gt;  &lt;p&gt;AI policy seems to be getting worse, like the “Pro-AI” super PAC [launched earlier this year by executives from OpenAI and Andreessen Horowitz to lobby for a deregulatory agenda], and the deranged and/or dishonest tweets from Sriram Krishnan and David Sacks. AI safety research is progressing at the usual pace, which is excitingly rapid compared to most fields, but slow compared to how fast it needs to be.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_30"&gt;&lt;p&gt;We said on the first page of “AI 2027” that our timelines were somewhat longer than 2027. So even when we launched AI 2027, we expected there to be a bunch of critics in 2028 triumphantly saying we’ve been discredited, like the tweets from Sacks and Krishnan. But we thought, and continue to think, that the intelligence explosion will probably happen sometime in the next five to 10 years, and that when it does, people will remember our scenario and realize it was closer to the truth than anything else available in 2025.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Predicting the future is hard, but it’s valuable to try; people should aim to communicate their uncertainty about the future in a way that is specific and falsifiable. This is what we’ve done and very few others have done. Our critics mostly haven’t made predictions of their own and often exaggerate and mischaracterize our views. They say our timelines are shorter than they are or ever were, or they say we are more confident than we are or were.&lt;/p&gt;  &lt;p&gt;I feel pretty good about having longer timelines to AGI. It feels like I just got a better prognosis from my doctor. The situation is still basically the same, though.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Garrison Lovely&lt;/em&gt;&lt;em&gt; is a freelance journalist and the author of &lt;/em&gt;Obsolete&lt;em&gt;, an &lt;/em&gt;&lt;em&gt;online publication&lt;/em&gt;&lt;em&gt; and forthcoming &lt;/em&gt;&lt;em&gt;book&lt;/em&gt;&lt;em&gt; on the discourse, economics, and geopolitics of the race to build machine superintelligence (out spring 2026). His writing on AI has appeared in the&lt;/em&gt; New York Times&lt;em&gt;, &lt;/em&gt;Nature&lt;em&gt;, &lt;/em&gt;Bloomberg&lt;em&gt;,&lt;/em&gt; Time&lt;em&gt;, the&lt;/em&gt; Guardian&lt;em&gt;, &lt;/em&gt;The Verge&lt;em&gt;, and elsewhere.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129171/the-ai-doomers-feel-undeterred/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>A brief history of Sam Altman’s hype (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129169/a-brief-history-of-sam-altmans-hype/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-6.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="cst-block "&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;Each time you’ve heard a borderline outlandish idea of what AI will be capable of, it often turns out that Sam Altman was, if not the first to articulate it, at least the most persuasive and influential voice behind it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For more than a decade he has been known in Silicon Valley as a world-class fundraiser and persuader. OpenAI’s early releases around 2020 set the stage for a mania around large language models, and the launch of ChatGPT in November 2022 granted Altman a world stage on which to present his new thesis: that these models mirror human intelligence and could swing the doors open to a healthier and wealthier techno-utopia.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Throughout, Altman’s words have set the agenda. He has framed a prospective superintelligent AI as either humanistic or catastrophic, depending on what effect he was hoping to create, what he was raising money for, or which tech giant seemed like his most formidable competitor at the moment.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;Examining Altman’s statements over the years reveals just how much his outlook has powered today’s AI boom. Even among Silicon Valley’s many hypesters, he’s been especially willing to speak about open questions—whether large language models contain the ingredients of human thought, whether language can also produce intelligence—as if they were already answered.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;What he says about AI is rarely provable when he says it, but it persuades us of one thing: This road we’re on with AI can go somewhere either great or terrifying, and OpenAI will need epic sums to steer it toward the right destination. In this sense, he is the ultimate hype man.&lt;/p&gt;    &lt;p&gt;To understand how his voice has shaped our understanding of what AI can do, we read almost everything he’s ever said about the technology (we requested an interview with Altman, but he was not made available).&amp;nbsp;&lt;/p&gt;    &lt;p&gt;His own words trace how we arrived here.&lt;/p&gt; &lt;/div&gt;    &lt;h4 class="wp-block-heading"&gt;In conclusion …&amp;nbsp;&lt;/h4&gt;  &lt;p&gt;Altman didn’t dupe the world. OpenAI has ushered in a genuine tech revolution, with increasingly impressive language models that have attracted millions of users. Even skeptics would concede that LLMs’ conversational ability is astonishing.&lt;/p&gt;  &lt;p&gt;But Altman’s hype has always hinged less on today’s capabilities than on a philosophical tomorrow—an outlook that quite handily doubles as a case for more capital and friendlier regulation. Long before large language models existed, he was imagining an AI powerful enough to require wealth redistribution, just as he imagined humanity colonizing other planets. Again and again, promises of a destination—abundance, superintelligence, a healthier and wealthier world—have come first, and the evidence second.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even if LLMs eventually hit a wall, there’s little reason to think his faith in a techno-utopian future will falter. The vision was never really about the particulars of the current model anyway.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/MIT_AI-Hype_2-6.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div class="cst-block "&gt;&lt;div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_1 html_first"&gt; &lt;p&gt;Each time you’ve heard a borderline outlandish idea of what AI will be capable of, it often turns out that Sam Altman was, if not the first to articulate it, at least the most persuasive and influential voice behind it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For more than a decade he has been known in Silicon Valley as a world-class fundraiser and persuader. OpenAI’s early releases around 2020 set the stage for a mania around large language models, and the launch of ChatGPT in November 2022 granted Altman a world stage on which to present his new thesis: that these models mirror human intelligence and could swing the doors open to a healthier and wealthier techno-utopia.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;&lt;em&gt;This story is part of MIT Technology Review’s &lt;strong&gt;Hype Correction&lt;/strong&gt; package, a series that resets expectations about what AI is, what it makes possible, and where we go next.&lt;/em&gt;&lt;/p&gt;  &lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;  &lt;p&gt;Throughout, Altman’s words have set the agenda. He has framed a prospective superintelligent AI as either humanistic or catastrophic, depending on what effect he was hoping to create, what he was raising money for, or which tech giant seemed like his most formidable competitor at the moment.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;Examining Altman’s statements over the years reveals just how much his outlook has powered today’s AI boom. Even among Silicon Valley’s many hypesters, he’s been especially willing to speak about open questions—whether large language models contain the ingredients of human thought, whether language can also produce intelligence—as if they were already answered.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained"&gt; &lt;p&gt;What he says about AI is rarely provable when he says it, but it persuades us of one thing: This road we’re on with AI can go somewhere either great or terrifying, and OpenAI will need epic sums to steer it toward the right destination. In this sense, he is the ultimate hype man.&lt;/p&gt;    &lt;p&gt;To understand how his voice has shaped our understanding of what AI can do, we read almost everything he’s ever said about the technology (we requested an interview with Altman, but he was not made available).&amp;nbsp;&lt;/p&gt;    &lt;p&gt;His own words trace how we arrived here.&lt;/p&gt; &lt;/div&gt;    &lt;h4 class="wp-block-heading"&gt;In conclusion …&amp;nbsp;&lt;/h4&gt;  &lt;p&gt;Altman didn’t dupe the world. OpenAI has ushered in a genuine tech revolution, with increasingly impressive language models that have attracted millions of users. Even skeptics would concede that LLMs’ conversational ability is astonishing.&lt;/p&gt;  &lt;p&gt;But Altman’s hype has always hinged less on today’s capabilities than on a philosophical tomorrow—an outlook that quite handily doubles as a case for more capital and friendlier regulation. Long before large language models existed, he was imagining an AI powerful enough to require wealth redistribution, just as he imagined humanity colonizing other planets. Again and again, promises of a destination—abundance, superintelligence, a healthier and wealthier world—have come first, and the evidence second.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Even if LLMs eventually hit a wall, there’s little reason to think his faith in a techno-utopian future will falter. The vision was never really about the particulars of the current model anyway.&amp;nbsp;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129169/a-brief-history-of-sam-altmans-hype/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>Walmart’s AI strategy: Beyond the hype, what’s actually working (AI News)</title><link>https://www.artificialintelligence-news.com/news/walmart-ai-strategy-agentic-future/</link><description>&lt;p&gt;Walmart’s December 9 transfer to Nasdaq wasn’t just a symbolic gesture. The US$905 billion retailer is making its boldest claim yet: that it’s no longer a traditional discount chain, but a tech-powered enterprise using AI to fundamentally rewire its retail operations.&lt;/p&gt;&lt;p&gt;But beyond the marketing spin and the parade of AI announcements, what’s genuinely transforming at the world’s largest retailer – and where are the gaps between ambition and execution?&lt;/p&gt;&lt;h3&gt;The Agentic AI pivot: Purpose-built, not off-the-shelf&lt;/h3&gt;&lt;p&gt;Walmart’s AI strategy diverges sharply from competitors chasing generic large language models. According to CTO Hari Vasudev, the company is deploying what it calls “purpose-built agentic AI” – specialised tools trained on Walmart’s proprietary retail data rather than one-size-fits-all solutions.&lt;/p&gt;&lt;p&gt;“Our approach to agentic AI at Walmart is surgical,” Vasudev wrote in a May 2025 blog post. “Extensive early testing proved that, for us, agents work best when deployed for highly specific tasks, to produce outputs that can then be stitched together to orchestrate and solve complex workflows.”&lt;/p&gt;&lt;p&gt;This translates to tangible applications, the company says: Walmart’s “Trend-to-Product” system cuts fashion production timelines by 18 weeks. Its GenAI Customer Support Assistant now routes and resolves issues autonomously without human intervention.&lt;/p&gt;&lt;p&gt;Developer productivity tools handle test generation and error resolution in CI/CD pipelines. Meanwhile, the company’s retail-specific LLM “Wallaby” – trained on decades of Walmart transaction data – powers everything from item comparison to personalised shopping journey completion.&lt;/p&gt;&lt;p&gt;The infrastructure underpinning this? Element, Walmart’s proprietary MLOps platform, designed to avoid vendor lock-in and optimise GPU use in multiple cloud providers. It’s an in-house “factory” that gives Walmart speed and flexibility competitors wrestling with third-party platforms can’t match.&lt;/p&gt;&lt;h3&gt;Real numbers: Where AI delivers measurable impact&lt;/h3&gt;&lt;p&gt;Walmart has been unusually transparent about specific ROI metrics, offering a rare glimpse into enterprise AI economics:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Data operations:&lt;/strong&gt; GenAI improved over 850 million product catalogue data points – a task that would have required 100 times the headcount using manual processes, according to CEO Doug McMillon’s August 2024 earnings call.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Supply chain efficiency:&lt;/strong&gt; AI-powered route optimisation eliminated 30 million unnecessary delivery miles and avoided 94 million pounds (42,000 tons) of CO2 emissions. The company won the prestigious Franz Edelman Award in 2023 for this technology – and has since commercialised it as a SaaS product for other businesses.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Store operations: &lt;/strong&gt;Digital twin technology predicts refrigeration failures up to two weeks in advance, auto-generating work orders complete with visual models, wiring diagrams, and required parts. Sam’s Club’s AI-powered exit technology has reduced member checkout times by 21%, with over 64% of members now using the friction-free system in all locations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Customer experience:&lt;/strong&gt; Dynamic delivery algorithms analyse traffic patterns, weather conditions, and order complexity to predict delivery times down to the minute, enabling 17-minute express deliveries in markets the company has tested.&lt;/p&gt;&lt;h3&gt;The human cost: “AI will change every job”&lt;/h3&gt;&lt;p&gt;McMillon hasn’t sugarcoated the workforce implications. Speaking at a Bentonville workforce conference in September 2025, he said: “It’s very clear that AI is going to change literally every job. Maybe there’s a job in the world that AI won’t change, but I haven’t thought of it.”&lt;/p&gt;&lt;p&gt;But Walmart positions this as a transformation not an elimination. McMillon expects total headcount to remain flat even as revenue grows – meaning jobs will shift, not disappear. White-collar roles face the earliest disruption through chatbots handling customer service and supply chain tracking, while store and warehouse workers will eventually see tasks absorbed by autonomous systems.&lt;/p&gt;&lt;p&gt;The company is investing heavily in re-skilling programmes. “We’ve got to create the opportunity for everybody to make it to the other side,” McMillon said at the Bentonville conference. Automation equipment operator Chance at Walmart’s Palestine, Texas, distribution centre described the shift: “It used to be 85% physical. Now it’s 85% mental. I’m solving problems with my mind, not just my body.”&lt;/p&gt;&lt;h3&gt;The Nasdaq gambit: Repositioning for tech valuations&lt;/h3&gt;&lt;p&gt;Walmart’s exchange transfer was explicitly framed around its AI transformation. CFO John David Rainey stated the move reflects the company “setting a new standard for omnichannel retail by integrating automation and AI.”&lt;/p&gt;&lt;p&gt;The subtext? Walmart wants the valuation multiples tech companies command. At a P/E ratio of 40.3x – higher than Amazon and Microsoft – the market is partially buying the transformation story. Potential inclusion in the tech-heavy Nasdaq 100 index would drive passive fund investment regardless of AI execution.&lt;/p&gt;&lt;p&gt;Analysts are split on whether the premium is justified. Jefferies’ Corey Tarlowe argued the move signals Walmart is “less of a traditional retail corporation and more of a technology firm.” But sceptics note the company still derives revenue from razor-thin retail margins, not high-margin software or cloud services – despite commercialising tools like Route Optimisation.&lt;/p&gt;&lt;h3&gt;Verdict: Genuine transformation with execution risk&lt;/h3&gt;&lt;p&gt;Walmart’s AI strategy is neither pure hype nor guaranteed success. The company is making structural investments in proprietary infrastructure, deploying AI at genuine scale with published operational benefits, and acknowledging workforce implications most enterprises dodge.&lt;/p&gt;&lt;p&gt;But significant execution risks remain: managing fragmented agent ecosystems, preventing algorithmic bias at scale, competing against external shopping agents, and determining appropriate automation boundaries while maintaining accuracy.&lt;/p&gt;&lt;p&gt;The company’s candidness about challenges – “often, a co-pilot model, with humans and AI working as a team, is the most effective approach” – suggests leadership understands AI isn’t a silver bullet.&lt;/p&gt;&lt;p&gt;For enterprises watching Walmart’s playbook, the lesson is to build for specificity, not generality. Invest in proprietary data moats, plan for workforce transformation, not just cost reduction, and recognise that even with massive resources and technical talent, agentic AI remains early-stage technology with genuine limitations.&lt;/p&gt;&lt;p&gt;The question isn’t whether Walmart is using AI – it demonstrably is. It’s whether this surgical, infrastructure-heavy approach delivers sustainable competitive advantage, or if the company is automating itself into the same low-margin trap with shinier tools.&lt;/p&gt;&lt;p&gt;That answer won’t be clear for several years – but Walmart’s willingness to bet its US$905 billion in market cap on the transformation suggests leadership believes the former.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Walmart and Amazon drive retail transformation with AI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Walmart’s December 9 transfer to Nasdaq wasn’t just a symbolic gesture. The US$905 billion retailer is making its boldest claim yet: that it’s no longer a traditional discount chain, but a tech-powered enterprise using AI to fundamentally rewire its retail operations.&lt;/p&gt;&lt;p&gt;But beyond the marketing spin and the parade of AI announcements, what’s genuinely transforming at the world’s largest retailer – and where are the gaps between ambition and execution?&lt;/p&gt;&lt;h3&gt;The Agentic AI pivot: Purpose-built, not off-the-shelf&lt;/h3&gt;&lt;p&gt;Walmart’s AI strategy diverges sharply from competitors chasing generic large language models. According to CTO Hari Vasudev, the company is deploying what it calls “purpose-built agentic AI” – specialised tools trained on Walmart’s proprietary retail data rather than one-size-fits-all solutions.&lt;/p&gt;&lt;p&gt;“Our approach to agentic AI at Walmart is surgical,” Vasudev wrote in a May 2025 blog post. “Extensive early testing proved that, for us, agents work best when deployed for highly specific tasks, to produce outputs that can then be stitched together to orchestrate and solve complex workflows.”&lt;/p&gt;&lt;p&gt;This translates to tangible applications, the company says: Walmart’s “Trend-to-Product” system cuts fashion production timelines by 18 weeks. Its GenAI Customer Support Assistant now routes and resolves issues autonomously without human intervention.&lt;/p&gt;&lt;p&gt;Developer productivity tools handle test generation and error resolution in CI/CD pipelines. Meanwhile, the company’s retail-specific LLM “Wallaby” – trained on decades of Walmart transaction data – powers everything from item comparison to personalised shopping journey completion.&lt;/p&gt;&lt;p&gt;The infrastructure underpinning this? Element, Walmart’s proprietary MLOps platform, designed to avoid vendor lock-in and optimise GPU use in multiple cloud providers. It’s an in-house “factory” that gives Walmart speed and flexibility competitors wrestling with third-party platforms can’t match.&lt;/p&gt;&lt;h3&gt;Real numbers: Where AI delivers measurable impact&lt;/h3&gt;&lt;p&gt;Walmart has been unusually transparent about specific ROI metrics, offering a rare glimpse into enterprise AI economics:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Data operations:&lt;/strong&gt; GenAI improved over 850 million product catalogue data points – a task that would have required 100 times the headcount using manual processes, according to CEO Doug McMillon’s August 2024 earnings call.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Supply chain efficiency:&lt;/strong&gt; AI-powered route optimisation eliminated 30 million unnecessary delivery miles and avoided 94 million pounds (42,000 tons) of CO2 emissions. The company won the prestigious Franz Edelman Award in 2023 for this technology – and has since commercialised it as a SaaS product for other businesses.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Store operations: &lt;/strong&gt;Digital twin technology predicts refrigeration failures up to two weeks in advance, auto-generating work orders complete with visual models, wiring diagrams, and required parts. Sam’s Club’s AI-powered exit technology has reduced member checkout times by 21%, with over 64% of members now using the friction-free system in all locations.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Customer experience:&lt;/strong&gt; Dynamic delivery algorithms analyse traffic patterns, weather conditions, and order complexity to predict delivery times down to the minute, enabling 17-minute express deliveries in markets the company has tested.&lt;/p&gt;&lt;h3&gt;The human cost: “AI will change every job”&lt;/h3&gt;&lt;p&gt;McMillon hasn’t sugarcoated the workforce implications. Speaking at a Bentonville workforce conference in September 2025, he said: “It’s very clear that AI is going to change literally every job. Maybe there’s a job in the world that AI won’t change, but I haven’t thought of it.”&lt;/p&gt;&lt;p&gt;But Walmart positions this as a transformation not an elimination. McMillon expects total headcount to remain flat even as revenue grows – meaning jobs will shift, not disappear. White-collar roles face the earliest disruption through chatbots handling customer service and supply chain tracking, while store and warehouse workers will eventually see tasks absorbed by autonomous systems.&lt;/p&gt;&lt;p&gt;The company is investing heavily in re-skilling programmes. “We’ve got to create the opportunity for everybody to make it to the other side,” McMillon said at the Bentonville conference. Automation equipment operator Chance at Walmart’s Palestine, Texas, distribution centre described the shift: “It used to be 85% physical. Now it’s 85% mental. I’m solving problems with my mind, not just my body.”&lt;/p&gt;&lt;h3&gt;The Nasdaq gambit: Repositioning for tech valuations&lt;/h3&gt;&lt;p&gt;Walmart’s exchange transfer was explicitly framed around its AI transformation. CFO John David Rainey stated the move reflects the company “setting a new standard for omnichannel retail by integrating automation and AI.”&lt;/p&gt;&lt;p&gt;The subtext? Walmart wants the valuation multiples tech companies command. At a P/E ratio of 40.3x – higher than Amazon and Microsoft – the market is partially buying the transformation story. Potential inclusion in the tech-heavy Nasdaq 100 index would drive passive fund investment regardless of AI execution.&lt;/p&gt;&lt;p&gt;Analysts are split on whether the premium is justified. Jefferies’ Corey Tarlowe argued the move signals Walmart is “less of a traditional retail corporation and more of a technology firm.” But sceptics note the company still derives revenue from razor-thin retail margins, not high-margin software or cloud services – despite commercialising tools like Route Optimisation.&lt;/p&gt;&lt;h3&gt;Verdict: Genuine transformation with execution risk&lt;/h3&gt;&lt;p&gt;Walmart’s AI strategy is neither pure hype nor guaranteed success. The company is making structural investments in proprietary infrastructure, deploying AI at genuine scale with published operational benefits, and acknowledging workforce implications most enterprises dodge.&lt;/p&gt;&lt;p&gt;But significant execution risks remain: managing fragmented agent ecosystems, preventing algorithmic bias at scale, competing against external shopping agents, and determining appropriate automation boundaries while maintaining accuracy.&lt;/p&gt;&lt;p&gt;The company’s candidness about challenges – “often, a co-pilot model, with humans and AI working as a team, is the most effective approach” – suggests leadership understands AI isn’t a silver bullet.&lt;/p&gt;&lt;p&gt;For enterprises watching Walmart’s playbook, the lesson is to build for specificity, not generality. Invest in proprietary data moats, plan for workforce transformation, not just cost reduction, and recognise that even with massive resources and technical talent, agentic AI remains early-stage technology with genuine limitations.&lt;/p&gt;&lt;p&gt;The question isn’t whether Walmart is using AI – it demonstrably is. It’s whether this surgical, infrastructure-heavy approach delivers sustainable competitive advantage, or if the company is automating itself into the same low-margin trap with shinier tools.&lt;/p&gt;&lt;p&gt;That answer won’t be clear for several years – but Walmart’s willingness to bet its US$905 billion in market cap on the transformation suggests leadership believes the former.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Walmart and Amazon drive retail transformation with AI&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/walmart-ai-strategy-agentic-future/</guid><pubDate>Mon, 15 Dec 2025 10:00:00 +0000</pubDate></item><item><title>Mirelo raises $41M from Index and a16z to solve AI video’s silent problem (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/mirelo-raises-41m-from-index-and-a16z-to-solve-ai-videos-silent-problem/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Mirelo-cofounders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI lets anyone create videos, but many AI video creation tools lack support for audio. Mirelo is building AI that adds soundtracks to match the video’s action.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the Berlin-based startup released Mirelo SFX v1.5, an AI model that interprets videos to add synced sound effects (SFX).&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This attracted attention from VCs gearing up for a generative AI revolution in games. The two-year-old German startup has raised a $41 million seed round led by Index Ventures and Andreessen Horowitz, TechCrunch learned exclusively.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new capital will help Mirelo compete more effectively in its emerging category. While it was still in stealth mode and resource-constrained, large companies such as Sony and Tencent released video-to-SFX models. So did Kuaishou-owned Kling AI, out of China, and ElevenLabs, which is also backed by a16z.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Mirelo already differs from them by its narrower focus, beating these models in the long run requires the startup to make additional hires. Altogether, the startup expects its team of 10 people to “double if not triple” in headcount by the end of next year, Mirelo CEO and co-founder CJ Simon-Gabriel told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These new hires will support Mirelo’s R&amp;amp;D, as well as its product and go-to-market strategy. The startup published its models on Fal.ai and Replicate, and expects API usage to drive most of its revenue in the short term, Simon-Gabriel said. But it is also investing in building out its workspace for creators, Mirelo Studio, which could eventually support full professional use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Mirelo prepares to scale, the startup and its investors are also anticipating concerns around training data that have dogged other generative AI companies. According to Georgia Stevenson, who led Index’s investments, Mirelo based its models on public and purchased sound libraries, and is signing revenue-sharing partnerships that respect artists’ rights.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a tension inherent to generative AI tools, but Mirelo isn’t displacing musicians and sound designers — at least not yet. With a freemium model including a recommended plan for creators priced at €20/month (approximately $23.50), the startup is mostly targeting amateurs and prosumers hoping to unmute AI-generated videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Simon-Gabriel, creators can’t fully benefit from this new potential without audio. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“George Lucas said that sound is 50% of the movie-going experience. It’s not an overstatement,” he said. “If anything, it’s an understatement. You can take exactly the same images, and the sound will shape a completely different ambience, depending on the sound and the music that you put in there.”&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;He and his co-founder, Florian Wenzel, are both AI researchers and musicians themselves, and the startup has AI music generation on its roadmap. But Mirelo is seeing more pull for sound effects, in part because there is less research happening than in other AI fields, Simon-Gabriel said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s easier to build a real moat here, and then to capitalize on it,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This could pay off for Mirelo. Simon-Gabriel declined to disclose its new valuation, but said it had increased “very significantly” compared to its previously undisclosed pre-seed round. That earlier round was led by Berlin-based firm Atlantic, which also participated in the new funding, bringing Mirelo’s total raised to $44 million and helping close its resource gap.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup is also backed by angels who lend credibility to its technology and could open new doors, including Mistral CEO Arthur Mensch, Hugging Face chief science officer Thomas Wolf, Fal.ai co-founder Burkay Gur, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the team is aware that AI-generated videos may not be mute for long. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Gemini’s video generator now incorporates soundtracks powered by DeepMind’s Veo 3.1 video-to-audio model. But if anything, Simon-Gabriel sounds vindicated. “Now, suddenly, people realize, ‘Oh, maybe we should add sound.’ But, of course, you should add some. It’s a bit like silent movies versus talkies, right? It does make quite a difference!”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Mirelo-cofounders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI lets anyone create videos, but many AI video creation tools lack support for audio. Mirelo is building AI that adds soundtracks to match the video’s action.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this year, the Berlin-based startup released Mirelo SFX v1.5, an AI model that interprets videos to add synced sound effects (SFX).&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This attracted attention from VCs gearing up for a generative AI revolution in games. The two-year-old German startup has raised a $41 million seed round led by Index Ventures and Andreessen Horowitz, TechCrunch learned exclusively.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This new capital will help Mirelo compete more effectively in its emerging category. While it was still in stealth mode and resource-constrained, large companies such as Sony and Tencent released video-to-SFX models. So did Kuaishou-owned Kling AI, out of China, and ElevenLabs, which is also backed by a16z.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Mirelo already differs from them by its narrower focus, beating these models in the long run requires the startup to make additional hires. Altogether, the startup expects its team of 10 people to “double if not triple” in headcount by the end of next year, Mirelo CEO and co-founder CJ Simon-Gabriel told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These new hires will support Mirelo’s R&amp;amp;D, as well as its product and go-to-market strategy. The startup published its models on Fal.ai and Replicate, and expects API usage to drive most of its revenue in the short term, Simon-Gabriel said. But it is also investing in building out its workspace for creators, Mirelo Studio, which could eventually support full professional use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Mirelo prepares to scale, the startup and its investors are also anticipating concerns around training data that have dogged other generative AI companies. According to Georgia Stevenson, who led Index’s investments, Mirelo based its models on public and purchased sound libraries, and is signing revenue-sharing partnerships that respect artists’ rights.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a tension inherent to generative AI tools, but Mirelo isn’t displacing musicians and sound designers — at least not yet. With a freemium model including a recommended plan for creators priced at €20/month (approximately $23.50), the startup is mostly targeting amateurs and prosumers hoping to unmute AI-generated videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to Simon-Gabriel, creators can’t fully benefit from this new potential without audio. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“George Lucas said that sound is 50% of the movie-going experience. It’s not an overstatement,” he said. “If anything, it’s an understatement. You can take exactly the same images, and the sound will shape a completely different ambience, depending on the sound and the music that you put in there.”&lt;/p&gt;









&lt;p class="wp-block-paragraph"&gt;He and his co-founder, Florian Wenzel, are both AI researchers and musicians themselves, and the startup has AI music generation on its roadmap. But Mirelo is seeing more pull for sound effects, in part because there is less research happening than in other AI fields, Simon-Gabriel said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s easier to build a real moat here, and then to capitalize on it,” he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This could pay off for Mirelo. Simon-Gabriel declined to disclose its new valuation, but said it had increased “very significantly” compared to its previously undisclosed pre-seed round. That earlier round was led by Berlin-based firm Atlantic, which also participated in the new funding, bringing Mirelo’s total raised to $44 million and helping close its resource gap.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup is also backed by angels who lend credibility to its technology and could open new doors, including Mistral CEO Arthur Mensch, Hugging Face chief science officer Thomas Wolf, Fal.ai co-founder Burkay Gur, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, the team is aware that AI-generated videos may not be mute for long. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, Gemini’s video generator now incorporates soundtracks powered by DeepMind’s Veo 3.1 video-to-audio model. But if anything, Simon-Gabriel sounds vindicated. “Now, suddenly, people realize, ‘Oh, maybe we should add sound.’ But, of course, you should add some. It’s a bit like silent movies versus talkies, right? It does make quite a difference!”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/mirelo-raises-41m-from-index-and-a16z-to-solve-ai-videos-silent-problem/</guid><pubDate>Mon, 15 Dec 2025 12:30:00 +0000</pubDate></item><item><title>[NEW] The Download: introducing the AI Hype Correction package (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1129719/the-download-introducing-the-ai-hype-correction-package/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Introducing: the AI Hype Correction package&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;AI is going to reproduce human intelligence. AI will eliminate disease. AI is the single biggest, most important invention in human history. You've likely heard it all—but probably none of these things are true.&lt;/p&gt;&lt;p&gt;AI &lt;em&gt;is&lt;/em&gt; changing our world, but we don't yet know the real winners, or how this will all shake out.&lt;/p&gt;&lt;p&gt;After a few years of out-of-control hype, people are now starting to re-calibrate what AI is, what it can do, and how we should think about its ultimate impact.&lt;/p&gt;&lt;p&gt;Here, at the end of 2025, we're starting the post-hype phase. This new package of stories, called Hype Correction, is a way to reset expectations—a critical look at where we are, what AI makes possible, and where we go next.&lt;/p&gt;&lt;p&gt;Here’s a sneak peek at what you can expect:&lt;/p&gt;  &lt;p&gt;+ An introduction to four ways of thinking about the great AI hype correction of 2025.&lt;/p&gt;&lt;p&gt;+&amp;nbsp; While it’s safe to say we’re definitely in an AI bubble right now, what’s less clear is what it really looks like—and what comes after it pops. Read the full story.&lt;/p&gt;&lt;p&gt;+ Why OpenAI’s Sam Altman can be traced back to so many of the more outlandish proclamations about AI doing the rounds these days. Read the full story.&lt;/p&gt;&lt;p&gt;+ It’s a weird time to be an AI doomer. But they’re not giving up.&lt;/p&gt;&lt;p&gt;+ AI coding is now everywhere—but despite the billions of dollars being poured into improving AI models’ coding abilities, not everyone is convinced. Read the full story.&lt;/p&gt;&lt;p&gt;+ If we really want to start finding new kinds of materials faster, AI materials discovery needs to make it out of the lab and move into the real world. Read the full story.&lt;/p&gt;&lt;p&gt;+ Why reports of AI’s potential to replace trained human lawyers are greatly exaggerated.&lt;/p&gt;&lt;p&gt;+ Dr. Margaret Mitchell, chief ethics scientist at AI startup Hugging Face, explains why the generative AI hype train is distracting us from what AI actually is and what it can—and crucially, cannot—do. Read the full story.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 iRobot has filed for bankruptcy&lt;br /&gt;&lt;/strong&gt;The Roomba maker is considering handing over control to its main Chinese supplier. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;A proposed Amazon acquisition fell through close to two years ago. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;How the company lost its way. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;A Roomba recorded a woman on the toilet. How did screenshots end up on Facebook? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Meta’s 2025 has been a total rollercoaster ride&lt;br /&gt;&lt;/strong&gt;From its controversial AI team to Mark Zuckerberg’s newfound appreciation for masculine energy. (Insider $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 The Trump administration is giving the crypto industry a much easier ride&lt;/strong&gt;&lt;br /&gt;It’s dismissed crypto lawsuits involving many firms with financial ties to Trump. (NYT $)&lt;br /&gt;+ &lt;em&gt;Celebrities are feeling emboldened to flog crypto once again. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;A bitcoin investor wants to set up a crypto libertarian community in the Caribbean. &lt;/em&gt;(FT $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;4 There’s a new weight-loss drug in town&lt;/strong&gt;&lt;br /&gt;And people are already taking it, even though it’s unapproved. (Wired $)&lt;br /&gt;+ &lt;em&gt;What we still don’t know about weight-loss drugs. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Chinese billionaires are having dozens of US-born surrogate babies&lt;/strong&gt;&lt;br /&gt;An entire industry has sprung up to support them. (WSJ $)&lt;br /&gt;+ &lt;em&gt;A controversial Chinese CRISPR scientist is still hopeful about embryo gene editing. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Trump’s “big beautiful bill” funding hinges on states integrating AI into healthcare&lt;br /&gt;&lt;/strong&gt;Experts fear it'll be used as a cost-cutting measure, even if it doesn’t work. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Artificial intelligence is infiltrating health care. We shouldn’t let it make all the decisions. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Extreme rainfall is wreaking havoc in the desert&lt;br /&gt;&lt;/strong&gt;Oman and the UAE are unaccustomed to increasingly common torrential downpours. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Data centers are being built in countries that are too hot for them&lt;br /&gt;&lt;/strong&gt;Which makes it a lot harder to cool them sufficiently. (Rest of World)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;9 Why AI image generators are getting deliberately worse&lt;br /&gt;&lt;/strong&gt;Their makers are pursuing realism—not that overly polished, Uncanny Valley look. (The Verge)&lt;br /&gt;+ &lt;em&gt;Inside the AI attention economy wars. &lt;/em&gt;(NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 How a tiny Swedish city became a major video game hub&lt;/strong&gt;&lt;br /&gt;Skövde has formed an unlikely community of cutting-edge developers. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Google DeepMind is using Gemini to train agents inside one of Skövde’s biggest franchises. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“They don’t care about the games. They don’t care about the art. They just want their money.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Anna C Webster, chair of the freelancing committee of the United Videogame Workers union, tells the Guardian why their members are protesting the prestigious 2025 Game Awards in the wake of major layoffs.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1129721" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/image_0bbe51.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Recapturing early internet whimsy with HTML&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Websites weren’t always slick digital experiences.&lt;/p&gt;&lt;p&gt;There was a time when surfing the web involved opening tabs that played music against your will and sifting through walls of text on a colored background. In the 2000s, before Squarespace and social media, websites were manifestations of individuality—built from scratch using HTML, by users who had some knowledge of code.&lt;/p&gt;&lt;p&gt;Scattered across the web are communities of programmers working to revive this seemingly outdated approach. And the movement is anything but a superficial appeal to retro aesthetics—it’s about celebrating the human touch in digital experiences. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tiffany Ng&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+&amp;nbsp; Here’s how a bit of math can help you wrap your presents much more neatly this year.&lt;br /&gt;+ It seems that humans mastered making fire way, way earlier than we realized.&lt;br /&gt;+ The Arab-owned cafes opening up across the US sound warm and welcoming.&lt;br /&gt;+ How to give a gift the recipient will still be using and loving for decades to come.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Introducing: the AI Hype Correction package&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;AI is going to reproduce human intelligence. AI will eliminate disease. AI is the single biggest, most important invention in human history. You've likely heard it all—but probably none of these things are true.&lt;/p&gt;&lt;p&gt;AI &lt;em&gt;is&lt;/em&gt; changing our world, but we don't yet know the real winners, or how this will all shake out.&lt;/p&gt;&lt;p&gt;After a few years of out-of-control hype, people are now starting to re-calibrate what AI is, what it can do, and how we should think about its ultimate impact.&lt;/p&gt;&lt;p&gt;Here, at the end of 2025, we're starting the post-hype phase. This new package of stories, called Hype Correction, is a way to reset expectations—a critical look at where we are, what AI makes possible, and where we go next.&lt;/p&gt;&lt;p&gt;Here’s a sneak peek at what you can expect:&lt;/p&gt;  &lt;p&gt;+ An introduction to four ways of thinking about the great AI hype correction of 2025.&lt;/p&gt;&lt;p&gt;+&amp;nbsp; While it’s safe to say we’re definitely in an AI bubble right now, what’s less clear is what it really looks like—and what comes after it pops. Read the full story.&lt;/p&gt;&lt;p&gt;+ Why OpenAI’s Sam Altman can be traced back to so many of the more outlandish proclamations about AI doing the rounds these days. Read the full story.&lt;/p&gt;&lt;p&gt;+ It’s a weird time to be an AI doomer. But they’re not giving up.&lt;/p&gt;&lt;p&gt;+ AI coding is now everywhere—but despite the billions of dollars being poured into improving AI models’ coding abilities, not everyone is convinced. Read the full story.&lt;/p&gt;&lt;p&gt;+ If we really want to start finding new kinds of materials faster, AI materials discovery needs to make it out of the lab and move into the real world. Read the full story.&lt;/p&gt;&lt;p&gt;+ Why reports of AI’s potential to replace trained human lawyers are greatly exaggerated.&lt;/p&gt;&lt;p&gt;+ Dr. Margaret Mitchell, chief ethics scientist at AI startup Hugging Face, explains why the generative AI hype train is distracting us from what AI actually is and what it can—and crucially, cannot—do. Read the full story.&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 iRobot has filed for bankruptcy&lt;br /&gt;&lt;/strong&gt;The Roomba maker is considering handing over control to its main Chinese supplier. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;A proposed Amazon acquisition fell through close to two years ago. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;How the company lost its way. &lt;/em&gt;(TechCrunch)&lt;br /&gt;+ &lt;em&gt;A Roomba recorded a woman on the toilet. How did screenshots end up on Facebook? &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 Meta’s 2025 has been a total rollercoaster ride&lt;br /&gt;&lt;/strong&gt;From its controversial AI team to Mark Zuckerberg’s newfound appreciation for masculine energy. (Insider $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 The Trump administration is giving the crypto industry a much easier ride&lt;/strong&gt;&lt;br /&gt;It’s dismissed crypto lawsuits involving many firms with financial ties to Trump. (NYT $)&lt;br /&gt;+ &lt;em&gt;Celebrities are feeling emboldened to flog crypto once again. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;A bitcoin investor wants to set up a crypto libertarian community in the Caribbean. &lt;/em&gt;(FT $)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;4 There’s a new weight-loss drug in town&lt;/strong&gt;&lt;br /&gt;And people are already taking it, even though it’s unapproved. (Wired $)&lt;br /&gt;+ &lt;em&gt;What we still don’t know about weight-loss drugs. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Chinese billionaires are having dozens of US-born surrogate babies&lt;/strong&gt;&lt;br /&gt;An entire industry has sprung up to support them. (WSJ $)&lt;br /&gt;+ &lt;em&gt;A controversial Chinese CRISPR scientist is still hopeful about embryo gene editing. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;6 Trump’s “big beautiful bill” funding hinges on states integrating AI into healthcare&lt;br /&gt;&lt;/strong&gt;Experts fear it'll be used as a cost-cutting measure, even if it doesn’t work. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Artificial intelligence is infiltrating health care. We shouldn’t let it make all the decisions. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Extreme rainfall is wreaking havoc in the desert&lt;br /&gt;&lt;/strong&gt;Oman and the UAE are unaccustomed to increasingly common torrential downpours. (WP $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Data centers are being built in countries that are too hot for them&lt;br /&gt;&lt;/strong&gt;Which makes it a lot harder to cool them sufficiently. (Rest of World)&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;9 Why AI image generators are getting deliberately worse&lt;br /&gt;&lt;/strong&gt;Their makers are pursuing realism—not that overly polished, Uncanny Valley look. (The Verge)&lt;br /&gt;+ &lt;em&gt;Inside the AI attention economy wars. &lt;/em&gt;(NY Mag $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 How a tiny Swedish city became a major video game hub&lt;/strong&gt;&lt;br /&gt;Skövde has formed an unlikely community of cutting-edge developers. (The Guardian)&lt;br /&gt;+ &lt;em&gt;Google DeepMind is using Gemini to train agents inside one of Skövde’s biggest franchises. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p class="has-large-font-size"&gt;&lt;strong&gt;“They don’t care about the games. They don’t care about the art. They just want their money.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Anna C Webster, chair of the freelancing committee of the United Videogame Workers union, tells the Guardian why their members are protesting the prestigious 2025 Game Awards in the wake of major layoffs.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; 
 &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1129721" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/image_0bbe51.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;Recapturing early internet whimsy with HTML&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Websites weren’t always slick digital experiences.&lt;/p&gt;&lt;p&gt;There was a time when surfing the web involved opening tabs that played music against your will and sifting through walls of text on a colored background. In the 2000s, before Squarespace and social media, websites were manifestations of individuality—built from scratch using HTML, by users who had some knowledge of code.&lt;/p&gt;&lt;p&gt;Scattered across the web are communities of programmers working to revive this seemingly outdated approach. And the movement is anything but a superficial appeal to retro aesthetics—it’s about celebrating the human touch in digital experiences. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Tiffany Ng&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+&amp;nbsp; Here’s how a bit of math can help you wrap your presents much more neatly this year.&lt;br /&gt;+ It seems that humans mastered making fire way, way earlier than we realized.&lt;br /&gt;+ The Arab-owned cafes opening up across the US sound warm and welcoming.&lt;br /&gt;+ How to give a gift the recipient will still be using and loving for decades to come.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1129719/the-download-introducing-the-ai-hype-correction-package/</guid><pubDate>Mon, 15 Dec 2025 13:10:00 +0000</pubDate></item><item><title>[NEW] AWS’s legacy will be in AI success (AI News)</title><link>https://www.artificialintelligence-news.com/news/awss-legacy-will-be-in-ai-success/</link><description>&lt;p&gt;As the company that kick-started the cloud computing revolution, Amazon is one of the world’s biggest companies whose practices in all things technological can be regarded as a blueprint for implementing new technology.&lt;/p&gt;&lt;p&gt;This article looks at some of the ways that the company is deploying AI in its operations.&lt;/p&gt;&lt;p&gt;Amazon’s latest AI strategy has progressed from basic chatbots to agentic AI: systems that can plan and execute multi-step work using different tools and across processes. As a company, Amazon sits at the intersection of cloud infrastructure (in the form of AWS), logistics, retail, and customer service, all of which are areas where small efficiency gains can have massive impact.&lt;/p&gt;&lt;p&gt;In early 2025, Amazon made its AI intentions clear for its cloud company, AWS, by forming a new group focused internally on agentic AI. According to reporting on an internal email, AWS leadership described agentic AI as a potential “multi-billion” business, underscoring that the technology is regarded as a new platform layer, not a standalone feature.&lt;/p&gt;&lt;p&gt;The company was not afraid to say that its workforce is expected to shrink because of the technology. In June 2025, Amazon CEO Andy Jassy told employees that widespread use of generative AI and agents will change how work is done, and that over the next few years, Amazon expects routine work to become faster and more automated, slowing hiring, changing roles, and shrinking some job categories, even if other categories grow.&lt;/p&gt;&lt;p&gt;Amazon’s best use cases are high-volume, rules-bound workflows that require a lot of searching, checking, routing, and logging. These are or will have significant impact in forecasting, delivery mapping, customer service, and product content. /Reuters/ noted examples like inventory optimisation, improved customer service, and better product detail pages as internal targets for gen AI.&lt;/p&gt;&lt;p&gt;Amazon has described AI-enabled upgrades in its US operations that hint at where an agentic approach may take shape. In June 2025, it outlined AI innovations that included a generative AI system to improve delivery location accuracy, a new demand forecasting model to predict what customers want (and where), and an agentic AI team looking at enabling robots to understand natural-language&lt;/p&gt;&lt;p&gt;Consumer agents are where autonomy first becomes real, because systems can take actions, even where there’s money involved. Reporting in &lt;em&gt;The Verge&lt;/em&gt; about Alexa+ highlighted features like monitoring items for price drops and (optionally) purchasing for the user automatically once a threshold is hit, a concrete example of the agentic concept in everyday terms: users setting constraints (in the form of price thresholds), and the system watches and executes inside said boundaries.&lt;/p&gt;&lt;p&gt;Amazon’s Rufus assistant is positioned as an AI interface to shopping, one that helps customers find products, do comparisons, and understand the trade-offs between various choices. Amazon describes Rufus as powered by generative (and increasingly agentic) AI to make shopping faster, with personalisation created by a user’s shopping history and current context. Agents therefore become the a shopping interface, with their value to the retailer in shortening journey from intent to final purchase.&lt;/p&gt;&lt;p&gt;Internally, AWS is producing agentic ‘building blocks’. Agents for Amazon Bedrock are designed to execute multi-step tasks by orchestrating models with tools use and integration with other platforms. The Amazon Bedrock AgentCore is presented as a platform to build [PDF], deploy, and operate agents securely at scale. It has features like runtime hosting, memory, observability dashboards, and evaluation.&lt;/p&gt;&lt;p&gt;AgentCore is Amazon’s attempt to become the default infrastructure layer for supervised enterprise agents, especially for organisations that need auditability, access controls, and reliability.&lt;/p&gt;&lt;p&gt;If Amazon succeeds, the next phase for the technology is managed AI, comprising of mechanisms that grant or revoke permissions for tools and data access, the monitoring of agents’ behaviour, evaluation of performance and whether governance guidelines are being met, and the establishment of escalation paths when agents hit uncertainty.&lt;/p&gt;&lt;p&gt;The signals to the workforce have been baked into leadership messaging at the company. Fewer people will be required for some corporate tasks, and there will be more roles that can design workflows, govern the models, keep systems secure, and audit the outcomes of agentic AI use.&lt;/p&gt;&lt;p&gt;Proven as a leader in technology, Amazon’s stance on AI and the meaningful ways in which it’s implementing AI are a description of the paths enterprise companies may follow. Winning the productivity gains and lowered costs that AI technology promises is not as simple as plugging in a local device, or spinning up a new cloud instance. But the company can be seen as lighting the way for others to follow. Whether it’s supervising agents or deflecting customer queries to automated answering systems, AI is changing this technology giant in every possible way.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: CHEN – The Arousing, Thunder – arouse, excite, inspire; thunder rising from below; awe, alarm, trembling; fertilizing intrusion. The ideogram: excitement and rain” – public domain)&lt;/em&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;As the company that kick-started the cloud computing revolution, Amazon is one of the world’s biggest companies whose practices in all things technological can be regarded as a blueprint for implementing new technology.&lt;/p&gt;&lt;p&gt;This article looks at some of the ways that the company is deploying AI in its operations.&lt;/p&gt;&lt;p&gt;Amazon’s latest AI strategy has progressed from basic chatbots to agentic AI: systems that can plan and execute multi-step work using different tools and across processes. As a company, Amazon sits at the intersection of cloud infrastructure (in the form of AWS), logistics, retail, and customer service, all of which are areas where small efficiency gains can have massive impact.&lt;/p&gt;&lt;p&gt;In early 2025, Amazon made its AI intentions clear for its cloud company, AWS, by forming a new group focused internally on agentic AI. According to reporting on an internal email, AWS leadership described agentic AI as a potential “multi-billion” business, underscoring that the technology is regarded as a new platform layer, not a standalone feature.&lt;/p&gt;&lt;p&gt;The company was not afraid to say that its workforce is expected to shrink because of the technology. In June 2025, Amazon CEO Andy Jassy told employees that widespread use of generative AI and agents will change how work is done, and that over the next few years, Amazon expects routine work to become faster and more automated, slowing hiring, changing roles, and shrinking some job categories, even if other categories grow.&lt;/p&gt;&lt;p&gt;Amazon’s best use cases are high-volume, rules-bound workflows that require a lot of searching, checking, routing, and logging. These are or will have significant impact in forecasting, delivery mapping, customer service, and product content. /Reuters/ noted examples like inventory optimisation, improved customer service, and better product detail pages as internal targets for gen AI.&lt;/p&gt;&lt;p&gt;Amazon has described AI-enabled upgrades in its US operations that hint at where an agentic approach may take shape. In June 2025, it outlined AI innovations that included a generative AI system to improve delivery location accuracy, a new demand forecasting model to predict what customers want (and where), and an agentic AI team looking at enabling robots to understand natural-language&lt;/p&gt;&lt;p&gt;Consumer agents are where autonomy first becomes real, because systems can take actions, even where there’s money involved. Reporting in &lt;em&gt;The Verge&lt;/em&gt; about Alexa+ highlighted features like monitoring items for price drops and (optionally) purchasing for the user automatically once a threshold is hit, a concrete example of the agentic concept in everyday terms: users setting constraints (in the form of price thresholds), and the system watches and executes inside said boundaries.&lt;/p&gt;&lt;p&gt;Amazon’s Rufus assistant is positioned as an AI interface to shopping, one that helps customers find products, do comparisons, and understand the trade-offs between various choices. Amazon describes Rufus as powered by generative (and increasingly agentic) AI to make shopping faster, with personalisation created by a user’s shopping history and current context. Agents therefore become the a shopping interface, with their value to the retailer in shortening journey from intent to final purchase.&lt;/p&gt;&lt;p&gt;Internally, AWS is producing agentic ‘building blocks’. Agents for Amazon Bedrock are designed to execute multi-step tasks by orchestrating models with tools use and integration with other platforms. The Amazon Bedrock AgentCore is presented as a platform to build [PDF], deploy, and operate agents securely at scale. It has features like runtime hosting, memory, observability dashboards, and evaluation.&lt;/p&gt;&lt;p&gt;AgentCore is Amazon’s attempt to become the default infrastructure layer for supervised enterprise agents, especially for organisations that need auditability, access controls, and reliability.&lt;/p&gt;&lt;p&gt;If Amazon succeeds, the next phase for the technology is managed AI, comprising of mechanisms that grant or revoke permissions for tools and data access, the monitoring of agents’ behaviour, evaluation of performance and whether governance guidelines are being met, and the establishment of escalation paths when agents hit uncertainty.&lt;/p&gt;&lt;p&gt;The signals to the workforce have been baked into leadership messaging at the company. Fewer people will be required for some corporate tasks, and there will be more roles that can design workflows, govern the models, keep systems secure, and audit the outcomes of agentic AI use.&lt;/p&gt;&lt;p&gt;Proven as a leader in technology, Amazon’s stance on AI and the meaningful ways in which it’s implementing AI are a description of the paths enterprise companies may follow. Winning the productivity gains and lowered costs that AI technology promises is not as simple as plugging in a local device, or spinning up a new cloud instance. But the company can be seen as lighting the way for others to follow. Whether it’s supervising agents or deflecting customer queries to automated answering systems, AI is changing this technology giant in every possible way.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image source: CHEN – The Arousing, Thunder – arouse, excite, inspire; thunder rising from below; awe, alarm, trembling; fertilizing intrusion. The ideogram: excitement and rain” – public domain)&lt;/em&gt;&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/awss-legacy-will-be-in-ai-success/</guid><pubDate>Mon, 15 Dec 2025 13:44:11 +0000</pubDate></item><item><title>[NEW] How to Fine-Tune an LLM on NVIDIA GPUs With Unsloth (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Modern workflows showcase the endless possibilities of generative and agentic AI on PCs.&lt;/p&gt;
&lt;p&gt;Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing one’s schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks.&lt;/p&gt;
&lt;p&gt;That’s where fine-tuning comes in.&lt;/p&gt;
&lt;p&gt;Unsloth, one of the world’s most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. It’s optimized for efficient, low-memory training on NVIDIA GPUs — from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark, the world’s smallest AI supercomputer.&lt;/p&gt;
&lt;p&gt;Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Teaching AI New Tricks&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand.&lt;/p&gt;
&lt;p&gt;Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Parameter-efficient fine-tuning (such as LoRA or QLoRA)&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates only a small portion of the model for faster, lower-cost training. It’s a smarter and efficient way to enhance a model without altering it drastically.&lt;/li&gt;
&lt;li&gt;Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied — including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior.&lt;/li&gt;
&lt;li&gt;Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Full fine-tuning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates all of the model’s parameters — useful for teaching the model to follow specific formats or styles.&lt;/li&gt;
&lt;li&gt;Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner.&lt;/li&gt;
&lt;li&gt;Requirements: Large dataset (1,000+ prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Reinforcement learning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference — and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unsloth’s Reinforcement Learning Guide for details.&lt;/li&gt;
&lt;li&gt;Target use case: Improving the accuracy of a model in a particular domain — such as law or medicine — or building autonomous agents that can orchestrate actions on a user’s behalf.&lt;/li&gt;
&lt;li&gt;Requirements:&amp;nbsp; A process that contains an action model, a reward model and an environment for the model to learn from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88327"&gt;&lt;img alt="alt" class="size-full wp-image-88327" height="712" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Fine-tuning-requirements-on-Unsloth.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88327"&gt;Fine-tuning requirements on Unsloth.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently.&lt;/p&gt;
&lt;p&gt;Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training.&lt;/p&gt;
&lt;p&gt;Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unsloth’s ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers.&lt;/p&gt;
&lt;p&gt;The framework is built and optimized for NVIDIA hardware — from GeForce RTX laptops to RTX PRO workstations and DGX Spark — providing peak performance while reducing VRAM consumption.&lt;/p&gt;
&lt;p&gt;Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows.&lt;/p&gt;
&lt;p&gt;Check out some of these Unsloth guides:&lt;/p&gt;

&lt;p&gt;Learn how to install Unsloth on NVIDIA DGX Spark. Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;p&gt;For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Available Now: NVIDIA Nemotron 3 Family of Open Models&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The new Nemotron 3 family of open models — in Nano, Super, and Ultra sizes — built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications.&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. It’s optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 60% fewer reasoning tokens, significantly reducing inference cost.&lt;/li&gt;
&lt;li&gt;A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while&amp;nbsp;Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026.&lt;/p&gt;
&lt;p&gt;NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth.&lt;/p&gt;
&lt;p&gt;Download Nemotron 3 Nano now from Hugging Face, or experiment with it through Llama.cpp and LM Studio.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DGX Spark: A Compact AI Powerhouse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally.&lt;/p&gt;
&lt;p&gt;For fine-tuning, DGX Spark enables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Larger model sizes.&lt;/b&gt; Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Spark’s unified memory.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;More advanced techniques. &lt;/b&gt;Full fine-tuning and reinforcement-learning-based workflows — which demand more memory and higher throughput — run significantly faster on DGX Spark.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Local control without cloud queues.&lt;/b&gt; Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DGX Spark’s strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines.&lt;/p&gt;
&lt;p&gt;The table below shows performance for fine-tuning the Llama family of models on DGX Spark.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88330"&gt;&lt;img alt="alt" class="size-full wp-image-88330" height="383" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Performance-for-fine-tuning-Llama-family-of-models-on-DGX-Spark.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88330"&gt;Performance for fine-tuning Llama family of models on DGX Spark.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark.&lt;/p&gt;
&lt;p&gt;Learn more about how DGX Spark enables intensive AI tasks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI — The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;🚀 &lt;b&gt;FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%.&lt;/p&gt;
&lt;p&gt;✨ &lt;b&gt;Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;🤝Mistral AI Unveils New Model Family Optimized for NVIDIA GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;🎨Blender 5.0 Lands With HDR Color and Major Performance Gains&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Modern workflows showcase the endless possibilities of generative and agentic AI on PCs.&lt;/p&gt;
&lt;p&gt;Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing one’s schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks.&lt;/p&gt;
&lt;p&gt;That’s where fine-tuning comes in.&lt;/p&gt;
&lt;p&gt;Unsloth, one of the world’s most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. It’s optimized for efficient, low-memory training on NVIDIA GPUs — from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark, the world’s smallest AI supercomputer.&lt;/p&gt;
&lt;p&gt;Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Teaching AI New Tricks&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand.&lt;/p&gt;
&lt;p&gt;Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Parameter-efficient fine-tuning (such as LoRA or QLoRA)&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates only a small portion of the model for faster, lower-cost training. It’s a smarter and efficient way to enhance a model without altering it drastically.&lt;/li&gt;
&lt;li&gt;Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied — including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior.&lt;/li&gt;
&lt;li&gt;Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Full fine-tuning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Updates all of the model’s parameters — useful for teaching the model to follow specific formats or styles.&lt;/li&gt;
&lt;li&gt;Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner.&lt;/li&gt;
&lt;li&gt;Requirements: Large dataset (1,000+ prompt-sample pairs).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Reinforcement learning&lt;/b&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference — and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unsloth’s Reinforcement Learning Guide for details.&lt;/li&gt;
&lt;li&gt;Target use case: Improving the accuracy of a model in a particular domain — such as law or medicine — or building autonomous agents that can orchestrate actions on a user’s behalf.&lt;/li&gt;
&lt;li&gt;Requirements:&amp;nbsp; A process that contains an action model, a reward model and an environment for the model to learn from.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88327"&gt;&lt;img alt="alt" class="size-full wp-image-88327" height="712" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Fine-tuning-requirements-on-Unsloth.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88327"&gt;Fine-tuning requirements on Unsloth.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently.&lt;/p&gt;
&lt;p&gt;Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training.&lt;/p&gt;
&lt;p&gt;Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unsloth’s ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers.&lt;/p&gt;
&lt;p&gt;The framework is built and optimized for NVIDIA hardware — from GeForce RTX laptops to RTX PRO workstations and DGX Spark — providing peak performance while reducing VRAM consumption.&lt;/p&gt;
&lt;p&gt;Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows.&lt;/p&gt;
&lt;p&gt;Check out some of these Unsloth guides:&lt;/p&gt;

&lt;p&gt;Learn how to install Unsloth on NVIDIA DGX Spark. Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform.&lt;/p&gt;
&lt;p&gt;For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Available Now: NVIDIA Nemotron 3 Family of Open Models&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The new Nemotron 3 family of open models — in Nano, Super, and Ultra sizes — built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications.&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. It’s optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 60% fewer reasoning tokens, significantly reducing inference cost.&lt;/li&gt;
&lt;li&gt;A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while&amp;nbsp;Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026.&lt;/p&gt;
&lt;p&gt;NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth.&lt;/p&gt;
&lt;p&gt;Download Nemotron 3 Nano now from Hugging Face, or experiment with it through Llama.cpp and LM Studio.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;DGX Spark: A Compact AI Powerhouse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally.&lt;/p&gt;
&lt;p&gt;For fine-tuning, DGX Spark enables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Larger model sizes.&lt;/b&gt; Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Spark’s unified memory.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;More advanced techniques. &lt;/b&gt;Full fine-tuning and reinforcement-learning-based workflows — which demand more memory and higher throughput — run significantly faster on DGX Spark.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Local control without cloud queues.&lt;/b&gt; Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DGX Spark’s strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines.&lt;/p&gt;
&lt;p&gt;The table below shows performance for fine-tuning the Llama family of models on DGX Spark.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_88330"&gt;&lt;img alt="alt" class="size-full wp-image-88330" height="383" src="https://blogs.nvidia.com/wp-content/uploads/2025/12/Performance-for-fine-tuning-Llama-family-of-models-on-DGX-Spark.jpg" width="923" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-88330"&gt;Performance for fine-tuning Llama family of models on DGX Spark.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark.&lt;/p&gt;
&lt;p&gt;Learn more about how DGX Spark enables intensive AI tasks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI — The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;🚀 &lt;b&gt;FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%.&lt;/p&gt;
&lt;p&gt;✨ &lt;b&gt;Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;🤝Mistral AI Unveils New Model Family Optimized for NVIDIA GPUs&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;🎨Blender 5.0 Lands With HDR Color and Major Performance Gains&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</guid><pubDate>Mon, 15 Dec 2025 14:00:11 +0000</pubDate></item><item><title>[NEW] Nemotron 3 Nano \- A new Standard for Efficient, Open, and Intelligent Agentic Models (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Chintan's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/59ef303ccd0ee50d169be4b14008cd21.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
If 2025 was the year of AI agents, then 2026 is gearing up to be the year of, well, multi-agents. This leap to the next step requires models that produce a lot of tokens, generated by lightweight accurate models.
&lt;p&gt;However, this transition also forces difficult tradeoffs. Smaller models are fast and cheap but often lack the reasoning depth, robustness, and long context capacity needed for advanced multi-agents. Larger models deliver strong accuracy, but are too slow and expensive when many agents are running in parallel. As agentic systems grow, inference costs spiral, context windows become a bottleneck, and reliability starts to degrade, making efficiency of utmost importance.&lt;/p&gt;
&lt;p&gt;Striking the right balance is what led NVIDIA to produce the &lt;strong&gt;NVIDIA Nemotron 3 Nano 30B A3B&lt;/strong&gt;, part of our Nemotron 3 family of models (Nano,  Super, and Ultra). &lt;/p&gt;
&lt;p&gt;Nano utilizes a hybrid Mamba-Transformer Mixture-of-Experts (MoE) architecture with a 1M-token context window. (🔥🔥🔥) enabling developers to build high-throughput, reliable agents that are more accurate, more scalable, and capable of specialized sub-tasks in long-running multi-step workflows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt;  Mamba‑2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt;  Across reasoning, coding, tools, and multi-step agentic tasks  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap “thinking” tokens and keep inference cost predictable  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and build.nvidia.com endpoints  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the nvidia-open-model-license&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="A two-panel figure comparing Nemotron 3 Nano with Qwen3-30B and GPT-OSS-20B. The left panel displays accuracy scores, showing Nano equal or higher across benchmarks. The right panel displays inference throughput bars, where the Nano is significantly taller; illustrating 3.3x speed over Qwen3 and 2.2x over GPT-OSS." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/4DT2feJmd_ICbzZ-xKqd5.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Nemotron 3 Nano matches or exceeds the accuracy of Qwen3-30B and GPT-OSS-20B while delivering dramatically higher throughput. In an 8K input / 16K output configuration on a single H200 GPU, Nano achieves 3.3x higher throughput than Qwen3-30B and 2.2x higher than GPT-OSS-20B.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B/A3B) is our latest small-but-powerful reasoning model, building on the success of Nemotron Nano 2's hybrid Mamba-2 + Transformer architecture, reasoning ON/OFF modes, and explicit thinking budgets—while introducing a major architectural upgrade: a sparse Mixture-of-Experts (MoE) design.&lt;/p&gt;
&lt;p&gt;At high level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~3.6B active parameters per token&lt;/strong&gt;, thanks to the MoE routing  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid layer stack&lt;/strong&gt; with interleaved Mamba‑2 layers and grouped-query attention (GQA) Transformer layers   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A learned multi-layer perceptron (MLP) router&lt;/strong&gt; that activates 6 of 128 experts on each forward pass, delivering both efficiency and reasoning accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This combination enables Nemotron 3 Nano to behave like a much larger model in terms of reasoning quality—while maintaining the speed and cost profile expected of a lightweight architecture&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the Nemotron-Nano-3-30B-A3B architecture showing four sequential blocks. Each block contains repeating Mamba-2 layers and MoE units, with attention layers interspersed in the first and third blocks. The blocks repeat x5, x3, x1, and x4 times respectively, illustrating the hybrid Mamba-Transformer design with MoE layers replacing FFNs." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/OZe39if73d6P8vi6sDDNd.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Nemotron 3 Nano architecture. It uses a hybrid Mamba-Transformer backbone, similar to Nemotron Nano v2, but replaces standard feed-forward network (FFN) layers with sparse MoE layers to significantly boost efficiency and scalability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano is built for agentic, reasoning, tool-use, and chat tasks and supports a&lt;br /&gt;context length up to 1M tokens.&lt;/p&gt;
&lt;p&gt;It extends the Nemotron model family we released earlier in the year, continuing the progression toward increasingly accurate and efficient open models for reasoning and agent development.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The roadmap graphic illustrates the evolution of Nemotron model families: **Nemotron 1** enhances Llama models with stronger reasoning capabilities, **Nemotron 2** introduces a hybrid Mamba-Transformer architecture, delivering state-of-the-art accuracy and efficiency, **Nemotron 3** adds sparse MoE to the hybrid design, further improving accuracy, throughput, latency, and overall compute efficiency." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/O-jBDwGrrb9T3GGi1qAHo.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: NVIDIA Nemotron family of open models is engineered for advanced reasoning and agentic tasks, delivering leading accuracies, and best-in-class efficiencies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We employed a multi-stage pipeline combining massive-scale pre-training, specialized supervised fine-tuning (SFT), and advanced reinforcement learning techniques to refine the reasoning abilities and agentic behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Pre-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron 3 Nano was trained on a  25-trillion-token corpus (including 2.5T of new Common Crawl tokens), spanning web crawls, code and math, Wikipedia and academic text, multilingual content (15 Pre-training followed a two-phase strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phase 1: Diversity (first 94%)&lt;/strong&gt;&lt;br /&gt;Broad , diverse mixture to maximize coverage and generalization.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phase 2: Quality (final 6%)&lt;/strong&gt;&lt;br /&gt;High-quality sources such as Wikipedia to refine accuracy and consistency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Long-Context Extension&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Context length for Nemotron 3 Nano was extended by adding a continued pre-training (CPT) stage at 512k sequence length. A mixture of 512k and 4k sequence length training preserved short benchmark scores while extending the context length. We included synthetic data designed to support long-range retrieval, multi-hop reasoning, multi-document information aggregation, and related capabilities across different stages of training.&lt;/p&gt;
&lt;p&gt;We are releasing a large portion of these  pretraining datasets openly on Hugging Face. These additions contribute 3 trillion new tokens to the Nemotron-Pretraining series, with higher-fidelity coverage of code, math, and reasoning. Enhanced synthetic augmentation and annotation pipelines increase data density and structure, improving training efficiency and directly contributing to Nemotron-3 Nano's strong quality profile.&lt;/p&gt;
&lt;p&gt;With Nemotron 3, we’ve learned that quantity without quality isn’t useful. Our pre-training data continues to shift toward efficient data: smarter filtration, rewritten and improved samples, and nearly half a trillion tokens of rescued math and code that previous pipelines would have discarded. This focus on signal over noise directly enables smarter, smaller models that are cheaper to train and run, without sacrificing accuracy.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This included Supervised fine-tuning (SFT) and two distinct stages of reinforcement learning, RLVR and RLHF. These stages specialize the model for agentic workflows, tool use, high-quality reasoning, and chat tasks.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Supervised Finetuning&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our SFT recipe was improved from Nano v2 to better support complex agentic behaviors. Improvements included greater dataset diversity, higher data quality, and explicit training for multi-step and multi-turn reasoning.&lt;/p&gt;
&lt;p&gt;The model learns both reasoning ON/OFF modes directly from the chat template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reasoning ON:&lt;/strong&gt; multi-step mode, where the model preserves and builds upon its prior chain-of-thought within a task.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning OFF:&lt;/strong&gt; multi-turn mode, where reasoning content is not carried over across turns, ensuring concise responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="The graph from Artificial Analysis plots small language reasoning models on intelligence index on the y-axis and output tokens per second on the x-axis. Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym" src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/i169eq3mbepGags0uIB-y.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym&lt;/p&gt;
&lt;p&gt;We are releasing the majority of SFT datasets and codebase openly.&lt;/p&gt;
&lt;p&gt;Our new post-training data release also expands the intelligence of the model by design. We added 13 million new post-training samples—nearly tripling our previous release and making this the largest openly available post-training corpus by 2.5×. To reach higher reasoning accuracy, we blended cross-disciplinary domains including code, math, physics, and chemistry to create novel, multi-step problems that don’t exist in scraped web data. This helps the model reason about questions that fall between fields, where real scientific and technical progress often happens.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Multi environment Reinforcement Learning from Verifiable Rewards (RLVR)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Nemotron 3 Nano was trained simultaneously across many distinct environments - spanning math, code, question answering, instruction following, multi-step tool use, multi-turn conversations, and structured output among others, using synchronous GRPO (Group Relative Policy Optimization). This multi-environment RLVR stage ensures uniform improvement across domains, reduced overfitting to any single benchmark, and more reliable agentic behavior in real-world workflows.&lt;/p&gt;
&lt;p&gt;&lt;img alt="This figure shows multiple different environment reward curves over training steps, showcasing how the model was learning many different capabilities simultaneously." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/ha57qyj3KKsxmxea1frIR.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Uniform improvements due to training simultaneously on multiple RL environments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Models need more than textbooks to train — they need a gym. NVIDIA is one of the only open model providers releasing both reinforcement learning datasets and the environments used to train them. This enables developers to test agents, capture critical edge cases, and prevent model drift over time. In this release, we are adding 10+ new RL environments covering competitive coding, advanced math, and even real-world calendar scheduling.&lt;/p&gt;
&lt;p&gt;We are also open-sourcing all the essential RLVR infrastructure—the environments including their datasets and code used to build and scale them. These components form the foundation of the new NVIDIA NeMo Gym library, which enables scalable RL environment construction. &lt;/p&gt;
&lt;p&gt;Training at scale is executed using NVIDIA NeMo RL, our high-performance RL library.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforce Learning Using Human Feedback (RLHF)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To further refine the model’s conversational quality, we trained a generative reward model (GenRM) using GRPO on Qwen3-235B-A22B. &lt;/p&gt;
&lt;p&gt;Given a conversation history, a new user query, and two candidate assistant responses, the GenRM explicitly reasons about the strengths and weaknesses of each response, produces individual helpfulness scores and generates a relative ranking between the candidates. These  reward signals are then used in an RLHF stage to improve helpfulness, coherence, correctness, and overall chat experience in Nemotron 3 Nano.&lt;/p&gt;
&lt;p&gt;The combined post-training pipeline—SFT+RLVR+RLHF—produces the final Nemotron 3 Nano 30B-A3B model.&lt;/p&gt;
&lt;p&gt;As models evolve into multi-step agents that use tools, they face entirely new safety and security challenges. To support responsible deployment, we are releasing an agentic safety dataset featuring nearly 11,000 labeled traces from realistic, tool-using workflows. This gives developers the data they need to evaluate, diagnose, and mitigate safety risks before agentic systems reach production.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Why We Needed Better RL Infrastructure
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;During development, the limitations of existing RL tooling became clear. Training large reasoning models with RL is difficult because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-step rollouts are complex to orchestrate  &lt;/li&gt;
&lt;li&gt;Tool integrations are often brittle  &lt;/li&gt;
&lt;li&gt;Orchestration logic can conflict with training loop design  &lt;/li&gt;
&lt;li&gt;Collecting rollout data at scale is slow and difficult  &lt;/li&gt;
&lt;li&gt;Most high-quality RL environments are closed and proprietary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, meaningful RL training has historically been accessible only to major AI labs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		NeMo Gym: Opening RL to Everyone
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To overcome these challenges NVIDIA built NeMo Gym, an open-source standardized library for building and scaling RL environments.&lt;br /&gt;NeMo Gym powers reinforcement learning pipelines used in Nemotron 3 Nano, and now gives developers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ready-to-use RL environments across math, code, tool use, multi-turn reasoning, and agentic workflows  &lt;/li&gt;
&lt;li&gt;The ability to build custom RL environments with verifiable reward logic  &lt;/li&gt;
&lt;li&gt;Ecosystem interoperability with NeMo RL and other training frameworks (TRL, Unsloth, VeRL underway)  &lt;/li&gt;
&lt;li&gt;High-throughput rollout orchestration, enabling large-scale RL training  &lt;/li&gt;
&lt;li&gt;A practical pathway to perform RL on their own models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NeMo Gym is a flexible open source library for building and running RL training environments. It is part of the broader NVIDIA NeMo software suite for end-to-end model training and provides infrastructure for designing, running, and scaling complex RL environments. &lt;/p&gt;
&lt;p&gt;Battle tested through the development of the entire Nemotron 3 model family, NeMo Gym includes the core environment development infrastructure, a growing collection of ready-to-use training environments alongside the datasets used in RLVR, and integration with NeMo RL, the high-performance and efficient RL training engine with support for advanced RL training algorithms, end-to-end FP8 training and async RL.&lt;/p&gt;
&lt;p&gt;With NeMo Gym, teams can quickly assemble environments using modular server components and templates, integrate external tools, systems, or databases, and orchestrate long-context, multi-step, multi-turn rollouts. This allows training environments to be iterated on and shared independent of the training loop.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram illustrating interaction between an RL Training Framework on the left and NeMo Gym on the right. The training framework sends task prompts  to the agent server in the NeMo Gym. The agent server coordinates with the policy model server and external resources server to collect rollouts and verify task performance. The scored trajectories are returned back to the Training Framework for model updates." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/go-hdGzZpaKRJChrl-o6k.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: How NeMo Gym fits into the RL training loop: The RL training framework (e.g., NeMo RL) sends task prompts to NeMo Gym, which operates as a set of independent HTTP services. Inside NeMo Gym, the agent server orchestrates rollouts by coordinating the policy model server (generation) and external resources server (tools and rewards). NeMo Gym returns model trajectories and rewards to the training framework, which then  updates and refits the policy model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By decoupling RL environments from RL training frameworks, NeMo Gym works seamlessly with many popular training frameworks (such as NeMo RL), supports high-throughput, concurrent rollout collection, and enables large-scale distributed RL training. This separation of concerns makes it easy to scale RL workflows and adapt environments as training objectives evolve.&lt;/p&gt;
&lt;p&gt;To accelerate experimentation, NeMo Gym ships with an expanding RL Hub—a catalog of ready-to-use domain-specific environments that developers can use immediately or extend. Current domains include math, coding, instruction following, multi-step tool use, multi-turn structured conversations. Practitioners can fine-tune models on these environments out of the box, reuse community contributions, or publish their own.&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B A3B) delivers state-of-the-art accuracy in an exceptionally cost-efficient package. It offers up to 3.3x higher throughput than leading open-source models of similar size (see Figure 1), while supporting a 1M-token context window —performing well on long-context reasoning benchmarks.&lt;/p&gt;
&lt;p&gt;Built for high-volume, real-time execution, Nemotron 3 Nano excels in math and coding, multi-step tool calling, and multi-turn agentic workflows. It also retains the classic Nemotron Thinking ON/OFF modes and Thinking Budget controls, giving developers the ability to tune exactly how much the model thinks for each task.&lt;/p&gt;
&lt;p&gt;With this release, we are also introducing NeMo Gym, containing ready-to-use training environments we developed during the course of Nemotron 3 training, and the infrastructure to build your own training environments and scale rollout collection. &lt;/p&gt;
&lt;p&gt;We are releasing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full model weights  &lt;/li&gt;
&lt;li&gt;The complete training recipe, including SFT, RLVR, and RLHF  &lt;/li&gt;
&lt;li&gt;Most of the datasets (pre-training, post-training) used throughout the training pipeline  &lt;/li&gt;
&lt;li&gt;Training frameworks that power Nemotron 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything you need to study, reproduce, or extend the model is available openly.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron 3 Nano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Download the model:&lt;/strong&gt; Now available on Hugging Face.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Try hosted endpoints:&lt;/strong&gt; Run queries instantly on OpenRouter or build.nvidia.com.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy at scale:&lt;/strong&gt; Use our cookbooks for vLLM, TRT-LLM, and SGLang  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experiment, develop and run at the edge:&lt;/strong&gt; Available on edge devices such as NVIDIA RTX AI PCs and Workstations and DGX Spark via Llama.cpp, LM Studio and Unsloth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a deep dive into the architecture, datasets, and benchmarks, read the full Nemotron 3 Nano Technical Report.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Chintan's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/59ef303ccd0ee50d169be4b14008cd21.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
If 2025 was the year of AI agents, then 2026 is gearing up to be the year of, well, multi-agents. This leap to the next step requires models that produce a lot of tokens, generated by lightweight accurate models.
&lt;p&gt;However, this transition also forces difficult tradeoffs. Smaller models are fast and cheap but often lack the reasoning depth, robustness, and long context capacity needed for advanced multi-agents. Larger models deliver strong accuracy, but are too slow and expensive when many agents are running in parallel. As agentic systems grow, inference costs spiral, context windows become a bottleneck, and reliability starts to degrade, making efficiency of utmost importance.&lt;/p&gt;
&lt;p&gt;Striking the right balance is what led NVIDIA to produce the &lt;strong&gt;NVIDIA Nemotron 3 Nano 30B A3B&lt;/strong&gt;, part of our Nemotron 3 family of models (Nano,  Super, and Ultra). &lt;/p&gt;
&lt;p&gt;Nano utilizes a hybrid Mamba-Transformer Mixture-of-Experts (MoE) architecture with a 1M-token context window. (🔥🔥🔥) enabling developers to build high-throughput, reliable agents that are more accurate, more scalable, and capable of specialized sub-tasks in long-running multi-step workflows.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hybrid Mamba-Transformer MoE architecture:&lt;/strong&gt;  Mamba‑2 for long-context, low-latency inference combined with transformer attention for high-accuracy, fine-grained reasoning  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters, ~3.6B active per token:&lt;/strong&gt; Designed for high throughput and low latency  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Exceptional inference efficiency:&lt;/strong&gt; Up to 4x faster than Nemotron Nano 2 and up to 3.3x faster than leading models in its size category  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best-in-class reasoning accuracy:&lt;/strong&gt;  Across reasoning, coding, tools, and multi-step agentic tasks  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning controls:&lt;/strong&gt; Reasoning ON/OFF modes plus a configurable thinking budget to cap “thinking” tokens and keep inference cost predictable  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1M-token context window:&lt;/strong&gt; Ideal for long-horizon workflows, retrieval-augmented tasks, and persistent memory  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fully open:&lt;/strong&gt; Open Weights, datasets, training recipes, and framework  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A full open data stack&lt;/strong&gt;: 3T new high-quality pre-training tokens, 13M cross-disciplinary post-training samples, 10+ RL environments with datasets covering more than 900k tasks in math, coding, reasoning, and tool-use, and ~11k agent-safety traces  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easy deployment:&lt;/strong&gt; Seamless serving with vLLM and SGLang, and integration via OpenRouter, popular inference service providers, and build.nvidia.com endpoints  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Released under the nvidia-open-model-license&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="A two-panel figure comparing Nemotron 3 Nano with Qwen3-30B and GPT-OSS-20B. The left panel displays accuracy scores, showing Nano equal or higher across benchmarks. The right panel displays inference throughput bars, where the Nano is significantly taller; illustrating 3.3x speed over Qwen3 and 2.2x over GPT-OSS." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/4DT2feJmd_ICbzZ-xKqd5.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; Nemotron 3 Nano matches or exceeds the accuracy of Qwen3-30B and GPT-OSS-20B while delivering dramatically higher throughput. In an 8K input / 16K output configuration on a single H200 GPU, Nano achieves 3.3x higher throughput than Qwen3-30B and 2.2x higher than GPT-OSS-20B.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B/A3B) is our latest small-but-powerful reasoning model, building on the success of Nemotron Nano 2's hybrid Mamba-2 + Transformer architecture, reasoning ON/OFF modes, and explicit thinking budgets—while introducing a major architectural upgrade: a sparse Mixture-of-Experts (MoE) design.&lt;/p&gt;
&lt;p&gt;At high level:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;31.6B total parameters&lt;/strong&gt;  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;~3.6B active parameters per token&lt;/strong&gt;, thanks to the MoE routing  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hybrid layer stack&lt;/strong&gt; with interleaved Mamba‑2 layers and grouped-query attention (GQA) Transformer layers   &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;A learned multi-layer perceptron (MLP) router&lt;/strong&gt; that activates 6 of 128 experts on each forward pass, delivering both efficiency and reasoning accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This combination enables Nemotron 3 Nano to behave like a much larger model in terms of reasoning quality—while maintaining the speed and cost profile expected of a lightweight architecture&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram of the Nemotron-Nano-3-30B-A3B architecture showing four sequential blocks. Each block contains repeating Mamba-2 layers and MoE units, with attention layers interspersed in the first and third blocks. The blocks repeat x5, x3, x1, and x4 times respectively, illustrating the hybrid Mamba-Transformer design with MoE layers replacing FFNs." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/OZe39if73d6P8vi6sDDNd.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Nemotron 3 Nano architecture. It uses a hybrid Mamba-Transformer backbone, similar to Nemotron Nano v2, but replaces standard feed-forward network (FFN) layers with sparse MoE layers to significantly boost efficiency and scalability.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Nemotron 3 Nano is built for agentic, reasoning, tool-use, and chat tasks and supports a&lt;br /&gt;context length up to 1M tokens.&lt;/p&gt;
&lt;p&gt;It extends the Nemotron model family we released earlier in the year, continuing the progression toward increasingly accurate and efficient open models for reasoning and agent development.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The roadmap graphic illustrates the evolution of Nemotron model families: **Nemotron 1** enhances Llama models with stronger reasoning capabilities, **Nemotron 2** introduces a hybrid Mamba-Transformer architecture, delivering state-of-the-art accuracy and efficiency, **Nemotron 3** adds sparse MoE to the hybrid design, further improving accuracy, throughput, latency, and overall compute efficiency." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/O-jBDwGrrb9T3GGi1qAHo.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: NVIDIA Nemotron family of open models is engineered for advanced reasoning and agentic tasks, delivering leading accuracies, and best-in-class efficiencies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We employed a multi-stage pipeline combining massive-scale pre-training, specialized supervised fine-tuning (SFT), and advanced reinforcement learning techniques to refine the reasoning abilities and agentic behavior.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Pre-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Nemotron 3 Nano was trained on a  25-trillion-token corpus (including 2.5T of new Common Crawl tokens), spanning web crawls, code and math, Wikipedia and academic text, multilingual content (15 Pre-training followed a two-phase strategy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phase 1: Diversity (first 94%)&lt;/strong&gt;&lt;br /&gt;Broad , diverse mixture to maximize coverage and generalization.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Phase 2: Quality (final 6%)&lt;/strong&gt;&lt;br /&gt;High-quality sources such as Wikipedia to refine accuracy and consistency.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Long-Context Extension&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Context length for Nemotron 3 Nano was extended by adding a continued pre-training (CPT) stage at 512k sequence length. A mixture of 512k and 4k sequence length training preserved short benchmark scores while extending the context length. We included synthetic data designed to support long-range retrieval, multi-hop reasoning, multi-document information aggregation, and related capabilities across different stages of training.&lt;/p&gt;
&lt;p&gt;We are releasing a large portion of these  pretraining datasets openly on Hugging Face. These additions contribute 3 trillion new tokens to the Nemotron-Pretraining series, with higher-fidelity coverage of code, math, and reasoning. Enhanced synthetic augmentation and annotation pipelines increase data density and structure, improving training efficiency and directly contributing to Nemotron-3 Nano's strong quality profile.&lt;/p&gt;
&lt;p&gt;With Nemotron 3, we’ve learned that quantity without quality isn’t useful. Our pre-training data continues to shift toward efficient data: smarter filtration, rewritten and improved samples, and nearly half a trillion tokens of rescued math and code that previous pipelines would have discarded. This focus on signal over noise directly enables smarter, smaller models that are cheaper to train and run, without sacrificing accuracy.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Post-Training
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;This included Supervised fine-tuning (SFT) and two distinct stages of reinforcement learning, RLVR and RLHF. These stages specialize the model for agentic workflows, tool use, high-quality reasoning, and chat tasks.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Supervised Finetuning&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Our SFT recipe was improved from Nano v2 to better support complex agentic behaviors. Improvements included greater dataset diversity, higher data quality, and explicit training for multi-step and multi-turn reasoning.&lt;/p&gt;
&lt;p&gt;The model learns both reasoning ON/OFF modes directly from the chat template:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reasoning ON:&lt;/strong&gt; multi-step mode, where the model preserves and builds upon its prior chain-of-thought within a task.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reasoning OFF:&lt;/strong&gt; multi-turn mode, where reasoning content is not carried over across turns, ensuring concise responses.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="The graph from Artificial Analysis plots small language reasoning models on intelligence index on the y-axis and output tokens per second on the x-axis. Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym" src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/i169eq3mbepGags0uIB-y.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; Nemotron 3 Nano delivers the highest throughput efficiency using the hybrid MoE architecture and leading accuracy with advanced Reinforcement Learning using NeMo Gym&lt;/p&gt;
&lt;p&gt;We are releasing the majority of SFT datasets and codebase openly.&lt;/p&gt;
&lt;p&gt;Our new post-training data release also expands the intelligence of the model by design. We added 13 million new post-training samples—nearly tripling our previous release and making this the largest openly available post-training corpus by 2.5×. To reach higher reasoning accuracy, we blended cross-disciplinary domains including code, math, physics, and chemistry to create novel, multi-step problems that don’t exist in scraped web data. This helps the model reason about questions that fall between fields, where real scientific and technical progress often happens.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Multi environment Reinforcement Learning from Verifiable Rewards (RLVR)&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Nemotron 3 Nano was trained simultaneously across many distinct environments - spanning math, code, question answering, instruction following, multi-step tool use, multi-turn conversations, and structured output among others, using synchronous GRPO (Group Relative Policy Optimization). This multi-environment RLVR stage ensures uniform improvement across domains, reduced overfitting to any single benchmark, and more reliable agentic behavior in real-world workflows.&lt;/p&gt;
&lt;p&gt;&lt;img alt="This figure shows multiple different environment reward curves over training steps, showcasing how the model was learning many different capabilities simultaneously." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/ha57qyj3KKsxmxea1frIR.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Uniform improvements due to training simultaneously on multiple RL environments.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Models need more than textbooks to train — they need a gym. NVIDIA is one of the only open model providers releasing both reinforcement learning datasets and the environments used to train them. This enables developers to test agents, capture critical edge cases, and prevent model drift over time. In this release, we are adding 10+ new RL environments covering competitive coding, advanced math, and even real-world calendar scheduling.&lt;/p&gt;
&lt;p&gt;We are also open-sourcing all the essential RLVR infrastructure—the environments including their datasets and code used to build and scale them. These components form the foundation of the new NVIDIA NeMo Gym library, which enables scalable RL environment construction. &lt;/p&gt;
&lt;p&gt;Training at scale is executed using NVIDIA NeMo RL, our high-performance RL library.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Reinforce Learning Using Human Feedback (RLHF)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To further refine the model’s conversational quality, we trained a generative reward model (GenRM) using GRPO on Qwen3-235B-A22B. &lt;/p&gt;
&lt;p&gt;Given a conversation history, a new user query, and two candidate assistant responses, the GenRM explicitly reasons about the strengths and weaknesses of each response, produces individual helpfulness scores and generates a relative ranking between the candidates. These  reward signals are then used in an RLHF stage to improve helpfulness, coherence, correctness, and overall chat experience in Nemotron 3 Nano.&lt;/p&gt;
&lt;p&gt;The combined post-training pipeline—SFT+RLVR+RLHF—produces the final Nemotron 3 Nano 30B-A3B model.&lt;/p&gt;
&lt;p&gt;As models evolve into multi-step agents that use tools, they face entirely new safety and security challenges. To support responsible deployment, we are releasing an agentic safety dataset featuring nearly 11,000 labeled traces from realistic, tool-using workflows. This gives developers the data they need to evaluate, diagnose, and mitigate safety risks before agentic systems reach production.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Why We Needed Better RL Infrastructure
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;During development, the limitations of existing RL tooling became clear. Training large reasoning models with RL is difficult because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-step rollouts are complex to orchestrate  &lt;/li&gt;
&lt;li&gt;Tool integrations are often brittle  &lt;/li&gt;
&lt;li&gt;Orchestration logic can conflict with training loop design  &lt;/li&gt;
&lt;li&gt;Collecting rollout data at scale is slow and difficult  &lt;/li&gt;
&lt;li&gt;Most high-quality RL environments are closed and proprietary&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, meaningful RL training has historically been accessible only to major AI labs.&lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		NeMo Gym: Opening RL to Everyone
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;To overcome these challenges NVIDIA built NeMo Gym, an open-source standardized library for building and scaling RL environments.&lt;br /&gt;NeMo Gym powers reinforcement learning pipelines used in Nemotron 3 Nano, and now gives developers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ready-to-use RL environments across math, code, tool use, multi-turn reasoning, and agentic workflows  &lt;/li&gt;
&lt;li&gt;The ability to build custom RL environments with verifiable reward logic  &lt;/li&gt;
&lt;li&gt;Ecosystem interoperability with NeMo RL and other training frameworks (TRL, Unsloth, VeRL underway)  &lt;/li&gt;
&lt;li&gt;High-throughput rollout orchestration, enabling large-scale RL training  &lt;/li&gt;
&lt;li&gt;A practical pathway to perform RL on their own models&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NeMo Gym is a flexible open source library for building and running RL training environments. It is part of the broader NVIDIA NeMo software suite for end-to-end model training and provides infrastructure for designing, running, and scaling complex RL environments. &lt;/p&gt;
&lt;p&gt;Battle tested through the development of the entire Nemotron 3 model family, NeMo Gym includes the core environment development infrastructure, a growing collection of ready-to-use training environments alongside the datasets used in RLVR, and integration with NeMo RL, the high-performance and efficient RL training engine with support for advanced RL training algorithms, end-to-end FP8 training and async RL.&lt;/p&gt;
&lt;p&gt;With NeMo Gym, teams can quickly assemble environments using modular server components and templates, integrate external tools, systems, or databases, and orchestrate long-context, multi-step, multi-turn rollouts. This allows training environments to be iterated on and shared independent of the training loop.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram illustrating interaction between an RL Training Framework on the left and NeMo Gym on the right. The training framework sends task prompts  to the agent server in the NeMo Gym. The agent server coordinates with the policy model server and external resources server to collect rollouts and verify task performance. The scored trajectories are returned back to the Training Framework for model updates." src="https://cdn-uploads.huggingface.co/production/uploads/65df9200dc3292a8983e5017/go-hdGzZpaKRJChrl-o6k.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: How NeMo Gym fits into the RL training loop: The RL training framework (e.g., NeMo RL) sends task prompts to NeMo Gym, which operates as a set of independent HTTP services. Inside NeMo Gym, the agent server orchestrates rollouts by coordinating the policy model server (generation) and external resources server (tools and rewards). NeMo Gym returns model trajectories and rewards to the training framework, which then  updates and refits the policy model.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By decoupling RL environments from RL training frameworks, NeMo Gym works seamlessly with many popular training frameworks (such as NeMo RL), supports high-throughput, concurrent rollout collection, and enables large-scale distributed RL training. This separation of concerns makes it easy to scale RL workflows and adapt environments as training objectives evolve.&lt;/p&gt;
&lt;p&gt;To accelerate experimentation, NeMo Gym ships with an expanding RL Hub—a catalog of ready-to-use domain-specific environments that developers can use immediately or extend. Current domains include math, coding, instruction following, multi-step tool use, multi-turn structured conversations. Practitioners can fine-tune models on these environments out of the box, reuse community contributions, or publish their own.&lt;/p&gt;

&lt;p&gt;Nemotron 3 Nano (30B A3B) delivers state-of-the-art accuracy in an exceptionally cost-efficient package. It offers up to 3.3x higher throughput than leading open-source models of similar size (see Figure 1), while supporting a 1M-token context window —performing well on long-context reasoning benchmarks.&lt;/p&gt;
&lt;p&gt;Built for high-volume, real-time execution, Nemotron 3 Nano excels in math and coding, multi-step tool calling, and multi-turn agentic workflows. It also retains the classic Nemotron Thinking ON/OFF modes and Thinking Budget controls, giving developers the ability to tune exactly how much the model thinks for each task.&lt;/p&gt;
&lt;p&gt;With this release, we are also introducing NeMo Gym, containing ready-to-use training environments we developed during the course of Nemotron 3 training, and the infrastructure to build your own training environments and scale rollout collection. &lt;/p&gt;
&lt;p&gt;We are releasing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full model weights  &lt;/li&gt;
&lt;li&gt;The complete training recipe, including SFT, RLVR, and RLHF  &lt;/li&gt;
&lt;li&gt;Most of the datasets (pre-training, post-training) used throughout the training pipeline  &lt;/li&gt;
&lt;li&gt;Training frameworks that power Nemotron 3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Everything you need to study, reproduce, or extend the model is available openly.&lt;/p&gt;
&lt;p&gt;Get started with Nemotron 3 Nano:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Download the model:&lt;/strong&gt; Now available on Hugging Face.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Try hosted endpoints:&lt;/strong&gt; Run queries instantly on OpenRouter or build.nvidia.com.  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deploy at scale:&lt;/strong&gt; Use our cookbooks for vLLM, TRT-LLM, and SGLang  &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experiment, develop and run at the edge:&lt;/strong&gt; Available on edge devices such as NVIDIA RTX AI PCs and Workstations and DGX Spark via Llama.cpp, LM Studio and Unsloth&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a deep dive into the architecture, datasets, and benchmarks, read the full Nemotron 3 Nano Technical Report.&lt;/p&gt;
&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/nemotron-3-nano-efficient-open-intelligent-models</guid><pubDate>Mon, 15 Dec 2025 14:08:17 +0000</pubDate></item><item><title>[NEW] Nvidia reportedly weighs ramping up H200 production to meet surging demand in China (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/nvidia-is-reportedly-weighs-ramping-up-h200-production-to-meet-surging-demand-in-china/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After successfully lobbying the Trump administration to approve the sale of its H200 chips to China, Nvidia is now thinking of ramping up production of the chips as Chinese companies rush to place orders, Reuters reported, citing anonymous sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most powerful of Nvidia’s previous Hopper generation of graphics processing units (GPUs) made for training large language models, the H200 chips previously could not be sold in China, as the previous Biden administration had proposed rules limiting sales of advanced AI chips in the country. But the Department of Commerce last week gave Nvidia the nod to sell H200 GPUs in China, in exchange for a 25% cut of sales of those chips. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia is now seeing such strong demand from Chinese companies that it is considering adding more capacity, Reuters reported. However, Chinese officials are still deciding whether to allow the import of the H200 chips, which are said to be significantly more powerful than the H20 GPUs Nvidia had customized to sell in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the chipmaker, expanding production of the H200 GPUs would let it tap latent demand in a country that is racing to develop its own homegrown AI chips. Competition and national security concerns in the West have hampered the availability of the latest and most powerful hardware for training AI models in China, where companies have resorted to focusing on efficiency over sheer scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chinese companies, including Alibaba and ByteDance, which are developing their own AI models, have already been in touch with Nvidia to figure out large orders for the H200 chips, which are being produced in limited quantities, the report added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are managing our supply chain to ensure that licensed sales of the H200 to authorized customers in China will&amp;nbsp;have no impact on our ability to supply customers in the United States,” an Nvidia spokesperson said in an emailed statement.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/03/GettyImages-2205210966.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After successfully lobbying the Trump administration to approve the sale of its H200 chips to China, Nvidia is now thinking of ramping up production of the chips as Chinese companies rush to place orders, Reuters reported, citing anonymous sources.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The most powerful of Nvidia’s previous Hopper generation of graphics processing units (GPUs) made for training large language models, the H200 chips previously could not be sold in China, as the previous Biden administration had proposed rules limiting sales of advanced AI chips in the country. But the Department of Commerce last week gave Nvidia the nod to sell H200 GPUs in China, in exchange for a 25% cut of sales of those chips. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nvidia is now seeing such strong demand from Chinese companies that it is considering adding more capacity, Reuters reported. However, Chinese officials are still deciding whether to allow the import of the H200 chips, which are said to be significantly more powerful than the H20 GPUs Nvidia had customized to sell in China.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the chipmaker, expanding production of the H200 GPUs would let it tap latent demand in a country that is racing to develop its own homegrown AI chips. Competition and national security concerns in the West have hampered the availability of the latest and most powerful hardware for training AI models in China, where companies have resorted to focusing on efficiency over sheer scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Chinese companies, including Alibaba and ByteDance, which are developing their own AI models, have already been in touch with Nvidia to figure out large orders for the H200 chips, which are being produced in limited quantities, the report added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are managing our supply chain to ensure that licensed sales of the H200 to authorized customers in China will&amp;nbsp;have no impact on our ability to supply customers in the United States,” an Nvidia spokesperson said in an emailed statement.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/nvidia-is-reportedly-weighs-ramping-up-h200-production-to-meet-surging-demand-in-china/</guid><pubDate>Mon, 15 Dec 2025 14:28:20 +0000</pubDate></item><item><title>[NEW] First Voyage raises $2.5M for its AI companion that helps you build habits (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/15/first-voyage-raises-2-5m-for-its-ai-companion-helps-you-build-habits/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a world that’s rapidly filling up with AI-generated content, a startup called First Voyage wants to help people avoid all the AI slop blasted their way and instead build the habits they want. And it’s doing that by way of an AI companion app: Called Momo Self Care, the app offers a digital pet called Momo that you can take care of, and in return, it’ll remind you to complete habit-building tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can set up reminders for what tasks they want to complete, and Momo will remind you of them. Similar to the hit productivity app Focus Friend, Momo also rewards you with coins for completing tasks that can be used to&amp;nbsp;purchase&amp;nbsp;items within the app to further customize the pet. Users can also talk to Momo about self-care, and the AI companion will recommend habits and tasks based on what you want to achieve.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Momo helps users become the best versions of themselves, and users reward Momo with care, affection, and cute accessories,” co-founder and CEO Besart Çopa told TechCrunch. He launched the company with Egehan Ozsoy, who serves as CTO. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, First Voyage said it had raised $2.5 million in a seed funding round from a16z speedrun, SignalFire, True Global, and other investors.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075984" height="308" src="https://techcrunch.com/wp-content/uploads/2025/12/Momo-founders.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Copa said Momo users have already created more than 2 million tasks on the platform, and the most popular habits relate to productivity, spirituality, and mindfulness.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But with the wave of AI apps and toys hitting the market, not to mention the burgeoning influence of AI chatbots like ChatGPT, Claude, and Grok, there’s increasing concern that these new, so-called “companions” can lead to more harm than good.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Çopa, for one,&amp;nbsp;believes that relationships between AI characters and humans will only increase in the next few years. However, he noted that the increasing number of AI apps aimed at wellness and self-care is at least better than those that target base urges. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We are happy so many founders [and] startups are working in the AI self-care wellness space instead of building waifus,” he said, adding that the “personalization capability of AI will take the impact of these relationships to another level.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075853" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/Product-Image-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage / Momo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He noted that Momo has baked in safety guardrails, such as prompt filters to make sure that conversations between the AI and users stay within appropriate boundaries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh cash from the fundraise will be used to help launch Momo on the Android app store (it’s&amp;nbsp;already available on iOS). The First Voyage team also hopes to make Momo more intelligent in how it interacts with people.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We hope Momo and the community around it become a defining consumer brand that uses the best of AI, animation, and&amp;nbsp;gamification&amp;nbsp;to improve as many lives as possible,” Çopa&amp;nbsp;said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;In a world that’s rapidly filling up with AI-generated content, a startup called First Voyage wants to help people avoid all the AI slop blasted their way and instead build the habits they want. And it’s doing that by way of an AI companion app: Called Momo Self Care, the app offers a digital pet called Momo that you can take care of, and in return, it’ll remind you to complete habit-building tasks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can set up reminders for what tasks they want to complete, and Momo will remind you of them. Similar to the hit productivity app Focus Friend, Momo also rewards you with coins for completing tasks that can be used to&amp;nbsp;purchase&amp;nbsp;items within the app to further customize the pet. Users can also talk to Momo about self-care, and the AI companion will recommend habits and tasks based on what you want to achieve.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Momo helps users become the best versions of themselves, and users reward Momo with care, affection, and cute accessories,” co-founder and CEO Besart Çopa told TechCrunch. He launched the company with Egehan Ozsoy, who serves as CTO. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Monday, First Voyage said it had raised $2.5 million in a seed funding round from a16z speedrun, SignalFire, True Global, and other investors.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075984" height="308" src="https://techcrunch.com/wp-content/uploads/2025/12/Momo-founders.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Copa said Momo users have already created more than 2 million tasks on the platform, and the most popular habits relate to productivity, spirituality, and mindfulness.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But with the wave of AI apps and toys hitting the market, not to mention the burgeoning influence of AI chatbots like ChatGPT, Claude, and Grok, there’s increasing concern that these new, so-called “companions” can lead to more harm than good.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Çopa, for one,&amp;nbsp;believes that relationships between AI characters and humans will only increase in the next few years. However, he noted that the increasing number of AI apps aimed at wellness and self-care is at least better than those that target base urges. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“We are happy so many founders [and] startups are working in the AI self-care wellness space instead of building waifus,” he said, adding that the “personalization capability of AI will take the impact of these relationships to another level.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3075853" height="383" src="https://techcrunch.com/wp-content/uploads/2025/12/Product-Image-3.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;First Voyage / Momo&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;He noted that Momo has baked in safety guardrails, such as prompt filters to make sure that conversations between the AI and users stay within appropriate boundaries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh cash from the fundraise will be used to help launch Momo on the Android app store (it’s&amp;nbsp;already available on iOS). The First Voyage team also hopes to make Momo more intelligent in how it interacts with people.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We hope Momo and the community around it become a defining consumer brand that uses the best of AI, animation, and&amp;nbsp;gamification&amp;nbsp;to improve as many lives as possible,” Çopa&amp;nbsp;said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/15/first-voyage-raises-2-5m-for-its-ai-companion-helps-you-build-habits/</guid><pubDate>Mon, 15 Dec 2025 14:48:24 +0000</pubDate></item><item><title>[NEW] The fast and the future-focused are revolutionizing motorsport (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/15/1127432/the-fast-and-the-future-focused-are-revolutionizing-motorsport/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Rohit-Agnihotri-Dan-Cherowbrier-v2.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Infosys&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When the ABB FIA Formula E World Championship launched its first race through Beijing’s Olympic Park in 2014, the idea of all-electric motorsport still bordered on experimental. Batteries couldn’t yet last a full race, and drivers had to switch cars mid-competition. Just over a decade later, Formula E has evolved into a global entertainment brand broadcast in 150 countries, driving both technological innovation and cultural change in sport.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Gen4, that's to come next year," says Dan Cherowbrier, Formula E’s chief technology and information officer. "You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster—it's actually more than traditional [internal combustion engines] ICE."&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That acceleration isn’t just happening on the track. Formula E’s digital transformation, powered by its partnership with Infosys, is redefining what it means to be a fan. “It's a movement to make motor sport accessible and exciting for the new generation,” says principal technologist at Infosys, Rohit Agnihotri.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From real-time leaderboards and predictive tools to personalized storylines that adapt to what individual fans care most about—whether it's a driver rivalry or battery performance—Formula E and Infosys are using AI-powered platforms to create fan experiences as dynamic as the races themselves. "Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive," says Agnihotri.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;AI is also transforming how the organization itself operates. "Historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud," Cherowbrier notes. "What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As audiences diversify and expectations evolve, Formula E is also a case study in sustainable innovation. Machine learning tools now help determine the most carbon-optimal way to ship batteries across continents, while remote broadcast production has sharply reduced travel emissions and democratized the company's workforce. These advances show how digital intelligence can expand reach without deepening carbon footprints.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For Cherowbrier, this convergence of sport, sustainability, and technology is just the beginning. With its data-driven approach to performance, experience, and impact, Formula E is offering a glimpse into how entertainment, innovation, and environmental responsibility can move forward in tandem.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Our goal is clear," says Agnihotri. "Help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Infosys.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript:&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab, and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The ABB FIA Formula E World Championship, the world's first all-electric racing series, made its debut in the grounds of the Olympic Park in Beijing in 2014. A little more than 10 years later, it's a global entertainment brand with 10 teams, 20 drivers, and broadcasts in 150 countries. Technology is central to how Formula E is navigating that scale and to how it's delivering more powerful personalized experiences.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Two words for you: elevated fandom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CTIO of Formula E.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;This episode is produced in partnership with Infosys.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome, Rohit and Dan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan Cherowbrier&lt;/em&gt;: Hi. Thanks for having us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Dan, as I mentioned there, the first season of the ABB FIA Formula E World Championship launched in 2014. Can you talk us through how the first all-electric motor sport has evolved in the last decade? How has it changed in terms of its scale, the markets it operates in, and also, its audiences, of course?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: When Formula E launched back in 2014, there were hardly any domestic EVs on the road. And probably if you're from London, the ones you remember are the hybrid Priuses; that was what we knew of really. And at the time, they were unable to get a battery big enough for a car to do a full race. So the first generation of car, the first couple of seasons, the driver had to do a pit stop midway through the race, get out of one car, and get in another car, and then carry on, which sounds almost farcical now, but it's what you had to do then to drive innovation, is to do that in order to go to the next stage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then in Gen2, that came up four years later, they had a battery big enough to start full races and start to actually make it a really good sport. Gen3, they're going for some real speeds and making it happen. Gen4, that's to come next year, you'll see acceleration in line with Formula One. I've been fortunate enough to see some of the testing. You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster, it's actually more than traditional ICE.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's the tech of the car. But then, if you also look at the sport and how people have come to it and the fans and the demographic of the fans, a lot has changed in the last 11 years. We were out to enter season 12. In the last 11 years, we've had a complete democratization of how people access content and what people want from content. And as a new generation of fan coming through. This new generation of fan is younger. They're more gender diverse. We have much closer to 50-50 representation in our fan base. And they want things personalized, and they're very demanding about how they want it and the experience they expect. No longer are you just able to give them one race and everybody watches the same thing. We need to make things for them. You see that sort of change that's come through in the last 11 years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It's a huge amount of change in just over a decade, isn't it? To navigate. And I wonder, Rohit, what was the strategic plan for Infosys when associating with Formula E? What did Infosys see in partnering with such a young sport?&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. That's a great question, Megan. When we looked at Formula E, we didn't just see a racing championship. We saw the future. A sport, that's electric, sustainable, and digital first. That's exactly where Infosys wants to be, at the intersection of technology, innovation, and purpose. Our plan has three big goals. First, grow the fan base. Formula E wants to reach 500 million fans by 2030. That is not just a number. It's a movement to make motor sport accessible and exciting for the new generation. To make that happen, we are building an AI-powered platform that gives personalized content to the fans, so that every fan feels connected and valued. Imagine a fan in Tokyo getting race insights tailored for their favorite driver, while another in London gets a sustainability story that matters to him. That's the level of personalization we are aiming for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Second, bringing technology innovation. We have already launched the Stats Centre, which turns race data into interactive stories. And soon, Race Centre will take this to the next level with real time leaderboards to the race or tracks, overtakes, attack mode timelines, and even AI generated live commentary. Fans will not just watch, they will interact, predict podium finishes, and share their views globally. And third, supports sustainability. Formula E is already net-zero, but now their goal is to cut carbon by 45% by 2030. We'll be enabling that through AI-driven sustainability, data management, tracking every watt of energy, every logistics decision. and modeling scenarios to make racing even greener. Partnering with a young sport gives us a chance to shape its digital future and show how technology can make racing exciting and responsible. For us, Formula E is not just a sport, it's a statement about where the world is headed.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. 500 million fans, that's a huge number, isn't it? And with more scale often comes a kind of greater expectation. Dan, I know you touched on this a little in your first question, but what is it that your fans now really want from their interactions? Can you talk a bit more about what experiences they're looking for? And also, how complex that really is to deliver that as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I think a really telling thing about the modern day fan is I probably can't tell you what they want from their experiences, because it's individual and it's unique for each of them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And it's changing and it's changing so fast. What somebody wants this month is going to be different from what they want in a couple of months' time. And we're having to learn to adapt to that. My CTO title, we often put focus on the technology in the middle of it. That's what the T is. Actually, if you think about it, it’s continual transformation officer. You are constantly trying to change what you deliver and how you deliver it. Because if fans come through, they find new experiences, they find that in other sports. Sometimes not in sports, they find it outside, and then they're coming in, and they expect that from you. So how can we make them more part of the sport, more personalized experience, get to know the athletes and the personalities and the characters within it? We're a very technology centric sport. A lot of motor sport is, but really, people want to see people, right? And even when it's technology, they want to see people interacting with technology, and it's how do you get that out to show people.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Yeah, it's no mean feat. Rohit, you've worked with brands on delivering these sort of fan experiences across different sports. Is motor sports perhaps more complicated than others, given that fans watch racing for different reasons than just a win? They could be focused on team dynamics, a particular driver, the way the engine is built, and so on and so forth. How does motor sports compare and how important is it therefore, that Formula E has embraced technology to manage expectations?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah, that's an interesting point. Motor sports are definitely more complex than other sports. Fans don't just care about who wins, they care about how some follow team strategies, others love driver rivalries, and many are fascinated by the car technology. Formula E adds another layer, sustainability and electric innovation. This makes personalization really important. Fans want more than results. They want stories and insights. Formula E understood this early and embraced technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Think about the data behind a single race, lap times, energy usage, battery performance, attack mode activation, pit strategies, it's a lot of data. If you just show the raw numbers, it's overwhelming. But with Infosys Topaz, we turn that into simple and engaging stories. Fans can see how a driver fought back from 10th place to finish on the podium, or how a team managed energy better to gain an edge. And for new fans, we are adding explainer videos and interactive tools in the Race Center, so that they can learn about their sport easily. This is important because Formula E is still young, and many fans are discovering it for the first time. Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: There's an awful lot going on there. What are some of the other ways that Formula E has already put generative AI and other emerging technologies to use? Dan, when we've spoken about the demand for more personalized experiences, for example.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I see the implementation of AI for us in three areas. We have AI within the sport. That's in our DNA of the sport. Now, each team is using that, but how can we use that as a championship as well? How do we make it a competitive landscape? Now, we have AI that is in the fan-facing product. That's what we're working heavily on Infosys with, but we also have it in our broadcast product. As an example, you might have heard of a super slow-mo camera. A super slow-mo camera is basically, by taking three cameras and having them in exactly the same place so that you get three times the frame rate, and then you can do a slow-motion shot from that. And they used to be really expensive. Quite bulky cameras to put in. We are now using AI to take a traditional camera and interpolate between two frames to make it into a super slow image, and you wouldn't really know the difference. Now, the joy of that, it means every camera can now be a super slow-mo camera.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Wow.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: In other ways, we use it a little bit in our graphics products, and we iterate and we use it for things like showing driver audio. When the driver is speaking to his engineer or her engineer in the garage, we show that text now on screen. We do that using AI. We use AI to pick out the difference between the driver and another driver and the team engineer or the team principal and show that in a really good way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And we wouldn't be able to do that. We're not big enough to have a team of 24 people on stenographers typing. We have to use AI to be able to do that. That's what's really helped us grow. And then the last one is, how we use it in our business. Because ultimately, as we've got the fans, we've got the sport, but we also are running a business and we have to pick up these racetracks and move them around the world, and we have all these staff who have to get places. We have insurance who has to do all that kind of stuff, and we use it heavily in that area, particularly when it comes to what has a carbon impact for us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So things like our freight and our travel. And we are using the AI tools to tell us, a battery for instance, should we fly it? Should we send it by sea freight? Should we send it by row freight? Or should we just have lots of them? And that sort of depends. Now, a battery, if it was heavy, you'd think you probably wouldn't fly it. But actually, because of the materials in it, because of the source materials that make it, we're better off flying it. We've used AI to work through all those different machinations of things that would be too difficult to do at speed for a person.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Well, sounds like there's some fascinating things going on. I mean, of course, for a global brand, there is also the challenge of working in different markets. You mentioned moving everything around the world there. Each market with its own legal frameworks around data privacy, AI. How has technology also helped you navigate all of that, Dan?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The other really interesting thing about AI is... I've worked in technology leadership roles for some time now. And historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud and things like that. What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool. And we're trying to accommodate all of that and it's a great pleasure to see people that are so keen. AI is driving the tech adoption in general, which really helps the business.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Megan: Dan, as the world's first all-electric motor sport series, sustainability is obviously a real cornerstone of what Formula E is looking to do. Can you share with us how technology is helping you to achieve some of your ambitions when it comes to sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We've been the only sport with a certified net-zero pathway, and we have to stay that part. It's a really core fundamental part of our DNA. I sit on our management team here. There is a sustainability VP that sits there as well, who checks and challenges everything we do. She looks at the data centers we use, why we use them, why we've made the decisions we've made, to make sure that we're making them all for the right reasons and the right ways. We specifically embed technology in a couple of ways. One is, we mentioned a little bit earlier, on our freight. Formula E's freight for the whole championship is probably akin to one Formula One team, but it's still by far, our biggest contributor to our impact. So we look about how we can make sure that we've refined that to get the minimum amount of air freight and sea freight, and use local wherever we can. That's also part of our pledge about investing in the communities that we race in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The second then is about our staff travel. And we've done a really big piece of work over the last four to five years, partly accelerated through the covid-19 era actually, of doing remote working and remote TV production. Used to be traditionally, you would fly a hundred plus people out to racetracks, and then they would make the television all on site in trucks, and then they would be satellite distributed out of the venue. Now, what we do is we put in some internet connections, dual and diverse internet connections, and we stream every single camera back.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: That means on site, we only need camera operators. Some of them actually, are remotely operated anyway, but we need camera operators, and then some engineering teams to just keep everything running. And then back in our home base, which is in London, in the UK, we have our remote production center where we layer on direction, graphics, audio, replay, team radio, all of those bits that break the color and make the program and add to that significant body of people. We do that all remotely now. Really interesting actually, a bit. So that's the carbon sustainability story, but there is a further ESG piece that comes out of it and we haven't really accommodated when we went into it, is the diversity in our workforce by doing that. We were discovering that we had quite a young, equally diverse workforce until around the age of 30. And then once that happened, then we were finding we were losing women, and that's really because they didn't want to travel.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And that's the age of people starting to have children, and things were starting to change. And then we had some men that were traveling instead, and they weren't seeing their children and it was sort of dividing it unnecessarily. But by going remote, by having so much of our people able to remotely... Or even if they do have to travel, they're not traveling every single week. They're now doing that one in three. They're able to maintain the careers and the jobs they want to do, whilst having a family lifestyle. And it also just makes a better product by having people in that environment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: That's such an interesting perspective, isn't it? It's a way of environmental sustainability intersects with social sustainability. And Rohit, and your work are so interesting. And Rohit, can you share any of the ways that Infosys has worked with Formula E, in terms of the role of technology as we say, in furthering those ambitions around sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys understands that sustainability is at the heart of Formula E, and it's a big part of why this partnership matters. Formula E is already net-zero certified, but now, they have an ambitious goal to cut carbon emissions by 45%. Infosys is helping in two ways. First, we have built AI-powered sustainability data tools that make carbon reporting accurate and traceable. Every watt of energy, every logistic decision, every material use can be tracked. Second, we use predictive analytics to model scenarios, like how changing race logistics or battery technology impact emissions so Formula E can make smarter, greener decisions. For us, it's about turning sustainability from a report into an action plan, and making Formula E a global leader in green motor sport.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: And in April 2025, Formula E working with Infosys launched its Stats Centre, which provides fans with interactive access to the performances of their drivers and teams, key milestones and narratives. I know you touched on this before, but I wonder if you could tell us a bit more about the design of that platform, Rohit, and how it fits into Formula E's wider plans to personalize that fan experience?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Sure. The Stats Centre was a big step forward. Before this, fans had access to basic statistics on the website and the mobile app, but nothing told the full story and we wanted to change that. Built on Infosys Topaz, the Stats Centre uses AI to turn race data into interactive stories. Fans can explore key stat cards that adapt to race timelines, and even chat with an AI companion to get instant answers. It's like having a person race analyst at your fingertips. And we are going further. Next year, we'll launch Race Centre. It'll have live data boards, 2D track maps showing every driver's position, overtakes and more attack timelines, and AI-generated commentary. Fans can predict podium finishes, vote for the driver of the race, and share their views on social media. Plus, we are adding video explainers for new fans, covering rules, strategies, and car technology. Our goal is simple: make every moment exciting and easy to understand. Whether you are a hardcore fan or someone watching Formula E for the first time, you'll feel connected and informed.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Sounds brilliant. And as you've explained, Dan, leveraging data and AI can come with these huge benefits when it comes to the depth of fan experience that you can deliver, but it can also expose you to some challenges. How are you navigating those at Formula E?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The AI generation has presented two significant challenges to us. One is that traditional SEO, traditional search engine optimization, goes out the window. Right? You are now looking at how do we design and build our systems and how do we populate them with the right content and the right data, so that the engines are picking it up correctly and displaying it? The way that the foundational models are built and the speed and the cadence of which they're updated, means quite often... We're a very fast-changing organization. We're a fast-changing product. Often, the models don't keep up. And that's because they are a point in time when they were trained. And that's something that the big organizations, the big tech organizations will fix with time. But for now, what we have to do is we have to learn about how we can present our fan-facing, web-facing products to show that correctly. That's all about having really accurate first-party content, effectively earned media. That's the piece we need to do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then the second sort of challenge is sadly, whilst these tools are available to all of us, and we are using them effectively, so are another part of the technology landscape, and that is the cybersecurity basically they come with. If you look at the speed of the cadence and severity of hacks that are happening now, it's just growing and growing and growing, and that's because they have access to these tools too. And we're having to really up our game and professionalize. And that's really hard for an innovative organization. You don't want to shut everything down. You don't want to protect everything too much because you want people to be able to try new things. Right? If I block everything to only things that the IT team had heard of, we'd never get anything new in, and it's about getting that balance right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: Rohit, you probably have similar experiences?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: How has Infosys worked with Formula E to help it navigate some of that, Rohit?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys has helped Formula E tackle some of the challenges in three key ways, simplify complex race data into engaging fan experience through platforms like Stats Centre, building a secure and scalable cloud data backbone for the real-time insights, and enabling sustainability goals with AI-driven carbon tracking and predictive analytics. This solution makes the sport interactive, more digital, and more responsible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. I wondered if we could close with a bit of a future forward look. Can you share with us any innovations on the horizon at Formula E that you are really excited about, Dan?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We have mentioned the Race Centre is going to launch in the next couple of months, but the really exciting thing for me is we've got an amazing season ahead of us. It's the last season of our Gen3 car, with 10 really exciting teams on the grid. We are going at speed with our tech innovation roadmap and what our fans want. And we're building up towards our Gen4 car, which will come out for season 13 in a year's time. That will get launched in 2026, and I think it will be a game changer in how people perceive electric motor sport and electric cars in general.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It sounds like there's all sorts of exciting things going on. And Rohit too, what's coming up via this partnership that you are really looking forward to sharing with everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Two things stand out for me. First is the AI-powered fan data platform that I've already spoken about. Second is the launch of Race Centre. It's going to change how fans experience live racing. And beyond final engagement, we are helping Formula E lead in sustainability with AI tools that model carbon impact and optimize logistics. This means every race can be smarter and greener. Our goal is clear: help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Thank you so much, both. That was Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CITO of Formula E, whom I spoke with from Brighton, England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/Rohit-Agnihotri-Dan-Cherowbrier-v2.png?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;&lt;span class="sponsoredModule__name--dbd90349922f15155a4c483b397356c2"&gt;Infosys&lt;/span&gt;&lt;/p&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt;   &lt;p&gt;When the ABB FIA Formula E World Championship launched its first race through Beijing’s Olympic Park in 2014, the idea of all-electric motorsport still bordered on experimental. Batteries couldn’t yet last a full race, and drivers had to switch cars mid-competition. Just over a decade later, Formula E has evolved into a global entertainment brand broadcast in 150 countries, driving both technological innovation and cultural change in sport.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Gen4, that's to come next year," says Dan Cherowbrier, Formula E’s chief technology and information officer. "You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster—it's actually more than traditional [internal combustion engines] ICE."&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;That acceleration isn’t just happening on the track. Formula E’s digital transformation, powered by its partnership with Infosys, is redefining what it means to be a fan. “It's a movement to make motor sport accessible and exciting for the new generation,” says principal technologist at Infosys, Rohit Agnihotri.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;From real-time leaderboards and predictive tools to personalized storylines that adapt to what individual fans care most about—whether it's a driver rivalry or battery performance—Formula E and Infosys are using AI-powered platforms to create fan experiences as dynamic as the races themselves. "Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive," says Agnihotri.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;AI is also transforming how the organization itself operates. "Historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud," Cherowbrier notes. "What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;As audiences diversify and expectations evolve, Formula E is also a case study in sustainable innovation. Machine learning tools now help determine the most carbon-optimal way to ship batteries across continents, while remote broadcast production has sharply reduced travel emissions and democratized the company's workforce. These advances show how digital intelligence can expand reach without deepening carbon footprints.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For Cherowbrier, this convergence of sport, sustainability, and technology is just the beginning. With its data-driven approach to performance, experience, and impact, Formula E is offering a glimpse into how entertainment, innovation, and environmental responsibility can move forward in tandem.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;"Our goal is clear," says Agnihotri. "Help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever."&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This episode of Business Lab is produced in partnership with Infosys.&lt;/em&gt;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Full Transcript:&amp;nbsp;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;em&gt;Megan Tatum:&lt;/em&gt; From MIT Technology Review, I'm Megan Tatum, and this is Business Lab, the show that helps business leaders make sense of new technologies coming out of the lab, and into the marketplace.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The ABB FIA Formula E World Championship, the world's first all-electric racing series, made its debut in the grounds of the Olympic Park in Beijing in 2014. A little more than 10 years later, it's a global entertainment brand with 10 teams, 20 drivers, and broadcasts in 150 countries. Technology is central to how Formula E is navigating that scale and to how it's delivering more powerful personalized experiences.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Two words for you: elevated fandom.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;My guests today are Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CTIO of Formula E.&amp;nbsp;&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;This episode is produced in partnership with Infosys.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome, Rohit and Dan.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan Cherowbrier&lt;/em&gt;: Hi. Thanks for having us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Dan, as I mentioned there, the first season of the ABB FIA Formula E World Championship launched in 2014. Can you talk us through how the first all-electric motor sport has evolved in the last decade? How has it changed in terms of its scale, the markets it operates in, and also, its audiences, of course?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: When Formula E launched back in 2014, there were hardly any domestic EVs on the road. And probably if you're from London, the ones you remember are the hybrid Priuses; that was what we knew of really. And at the time, they were unable to get a battery big enough for a car to do a full race. So the first generation of car, the first couple of seasons, the driver had to do a pit stop midway through the race, get out of one car, and get in another car, and then carry on, which sounds almost farcical now, but it's what you had to do then to drive innovation, is to do that in order to go to the next stage.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then in Gen2, that came up four years later, they had a battery big enough to start full races and start to actually make it a really good sport. Gen3, they're going for some real speeds and making it happen. Gen4, that's to come next year, you'll see acceleration in line with Formula One. I've been fortunate enough to see some of the testing. You will see a really quite impressive car that starts us to question whether EV is there. It's actually faster, it's actually more than traditional ICE.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's the tech of the car. But then, if you also look at the sport and how people have come to it and the fans and the demographic of the fans, a lot has changed in the last 11 years. We were out to enter season 12. In the last 11 years, we've had a complete democratization of how people access content and what people want from content. And as a new generation of fan coming through. This new generation of fan is younger. They're more gender diverse. We have much closer to 50-50 representation in our fan base. And they want things personalized, and they're very demanding about how they want it and the experience they expect. No longer are you just able to give them one race and everybody watches the same thing. We need to make things for them. You see that sort of change that's come through in the last 11 years.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It's a huge amount of change in just over a decade, isn't it? To navigate. And I wonder, Rohit, what was the strategic plan for Infosys when associating with Formula E? What did Infosys see in partnering with such a young sport?&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. That's a great question, Megan. When we looked at Formula E, we didn't just see a racing championship. We saw the future. A sport, that's electric, sustainable, and digital first. That's exactly where Infosys wants to be, at the intersection of technology, innovation, and purpose. Our plan has three big goals. First, grow the fan base. Formula E wants to reach 500 million fans by 2030. That is not just a number. It's a movement to make motor sport accessible and exciting for the new generation. To make that happen, we are building an AI-powered platform that gives personalized content to the fans, so that every fan feels connected and valued. Imagine a fan in Tokyo getting race insights tailored for their favorite driver, while another in London gets a sustainability story that matters to him. That's the level of personalization we are aiming for.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Second, bringing technology innovation. We have already launched the Stats Centre, which turns race data into interactive stories. And soon, Race Centre will take this to the next level with real time leaderboards to the race or tracks, overtakes, attack mode timelines, and even AI generated live commentary. Fans will not just watch, they will interact, predict podium finishes, and share their views globally. And third, supports sustainability. Formula E is already net-zero, but now their goal is to cut carbon by 45% by 2030. We'll be enabling that through AI-driven sustainability, data management, tracking every watt of energy, every logistics decision. and modeling scenarios to make racing even greener. Partnering with a young sport gives us a chance to shape its digital future and show how technology can make racing exciting and responsible. For us, Formula E is not just a sport, it's a statement about where the world is headed.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. 500 million fans, that's a huge number, isn't it? And with more scale often comes a kind of greater expectation. Dan, I know you touched on this a little in your first question, but what is it that your fans now really want from their interactions? Can you talk a bit more about what experiences they're looking for? And also, how complex that really is to deliver that as well?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I think a really telling thing about the modern day fan is I probably can't tell you what they want from their experiences, because it's individual and it's unique for each of them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Of course.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And it's changing and it's changing so fast. What somebody wants this month is going to be different from what they want in a couple of months' time. And we're having to learn to adapt to that. My CTO title, we often put focus on the technology in the middle of it. That's what the T is. Actually, if you think about it, it’s continual transformation officer. You are constantly trying to change what you deliver and how you deliver it. Because if fans come through, they find new experiences, they find that in other sports. Sometimes not in sports, they find it outside, and then they're coming in, and they expect that from you. So how can we make them more part of the sport, more personalized experience, get to know the athletes and the personalities and the characters within it? We're a very technology centric sport. A lot of motor sport is, but really, people want to see people, right? And even when it's technology, they want to see people interacting with technology, and it's how do you get that out to show people.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Yeah, it's no mean feat. Rohit, you've worked with brands on delivering these sort of fan experiences across different sports. Is motor sports perhaps more complicated than others, given that fans watch racing for different reasons than just a win? They could be focused on team dynamics, a particular driver, the way the engine is built, and so on and so forth. How does motor sports compare and how important is it therefore, that Formula E has embraced technology to manage expectations?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah, that's an interesting point. Motor sports are definitely more complex than other sports. Fans don't just care about who wins, they care about how some follow team strategies, others love driver rivalries, and many are fascinated by the car technology. Formula E adds another layer, sustainability and electric innovation. This makes personalization really important. Fans want more than results. They want stories and insights. Formula E understood this early and embraced technology.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Think about the data behind a single race, lap times, energy usage, battery performance, attack mode activation, pit strategies, it's a lot of data. If you just show the raw numbers, it's overwhelming. But with Infosys Topaz, we turn that into simple and engaging stories. Fans can see how a driver fought back from 10th place to finish on the podium, or how a team managed energy better to gain an edge. And for new fans, we are adding explainer videos and interactive tools in the Race Center, so that they can learn about their sport easily. This is important because Formula E is still young, and many fans are discovering it for the first time. Technology is not just about meeting expectations; it's elevating the entire fan experience and making the sport more inclusive.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: There's an awful lot going on there. What are some of the other ways that Formula E has already put generative AI and other emerging technologies to use? Dan, when we've spoken about the demand for more personalized experiences, for example.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: I see the implementation of AI for us in three areas. We have AI within the sport. That's in our DNA of the sport. Now, each team is using that, but how can we use that as a championship as well? How do we make it a competitive landscape? Now, we have AI that is in the fan-facing product. That's what we're working heavily on Infosys with, but we also have it in our broadcast product. As an example, you might have heard of a super slow-mo camera. A super slow-mo camera is basically, by taking three cameras and having them in exactly the same place so that you get three times the frame rate, and then you can do a slow-motion shot from that. And they used to be really expensive. Quite bulky cameras to put in. We are now using AI to take a traditional camera and interpolate between two frames to make it into a super slow image, and you wouldn't really know the difference. Now, the joy of that, it means every camera can now be a super slow-mo camera.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Wow.&amp;nbsp;&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: In other ways, we use it a little bit in our graphics products, and we iterate and we use it for things like showing driver audio. When the driver is speaking to his engineer or her engineer in the garage, we show that text now on screen. We do that using AI. We use AI to pick out the difference between the driver and another driver and the team engineer or the team principal and show that in a really good way.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;And we wouldn't be able to do that. We're not big enough to have a team of 24 people on stenographers typing. We have to use AI to be able to do that. That's what's really helped us grow. And then the last one is, how we use it in our business. Because ultimately, as we've got the fans, we've got the sport, but we also are running a business and we have to pick up these racetracks and move them around the world, and we have all these staff who have to get places. We have insurance who has to do all that kind of stuff, and we use it heavily in that area, particularly when it comes to what has a carbon impact for us.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;So things like our freight and our travel. And we are using the AI tools to tell us, a battery for instance, should we fly it? Should we send it by sea freight? Should we send it by row freight? Or should we just have lots of them? And that sort of depends. Now, a battery, if it was heavy, you'd think you probably wouldn't fly it. But actually, because of the materials in it, because of the source materials that make it, we're better off flying it. We've used AI to work through all those different machinations of things that would be too difficult to do at speed for a person.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Well, sounds like there's some fascinating things going on. I mean, of course, for a global brand, there is also the challenge of working in different markets. You mentioned moving everything around the world there. Each market with its own legal frameworks around data privacy, AI. How has technology also helped you navigate all of that, Dan?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The other really interesting thing about AI is... I've worked in technology leadership roles for some time now. And historically, we would be going around the company, banging on everyone's doors and dragging them towards technology, making them use systems, making them move things to the cloud and things like that. What AI has done is it's turned that around on its head, and we now have people turning up, banging on our door because they want to use this tool, they want to use that tool. And we're trying to accommodate all of that and it's a great pleasure to see people that are so keen. AI is driving the tech adoption in general, which really helps the business.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Megan: Dan, as the world's first all-electric motor sport series, sustainability is obviously a real cornerstone of what Formula E is looking to do. Can you share with us how technology is helping you to achieve some of your ambitions when it comes to sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We've been the only sport with a certified net-zero pathway, and we have to stay that part. It's a really core fundamental part of our DNA. I sit on our management team here. There is a sustainability VP that sits there as well, who checks and challenges everything we do. She looks at the data centers we use, why we use them, why we've made the decisions we've made, to make sure that we're making them all for the right reasons and the right ways. We specifically embed technology in a couple of ways. One is, we mentioned a little bit earlier, on our freight. Formula E's freight for the whole championship is probably akin to one Formula One team, but it's still by far, our biggest contributor to our impact. So we look about how we can make sure that we've refined that to get the minimum amount of air freight and sea freight, and use local wherever we can. That's also part of our pledge about investing in the communities that we race in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The second then is about our staff travel. And we've done a really big piece of work over the last four to five years, partly accelerated through the covid-19 era actually, of doing remote working and remote TV production. Used to be traditionally, you would fly a hundred plus people out to racetracks, and then they would make the television all on site in trucks, and then they would be satellite distributed out of the venue. Now, what we do is we put in some internet connections, dual and diverse internet connections, and we stream every single camera back.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: That means on site, we only need camera operators. Some of them actually, are remotely operated anyway, but we need camera operators, and then some engineering teams to just keep everything running. And then back in our home base, which is in London, in the UK, we have our remote production center where we layer on direction, graphics, audio, replay, team radio, all of those bits that break the color and make the program and add to that significant body of people. We do that all remotely now. Really interesting actually, a bit. So that's the carbon sustainability story, but there is a further ESG piece that comes out of it and we haven't really accommodated when we went into it, is the diversity in our workforce by doing that. We were discovering that we had quite a young, equally diverse workforce until around the age of 30. And then once that happened, then we were finding we were losing women, and that's really because they didn't want to travel.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: And that's the age of people starting to have children, and things were starting to change. And then we had some men that were traveling instead, and they weren't seeing their children and it was sort of dividing it unnecessarily. But by going remote, by having so much of our people able to remotely... Or even if they do have to travel, they're not traveling every single week. They're now doing that one in three. They're able to maintain the careers and the jobs they want to do, whilst having a family lifestyle. And it also just makes a better product by having people in that environment.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: That's such an interesting perspective, isn't it? It's a way of environmental sustainability intersects with social sustainability. And Rohit, and your work are so interesting. And Rohit, can you share any of the ways that Infosys has worked with Formula E, in terms of the role of technology as we say, in furthering those ambitions around sustainability?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys understands that sustainability is at the heart of Formula E, and it's a big part of why this partnership matters. Formula E is already net-zero certified, but now, they have an ambitious goal to cut carbon emissions by 45%. Infosys is helping in two ways. First, we have built AI-powered sustainability data tools that make carbon reporting accurate and traceable. Every watt of energy, every logistic decision, every material use can be tracked. Second, we use predictive analytics to model scenarios, like how changing race logistics or battery technology impact emissions so Formula E can make smarter, greener decisions. For us, it's about turning sustainability from a report into an action plan, and making Formula E a global leader in green motor sport.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: And in April 2025, Formula E working with Infosys launched its Stats Centre, which provides fans with interactive access to the performances of their drivers and teams, key milestones and narratives. I know you touched on this before, but I wonder if you could tell us a bit more about the design of that platform, Rohit, and how it fits into Formula E's wider plans to personalize that fan experience?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Sure. The Stats Centre was a big step forward. Before this, fans had access to basic statistics on the website and the mobile app, but nothing told the full story and we wanted to change that. Built on Infosys Topaz, the Stats Centre uses AI to turn race data into interactive stories. Fans can explore key stat cards that adapt to race timelines, and even chat with an AI companion to get instant answers. It's like having a person race analyst at your fingertips. And we are going further. Next year, we'll launch Race Centre. It'll have live data boards, 2D track maps showing every driver's position, overtakes and more attack timelines, and AI-generated commentary. Fans can predict podium finishes, vote for the driver of the race, and share their views on social media. Plus, we are adding video explainers for new fans, covering rules, strategies, and car technology. Our goal is simple: make every moment exciting and easy to understand. Whether you are a hardcore fan or someone watching Formula E for the first time, you'll feel connected and informed.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Sounds brilliant. And as you've explained, Dan, leveraging data and AI can come with these huge benefits when it comes to the depth of fan experience that you can deliver, but it can also expose you to some challenges. How are you navigating those at Formula E?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: The AI generation has presented two significant challenges to us. One is that traditional SEO, traditional search engine optimization, goes out the window. Right? You are now looking at how do we design and build our systems and how do we populate them with the right content and the right data, so that the engines are picking it up correctly and displaying it? The way that the foundational models are built and the speed and the cadence of which they're updated, means quite often... We're a very fast-changing organization. We're a fast-changing product. Often, the models don't keep up. And that's because they are a point in time when they were trained. And that's something that the big organizations, the big tech organizations will fix with time. But for now, what we have to do is we have to learn about how we can present our fan-facing, web-facing products to show that correctly. That's all about having really accurate first-party content, effectively earned media. That's the piece we need to do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Then the second sort of challenge is sadly, whilst these tools are available to all of us, and we are using them effectively, so are another part of the technology landscape, and that is the cybersecurity basically they come with. If you look at the speed of the cadence and severity of hacks that are happening now, it's just growing and growing and growing, and that's because they have access to these tools too. And we're having to really up our game and professionalize. And that's really hard for an innovative organization. You don't want to shut everything down. You don't want to protect everything too much because you want people to be able to try new things. Right? If I block everything to only things that the IT team had heard of, we'd never get anything new in, and it's about getting that balance right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Right.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: Rohit, you probably have similar experiences?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: How has Infosys worked with Formula E to help it navigate some of that, Rohit?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Yeah. Infosys has helped Formula E tackle some of the challenges in three key ways, simplify complex race data into engaging fan experience through platforms like Stats Centre, building a secure and scalable cloud data backbone for the real-time insights, and enabling sustainability goals with AI-driven carbon tracking and predictive analytics. This solution makes the sport interactive, more digital, and more responsible.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. I wondered if we could close with a bit of a future forward look. Can you share with us any innovations on the horizon at Formula E that you are really excited about, Dan?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt;&lt;p&gt;&lt;em&gt;Dan&lt;/em&gt;: We have mentioned the Race Centre is going to launch in the next couple of months, but the really exciting thing for me is we've got an amazing season ahead of us. It's the last season of our Gen3 car, with 10 really exciting teams on the grid. We are going at speed with our tech innovation roadmap and what our fans want. And we're building up towards our Gen4 car, which will come out for season 13 in a year's time. That will get launched in 2026, and I think it will be a game changer in how people perceive electric motor sport and electric cars in general.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: It sounds like there's all sorts of exciting things going on. And Rohit too, what's coming up via this partnership that you are really looking forward to sharing with everyone?&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Rohit&lt;/em&gt;: Two things stand out for me. First is the AI-powered fan data platform that I've already spoken about. Second is the launch of Race Centre. It's going to change how fans experience live racing. And beyond final engagement, we are helping Formula E lead in sustainability with AI tools that model carbon impact and optimize logistics. This means every race can be smarter and greener. Our goal is clear: help Formula E be the most digital and sustainable motor sport in the world. The future is electric, and with AI, it's more engaging than ever.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Megan&lt;/em&gt;: Fantastic. Thank you so much, both. That was Rohit Agnihotri, principal technologist at Infosys, and Dan Cherowbrier, CITO of Formula E, whom I spoke with from Brighton, England.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That's it for this episode of Business Lab. I'm your host, Megan Tatum. I'm a contributing editor and host for Insights, the custom publishing division of MIT Technology Review. We were founded in 1899 at the Massachusetts Institute of Technology, and you can find us in print, on the web and at events each year around the world. For more information about us and the show, please check out our website at technologyreview.com.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This show is available wherever you get your podcasts. And if you enjoyed this episode, we hope you'll take a moment to rate and review us. Business Lab is a production of MIT Technology Review and this episode was produced by Giro Studios. Thanks for listening.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. It was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/15/1127432/the-fast-and-the-future-focused-are-revolutionizing-motorsport/</guid><pubDate>Mon, 15 Dec 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] Tokenization takes the lead in the fight for data security (AI | VentureBeat)</title><link>https://venturebeat.com/ai/tokenization-takes-the-lead-in-the-fight-for-data-security</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by Capital One&lt;/i&gt; &lt;i&gt;Software&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Tokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;During this VB in Conversation&lt;/a&gt;, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One’s own experience leveraging tokenization at scale. &lt;/p&gt;&lt;p&gt;Tokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications — including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.&lt;/p&gt;&lt;p&gt;&amp;quot;The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,&amp;quot; he explained. &amp;quot;The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The tokenization differentiator &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Most organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it’s being stored. But best-in-class organizations go even further, protecting data at birth, the moment it’s created.&lt;/p&gt;&lt;p&gt;At one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning — which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. &lt;/p&gt;&lt;p&gt;Tokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted — whether by the wrong person or the wrong machine — the data itself remains secure.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The business value of tokenization&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&amp;quot;Fundamentally you’re protecting data, and that’s priceless,&amp;quot; Raghu said. &amp;quot;Another thing that’s priceless – can you use that for modeling purposes subsequently? On the one hand, it’s a protection thing, and on the other hand it’s a business enabling thing.&amp;quot; &lt;/p&gt;&lt;p&gt;Because tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. &lt;/p&gt;&lt;p&gt;&amp;quot;If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,&amp;quot; Raghu said. &amp;quot;Conversely, if you don’t have that, there’s a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they’re limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that – operational impact, revenue impact, and obviously the peace of mind from a security standpoint.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Breaking down adoption barriers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Until now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That&amp;#x27;s one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.&lt;/p&gt;&lt;p&gt;&amp;quot;Capital One has gone through tokenization for more than a decade. We started doing it because we’re serving our 100 million banking customers. We want to protect that sensitive data,&amp;quot; Raghu said. &amp;quot;We’ve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We’ve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it’s a commercial offering.&amp;quot;&lt;/p&gt;&lt;p&gt;Vaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.&lt;/p&gt;&lt;p&gt;&amp;quot;We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,&amp;quot; Raghu said. &amp;quot;We’ve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.&amp;quot;&lt;/p&gt;&lt;p&gt;While conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer’s environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.&lt;/p&gt;&lt;p&gt;&amp;quot;We believe that fundamentally, tokenization should be easy to adopt,&amp;quot; Raghu said. &amp;quot;You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that’s been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that’s going to become a huge enabler.&amp;quot;&lt;/p&gt;&lt;p&gt;Don&amp;#x27;t miss &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;the whole conversation with Ravi Raghu, president, Capital One Software, here&lt;/a&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by Capital One&lt;/i&gt; &lt;i&gt;Software&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;Tokenization is emerging as a cornerstone of modern data security, helping businesses separate the value of their data from its risk. &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;During this VB in Conversation&lt;/a&gt;, Ravi Raghu, president, Capital One Software, talks about the ways tokenization can help reduce the value of breached data and preserve underlying data format and usability, including Capital One’s own experience leveraging tokenization at scale. &lt;/p&gt;&lt;p&gt;Tokenization, Raghu asserts, is a far superior technology. It converts sensitive data into a nonsensitive digital replacement, called a token, that maps back to the original, which is secured in a digital vault. The token placeholder preserves both the format and the utility of the sensitive data, and can be used across applications — including AI models. Because tokenization removes the need to manage encryption keys or dedicate compute to constant encrypting and decrypting, it offers one of the most scalable ways for companies to protect their most sensitive data, he added.&lt;/p&gt;&lt;p&gt;&amp;quot;The killer part, from a security standpoint, when you think about it relative to other methods, if a bad actor gets hold of the data, they get hold of tokens,&amp;quot; he explained. &amp;quot;The actual data is not sitting with the token, unlike other methods like encryption, where the actual data sits there, just waiting for someone to get hold of a key or use brute force to get to the real data. From every angle this is the ideal way one ought to go about protecting sensitive data.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The tokenization differentiator &lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Most organizations are just scratching the surface of data security, adding security at the very end, when data is read, to prevent an end user from accessing it. At minimum, organizations should focus on securing data on write, as it’s being stored. But best-in-class organizations go even further, protecting data at birth, the moment it’s created.&lt;/p&gt;&lt;p&gt;At one end of the safety spectrum is a simple lock-and-key approach that restricts access but leaves the underlying data intact. More advanced methods, like masking or modifying data, permanently alter its meaning — which can compromise its usefulness. File-level encryption provides broader protection for large volumes of stored data, but when you get down to field-level encryption (for example, a Social Security number), it becomes a bigger challenge. It takes a great deal of compute to encrypt a single field, and then to decrypt it at the point of usage. And still it has a fatal flaw: the original data is still right there, only needing the key to get access. &lt;/p&gt;&lt;p&gt;Tokenization avoids these pitfalls by replacing the original data with a surrogate that has no intrinsic value. If the token is intercepted — whether by the wrong person or the wrong machine — the data itself remains secure.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The business value of tokenization&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&amp;quot;Fundamentally you’re protecting data, and that’s priceless,&amp;quot; Raghu said. &amp;quot;Another thing that’s priceless – can you use that for modeling purposes subsequently? On the one hand, it’s a protection thing, and on the other hand it’s a business enabling thing.&amp;quot; &lt;/p&gt;&lt;p&gt;Because tokenization preserves the structure and ordinality of the original data, it can still be used for modeling and analytics, turning protection into a business enabler. Take private health data governed by HIPAA for example: tokenization means that data canbeused to build pricing models or for gene therapy research, while remaining compliant. &lt;/p&gt;&lt;p&gt;&amp;quot;If your data is already protected, you can then proliferate the usage of data across the entire enterprise and have everybody creating more and more value out of the data,&amp;quot; Raghu said. &amp;quot;Conversely, if you don’t have that, there’s a lot of reticence for enterprises today to have more people access it, or have more and more AI agents access their data. Ironically, they’re limiting the blast radius of innovation. The tokenization impact is massive, and there are many metrics you could use to measure that – operational impact, revenue impact, and obviously the peace of mind from a security standpoint.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Breaking down adoption barriers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Until now, the fundamental challenge with traditional tokenization has been performance. AI requires a scale and speed that is unprecedented. That&amp;#x27;s one of the major challenges Capital One addresses with Databolt, its vaultless tokenization solution, which can produce up to 4 million tokens per second.&lt;/p&gt;&lt;p&gt;&amp;quot;Capital One has gone through tokenization for more than a decade. We started doing it because we’re serving our 100 million banking customers. We want to protect that sensitive data,&amp;quot; Raghu said. &amp;quot;We’ve eaten our own dog food with our internal tokenization capability, over 100 billion times a month. We’ve taken that know-how and that capability, scale, and speed, and innovated so that the world can leverage it, so that it’s a commercial offering.&amp;quot;&lt;/p&gt;&lt;p&gt;Vaultless tokenization is an advanced form of tokenization that does not require a central database (vault) to store token mappings. Instead, it uses mathematical algorithms, cryptographic techniques, and deterministic mapping to generate tokens dynamically.This approach is faster, more scalable, and eliminates the security risk associated with managing a vault.&lt;/p&gt;&lt;p&gt;&amp;quot;We realized that for the scale and speed demands that we had, we needed to build out that capability ourselves,&amp;quot; Raghu said. &amp;quot;We’ve been iterating continuously on making sure that it can scale up to hundreds of billions of operations a month. All of our innovation has been around building IP and capability to do that thing at a battle-tested scale within our enterprise, for the purpose of serving our customers.&amp;quot;&lt;/p&gt;&lt;p&gt;While conventional tokenization methods can involve some complexity and slow down operations, Databolt seamlessly integrates with encrypted data warehouses, allowing businesses to maintain robust security without slowing performance or operations. Tokenization occurs in the customer’s environment, removing the need to communicate with an external network to perform tokenization operations, which can also slow performance.&lt;/p&gt;&lt;p&gt;&amp;quot;We believe that fundamentally, tokenization should be easy to adopt,&amp;quot; Raghu said. &amp;quot;You should be able to secure your data very quickly and operate at the speed and scale and cost needs that organizations have. I think that’s been a critical barrier so far for the mass scale adoption of tokenization. In an AI world, that’s going to become a huge enabler.&amp;quot;&lt;/p&gt;&lt;p&gt;Don&amp;#x27;t miss &lt;a href="https://www.youtube.com/watch?v=zGGgBrDqqHo"&gt;the whole conversation with Ravi Raghu, president, Capital One Software, here&lt;/a&gt;.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/tokenization-takes-the-lead-in-the-fight-for-data-security</guid><pubDate>Mon, 15 Dec 2025 15:00:00 +0000</pubDate></item><item><title>[NEW] CUGA on Hugging Face: Democratizing Configurable AI Agents (Hugging Face - Blog)</title><link>https://huggingface.co/blog/ibm-research/cuga-on-hugging-face</link><description>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Avi Yaeli's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/70501cdba867c50fe67bdf0f9675af04.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Introduction&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
AI agents are rapidly becoming essential for building intelligent applications, but creating robust, adaptable agents that scale across domains remains a challenge. Many existing frameworks struggle with brittleness, tool misuse, and failures when faced with complex workflows.
&lt;p&gt;&lt;strong&gt;CUGA (Configurable Generalist Agent)&lt;/strong&gt; was designed to overcome these limitations. It's an &lt;strong&gt;open-source, AI Agent&lt;/strong&gt; that combines flexibility, reliability, and ease of use for enterprise use cases. By abstracting orchestration complexity, CUGA empowers developers to focus on domain requirements rather than the internals of agent building. And now, with its integration into 🚀Hugging Face Spaces🚀, experimenting with CUGA and open models has never been easier.&lt;/p&gt;

	
		
	
	&lt;span&gt;
		&lt;strong&gt;What is CUGA?&lt;/strong&gt;
	&lt;/span&gt;

&lt;p&gt;CUGA is a &lt;strong&gt;configurable, general-purpose AI agent&lt;/strong&gt; that supports complex, multi-step tasks across web and API environments. It has achieved state-of-the-art performance on leading benchmarks:&lt;/p&gt;
&lt;p&gt;🥇 #1 on AppWorld - a benchmark with 750 real-world tasks across 457 APIs&lt;/p&gt;
&lt;p&gt;🥈 Top-tier on WebArena (#1 from 02/25 - 09/25) - showcases CUGA Computer Use capabilities with a complex benchmark for autonomous web agents across application domains&lt;/p&gt;
&lt;p&gt;At its core, CUGA offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High-performing generalist agent:&lt;/strong&gt; Benchmarked on complex web and API tasks, it combines best-of-breed agentic patterns (e.g. planner-executor, code-act) with structured planning and smart variable management to prevent hallucination and handle complexity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configurable reasoning modes:&lt;/strong&gt; Balance performance and cost/latency with flexible modes ranging from fast heuristics to deep planning, optimizing for your task requirements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer use&lt;/strong&gt;: Effortlessly combine UI interactions with API invocations in a workflow&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-tool integration&lt;/strong&gt;: Seamlessly integrate tools via OpenAPI specs, MCP servers, and LangChain, enabling rapid connection to REST APIs, custom protocols, and Python functions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrates with Langflow&lt;/strong&gt;: A low-code visual build experience for designing and deploying agent workflows without extensive coding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composable:&lt;/strong&gt; CUGA can be exposed as a tool to other agents, enabling nested reasoning and multi-agent collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We're also continuing to innovate with new experimental capabilities, including:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Configurable policy and human-in-the-loop instructions:&lt;/strong&gt; Improve alignment and ensure safe agent behavior in enterprise contexts&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Save-and-reuse capabilities:&lt;/strong&gt; Capture and reuse successful execution paths (plans, code, and trajectories) for faster and consistent behavior across repeated tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;img alt="CUGA Agentic Architecture" src="https://cdn-uploads.huggingface.co/production/uploads/6487da02c4b44322c124f39f/_26Cf6pVF7-JapcMreG0n.png" width="800" /&gt;
  &lt;br /&gt;&lt;strong&gt;Figure 1: CUGA Agentic Architecture&lt;/strong&gt;
&lt;/p&gt;


&lt;p&gt;The CUGA architecture begins with the user's message flowing into a chat layer that interprets intent and constructs the user's goal, based on context. A task planning and control component then decomposes this goal into structured subtasks, tracked programmatically through a dynamic task ledger. This ledger supports re-planning when needed, ensuring robust execution. Subtasks are delegated to specialized agents, such as the API agent, which uses an inner reasoning loop to generate pseudo-code instructions before invoking code in a secure sandbox. The system leverages a tool registry that goes beyond MCP protocols to parse and understand tool capabilities, enabling precise orchestration. Once all steps are completed, the final response is returned to the user, delivering reliable, policy-aligned outcomes.&lt;/p&gt;
&lt;p&gt;CUGA works best when inference is fast. When each call takes seconds, delays compound and force a tradeoff between agent capability and user experience. Running on high-performance inference platforms like Groq shows how fast inference fundamentally expands what agent architectures can achieve.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Open Source and Open Models&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA is fully &lt;strong&gt;open source, under the Apache 2.0 license,&lt;/strong&gt; and you can find us at cuga.dev.&lt;/p&gt;
&lt;p&gt;By embracing &lt;strong&gt;open models&lt;/strong&gt;, CUGA aligns with the Hugging Face ethos of democratizing AI-giving developers the freedom to choose models that best fit their needs, whether for experimentation or production.&lt;/p&gt;
&lt;p&gt;CUGA has been tested with a variety of open models, including gpt-oss-120b and Llama-4-Maverick-17B-128E-Instruct-fp8 (both hosted on Groq). Our Hugging Face Space uses gpt-oss-120b, with the model hosted on Groq, offering a rapid response time for LLM calls&lt;/p&gt;
&lt;p&gt;Groq runs open models on its custom‑built LPUs, which are designed for AI inference and optimal for repeated agent inferences required by CUGA's architecture, enabling planning, execution, and validation steps to finish fast. The result is strong cost and performance: open models are ~80-90% cheaper than closed alternatives; Groq's OpenAI-compatible APIs meet production latency needs, and CUGA stays fully configurable across models, providers, and deployment topologies.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Integration with Langflow: Visual Agent Design Made Simple&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To make agent development even more accessible, CUGA integrates with &lt;strong&gt;Langflow&lt;/strong&gt;, an open-source visual programming interface for building LLM-powered workflows. Its intuitive drag-and-drop interface reduces the barrier to entry for those who prefer low-code solutions.&lt;/p&gt;
&lt;p&gt;Starting with &lt;strong&gt;Langflow 1.7.0&lt;/strong&gt;, CUGA ships with its own widget, enabling users to assemble complex, multi-tool agents visually and deploy with a click. Give it a try at &lt;strong&gt;langflow.org&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Try the Hugging Face Demo: A Hands-On Preview&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We've launched a &lt;strong&gt;CUGA demo on Hugging Face Spaces&lt;/strong&gt; to give you a taste of what's possible. This demo showcases a &lt;strong&gt;small CRM system&lt;/strong&gt; and equips CUGA with &lt;strong&gt;20 preconfigured tools&lt;/strong&gt; for handling sales related data queries and API interactions through the API Agent. To make experimentation even more powerful, the demo provides &lt;strong&gt;access to workspace files&lt;/strong&gt;, enabling you to &lt;strong&gt;use predefined policies.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give it a try on&lt;/strong&gt; &lt;strong&gt;Hugging Face Spaces&lt;/strong&gt; and share your feedback!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Conclusion and Call to Action&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA brings a new level of flexibility and openness to AI agent building. To engage with us:&lt;/p&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div class="not-prose"&gt;&lt;div class="SVELTE_HYDRATER contents"&gt;&lt;div class="not-prose"&gt;&lt;div class="mb-12 flex flex-wrap items-center gap-x-5 gap-y-3.5"&gt;&lt;div class="flex items-center font-sans leading-tight"&gt;

&lt;span class="inline-block "&gt;&lt;span class="contents"&gt;&lt;img alt="Avi Yaeli's avatar" class="rounded-full! m-0 mr-2.5 size-9 sm:mr-3 sm:size-12" src="https://huggingface.co/avatars/70501cdba867c50fe67bdf0f9675af04.svg" /&gt;
				&lt;/span&gt;
	&lt;/span&gt;

				
			&lt;/div&gt;&lt;/div&gt;
	&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
					

					&lt;!-- HTML_TAG_START --&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Introduction&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
AI agents are rapidly becoming essential for building intelligent applications, but creating robust, adaptable agents that scale across domains remains a challenge. Many existing frameworks struggle with brittleness, tool misuse, and failures when faced with complex workflows.
&lt;p&gt;&lt;strong&gt;CUGA (Configurable Generalist Agent)&lt;/strong&gt; was designed to overcome these limitations. It's an &lt;strong&gt;open-source, AI Agent&lt;/strong&gt; that combines flexibility, reliability, and ease of use for enterprise use cases. By abstracting orchestration complexity, CUGA empowers developers to focus on domain requirements rather than the internals of agent building. And now, with its integration into 🚀Hugging Face Spaces🚀, experimenting with CUGA and open models has never been easier.&lt;/p&gt;

	
		
	
	&lt;span&gt;
		&lt;strong&gt;What is CUGA?&lt;/strong&gt;
	&lt;/span&gt;

&lt;p&gt;CUGA is a &lt;strong&gt;configurable, general-purpose AI agent&lt;/strong&gt; that supports complex, multi-step tasks across web and API environments. It has achieved state-of-the-art performance on leading benchmarks:&lt;/p&gt;
&lt;p&gt;🥇 #1 on AppWorld - a benchmark with 750 real-world tasks across 457 APIs&lt;/p&gt;
&lt;p&gt;🥈 Top-tier on WebArena (#1 from 02/25 - 09/25) - showcases CUGA Computer Use capabilities with a complex benchmark for autonomous web agents across application domains&lt;/p&gt;
&lt;p&gt;At its core, CUGA offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High-performing generalist agent:&lt;/strong&gt; Benchmarked on complex web and API tasks, it combines best-of-breed agentic patterns (e.g. planner-executor, code-act) with structured planning and smart variable management to prevent hallucination and handle complexity&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Configurable reasoning modes:&lt;/strong&gt; Balance performance and cost/latency with flexible modes ranging from fast heuristics to deep planning, optimizing for your task requirements&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Computer use&lt;/strong&gt;: Effortlessly combine UI interactions with API invocations in a workflow&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-tool integration&lt;/strong&gt;: Seamlessly integrate tools via OpenAPI specs, MCP servers, and LangChain, enabling rapid connection to REST APIs, custom protocols, and Python functions&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrates with Langflow&lt;/strong&gt;: A low-code visual build experience for designing and deploying agent workflows without extensive coding&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Composable:&lt;/strong&gt; CUGA can be exposed as a tool to other agents, enabling nested reasoning and multi-agent collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;We're also continuing to innovate with new experimental capabilities, including:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Configurable policy and human-in-the-loop instructions:&lt;/strong&gt; Improve alignment and ensure safe agent behavior in enterprise contexts&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Save-and-reuse capabilities:&lt;/strong&gt; Capture and reuse successful execution paths (plans, code, and trajectories) for faster and consistent behavior across repeated tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;
  &lt;img alt="CUGA Agentic Architecture" src="https://cdn-uploads.huggingface.co/production/uploads/6487da02c4b44322c124f39f/_26Cf6pVF7-JapcMreG0n.png" width="800" /&gt;
  &lt;br /&gt;&lt;strong&gt;Figure 1: CUGA Agentic Architecture&lt;/strong&gt;
&lt;/p&gt;


&lt;p&gt;The CUGA architecture begins with the user's message flowing into a chat layer that interprets intent and constructs the user's goal, based on context. A task planning and control component then decomposes this goal into structured subtasks, tracked programmatically through a dynamic task ledger. This ledger supports re-planning when needed, ensuring robust execution. Subtasks are delegated to specialized agents, such as the API agent, which uses an inner reasoning loop to generate pseudo-code instructions before invoking code in a secure sandbox. The system leverages a tool registry that goes beyond MCP protocols to parse and understand tool capabilities, enabling precise orchestration. Once all steps are completed, the final response is returned to the user, delivering reliable, policy-aligned outcomes.&lt;/p&gt;
&lt;p&gt;CUGA works best when inference is fast. When each call takes seconds, delays compound and force a tradeoff between agent capability and user experience. Running on high-performance inference platforms like Groq shows how fast inference fundamentally expands what agent architectures can achieve.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Open Source and Open Models&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA is fully &lt;strong&gt;open source, under the Apache 2.0 license,&lt;/strong&gt; and you can find us at cuga.dev.&lt;/p&gt;
&lt;p&gt;By embracing &lt;strong&gt;open models&lt;/strong&gt;, CUGA aligns with the Hugging Face ethos of democratizing AI-giving developers the freedom to choose models that best fit their needs, whether for experimentation or production.&lt;/p&gt;
&lt;p&gt;CUGA has been tested with a variety of open models, including gpt-oss-120b and Llama-4-Maverick-17B-128E-Instruct-fp8 (both hosted on Groq). Our Hugging Face Space uses gpt-oss-120b, with the model hosted on Groq, offering a rapid response time for LLM calls&lt;/p&gt;
&lt;p&gt;Groq runs open models on its custom‑built LPUs, which are designed for AI inference and optimal for repeated agent inferences required by CUGA's architecture, enabling planning, execution, and validation steps to finish fast. The result is strong cost and performance: open models are ~80-90% cheaper than closed alternatives; Groq's OpenAI-compatible APIs meet production latency needs, and CUGA stays fully configurable across models, providers, and deployment topologies.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Integration with Langflow: Visual Agent Design Made Simple&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;To make agent development even more accessible, CUGA integrates with &lt;strong&gt;Langflow&lt;/strong&gt;, an open-source visual programming interface for building LLM-powered workflows. Its intuitive drag-and-drop interface reduces the barrier to entry for those who prefer low-code solutions.&lt;/p&gt;
&lt;p&gt;Starting with &lt;strong&gt;Langflow 1.7.0&lt;/strong&gt;, CUGA ships with its own widget, enabling users to assemble complex, multi-tool agents visually and deploy with a click. Give it a try at &lt;strong&gt;langflow.org&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Try the Hugging Face Demo: A Hands-On Preview&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;We've launched a &lt;strong&gt;CUGA demo on Hugging Face Spaces&lt;/strong&gt; to give you a taste of what's possible. This demo showcases a &lt;strong&gt;small CRM system&lt;/strong&gt; and equips CUGA with &lt;strong&gt;20 preconfigured tools&lt;/strong&gt; for handling sales related data queries and API interactions through the API Agent. To make experimentation even more powerful, the demo provides &lt;strong&gt;access to workspace files&lt;/strong&gt;, enabling you to &lt;strong&gt;use predefined policies.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give it a try on&lt;/strong&gt; &lt;strong&gt;Hugging Face Spaces&lt;/strong&gt; and share your feedback!&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		&lt;strong&gt;Conclusion and Call to Action&lt;/strong&gt;
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;CUGA brings a new level of flexibility and openness to AI agent building. To engage with us:&lt;/p&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/ibm-research/cuga-on-hugging-face</guid><pubDate>Mon, 15 Dec 2025 16:01:04 +0000</pubDate></item><item><title>[NEW] NVIDIA Acquires Open-Source Workload Management Provider SchedMD (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/nvidia-acquires-schedmd/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA today announced it has acquired SchedMD — the leading developer of Slurm, an open-source workload management system for high-performance computing (HPC) and AI — to help strengthen the open-source software ecosystem and drive AI innovation for researchers, developers and enterprises.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to develop and distribute Slurm as open-source, vendor-neutral software, making it widely available to and supported by the broader HPC and AI community across diverse hardware and software environments.&lt;/p&gt;
&lt;p&gt;HPC and AI workloads involve complex computations running parallel tasks on clusters that require queuing, scheduling and allocating computational resources. As HPC and AI clusters get larger and more powerful, efficient resource utilization is critical.&lt;/p&gt;
&lt;p&gt;As the leading workload manager and job scheduler in scalability, throughput and complex policy management, Slurm is used in more than half of the top 10 and top 100 systems in the TOP500 list of supercomputers.&lt;/p&gt;
&lt;p&gt;Slurm, which is supported on the latest NVIDIA hardware, is also part of the critical infrastructure needed for generative AI, used by foundation model developers and AI builders to manage model training and inference needs.&lt;/p&gt;
&lt;p&gt;“We’re thrilled to join forces with NVIDIA, as this acquisition is the ultimate validation of Slurm’s critical role in the world’s most demanding HPC and AI environments,” said Danny Auble, CEO of SchedMD. “NVIDIA’s deep expertise and investment in accelerated computing will enhance the development of Slurm — which will continue to be open source — to meet the demands of the next generation of AI and supercomputing.”&lt;/p&gt;
&lt;p&gt;NVIDIA has been collaborating with SchedMD for over a decade and will continue investing in Slurm’s development to ensure it remains the leading open-source scheduler for HPC and AI.&lt;/p&gt;
&lt;p&gt;NVIDIA will accelerate SchedMD’s access to new systems — allowing users of NVIDIA’s accelerated computing platform to optimize workloads across their entire compute infrastructure — while also supporting a diverse hardware and software ecosystem, so customers can run heterogeneous clusters with the latest Slurm innovations.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to offer open-source software support, training and development for Slurm to SchedMD’s hundreds of customers, which include cloud providers, manufacturers, AI companies and research labs spanning industries such as autonomous driving, healthcare and life sciences, energy, financial services, manufacturing and government.&lt;/p&gt;
&lt;p&gt;Together with SchedMD, NVIDIA is bolstering the open-source software ecosystem to catalyze HPC and AI innovation across industries, at every scale.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/nvidiaheadquarters.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA today announced it has acquired SchedMD — the leading developer of Slurm, an open-source workload management system for high-performance computing (HPC) and AI — to help strengthen the open-source software ecosystem and drive AI innovation for researchers, developers and enterprises.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to develop and distribute Slurm as open-source, vendor-neutral software, making it widely available to and supported by the broader HPC and AI community across diverse hardware and software environments.&lt;/p&gt;
&lt;p&gt;HPC and AI workloads involve complex computations running parallel tasks on clusters that require queuing, scheduling and allocating computational resources. As HPC and AI clusters get larger and more powerful, efficient resource utilization is critical.&lt;/p&gt;
&lt;p&gt;As the leading workload manager and job scheduler in scalability, throughput and complex policy management, Slurm is used in more than half of the top 10 and top 100 systems in the TOP500 list of supercomputers.&lt;/p&gt;
&lt;p&gt;Slurm, which is supported on the latest NVIDIA hardware, is also part of the critical infrastructure needed for generative AI, used by foundation model developers and AI builders to manage model training and inference needs.&lt;/p&gt;
&lt;p&gt;“We’re thrilled to join forces with NVIDIA, as this acquisition is the ultimate validation of Slurm’s critical role in the world’s most demanding HPC and AI environments,” said Danny Auble, CEO of SchedMD. “NVIDIA’s deep expertise and investment in accelerated computing will enhance the development of Slurm — which will continue to be open source — to meet the demands of the next generation of AI and supercomputing.”&lt;/p&gt;
&lt;p&gt;NVIDIA has been collaborating with SchedMD for over a decade and will continue investing in Slurm’s development to ensure it remains the leading open-source scheduler for HPC and AI.&lt;/p&gt;
&lt;p&gt;NVIDIA will accelerate SchedMD’s access to new systems — allowing users of NVIDIA’s accelerated computing platform to optimize workloads across their entire compute infrastructure — while also supporting a diverse hardware and software ecosystem, so customers can run heterogeneous clusters with the latest Slurm innovations.&lt;/p&gt;
&lt;p&gt;NVIDIA will continue to offer open-source software support, training and development for Slurm to SchedMD’s hundreds of customers, which include cloud providers, manufacturers, AI companies and research labs spanning industries such as autonomous driving, healthcare and life sciences, energy, financial services, manufacturing and government.&lt;/p&gt;
&lt;p&gt;Together with SchedMD, NVIDIA is bolstering the open-source software ecosystem to catalyze HPC and AI innovation across industries, at every scale.&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/nvidia-acquires-schedmd/</guid><pubDate>Mon, 15 Dec 2025 16:30:11 +0000</pubDate></item><item><title>[NEW] Gemini provides automated feedback for theoretical computer scientists at STOC 2026 (The latest research from Google)</title><link>https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;Vincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project&lt;/i&gt;[8746db]&lt;i&gt;, with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Acknowledgements&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;&lt;i&gt;Vincent Cohen-Addad, Rajesh Jayaram, Jon Schneider, and David Woodruff co-led this project&lt;/i&gt;[8746db]&lt;i&gt;, with key contributions by Lalit Jain, Jieming Mao, and Vahab Mirrokni. We also thank the STOC 2026 PC chair Artur Czumaj and the many other authors who participated in this experiment and provided their valuable feedback, helpful suggestions, and discussions, including Mohammad Taghi Hajiaghayi, Ravi Kumar, Yossi Matias, and Sergei Vassilvitskii. Finally, this work builds on the efforts of the Deep Think team: Garrett Bingham, Irene Cai, Heng-Tze Cheng, Yong Cheng, Kristen Chiafullo, Vincent Cohen-Addad, Paul Covington, Golnaz Ghiasi, Chenjie Gu, Huan Gui, Ana Hosseini, Dawsen Hwang, Lalit Jain, Vihan Jain, Ragha Kotikalapudi, Chenkai Kuang, Chenkai Kuang, Maciej Kula, Nate Kushman, Jane Labanowski, Quoc Le, Jonathan Lee, Zhaoqi Leng, Steve Li, YaGuang Li, Hanzhao (Maggie) Lin, Evan Liu, Yuan Liu, Thang Luong, Jieming Mao, Vahab Mirrokni, Pol Moreno, Nigamaa Nayakanti, Aroonalok Pyne, Shubha Raghvendra, Sashank Reddi, Nikunj Saunshi, Siamak Shakeri, Archit Sharma, Xinying Song, Qijun Tan, Yi Tay, Trieu Trinh, Theophane Weber, Winnie Xu, Zicheng Xu, Shunyu Yao, Lijun Yu, Hao Zhou, Honglei Zhuang, and Song Zuo.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/gemini-provides-automated-feedback-for-theoretical-computer-scientists-at-stoc-2026/</guid><pubDate>Mon, 15 Dec 2025 17:37:00 +0000</pubDate></item></channel></rss>