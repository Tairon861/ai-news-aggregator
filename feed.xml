<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Dec 2025 12:51:54 +0000</lastBuildDate><item><title>Google’s answer to the AI arms race — promote the guy behind its data center tech (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/10/googles-answer-to-the-ai-arms-race-promote-the-guy-behind-its-data-center-tech/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-4.33.43-PM.png?resize=1200,699" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google just made a major move in the AI infrastructure arms race, elevating Amin Vahdat to chief technologist for AI infrastructure, a newly created position reporting directly to CEO Sundar Pichai, according to a memo first reported by Semafor and later confirmed by TechCrunch. It’s a signal of just how critical this work has become as Google pours up to $93 billion into capital expenditures by the end of 2025 — a number that parent company Alphabet expects will be a whole lot bigger next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat isn’t new to the game. The computer scientist, who holds a PhD from UC Berkeley and started as a research intern at Xerox PARC back in the early ’90s, has been quietly building Google’s AI backbone for the past 15 years. Before joining Google in 2010 as an engineering fellow and VP, he was an associate professor at Duke University and later a professor and SAIC Chair at UC San Diego. His academic credentials are formidable — with what appears to be around 395 published papers — and his research has always focused on making computers work more efficiently at massive scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Vahdat already maintains a high profile with Google. Just eight months ago, at Google Cloud Next, he took the stage to unveil the company’s seventh-generation TPU, called Ironwood, in his role as VP and GM of ML, Systems, and Cloud AI. The specs he rattled off at the event were staggering, too: over 9,000 chips per pod delivering 42.5 exaflops of compute — more than 24 times the power of the world’s No. 1 supercomputer at the time, he said. “Demand for AI compute has increased by a factor of 100 million in just eight years,” he told the audience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Behind the scenes, as noted by Semafor, Vahdat has been orchestrating the unglamorous and essential work that keeps Google competitive, including those custom TPU chips for AI training and inference that give Google an edge over rivals like OpenAI, as well as the Jupiter network, the super-fast internal network that allows all its servers to talk to each other and move massive amounts of data around. (In a blog post late last year, Vahdat said that Jupiter now scales to 13 petabits per second, explaining that’s enough bandwidth to theoretically support a video call for all 8 billion people on Earth simultaneously.) &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat has also been deeply involved in the ongoing development of the Borg software system, Google’s cluster management system that acts as the brain coordinating all the work happening across its data centers. And he has said he oversaw the development of Axion, Google’s first custom Arm-based general-purpose CPUs designed for data centers, which the company unveiled last year and continues to build.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, Vahdat is central to Google’s AI story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, in a market where top AI talent commands astronomical compensation and constant recruitment, Google’s decision to elevate Vahdat to the C-suite may also be about retention. When you’ve spent 15 years building someone into a linchpin of your AI strategy, you make sure they stay.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Screenshot-2025-12-10-at-4.33.43-PM.png?resize=1200,699" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google just made a major move in the AI infrastructure arms race, elevating Amin Vahdat to chief technologist for AI infrastructure, a newly created position reporting directly to CEO Sundar Pichai, according to a memo first reported by Semafor and later confirmed by TechCrunch. It’s a signal of just how critical this work has become as Google pours up to $93 billion into capital expenditures by the end of 2025 — a number that parent company Alphabet expects will be a whole lot bigger next year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat isn’t new to the game. The computer scientist, who holds a PhD from UC Berkeley and started as a research intern at Xerox PARC back in the early ’90s, has been quietly building Google’s AI backbone for the past 15 years. Before joining Google in 2010 as an engineering fellow and VP, he was an associate professor at Duke University and later a professor and SAIC Chair at UC San Diego. His academic credentials are formidable — with what appears to be around 395 published papers — and his research has always focused on making computers work more efficiently at massive scale.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Vahdat already maintains a high profile with Google. Just eight months ago, at Google Cloud Next, he took the stage to unveil the company’s seventh-generation TPU, called Ironwood, in his role as VP and GM of ML, Systems, and Cloud AI. The specs he rattled off at the event were staggering, too: over 9,000 chips per pod delivering 42.5 exaflops of compute — more than 24 times the power of the world’s No. 1 supercomputer at the time, he said. “Demand for AI compute has increased by a factor of 100 million in just eight years,” he told the audience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Behind the scenes, as noted by Semafor, Vahdat has been orchestrating the unglamorous and essential work that keeps Google competitive, including those custom TPU chips for AI training and inference that give Google an edge over rivals like OpenAI, as well as the Jupiter network, the super-fast internal network that allows all its servers to talk to each other and move massive amounts of data around. (In a blog post late last year, Vahdat said that Jupiter now scales to 13 petabits per second, explaining that’s enough bandwidth to theoretically support a video call for all 8 billion people on Earth simultaneously.) &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Vahdat has also been deeply involved in the ongoing development of the Borg software system, Google’s cluster management system that acts as the brain coordinating all the work happening across its data centers. And he has said he oversaw the development of Axion, Google’s first custom Arm-based general-purpose CPUs designed for data centers, which the company unveiled last year and continues to build.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In short, Vahdat is central to Google’s AI story.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Indeed, in a market where top AI talent commands astronomical compensation and constant recruitment, Google’s decision to elevate Vahdat to the C-suite may also be about retention. When you’ve spent 15 years building someone into a linchpin of your AI strategy, you make sure they stay.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/10/googles-answer-to-the-ai-arms-race-promote-the-guy-behind-its-data-center-tech/</guid><pubDate>Thu, 11 Dec 2025 01:10:04 +0000</pubDate></item><item><title>[NEW] New materials could boost the energy efficiency of microelectronics (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/new-materials-could-boost-energy-efficiency-microelectronics-1211</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-Memory-Transistors-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT researchers have developed a new fabrication method that could enable the production of more energy efficient electronics by stacking multiple functional components on top of one existing circuit.&lt;/p&gt;&lt;p&gt;In traditional circuits, logic devices that perform computation, like transistors, and memory devices that store data are built as separate components, forcing data to travel back and forth between them, which wastes energy.&lt;/p&gt;&lt;p&gt;This new electronics integration platform allows scientists to fabricate transistors and memory devices in one compact stack on a semiconductor chip. This eliminates much of that wasted energy while boosting the speed of computation.&lt;/p&gt;&lt;p&gt;Key to this advance is a newly developed material with unique properties and a more precise fabrication approach that reduces the number of defects in the material. This allows the researchers to make extremely tiny transistors with built-in memory that can perform faster than state-of-the-art devices while consuming less electricity than similar transistors.&lt;/p&gt;&lt;p&gt;By improving the energy efficiency of electronic devices, this new approach could help reduce the burgeoning electricity consumption of computation, especially for demanding applications like generative AI, deep learning, and computer vision tasks.&lt;/p&gt;&lt;p&gt;“We have to minimize the amount of energy we use for AI and other data-centric computation in the future because it is simply not sustainable. We will need new technology like this integration platform to continue that progress,” says Yanjie Shao, an MIT postdoc and lead author of two papers on these new transistors.&lt;/p&gt;&lt;p&gt;The new technique is described in two papers (one invited) that were presented at the IEEE International Electron Devices Meeting. Shao is joined on the papers by senior authors Jesús del Alamo, the Donner Professor of Engineering in the MIT Department of Electrical Engineering and Computer Science (EECS); Dimitri Antoniadis, the Ray and Maria Stata Professor of Electrical Engineering and Computer Science at MIT; as well as others at MIT, the University of Waterloo, and Samsung Electronics.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Flipping the problem&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Standard CMOS (complementary metal-oxide semiconductor) chips traditionally have a front end, where the active components like transistors and capacitors are fabricated, and a back end that includes wires called interconnects and other metal bonds that connect components of the chip.&lt;/p&gt;&lt;p&gt;But some energy is lost when data travel between these bonds, and slight misalignments can hamper performance. Stacking active components would reduce the distance data must travel and improve a chip’s energy efficiency.&lt;/p&gt;&lt;p&gt;Typically, it is difficult to stack silicon transistors on a CMOS chip because the high temperature required to fabricate additional devices on the front end would destroy the existing transistors underneath.&lt;/p&gt;&lt;p&gt;The MIT researchers turned this problem on its head, developing an integration technique to stack active components on the back end of the chip instead.&lt;/p&gt;&lt;p&gt;“If we can use this back-end platform to put in additional active layers of transistors, not just interconnects, that would make the integration density of the chip much higher and improve its energy efficiency,” Shao explains.&lt;/p&gt;&lt;p&gt;The researchers accomplished this using a new material, amorphous indium oxide, as the active channel layer of their back-end transistor. The active channel layer is where the transistor’s essential functions take place.&lt;/p&gt;&lt;p&gt;Due to the unique properties of indium oxide, they can “grow” an extremely thin layer of this material at a temperature of only about 150 degrees Celsius on the back end of an existing circuit without damaging the device on the front end.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Perfecting the process&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;They carefully optimized the fabrication process, which minimizes the number of defects in a layer of indium oxide material that is only about 2 nanometers thick.&lt;/p&gt;&lt;p&gt;A few defects, known as oxygen vacancies, are necessary for the transistor to switch on, but with too many defects it won’t work properly. This optimized fabrication process allows the researchers to produce an extremely tiny transistor that operates rapidly and cleanly, eliminating much of the additional energy required to switch a transistor between off and on.&lt;/p&gt;&lt;p&gt;Building on this approach, they also fabricated back-end transistors with integrated memory that are only about 20 nanometers in size. To do this, they added a layer of material called ferroelectric hafnium-zirconium-oxide as the memory component.&lt;/p&gt;&lt;p&gt;These compact memory transistors demonstrated switching speeds of only 10 nanoseconds, hitting the limit of the team’s measurement instruments. This switching also requires much lower voltage than similar devices, reducing electricity consumption.&lt;/p&gt;&lt;p&gt;And because the memory transistors are so tiny, the researchers can use them as a platform to study the fundamental physics of individual units of ferroelectric hafnium-zirconium-oxide.&lt;/p&gt;&lt;p&gt;“If we can better understand the physics, we can use this material for many new applications. The energy it uses is very minimal, and it gives us a lot of flexibility in how we can design devices. It really could open up many new avenues for the future,” Shao says.&lt;/p&gt;&lt;p&gt;The researchers also worked with a team at the University of Waterloo to develop a model of the performance of the back-end transistors, which is an important step before the devices can be integrated into larger circuits and electronic systems.&lt;/p&gt;&lt;p&gt;In the future, they want to build upon these demonstrations by integrating back-end memory transistors onto a single circuit. They also want to enhance the performance of the transistors and study how to more finely control the properties of ferroelectric hafnium-zirconium-oxide.&lt;/p&gt;&lt;p&gt;“Now, we can build a platform of versatile electronics on the back end of a chip that enable us to achieve high energy efficiency and many different functionalities in very small devices. We have a good device architecture and material to work with, but we need to keep innovating to uncover the ultimate performance limits,” Shao says.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by Semiconductor Research Corporation (SRC) and Intel. Fabrication was carried out at the MIT Microsystems Technology Laboratories and MIT.nano facilities.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202512/MIT-Memory-Transistors-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT researchers have developed a new fabrication method that could enable the production of more energy efficient electronics by stacking multiple functional components on top of one existing circuit.&lt;/p&gt;&lt;p&gt;In traditional circuits, logic devices that perform computation, like transistors, and memory devices that store data are built as separate components, forcing data to travel back and forth between them, which wastes energy.&lt;/p&gt;&lt;p&gt;This new electronics integration platform allows scientists to fabricate transistors and memory devices in one compact stack on a semiconductor chip. This eliminates much of that wasted energy while boosting the speed of computation.&lt;/p&gt;&lt;p&gt;Key to this advance is a newly developed material with unique properties and a more precise fabrication approach that reduces the number of defects in the material. This allows the researchers to make extremely tiny transistors with built-in memory that can perform faster than state-of-the-art devices while consuming less electricity than similar transistors.&lt;/p&gt;&lt;p&gt;By improving the energy efficiency of electronic devices, this new approach could help reduce the burgeoning electricity consumption of computation, especially for demanding applications like generative AI, deep learning, and computer vision tasks.&lt;/p&gt;&lt;p&gt;“We have to minimize the amount of energy we use for AI and other data-centric computation in the future because it is simply not sustainable. We will need new technology like this integration platform to continue that progress,” says Yanjie Shao, an MIT postdoc and lead author of two papers on these new transistors.&lt;/p&gt;&lt;p&gt;The new technique is described in two papers (one invited) that were presented at the IEEE International Electron Devices Meeting. Shao is joined on the papers by senior authors Jesús del Alamo, the Donner Professor of Engineering in the MIT Department of Electrical Engineering and Computer Science (EECS); Dimitri Antoniadis, the Ray and Maria Stata Professor of Electrical Engineering and Computer Science at MIT; as well as others at MIT, the University of Waterloo, and Samsung Electronics.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Flipping the problem&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Standard CMOS (complementary metal-oxide semiconductor) chips traditionally have a front end, where the active components like transistors and capacitors are fabricated, and a back end that includes wires called interconnects and other metal bonds that connect components of the chip.&lt;/p&gt;&lt;p&gt;But some energy is lost when data travel between these bonds, and slight misalignments can hamper performance. Stacking active components would reduce the distance data must travel and improve a chip’s energy efficiency.&lt;/p&gt;&lt;p&gt;Typically, it is difficult to stack silicon transistors on a CMOS chip because the high temperature required to fabricate additional devices on the front end would destroy the existing transistors underneath.&lt;/p&gt;&lt;p&gt;The MIT researchers turned this problem on its head, developing an integration technique to stack active components on the back end of the chip instead.&lt;/p&gt;&lt;p&gt;“If we can use this back-end platform to put in additional active layers of transistors, not just interconnects, that would make the integration density of the chip much higher and improve its energy efficiency,” Shao explains.&lt;/p&gt;&lt;p&gt;The researchers accomplished this using a new material, amorphous indium oxide, as the active channel layer of their back-end transistor. The active channel layer is where the transistor’s essential functions take place.&lt;/p&gt;&lt;p&gt;Due to the unique properties of indium oxide, they can “grow” an extremely thin layer of this material at a temperature of only about 150 degrees Celsius on the back end of an existing circuit without damaging the device on the front end.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Perfecting the process&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;They carefully optimized the fabrication process, which minimizes the number of defects in a layer of indium oxide material that is only about 2 nanometers thick.&lt;/p&gt;&lt;p&gt;A few defects, known as oxygen vacancies, are necessary for the transistor to switch on, but with too many defects it won’t work properly. This optimized fabrication process allows the researchers to produce an extremely tiny transistor that operates rapidly and cleanly, eliminating much of the additional energy required to switch a transistor between off and on.&lt;/p&gt;&lt;p&gt;Building on this approach, they also fabricated back-end transistors with integrated memory that are only about 20 nanometers in size. To do this, they added a layer of material called ferroelectric hafnium-zirconium-oxide as the memory component.&lt;/p&gt;&lt;p&gt;These compact memory transistors demonstrated switching speeds of only 10 nanoseconds, hitting the limit of the team’s measurement instruments. This switching also requires much lower voltage than similar devices, reducing electricity consumption.&lt;/p&gt;&lt;p&gt;And because the memory transistors are so tiny, the researchers can use them as a platform to study the fundamental physics of individual units of ferroelectric hafnium-zirconium-oxide.&lt;/p&gt;&lt;p&gt;“If we can better understand the physics, we can use this material for many new applications. The energy it uses is very minimal, and it gives us a lot of flexibility in how we can design devices. It really could open up many new avenues for the future,” Shao says.&lt;/p&gt;&lt;p&gt;The researchers also worked with a team at the University of Waterloo to develop a model of the performance of the back-end transistors, which is an important step before the devices can be integrated into larger circuits and electronic systems.&lt;/p&gt;&lt;p&gt;In the future, they want to build upon these demonstrations by integrating back-end memory transistors onto a single circuit. They also want to enhance the performance of the transistors and study how to more finely control the properties of ferroelectric hafnium-zirconium-oxide.&lt;/p&gt;&lt;p&gt;“Now, we can build a platform of versatile electronics on the back end of a chip that enable us to achieve high energy efficiency and many different functionalities in very small devices. We have a good device architecture and material to work with, but we need to keep innovating to uncover the ultimate performance limits,” Shao says.&lt;/p&gt;&lt;p&gt;This work is supported, in part, by Semiconductor Research Corporation (SRC) and Intel. Fabrication was carried out at the MIT Microsystems Technology Laboratories and MIT.nano facilities.&amp;nbsp;&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/new-materials-could-boost-energy-efficiency-microelectronics-1211</guid><pubDate>Thu, 11 Dec 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] Creating a glass box: How NetSuite is engineering trust into AI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/creating-a-glass-box-how-netsuite-is-engineering-trust-into-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by Oracle NetSuite&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When any company tells you it is their biggest product release in almost three decades, it’s worth listening. When the person saying it founded the world’s first cloud computing company, it’s time to take note. &lt;/p&gt;&lt;p&gt;At SuiteWorld 2025, Evan Goldberg, founder and EVP of Oracle NetSuite, did just that when he called NetSuite Next the company’s biggest product evolution in nearly three decades. But behind that sweeping vision lies a quieter shift — one centered on how AI behaves, not just what it can do. &lt;/p&gt;&lt;p&gt;“Every company is experimenting with AI,” says Brian Chess, SVP of Technology and AI at NetSuite. “Some ideas hit the mark, and some don’t, but each one teaches us something. That’s how innovation works.”&lt;/p&gt;&lt;p&gt;For Chess and Gary Wiessinger, SVP of Application Development at NetSuite, the challenge lies in governing AI responsibly. Rather than reinventing its system, NetSuite is extending the same principles into the AI era that have guided its strategy for 27 years — security, control, and auditability. The goal is to make AI actions traceable, permissions enforceable, and outcomes auditable.&lt;/p&gt;&lt;p&gt;The philosophy underpins what Chess calls a “glass-box” approach to enterprise AI, where decisions are visible and every agent operates within human-defined guardrails.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built on Oracle’s foundation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;NetSuite Next is the result of five years of development. It is built on Oracle Cloud Infrastructure (OCI), which is relied on by many of the world’s most important AI model providers, and has AI capabilities integrated directly into its core rather than added as a separate layer.&lt;/p&gt;&lt;p&gt;“We are building a fantastic foundation on OCI,” Chess says. “That infrastructure provides more than compute power.” &lt;/p&gt;&lt;p&gt;Built on the same OCI foundation that powers NetSuite today, NetSuite Next gives customers access to Oracle’s latest AI innovations along with the performance, scalability, and security of OCI’s enterprise-grade platform.&lt;/p&gt;&lt;p&gt;Wiessinger emphasizes the team&amp;#x27;s approach as “needs first, technology second.”&lt;/p&gt;&lt;p&gt;“We don’t take a technology-first approach,” he says. “We take a customer-needs-first approach and then figure out how to use the latest technology to solve those needs better.”&lt;/p&gt;&lt;p&gt;That philosophy extends across Oracle’s ecosystem. NetSuite’s collaboration with Oracle’s AI Database, Fusion Applications, Analytics, and Cloud Infrastructure teams helps NetSuite deliver capabilities that independent vendors can’t match, he says — an AI system that is both open to innovation and grounded in Oracle’s security and scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The data structure advantage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At the heart of the platform is a structured data model that serves as a critical advantage.&lt;/p&gt;&lt;p&gt;“One of the great things about NetSuite is, because the data comes in and it gets structured, the connections between the data are explicit,” Chess explains. “That means the AI can start exploring that knowledge graph that the company has been building up.”&lt;/p&gt;&lt;p&gt;Where general LLMs sift through unstructured text, NetSuite’s AI works from structured data, identifying precise links between transactions, accounts, and workflows to deliver context-aware insights. &lt;/p&gt;&lt;p&gt;Wiessinger adds, “The data we have spans financials, CRM, commerce, and HR. We can do more for customers because we see more of their business in one place.” &lt;/p&gt;&lt;p&gt;Combined with built-in business logic and metadata, that scope allows NetSuite to generate recommendations and insights that are accurate and explainable.&lt;/p&gt;&lt;p&gt;Oracle’s Redwood design system provides the visual layer for this data intelligence, creating what Goldberg described as a &amp;quot;modern, clean and intuitive&amp;quot; workspace where AI and humans collaborate naturally.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Designing for accountability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;One downside of enterprise AI is that many systems still function as a black box — they produce results but offer little visibility into how they reached them. NetSuite is different. It is designing its systems around transparency, making visibility a defining feature.&lt;/p&gt;&lt;p&gt;“When users can see how AI reached a decision — tracing the path from A to B — they don’t just verify accuracy,” Chess says. “They learn how the AI knew to do that.”&lt;/p&gt;&lt;p&gt;That visibility turns AI into a learning engine. As Chess puts it, transparency becomes a “fantastic teacher,” helping organizations understand, improve, and trust automation over time.&lt;/p&gt;&lt;p&gt;But Chess cautions against blind trust: “What’s disturbing is when someone presents something to me and says, ‘Look what AI gave me,’ as if that makes it authoritative. People need to ask, ‘&lt;i&gt;What grounded this? Why is it correct?’&lt;/i&gt;” &lt;/p&gt;&lt;p&gt;NetSuite’s answer is traceability. When someone asks, “Where did this number come from?” the system can show them the full reasoning behind it.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Governance by design&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;AI agents inside NetSuite Next follow the same governance model as employees: roles, permissions, and escalation rules. Role-based security embedded directly into workflows helps ensure that agents act only within authorized boundaries.&lt;/p&gt;&lt;p&gt;Wiessinger puts it plainly: “If AI generates a narrative summary of a report and it’s 80% of what the user would have written, that’s fine. We’ll learn from their feedback and make it even better. But booking to the general ledger is different. That has to be 100% correct and is where controls and human review really matter.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Auditing the algorithm&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Auditing has always been part of ERP’s DNA, and NetSuite now extends that discipline to AI. Every agent action, workflow adjustment, and model-generated code snippet is recorded within the system’s existing audit framework. &lt;/p&gt;&lt;p&gt;As Chess explains, “It’s the same audit trail you might use to figure out what the humans did. Code is auditable. When the LLM creates code and something happens in the system, we can trace back.”&lt;/p&gt;&lt;p&gt;That traceability transforms AI from a black box into a glass box. When an algorithm accelerates a payment or flags an anomaly, teams can see exactly which inputs and logic produced the decision — an essential safeguard for regulated industries and finance teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safe extensibility&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The other half of trust is freedom — the ability to extend AI without risking data exposure.&lt;/p&gt;&lt;p&gt;The NetSuite AI Connector Service and SuiteCloud Platform make that possible. Through standards like the Model Context Protocol (MCP), customers can connect external language models while keeping sensitive data secure inside Oracle’s environment.&lt;/p&gt;&lt;p&gt;“Businesses are hungry for AI,” Chess says. “They want to start putting it to work. But they also want to know those experiments can’t go off the rails. The NetSuite AI Connector Service and governance model give partners the freedom to innovate while maintaining the same audit and permission logic that govern native features.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Culture, experimentation, and guardrails&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Governance frameworks only work if people use them wisely. Both executives see AI adoption as a top-down and bottom-up process.&lt;/p&gt;&lt;p&gt;“The board is telling the CEO they need an AI strategy,” Chess says. “Meanwhile, employees are already using AI. If I were a CEO, I’d start by asking: what are you already doing, and what’s working?”&lt;/p&gt;&lt;p&gt;Wiessinger agrees that balance is key: “Some companies go all-in on a centralized AI team while others let everyone experiment freely. Neither works by itself. You need structure for major initiatives and freedom for grassroots innovation.”&lt;/p&gt;&lt;p&gt;He offers a simple example: “Write an email? Go crazy. Touch financials or employee data? Don’t go crazy with that.”&lt;/p&gt;&lt;p&gt;Experimentation, both emphasize, is imperative. “No one should wait for us or anyone else,” Wiessinger says. “Start testing, learn quickly, and be intentional about making it work for your business.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why transparent AI wins&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;As AI moves deeper into enterprise operations, governance will define competitive advantage as much as innovation. NetSuite’s approach — extending its heritage of ERP controls into the age of autonomous systems, built on Oracle’s secure cloud infrastructure and structured-data foundation — positions it to lead in both.&lt;/p&gt;&lt;p&gt;In a world of opaque models and risky promises, the companies that win won’t just build smarter AI. They’ll build AI you can trust.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;i&gt;Presented by Oracle NetSuite&lt;/i&gt;&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;When any company tells you it is their biggest product release in almost three decades, it’s worth listening. When the person saying it founded the world’s first cloud computing company, it’s time to take note. &lt;/p&gt;&lt;p&gt;At SuiteWorld 2025, Evan Goldberg, founder and EVP of Oracle NetSuite, did just that when he called NetSuite Next the company’s biggest product evolution in nearly three decades. But behind that sweeping vision lies a quieter shift — one centered on how AI behaves, not just what it can do. &lt;/p&gt;&lt;p&gt;“Every company is experimenting with AI,” says Brian Chess, SVP of Technology and AI at NetSuite. “Some ideas hit the mark, and some don’t, but each one teaches us something. That’s how innovation works.”&lt;/p&gt;&lt;p&gt;For Chess and Gary Wiessinger, SVP of Application Development at NetSuite, the challenge lies in governing AI responsibly. Rather than reinventing its system, NetSuite is extending the same principles into the AI era that have guided its strategy for 27 years — security, control, and auditability. The goal is to make AI actions traceable, permissions enforceable, and outcomes auditable.&lt;/p&gt;&lt;p&gt;The philosophy underpins what Chess calls a “glass-box” approach to enterprise AI, where decisions are visible and every agent operates within human-defined guardrails.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Built on Oracle’s foundation&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;NetSuite Next is the result of five years of development. It is built on Oracle Cloud Infrastructure (OCI), which is relied on by many of the world’s most important AI model providers, and has AI capabilities integrated directly into its core rather than added as a separate layer.&lt;/p&gt;&lt;p&gt;“We are building a fantastic foundation on OCI,” Chess says. “That infrastructure provides more than compute power.” &lt;/p&gt;&lt;p&gt;Built on the same OCI foundation that powers NetSuite today, NetSuite Next gives customers access to Oracle’s latest AI innovations along with the performance, scalability, and security of OCI’s enterprise-grade platform.&lt;/p&gt;&lt;p&gt;Wiessinger emphasizes the team&amp;#x27;s approach as “needs first, technology second.”&lt;/p&gt;&lt;p&gt;“We don’t take a technology-first approach,” he says. “We take a customer-needs-first approach and then figure out how to use the latest technology to solve those needs better.”&lt;/p&gt;&lt;p&gt;That philosophy extends across Oracle’s ecosystem. NetSuite’s collaboration with Oracle’s AI Database, Fusion Applications, Analytics, and Cloud Infrastructure teams helps NetSuite deliver capabilities that independent vendors can’t match, he says — an AI system that is both open to innovation and grounded in Oracle’s security and scale.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;The data structure advantage&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;At the heart of the platform is a structured data model that serves as a critical advantage.&lt;/p&gt;&lt;p&gt;“One of the great things about NetSuite is, because the data comes in and it gets structured, the connections between the data are explicit,” Chess explains. “That means the AI can start exploring that knowledge graph that the company has been building up.”&lt;/p&gt;&lt;p&gt;Where general LLMs sift through unstructured text, NetSuite’s AI works from structured data, identifying precise links between transactions, accounts, and workflows to deliver context-aware insights. &lt;/p&gt;&lt;p&gt;Wiessinger adds, “The data we have spans financials, CRM, commerce, and HR. We can do more for customers because we see more of their business in one place.” &lt;/p&gt;&lt;p&gt;Combined with built-in business logic and metadata, that scope allows NetSuite to generate recommendations and insights that are accurate and explainable.&lt;/p&gt;&lt;p&gt;Oracle’s Redwood design system provides the visual layer for this data intelligence, creating what Goldberg described as a &amp;quot;modern, clean and intuitive&amp;quot; workspace where AI and humans collaborate naturally.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Designing for accountability&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;One downside of enterprise AI is that many systems still function as a black box — they produce results but offer little visibility into how they reached them. NetSuite is different. It is designing its systems around transparency, making visibility a defining feature.&lt;/p&gt;&lt;p&gt;“When users can see how AI reached a decision — tracing the path from A to B — they don’t just verify accuracy,” Chess says. “They learn how the AI knew to do that.”&lt;/p&gt;&lt;p&gt;That visibility turns AI into a learning engine. As Chess puts it, transparency becomes a “fantastic teacher,” helping organizations understand, improve, and trust automation over time.&lt;/p&gt;&lt;p&gt;But Chess cautions against blind trust: “What’s disturbing is when someone presents something to me and says, ‘Look what AI gave me,’ as if that makes it authoritative. People need to ask, ‘&lt;i&gt;What grounded this? Why is it correct?’&lt;/i&gt;” &lt;/p&gt;&lt;p&gt;NetSuite’s answer is traceability. When someone asks, “Where did this number come from?” the system can show them the full reasoning behind it.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Governance by design&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;AI agents inside NetSuite Next follow the same governance model as employees: roles, permissions, and escalation rules. Role-based security embedded directly into workflows helps ensure that agents act only within authorized boundaries.&lt;/p&gt;&lt;p&gt;Wiessinger puts it plainly: “If AI generates a narrative summary of a report and it’s 80% of what the user would have written, that’s fine. We’ll learn from their feedback and make it even better. But booking to the general ledger is different. That has to be 100% correct and is where controls and human review really matter.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Auditing the algorithm&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Auditing has always been part of ERP’s DNA, and NetSuite now extends that discipline to AI. Every agent action, workflow adjustment, and model-generated code snippet is recorded within the system’s existing audit framework. &lt;/p&gt;&lt;p&gt;As Chess explains, “It’s the same audit trail you might use to figure out what the humans did. Code is auditable. When the LLM creates code and something happens in the system, we can trace back.”&lt;/p&gt;&lt;p&gt;That traceability transforms AI from a black box into a glass box. When an algorithm accelerates a payment or flags an anomaly, teams can see exactly which inputs and logic produced the decision — an essential safeguard for regulated industries and finance teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safe extensibility&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The other half of trust is freedom — the ability to extend AI without risking data exposure.&lt;/p&gt;&lt;p&gt;The NetSuite AI Connector Service and SuiteCloud Platform make that possible. Through standards like the Model Context Protocol (MCP), customers can connect external language models while keeping sensitive data secure inside Oracle’s environment.&lt;/p&gt;&lt;p&gt;“Businesses are hungry for AI,” Chess says. “They want to start putting it to work. But they also want to know those experiments can’t go off the rails. The NetSuite AI Connector Service and governance model give partners the freedom to innovate while maintaining the same audit and permission logic that govern native features.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Culture, experimentation, and guardrails&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Governance frameworks only work if people use them wisely. Both executives see AI adoption as a top-down and bottom-up process.&lt;/p&gt;&lt;p&gt;“The board is telling the CEO they need an AI strategy,” Chess says. “Meanwhile, employees are already using AI. If I were a CEO, I’d start by asking: what are you already doing, and what’s working?”&lt;/p&gt;&lt;p&gt;Wiessinger agrees that balance is key: “Some companies go all-in on a centralized AI team while others let everyone experiment freely. Neither works by itself. You need structure for major initiatives and freedom for grassroots innovation.”&lt;/p&gt;&lt;p&gt;He offers a simple example: “Write an email? Go crazy. Touch financials or employee data? Don’t go crazy with that.”&lt;/p&gt;&lt;p&gt;Experimentation, both emphasize, is imperative. “No one should wait for us or anyone else,” Wiessinger says. “Start testing, learn quickly, and be intentional about making it work for your business.”&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Why transparent AI wins&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;As AI moves deeper into enterprise operations, governance will define competitive advantage as much as innovation. NetSuite’s approach — extending its heritage of ERP controls into the age of autonomous systems, built on Oracle’s secure cloud infrastructure and structured-data foundation — positions it to lead in both.&lt;/p&gt;&lt;p&gt;In a world of opaque models and risky promises, the companies that win won’t just build smarter AI. They’ll build AI you can trust.&lt;/p&gt;&lt;hr /&gt;&lt;p&gt;&lt;i&gt;Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. For more information, contact &lt;/i&gt;&lt;a href="mailto:sales@venturebeat.com"&gt;&lt;i&gt;&lt;u&gt;sales@venturebeat.com&lt;/u&gt;&lt;/i&gt;&lt;/a&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/creating-a-glass-box-how-netsuite-is-engineering-trust-into-ai</guid><pubDate>Thu, 11 Dec 2025 05:00:00 +0000</pubDate></item><item><title>[NEW] Harness hits $5.5B valuation with $240M raise to automate AI’s ‘after-code’ gap (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/harness-hits-5-5b-valuation-with-240m-to-automate-ais-after-code-gap/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI DevOps tool Harness, founded in 2017 by serial entrepreneur Jyoti Bansal, is on track to exceed $250 million in annual recurring revenue in 2025, Bansal tells TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup just raised a fresh $240 million Series E funding that values the company at $5.5 billion post-money. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round includes a $200 million primary investment led by Goldman Sachs and a planned $40 million tender offer with participation from IVP, Menlo Ventures, and Unusual Ventures. The tender offer is intended to provide some liquidity to its long-term employees, Bansal said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new valuation is a 49% jump from its $3.7 billion valuation in a $230 million round in April 2022. With this funding, the startup has raised $570 million of equity to date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As AI accelerates code production, it is widening a bottleneck in the far larger “after-code” phase of software development — the testing, security checks, and deployment work that still consumes nearly 70% of engineering time. Harness’s tools help automate this sprawling, error-prone layer, even as enterprises grapple with rising AI code volume and the risks of shipping even a single line of faulty software into production systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bansal is well known among developers for building and selling app performance company AppDynamics to Cisco for $3.7 billion in 2017. So the post-coding world is an area Bansal knows well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harness uses AI agents to automate functions like testing, verification, security, and governance. It is built on a software delivery knowledge graph that maps code changes, services, deployments, tests, environments, incidents, policies, and costs. The knowledge graph helps differentiate Harness from other AI platforms, Bansal said, because it gives the system a deep understanding of each customer’s software delivery processes and architecture.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“This knowledge graph is the context that our AI agents use,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The purpose-built agents draw on that context to generate pipelines that match each customer’s specific policies, architecture and operational requirements.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harness also uses an orchestration engine that turns the AI’s recommendations into automated actions, with checks in place to make sure those changes are applied safely.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3074986" height="1049" src="https://techcrunch.com/wp-content/uploads/2025/12/harness-ai.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Harness&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As AI is not foolproof, Bansal said the system is designed with human oversight, noting that AI-generated tests or fixes are reviewed by engineers, compliance teams, or auditors before being put into use.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft’s GitHub, GitLab, Jenkins, and CloudBees are among the key competitors for Harness. But Harness has plenty of traction, claiming more than 1,000 enterprise customers, including United Airlines, Morningstar, Keller Williams and National Australia Bank. So far, the startup has handled 128 million deployments, 81 million builds, protected 1.2 trillion API calls, and helped customers optimize $1.9 billion in cloud spending over the past year, Bansal touts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The San Francisco–based company employs over 1,200 people across 14 offices worldwide, including in Europe and the U.K. Around 33% of its workforce is in India, where it has a large engineering team in Bengaluru and a corporate office in Gurugram. Moreover, the Bengaluru site is Harness’s biggest development center outside the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harness plans to use the new funding to expand its R&amp;amp;D efforts, hire “hundreds of engineers” at its Bengaluru office, and build out additional automated testing, deployment, and security capabilities while improving the accuracy of its AI systems. The company also intends to strengthen its U.S. go-to-market operations and significantly expand its presence in international markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It should also be noted that earlier this year, Bansal merged his software observability firm Traceable with Harness, and that move has helped the startup grow its ARR projection.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We brought the two companies together because we started to see that DevOps and application security are coming together in a very, very deep way,” said Bansal. “We have seen that turned out to be a very, very successful thesis this year … that’s driving a lot of growth for both of our DevOps and application security set of products.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this raise has allowed some employees to cash out a bit,  Bansal still plans on taking Harness public one day, he said, though he did not share a specific timeline.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That’s what our goals and plans depend on,” he said of an eventual IPO. “Our business is very, very healthy, very strong, high growth and margins, and it will be a great public company when the timing is right.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI DevOps tool Harness, founded in 2017 by serial entrepreneur Jyoti Bansal, is on track to exceed $250 million in annual recurring revenue in 2025, Bansal tells TechCrunch. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup just raised a fresh $240 million Series E funding that values the company at $5.5 billion post-money. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The round includes a $200 million primary investment led by Goldman Sachs and a planned $40 million tender offer with participation from IVP, Menlo Ventures, and Unusual Ventures. The tender offer is intended to provide some liquidity to its long-term employees, Bansal said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new valuation is a 49% jump from its $3.7 billion valuation in a $230 million round in April 2022. With this funding, the startup has raised $570 million of equity to date.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As AI accelerates code production, it is widening a bottleneck in the far larger “after-code” phase of software development — the testing, security checks, and deployment work that still consumes nearly 70% of engineering time. Harness’s tools help automate this sprawling, error-prone layer, even as enterprises grapple with rising AI code volume and the risks of shipping even a single line of faulty software into production systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bansal is well known among developers for building and selling app performance company AppDynamics to Cisco for $3.7 billion in 2017. So the post-coding world is an area Bansal knows well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harness uses AI agents to automate functions like testing, verification, security, and governance. It is built on a software delivery knowledge graph that maps code changes, services, deployments, tests, environments, incidents, policies, and costs. The knowledge graph helps differentiate Harness from other AI platforms, Bansal said, because it gives the system a deep understanding of each customer’s software delivery processes and architecture.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“This knowledge graph is the context that our AI agents use,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The purpose-built agents draw on that context to generate pipelines that match each customer’s specific policies, architecture and operational requirements.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harness also uses an orchestration engine that turns the AI’s recommendations into automated actions, with checks in place to make sure those changes are applied safely.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3074986" height="1049" src="https://techcrunch.com/wp-content/uploads/2025/12/harness-ai.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Harness&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As AI is not foolproof, Bansal said the system is designed with human oversight, noting that AI-generated tests or fixes are reviewed by engineers, compliance teams, or auditors before being put into use.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Microsoft’s GitHub, GitLab, Jenkins, and CloudBees are among the key competitors for Harness. But Harness has plenty of traction, claiming more than 1,000 enterprise customers, including United Airlines, Morningstar, Keller Williams and National Australia Bank. So far, the startup has handled 128 million deployments, 81 million builds, protected 1.2 trillion API calls, and helped customers optimize $1.9 billion in cloud spending over the past year, Bansal touts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The San Francisco–based company employs over 1,200 people across 14 offices worldwide, including in Europe and the U.K. Around 33% of its workforce is in India, where it has a large engineering team in Bengaluru and a corporate office in Gurugram. Moreover, the Bengaluru site is Harness’s biggest development center outside the U.S.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harness plans to use the new funding to expand its R&amp;amp;D efforts, hire “hundreds of engineers” at its Bengaluru office, and build out additional automated testing, deployment, and security capabilities while improving the accuracy of its AI systems. The company also intends to strengthen its U.S. go-to-market operations and significantly expand its presence in international markets.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It should also be noted that earlier this year, Bansal merged his software observability firm Traceable with Harness, and that move has helped the startup grow its ARR projection.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We brought the two companies together because we started to see that DevOps and application security are coming together in a very, very deep way,” said Bansal. “We have seen that turned out to be a very, very successful thesis this year … that’s driving a lot of growth for both of our DevOps and application security set of products.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While this raise has allowed some employees to cash out a bit,  Bansal still plans on taking Harness public one day, he said, though he did not share a specific timeline.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“That’s what our goals and plans depend on,” he said of an eventual IPO. “Our business is very, very healthy, very strong, high growth and margins, and it will be a great public company when the timing is right.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/harness-hits-5-5b-valuation-with-240m-to-automate-ais-after-code-gap/</guid><pubDate>Thu, 11 Dec 2025 10:30:00 +0000</pubDate></item><item><title>[NEW] Solar geoengineering startups are getting serious (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/12/11/1129239/solar-geoengineering-startups-are-getting-serious/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-2155252547.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Solar geoengineering aims to manipulate the climate by bouncing sunlight back into space. In theory, it could ease global warming. But as interest in the idea grows, so do concerns about potential consequences.&lt;/p&gt;  &lt;p&gt;A startup called Stardust Solutions recently raised a $60 million funding round, the largest known to date for a geoengineering startup. My colleague James Temple has a new story out about the company, and how its emergence is making some researchers nervous.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;So far, the field has been limited to debates, proposed academic research, and—sure—a few fringe actors to keep an eye on. Now things are getting more serious. What does it mean for geoengineering, and for the climate?&lt;/p&gt;  &lt;p&gt;Researchers have considered the possibility of addressing planetary warming this way for decades. We already know that volcanic eruptions, which spew sulfur dioxide into the atmosphere, can reduce temperatures. The thought is that we could mimic that natural process by spraying particles up there ourselves.&lt;/p&gt; 
 &lt;p&gt;The prospect is a controversial one, to put it lightly. Many have concerns about unintended consequences and uneven benefits. Even public research led by top institutions has faced barriers—one famous Harvard research program was officially canceled last year after years of debate.&lt;/p&gt;  &lt;p&gt;One of the difficulties of geoengineering is that in theory a single entity, like a startup company, could make decisions that have a widespread effect on the planet. And in the last few years, we’ve seen more interest in geoengineering from the private sector.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Three years ago, James broke the story that Make Sunsets, a California-based company, was already releasing particles into the atmosphere in an effort to tweak the climate.&lt;/p&gt;  &lt;p&gt;The company’s CEO Luke Iseman went to Baja California in Mexico, stuck some sulfur dioxide into a weather balloon, and sent it skyward. The amount of material was tiny, and it’s not clear that it even made it into the right part of the atmosphere to reflect any sunlight.&lt;/p&gt;  &lt;p&gt;But fears that this group or others could go rogue and do their own geoengineering led to widespread backlash. Mexico announced plans to restrict geoengineering experiments in the country a few weeks after that news broke.&lt;/p&gt;  &lt;p&gt;You can still buy cooling credits from Make Sunsets, and the company was just granted a patent for its system. But the startup is seen as something of a fringe actor.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Enter Stardust Solutions. The company has been working under the radar for a few years, but it has started talking about its work more publicly this year. In October, it announced a significant funding round, led by some top names in climate investing. “Stardust is serious, and now it’s raised serious money from serious people,” as James puts it in his new story.&lt;/p&gt;  &lt;p&gt;That’s making some experts nervous. Even those who believe we should be researching geoengineering are concerned about what it means for private companies to do so.&lt;/p&gt;  &lt;p&gt;“Adding business interests, profit motives, and rich investors into this situation just creates more cause for concern, complicating the ability of responsible scientists and engineers to carry out the work needed to advance our understanding,” write David Keith and Daniele Visioni, two leading figures in geoengineering research, in a recent opinion piece for &lt;em&gt;MIT Technology Review&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;Stardust insists that it won’t move forward with any geoengineering until and unless it’s commissioned to do so by governments and there are rules and bodies in place to govern use of the technology.&lt;/p&gt; 

 &lt;p&gt;But there’s no telling how financial pressure might change that, down the road. And we’re already seeing some of the challenges faced by a private company in this space: the need to keep trade secrets.&lt;/p&gt;  &lt;p&gt;Stardust is currently not sharing information about the particles it intends to release into the sky, though it says it plans to do so once it secures a patent, which could happen as soon as next year. The company argues that its proprietary particles will be safe, cheap to manufacture, and easier to track than the already abundant sulfur dioxide. But at this point, there’s no way for external experts to evaluate those claims.&lt;/p&gt;  &lt;p&gt;As Keith and Visioni put it: “Research won’t be useful unless it’s trusted, and trust depends on transparency.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-2155252547.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Solar geoengineering aims to manipulate the climate by bouncing sunlight back into space. In theory, it could ease global warming. But as interest in the idea grows, so do concerns about potential consequences.&lt;/p&gt;  &lt;p&gt;A startup called Stardust Solutions recently raised a $60 million funding round, the largest known to date for a geoengineering startup. My colleague James Temple has a new story out about the company, and how its emergence is making some researchers nervous.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;So far, the field has been limited to debates, proposed academic research, and—sure—a few fringe actors to keep an eye on. Now things are getting more serious. What does it mean for geoengineering, and for the climate?&lt;/p&gt;  &lt;p&gt;Researchers have considered the possibility of addressing planetary warming this way for decades. We already know that volcanic eruptions, which spew sulfur dioxide into the atmosphere, can reduce temperatures. The thought is that we could mimic that natural process by spraying particles up there ourselves.&lt;/p&gt; 
 &lt;p&gt;The prospect is a controversial one, to put it lightly. Many have concerns about unintended consequences and uneven benefits. Even public research led by top institutions has faced barriers—one famous Harvard research program was officially canceled last year after years of debate.&lt;/p&gt;  &lt;p&gt;One of the difficulties of geoengineering is that in theory a single entity, like a startup company, could make decisions that have a widespread effect on the planet. And in the last few years, we’ve seen more interest in geoengineering from the private sector.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Three years ago, James broke the story that Make Sunsets, a California-based company, was already releasing particles into the atmosphere in an effort to tweak the climate.&lt;/p&gt;  &lt;p&gt;The company’s CEO Luke Iseman went to Baja California in Mexico, stuck some sulfur dioxide into a weather balloon, and sent it skyward. The amount of material was tiny, and it’s not clear that it even made it into the right part of the atmosphere to reflect any sunlight.&lt;/p&gt;  &lt;p&gt;But fears that this group or others could go rogue and do their own geoengineering led to widespread backlash. Mexico announced plans to restrict geoengineering experiments in the country a few weeks after that news broke.&lt;/p&gt;  &lt;p&gt;You can still buy cooling credits from Make Sunsets, and the company was just granted a patent for its system. But the startup is seen as something of a fringe actor.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__container--08c53dd3bc9bd04e1e42e5f7ca641ab2"&gt;&lt;div class="whyItMatters__header--19f7f372f181cc6d4c06bc7362a44382"&gt;&lt;div class="whyItMatters__title--4af28c786a2bc93df05db111c6c30618"&gt;&lt;span class="whyItMatters__askAi--577f5fe6f54de43e37258d0f2aff4394"&gt;Ask AI&lt;/span&gt;&lt;div&gt;&lt;span class="whyItMatters__whyItMattersTitle--a3694998bb578e159bbd16690b8da390"&gt;Why it matters to you?&lt;/span&gt;&lt;span class="whyItMatters__betaBadge--9e84228b864d33d5b55479433fc91b8a"&gt;BETA&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div class="whyItMatters__description--e1334886c092fa469388d7a24e1e1a55"&gt;&lt;span class="initial-description"&gt;Here’s why this story might matter to you, according to AI. This is a beta feature and AI hallucinates—it might get weird&lt;/span&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="whyItMatters__questionContainer--ec1159210954852b9178c549600959a0"&gt;&lt;div&gt;&lt;button class="whyItMatters__actionButton--674934b6df433ac81e613372979cdb6c" type="button"&gt;Tell me why it matters&lt;/button&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;Enter Stardust Solutions. The company has been working under the radar for a few years, but it has started talking about its work more publicly this year. In October, it announced a significant funding round, led by some top names in climate investing. “Stardust is serious, and now it’s raised serious money from serious people,” as James puts it in his new story.&lt;/p&gt;  &lt;p&gt;That’s making some experts nervous. Even those who believe we should be researching geoengineering are concerned about what it means for private companies to do so.&lt;/p&gt;  &lt;p&gt;“Adding business interests, profit motives, and rich investors into this situation just creates more cause for concern, complicating the ability of responsible scientists and engineers to carry out the work needed to advance our understanding,” write David Keith and Daniele Visioni, two leading figures in geoengineering research, in a recent opinion piece for &lt;em&gt;MIT Technology Review&lt;/em&gt;.&lt;/p&gt;  &lt;p&gt;Stardust insists that it won’t move forward with any geoengineering until and unless it’s commissioned to do so by governments and there are rules and bodies in place to govern use of the technology.&lt;/p&gt; 

 &lt;p&gt;But there’s no telling how financial pressure might change that, down the road. And we’re already seeing some of the challenges faced by a private company in this space: the need to keep trade secrets.&lt;/p&gt;  &lt;p&gt;Stardust is currently not sharing information about the particles it intends to release into the sky, though it says it plans to do so once it secures a patent, which could happen as soon as next year. The company argues that its proprietary particles will be safe, cheap to manufacture, and easier to track than the already abundant sulfur dioxide. But at this point, there’s no way for external experts to evaluate those claims.&lt;/p&gt;  &lt;p&gt;As Keith and Visioni put it: “Research won’t be useful unless it’s trusted, and trust depends on transparency.”&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/12/11/1129239/solar-geoengineering-startups-are-getting-serious/</guid><pubDate>Thu, 11 Dec 2025 11:00:00 +0000</pubDate></item><item><title>[NEW] Port raises $100M at $800M valuation to take on Spotify’s Backstage (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/12/11/port-raises-100m-at-800m-valuation-to-take-on-spotifys-backstage/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Port-founders-e1764962616421.jpg?resize=1200,579" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify may be synonymous with music streaming, but it’s also got a wildly popular developer-tool side-hustle called “Backstage.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Backstage is an open source project that helps companies build their own internal developer portals: a catalog of their developer tools along with quick visualizations of the work the tools have done, and other metrics. But like many open-source projects, Backstage is a build-it-yourself option.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Israeli startup Port has been gaining big-name customers like GitHub, British Telecom and LG with a proprietary Backstage competitor: a dev tool portal that’s also now been geared to manage AI agents.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Port, founded in 2022, said it raised a fresh $100 million Series C round led by General Atlantic, with participation from Accel, Bessemer Venture Partners and Team8. The round values Port at $800 million and brings its total funding to date to $158 million. This Series C follows the company’s $35 million Series B led by Accel and Bessemer, announced in May.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of all the industries that LLM-based tech has infiltrated, coding is where it has the deepest roots. So, not surprisingly, developers are also on the cutting edge of building and adopting agents that can automate entire repeated processes — work far beyond asking AI to write some code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the problem here, according to Port co-founder and CEO Zohar Einy, it’s the wild west right now for such devtool agents at companies: finding them, sharing them, ensuring their work follows company standards and so on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Developers “want to take AI beyond just coding. They want it to resolve incidents, resolve security issues. They want it to take care of the release management,” Einy told TechCrunch.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But if agents are connected to all kinds of different tools and data sources, if the data is scattered among them, if they have no way to collaborate, and have no corporate standards and guardrails, “it creates chaos,” his product pitch goes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Port therefore offers more than just a catalog of dev and agent tools (although it does offer that). It supplies a layer of orchestration&amp;nbsp;with features that measure agent performance and&amp;nbsp;add a human-in-the-loop, as desired, to approval processes. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A feature called “context lake” defines the&amp;nbsp;data sources, context memory, and guardrails for agents. “It’s where you manage what agents ‘need to know’ to do their job safely and correctly,” Einy explained.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition to using Port&amp;nbsp;to catalog agents devs have already created using other tools, they can use Port to create new agents. Plus, Port also offers a few of its own ready-made agents, which can do things like resolving helpdesk tickets and dealing with provisioning.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Einy describes his product as handling the other 90% of what software programmers do that isn’t writing code. “It gives the engineers a user interface to control the agent, to iterate with the agent, to approve what it does that is not coding, that is all the 90%.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With its giant new war chest of cash, big name customers and tier-one VCs, Port looks like an agentic management startup to watch. But to say it faces competition is an understatement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The entire category of agentic management and orchestration is flooded with hopefuls, from big tech companies to startups, and they’re all coming at the various new problems in the space from different angles. A few of these include&amp;nbsp;LangChain, UiPath, Cortex and more.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/12/Port-founders-e1764962616421.jpg?resize=1200,579" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify may be synonymous with music streaming, but it’s also got a wildly popular developer-tool side-hustle called “Backstage.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Backstage is an open source project that helps companies build their own internal developer portals: a catalog of their developer tools along with quick visualizations of the work the tools have done, and other metrics. But like many open-source projects, Backstage is a build-it-yourself option.&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Israeli startup Port has been gaining big-name customers like GitHub, British Telecom and LG with a proprietary Backstage competitor: a dev tool portal that’s also now been geared to manage AI agents.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Thursday, Port, founded in 2022, said it raised a fresh $100 million Series C round led by General Atlantic, with participation from Accel, Bessemer Venture Partners and Team8. The round values Port at $800 million and brings its total funding to date to $158 million. This Series C follows the company’s $35 million Series B led by Accel and Bessemer, announced in May.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of all the industries that LLM-based tech has infiltrated, coding is where it has the deepest roots. So, not surprisingly, developers are also on the cutting edge of building and adopting agents that can automate entire repeated processes — work far beyond asking AI to write some code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the problem here, according to Port co-founder and CEO Zohar Einy, it’s the wild west right now for such devtool agents at companies: finding them, sharing them, ensuring their work follows company standards and so on.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Developers “want to take AI beyond just coding. They want it to resolve incidents, resolve security issues. They want it to take care of the release management,” Einy told TechCrunch.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But if agents are connected to all kinds of different tools and data sources, if the data is scattered among them, if they have no way to collaborate, and have no corporate standards and guardrails, “it creates chaos,” his product pitch goes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Port therefore offers more than just a catalog of dev and agent tools (although it does offer that). It supplies a layer of orchestration&amp;nbsp;with features that measure agent performance and&amp;nbsp;add a human-in-the-loop, as desired, to approval processes. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A feature called “context lake” defines the&amp;nbsp;data sources, context memory, and guardrails for agents. “It’s where you manage what agents ‘need to know’ to do their job safely and correctly,” Einy explained.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In addition to using Port&amp;nbsp;to catalog agents devs have already created using other tools, they can use Port to create new agents. Plus, Port also offers a few of its own ready-made agents, which can do things like resolving helpdesk tickets and dealing with provisioning.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Einy describes his product as handling the other 90% of what software programmers do that isn’t writing code. “It gives the engineers a user interface to control the agent, to iterate with the agent, to approve what it does that is not coding, that is all the 90%.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With its giant new war chest of cash, big name customers and tier-one VCs, Port looks like an agentic management startup to watch. But to say it faces competition is an understatement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The entire category of agentic management and orchestration is flooded with hopefuls, from big tech companies to startups, and they’re all coming at the various new problems in the space from different angles. A few of these include&amp;nbsp;LangChain, UiPath, Cortex and more.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/12/11/port-raises-100m-at-800m-valuation-to-take-on-spotifys-backstage/</guid><pubDate>Thu, 11 Dec 2025 11:00:00 +0000</pubDate></item></channel></rss>