<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 16 Oct 2025 06:31:47 +0000</lastBuildDate><item><title>Google releases new AI video model Veo 3.1 in Flow and API: what it means for enterprises (AI | VentureBeat)</title><link>https://venturebeat.com/ai/google-releases-new-ai-video-model-veo-3-1-in-flow-and-api-what-it-means-for</link><description>[unable to retrieve full-text content]&lt;p&gt;As expected after days of leaks and rumors online, Google has &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;unveiled Veo 3.1&lt;/a&gt;, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video. &lt;/p&gt;&lt;p&gt;While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app, &lt;a href="https://labs.google/fx/tools/flow"&gt;Flow&lt;/a&gt;, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.&lt;/p&gt;&lt;p&gt;The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;My &lt;a href="https://x.com/carlfranzen/status/1978522697014411322"&gt;initial tests&lt;/a&gt; showed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more &amp;quot;artificial&amp;quot; than by default than rivals such as &lt;a href="https://venturebeat.com/ai/openai-debuts-sora-2-ai-video-generator-app-with-sound-and-self-insertion"&gt;OpenAI&amp;#x27;s new Sora 2&lt;/a&gt;, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and &amp;quot;candid&amp;quot; style videos). &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Expanded Control Over Narrative and Audio&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 builds on its predecessor, Veo 3 (&lt;a href="https://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html"&gt;released back in May 2025&lt;/a&gt;) with enhanced support for dialogue, ambient sound, and other audio effects. &lt;/p&gt;&lt;p&gt;Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,&amp;quot; which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip&amp;#x27;s final frame. &lt;/p&gt;&lt;p&gt;Before, you had to add audio manually after using these features. &lt;/p&gt;&lt;p&gt;This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.&lt;/p&gt;&lt;p&gt;In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.&lt;/p&gt;&lt;p&gt;Google noted in &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;a blog post&lt;/a&gt; that the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Richer Inputs and Editing Capabilities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Reference images (up to three)&lt;/b&gt; to guide appearance and style in the final output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;First and last frame interpolation&lt;/b&gt; to generate seamless scenes between fixed endpoints&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Scene extension&lt;/b&gt; that continues a video’s action or motion beyond its current duration&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.&lt;/p&gt;&lt;p&gt;Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment Across Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is accessible through several of Google’s existing AI services:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://labs.google/fx/tools/flow"&gt;&lt;b&gt;Flow&lt;/b&gt;&lt;/a&gt;, Google’s own interface for AI-assisted filmmaking&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/video?example=dialogue"&gt;&lt;b&gt;Gemini API&lt;/b&gt;&lt;/a&gt;, targeted at developers building video capabilities into applications&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo-video-generation"&gt;&lt;b&gt;Vertex AI&lt;/b&gt;&lt;/a&gt;, where enterprise integration will soon support Veo’s “Scene Extension” and other key features&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Availability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Pricing and Access&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Veo 3.1 model is currently in &lt;b&gt;preview&lt;/b&gt; and available only on the &lt;b&gt;paid tier&lt;/b&gt; of the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Standard model&lt;/b&gt;: $0.40 per second of video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Fast model&lt;/b&gt;: $0.15 per second&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Specs and Output Control&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 outputs video at &lt;b&gt;720p or 1080p resolution&lt;/b&gt;, with a &lt;b&gt;24 fps frame rate&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;Duration options include &lt;b&gt;4, 6, or 8 seconds &lt;/b&gt;from a text prompt or uploaded images, with the ability to extend videos up to &lt;b&gt;148 seconds (more than 2 and half minutes!)&lt;/b&gt; when using the “Extend” feature.&lt;/p&gt;&lt;p&gt;New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Initial Reactions&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/mattshumer_/status/1978503288992461205"&gt;Matt Shumer,&lt;/a&gt; an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”&lt;/p&gt;&lt;p&gt;However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/MrDavids1/status/1978460666395505004"&gt;&lt;b&gt;Travis Davids&lt;/b&gt;&lt;/a&gt;, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system. &lt;/p&gt;&lt;p&gt;These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.&lt;/p&gt;&lt;p&gt;Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.&lt;/p&gt;&lt;p&gt;On the more positive end, &lt;a href="https://x.com/kimmonismus"&gt;@kimmonismus,&lt;/a&gt; an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.&lt;/p&gt;&lt;p&gt;Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Adoption and Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Since launching Flow five months ago, Google says over &lt;b&gt;275 million videos&lt;/b&gt; have been generated across various Veo models. &lt;/p&gt;&lt;p&gt;The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.&lt;/p&gt;&lt;p&gt;Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety and Responsible AI Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Videos generated with Veo 3.1 are watermarked using Google’s &lt;b&gt;SynthID&lt;/b&gt; technology, which embeds an imperceptible identifier to signal that the content is AI-generated. &lt;/p&gt;&lt;p&gt;Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.&lt;/p&gt;&lt;p&gt;For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where Veo 3.1 Stands Among a Crowded AI Video Model Space&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.&lt;/p&gt;&lt;p&gt;The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;As expected after days of leaks and rumors online, Google has &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;unveiled Veo 3.1&lt;/a&gt;, its latest AI video generation model, bringing a suite of creative and technical upgrades aimed at improving narrative control, audio integration, and realism in AI-generated video. &lt;/p&gt;&lt;p&gt;While the updates expand possibilities for hobbyists and content creators using Google’s online AI creation app, &lt;a href="https://labs.google/fx/tools/flow"&gt;Flow&lt;/a&gt;, the release also signals a growing opportunity for enterprises, developers, and creative teams seeking scalable, customizable video tools.&lt;/p&gt;&lt;p&gt;The quality is higher, the physics better, the pricing the same as before, and the control and editing features more robust and varied.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;My &lt;a href="https://x.com/carlfranzen/status/1978522697014411322"&gt;initial tests&lt;/a&gt; showed it to be a powerful and performant model that immediately delights with each generation. However, the look is more cinematic, polished and a little more &amp;quot;artificial&amp;quot; than by default than rivals such as &lt;a href="https://venturebeat.com/ai/openai-debuts-sora-2-ai-video-generator-app-with-sound-and-self-insertion"&gt;OpenAI&amp;#x27;s new Sora 2&lt;/a&gt;, released late last month, which may or may not be what a particular user is going after (Sora excels at handheld and &amp;quot;candid&amp;quot; style videos). &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Expanded Control Over Narrative and Audio&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 builds on its predecessor, Veo 3 (&lt;a href="https://www.cnbc.com/2025/05/20/google-ai-video-generator-audio-veo-3.html"&gt;released back in May 2025&lt;/a&gt;) with enhanced support for dialogue, ambient sound, and other audio effects. &lt;/p&gt;&lt;p&gt;Native audio generation is now available across several key features in Flow, including “Frames to Video,” “Ingredients to Video,” and “Extend,&amp;quot; which give users the ability to, respectively: turn still images into video; use items, characters and objects from multiple images in a single video; and generate longer clips than the initial 8 seconds, to more than 30 seconds or even 1+ plus when continuing from a prior clip&amp;#x27;s final frame. &lt;/p&gt;&lt;p&gt;Before, you had to add audio manually after using these features. &lt;/p&gt;&lt;p&gt;This addition gives users greater command over tone, emotion, and storytelling — capabilities that have previously required post-production work.&lt;/p&gt;&lt;p&gt;In enterprise contexts, this level of control may reduce the need for separate audio pipelines, offering an integrated way to create training content, marketing videos, or digital experiences with synchronized sound and visuals.&lt;/p&gt;&lt;p&gt;Google noted in &lt;a href="https://blog.google/technology/ai/veo-updates-flow/"&gt;a blog post&lt;/a&gt; that the updates reflect user feedback calling for deeper artistic control and improved audio support. Gallegos emphasizes the importance of making edits and refinements possible directly in Flow, without reworking scenes from scratch.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Richer Inputs and Editing Capabilities&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;With Veo 3.1, Google introduces support for multiple input types and more granular control over generated outputs. The model accepts text prompts, images, and video clips as input, and also supports:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Reference images (up to three)&lt;/b&gt; to guide appearance and style in the final output&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;First and last frame interpolation&lt;/b&gt; to generate seamless scenes between fixed endpoints&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Scene extension&lt;/b&gt; that continues a video’s action or motion beyond its current duration&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These tools aim to give enterprise users a way to fine-tune the look and feel of their content—useful for brand consistency or adherence to creative briefs.&lt;/p&gt;&lt;p&gt;Additional capabilities like “Insert” (add objects to scenes) and “Remove” (delete elements or characters) are also being introduced, though not all are immediately available through the Gemini API.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deployment Across Platforms&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is accessible through several of Google’s existing AI services:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://labs.google/fx/tools/flow"&gt;&lt;b&gt;Flow&lt;/b&gt;&lt;/a&gt;, Google’s own interface for AI-assisted filmmaking&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/video?example=dialogue"&gt;&lt;b&gt;Gemini API&lt;/b&gt;&lt;/a&gt;, targeted at developers building video capabilities into applications&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/veo-video-generation"&gt;&lt;b&gt;Vertex AI&lt;/b&gt;&lt;/a&gt;, where enterprise integration will soon support Veo’s “Scene Extension” and other key features&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Availability through these platforms allows enterprise customers to choose the right environment—GUI-based or programmatic—based on their teams and workflows.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Pricing and Access&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The Veo 3.1 model is currently in &lt;b&gt;preview&lt;/b&gt; and available only on the &lt;b&gt;paid tier&lt;/b&gt; of the Gemini API. The cost structure is the same as Veo 3, the preceding generation of AI video models from Google.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Standard model&lt;/b&gt;: $0.40 per second of video&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Fast model&lt;/b&gt;: $0.15 per second&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;There is no free tier, and users are charged only if a video is successfully generated. This model is consistent with previous Veo versions and provides predictable pricing for budget-conscious enterprise teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Technical Specs and Output Control&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 outputs video at &lt;b&gt;720p or 1080p resolution&lt;/b&gt;, with a &lt;b&gt;24 fps frame rate&lt;/b&gt;. &lt;/p&gt;&lt;p&gt;Duration options include &lt;b&gt;4, 6, or 8 seconds &lt;/b&gt;from a text prompt or uploaded images, with the ability to extend videos up to &lt;b&gt;148 seconds (more than 2 and half minutes!)&lt;/b&gt; when using the “Extend” feature.&lt;/p&gt;&lt;p&gt;New functionality also includes tighter control over subjects and environments. For example, enterprises can upload a product image or visual reference, and Veo 3.1 will generate scenes that preserve its appearance and stylistic cues across the video. This could streamline creative production pipelines for retail, advertising, and virtual content production teams.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Initial Reactions&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The broader creator and developer community has responded to Veo 3.1’s launch with a mix of optimism and tempered critique—particularly when comparing it to rival models like OpenAI’s Sora 2.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/mattshumer_/status/1978503288992461205"&gt;Matt Shumer,&lt;/a&gt; an AI founder of Otherside AI/Hyperwrite, and early adopter, described his initial reaction as “disappointment,” noting that Veo 3.1 is “noticeably worse than Sora 2” and also “quite a bit more expensive.”&lt;/p&gt;&lt;p&gt;However, he acknowledged that Google’s tooling—such as support for references and scene extension—is a bright spot in the release.&lt;/p&gt;&lt;p&gt;&lt;a href="https://x.com/MrDavids1/status/1978460666395505004"&gt;&lt;b&gt;Travis Davids&lt;/b&gt;&lt;/a&gt;, a 3D digital artist and AI content creator, echoed some of that sentiment. While he noted improvements in audio quality, particularly in sound effects and dialogue, he raised concerns about limitations that remain in the system. &lt;/p&gt;&lt;p&gt;These include the lack of custom voice support, an inability to select generated voices directly, and the continued cap at 8-second generations—despite some public claims about longer outputs.&lt;/p&gt;&lt;p&gt;Davids also pointed out that character consistency across changing camera angles still requires careful prompting, whereas other models like Sora 2 handle this more automatically. He questioned the absence of 1080p resolution for users on paid tiers like Flow Pro and expressed skepticism over feature parity.&lt;/p&gt;&lt;p&gt;On the more positive end, &lt;a href="https://x.com/kimmonismus"&gt;@kimmonismus,&lt;/a&gt; an AI newsletter writer, stated that “Veo 3.1 is amazing,” though still concluded that OpenAI’s latest model remains preferable overall.&lt;/p&gt;&lt;p&gt;Collectively, these early impressions suggest that while Veo 3.1 delivers meaningful tooling enhancements and new creative control features, expectations have shifted as competitors raise the bar on both quality and usability.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Adoption and Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Since launching Flow five months ago, Google says over &lt;b&gt;275 million videos&lt;/b&gt; have been generated across various Veo models. &lt;/p&gt;&lt;p&gt;The pace of adoption suggests significant interest not only from individuals but also from developers and businesses experimenting with automated content creation.&lt;/p&gt;&lt;p&gt;Thomas Iljic, Director of Product Management at Google Labs, highlights that Veo 3.1’s release brings capabilities closer to how human filmmakers plan and shoot. These include scene composition, continuity across shots, and coordinated audio—all areas that enterprises increasingly look to automate or streamline.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Safety and Responsible AI Use&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Videos generated with Veo 3.1 are watermarked using Google’s &lt;b&gt;SynthID&lt;/b&gt; technology, which embeds an imperceptible identifier to signal that the content is AI-generated. &lt;/p&gt;&lt;p&gt;Google applies safety filters and moderation across its APIs to help minimize privacy and copyright risks. Generated content is stored temporarily and deleted after two days unless downloaded.&lt;/p&gt;&lt;p&gt;For developers and enterprises, these features provide reassurance around provenance and compliance—critical in regulated or brand-sensitive industries.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Where Veo 3.1 Stands Among a Crowded AI Video Model Space&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Veo 3.1 is not just an iteration on prior models—it represents a deeper integration of multimodal inputs, storytelling control, and enterprise-level tooling. While creative professionals may see immediate benefits in editing workflows and fidelity, businesses exploring automation in training, advertising, or virtual experiences may find even greater value in the model’s composability and API support.&lt;/p&gt;&lt;p&gt;The early user feedback highlights that while Veo 3.1 offers valuable tooling, expectations around realism, voice control, and generation length are evolving rapidly. As Google expands access through Vertex AI and continues refining Veo, its competitive positioning in enterprise video generation will hinge on how quickly these user pain points are addressed.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-releases-new-ai-video-model-veo-3-1-in-flow-and-api-what-it-means-for</guid><pubDate>Wed, 15 Oct 2025 18:50:00 +0000</pubDate></item><item><title>Anthropic’s Claude Haiku 4.5 matches May’s frontier model at fraction of cost (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/anthropics-claude-haiku-4-5-matches-mays-frontier-model-at-fraction-of-cost/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Tiny, fast model hits coding scores similar to GPT-5 and Sonnet 4.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="absolute inset-0 w-full h-full object-cover hidden" height="359" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero-640x359.jpg" width="640" /&gt;
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="624" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero.jpg" width="1113" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic released Claude&amp;nbsp;Haiku 4.5, a small AI language model that reportedly delivers performance similar to what its frontier model Claude Sonnet 4 achieved five months ago but at one-third the cost and more than twice the speed. The new model is available now to all Claude app, web, and API users.&lt;/p&gt;
&lt;p&gt;If the benchmarks for Haiku 4.5 reported by Anthropic hold up to independent testing, the fact that the company can match some capabilities of its cutting-edge coding model from only five months ago (and GPT-5 in coding) while providing a dramatic speed increase and cost cut is notable.&lt;/p&gt;
&lt;p&gt;As a recap, Anthropic ships the Claude family in three model sizes: Haiku (small), Sonnet (medium), and Opus (large). The larger models are based on larger neural networks and typically include deeper contextual knowledge but are slower and more expensive to run. Due to a technique called distillation, companies like Anthropic have been able to craft smaller AI models that match the capability of larger, older models at functional tasks like coding, although it typically comes at the cost of omitting stored knowledge.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122654 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku benchmark results from Anthropic." class="center large" height="867" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_benchmarks-1024x867.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku benchmark results from Anthropic.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;That means if you wanted to converse with an AI model that might craft a deeper and more meaningful analysis of, say, foreign policy or world history, you might be better served talking to Sonnet or Opus (being aware that they can also be wrong and make things up). But if you just need quick coding assistance that's more about translation of concepts than general knowledge, Haiku might be the better pick due to its speed and lower cost.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And speaking of cost, Haiku 4.5 is included for subscribers of the Claude web and app plans. Through the API (for developers), the small model is priced at $1 per million input tokens and $5 per million output tokens. That compares to Sonnet 4.5 at $3 per million input and $15 per million output tokens, and Opus 4.1 at $15 per million input and $75 per million output tokens.&lt;/p&gt;
&lt;p&gt;The model serves as a cheaper drop-in replacement for two older models, Haiku 3.5 and Sonnet 4. "Users who rely on AI for real-time, low-latency tasks like chat assistants, customer service agents, or pair programming will appreciate Haiku 4.5’s combination of high intelligence and remarkable speed," Anthropic writes.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122655 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku answers the classic Ars Technica AI question, &amp;quot;Would the color be called 'magenta' if the town of Magenta didn't exist?&amp;quot;" class="center large" height="907" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_magenta-1024x907.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku answers the classic Ars Technica AI question, "Would the color be called 'magenta' if the town of Magenta didn't exist?"

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;On SWE-bench Verified, a test that measures performance on coding tasks, Haiku 4.5 scored 73.3 percent compared to Sonnet 4's similar performance level (72.7 percent). The model also reportedly surpasses Sonnet 4 at certain tasks like using computers, according to Anthropic's benchmarks. Claude Sonnet 4.5, released in late September, remains Anthropic's frontier model and what the company calls "the best coding model available."&lt;/p&gt;
&lt;p&gt;Haiku 4.5 also surprisingly edges up close to what OpenAI's GPT-5 can achieve in this particular set of benchmarks (as seen in the chart above), although since the results are self-reported and potentially cherry-picked to match a model's strengths, one should always take them with a grain of salt.&lt;/p&gt;
&lt;p&gt;Still, making a small, capable coding model may have unexpected advantages for agentic coding setups like Claude Code. Anthropic designed Haiku 4.5 to work alongside Sonnet 4.5 in multi-model workflows. In such a configuration, Anthropic says, Sonnet 4.5 could break down complex problems into multi-step plans, then coordinate multiple Haiku 4.5 instances to complete subtasks in parallel, like spinning off workers to get things done faster.&lt;/p&gt;
&lt;p&gt;For more details on the new model, Anthropic released a system card and documentation for developers.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Tiny, fast model hits coding scores similar to GPT-5 and Sonnet 4.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="absolute inset-0 w-full h-full object-cover hidden" height="359" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero-640x359.jpg" width="640" /&gt;
                  &lt;img alt="Anthropic's Haiku 4.5 logo." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="624" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_hero.jpg" width="1113" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Wednesday, Anthropic released Claude&amp;nbsp;Haiku 4.5, a small AI language model that reportedly delivers performance similar to what its frontier model Claude Sonnet 4 achieved five months ago but at one-third the cost and more than twice the speed. The new model is available now to all Claude app, web, and API users.&lt;/p&gt;
&lt;p&gt;If the benchmarks for Haiku 4.5 reported by Anthropic hold up to independent testing, the fact that the company can match some capabilities of its cutting-edge coding model from only five months ago (and GPT-5 in coding) while providing a dramatic speed increase and cost cut is notable.&lt;/p&gt;
&lt;p&gt;As a recap, Anthropic ships the Claude family in three model sizes: Haiku (small), Sonnet (medium), and Opus (large). The larger models are based on larger neural networks and typically include deeper contextual knowledge but are slower and more expensive to run. Due to a technique called distillation, companies like Anthropic have been able to craft smaller AI models that match the capability of larger, older models at functional tasks like coding, although it typically comes at the cost of omitting stored knowledge.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122654 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku benchmark results from Anthropic." class="center large" height="867" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_benchmarks-1024x867.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku benchmark results from Anthropic.

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;That means if you wanted to converse with an AI model that might craft a deeper and more meaningful analysis of, say, foreign policy or world history, you might be better served talking to Sonnet or Opus (being aware that they can also be wrong and make things up). But if you just need quick coding assistance that's more about translation of concepts than general knowledge, Haiku might be the better pick due to its speed and lower cost.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And speaking of cost, Haiku 4.5 is included for subscribers of the Claude web and app plans. Through the API (for developers), the small model is priced at $1 per million input tokens and $5 per million output tokens. That compares to Sonnet 4.5 at $3 per million input and $15 per million output tokens, and Opus 4.1 at $15 per million input and $75 per million output tokens.&lt;/p&gt;
&lt;p&gt;The model serves as a cheaper drop-in replacement for two older models, Haiku 3.5 and Sonnet 4. "Users who rely on AI for real-time, low-latency tasks like chat assistants, customer service agents, or pair programming will appreciate Haiku 4.5’s combination of high intelligence and remarkable speed," Anthropic writes.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2122655 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="Claude 4.5 Haiku answers the classic Ars Technica AI question, &amp;quot;Would the color be called 'magenta' if the town of Magenta didn't exist?&amp;quot;" class="center large" height="907" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/haiku45_magenta-1024x907.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Claude 4.5 Haiku answers the classic Ars Technica AI question, "Would the color be called 'magenta' if the town of Magenta didn't exist?"

          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;On SWE-bench Verified, a test that measures performance on coding tasks, Haiku 4.5 scored 73.3 percent compared to Sonnet 4's similar performance level (72.7 percent). The model also reportedly surpasses Sonnet 4 at certain tasks like using computers, according to Anthropic's benchmarks. Claude Sonnet 4.5, released in late September, remains Anthropic's frontier model and what the company calls "the best coding model available."&lt;/p&gt;
&lt;p&gt;Haiku 4.5 also surprisingly edges up close to what OpenAI's GPT-5 can achieve in this particular set of benchmarks (as seen in the chart above), although since the results are self-reported and potentially cherry-picked to match a model's strengths, one should always take them with a grain of salt.&lt;/p&gt;
&lt;p&gt;Still, making a small, capable coding model may have unexpected advantages for agentic coding setups like Claude Code. Anthropic designed Haiku 4.5 to work alongside Sonnet 4.5 in multi-model workflows. In such a configuration, Anthropic says, Sonnet 4.5 could break down complex problems into multi-step plans, then coordinate multiple Haiku 4.5 instances to complete subtasks in parallel, like spinning off workers to get things done faster.&lt;/p&gt;
&lt;p&gt;For more details on the new model, Anthropic released a system card and documentation for developers.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/anthropics-claude-haiku-4-5-matches-mays-frontier-model-at-fraction-of-cost/</guid><pubDate>Wed, 15 Oct 2025 18:53:00 +0000</pubDate></item><item><title>Anthropic is giving away its powerful Claude Haiku 4.5 AI for free to take on OpenAI (AI | VentureBeat)</title><link>https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; released &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Claude Haiku 4.5&lt;/u&gt;&lt;/a&gt; on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.&lt;/p&gt;&lt;p&gt;The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic&amp;#x27;s mid-sized &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4 model&lt;/u&gt;&lt;/a&gt; released in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.&lt;/p&gt;&lt;p&gt;&amp;quot;Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,&amp;quot; an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.&lt;/p&gt;&lt;p&gt;The launch comes just two weeks after Anthropic released &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Claude Sonnet 4.5&lt;/u&gt;&lt;/a&gt;, which the company bills as the world&amp;#x27;s best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfs &lt;a href="https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation"&gt;&lt;u&gt;Anthropic&amp;#x27;s $183 billion&lt;/u&gt;&lt;/a&gt;, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How free access to advanced AI could reshape the enterprise market&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of its &lt;a href="http://claude.ai"&gt;&lt;u&gt;Claude.ai&lt;/u&gt;&lt;/a&gt; platform. The decision effectively democratizes access to what the company characterizes as &amp;quot;near-frontier-level intelligence&amp;quot; — capabilities that would have been available only in expensive, premium models months ago.&lt;/p&gt;&lt;p&gt;&amp;quot;The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,&amp;quot; the Anthropic spokesperson told VentureBeat. &amp;quot;It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.&amp;quot;&lt;/p&gt;&lt;p&gt;This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticated &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5 model&lt;/u&gt;&lt;/a&gt; breaking down complex problems and delegating subtasks to multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.&lt;/p&gt;&lt;p&gt;The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s path to $7 billion in annual revenue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model launch coincides with revelations that Anthropic&amp;#x27;s business is experiencing explosive growth. The company&amp;#x27;s annual revenue run rate is &lt;a href="https://www.reuters.com/business/retail-consumer/anthropic-aims-nearly-triple-annualized-revenue-2026-sources-say-2025-10-15/"&gt;&lt;u&gt;approaching $7 billion this month&lt;/u&gt;&lt;/a&gt;, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.&lt;/p&gt;&lt;p&gt;The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic&amp;#x27;s most successful offerings is &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt;, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.&lt;/p&gt;&lt;p&gt;Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.&lt;/p&gt;&lt;p&gt;&amp;quot;The best products can be grounded in some kind of success metric or evaluation,&amp;quot; Krieger said on the &lt;a href="https://podcasts.apple.com/us/podcast/inside-claude-the-ai-coworker-era-mike-krieger-anthropic/id1759013677?i=1000731964089"&gt;&lt;u&gt;&amp;quot;Superhuman AI&amp;quot; podcast&lt;/u&gt;&lt;/a&gt;. &amp;quot;I&amp;#x27;ve seen that a lot in talking to companies that are deploying AI.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI safety testing matters more than ever for enterprise adoption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic&amp;#x27;s launch comes amid heightened scrutiny of the company&amp;#x27;s approach to AI safety and regulation. On Tuesday, David Sacks, the White House&amp;#x27;s AI &amp;quot;czar&amp;quot; and a venture capitalist, accused Anthropic of &amp;quot;&lt;a href="https://x.com/DavidSacks/status/1978145266269077891"&gt;&lt;u&gt;running a sophisticated regulatory capture strategy based on fear-mongering&amp;quot; that is &amp;quot;damaging the startup ecosystem&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;The attack targeted remarks by Jack Clark, Anthropic&amp;#x27;s British co-founder and head of policy, who had described being &amp;quot;&lt;a href="https://importai.substack.com/p/import-ai-431-technological-optimism"&gt;&lt;u&gt;deeply afraid&lt;/u&gt;&lt;/a&gt;&amp;quot; of AI&amp;#x27;s trajectory. Clark told Bloomberg he found Sacks&amp;#x27; criticism &amp;quot;&lt;a href="https://www.bloomberg.com/opinion/articles/2025-10-15/anthropic-s-ai-principles-make-it-a-white-house-target"&gt;&lt;u&gt;perplexing&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;Anthropic addressed such concerns head-on in its release materials, emphasizing that &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; underwent extensive safety testing. The company classified the model as &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-2&lt;/u&gt;&lt;/a&gt; — its AI Safety Level 2 standard — compared to the more restrictive &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-3&lt;/u&gt;&lt;/a&gt; designation for the more powerful Sonnet 4.5 and Opus 4.1 models.&lt;/p&gt;&lt;p&gt;&amp;quot;Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,&amp;quot; the spokesperson told VentureBeat. &amp;quot;In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.&amp;quot;&lt;/p&gt;&lt;p&gt;The company said its safety testing showed &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.&lt;/p&gt;&lt;p&gt;The emphasis on safety reflects Anthropic&amp;#x27;s founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI&amp;#x27;s direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Benchmark results show Haiku 4.5 competing with larger, more expensive models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;According to Anthropic&amp;#x27;s benchmarks, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; performs competitively with or exceeds several larger models across multiple evaluation criteria. On &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-bench Verified&lt;/u&gt;&lt;/a&gt;, a widely used test measuring AI systems&amp;#x27; ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4&amp;#x27;s 72.7% and close to GPT-5 Codex&amp;#x27;s 74.5%.&lt;/p&gt;&lt;p&gt;The model demonstrated particular strength in computer use tasks, achieving 50.7% on the &lt;a href="https://os-world.github.io/"&gt;&lt;u&gt;OSWorld benchmark&lt;/u&gt;&lt;/a&gt; compared to Sonnet 4&amp;#x27;s 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.&lt;/p&gt;&lt;p&gt;In coding-specific benchmarks like &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal-Bench&lt;/u&gt;&lt;/a&gt;, which tests AI agents&amp;#x27; ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5&amp;#x27;s 50.0% among Claude models.&lt;/p&gt;&lt;p&gt;The model maintains a 200,000-token context window for standard users, with developers accessing the &lt;a href="https://www.claude.com/platform/api"&gt;&lt;u&gt;Claude Developer Platform&lt;/u&gt;&lt;/a&gt; able to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What three major AI model releases in two months says about the competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;When asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company&amp;#x27;s focus on execution rather than competitive positioning.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,&amp;quot; the spokesperson said. &amp;quot;What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.&amp;quot;&lt;/p&gt;&lt;p&gt;That velocity stands in contrast to the company&amp;#x27;s earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.&lt;/p&gt;&lt;p&gt;That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today&amp;#x27;s budget constraints around AI deployment may ease considerably in coming years.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From customer service to code: Real-world applications for faster, cheaper AI&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The practical applications of &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; span a wide range of enterprise functions, from customer service to financial analysis to software development. The model&amp;#x27;s combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.&lt;/p&gt;&lt;p&gt;In financial services, the multi-agent architecture enabled by pairing &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; with &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.&lt;/p&gt;&lt;p&gt;For research organizations, the division of labor could compress timelines dramatically. &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; might orchestrate a comprehensive analysis while multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially &amp;quot;compressing weeks of research into hours,&amp;quot; according to Anthropic&amp;#x27;s use case descriptions.&lt;/p&gt;&lt;p&gt;Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model &amp;quot;hit a sweet spot we didn&amp;#x27;t think was possible: near-frontier coding quality with blazing speed and cost efficiency.&amp;quot; In Augment&amp;#x27;s internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5&amp;#x27;s performance while matching much larger models.&lt;/p&gt;&lt;p&gt;Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 &amp;quot;is blurring the lines&amp;quot; on traditional trade-offs between speed, cost and quality. &amp;quot;It&amp;#x27;s a fast frontier model that keeps costs efficient and signals where this class of models is headed.&amp;quot;&lt;/p&gt;&lt;p&gt;Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 &amp;quot;outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that&amp;#x27;s a game-changer for our unit economics.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The price of progress: What plummeting AI costs mean for enterprise strategy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For enterprises evaluating AI strategies, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.&lt;/p&gt;&lt;p&gt;The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.&lt;/p&gt;&lt;p&gt;The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.&lt;/p&gt;&lt;p&gt;The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4&amp;#x27;s capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic&amp;#x27;s release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.&lt;/p&gt;&lt;p&gt;For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company&amp;#x27;s projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.&lt;/p&gt;&lt;p&gt;Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.&lt;/p&gt;&lt;p&gt;In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.&lt;/p&gt;&lt;p&gt;

&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://anthropic.com/"&gt;&lt;u&gt;Anthropic&lt;/u&gt;&lt;/a&gt; released &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Claude Haiku 4.5&lt;/u&gt;&lt;/a&gt; on Wednesday, a smaller and significantly cheaper artificial intelligence model that matches the coding capabilities of systems that were considered cutting-edge just months ago, marking the latest salvo in an intensifying competition to dominate enterprise AI.&lt;/p&gt;&lt;p&gt;The model costs $1 per million input tokens and $5 per million output tokens — roughly one-third the price of Anthropic&amp;#x27;s mid-sized &lt;a href="https://www.anthropic.com/news/claude-4"&gt;&lt;u&gt;Sonnet 4 model&lt;/u&gt;&lt;/a&gt; released in May, while operating more than twice as fast. In certain tasks, particularly operating computers autonomously, Haiku 4.5 actually surpasses its more expensive predecessor.&lt;/p&gt;&lt;p&gt;&amp;quot;Haiku 4.5 is a clear leap in performance and is now largely as smart as Sonnet 4 while being significantly faster and one-third of the cost,&amp;quot; an Anthropic spokesperson told VentureBeat, underscoring how rapidly AI capabilities are becoming commoditized as the technology matures.&lt;/p&gt;&lt;p&gt;The launch comes just two weeks after Anthropic released &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Claude Sonnet 4.5&lt;/u&gt;&lt;/a&gt;, which the company bills as the world&amp;#x27;s best coding model, and two months after introducing Opus 4.1. The breakneck pace of releases reflects mounting pressure from OpenAI, whose $500 billion valuation dwarfs &lt;a href="https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation"&gt;&lt;u&gt;Anthropic&amp;#x27;s $183 billion&lt;/u&gt;&lt;/a&gt;, and which has inked a series of multibillion-dollar infrastructure deals while expanding its product lineup.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How free access to advanced AI could reshape the enterprise market&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;In an unusual move that could reshape competitive dynamics in the AI market, Anthropic is making Haiku 4.5 available for all free users of its &lt;a href="http://claude.ai"&gt;&lt;u&gt;Claude.ai&lt;/u&gt;&lt;/a&gt; platform. The decision effectively democratizes access to what the company characterizes as &amp;quot;near-frontier-level intelligence&amp;quot; — capabilities that would have been available only in expensive, premium models months ago.&lt;/p&gt;&lt;p&gt;&amp;quot;The launch of Claude Haiku 4.5 means that near-frontier-level intelligence is available for free to all users through Claude.ai,&amp;quot; the Anthropic spokesperson told VentureBeat. &amp;quot;It also offers significant advantages to our enterprise customers: Sonnet 4.5 can handle frontier planning while Haiku 4.5 powers sub-agents, enabling multi-agent systems that tackle complex refactors, migrations, and large features builds with speed and quality.&amp;quot;&lt;/p&gt;&lt;p&gt;This multi-agent architecture signals a significant shift in how AI systems are deployed. Rather than relying on a single, monolithic model, enterprises can now orchestrate teams of specialized AI agents: a more sophisticated &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5 model&lt;/u&gt;&lt;/a&gt; breaking down complex problems and delegating subtasks to multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents working in parallel. For software development teams, this could mean Sonnet 4.5 plans a major code refactoring while Haiku 4.5 agents simultaneously execute changes across dozens of files.&lt;/p&gt;&lt;p&gt;The approach mirrors how human organizations distribute work, and could prove particularly valuable for enterprises seeking to balance performance with cost efficiency — a critical consideration as AI deployment scales.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside Anthropic&amp;#x27;s path to $7 billion in annual revenue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model launch coincides with revelations that Anthropic&amp;#x27;s business is experiencing explosive growth. The company&amp;#x27;s annual revenue run rate is &lt;a href="https://www.reuters.com/business/retail-consumer/anthropic-aims-nearly-triple-annualized-revenue-2026-sources-say-2025-10-15/"&gt;&lt;u&gt;approaching $7 billion this month&lt;/u&gt;&lt;/a&gt;, Anthropic told Reuters, up from more than $5 billion reported in August. Internal projections obtained by Reuters suggest the company is targeting between $20 billion and $26 billion in annualized revenue for 2026, representing growth of more than 200% to nearly 300%.&lt;/p&gt;&lt;p&gt;The company now serves more than 300,000 business customers, with enterprise products accounting for approximately 80% of revenue. Among Anthropic&amp;#x27;s most successful offerings is &lt;a href="https://www.claude.com/product/claude-code"&gt;&lt;u&gt;Claude Code&lt;/u&gt;&lt;/a&gt;, a code-generation tool that has reached nearly $1 billion in annualized revenue since launching earlier this year.&lt;/p&gt;&lt;p&gt;Those numbers come as artificial intelligence enters what many in the industry characterize as a critical inflection point. After two years of what Anthropic Chief Product Officer Mike Krieger recently described as &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; — where companies adopted AI tools without clear success metrics — enterprises are now demanding measurable returns on investment.&lt;/p&gt;&lt;p&gt;&amp;quot;The best products can be grounded in some kind of success metric or evaluation,&amp;quot; Krieger said on the &lt;a href="https://podcasts.apple.com/us/podcast/inside-claude-the-ai-coworker-era-mike-krieger-anthropic/id1759013677?i=1000731964089"&gt;&lt;u&gt;&amp;quot;Superhuman AI&amp;quot; podcast&lt;/u&gt;&lt;/a&gt;. &amp;quot;I&amp;#x27;ve seen that a lot in talking to companies that are deploying AI.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprises evaluating AI tools, the calculus increasingly centers on concrete productivity gains. Google CEO Sundar Pichai claimed in June that AI had generated a 10% boost in engineering velocity at his company — though measuring such improvements across different roles and use cases remains challenging, as Krieger acknowledged.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI safety testing matters more than ever for enterprise adoption&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Anthropic&amp;#x27;s launch comes amid heightened scrutiny of the company&amp;#x27;s approach to AI safety and regulation. On Tuesday, David Sacks, the White House&amp;#x27;s AI &amp;quot;czar&amp;quot; and a venture capitalist, accused Anthropic of &amp;quot;&lt;a href="https://x.com/DavidSacks/status/1978145266269077891"&gt;&lt;u&gt;running a sophisticated regulatory capture strategy based on fear-mongering&amp;quot; that is &amp;quot;damaging the startup ecosystem&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;The attack targeted remarks by Jack Clark, Anthropic&amp;#x27;s British co-founder and head of policy, who had described being &amp;quot;&lt;a href="https://importai.substack.com/p/import-ai-431-technological-optimism"&gt;&lt;u&gt;deeply afraid&lt;/u&gt;&lt;/a&gt;&amp;quot; of AI&amp;#x27;s trajectory. Clark told Bloomberg he found Sacks&amp;#x27; criticism &amp;quot;&lt;a href="https://www.bloomberg.com/opinion/articles/2025-10-15/anthropic-s-ai-principles-make-it-a-white-house-target"&gt;&lt;u&gt;perplexing&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;Anthropic addressed such concerns head-on in its release materials, emphasizing that &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; underwent extensive safety testing. The company classified the model as &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-2&lt;/u&gt;&lt;/a&gt; — its AI Safety Level 2 standard — compared to the more restrictive &lt;a href="https://www.anthropic.com/news/anthropics-responsible-scaling-policy"&gt;&lt;u&gt;ASL-3&lt;/u&gt;&lt;/a&gt; designation for the more powerful Sonnet 4.5 and Opus 4.1 models.&lt;/p&gt;&lt;p&gt;&amp;quot;Our teams have red-teamed and tested our agentic capabilities to the limits in order to assess whether it can be used to engage in harmful activity like generating misinformation or promoting fraudulent behavior like scams,&amp;quot; the spokesperson told VentureBeat. &amp;quot;In our automated alignment assessment, it showed a statistically significantly lower overall rate of misaligned behaviors than both Claude Sonnet 4.5 and Claude Opus 4.1 — making it, by this metric, our safest model yet.&amp;quot;&lt;/p&gt;&lt;p&gt;The company said its safety testing showed &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; poses only limited risks regarding the production of chemical, biological, radiological and nuclear weapons. Anthropic has also implemented classifiers designed to detect and filter prompt injection attacks, a common method for attempting to manipulate AI systems into producing harmful content.&lt;/p&gt;&lt;p&gt;The emphasis on safety reflects Anthropic&amp;#x27;s founding mission. The company was established in 2021 by former OpenAI executives, including siblings Dario and Daniela Amodei, who left amid concerns about OpenAI&amp;#x27;s direction following its partnership with Microsoft. Anthropic has positioned itself as taking a more cautious, research-oriented approach to AI development.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Benchmark results show Haiku 4.5 competing with larger, more expensive models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;According to Anthropic&amp;#x27;s benchmarks, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; performs competitively with or exceeds several larger models across multiple evaluation criteria. On &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-bench Verified&lt;/u&gt;&lt;/a&gt;, a widely used test measuring AI systems&amp;#x27; ability to solve real-world software engineering problems, Haiku 4.5 scored 73.3% — slightly ahead of Sonnet 4&amp;#x27;s 72.7% and close to GPT-5 Codex&amp;#x27;s 74.5%.&lt;/p&gt;&lt;p&gt;The model demonstrated particular strength in computer use tasks, achieving 50.7% on the &lt;a href="https://os-world.github.io/"&gt;&lt;u&gt;OSWorld benchmark&lt;/u&gt;&lt;/a&gt; compared to Sonnet 4&amp;#x27;s 42.2%. This capability allows the AI to interact directly with computer interfaces — clicking buttons, filling forms, navigating applications — which could prove transformative for automating routine digital tasks.&lt;/p&gt;&lt;p&gt;In coding-specific benchmarks like &lt;a href="https://www.tbench.ai/"&gt;&lt;u&gt;Terminal-Bench&lt;/u&gt;&lt;/a&gt;, which tests AI agents&amp;#x27; ability to complete complex software tasks using command-line tools, Haiku 4.5 scored 41.0%, trailing only Sonnet 4.5&amp;#x27;s 50.0% among Claude models.&lt;/p&gt;&lt;p&gt;The model maintains a 200,000-token context window for standard users, with developers accessing the &lt;a href="https://www.claude.com/platform/api"&gt;&lt;u&gt;Claude Developer Platform&lt;/u&gt;&lt;/a&gt; able to use a 1-million-token context window. That expanded capacity means the model can process extremely large codebases or documents in a single request — roughly equivalent to a 1,500-page book.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What three major AI model releases in two months says about the competition&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;When asked about the rapid succession of model releases, the Anthropic spokesperson emphasized the company&amp;#x27;s focus on execution rather than competitive positioning.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;re focused on shipping the best possible products for our customers — and our shipping velocity speaks for itself,&amp;quot; the spokesperson said. &amp;quot;What was state-of-the-art just five months ago is now faster, cheaper, and more accessible.&amp;quot;&lt;/p&gt;&lt;p&gt;That velocity stands in contrast to the company&amp;#x27;s earlier, more measured release schedule. Anthropic appeared to have paused development of its Haiku line after releasing version 3.5 at the end of last year, leading some observers to speculate the company had deprioritized smaller models.&lt;/p&gt;&lt;p&gt;That rapid price-performance improvement validates a core promise of artificial intelligence: that capabilities will become dramatically cheaper over time as the technology matures and companies optimize their models. For enterprises, it suggests that today&amp;#x27;s budget constraints around AI deployment may ease considerably in coming years.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From customer service to code: Real-world applications for faster, cheaper AI&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The practical applications of &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; span a wide range of enterprise functions, from customer service to financial analysis to software development. The model&amp;#x27;s combination of speed and intelligence makes it particularly suited for real-time, low-latency tasks like chatbot conversations and customer support interactions, where delays of even a few seconds can degrade user experience.&lt;/p&gt;&lt;p&gt;In financial services, the multi-agent architecture enabled by pairing &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; with &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; could transform how firms monitor markets and manage risk. Anthropic envisions Haiku 4.5 monitoring thousands of data streams simultaneously — tracking regulatory changes, market signals and portfolio risks — while Sonnet 4.5 handles complex predictive modeling and strategic analysis.&lt;/p&gt;&lt;p&gt;For research organizations, the division of labor could compress timelines dramatically. &lt;a href="https://www.anthropic.com/news/claude-sonnet-4-5"&gt;&lt;u&gt;Sonnet 4.5&lt;/u&gt;&lt;/a&gt; might orchestrate a comprehensive analysis while multiple &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; agents parallelize literature reviews, data gathering and document synthesis across dozens of sources, potentially &amp;quot;compressing weeks of research into hours,&amp;quot; according to Anthropic&amp;#x27;s use case descriptions.&lt;/p&gt;&lt;p&gt;Several companies have already integrated Haiku 4.5 and reported positive results. Guy Gur-Ari, co-founder of coding startup Augment, said the model &amp;quot;hit a sweet spot we didn&amp;#x27;t think was possible: near-frontier coding quality with blazing speed and cost efficiency.&amp;quot; In Augment&amp;#x27;s internal testing, Haiku 4.5 achieved 90% of Sonnet 4.5&amp;#x27;s performance while matching much larger models.&lt;/p&gt;&lt;p&gt;Jeff Wang, CEO of Windsurf, another coding-focused startup, said Haiku 4.5 &amp;quot;is blurring the lines&amp;quot; on traditional trade-offs between speed, cost and quality. &amp;quot;It&amp;#x27;s a fast frontier model that keeps costs efficient and signals where this class of models is headed.&amp;quot;&lt;/p&gt;&lt;p&gt;Jon Noronha, co-founder of presentation software company Gamma, reported that Haiku 4.5 &amp;quot;outperformed our current models on instruction-following for slide text generation, achieving 65% accuracy versus 44% from our premium tier model — that&amp;#x27;s a game-changer for our unit economics.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The price of progress: What plummeting AI costs mean for enterprise strategy&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For enterprises evaluating AI strategies, &lt;a href="https://www.anthropic.com/news/claude-haiku-4-5"&gt;&lt;u&gt;Haiku 4.5&lt;/u&gt;&lt;/a&gt; presents both opportunity and challenge. The opportunity lies in accessing sophisticated AI capabilities at dramatically lower costs, potentially making viable entire categories of applications that were previously too expensive to deploy at scale.&lt;/p&gt;&lt;p&gt;The challenge is keeping pace with a technology landscape that is evolving faster than most organizations can absorb. As Krieger noted in his recent podcast appearance, companies are moving beyond &amp;quot;&lt;a href="https://www.businessinsider.com/anthropic-cpo-companies-success-metrics-avoid-ai-fomo-2025-10"&gt;&lt;u&gt;AI FOMO&lt;/u&gt;&lt;/a&gt;&amp;quot; to demand concrete metrics and demonstrated value. But establishing those metrics and evaluation frameworks takes time — time that may be in short supply as competitors race ahead.&lt;/p&gt;&lt;p&gt;The shift from single-model deployments to multi-agent architectures also requires new ways of thinking about AI systems. Rather than viewing AI as a monolithic assistant, enterprises must learn to orchestrate multiple specialized agents, each optimized for particular tasks — more akin to managing a team than operating a tool.&lt;/p&gt;&lt;p&gt;The fundamental economics of AI are shifting with remarkable speed. Five months ago, Sonnet 4&amp;#x27;s capabilities commanded premium pricing and represented the cutting edge. Today, Haiku 4.5 delivers similar performance at a third of the cost. If that trajectory continues — and both Anthropic&amp;#x27;s release schedule and competitive pressure from OpenAI and Google suggest it will — the AI capabilities that seem remarkable today may be routine and inexpensive within a year.&lt;/p&gt;&lt;p&gt;For Anthropic, the challenge will be translating technical achievements into sustainable business growth while maintaining the safety-focused approach that differentiates it from competitors. The company&amp;#x27;s projected revenue growth to as much as $26 billion by 2026 suggests strong market traction, but achieving those targets will require continued innovation and successful execution across an increasingly complex product portfolio.&lt;/p&gt;&lt;p&gt;Whether enterprises will choose Claude over increasingly capable alternatives from OpenAI, Google and a growing field of competitors remains an open question. But Anthropic is making a clear bet: that the future of AI belongs not to whoever builds the single most powerful model, but to whoever can deliver the right intelligence, at the right speed, at the right price — and make it accessible to everyone.&lt;/p&gt;&lt;p&gt;In an industry where the promise of artificial intelligence has long outpaced reality, Anthropic is betting that delivering on that promise, faster and cheaper than anyone expected, will be enough to win. And with pricing dropping by two-thirds in just five months while performance holds steady, that promise is starting to look like reality.&lt;/p&gt;&lt;p&gt;

&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/anthropic-is-giving-away-its-powerful-claude-haiku-4-5-ai-for-free-to-take</guid><pubDate>Wed, 15 Oct 2025 19:00:00 +0000</pubDate></item><item><title>Army general says he’s using AI to improve “decision-making” (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/army-general-says-hes-using-ai-to-improve-decision-making/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        "AI is one thing that, as a commander, it’s been very, very interesting for me."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Team of army experts in data center analyzing missiles flight paths with deep learning tools.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last month, OpenAI published a usage study showing that nearly 15 percent of work-related conversations on ChatGPT had to deal with "making decisions and solving problems." Now comes word that at least one high-level member of the US military is using LLMs for the same purpose.&lt;/p&gt;
&lt;p&gt;At the Association of the US Army Conference in Washington, DC, this week, Maj. Gen. William "Hank" Taylor reportedly said that "Chat and I are really close lately," using a distressingly familiar diminutive nickname to refer to an unspecified AI chatbot. "AI is one thing that, as a commander, it’s been very, very interesting for me."&lt;/p&gt;
&lt;p&gt;Military-focused news site DefenseScoop reports that Taylor told a roundtable group of reporters that he and the Eighth Army he commands out of South Korea are "regularly using" AI to modernize their predictive analysis for logistical planning and operational purposes. That is helpful for paperwork tasks like "just being able to write our weekly reports and things," Taylor said, but it also aids in informing their overall direction.&lt;/p&gt;
&lt;p&gt;“One of the things that recently I’ve been personally working on with my soldiers is decision-making—individual decision-making," Taylor said. "And how [we make decisions] in our own individual life, when we make decisions, it’s important. So, that’s something I’ve been asking and trying to build models to help all of us. Especially, [on] how do I make decisions, personal decisions, right — that affect not only me, but my organization and overall readiness?"&lt;/p&gt;
&lt;p&gt;That's still a far cry from the &lt;em&gt;Terminator&lt;/em&gt; vision of autonomous AI weapon systems that take lethal decisions out of human hands. Still, using LLMs for military decision-making&amp;nbsp;might give pause to anyone familiar with the models' well-known propensity to confabulate fake citations and sycophantically flatter users.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In May, the Army rolled out the Army Enterprise LLM Workspace—built on the commercial Ask Sage platform—to streamline simple text-based tasks such as press releases and personnel descriptions. For other so-called "back office" military work, though, early tests have shown that generative AI might not always be the most efficient use of the military budget.&lt;/p&gt;
&lt;p&gt;"There are many times that we find folks using this technology to answer something that we could just do in a spreadsheet with one math problem, and we’re paying a lot more money to do it,” Army CIO Leonel Garciga told DefenseScoop in August. "Is the juice worth the squeeze? Or is there another way to get at the same problem that may be less cool from a tech perspective, but more viable from an execution perspective?"&lt;/p&gt;
&lt;p&gt;In 2023, the US State Department listed the best practices for military use of AI, focused on ethical and responsible deployment of AI tools within a human chain of command. The report stressed that humans should remain in control of "decisions concerning nuclear weapons employment" and should maintain the capability to "disengage or deactivate deployed systems that demonstrate unintended behavior."&lt;/p&gt;
&lt;p&gt;Since then, the military has shown interest in using AI technology in the field for everything from automated targeting systems on drones to "improving situational awareness" via an OpenAI partnership with military contractor Anduril. In January 2024, OpenAI removed a prohibition on "military and warfare uses" from ChatGPT's usage policies, while still barring customers from "devlop[ing] or us[ing] weapons" via the LLM.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        "AI is one thing that, as a commander, it’s been very, very interesting for me."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2219849110-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Team of army experts in data center analyzing missiles flight paths with deep learning tools.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last month, OpenAI published a usage study showing that nearly 15 percent of work-related conversations on ChatGPT had to deal with "making decisions and solving problems." Now comes word that at least one high-level member of the US military is using LLMs for the same purpose.&lt;/p&gt;
&lt;p&gt;At the Association of the US Army Conference in Washington, DC, this week, Maj. Gen. William "Hank" Taylor reportedly said that "Chat and I are really close lately," using a distressingly familiar diminutive nickname to refer to an unspecified AI chatbot. "AI is one thing that, as a commander, it’s been very, very interesting for me."&lt;/p&gt;
&lt;p&gt;Military-focused news site DefenseScoop reports that Taylor told a roundtable group of reporters that he and the Eighth Army he commands out of South Korea are "regularly using" AI to modernize their predictive analysis for logistical planning and operational purposes. That is helpful for paperwork tasks like "just being able to write our weekly reports and things," Taylor said, but it also aids in informing their overall direction.&lt;/p&gt;
&lt;p&gt;“One of the things that recently I’ve been personally working on with my soldiers is decision-making—individual decision-making," Taylor said. "And how [we make decisions] in our own individual life, when we make decisions, it’s important. So, that’s something I’ve been asking and trying to build models to help all of us. Especially, [on] how do I make decisions, personal decisions, right — that affect not only me, but my organization and overall readiness?"&lt;/p&gt;
&lt;p&gt;That's still a far cry from the &lt;em&gt;Terminator&lt;/em&gt; vision of autonomous AI weapon systems that take lethal decisions out of human hands. Still, using LLMs for military decision-making&amp;nbsp;might give pause to anyone familiar with the models' well-known propensity to confabulate fake citations and sycophantically flatter users.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In May, the Army rolled out the Army Enterprise LLM Workspace—built on the commercial Ask Sage platform—to streamline simple text-based tasks such as press releases and personnel descriptions. For other so-called "back office" military work, though, early tests have shown that generative AI might not always be the most efficient use of the military budget.&lt;/p&gt;
&lt;p&gt;"There are many times that we find folks using this technology to answer something that we could just do in a spreadsheet with one math problem, and we’re paying a lot more money to do it,” Army CIO Leonel Garciga told DefenseScoop in August. "Is the juice worth the squeeze? Or is there another way to get at the same problem that may be less cool from a tech perspective, but more viable from an execution perspective?"&lt;/p&gt;
&lt;p&gt;In 2023, the US State Department listed the best practices for military use of AI, focused on ethical and responsible deployment of AI tools within a human chain of command. The report stressed that humans should remain in control of "decisions concerning nuclear weapons employment" and should maintain the capability to "disengage or deactivate deployed systems that demonstrate unintended behavior."&lt;/p&gt;
&lt;p&gt;Since then, the military has shown interest in using AI technology in the field for everything from automated targeting systems on drones to "improving situational awareness" via an OpenAI partnership with military contractor Anduril. In January 2024, OpenAI removed a prohibition on "military and warfare uses" from ChatGPT's usage policies, while still barring customers from "devlop[ing] or us[ing] weapons" via the LLM.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/army-general-says-hes-using-ai-to-improve-decision-making/</guid><pubDate>Wed, 15 Oct 2025 21:31:00 +0000</pubDate></item><item><title>[NEW] Method teaches generative AI models to locate personalized objects (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/method-teaches-generative-ai-models-locate-personalized-objects-1016</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/MIT-VLM-local-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Say a person takes their French Bulldog, Bowser, to the dog park. Identifying Bowser as he plays among the other canines is easy for the dog-owner to do while onsite.&lt;/p&gt;&lt;p&gt;But if someone wants to use a generative AI model like GPT-5 to monitor their pet while they are at work, the model could fail at this basic task. Vision-language models like GPT-5 often excel at recognizing general objects, like a dog, but they perform poorly at locating personalized objects, like Bowser the French Bulldog.&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;To address this shortcoming, researchers from MIT and the MIT-IBM Watson AI Lab have introduced a new training method that teaches vision-language models to localize personalized objects in a scene.&lt;/p&gt;&lt;p&gt;Their method uses carefully prepared video-tracking data in which the same object is tracked across multiple frames. They designed the dataset so the model must focus on contextual clues to identify the personalized object, rather than relying on knowledge it previously memorized.&lt;/p&gt;&lt;p&gt;When given a few example images showing a personalized object, like someone’s pet, the retrained model is better able to identify the location of that same pet in a new image.&lt;/p&gt;&lt;p&gt;Models retrained with their method outperformed state-of-the-art systems at this task. Importantly, their technique leaves the rest of the model’s general abilities intact.&lt;/p&gt;&lt;p&gt;This new approach could help future AI systems track specific objects across time, like a child’s backpack, or localize objects of interest, such as a species of animal in ecological monitoring. It could also aid in the development of AI-driven assistive technologies that help visually impaired users find certain items in a room.&lt;/p&gt;&lt;p&gt;“Ultimately, we want these models to be able to learn from context, just like humans do. If a model can do this well, rather than retraining it for each new task, we could just provide a few examples and it would infer how to perform the task from that context. This is a very powerful ability,” says Jehanzeb Mirza, an MIT postdoc and senior author of a paper on this technique.&lt;/p&gt;&lt;p&gt;Mirza is joined on the paper by co-lead authors Sivan Doveh, a graduate student at Weizmann Institute of Science; and Nimrod Shabtay, a researcher at IBM Research; James Glass, a senior research scientist and the head of the Spoken Language Systems Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL); and others. The work will be presented at the International Conference on Computer Vision.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;An unexpected shortcoming&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Researchers have found that large language models (LLMs) can excel at learning from context. If they feed an LLM a few examples of a task, like addition problems, it can learn to answer new addition problems based on the context that has been provided.&lt;/p&gt;&lt;p&gt;A vision-language model (VLM) is essentially an LLM with a visual component connected to it, so the MIT researchers thought it would inherit the LLM’s in-context learning capabilities. But this is not the case.&lt;/p&gt;&lt;p&gt;“The research community has not been able to find a black-and-white answer to this particular problem yet. The bottleneck could arise from the fact that some visual information is lost in the process of merging the two components together, but we just don’t know,” Mirza says.&lt;/p&gt;&lt;p&gt;The researchers set out to improve VLMs abilities to do in-context localization, which involves finding a specific object in a new image. They focused on the data used to retrain existing VLMs for a new task, a process called fine-tuning.&lt;/p&gt;&lt;p&gt;Typical fine-tuning data are gathered from random sources and depict collections of everyday objects. One image might contain cars parked on a street, while another includes a bouquet of flowers.&lt;/p&gt;&lt;p&gt;“There is no real coherence in these data, so the model never learns to recognize the same object in multiple images,” he says.&lt;/p&gt;&lt;p&gt;To fix this problem, the researchers developed a new dataset by curating samples from existing video-tracking data. These data are video clips showing the same object moving through a scene, like a tiger walking across a grassland.&lt;/p&gt;&lt;p&gt;They cut frames from these videos and structured the dataset so each input would consist of multiple images showing the same object in different contexts, with example questions and answers about its location.&lt;/p&gt;&lt;p&gt;“By using multiple images of the same object in different contexts, we encourage the model to consistently localize that object of interest by focusing on the context,” Mirza explains.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Forcing the focus&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;But the researchers found that VLMs tend to cheat. Instead of answering based on context clues, they will identify the object using knowledge gained during pretraining.&lt;/p&gt;&lt;p&gt;For instance, since the model already learned that an image of a tiger and the label “tiger” are correlated, it could identify the tiger crossing the grassland based on this pretrained knowledge, instead of inferring from context.&lt;/p&gt;&lt;p&gt;To solve this problem, the researchers used pseudo-names rather than actual object category names in the dataset. In this case, they changed the name of the tiger to “Charlie.”&lt;/p&gt;&lt;p&gt;“It took us a while to figure out how to prevent the model from cheating. But we changed the game for the model. The model does not know that ‘Charlie’ can be a tiger, so it is forced to look at the context,” he says.&lt;/p&gt;&lt;p&gt;The researchers also faced challenges in finding the best way to prepare the data. If the frames are too close together, the background would not change enough to provide data diversity.&lt;/p&gt;&lt;p&gt;In the end, finetuning VLMs with this new dataset improved accuracy at personalized localization by about 12 percent on average. When they included the dataset with pseudo-names, the performance gains reached 21 percent.&lt;/p&gt;&lt;p&gt;As model size increases, their technique leads to greater performance gains.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to study possible reasons VLMs don’t inherit in-context learning capabilities from their base LLMs. In addition, they plan to explore additional mechanisms to improve the performance of a VLM without the need to retrain it with new data.&lt;/p&gt;&lt;p&gt;“This work reframes few-shot personalized object localization — adapting on the fly to the same object across new scenes — as an instruction-tuning problem and uses video-tracking sequences to teach VLMs to localize based on visual context rather than class priors. It also introduces the first benchmark for this setting with solid gains across open and proprietary VLMs. Given the immense significance of quick, instance-specific grounding — often without finetuning — for users of real-world workflows (such as robotics, augmented reality assistants, creative tools, etc.), the practical, data-centric recipe offered by this work can help enhance the widespread adoption of vision-language foundation models,” says Saurav Jha, a postdoc at the Mila-Quebec Artificial Intelligence Institute, who was not involved with this work.&lt;/p&gt;&lt;p&gt;Additional co-authors are Wei Lin, a research associate at Johannes Kepler University; Eli Schwartz, a research scientist at IBM Research; Hilde Kuehne, professor of computer science at&amp;nbsp;Tuebingen AI Center and an affiliated professor at the MIT-IBM Watson AI Lab; Raja Giryes, an associate professor at Tel Aviv University; Rogerio Feris, a principal scientist and manager at the MIT-IBM Watson AI Lab; Leonid Karlinsky, a principal research scientist at IBM Research; Assaf Arbelle, a senior research scientist at IBM Research; and Shimon Ullman, the Samy and Ruth Cohn Professor of Computer Science at the Weizmann Institute of Science.&lt;/p&gt;&lt;p&gt;This research was funded, in part, by the MIT-IBM Watson AI Lab.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/MIT-VLM-local-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Say a person takes their French Bulldog, Bowser, to the dog park. Identifying Bowser as he plays among the other canines is easy for the dog-owner to do while onsite.&lt;/p&gt;&lt;p&gt;But if someone wants to use a generative AI model like GPT-5 to monitor their pet while they are at work, the model could fail at this basic task. Vision-language models like GPT-5 often excel at recognizing general objects, like a dog, but they perform poorly at locating personalized objects, like Bowser the French Bulldog.&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/p&gt;&lt;p&gt;To address this shortcoming, researchers from MIT and the MIT-IBM Watson AI Lab have introduced a new training method that teaches vision-language models to localize personalized objects in a scene.&lt;/p&gt;&lt;p&gt;Their method uses carefully prepared video-tracking data in which the same object is tracked across multiple frames. They designed the dataset so the model must focus on contextual clues to identify the personalized object, rather than relying on knowledge it previously memorized.&lt;/p&gt;&lt;p&gt;When given a few example images showing a personalized object, like someone’s pet, the retrained model is better able to identify the location of that same pet in a new image.&lt;/p&gt;&lt;p&gt;Models retrained with their method outperformed state-of-the-art systems at this task. Importantly, their technique leaves the rest of the model’s general abilities intact.&lt;/p&gt;&lt;p&gt;This new approach could help future AI systems track specific objects across time, like a child’s backpack, or localize objects of interest, such as a species of animal in ecological monitoring. It could also aid in the development of AI-driven assistive technologies that help visually impaired users find certain items in a room.&lt;/p&gt;&lt;p&gt;“Ultimately, we want these models to be able to learn from context, just like humans do. If a model can do this well, rather than retraining it for each new task, we could just provide a few examples and it would infer how to perform the task from that context. This is a very powerful ability,” says Jehanzeb Mirza, an MIT postdoc and senior author of a paper on this technique.&lt;/p&gt;&lt;p&gt;Mirza is joined on the paper by co-lead authors Sivan Doveh, a graduate student at Weizmann Institute of Science; and Nimrod Shabtay, a researcher at IBM Research; James Glass, a senior research scientist and the head of the Spoken Language Systems Group in the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL); and others. The work will be presented at the International Conference on Computer Vision.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;An unexpected shortcoming&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Researchers have found that large language models (LLMs) can excel at learning from context. If they feed an LLM a few examples of a task, like addition problems, it can learn to answer new addition problems based on the context that has been provided.&lt;/p&gt;&lt;p&gt;A vision-language model (VLM) is essentially an LLM with a visual component connected to it, so the MIT researchers thought it would inherit the LLM’s in-context learning capabilities. But this is not the case.&lt;/p&gt;&lt;p&gt;“The research community has not been able to find a black-and-white answer to this particular problem yet. The bottleneck could arise from the fact that some visual information is lost in the process of merging the two components together, but we just don’t know,” Mirza says.&lt;/p&gt;&lt;p&gt;The researchers set out to improve VLMs abilities to do in-context localization, which involves finding a specific object in a new image. They focused on the data used to retrain existing VLMs for a new task, a process called fine-tuning.&lt;/p&gt;&lt;p&gt;Typical fine-tuning data are gathered from random sources and depict collections of everyday objects. One image might contain cars parked on a street, while another includes a bouquet of flowers.&lt;/p&gt;&lt;p&gt;“There is no real coherence in these data, so the model never learns to recognize the same object in multiple images,” he says.&lt;/p&gt;&lt;p&gt;To fix this problem, the researchers developed a new dataset by curating samples from existing video-tracking data. These data are video clips showing the same object moving through a scene, like a tiger walking across a grassland.&lt;/p&gt;&lt;p&gt;They cut frames from these videos and structured the dataset so each input would consist of multiple images showing the same object in different contexts, with example questions and answers about its location.&lt;/p&gt;&lt;p&gt;“By using multiple images of the same object in different contexts, we encourage the model to consistently localize that object of interest by focusing on the context,” Mirza explains.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Forcing the focus&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;But the researchers found that VLMs tend to cheat. Instead of answering based on context clues, they will identify the object using knowledge gained during pretraining.&lt;/p&gt;&lt;p&gt;For instance, since the model already learned that an image of a tiger and the label “tiger” are correlated, it could identify the tiger crossing the grassland based on this pretrained knowledge, instead of inferring from context.&lt;/p&gt;&lt;p&gt;To solve this problem, the researchers used pseudo-names rather than actual object category names in the dataset. In this case, they changed the name of the tiger to “Charlie.”&lt;/p&gt;&lt;p&gt;“It took us a while to figure out how to prevent the model from cheating. But we changed the game for the model. The model does not know that ‘Charlie’ can be a tiger, so it is forced to look at the context,” he says.&lt;/p&gt;&lt;p&gt;The researchers also faced challenges in finding the best way to prepare the data. If the frames are too close together, the background would not change enough to provide data diversity.&lt;/p&gt;&lt;p&gt;In the end, finetuning VLMs with this new dataset improved accuracy at personalized localization by about 12 percent on average. When they included the dataset with pseudo-names, the performance gains reached 21 percent.&lt;/p&gt;&lt;p&gt;As model size increases, their technique leads to greater performance gains.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to study possible reasons VLMs don’t inherit in-context learning capabilities from their base LLMs. In addition, they plan to explore additional mechanisms to improve the performance of a VLM without the need to retrain it with new data.&lt;/p&gt;&lt;p&gt;“This work reframes few-shot personalized object localization — adapting on the fly to the same object across new scenes — as an instruction-tuning problem and uses video-tracking sequences to teach VLMs to localize based on visual context rather than class priors. It also introduces the first benchmark for this setting with solid gains across open and proprietary VLMs. Given the immense significance of quick, instance-specific grounding — often without finetuning — for users of real-world workflows (such as robotics, augmented reality assistants, creative tools, etc.), the practical, data-centric recipe offered by this work can help enhance the widespread adoption of vision-language foundation models,” says Saurav Jha, a postdoc at the Mila-Quebec Artificial Intelligence Institute, who was not involved with this work.&lt;/p&gt;&lt;p&gt;Additional co-authors are Wei Lin, a research associate at Johannes Kepler University; Eli Schwartz, a research scientist at IBM Research; Hilde Kuehne, professor of computer science at&amp;nbsp;Tuebingen AI Center and an affiliated professor at the MIT-IBM Watson AI Lab; Raja Giryes, an associate professor at Tel Aviv University; Rogerio Feris, a principal scientist and manager at the MIT-IBM Watson AI Lab; Leonid Karlinsky, a principal research scientist at IBM Research; Assaf Arbelle, a senior research scientist at IBM Research; and Shimon Ullman, the Samy and Ruth Cohn Professor of Computer Science at the Weizmann Institute of Science.&lt;/p&gt;&lt;p&gt;This research was funded, in part, by the MIT-IBM Watson AI Lab.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/method-teaches-generative-ai-models-locate-personalized-objects-1016</guid><pubDate>Thu, 16 Oct 2025 04:00:00 +0000</pubDate></item></channel></rss>