<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 09 Dec 2025 12:50:36 +0000</lastBuildDate><item><title>Z.ai debuts open source GLM-4.6V, a native tool-calling vision model for multimodal reasoning (AI | VentureBeat)</title><link>https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for</link><description>[unable to retrieve full-text content]&lt;p&gt;Chinese AI startup Zhipu AI aka &lt;a href="https://z.ai/blog/glm-4.6v"&gt;&lt;b&gt;Z.ai has released its GLM-4.6V series&lt;/b&gt;&lt;/a&gt;, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. &lt;/p&gt;&lt;p&gt;The release includes two models in &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; sizes: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V (106B)&lt;/b&gt;, a larger 106-billion parameter model aimed at cloud-scale inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V-Flash (9B)&lt;/b&gt;, a smaller model of only 9 billion parameters designed for low-latency, local applications&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Recall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.&lt;/p&gt;&lt;p&gt;However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.&lt;/p&gt;&lt;p&gt;The defining innovation in this series is the introduction of &lt;b&gt;native function calling&lt;/b&gt; in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. &lt;/p&gt;&lt;p&gt;With a 128,000 token context length (equivalent to a 300-page novel&amp;#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&amp;#x27;s available in the following formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/vlm/glm-4.6v"&gt;API access&lt;/a&gt; via OpenAI-compatible interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.z.ai"&gt;Try the demo&lt;/a&gt; on Zhipu’s web interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Download weights&lt;/a&gt; from Hugging Face&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Desktop assistant app available on &lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App"&gt;Hugging Face Spaces&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Licensing and Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM‑4.6V and GLM‑4.6V‑Flash are distributed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT license&lt;/a&gt;, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. &lt;/p&gt;&lt;p&gt;This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.&lt;/p&gt;&lt;p&gt;Model weights and documentation are publicly hosted on &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face&lt;/a&gt;, with supporting code and tooling available on &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Architecture and Technical Capabilities&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. &lt;/p&gt;&lt;p&gt;Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. &lt;/p&gt;&lt;p&gt;Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.&lt;/p&gt;&lt;p&gt;A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. &lt;/p&gt;&lt;p&gt;In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.&lt;/p&gt;&lt;p&gt;On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Native Multimodal Tool Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.&lt;/p&gt;&lt;p&gt;The tool invocation mechanism works bi-directionally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In practice, this means GLM-4.6V can complete tasks such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Generating structured reports from mixed-format documents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Performing visual audit of candidate images&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Automatically cropping figures from papers during generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conducting visual web search and answering multimodal queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;High Performance Benchmarks Compared to Other Similar-Sized Models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. &lt;/p&gt;&lt;p&gt;According to the benchmark chart released by Zhipu AI:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Example scores from the leaderboard include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Frontend Automation and Long-Context Workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Replicate pixel-accurate HTML/CSS/JS from UI screenshots&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accept natural language editing commands to modify layouts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify and manipulate specific UI components visually&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.&lt;/p&gt;&lt;p&gt;In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;150 pages of text (input)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;200 slide decks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1-hour videos&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Training and Reinforcement Learning&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progress&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial grounding&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Function-aware training: Uses structured tags (e.g., &amp;lt;think&amp;gt;, &amp;lt;answer&amp;gt;, &amp;lt;|begin_of_box|&amp;gt;) to align reasoning and answer formatting&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domains&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing (API)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash: Free&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Compared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:&lt;/p&gt;&lt;p&gt;&lt;i&gt;USD per 1M tokens — sorted lowest → highest total cost&lt;/i&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.56&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM‑4.6V&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.30&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.90&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.20&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/overview/pricing"&gt;Z.AI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/pricing"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$17.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$90.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;b&gt;Previous Releases: GLM‑4.5 Series and Enterprise Applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. &lt;/p&gt;&lt;p&gt;The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. &lt;/p&gt;&lt;p&gt;The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.&lt;/p&gt;&lt;p&gt;Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipel&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ecosystem Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Integrated visual tool usage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Structured multimodal generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Agent-oriented memory and decision logic&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. &lt;/p&gt;&lt;p&gt;The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Takeaway for Enterprise Leaders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems&lt;!-- --&gt;.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Chinese AI startup Zhipu AI aka &lt;a href="https://z.ai/blog/glm-4.6v"&gt;&lt;b&gt;Z.ai has released its GLM-4.6V series&lt;/b&gt;&lt;/a&gt;, a new generation of open-source vision-language models (VLMs) optimized for multimodal reasoning, frontend automation, and high-efficiency deployment. &lt;/p&gt;&lt;p&gt;The release includes two models in &amp;quot;large&amp;quot; and &amp;quot;small&amp;quot; sizes: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V (106B)&lt;/b&gt;, a larger 106-billion parameter model aimed at cloud-scale inference&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GLM-4.6V-Flash (9B)&lt;/b&gt;, a smaller model of only 9 billion parameters designed for low-latency, local applications&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Recall that generally speaking, models with more parameters — or internal settings governing their behavior, i.e. weights and biases — are more powerful, performant, and capable of performing at a higher general level across more varied tasks.&lt;/p&gt;&lt;p&gt;However, smaller models can offer better efficiency for edge or real-time applications where latency and resource constraints are critical.&lt;/p&gt;&lt;p&gt;The defining innovation in this series is the introduction of &lt;b&gt;native function calling&lt;/b&gt; in a vision-language model—enabling direct use of tools such as search, cropping, or chart recognition with visual inputs. &lt;/p&gt;&lt;p&gt;With a 128,000 token context length (equivalent to a 300-page novel&amp;#x27;s worth of text exchanged in a single input/output interaction with the user) and state-of-the-art (SoTA) results across more than 20 benchmarks, the GLM-4.6V series positions itself as a highly competitive alternative to both closed and open-source VLMs. It&amp;#x27;s available in the following formats:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/vlm/glm-4.6v"&gt;API access&lt;/a&gt; via OpenAI-compatible interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://chat.z.ai"&gt;Try the demo&lt;/a&gt; on Zhipu’s web interface&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Download weights&lt;/a&gt; from Hugging Face&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Desktop assistant app available on &lt;a href="https://huggingface.co/spaces/zai-org/GLM-4.5V-Demo-App"&gt;Hugging Face Spaces&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;Licensing and Enterprise Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM‑4.6V and GLM‑4.6V‑Flash are distributed under the &lt;a href="https://opensource.org/licenses/MIT"&gt;MIT license&lt;/a&gt;, a permissive open-source license that allows free commercial and non-commercial use, modification, redistribution, and local deployment without obligation to open-source derivative works. &lt;/p&gt;&lt;p&gt;This licensing model makes the series suitable for enterprise adoption, including scenarios that require full control over infrastructure, compliance with internal governance, or air-gapped environments.&lt;/p&gt;&lt;p&gt;Model weights and documentation are publicly hosted on &lt;a href="https://huggingface.co/collections/zai-org/glm-46v"&gt;Hugging Face&lt;/a&gt;, with supporting code and tooling available on &lt;a href="https://github.com/zai-org/GLM-V"&gt;GitHub&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;The MIT license ensures maximum flexibility for integration into proprietary systems, including internal tools, production pipelines, and edge deployments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Architecture and Technical Capabilities&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V models follow a conventional encoder-decoder architecture with significant adaptations for multimodal input. &lt;/p&gt;&lt;p&gt;Both models incorporate a Vision Transformer (ViT) encoder—based on AIMv2-Huge—and an MLP projector to align visual features with a large language model (LLM) decoder. &lt;/p&gt;&lt;p&gt;Video inputs benefit from 3D convolutions and temporal compression, while spatial encoding is handled using 2D-RoPE and bicubic interpolation of absolute positional embeddings.&lt;/p&gt;&lt;p&gt;A key technical feature is the system’s support for arbitrary image resolutions and aspect ratios, including wide panoramic inputs up to 200:1. &lt;/p&gt;&lt;p&gt;In addition to static image and document parsing, GLM-4.6V can ingest temporal sequences of video frames with explicit timestamp tokens, enabling robust temporal reasoning.&lt;/p&gt;&lt;p&gt;On the decoding side, the model supports token generation aligned with function-calling protocols, allowing for structured reasoning across text, image, and tool outputs. This is supported by extended tokenizer vocabulary and output formatting templates to ensure consistent API or agent compatibility.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Native Multimodal Tool Use&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V introduces native multimodal function calling, allowing visual assets—such as screenshots, images, and documents—to be passed directly as parameters to tools. This eliminates the need for intermediate text-only conversions, which have historically introduced information loss and complexity.&lt;/p&gt;&lt;p&gt;The tool invocation mechanism works bi-directionally:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Input tools can be passed images or videos directly (e.g., document pages to crop or analyze).&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Output tools such as chart renderers or web snapshot utilities return visual data, which GLM-4.6V integrates directly into the reasoning chain.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In practice, this means GLM-4.6V can complete tasks such as:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Generating structured reports from mixed-format documents&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Performing visual audit of candidate images&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Automatically cropping figures from papers during generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Conducting visual web search and answering multimodal queries&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;&lt;b&gt;High Performance Benchmarks Compared to Other Similar-Sized Models&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GLM-4.6V was evaluated across more than 20 public benchmarks covering general VQA, chart understanding, OCR, STEM reasoning, frontend replication, and multimodal agents. &lt;/p&gt;&lt;p&gt;According to the benchmark chart released by Zhipu AI:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V (106B) achieves SoTA or near-SoTA scores among open-source models of comparable size (106B) on MMBench, MathVista, MMLongBench, ChartQAPro, RefCOCO, TreeBench, and more.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash (9B) outperforms other lightweight models (e.g., Qwen3-VL-8B, GLM-4.1V-9B) across almost all categories tested.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;The 106B model’s 128K-token window allows it to outperform larger models like Step-3 (321B) and Qwen3-VL-235B on long-context document tasks, video summarization, and structured multimodal reasoning.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Example scores from the leaderboard include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;MathVista: 88.2 (GLM-4.6V) vs. 84.6 (GLM-4.5V) vs. 81.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;WebVoyager: 81.0 vs. 68.4 (Qwen3-VL-8B)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ref-L4-test: 88.9 vs. 89.5 (GLM-4.5V), but with better grounding fidelity at 87.7 (Flash) vs. 86.8&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Both models were evaluated using the vLLM inference backend and support SGLang for video-based tasks.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Frontend Automation and Long-Context Workflows&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI emphasized GLM-4.6V’s ability to support frontend development workflows. The model can:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Replicate pixel-accurate HTML/CSS/JS from UI screenshots&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Accept natural language editing commands to modify layouts&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Identify and manipulate specific UI components visually&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This capability is integrated into an end-to-end visual programming interface, where the model iterates on layout, design intent, and output code using its native understanding of screen captures.&lt;/p&gt;&lt;p&gt;In long-document scenarios, GLM-4.6V can process up to 128,000 tokens—enabling a single inference pass across:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;150 pages of text (input)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;200 slide decks&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;1-hour videos&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI reported successful use of the model in financial analysis across multi-document corpora and in summarizing full-length sports broadcasts with timestamped event detection.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Training and Reinforcement Learning&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The model was trained using multi-stage pre-training followed by supervised fine-tuning (SFT) and reinforcement learning (RL). Key innovations include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Curriculum Sampling (RLCS): Dynamically adjusts the difficulty of training samples based on model progress&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Multi-domain reward systems: Task-specific verifiers for STEM, chart reasoning, GUI agents, video QA, and spatial grounding&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Function-aware training: Uses structured tags (e.g., &amp;lt;think&amp;gt;, &amp;lt;answer&amp;gt;, &amp;lt;|begin_of_box|&amp;gt;) to align reasoning and answer formatting&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The reinforcement learning pipeline emphasizes verifiable rewards (RLVR) over human feedback (RLHF) for scalability, and avoids KL/entropy losses to stabilize training across multimodal domains&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing (API)&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Zhipu AI offers competitive pricing for the GLM-4.6V series, with both the flagship model and its lightweight variant positioned for high accessibility.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V: $0.30 (input) / $0.90 (output) per 1M tokens&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;GLM-4.6V-Flash: Free&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Compared to major vision-capable and text-first LLMs, GLM-4.6V is among the most cost-efficient for multimodal reasoning at scale. Below is a comparative snapshot of pricing across providers:&lt;/p&gt;&lt;p&gt;&lt;i&gt;USD per 1M tokens — sorted lowest → highest total cost&lt;/i&gt;&lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Total Cost&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Source&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 4.5 Turbo&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.11&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.45&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.56&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GLM‑4.6V&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.30&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$0.90&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;$1.20&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.z.ai/guides/overview/pricing"&gt;Z.AI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4.1 Fast (non-reasoning)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-chat (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;deepseek-reasoner (V3.2-Exp)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.28&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.42&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.70&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://api-docs.deepseek.com/quick_start/pricing"&gt;DeepSeek&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen 3 Plus&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.20&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;ERNIE 5.0&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.85&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Blfmc9do4"&gt;Qianfan&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Qwen-Max&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.60&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$6.40&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://www.alibabacloud.com/en/campaign/qwen-ai-landing-page?_p_lc=1&amp;amp;src=qwenai"&gt;Alibaba Cloud&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;GPT-5.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://openai.com/pricing"&gt;OpenAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$11.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (≤200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$12.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$14.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 2.5 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$17.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Grok 4 (0709)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$3.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.x.ai/docs/models?cluster=us-east-1#detailed-pricing-for-all-grok-models"&gt;xAI&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Gemini 3 Pro (&amp;gt;200K)&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$4.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$18.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$22.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/pricing"&gt;Google&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;Claude Opus 4.1&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$15.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$75.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$90.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;a href="https://docs.anthropic.com/claude/docs/models-overview"&gt;Anthropic&lt;/a&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;&lt;b&gt;Previous Releases: GLM‑4.5 Series and Enterprise Applications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Prior to GLM‑4.6V, Z.ai released the GLM‑4.5 family in mid-2025, establishing the company as a serious contender in open-source LLM development. &lt;/p&gt;&lt;p&gt;The flagship GLM‑4.5 and its smaller sibling GLM‑4.5‑Air both support reasoning, tool use, coding, and agentic behaviors, while offering strong performance across standard benchmarks. &lt;/p&gt;&lt;p&gt;The models introduced dual reasoning modes (“thinking” and “non-thinking”) and could automatically generate complete PowerPoint presentations from a single prompt — a feature positioned for use in enterprise reporting, education, and internal comms workflows. Z.ai also extended the GLM‑4.5 series with additional variants such as GLM‑4.5‑X, AirX, and Flash, targeting ultra-fast inference and low-cost scenarios.&lt;/p&gt;&lt;p&gt;Together, these features position the GLM‑4.5 series as a cost-effective, open, and production-ready alternative for enterprises needing autonomy over model deployment, lifecycle management, and integration pipel&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Ecosystem Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GLM-4.6V release represents a notable advance in open-source multimodal AI. While large vision-language models have proliferated over the past year, few offer:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Integrated visual tool usage&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Structured multimodal generation&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Agent-oriented memory and decision logic&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Zhipu AI’s emphasis on “closing the loop” from perception to action via native function calling marks a step toward agentic multimodal systems. &lt;/p&gt;&lt;p&gt;The model’s architecture and training pipeline show a continued evolution of the GLM family, positioning it competitively alongside offerings like OpenAI’s GPT-4V and Google DeepMind’s Gemini-VL.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Takeaway for Enterprise Leaders&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;With GLM-4.6V, Zhipu AI introduces an open-source VLM capable of native visual tool use, long-context reasoning, and frontend automation. It sets new performance marks among models of similar size and provides a scalable platform for building agentic, multimodal AI systems&lt;!-- --&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/z-ai-debuts-open-source-glm-4-6v-a-native-tool-calling-vision-model-for</guid><pubDate>Tue, 09 Dec 2025 01:03:00 +0000</pubDate></item><item><title>[NEW] Newsweek: Building AI-resilience for the next era of information (AI News)</title><link>https://www.artificialintelligence-news.com/news/newsweek-building-ai-resilience-for-the-next-era-of-information/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/pexels-markusspiske-330771-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence is transforming the way information is created, summarised, and delivered. For publishers, the shift is already visible. Search engines provide AI-generated overviews, users get answers without clicking, and content is scraped by large language models that train on decades of journalism.&lt;/p&gt;&lt;p&gt;In this environment one question remains: How does a publisher survive when the traditional rules of distribution fall apart? Dev Pragad, the CEO of Newsweek, is offering one of the clearest answers.&lt;/p&gt;&lt;p&gt;Pragad’s strategy begins with an acknowledgement of reality. In his view, publishers need to accept the search-driven traffic model that defined the digital era is no longer dependable. AI-powered answer engines are restructuring the way users interact with information. A user might ask a question, receive a summary generated by an LLM, and never visit the publisher’s website. Page views become unpredictable, programmatic advertising becomes unstable, and legacy structures become vulnerable.&lt;/p&gt;&lt;p&gt;Rather than respond with fear, Dev Pragad has taken a proactive approach grounded in three core areas.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Redesign the brand so that it remains visually strong in any context.&lt;/li&gt;&lt;li&gt;Diversify revenue so the business is not tied to a single distribution mechanism.&lt;/li&gt;&lt;li&gt;Expand those content formats that are less dependent on search engines and more aligned with the new habits of audiences.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In September 2025 Newsweek unveiled its redesigned identity under the tagline ‘A World Drawn Closer’. This redesign, created with 2×4, introduced a refined wordmark, a bold ‘N’ icon, and a unified visual system used for print, digital, video and international editions. For the AI era such a coherence matters. An AI summary might reference Newsweek visually, a feed might show a thumbnail with minimal space, and a social clip might require brand clarity in a fraction of a second.&lt;/p&gt;&lt;p&gt;The new design prepares Newsweek for the new reality by making the brand easy to identify.&lt;/p&gt;&lt;p&gt;The editorial shift under Dev Pragad is also significant. &lt;em&gt;Newsmakers&lt;/em&gt;, the series that features cultural leaders (Spike Lee, Liam Neeson, and Clark Hunt, for example), is available free on YouTube and digital platforms.&lt;/p&gt;&lt;p&gt;The decision to make the series accessible at no cost is strategic. Video that travels across platforms is harder for AI summaries to replace. It is more immersive, and it reaches audiences directly, plus it builds brand equity and cultural relevance beyond search traffic.&lt;/p&gt;&lt;p&gt;In interviews Pragad has said Newsmakers represents the future of journalism, blending storytelling, accessibility and platform fluency. Each episode is supported by a companion article and a collectable cover, creating a cross media footprint that is not reliant on one format or algorithm.&lt;/p&gt;&lt;p&gt;In addition to editorial innovation, Newsweek is evolving its business architecture to withstand AI driven disruption. While digital advertising remains part of the company’s revenue model, Pragad has expanded the title into events, direct advertising relationships, data driven rankings, and verticals such as healthcare. This approach creates multiple revenue streams that do not depend on unpredictable traffic patterns.&lt;/p&gt;&lt;p&gt;Another factor shaping Newsweek’s AI strategy is the way large language models scrape content. Newsweek monitors this activity through systems like TollBit which track bot behaviour and provide insight into how often AI engines attempt to access the site. Pragad has turned down licensing deals that undervalued the worth of Newsweek’s archives and has advocated for fair compensation for the use of publisher content. He believes publishers must negotiate collectively and maintain leverage rather than rush into agreements that minimise the value of their intellectual property.&lt;/p&gt;&lt;p&gt;The redesign is also in response to the challenge of brand recognition in a world dominated by fast-moving feeds and AI-driven surfaces. Clear typography, concise visual hierarchy, and a distinct colour palette support recognition across AI-generated snippets, smart devices, social networks, and search previews. This is a design built for the realities of the modern information economy.&lt;/p&gt;&lt;p&gt;Newsweek’s growth reflects the strength of these choices. The publication has been recognised as one of the fastest-rising digital news destinations in the US, and global audience numbers continue to climb. Although the company continues to evolve its revenue structure, its editorial mission remains grounded in fairness and trust. The new tagline reflects that commitment. Journalism brings the world closer when it is clear, accessible, and human-centred.&lt;/p&gt;&lt;p&gt;The AI revolution has placed publishers in a difficult position, yet it has also opened an opportunity. Those willing to rethink design, editorial formats, AI licensing, distribution, and revenue have the chance to define what comes next. Under Dev Pragad Newsweek is doing exactly that. The company is no longer relying on assumptions about how audiences discover information. It’s building a future in which journalism can coexist with AI, not be erased by it.&lt;/p&gt;&lt;p&gt;Dev Pragad has created a blueprint that demonstrates how a legacy publisher can reinvent itself for the AI age. Through design clarity, accessible cultural storytelling, diversified business models, and a firm stance on content value, he is positioning Newsweek not only to survive, but to lead in a world where information flows faster and more unpredictably than before. The result is a modern media entity built for a new era of intelligence, creativity, and connection.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/pexels-markusspiske-330771-scaled.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Artificial intelligence is transforming the way information is created, summarised, and delivered. For publishers, the shift is already visible. Search engines provide AI-generated overviews, users get answers without clicking, and content is scraped by large language models that train on decades of journalism.&lt;/p&gt;&lt;p&gt;In this environment one question remains: How does a publisher survive when the traditional rules of distribution fall apart? Dev Pragad, the CEO of Newsweek, is offering one of the clearest answers.&lt;/p&gt;&lt;p&gt;Pragad’s strategy begins with an acknowledgement of reality. In his view, publishers need to accept the search-driven traffic model that defined the digital era is no longer dependable. AI-powered answer engines are restructuring the way users interact with information. A user might ask a question, receive a summary generated by an LLM, and never visit the publisher’s website. Page views become unpredictable, programmatic advertising becomes unstable, and legacy structures become vulnerable.&lt;/p&gt;&lt;p&gt;Rather than respond with fear, Dev Pragad has taken a proactive approach grounded in three core areas.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Redesign the brand so that it remains visually strong in any context.&lt;/li&gt;&lt;li&gt;Diversify revenue so the business is not tied to a single distribution mechanism.&lt;/li&gt;&lt;li&gt;Expand those content formats that are less dependent on search engines and more aligned with the new habits of audiences.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;In September 2025 Newsweek unveiled its redesigned identity under the tagline ‘A World Drawn Closer’. This redesign, created with 2×4, introduced a refined wordmark, a bold ‘N’ icon, and a unified visual system used for print, digital, video and international editions. For the AI era such a coherence matters. An AI summary might reference Newsweek visually, a feed might show a thumbnail with minimal space, and a social clip might require brand clarity in a fraction of a second.&lt;/p&gt;&lt;p&gt;The new design prepares Newsweek for the new reality by making the brand easy to identify.&lt;/p&gt;&lt;p&gt;The editorial shift under Dev Pragad is also significant. &lt;em&gt;Newsmakers&lt;/em&gt;, the series that features cultural leaders (Spike Lee, Liam Neeson, and Clark Hunt, for example), is available free on YouTube and digital platforms.&lt;/p&gt;&lt;p&gt;The decision to make the series accessible at no cost is strategic. Video that travels across platforms is harder for AI summaries to replace. It is more immersive, and it reaches audiences directly, plus it builds brand equity and cultural relevance beyond search traffic.&lt;/p&gt;&lt;p&gt;In interviews Pragad has said Newsmakers represents the future of journalism, blending storytelling, accessibility and platform fluency. Each episode is supported by a companion article and a collectable cover, creating a cross media footprint that is not reliant on one format or algorithm.&lt;/p&gt;&lt;p&gt;In addition to editorial innovation, Newsweek is evolving its business architecture to withstand AI driven disruption. While digital advertising remains part of the company’s revenue model, Pragad has expanded the title into events, direct advertising relationships, data driven rankings, and verticals such as healthcare. This approach creates multiple revenue streams that do not depend on unpredictable traffic patterns.&lt;/p&gt;&lt;p&gt;Another factor shaping Newsweek’s AI strategy is the way large language models scrape content. Newsweek monitors this activity through systems like TollBit which track bot behaviour and provide insight into how often AI engines attempt to access the site. Pragad has turned down licensing deals that undervalued the worth of Newsweek’s archives and has advocated for fair compensation for the use of publisher content. He believes publishers must negotiate collectively and maintain leverage rather than rush into agreements that minimise the value of their intellectual property.&lt;/p&gt;&lt;p&gt;The redesign is also in response to the challenge of brand recognition in a world dominated by fast-moving feeds and AI-driven surfaces. Clear typography, concise visual hierarchy, and a distinct colour palette support recognition across AI-generated snippets, smart devices, social networks, and search previews. This is a design built for the realities of the modern information economy.&lt;/p&gt;&lt;p&gt;Newsweek’s growth reflects the strength of these choices. The publication has been recognised as one of the fastest-rising digital news destinations in the US, and global audience numbers continue to climb. Although the company continues to evolve its revenue structure, its editorial mission remains grounded in fairness and trust. The new tagline reflects that commitment. Journalism brings the world closer when it is clear, accessible, and human-centred.&lt;/p&gt;&lt;p&gt;The AI revolution has placed publishers in a difficult position, yet it has also opened an opportunity. Those willing to rethink design, editorial formats, AI licensing, distribution, and revenue have the chance to define what comes next. Under Dev Pragad Newsweek is doing exactly that. The company is no longer relying on assumptions about how audiences discover information. It’s building a future in which journalism can coexist with AI, not be erased by it.&lt;/p&gt;&lt;p&gt;Dev Pragad has created a blueprint that demonstrates how a legacy publisher can reinvent itself for the AI age. Through design clarity, accessible cultural storytelling, diversified business models, and a firm stance on content value, he is positioning Newsweek not only to survive, but to lead in a world where information flows faster and more unpredictably than before. The result is a modern media entity built for a new era of intelligence, creativity, and connection.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/newsweek-building-ai-resilience-for-the-next-era-of-information/</guid><pubDate>Tue, 09 Dec 2025 08:29:21 +0000</pubDate></item><item><title>[NEW] How people really use AI: The surprising truth from analysing billions of interactions (AI News)</title><link>https://www.artificialintelligence-news.com/news/how-people-really-use-ai-the-surprising-truth-from-analysing-billions-of-interactions/</link><description>&lt;p&gt;For the past year, we’ve been told that artificial intelligence is&amp;nbsp;revolutionising&amp;nbsp;productivity—helping us write emails, generate code, and summarise documents. But what if the reality of how people actually use AI is completely different from what we’ve been led to believe?&lt;/p&gt;&lt;p&gt;A data-driven&amp;nbsp;study&amp;nbsp;by OpenRouter has just pulled back the curtain on real-world AI usage by&amp;nbsp;analysing&amp;nbsp;over 100 trillion tokens—essentially billions upon billions of conversations and interactions with large language models like ChatGPT, Claude, and dozens of others. The findings challenge many assumptions about the AI revolution.&lt;/p&gt;&lt;p&gt;​​OpenRouter is a multi-model AI inference platform that routes requests across more than 300 models from over 60 providers—from OpenAI and Anthropic to open-source alternatives like DeepSeek and Meta’s LLaMA.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With over 50% of its usage originating outside the United States and serving millions of developers globally, the platform offers a unique cross-section of how AI is actually deployed across different geographies, use cases, and user types.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Importantly, the study&amp;nbsp;analysed&amp;nbsp;metadata from billions of interactions without accessing the actual text of conversations, preserving user privacy while revealing behavioural patterns.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111217" height="677" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.36.21-PM-1024x677.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Open-source AI models have grown to capture approximately one-third of total usage by late 2025, with notable spikes following major releases.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-roleplay-revolution-nobody-saw-coming"&gt;The roleplay revolution nobody saw coming&lt;/h3&gt;&lt;p&gt;Perhaps the most surprising discovery: more than half of all open-source AI model usage isn’t for productivity at all. It’s for roleplay and creative storytelling.&lt;/p&gt;&lt;p&gt;Yes, you read that right. While tech executives tout AI’s potential to transform business, users are spending the majority of their time engaging in character-driven conversations, interactive fiction, and gaming scenarios.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Over 50% of open-source model interactions fall into this category, dwarfing even programming assistance.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111218" height="576" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.38.37-PM-1024x576.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;“This counters an assumption that LLMs are mostly used for writing code, emails, or summaries,” the report states. “In reality, many users engage with these models for companionship or exploration.”&lt;/p&gt;&lt;p&gt;This isn’t just casual chatting. The data shows users treat AI models as structured roleplaying engines, with 60% of roleplay tokens falling under specific gaming scenarios and creative writing contexts. It’s a massive, largely invisible use case that’s reshaping how AI companies think about their products.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-programming-s-meteoric-rise"&gt;Programming’s meteoric rise&lt;/h3&gt;&lt;p&gt;While roleplay dominates open-source usage, programming has become the fastest-growing category across all AI models. At the start of 2025, coding-related queries accounted for just 11% of total AI usage. By the end of the year, that figure had exploded to over 50%.&lt;/p&gt;&lt;p&gt;This growth reflects AI’s deepening integration into software development. Average prompt lengths for programming tasks have grown fourfold, from around 1,500 tokens to over 6,000, with some code-related requests exceeding 20,000 tokens—roughly equivalent to feeding an entire codebase into an AI model for analysis.&lt;/p&gt;&lt;p&gt;For context, programming queries now generate some of the longest and most complex interactions in the entire AI ecosystem. Developers aren’t just asking for simple code snippets anymore; they’re conducting sophisticated debugging sessions, architectural reviews, and multi-step problem solving.&lt;/p&gt;&lt;p&gt;Anthropic’s Claude models dominate this space, capturing over 60% of programming-related usage for most of 2025, though competition is intensifying as Google, OpenAI, and open-source alternatives gain ground.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111219" height="474" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.39.57-PM-1024x474.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Programming-related queries exploded from 11% of total AI usage in early 2025 to over 50% by year’s end.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-chinese-ai-surge"&gt;The Chinese AI surge&lt;/h3&gt;&lt;p&gt;Another major revelation: Chinese AI models now account for approximately 30% of global usage—nearly triple their 13% share at the start of 2025.&lt;/p&gt;&lt;p&gt;Models from DeepSeek, Qwen (Alibaba), and Moonshot AI have rapidly gained traction, with DeepSeek alone processing 14.37 trillion tokens during the study period. This represents a fundamental shift in the global AI landscape, where Western companies no longer hold unchallenged dominance.&lt;/p&gt;&lt;p&gt;Simplified Chinese is now the second-most common language for AI interactions globally at 5% of total usage, behind only English at 83%. Asia’s overall share of AI spending more than doubled from 13% to 31%, with Singapore emerging as the second-largest country by usage after the United States.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111220" height="665" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.43.29-PM-1024x665.png" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-rise-of-agentic-ai"&gt;The rise of “Agentic” AI&lt;/h3&gt;&lt;p&gt;The study introduces a concept that will define AI’s next phase: agentic inference. This means AI models are no longer just answering single questions—they’re executing multi-step tasks, calling external tools, and reasoning across extended conversations.&lt;/p&gt;&lt;p&gt;The share of AI interactions classified as “reasoning-optimised” jumped from nearly zero in early 2025 to over 50% by year’s end. This reflects a fundamental shift from AI as a text generator to AI as an autonomous agent capable of planning and execution.&lt;/p&gt;&lt;p&gt;“The median LLM request is no longer a simple question or isolated instruction,” the researchers explain. “Instead, it is part of a structured, agent-like loop, invoking external tools, reasoning over state, and persisting across longer contexts.”&lt;/p&gt;&lt;p&gt;Think of it this way: instead of asking AI to “write a function,” you’re now asking it to “debug this codebase, identify the performance bottleneck, and implement a solution”—and it can actually do it.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-glass-slipper-effect"&gt;The “Glass Slipper Effect”&lt;/h3&gt;&lt;p&gt;One of the study’s most fascinating insights relates to user retention. Researchers discovered what they call the Cinderella “Glass Slipper” effect—a phenomenon where AI models that are “first to solve” a critical problem create lasting user loyalty.&lt;/p&gt;&lt;p&gt;When a newly released model perfectly matches a previously unmet need—the metaphorical “glass slipper”—those early users stick around far longer than later adopters. For example, the June 2025 cohort of Google’s Gemini 2.5 Pro retained approximately 40% of users at month five, substantially higher than later cohorts.&lt;/p&gt;&lt;p&gt;This challenges conventional wisdom about AI competition. Being first matters, but specifically being first to solve a high-value problem creates a durable competitive advantage. Users embed these models into their workflows, making switching costly both technically and behaviorally.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-cost-doesn-t-matter-as-much-as-you-d-think"&gt;Cost doesn’t matter (as much as you’d think)&lt;/h3&gt;&lt;p&gt;Perhaps counterintuitively, the study reveals that AI usage is relatively price-inelastic. A 10% decrease in price corresponds to only about a 0.5-0.7% increase in usage.&lt;/p&gt;&lt;p&gt;Premium models from Anthropic and OpenAI command $2-35 per million tokens while maintaining high usage, while budget options like DeepSeek and Google’s Gemini Flash achieve similar scale at under $0.40 per million tokens. Both coexist successfully.&lt;/p&gt;&lt;p&gt;“The LLM market does not seem to behave like a commodity just yet,” the report concludes. “Users balance cost with reasoning quality, reliability, and breadth of capability.”&lt;/p&gt;&lt;p&gt;This means AI hasn’t become a race to the bottom on pricing. Quality, reliability, and capability still command premiums—at least for now.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-means-going-forward"&gt;What this means going forward&lt;/h3&gt;&lt;p&gt;The OpenRouter study paints a picture of real-world AI usage that’s far more nuanced than industry narratives suggest. Yes, AI is transforming programming and professional work. But it’s also creating entirely new categories of human-computer interaction through roleplay and creative applications.&lt;/p&gt;&lt;p&gt;The market is diversifying geographically, with China emerging as a major force. The technology is evolving from simple text generation to complex, multi-step reasoning. And user loyalty depends less on being first to market than on being first to truly solve a problem.&lt;/p&gt;&lt;p&gt;As the report notes, “ways in which people use LLMs do not always align with expectations and vary significantly country by country, state by state, use case by use case.”&lt;/p&gt;&lt;p&gt;Understanding these real-world patterns—not just benchmark scores or marketing claims—will be crucial as AI becomes further embedded in daily life. The gap between how we think AI is used and how it’s actually used is wider than most realise. This study helps close that gap.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;Deep Cogito v2: Open-source AI that hones its reasoning skills&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;For the past year, we’ve been told that artificial intelligence is&amp;nbsp;revolutionising&amp;nbsp;productivity—helping us write emails, generate code, and summarise documents. But what if the reality of how people actually use AI is completely different from what we’ve been led to believe?&lt;/p&gt;&lt;p&gt;A data-driven&amp;nbsp;study&amp;nbsp;by OpenRouter has just pulled back the curtain on real-world AI usage by&amp;nbsp;analysing&amp;nbsp;over 100 trillion tokens—essentially billions upon billions of conversations and interactions with large language models like ChatGPT, Claude, and dozens of others. The findings challenge many assumptions about the AI revolution.&lt;/p&gt;&lt;p&gt;​​OpenRouter is a multi-model AI inference platform that routes requests across more than 300 models from over 60 providers—from OpenAI and Anthropic to open-source alternatives like DeepSeek and Meta’s LLaMA.&amp;nbsp;&lt;/p&gt;&lt;p&gt;With over 50% of its usage originating outside the United States and serving millions of developers globally, the platform offers a unique cross-section of how AI is actually deployed across different geographies, use cases, and user types.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Importantly, the study&amp;nbsp;analysed&amp;nbsp;metadata from billions of interactions without accessing the actual text of conversations, preserving user privacy while revealing behavioural patterns.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111217" height="677" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.36.21-PM-1024x677.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Open-source AI models have grown to capture approximately one-third of total usage by late 2025, with notable spikes following major releases.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-roleplay-revolution-nobody-saw-coming"&gt;The roleplay revolution nobody saw coming&lt;/h3&gt;&lt;p&gt;Perhaps the most surprising discovery: more than half of all open-source AI model usage isn’t for productivity at all. It’s for roleplay and creative storytelling.&lt;/p&gt;&lt;p&gt;Yes, you read that right. While tech executives tout AI’s potential to transform business, users are spending the majority of their time engaging in character-driven conversations, interactive fiction, and gaming scenarios.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Over 50% of open-source model interactions fall into this category, dwarfing even programming assistance.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111218" height="576" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.38.37-PM-1024x576.png" width="1024" /&gt;&lt;/figure&gt;&lt;p&gt;“This counters an assumption that LLMs are mostly used for writing code, emails, or summaries,” the report states. “In reality, many users engage with these models for companionship or exploration.”&lt;/p&gt;&lt;p&gt;This isn’t just casual chatting. The data shows users treat AI models as structured roleplaying engines, with 60% of roleplay tokens falling under specific gaming scenarios and creative writing contexts. It’s a massive, largely invisible use case that’s reshaping how AI companies think about their products.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-programming-s-meteoric-rise"&gt;Programming’s meteoric rise&lt;/h3&gt;&lt;p&gt;While roleplay dominates open-source usage, programming has become the fastest-growing category across all AI models. At the start of 2025, coding-related queries accounted for just 11% of total AI usage. By the end of the year, that figure had exploded to over 50%.&lt;/p&gt;&lt;p&gt;This growth reflects AI’s deepening integration into software development. Average prompt lengths for programming tasks have grown fourfold, from around 1,500 tokens to over 6,000, with some code-related requests exceeding 20,000 tokens—roughly equivalent to feeding an entire codebase into an AI model for analysis.&lt;/p&gt;&lt;p&gt;For context, programming queries now generate some of the longest and most complex interactions in the entire AI ecosystem. Developers aren’t just asking for simple code snippets anymore; they’re conducting sophisticated debugging sessions, architectural reviews, and multi-step problem solving.&lt;/p&gt;&lt;p&gt;Anthropic’s Claude models dominate this space, capturing over 60% of programming-related usage for most of 2025, though competition is intensifying as Google, OpenAI, and open-source alternatives gain ground.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111219" height="474" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.39.57-PM-1024x474.png" width="1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Programming-related queries exploded from 11% of total AI usage in early 2025 to over 50% by year’s end.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-chinese-ai-surge"&gt;The Chinese AI surge&lt;/h3&gt;&lt;p&gt;Another major revelation: Chinese AI models now account for approximately 30% of global usage—nearly triple their 13% share at the start of 2025.&lt;/p&gt;&lt;p&gt;Models from DeepSeek, Qwen (Alibaba), and Moonshot AI have rapidly gained traction, with DeepSeek alone processing 14.37 trillion tokens during the study period. This represents a fundamental shift in the global AI landscape, where Western companies no longer hold unchallenged dominance.&lt;/p&gt;&lt;p&gt;Simplified Chinese is now the second-most common language for AI interactions globally at 5% of total usage, behind only English at 83%. Asia’s overall share of AI spending more than doubled from 13% to 31%, with Singapore emerging as the second-largest country by usage after the United States.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-111220" height="665" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/12/Screenshot-2025-12-09-at-3.43.29-PM-1024x665.png" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-the-rise-of-agentic-ai"&gt;The rise of “Agentic” AI&lt;/h3&gt;&lt;p&gt;The study introduces a concept that will define AI’s next phase: agentic inference. This means AI models are no longer just answering single questions—they’re executing multi-step tasks, calling external tools, and reasoning across extended conversations.&lt;/p&gt;&lt;p&gt;The share of AI interactions classified as “reasoning-optimised” jumped from nearly zero in early 2025 to over 50% by year’s end. This reflects a fundamental shift from AI as a text generator to AI as an autonomous agent capable of planning and execution.&lt;/p&gt;&lt;p&gt;“The median LLM request is no longer a simple question or isolated instruction,” the researchers explain. “Instead, it is part of a structured, agent-like loop, invoking external tools, reasoning over state, and persisting across longer contexts.”&lt;/p&gt;&lt;p&gt;Think of it this way: instead of asking AI to “write a function,” you’re now asking it to “debug this codebase, identify the performance bottleneck, and implement a solution”—and it can actually do it.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-glass-slipper-effect"&gt;The “Glass Slipper Effect”&lt;/h3&gt;&lt;p&gt;One of the study’s most fascinating insights relates to user retention. Researchers discovered what they call the Cinderella “Glass Slipper” effect—a phenomenon where AI models that are “first to solve” a critical problem create lasting user loyalty.&lt;/p&gt;&lt;p&gt;When a newly released model perfectly matches a previously unmet need—the metaphorical “glass slipper”—those early users stick around far longer than later adopters. For example, the June 2025 cohort of Google’s Gemini 2.5 Pro retained approximately 40% of users at month five, substantially higher than later cohorts.&lt;/p&gt;&lt;p&gt;This challenges conventional wisdom about AI competition. Being first matters, but specifically being first to solve a high-value problem creates a durable competitive advantage. Users embed these models into their workflows, making switching costly both technically and behaviorally.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-cost-doesn-t-matter-as-much-as-you-d-think"&gt;Cost doesn’t matter (as much as you’d think)&lt;/h3&gt;&lt;p&gt;Perhaps counterintuitively, the study reveals that AI usage is relatively price-inelastic. A 10% decrease in price corresponds to only about a 0.5-0.7% increase in usage.&lt;/p&gt;&lt;p&gt;Premium models from Anthropic and OpenAI command $2-35 per million tokens while maintaining high usage, while budget options like DeepSeek and Google’s Gemini Flash achieve similar scale at under $0.40 per million tokens. Both coexist successfully.&lt;/p&gt;&lt;p&gt;“The LLM market does not seem to behave like a commodity just yet,” the report concludes. “Users balance cost with reasoning quality, reliability, and breadth of capability.”&lt;/p&gt;&lt;p&gt;This means AI hasn’t become a race to the bottom on pricing. Quality, reliability, and capability still command premiums—at least for now.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-this-means-going-forward"&gt;What this means going forward&lt;/h3&gt;&lt;p&gt;The OpenRouter study paints a picture of real-world AI usage that’s far more nuanced than industry narratives suggest. Yes, AI is transforming programming and professional work. But it’s also creating entirely new categories of human-computer interaction through roleplay and creative applications.&lt;/p&gt;&lt;p&gt;The market is diversifying geographically, with China emerging as a major force. The technology is evolving from simple text generation to complex, multi-step reasoning. And user loyalty depends less on being first to market than on being first to truly solve a problem.&lt;/p&gt;&lt;p&gt;As the report notes, “ways in which people use LLMs do not always align with expectations and vary significantly country by country, state by state, use case by use case.”&lt;/p&gt;&lt;p&gt;Understanding these real-world patterns—not just benchmark scores or marketing claims—will be crucial as AI becomes further embedded in daily life. The gap between how we think AI is used and how it’s actually used is wider than most realise. This study helps close that gap.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;Deep Cogito v2: Open-source AI that hones its reasoning skills&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110949" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-18.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/how-people-really-use-ai-the-surprising-truth-from-analysing-billions-of-interactions/</guid><pubDate>Tue, 09 Dec 2025 09:00:00 +0000</pubDate></item></channel></rss>