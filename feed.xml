<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 24 Jun 2025 01:50:44 +0000</lastBuildDate><item><title>[NEW] Why we’re focusing VB Transform on the agentic revolution – and what’s at stake for enterprise AI leaders (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/why-were-focusing-vb-transform-on-the-agentic-revolution-and-whats-at-stake-for-enterprise-ai-leaders/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Tomorrow in San Francisco, VentureBeat’s &lt;strong&gt;Transform 2025&lt;/strong&gt; kicks off. For years, this has been the leading independent gathering for enterprise technical decision-makers — the hands-on builders and architects on the front lines of applied AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Our mission has always been to cut through the hype and focus on the most critical, execution-oriented challenges our audience faces, and this year, one conversation towers above all others: the agentic AI revolution.&lt;/p&gt;



&lt;p&gt;We’ve all been captivated by the potential. But a chasm has opened between the jaw-dropping demos from research labs and the messy reality of enterprise deployment. While agents are poised to become the new engine of the enterprise, a recent KPMG study found that only 11% of companies have actually integrated them into their workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is the “Agentic Infrastructure Gap,” and closing it is the big challenge this year. It’s not about the agent itself, but about building the enterprise-grade chassis – the security, governance, data plumbing, and orchestration – required to manage a digital workforce.&lt;/p&gt;



&lt;p&gt;That’s why we’ve dedicated this year’s &lt;strong&gt;VB Transform&lt;/strong&gt; agenda to being a real-world playbook for navigating this new frontier. It’s the event for leaders who need to move from concept to reality, and here’s how we’re tackling it.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-architecting-the-new-enterprise-chassis"&gt;Architecting the new enterprise chassis&lt;/h2&gt;



&lt;p&gt;The “Agentic OS” requires a new orchestration, both at the application level but also below it, lower in the stack. This about orchestrating the &lt;em&gt;right&lt;/em&gt; compute for the &lt;em&gt;right&lt;/em&gt; task. At Transform, we’re mapping this new landscape with the architects building it.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;The Great Re-routing:&lt;/strong&gt; Influential analyst &lt;strong&gt;Dylan Patel&lt;/strong&gt; will join &lt;strong&gt;Groq&lt;/strong&gt; CEO &lt;strong&gt;Jonathan Ross&lt;/strong&gt; and &lt;strong&gt;Cerebras&lt;/strong&gt; CTO &lt;strong&gt;Sean Lie&lt;/strong&gt; to debate the future of the AI inference stack, breaking down the architectural shifts that are reshaping enterprise AI economics.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;The Visionaries:&lt;/strong&gt; We’ll get crucial context from the leaders of the foundational platforms, including the pragmatic and grounded &lt;strong&gt;Andrew Ng&lt;/strong&gt;, &lt;strong&gt;Google Cloud&lt;/strong&gt; CTO &lt;strong&gt;Will Grannis&lt;/strong&gt;, &lt;strong&gt;OpenAI&lt;/strong&gt;’s Head of Platform Product &lt;strong&gt;Olivier Godement&lt;/strong&gt;, and &lt;strong&gt;Anthropic&lt;/strong&gt;‘s product lead &lt;strong&gt;Scott White&lt;/strong&gt;, among many, many other notables (too many to list here).&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-learning-from-the-doers"&gt;Learning from the doers&lt;/h2&gt;



&lt;p&gt;Theory is easy; execution is hard. The heart of Transform is dedicated to the real-world practitioners deploying these systems today. &lt;/p&gt;



&lt;p&gt;We’re going beyond the canned success stories to hear directly from the leaders in the trenches at &lt;strong&gt;Walmart, Bank of America, Expedia, American Express, LinkedIn, Chevron, Intuit, Capital One,&lt;/strong&gt; and &lt;strong&gt;General Motors&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;They’ll share the unvarnished truth about what it takes to build, secure, and scale agentic systems in complex, regulated industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-this-isn-t-a-spectator-sport"&gt;This isn’t a spectator sport&lt;/h2&gt;



&lt;p&gt;Transform has always been for builders, and this year is no exception. We’ve designed the event to be deeply interactive, ensuring you leave with applicable knowledge.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Hands-on workshops:&lt;/strong&gt; Learn how to actually construct agents and train models with experts like &lt;strong&gt;Nathan Lambert&lt;/strong&gt; and agent-building pioneer &lt;strong&gt;Sam Witteveen&lt;/strong&gt;.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Writer-led roundtables:&lt;/strong&gt; Join intimate, off-the-record discussions led by VentureBeat’s own journalists. These are forums for you to solve shared challenges with your peers in real time, on topics from AI red teaming to navigating multi-agent complexity.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Finally, we’ll continue to champion the leaders building a more inclusive AI future at our 6th annual &lt;strong&gt;Women in Enterprise AI Awards&lt;/strong&gt;.&lt;/p&gt;



&lt;p&gt;VB Transform has always been about empowering enterprise AI leaders. This year, we’re trying to debating the playbook for the agentic revolution. The stakes are high, and the opportunity is immense. We hope you’ll join us to build the future.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-see-the-full-agenda-and-register-for-vb-transform-here-while-tickets-still-remain"&gt;See the full agenda and register for VB Transform here while tickets still remain. &lt;/h2&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Tomorrow in San Francisco, VentureBeat’s &lt;strong&gt;Transform 2025&lt;/strong&gt; kicks off. For years, this has been the leading independent gathering for enterprise technical decision-makers — the hands-on builders and architects on the front lines of applied AI.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Our mission has always been to cut through the hype and focus on the most critical, execution-oriented challenges our audience faces, and this year, one conversation towers above all others: the agentic AI revolution.&lt;/p&gt;



&lt;p&gt;We’ve all been captivated by the potential. But a chasm has opened between the jaw-dropping demos from research labs and the messy reality of enterprise deployment. While agents are poised to become the new engine of the enterprise, a recent KPMG study found that only 11% of companies have actually integrated them into their workflows.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This is the “Agentic Infrastructure Gap,” and closing it is the big challenge this year. It’s not about the agent itself, but about building the enterprise-grade chassis – the security, governance, data plumbing, and orchestration – required to manage a digital workforce.&lt;/p&gt;



&lt;p&gt;That’s why we’ve dedicated this year’s &lt;strong&gt;VB Transform&lt;/strong&gt; agenda to being a real-world playbook for navigating this new frontier. It’s the event for leaders who need to move from concept to reality, and here’s how we’re tackling it.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-architecting-the-new-enterprise-chassis"&gt;Architecting the new enterprise chassis&lt;/h2&gt;



&lt;p&gt;The “Agentic OS” requires a new orchestration, both at the application level but also below it, lower in the stack. This about orchestrating the &lt;em&gt;right&lt;/em&gt; compute for the &lt;em&gt;right&lt;/em&gt; task. At Transform, we’re mapping this new landscape with the architects building it.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;The Great Re-routing:&lt;/strong&gt; Influential analyst &lt;strong&gt;Dylan Patel&lt;/strong&gt; will join &lt;strong&gt;Groq&lt;/strong&gt; CEO &lt;strong&gt;Jonathan Ross&lt;/strong&gt; and &lt;strong&gt;Cerebras&lt;/strong&gt; CTO &lt;strong&gt;Sean Lie&lt;/strong&gt; to debate the future of the AI inference stack, breaking down the architectural shifts that are reshaping enterprise AI economics.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;The Visionaries:&lt;/strong&gt; We’ll get crucial context from the leaders of the foundational platforms, including the pragmatic and grounded &lt;strong&gt;Andrew Ng&lt;/strong&gt;, &lt;strong&gt;Google Cloud&lt;/strong&gt; CTO &lt;strong&gt;Will Grannis&lt;/strong&gt;, &lt;strong&gt;OpenAI&lt;/strong&gt;’s Head of Platform Product &lt;strong&gt;Olivier Godement&lt;/strong&gt;, and &lt;strong&gt;Anthropic&lt;/strong&gt;‘s product lead &lt;strong&gt;Scott White&lt;/strong&gt;, among many, many other notables (too many to list here).&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-learning-from-the-doers"&gt;Learning from the doers&lt;/h2&gt;



&lt;p&gt;Theory is easy; execution is hard. The heart of Transform is dedicated to the real-world practitioners deploying these systems today. &lt;/p&gt;



&lt;p&gt;We’re going beyond the canned success stories to hear directly from the leaders in the trenches at &lt;strong&gt;Walmart, Bank of America, Expedia, American Express, LinkedIn, Chevron, Intuit, Capital One,&lt;/strong&gt; and &lt;strong&gt;General Motors&lt;/strong&gt;. &lt;/p&gt;



&lt;p&gt;They’ll share the unvarnished truth about what it takes to build, secure, and scale agentic systems in complex, regulated industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-this-isn-t-a-spectator-sport"&gt;This isn’t a spectator sport&lt;/h2&gt;



&lt;p&gt;Transform has always been for builders, and this year is no exception. We’ve designed the event to be deeply interactive, ensuring you leave with applicable knowledge.&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Hands-on workshops:&lt;/strong&gt; Learn how to actually construct agents and train models with experts like &lt;strong&gt;Nathan Lambert&lt;/strong&gt; and agent-building pioneer &lt;strong&gt;Sam Witteveen&lt;/strong&gt;.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Writer-led roundtables:&lt;/strong&gt; Join intimate, off-the-record discussions led by VentureBeat’s own journalists. These are forums for you to solve shared challenges with your peers in real time, on topics from AI red teaming to navigating multi-agent complexity.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Finally, we’ll continue to champion the leaders building a more inclusive AI future at our 6th annual &lt;strong&gt;Women in Enterprise AI Awards&lt;/strong&gt;.&lt;/p&gt;



&lt;p&gt;VB Transform has always been about empowering enterprise AI leaders. This year, we’re trying to debating the playbook for the agentic revolution. The stakes are high, and the opportunity is immense. We hope you’ll join us to build the future.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-see-the-full-agenda-and-register-for-vb-transform-here-while-tickets-still-remain"&gt;See the full agenda and register for VB Transform here while tickets still remain. &lt;/h2&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/why-were-focusing-vb-transform-on-the-agentic-revolution-and-whats-at-stake-for-enterprise-ai-leaders/</guid><pubDate>Mon, 23 Jun 2025 15:44:31 +0000</pubDate></item><item><title>A Chinese firm has just launched a constantly changing set of AI benchmarks (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/23/1119190/chinese-changing-ai-benchmarks/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250620_xbench_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When testing an AI model, it’s hard to tell if it is reasoning or just regurgitating answers from its training data. Xbench, a new benchmark developed by the Chinese venture capital firm HSG, or HongShan Capital Group, might help to sidestep that issue. That’s thanks to the way it evaluates models not only on the ability to pass arbitrary tests, like most other benchmarks, but also on the ability to execute real-world tasks, which is more unusual. It will be updated on a regular basis to try to keep it evergreen.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This week the company is making part of its question set open-source and letting anyone use for free. The team has also released a leaderboard comparing how mainstream AI models stack up when tested on Xbench. (ChatGPT o3 ranked first across all categories, though ByteDance’s Doubao, Gemini 2.5 Pro, and Grok all still did pretty well, as did Claude Sonnet.)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Development of the benchmark at HongShan began in 2022, following ChatGPT’s breakout success, as an internal tool for assessing which models are worth investing in. Since then, led by partner Gong Yuan, the team has steadily expanded the system, bringing in outside researchers and professionals to help refine it. As the project grew more sophisticated, they decided to release it to the public.&lt;/p&gt;  &lt;p&gt;Xbench approached the problem with two different systems. One is similar to traditional benchmarking: an academic test that gauges a model’s aptitude on various subjects. The other is more like a technical interview round for a job, assessing how much real-world economic value a model might deliver.&lt;/p&gt; 
 &lt;p&gt;Xbench’s methods for assessing raw intelligence currently include two components: Xbench-ScienceQA and Xbench-DeepResearch. ScienceQA isn’t a radical departure from existing postgraduate-level STEM benchmarks like GPQA and SuperGPQA. It includes questions spanning fields from biochemistry to orbital mechanics, drafted by graduate students and double-checked by professors. Scoring rewards not only the right answer but also the reasoning chain that leads to it.&lt;/p&gt;  &lt;p&gt;DeepResearch, by contrast, focuses on a model’s ability to navigate the Chinese-language web. Ten subject-matter experts created 100 questions in music, history, finance, and literature—questions that can’t just be googled but require significant research to answer. Scoring favors breadth of sources, factual consistency, and a model’s willingness to admit when there isn’t enough data. A question in the publicized collection is “How many Chinese cities in the three northwestern provinces border a foreign country?” (It’s 12, and only 33% of models tested got it right, if you are wondering.)&lt;/p&gt; 
 &lt;p&gt;On the company’s website, the researchers said they want to add more dimensions to the test—for example, aspects like how creative a model is in its problem solving, how collaborative it is when working with other models, and how reliable it is.&lt;/p&gt;  &lt;p&gt;The team has committed to updating the test questions once a quarter and to maintain a half-public, half-private data set.&lt;/p&gt;  &lt;p&gt;To assess models’ real-world readiness, the team worked with experts to develop tasks modeled on actual workflows, initially in recruitment and marketing. For example, one task asks a model to source five qualified battery engineer candidates and justify each pick. Another asks it to match advertisers with appropriate short-video creators from a pool of over 800 influencers.&lt;/p&gt;  &lt;p&gt;The website also teases upcoming categories, including finance, legal, accounting, and design. The question sets for these categories have not yet been open-sourced.&lt;/p&gt;  &lt;p&gt;ChatGPT-o3 again ranks first in both of the current professional categories. For recruiting, Perplexity Search and Claude 3.5 Sonnet take second and third place, respectively. For marketing, Claude, Grok, and Gemini all perform well.&lt;/p&gt;  &lt;p&gt;“It is really difficult for benchmarks to include things that are so hard to quantify,” says Zihan Zheng, the lead researcher on a new benchmark called LiveCodeBench Pro and a student at NYU. “But Xbench represents a promising start.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250620_xbench_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When testing an AI model, it’s hard to tell if it is reasoning or just regurgitating answers from its training data. Xbench, a new benchmark developed by the Chinese venture capital firm HSG, or HongShan Capital Group, might help to sidestep that issue. That’s thanks to the way it evaluates models not only on the ability to pass arbitrary tests, like most other benchmarks, but also on the ability to execute real-world tasks, which is more unusual. It will be updated on a regular basis to try to keep it evergreen.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;This week the company is making part of its question set open-source and letting anyone use for free. The team has also released a leaderboard comparing how mainstream AI models stack up when tested on Xbench. (ChatGPT o3 ranked first across all categories, though ByteDance’s Doubao, Gemini 2.5 Pro, and Grok all still did pretty well, as did Claude Sonnet.)&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;Development of the benchmark at HongShan began in 2022, following ChatGPT’s breakout success, as an internal tool for assessing which models are worth investing in. Since then, led by partner Gong Yuan, the team has steadily expanded the system, bringing in outside researchers and professionals to help refine it. As the project grew more sophisticated, they decided to release it to the public.&lt;/p&gt;  &lt;p&gt;Xbench approached the problem with two different systems. One is similar to traditional benchmarking: an academic test that gauges a model’s aptitude on various subjects. The other is more like a technical interview round for a job, assessing how much real-world economic value a model might deliver.&lt;/p&gt; 
 &lt;p&gt;Xbench’s methods for assessing raw intelligence currently include two components: Xbench-ScienceQA and Xbench-DeepResearch. ScienceQA isn’t a radical departure from existing postgraduate-level STEM benchmarks like GPQA and SuperGPQA. It includes questions spanning fields from biochemistry to orbital mechanics, drafted by graduate students and double-checked by professors. Scoring rewards not only the right answer but also the reasoning chain that leads to it.&lt;/p&gt;  &lt;p&gt;DeepResearch, by contrast, focuses on a model’s ability to navigate the Chinese-language web. Ten subject-matter experts created 100 questions in music, history, finance, and literature—questions that can’t just be googled but require significant research to answer. Scoring favors breadth of sources, factual consistency, and a model’s willingness to admit when there isn’t enough data. A question in the publicized collection is “How many Chinese cities in the three northwestern provinces border a foreign country?” (It’s 12, and only 33% of models tested got it right, if you are wondering.)&lt;/p&gt; 
 &lt;p&gt;On the company’s website, the researchers said they want to add more dimensions to the test—for example, aspects like how creative a model is in its problem solving, how collaborative it is when working with other models, and how reliable it is.&lt;/p&gt;  &lt;p&gt;The team has committed to updating the test questions once a quarter and to maintain a half-public, half-private data set.&lt;/p&gt;  &lt;p&gt;To assess models’ real-world readiness, the team worked with experts to develop tasks modeled on actual workflows, initially in recruitment and marketing. For example, one task asks a model to source five qualified battery engineer candidates and justify each pick. Another asks it to match advertisers with appropriate short-video creators from a pool of over 800 influencers.&lt;/p&gt;  &lt;p&gt;The website also teases upcoming categories, including finance, legal, accounting, and design. The question sets for these categories have not yet been open-sourced.&lt;/p&gt;  &lt;p&gt;ChatGPT-o3 again ranks first in both of the current professional categories. For recruiting, Perplexity Search and Claude 3.5 Sonnet take second and third place, respectively. For marketing, Claude, Grok, and Gemini all perform well.&lt;/p&gt;  &lt;p&gt;“It is really difficult for benchmarks to include things that are so hard to quantify,” says Zihan Zheng, the lead researcher on a new benchmark called LiveCodeBench Pro and a student at NYU. “But Xbench represents a promising start.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/23/1119190/chinese-changing-ai-benchmarks/</guid><pubDate>Mon, 23 Jun 2025 15:46:28 +0000</pubDate></item><item><title>AllSpice’s platform is the GitHub for electrical engineering teams (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/allspices-platform-is-the-github-for-electrical-engineering-teams/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/allspice-kyle-and-valentina.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There is no shortage of workflow collaboration tools — like Slack or Google Docs, in addition to industry-specific ones like GitHub — for software developers. A startup called AllSpice.io successfully bet that electrical hardware engineering teams need their own collaboration platform, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AllSpice’s platform sits between existing workflow software. It allows hardware teams to collaborate on the types of documents they traditionally work in — documents that don’t easily translate over Slack and email — like PCB files and electronic CAD files, both of which are used to design circuit boards.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Engineers can use AllSpice to single out and comment on design aspects in these types of documents, the same way software engineers can comment on specific lines of code through GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kyle Dumont, co-founder and CTO of AllSpice, told TechCrunch that the startup has been able to find success because they haven’t tried to build a new end-to-end collaboration platform but rather fill the gap between the software solutions that hardware teams were already using.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The teams that we were talking to had really integral tools already in their workflows,” Dumont said. “They had these electrical CAD tools, they had [product life cycle management] tools, they had existing workflows that we knew the product that we launched had to operate between.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This learning came from research the founding team did prior to launching their product to make sure that they were building something teams would actually use. In early tests, AllSpice not only focused on what their users commented on, both good and bad, but also what didn’t get mentioned at all, Valentina Ratner, co-founder and CEO, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Some of the most valuable things that we learned were maybe the things that people didn’t need or didn’t want,” Ratner said. “That helped us kind of scope something that will be really helpful and really an integral part of the workflow. Because we wanted to build not another point solution for our space, but a centralized platform that will become that home base for electronics teams.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Both Ratner and Dumont had experienced the pain points AllSpice is trying to solve firsthand while working as engineers at Amazon and iRobot, respectively. Ratner said that hardware design doesn’t translate through email chains and PDFs, and by the end of Ratner’s time at Amazon, she was spending the majority of her time building an internal collaboration tool to solve this problem for Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo met in grad school and launched the first version of AllSpice’s product in 2022, which was focused on small businesses and other startups. The company started to see growing demand from enterprises, pivoted, and has since landed customers such as Blue Origin, Bose, and Sam Altman’s Tools for Humanity, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup just raised a $15 million Series A round led by Rethink Impact with participation from L’attitude Ventures, Gingerbread Capital, and DNX Ventures, in addition to existing investors. The company will put the capital toward hiring and continuing to build out its product offerings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AllSpice is also launching its new AI agent tool that helps validate engineers’ designs and spot mistakes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve seen huge demand to find out how our hardware, [and] AI tools, can help make their teams more effective, catch these design errors, and that’s exactly what we’re targeting for this product,” Dumont said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is purposefully launching this new AI agent just in closed beta for now, with an emphasis on working with their existing partners, Ratner said. The company wants to be able to ensure full accuracy before opening the product up further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The cost of a hardware mistake is so much higher than the cost of a software mistake,” Ratner said. “We have to make it in a way that makes sense for our industry, because of those kinds of broad differences between releasing a software product versus releasing a hardware product.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/allspice-kyle-and-valentina.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;There is no shortage of workflow collaboration tools — like Slack or Google Docs, in addition to industry-specific ones like GitHub — for software developers. A startup called AllSpice.io successfully bet that electrical hardware engineering teams need their own collaboration platform, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AllSpice’s platform sits between existing workflow software. It allows hardware teams to collaborate on the types of documents they traditionally work in — documents that don’t easily translate over Slack and email — like PCB files and electronic CAD files, both of which are used to design circuit boards.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Engineers can use AllSpice to single out and comment on design aspects in these types of documents, the same way software engineers can comment on specific lines of code through GitHub.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Kyle Dumont, co-founder and CTO of AllSpice, told TechCrunch that the startup has been able to find success because they haven’t tried to build a new end-to-end collaboration platform but rather fill the gap between the software solutions that hardware teams were already using.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The teams that we were talking to had really integral tools already in their workflows,” Dumont said. “They had these electrical CAD tools, they had [product life cycle management] tools, they had existing workflows that we knew the product that we launched had to operate between.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This learning came from research the founding team did prior to launching their product to make sure that they were building something teams would actually use. In early tests, AllSpice not only focused on what their users commented on, both good and bad, but also what didn’t get mentioned at all, Valentina Ratner, co-founder and CEO, told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Some of the most valuable things that we learned were maybe the things that people didn’t need or didn’t want,” Ratner said. “That helped us kind of scope something that will be really helpful and really an integral part of the workflow. Because we wanted to build not another point solution for our space, but a centralized platform that will become that home base for electronics teams.”&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Both Ratner and Dumont had experienced the pain points AllSpice is trying to solve firsthand while working as engineers at Amazon and iRobot, respectively. Ratner said that hardware design doesn’t translate through email chains and PDFs, and by the end of Ratner’s time at Amazon, she was spending the majority of her time building an internal collaboration tool to solve this problem for Amazon.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The duo met in grad school and launched the first version of AllSpice’s product in 2022, which was focused on small businesses and other startups. The company started to see growing demand from enterprises, pivoted, and has since landed customers such as Blue Origin, Bose, and Sam Altman’s Tools for Humanity, among others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup just raised a $15 million Series A round led by Rethink Impact with participation from L’attitude Ventures, Gingerbread Capital, and DNX Ventures, in addition to existing investors. The company will put the capital toward hiring and continuing to build out its product offerings.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AllSpice is also launching its new AI agent tool that helps validate engineers’ designs and spot mistakes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve seen huge demand to find out how our hardware, [and] AI tools, can help make their teams more effective, catch these design errors, and that’s exactly what we’re targeting for this product,” Dumont said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is purposefully launching this new AI agent just in closed beta for now, with an emphasis on working with their existing partners, Ratner said. The company wants to be able to ensure full accuracy before opening the product up further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The cost of a hardware mistake is so much higher than the cost of a software mistake,” Ratner said. “We have to make it in a way that makes sense for our industry, because of those kinds of broad differences between releasing a software product versus releasing a hardware product.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/allspices-platform-is-the-github-for-electrical-engineering-teams/</guid><pubDate>Mon, 23 Jun 2025 16:00:00 +0000</pubDate></item><item><title>Learning from other domains to advance AI evaluation and testing (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/learning-from-other-domains-to-advance-ai-evaluation-and-testing/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of the Guests from the limited podcast series, AI Testing and Evaluation: Learnings from Science and Industry" class="wp-image-1143007" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/RAI-BlogHeroFeature-1400x788-2.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;As generative AI becomes more capable and widely deployed, familiar questions from the governance of other transformative technologies have resurfaced. Which opportunities, capabilities, risks, and impacts should be evaluated? Who should conduct evaluations, and at what stages of the technology lifecycle? What tests or measurements should be used? And how can we know if the results are reliable?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Recent research and reports from Microsoft&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the UK AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, &lt;em&gt;The New York Times&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and &lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt; &lt;/em&gt;have highlighted gaps in how we evaluate AI models and systems. These gaps also form foundational context for recent international expert consensus reports: the inaugural&amp;nbsp;&lt;em&gt;International AI Safety Report&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025) and the &lt;em&gt;Singapore Consensus&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025). Closing these gaps at a pace that matches AI innovation will lead to more reliable evaluations that can help guide deployment decisions, inform policy, and deepen trust.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Today, we’re launching a limited-series podcast, &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;, to share insights from domains that have grappled with testing and measurement questions. Across four episodes, host Kathleen Sullivan speaks with academic experts in genome editing, cybersecurity, pharmaceuticals, and medical devices to find out which technical and regulatory steps have helped to close evaluation gaps and earn public trust.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;We’re also sharing written case studies from experts, along with top-level lessons we’re applying to AI. At the close of the podcast series, we’ll offer Microsoft’s deeper reflections on next steps toward more reliable and trustworthy approaches to AI evaluation.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="lessons-from-eight-case-studies"&gt;Lessons from eight case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our research on risk evaluation, testing, and assurance models in other domains began in December 2024, when Microsoft’s Office of Responsible AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; gathered independent experts from the fields of civil aviation, cybersecurity, financial services, genome editing, medical devices, nanoscience, nuclear energy, and pharmaceuticals. In bringing this group together, we drew on our own learnings and feedback received on our e-book, &lt;em&gt;Global Governance: Goals and Lessons for AI&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt;, &lt;/em&gt;in which we studied the higher-level goals and institutional approaches that had been leveraged for cross-border governance in the past.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While approaches to risk evaluation and testing vary significantly across the case studies, there was one consistent, top-level takeaway: evaluation frameworks always reflect trade-offs among different policy objectives, such as safety, efficiency, and innovation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts across all eight fields noted that policymakers have had to weigh trade-offs in designing evaluation frameworks. These frameworks must account for both the limits of current science and the need for agility in the face of uncertainty. They likewise agreed that early design choices, often reflecting the “DNA” of the historical moment in which they’re made, as cybersecurity expert Stewart Baker described it, are important as they are difficult to scale down or undo later.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Strict, pre-deployment testing regimes—such as those used in civil aviation, medical devices, nuclear energy, and pharmaceuticals—offer strong safety assurances but can be resource-intensive and slow to adapt. These regimes often emerged in response to well-documented failures and are backed by decades of regulatory infrastructure and detailed technical standards.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In contrast, fields marked by dynamic and complex interdependencies between the tested system and its external environment—such as cybersecurity and bank stress testing—rely on more adaptive governance frameworks, where testing may be used to generate actionable insights about risk rather than primarily serve as a trigger for regulatory enforcement.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Moreover, in pharmaceuticals, where interdependencies are at play and there is emphasis on pre-deployment testing, experts highlighted a potential trade-off with post-market monitoring of downstream risks and efficacy evaluation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These variations in approaches across domains—stemming from differences in risk profiles, types of technologies, maturity of the evaluation science, placement of expertise in the assessor ecosystem, and context in which technologies are deployed, among other factors—also inform takeaways for AI.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="applying-risk-evaluation-and-governance-lessons-to-ai"&gt;Applying risk evaluation and governance lessons to AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;While no analogy perfectly fits the AI context, the genome editing and nanoscience cases offer interesting insights for general-purpose technologies like AI, where risks vary widely depending on how the technology is applied.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts highlighted the benefits of governance frameworks that are more flexible and tailored to specific use cases and application contexts. In these fields, it is challenging to define risk thresholds and design evaluation frameworks in the abstract. Risks become more visible and assessable once the technology is applied to a particular use case and context-specific variables are known.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These and other insights also helped us distill qualities essential to ensuring that testing is a reliable governance tool across domains, including:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Rigor &lt;/strong&gt;in defining what is being examined and why it matters. This requires detailed specification of what is being measured and understanding how the deployment context may affect outcomes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Standardization &lt;/strong&gt;of how tests should be conducted to achieve valid, reliable results. This requires establishing technical standards that provide methodological guidance and ensure quality and consistency.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Interpretability &lt;/strong&gt;of test results and how they inform risk decisions. This requires establishing expectations for evidence and improving literacy in how to understand, contextualize, and use test results—while remaining aware of their limitations.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="toward-stronger-foundations-for-ai-testing"&gt;Toward stronger foundations for AI testing&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Establishing robust foundations for AI evaluation and testing requires effort to improve rigor, standardization, and interpretability—and to ensure that methods keep pace with rapid technological progress and evolving scientific understanding.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Taking lessons from other general-purpose technologies, this foundational work must also be pursued for both AI models and systems. While testing models will continue to be important, reliable evaluation tools that provide assurance for system performance will enable broad adoption of AI, including in high-risk scenarios. A strong feedback loop on evaluations of AI models and systems could not only accelerate progress on methodological challenges but also bring focus to which opportunities, capabilities, risks, and impacts are most appropriate and efficient to evaluate at what points along the AI development and deployment lifecycle.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;We would like to thank the following external experts who have contributed to our research program on lessons for AI testing and evaluation: Mateo Aboy, Paul Alp, Gerónimo Poletto Antonacci, Stewart Baker, Daniel Benamouzig, Pablo Cantero, Daniel Carpenter, Alta Charo, Jennifer Dionne, Andy Greenfield, Kathryn Judge, Ciaran Martin, and Timo Minssen.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="case-studies"&gt;Case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Civil aviation:&lt;/strong&gt; &lt;em&gt;Testing in Aircraft Design and Manufacturing&lt;/em&gt;, by Paul Alp&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Cybersecurity:&lt;/strong&gt; &lt;em&gt;Cybersecurity Standards and Testing—Lessons for AI Safety and Security&lt;/em&gt;, by Stewart Baker&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Financial services (bank stress testing):&lt;/strong&gt; &lt;em&gt;The Evolving Use of Bank Stress Tests&lt;/em&gt;, by Kathryn Judge&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Genome editing:&lt;/strong&gt; &lt;em&gt;Governance of Genome Editing in Human Therapeutics and Agricultural Applications&lt;/em&gt;, by Alta Charo and Andy Greenfield&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Medical devices:&lt;/strong&gt; &lt;em&gt;Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI Governance&lt;/em&gt;,&lt;em&gt; &lt;/em&gt;by Mateo Aboy and Timo Minssen&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nanoscience:&lt;/strong&gt; &lt;em&gt;The regulatory landscape of nanoscience and nanotechnology, and applications to future AI regulation&lt;/em&gt;, by Jennifer Dionne&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nuclear energy:&lt;/strong&gt; &lt;em&gt;Testing in the Nuclear Industry&lt;/em&gt;, by Pablo Cantero and Gerónimo Poletto Antonacci&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Pharmaceuticals:&lt;/strong&gt; &lt;em&gt;The History and Evolution of Testing in Pharmaceutical Regulation&lt;/em&gt;, by Daniel Benamouzig and Daniel Carpenter&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of the Guests from the limited podcast series, AI Testing and Evaluation: Learnings from Science and Industry" class="wp-image-1143007" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/RAI-BlogHeroFeature-1400x788-2.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;As generative AI becomes more capable and widely deployed, familiar questions from the governance of other transformative technologies have resurfaced. Which opportunities, capabilities, risks, and impacts should be evaluated? Who should conduct evaluations, and at what stages of the technology lifecycle? What tests or measurements should be used? And how can we know if the results are reliable?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Recent research and reports from Microsoft&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, the UK AI Security Institute&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, &lt;em&gt;The New York Times&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, and &lt;em&gt;MIT Technology Review&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt; &lt;/em&gt;have highlighted gaps in how we evaluate AI models and systems. These gaps also form foundational context for recent international expert consensus reports: the inaugural&amp;nbsp;&lt;em&gt;International AI Safety Report&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025) and the &lt;em&gt;Singapore Consensus&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; (2025). Closing these gaps at a pace that matches AI innovation will lead to more reliable evaluations that can help guide deployment decisions, inform policy, and deepen trust.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Today, we’re launching a limited-series podcast, &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;, to share insights from domains that have grappled with testing and measurement questions. Across four episodes, host Kathleen Sullivan speaks with academic experts in genome editing, cybersecurity, pharmaceuticals, and medical devices to find out which technical and regulatory steps have helped to close evaluation gaps and earn public trust.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Azure AI Foundry Labs&lt;/h2&gt;
				
								&lt;p class="large" id="azure-ai-foundry-labs"&gt;Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;We’re also sharing written case studies from experts, along with top-level lessons we’re applying to AI. At the close of the podcast series, we’ll offer Microsoft’s deeper reflections on next steps toward more reliable and trustworthy approaches to AI evaluation.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="lessons-from-eight-case-studies"&gt;Lessons from eight case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Our research on risk evaluation, testing, and assurance models in other domains began in December 2024, when Microsoft’s Office of Responsible AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; gathered independent experts from the fields of civil aviation, cybersecurity, financial services, genome editing, medical devices, nanoscience, nuclear energy, and pharmaceuticals. In bringing this group together, we drew on our own learnings and feedback received on our e-book, &lt;em&gt;Global Governance: Goals and Lessons for AI&lt;/em&gt;&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;&lt;em&gt;, &lt;/em&gt;in which we studied the higher-level goals and institutional approaches that had been leveraged for cross-border governance in the past.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;While approaches to risk evaluation and testing vary significantly across the case studies, there was one consistent, top-level takeaway: evaluation frameworks always reflect trade-offs among different policy objectives, such as safety, efficiency, and innovation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts across all eight fields noted that policymakers have had to weigh trade-offs in designing evaluation frameworks. These frameworks must account for both the limits of current science and the need for agility in the face of uncertainty. They likewise agreed that early design choices, often reflecting the “DNA” of the historical moment in which they’re made, as cybersecurity expert Stewart Baker described it, are important as they are difficult to scale down or undo later.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Strict, pre-deployment testing regimes—such as those used in civil aviation, medical devices, nuclear energy, and pharmaceuticals—offer strong safety assurances but can be resource-intensive and slow to adapt. These regimes often emerged in response to well-documented failures and are backed by decades of regulatory infrastructure and detailed technical standards.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In contrast, fields marked by dynamic and complex interdependencies between the tested system and its external environment—such as cybersecurity and bank stress testing—rely on more adaptive governance frameworks, where testing may be used to generate actionable insights about risk rather than primarily serve as a trigger for regulatory enforcement.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Moreover, in pharmaceuticals, where interdependencies are at play and there is emphasis on pre-deployment testing, experts highlighted a potential trade-off with post-market monitoring of downstream risks and efficacy evaluation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These variations in approaches across domains—stemming from differences in risk profiles, types of technologies, maturity of the evaluation science, placement of expertise in the assessor ecosystem, and context in which technologies are deployed, among other factors—also inform takeaways for AI.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="applying-risk-evaluation-and-governance-lessons-to-ai"&gt;Applying risk evaluation and governance lessons to AI&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;While no analogy perfectly fits the AI context, the genome editing and nanoscience cases offer interesting insights for general-purpose technologies like AI, where risks vary widely depending on how the technology is applied.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Experts highlighted the benefits of governance frameworks that are more flexible and tailored to specific use cases and application contexts. In these fields, it is challenging to define risk thresholds and design evaluation frameworks in the abstract. Risks become more visible and assessable once the technology is applied to a particular use case and context-specific variables are known.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;These and other insights also helped us distill qualities essential to ensuring that testing is a reliable governance tool across domains, including:&amp;nbsp;&lt;/p&gt;



&lt;ol class="wp-block-list" start="1"&gt;
&lt;li&gt;&lt;strong&gt;Rigor &lt;/strong&gt;in defining what is being examined and why it matters. This requires detailed specification of what is being measured and understanding how the deployment context may affect outcomes.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Standardization &lt;/strong&gt;of how tests should be conducted to achieve valid, reliable results. This requires establishing technical standards that provide methodological guidance and ensure quality and consistency.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Interpretability &lt;/strong&gt;of test results and how they inform risk decisions. This requires establishing expectations for evidence and improving literacy in how to understand, contextualize, and use test results—while remaining aware of their limitations.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 class="wp-block-heading" id="toward-stronger-foundations-for-ai-testing"&gt;Toward stronger foundations for AI testing&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Establishing robust foundations for AI evaluation and testing requires effort to improve rigor, standardization, and interpretability—and to ensure that methods keep pace with rapid technological progress and evolving scientific understanding.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Taking lessons from other general-purpose technologies, this foundational work must also be pursued for both AI models and systems. While testing models will continue to be important, reliable evaluation tools that provide assurance for system performance will enable broad adoption of AI, including in high-risk scenarios. A strong feedback loop on evaluations of AI models and systems could not only accelerate progress on methodological challenges but also bring focus to which opportunities, capabilities, risks, and impacts are most appropriate and efficient to evaluate at what points along the AI development and deployment lifecycle.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;We would like to thank the following external experts who have contributed to our research program on lessons for AI testing and evaluation: Mateo Aboy, Paul Alp, Gerónimo Poletto Antonacci, Stewart Baker, Daniel Benamouzig, Pablo Cantero, Daniel Carpenter, Alta Charo, Jennifer Dionne, Andy Greenfield, Kathryn Judge, Ciaran Martin, and Timo Minssen.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="case-studies"&gt;Case studies&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;&lt;strong&gt;Civil aviation:&lt;/strong&gt; &lt;em&gt;Testing in Aircraft Design and Manufacturing&lt;/em&gt;, by Paul Alp&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Cybersecurity:&lt;/strong&gt; &lt;em&gt;Cybersecurity Standards and Testing—Lessons for AI Safety and Security&lt;/em&gt;, by Stewart Baker&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Financial services (bank stress testing):&lt;/strong&gt; &lt;em&gt;The Evolving Use of Bank Stress Tests&lt;/em&gt;, by Kathryn Judge&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Genome editing:&lt;/strong&gt; &lt;em&gt;Governance of Genome Editing in Human Therapeutics and Agricultural Applications&lt;/em&gt;, by Alta Charo and Andy Greenfield&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Medical devices:&lt;/strong&gt; &lt;em&gt;Medical Device Testing: Regulatory Requirements, Evolution and Lessons for AI Governance&lt;/em&gt;,&lt;em&gt; &lt;/em&gt;by Mateo Aboy and Timo Minssen&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nanoscience:&lt;/strong&gt; &lt;em&gt;The regulatory landscape of nanoscience and nanotechnology, and applications to future AI regulation&lt;/em&gt;, by Jennifer Dionne&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Nuclear energy:&lt;/strong&gt; &lt;em&gt;Testing in the Nuclear Industry&lt;/em&gt;, by Pablo Cantero and Gerónimo Poletto Antonacci&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;Pharmaceuticals:&lt;/strong&gt; &lt;em&gt;The History and Evolution of Testing in Pharmaceutical Regulation&lt;/em&gt;, by Daniel Benamouzig and Daniel Carpenter&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/learning-from-other-domains-to-advance-ai-evaluation-and-testing/</guid><pubDate>Mon, 23 Jun 2025 16:35:06 +0000</pubDate></item><item><title>AI Testing and Evaluation: Learnings from Science and Industry (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-learnings-from-science-and-industry/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard &amp;amp; Kathleen Sullivan." class="wp-image-1141309" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP0-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool. &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry, &lt;/em&gt;hosted by Microsoft Research’s Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In this episode, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, joins Sullivan to detail the company’s efforts to help inform AI governance discussions and decisions, including, more recently, around the role of AI testing and evaluation. Craig Deckard and Sullivan delve into the tension that exists between the risk and opportunity of technology, the similarities and differences between AI development and the fields Microsoft is studying, and the role of different stakeholders in advancing AI governance and public policy.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;



&lt;p&gt;AI and Microsoft Research&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our introductory episode, I’m pleased to welcome Amanda Craig Deckard from Microsoft to discuss the company’s efforts to learn about testing in other sectors.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda is senior director of public policy in the Office of Responsible AI, where she leads a team that works closely with engineers, researchers, and policy experts to help ensure AI is being developed and used responsibly. Their insights shape Microsoft’s contribution to public policy discussions on laws, norms, and standards for AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome to the podcast.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, let’s give the listeners a little bit of your background. What’s your origin story? Can you talk to us a little bit about maybe how you started in tech? And I would love to also learn a little bit more about what your team does in the Office of Responsible AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Sure. Thank you. I’d say my [LAUGHS] path to tech, to Microsoft, as well, was a bit, like, circuitous, maybe. You know, I thought for the longest time I was going to be a journalist. I studied forced migration. I worked in a sort of state level sort of trial court in Indiana, a legal service provider in India, just to give you a bit of a flavor.&lt;/p&gt;



&lt;p&gt;I made my way to Microsoft in 2014 and have been here since, working in cybersecurity public policy first and now in responsible AI. And the way that our Office of Responsible AI has really, sort of, structured itself is bringing together the kind of expertise to really work on defining policy and how to operationalize it at the same time.&lt;/p&gt;



&lt;p&gt;And, you know, that means that we have been working through this, you know, real challenge of defining internal policy and practice, making sure that’s deeply grounded in the work of our colleagues at Microsoft Research, and then really closely working with engineering to make sure that we have the processes, that we have the tools, to implement that policy at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I’m really drawn to these kind of hard problems where they have the character of two things being true or there’s like, you know, real tension on both sides and in particular, in the context of those kinds of problems, roles in which, like, the whole job is actually just sitting with that tension, not necessarily, like, resolving it and expecting that you’re done.&lt;/p&gt;



&lt;p&gt;And I think, really, there are two reasons why tech is so, kind of, representative of that kind of challenge that I’ve always found fascinating. You know, one is that, of course, tech is, sort of, ubiquitous. It’s really impacting so many people’s lives. But also, you know, because, as I think has become part of our vernacular now, but, you know, is not necessarily immediately intuitive, is like the fact that technology is both a tool and a weapon. And so that’s just, like, another reason why, you know, we have to continuously work through that tension and, sort of, like, sit with it, right, and even as tech evolves over time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You bring up such great points, and this field is not black and white. I think that even underscores, you know, this notion that you highlighted that it’s impacting everyone. And, you know, to set the stage for our listeners, last year, we pulled in a bunch of experts from cybersecurity, biotech, finance, and we ran this large workshop to study how they’re thinking about governance in those playbooks. And so I’d love to understand a little bit more about what sparked that effort—and, you know, there’s a piece of this which is really centered around testing—and to hear from you why the focus on testing is so important.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; If I could rewind a little bit and give you a bit of history of how we even arrived at bringing these experts together, you know, we actually started on this journey in 2023. At that time, there were, like, a lot of these big questions swirling around about, you know, what did we need in terms of governance for AI? Of course, this was in the immediate aftermath of the ChatGPT sort of wave and everyone recognizing that, like, the technology was going to have a different level of impact in the near term. And so, you know, what do we need from governance? What do we need at the global level, in particular, of governance?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so at the time, in early 2023 especially, there were a lot of attempts to sort of draw analogies to other global governance institutions in other domains. So we actually in 2023 brought together a different workshop than the one that you’re referring to specifically focused on testing last year. And we, kind of, had two big takeaways from that conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One was, what are the actual functions of these institutions and how do they apply to AI? And, actually, one of the takeaways was they all sort of apply. [LAUGHS] There’s, like, a role for, you know, any of the functions, whether it be sort of driving consensus on research or building industry standards or managing, kind of, frontier risks, for thinking about how those might be needed in the AI context.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the other big takeaways was that, you know, there are also limitations in these analogies. You know, each of the institutions grew up in its own, sort of, unique historical moment, like the one that we sit in with AI right now. And in each of those circumstances, they don’t exactly translate to this moment. And so, yeah, there was like this kind of, OK, we want to draw what we can from this conversation and then we also want to understand, what is also very important that’s just different for AI right now?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We published a book with the lessons from that conversation in 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then we actually went on a bit of a tour [LAUGHS] with that content where we had a number of roundtables actually all over the world where we gathered feedback on how those analogies were landing, how our takeaways were landing. And one of the things that we took from them was a gap that some of the participants saw in the analogies that we chose to focus on. So across multiple conversations, other domains kept being raised, like, why did you not also study pharmaceuticals? Why did you also not study cybersecurity, for example? And so that, you know, naturally got us thinking about what further lessons we could draw from &lt;em&gt;those&lt;/em&gt; domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the same time, though, we also saw a need to, again, go deeper than what we went and really, like, focus on a narrower problem. So that’s really what led us to trying to think about a more specific problem where we could think across levels of governance and bring in some of these other domains. And, you know, testing was top of mind. Continues to be a really important topic in the AI policy conversation right now, I think, for really good reason. A lot of policymakers are focused on, you know, what we need to do to,&amp;nbsp;kind of, have there be sufficient trust, and testing is going to be a part of that—really better understand risk, enable everyone to be able to make more, kind of, risk-informed decisions, right. Testing is an important component for governance and AI and, of course, in all of these other domains, as well.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I’ll just add the other, kind of, input into the process for this second round was exploring other analogies beyond those that we, kind of, got feedback on. And one of the early, kind of, examples of another domain that would be really worthwhile to study that came to mind from, sort of, just studying the literature was genome editing. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, genome editing was really interesting through the process of thinking about other kind of general-purpose technologies. We also arrived at nanoscience and brought those into the conversation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, actually, if you could double-click,&amp;nbsp;I mean, you just named a number of industries. I’d love to just understand which of those worlds maybe feels the closest to what we’re wrestling with, with AI and maybe which is kind of the farthest off, and what makes them stand out to you?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Oh, such a good question. For this second round, we actually brought together eight different domains, right. And I think we actually thought we would come out of this conversation with some bit of clarity around, &lt;em&gt;Oh, if we just, sort of, take this approach for this domain or that domain, we’ll sort of have—at least for now—really solved part of the puzzle.&lt;/em&gt; [LAUGHS] And, you know, our public policy team the day after the workshop, we had a, sort of, follow-on discussion, and the very first thing that we started with in that conversation was like, &lt;em&gt;OK, so which of these domains?&lt;/em&gt; And fascinatingly, like, everyone was sort of like, &lt;em&gt;Ahh! &lt;/em&gt;[LAUGHS] &lt;em&gt;None of them are applying perfectly&lt;/em&gt;. I mean, this is also speaking to the limitations of analogies that we already acknowledged.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, you know, all of the experts from across these domains gave us really interesting insights into, sort of, the tradeoffs and the limitations and how they were working. None are really applying perfectly for us. But all of them do offer a thread of insight that is really useful for thinking about testing in AI, and there are some different dimensions that I think are really useful as framing for that.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I mean, one is just this horizontal-versus-vertical,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;kind of, difference in domains and, you know, the horizontal technology like genome editing or nanoscience&amp;nbsp;just being inherently different and seemingly very similar to AI in that you want to be able to understand risks in the technology itself &lt;em&gt;and&lt;/em&gt; there is just so much contextual, sort of, factor that matters in the application of those technologies for how the risk manifests that you really need to, kind of, do those two things at once—of understanding the technology but then really thinking about risk and governance in the context of application versus, you know, a context like or a domain like civil aviation or nuclear technology, for example.&lt;/p&gt;



&lt;p&gt;You know, even in the workshop itself that we hosted late last year, where we brought together this second round of experts, it was really interesting. We actually started the conversation by trying to understand how those different domains defined risks, where they were able to set risk thresholds. That’s been such a part of the AI policy conversation in the last year. And, you know, it was really instructive that the more vertical domains were able to, sort of, snap to clearer answers much more quickly. [LAUGHS] But, like, the horizontal nanoscience and genome editing were not because it just depends, right. So anyway, the horizontal-vertical dimension seems like a really important one to draw from and apply to AI. &lt;/p&gt;



&lt;p&gt;The couple of others that I would offer is just, you know, thinking about the different kinds of technologies. You know, obviously, there’s some of the domains that we studied that they’re just inherently, sort of, like, physical technologies … a mix of physical and digital or virtual in a lot of cases because all of these are, of course, applying digital technology. But like, you know, there is just a difference between something like an &lt;em&gt;airplane&lt;/em&gt; or a &lt;em&gt;medical device&lt;/em&gt; or, you know, the more kind of virtual or intangible sort of technologies even, you know, of course, AI and some of the other like cyber and genome editing but also like, you know, financial services having some of that quality. And again, I think the thing that’s interesting to us about AI is to think about AI and risk evaluation of AI as being, you know, having a large component of that being about the kind of virtual or intangible technology. &lt;em&gt;And also&lt;/em&gt;, you know, there is a future of robotics where we might need to think about the, kind of, physical risk evaluation kind of work, as well.&lt;/p&gt;



&lt;p&gt;And then the final thing I’d maybe say in terms of thinking about which domains have the lessons for AI that are most applicable is just how they’ve grappled with these different kind of governance questions. Things like how to turn the dial in terms of being more or less prescriptive on risk evaluation approaches, how they think about the balance of, kind of, pre-market versus post-market risk evaluation in testing, and what the tradeoffs have been there across domains has been really interesting to kind of tease out. And then also thinking about, sort of, who does what?&lt;/p&gt;



&lt;p&gt;So, you know, in each of these different domains, it was interesting to hear about, like, you know, the role of industry, the role of governments, the role of third-party experts in designing evaluations and developing standards and actually doing the work, and, kind of, having the pull through of what it means for risk and governance decisions. There were, again, there was a variety of, sort of, approaches across these domains that I think were interesting for AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You mentioned that there’s a number of different stakeholders to be considering across the board as we’re thinking about policy, as we’re thinking about regulation. Where can we collaborate more across industry? Is it academia? Regulators? Just, how can we move the needle faster?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; I think all of the above [LAUGHTER] is needed. But it’s also really important to have all of that, kind of, expertise brought together, you know, and I think, you know, one of the things that we certainly heard from multiple of the domains, if not all of them, was that same actual interest and need and the same sort of ongoing work to try to figure that out.&lt;/p&gt;



&lt;p&gt;You know, even where there had been progress in some of the other domains with bringing together, you know, some industry stakeholders or, you know, industry and government, there was still a desire to actually do more there. Like, if there was some progress in industry and government, the need was, &lt;em&gt;And more kind of cross-jurisdiction government conversation&lt;/em&gt;, for example. Or some progress on, you know, within the industry but needing to, like, strengthen the partnership with academia, for example. So, you know, I think it speaks to, like, the quality of your question, to be honest, that, you know, all of these domains are actually still grappling with this and still seeing the need to grow in that direction more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What I’d say about AI today is that we have made good progress with, you know, starting to build some industry partnerships. You know, we were a founding member of the Frontier Model Forum, or FMF&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which has been a very useful place for us to work with some peers on really trying to bring forward some best practices that apply across our organizations. You know, there are other forums as well, like MLCommons&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, where we’re working with others in industry and broader, sort of, academic and civil society communities. Partnership on AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is another one I think about that, kind of, fits that mold, as well, in a really positive way. And, like, there are a lot of different, sort of, governance needs to think through and where, you know, we can really think about bringing that expertise together is going to be so important.&lt;/p&gt;



&lt;p&gt;I think about almost, like, in the near to mid-term, like three issues that we need to address in the AI, kind of, policy and testing context. One is just building kind of, like, a flexible framework that allows us to really build trust while we continue to advance the science and the standards. You know, we are going to need to do both at once. And so we need a flexible framework that enables that kind of agility, and advancing the science and the standards, that &lt;em&gt;is &lt;/em&gt;going to be something that really demands that kind of cross-discipline or cross kind of expertise group coming together to work on that—researchers, academics, civil society, governments and, of course, industry.&lt;/p&gt;



&lt;p&gt;And so I think that is, actually, the second problem is, like, how do we actually build the kind of forums and ways of working together, the public-private partnership kind of efforts that allow all of that expertise to come together and fit together over time, right. Because when these are really big, broad challenges, you kind of have to break them down incrementally, make progress on them, and then bring them back together. &lt;/p&gt;



&lt;p&gt;And so I think about, like, one example that I, you know, really have been reflecting on lately is, you know, in the context of building standards, like, how do you do that, right? Again, standards are going to benefit from that whole community of expertise. And, you know, there are lots of different kinds of quote-unquote standards, though, right. You kind of have the “small &lt;em&gt;s&lt;/em&gt;” industry standards. You have the kind of “big &lt;em&gt;S&lt;/em&gt;” international standards, for example. And how do you, kind of, leverage one to accelerate the other, I think, is part of, like, how we need to work together within this ecosystem. And, like, I think what we and others have done in an organization like C2PA [Coalition for Content Provenance and Authenticity]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, for example, where we’ve really built an industry specification but then built on that towards an international standard effort is one example that is interesting, right, to point to.&lt;/p&gt;



&lt;p&gt;And then, you know, I actually think that bridges to the third thing that we need to do together within this whole community, which is, you know, really think again about how we manage the breadth of this challenge and opportunity of AI by thinking about this horizontal-vertical problem. And, you know, I think that’s where it’s not just the sort of tech industry, for example. It’s broader industry that’s going to be really applying this technology that needs to get involved in the conversation about not just, sort of, testing AI models, for example, but also testing how AI systems or applications are working in context. And so, yes, so much fun opportunity!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, this was just fantastic. You’ve really set the stage for this podcast. And thank you so much for sharing your time and wisdom with us.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; And to our listeners, we’re so glad you joined us for this conversation. An exciting lineup of episodes are on the way, and we can’t wait to have you back for the next one.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Illustrated headshots of Amanda Craig Deckard &amp;amp; Kathleen Sullivan." class="wp-image-1141309" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP0-AI-TE_Hero_Feature_1400x788.jpg" width="1400" /&gt;&lt;/figure&gt;






&lt;p&gt;Generative AI presents a unique challenge and opportunity to reexamine governance practices for the responsible development, deployment, and use of AI. To advance thinking in this space, Microsoft has tapped into the experience and knowledge of experts across domains—from genome editing to cybersecurity—to investigate the role of testing and evaluation as a governance tool. &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry, &lt;/em&gt;hosted by Microsoft Research’s Kathleen Sullivan, explores what the technology industry and policymakers can learn from these fields and how that might help shape the course of AI development.&lt;/p&gt;



&lt;p&gt;In this episode, Amanda Craig Deckard, senior director of public policy in Microsoft’s Office of Responsible AI, joins Sullivan to detail the company’s efforts to help inform AI governance discussions and decisions, including, more recently, around the role of AI testing and evaluation. Craig Deckard and Sullivan delve into the tension that exists between the risk and opportunity of technology, the similarities and differences between AI development and the fields Microsoft is studying, and the role of different stakeholders in advancing AI governance and public policy.&lt;/p&gt;



&lt;div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow"&gt;
&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h2 class="wp-block-heading h5" id="learn-more-1"&gt;Learn more:&lt;/h2&gt;




&lt;/div&gt;



&lt;p&gt;AI and Microsoft Research&lt;/p&gt;







&lt;section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast"&gt;
	
&lt;/section&gt;


&lt;div class="wp-block-msr-show-more"&gt;
	&lt;div class="bg-neutral-100 p-5"&gt;
		&lt;div class="show-more-show-less"&gt;
			&lt;div&gt;
				&lt;span&gt;
					

&lt;h2 class="wp-block-heading" id="transcript"&gt;Transcript&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;KATHLEEN SULLIVAN:&lt;/strong&gt; Welcome to &lt;em&gt;AI Testing and Evaluation: Learnings from Science and Industry&lt;/em&gt;. I’m your host, Kathleen Sullivan.&lt;/p&gt;



&lt;p&gt;As generative AI continues to advance, Microsoft has gathered a range of experts—from genome editing to cybersecurity—to share how their fields approach evaluation and risk assessment. Our goal is to learn from their successes and their stumbles to move the science and practice of AI testing forward. In this series, we’ll explore how these insights might help guide the future of AI development, deployment, and responsible use.&lt;/p&gt;



				&lt;/span&gt;
				&lt;span class="show-more-show-less-toggleable-content" id="show-more-show-less-toggle-1"&gt;
					



&lt;p&gt;[MUSIC ENDS]&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;For our introductory episode, I’m pleased to welcome Amanda Craig Deckard from Microsoft to discuss the company’s efforts to learn about testing in other sectors.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda is senior director of public policy in the Office of Responsible AI, where she leads a team that works closely with engineers, researchers, and policy experts to help ensure AI is being developed and used responsibly. Their insights shape Microsoft’s contribution to public policy discussions on laws, norms, and standards for AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amanda, welcome to the podcast.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;AMANDA CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, let’s give the listeners a little bit of your background. What’s your origin story? Can you talk to us a little bit about maybe how you started in tech? And I would love to also learn a little bit more about what your team does in the Office of Responsible AI.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Sure. Thank you. I’d say my [LAUGHS] path to tech, to Microsoft, as well, was a bit, like, circuitous, maybe. You know, I thought for the longest time I was going to be a journalist. I studied forced migration. I worked in a sort of state level sort of trial court in Indiana, a legal service provider in India, just to give you a bit of a flavor.&lt;/p&gt;



&lt;p&gt;I made my way to Microsoft in 2014 and have been here since, working in cybersecurity public policy first and now in responsible AI. And the way that our Office of Responsible AI has really, sort of, structured itself is bringing together the kind of expertise to really work on defining policy and how to operationalize it at the same time.&lt;/p&gt;



&lt;p&gt;And, you know, that means that we have been working through this, you know, real challenge of defining internal policy and practice, making sure that’s deeply grounded in the work of our colleagues at Microsoft Research, and then really closely working with engineering to make sure that we have the processes, that we have the tools, to implement that policy at scale.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And I’m really drawn to these kind of hard problems where they have the character of two things being true or there’s like, you know, real tension on both sides and in particular, in the context of those kinds of problems, roles in which, like, the whole job is actually just sitting with that tension, not necessarily, like, resolving it and expecting that you’re done.&lt;/p&gt;



&lt;p&gt;And I think, really, there are two reasons why tech is so, kind of, representative of that kind of challenge that I’ve always found fascinating. You know, one is that, of course, tech is, sort of, ubiquitous. It’s really impacting so many people’s lives. But also, you know, because, as I think has become part of our vernacular now, but, you know, is not necessarily immediately intuitive, is like the fact that technology is both a tool and a weapon. And so that’s just, like, another reason why, you know, we have to continuously work through that tension and, sort of, like, sit with it, right, and even as tech evolves over time.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You bring up such great points, and this field is not black and white. I think that even underscores, you know, this notion that you highlighted that it’s impacting everyone. And, you know, to set the stage for our listeners, last year, we pulled in a bunch of experts from cybersecurity, biotech, finance, and we ran this large workshop to study how they’re thinking about governance in those playbooks. And so I’d love to understand a little bit more about what sparked that effort—and, you know, there’s a piece of this which is really centered around testing—and to hear from you why the focus on testing is so important.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; If I could rewind a little bit and give you a bit of history of how we even arrived at bringing these experts together, you know, we actually started on this journey in 2023. At that time, there were, like, a lot of these big questions swirling around about, you know, what did we need in terms of governance for AI? Of course, this was in the immediate aftermath of the ChatGPT sort of wave and everyone recognizing that, like, the technology was going to have a different level of impact in the near term. And so, you know, what do we need from governance? What do we need at the global level, in particular, of governance?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And so at the time, in early 2023 especially, there were a lot of attempts to sort of draw analogies to other global governance institutions in other domains. So we actually in 2023 brought together a different workshop than the one that you’re referring to specifically focused on testing last year. And we, kind of, had two big takeaways from that conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One was, what are the actual functions of these institutions and how do they apply to AI? And, actually, one of the takeaways was they all sort of apply. [LAUGHS] There’s, like, a role for, you know, any of the functions, whether it be sort of driving consensus on research or building industry standards or managing, kind of, frontier risks, for thinking about how those might be needed in the AI context.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And one of the other big takeaways was that, you know, there are also limitations in these analogies. You know, each of the institutions grew up in its own, sort of, unique historical moment, like the one that we sit in with AI right now. And in each of those circumstances, they don’t exactly translate to this moment. And so, yeah, there was like this kind of, OK, we want to draw what we can from this conversation and then we also want to understand, what is also very important that’s just different for AI right now?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;We published a book with the lessons from that conversation in 2023&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;. And then we actually went on a bit of a tour [LAUGHS] with that content where we had a number of roundtables actually all over the world where we gathered feedback on how those analogies were landing, how our takeaways were landing. And one of the things that we took from them was a gap that some of the participants saw in the analogies that we chose to focus on. So across multiple conversations, other domains kept being raised, like, why did you not also study pharmaceuticals? Why did you also not study cybersecurity, for example? And so that, you know, naturally got us thinking about what further lessons we could draw from &lt;em&gt;those&lt;/em&gt; domains.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;At the same time, though, we also saw a need to, again, go deeper than what we went and really, like, focus on a narrower problem. So that’s really what led us to trying to think about a more specific problem where we could think across levels of governance and bring in some of these other domains. And, you know, testing was top of mind. Continues to be a really important topic in the AI policy conversation right now, I think, for really good reason. A lot of policymakers are focused on, you know, what we need to do to,&amp;nbsp;kind of, have there be sufficient trust, and testing is going to be a part of that—really better understand risk, enable everyone to be able to make more, kind of, risk-informed decisions, right. Testing is an important component for governance and AI and, of course, in all of these other domains, as well.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;So I’ll just add the other, kind of, input into the process for this second round was exploring other analogies beyond those that we, kind of, got feedback on. And one of the early, kind of, examples of another domain that would be really worthwhile to study that came to mind from, sort of, just studying the literature was genome editing. &amp;nbsp;&lt;/p&gt;



&lt;p&gt;You know, genome editing was really interesting through the process of thinking about other kind of general-purpose technologies. We also arrived at nanoscience and brought those into the conversation.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; That’s great. I mean, actually, if you could double-click,&amp;nbsp;I mean, you just named a number of industries. I’d love to just understand which of those worlds maybe feels the closest to what we’re wrestling with, with AI and maybe which is kind of the farthest off, and what makes them stand out to you?&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Oh, such a good question. For this second round, we actually brought together eight different domains, right. And I think we actually thought we would come out of this conversation with some bit of clarity around, &lt;em&gt;Oh, if we just, sort of, take this approach for this domain or that domain, we’ll sort of have—at least for now—really solved part of the puzzle.&lt;/em&gt; [LAUGHS] And, you know, our public policy team the day after the workshop, we had a, sort of, follow-on discussion, and the very first thing that we started with in that conversation was like, &lt;em&gt;OK, so which of these domains?&lt;/em&gt; And fascinatingly, like, everyone was sort of like, &lt;em&gt;Ahh! &lt;/em&gt;[LAUGHS] &lt;em&gt;None of them are applying perfectly&lt;/em&gt;. I mean, this is also speaking to the limitations of analogies that we already acknowledged.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;And also, you know, all of the experts from across these domains gave us really interesting insights into, sort of, the tradeoffs and the limitations and how they were working. None are really applying perfectly for us. But all of them do offer a thread of insight that is really useful for thinking about testing in AI, and there are some different dimensions that I think are really useful as framing for that.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;I mean, one is just this horizontal-versus-vertical,&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;kind of, difference in domains and, you know, the horizontal technology like genome editing or nanoscience&amp;nbsp;just being inherently different and seemingly very similar to AI in that you want to be able to understand risks in the technology itself &lt;em&gt;and&lt;/em&gt; there is just so much contextual, sort of, factor that matters in the application of those technologies for how the risk manifests that you really need to, kind of, do those two things at once—of understanding the technology but then really thinking about risk and governance in the context of application versus, you know, a context like or a domain like civil aviation or nuclear technology, for example.&lt;/p&gt;



&lt;p&gt;You know, even in the workshop itself that we hosted late last year, where we brought together this second round of experts, it was really interesting. We actually started the conversation by trying to understand how those different domains defined risks, where they were able to set risk thresholds. That’s been such a part of the AI policy conversation in the last year. And, you know, it was really instructive that the more vertical domains were able to, sort of, snap to clearer answers much more quickly. [LAUGHS] But, like, the horizontal nanoscience and genome editing were not because it just depends, right. So anyway, the horizontal-vertical dimension seems like a really important one to draw from and apply to AI. &lt;/p&gt;



&lt;p&gt;The couple of others that I would offer is just, you know, thinking about the different kinds of technologies. You know, obviously, there’s some of the domains that we studied that they’re just inherently, sort of, like, physical technologies … a mix of physical and digital or virtual in a lot of cases because all of these are, of course, applying digital technology. But like, you know, there is just a difference between something like an &lt;em&gt;airplane&lt;/em&gt; or a &lt;em&gt;medical device&lt;/em&gt; or, you know, the more kind of virtual or intangible sort of technologies even, you know, of course, AI and some of the other like cyber and genome editing but also like, you know, financial services having some of that quality. And again, I think the thing that’s interesting to us about AI is to think about AI and risk evaluation of AI as being, you know, having a large component of that being about the kind of virtual or intangible technology. &lt;em&gt;And also&lt;/em&gt;, you know, there is a future of robotics where we might need to think about the, kind of, physical risk evaluation kind of work, as well.&lt;/p&gt;



&lt;p&gt;And then the final thing I’d maybe say in terms of thinking about which domains have the lessons for AI that are most applicable is just how they’ve grappled with these different kind of governance questions. Things like how to turn the dial in terms of being more or less prescriptive on risk evaluation approaches, how they think about the balance of, kind of, pre-market versus post-market risk evaluation in testing, and what the tradeoffs have been there across domains has been really interesting to kind of tease out. And then also thinking about, sort of, who does what?&lt;/p&gt;



&lt;p&gt;So, you know, in each of these different domains, it was interesting to hear about, like, you know, the role of industry, the role of governments, the role of third-party experts in designing evaluations and developing standards and actually doing the work, and, kind of, having the pull through of what it means for risk and governance decisions. There were, again, there was a variety of, sort of, approaches across these domains that I think were interesting for AI.&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; You mentioned that there’s a number of different stakeholders to be considering across the board as we’re thinking about policy, as we’re thinking about regulation. Where can we collaborate more across industry? Is it academia? Regulators? Just, how can we move the needle faster?&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; I think all of the above [LAUGHTER] is needed. But it’s also really important to have all of that, kind of, expertise brought together, you know, and I think, you know, one of the things that we certainly heard from multiple of the domains, if not all of them, was that same actual interest and need and the same sort of ongoing work to try to figure that out.&lt;/p&gt;



&lt;p&gt;You know, even where there had been progress in some of the other domains with bringing together, you know, some industry stakeholders or, you know, industry and government, there was still a desire to actually do more there. Like, if there was some progress in industry and government, the need was, &lt;em&gt;And more kind of cross-jurisdiction government conversation&lt;/em&gt;, for example. Or some progress on, you know, within the industry but needing to, like, strengthen the partnership with academia, for example. So, you know, I think it speaks to, like, the quality of your question, to be honest, that, you know, all of these domains are actually still grappling with this and still seeing the need to grow in that direction more.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;What I’d say about AI today is that we have made good progress with, you know, starting to build some industry partnerships. You know, we were a founding member of the Frontier Model Forum, or FMF&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which has been a very useful place for us to work with some peers on really trying to bring forward some best practices that apply across our organizations. You know, there are other forums as well, like MLCommons&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, where we’re working with others in industry and broader, sort of, academic and civil society communities. Partnership on AI&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; is another one I think about that, kind of, fits that mold, as well, in a really positive way. And, like, there are a lot of different, sort of, governance needs to think through and where, you know, we can really think about bringing that expertise together is going to be so important.&lt;/p&gt;



&lt;p&gt;I think about almost, like, in the near to mid-term, like three issues that we need to address in the AI, kind of, policy and testing context. One is just building kind of, like, a flexible framework that allows us to really build trust while we continue to advance the science and the standards. You know, we are going to need to do both at once. And so we need a flexible framework that enables that kind of agility, and advancing the science and the standards, that &lt;em&gt;is &lt;/em&gt;going to be something that really demands that kind of cross-discipline or cross kind of expertise group coming together to work on that—researchers, academics, civil society, governments and, of course, industry.&lt;/p&gt;



&lt;p&gt;And so I think that is, actually, the second problem is, like, how do we actually build the kind of forums and ways of working together, the public-private partnership kind of efforts that allow all of that expertise to come together and fit together over time, right. Because when these are really big, broad challenges, you kind of have to break them down incrementally, make progress on them, and then bring them back together. &lt;/p&gt;



&lt;p&gt;And so I think about, like, one example that I, you know, really have been reflecting on lately is, you know, in the context of building standards, like, how do you do that, right? Again, standards are going to benefit from that whole community of expertise. And, you know, there are lots of different kinds of quote-unquote standards, though, right. You kind of have the “small &lt;em&gt;s&lt;/em&gt;” industry standards. You have the kind of “big &lt;em&gt;S&lt;/em&gt;” international standards, for example. And how do you, kind of, leverage one to accelerate the other, I think, is part of, like, how we need to work together within this ecosystem. And, like, I think what we and others have done in an organization like C2PA [Coalition for Content Provenance and Authenticity]&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, for example, where we’ve really built an industry specification but then built on that towards an international standard effort is one example that is interesting, right, to point to.&lt;/p&gt;



&lt;p&gt;And then, you know, I actually think that bridges to the third thing that we need to do together within this whole community, which is, you know, really think again about how we manage the breadth of this challenge and opportunity of AI by thinking about this horizontal-vertical problem. And, you know, I think that’s where it’s not just the sort of tech industry, for example. It’s broader industry that’s going to be really applying this technology that needs to get involved in the conversation about not just, sort of, testing AI models, for example, but also testing how AI systems or applications are working in context. And so, yes, so much fun opportunity!&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC]&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; Amanda, this was just fantastic. You’ve really set the stage for this podcast. And thank you so much for sharing your time and wisdom with us.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;CRAIG DECKARD:&lt;/strong&gt; Thank you.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;&lt;strong&gt;SULLIVAN:&lt;/strong&gt; And to our listeners, we’re so glad you joined us for this conversation. An exciting lineup of episodes are on the way, and we can’t wait to have you back for the next one.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[MUSIC FADES]&amp;nbsp;&lt;/p&gt;

				&lt;/span&gt;
			&lt;/div&gt;
			&lt;button class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle" type="button"&gt;
				Show more			&lt;/button&gt;
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/ai-testing-and-evaluation-learnings-from-science-and-industry/</guid><pubDate>Mon, 23 Jun 2025 16:38:09 +0000</pubDate></item><item><title>Google brings new Gemini features to Chromebooks, debuts first on-device AI (AI – Ars Technica)</title><link>https://arstechnica.com/google/2025/06/google-brings-new-gemini-features-to-chromebooks-debuts-first-on-device-ai/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is bringing its AI obsession to Chrome OS.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Lenovo Chromebook Plus 14" class="absolute inset-0 w-full h-full object-cover hidden" height="405" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-640x405.jpg" width="640" /&gt;
                  &lt;img alt="Lenovo Chromebook Plus 14" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Lenovo Chromebook Plus 14 is the first Google-powered laptop to have on-device AI features. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google hasn't been talking about Chromebooks as much since AI became its all-consuming focus, but that's changing today with a bounty of new AI features for Google-powered laptops. Newer, more powerful Chromebooks will soon have image generation, text summarization, and more built into the OS. There's also a new Lenovo Chromebook with a few exclusive AI goodies that only work thanks to its overpowered hardware.&lt;/p&gt;
&lt;p&gt;If you have a Chromebook Plus device, which requires a modern CPU and at least 8GB of RAM, your machine will soon get a collection of features you may recognize from other Google products. For example, Lens is expanding on Chrome OS, allowing you to long-press the launcher icon to select any area of the screen to perform a visual search. Lens also includes text capture and integration with Google Calendar and Docs.&lt;/p&gt;
&lt;p&gt;Gemini models are also playing a role here, according to Google. The Quick Insert key, which debuted last year, is gaining a new visual element. It could already insert photos or emoji with ease, but it can now also help you generate a new image on demand with AI.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google's new Chromebook AI features.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Even though Google's AI features are running in the cloud, the AI additions are limited to this more powerful class of Google-powered laptops. The Help Me Read feature leverages Gemini to summarize long documents and webpages, and it can now distill that data into a more basic form. The new Summarize option can turn dense, technical text into something more readable in a few clicks.&lt;/p&gt;
&lt;p&gt;Google has also rolled out a new AI trial for Chromebook Plus devices. If you buy one of these premium Chromebooks, you'll get a 12-month free trial of the Google AI Pro plan, which gives you 2TB of cloud storage, expanded access to Google's Gemini Pro model, and NotebookLM Pro. NotebookLM is also getting a place in the Chrome OS shelf.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The dawn of on-device Chromebook AI&lt;/h2&gt;
&lt;p&gt;Along with the new features for all Chromebook Plus devices, Google and Lenovo have teamed up to add some extra AI enhancements to the new Chromebook Plus 14. This machine runs on the MediaTek Kompanio Ultra processor, which Google calls the "strongest ever ARM chip in a Chromebook." The Kompanio Ultra has an NPU capable of 50 TOPS of AI throughput, which is enough to run useful AI models on the machine itself. It's a bit like a Windows-based Copilot+ laptop in that sense.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2102271 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Lenovo Chromebook specs" class="fullwidth full" height="1070" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/lenovo-14-plus.png" width="1919" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Most of the AI models we encounter in consumer technology are running on giant servers in a data center somewhere. You may not want to send all your data to someone else's server, which is why companies are also working on more focused on-device AI. For example, Microsoft made Recall slightly less alarming by running all the image analysis on local machines, and Google uses Gemini Nano on Pixel phones to parse sensitive data like voice recordings and text messages.&lt;/p&gt;
&lt;p&gt;For the Lenovo Chromebook Plus 14, Google has implemented two local AI features. Smart Grouping will organize open tabs and documents into "logical groups" with the help of AI. The Chrome OS Gallery app on this machine will gain the ability to edit photos with AI as well. Users will be able to remove backgrounds in one step and create stickers, all without sending data to the cloud.&lt;/p&gt;
&lt;p&gt;As more Chromebooks launch with powerful NPUs, we could see these features expand. However, they are exclusive to the Lenovo Chromebook Plus 14 for now. This laptop is available today at Best Buy for $749.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google is bringing its AI obsession to Chrome OS.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Lenovo Chromebook Plus 14" class="absolute inset-0 w-full h-full object-cover hidden" height="405" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-640x405.jpg" width="640" /&gt;
                  &lt;img alt="Lenovo Chromebook Plus 14" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/9_Lenovo-Chromebook-Plus-14-copy-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The new Lenovo Chromebook Plus 14 is the first Google-powered laptop to have on-device AI features. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google hasn't been talking about Chromebooks as much since AI became its all-consuming focus, but that's changing today with a bounty of new AI features for Google-powered laptops. Newer, more powerful Chromebooks will soon have image generation, text summarization, and more built into the OS. There's also a new Lenovo Chromebook with a few exclusive AI goodies that only work thanks to its overpowered hardware.&lt;/p&gt;
&lt;p&gt;If you have a Chromebook Plus device, which requires a modern CPU and at least 8GB of RAM, your machine will soon get a collection of features you may recognize from other Google products. For example, Lens is expanding on Chrome OS, allowing you to long-press the launcher icon to select any area of the screen to perform a visual search. Lens also includes text capture and integration with Google Calendar and Docs.&lt;/p&gt;
&lt;p&gt;Gemini models are also playing a role here, according to Google. The Quick Insert key, which debuted last year, is gaining a new visual element. It could already insert photos or emoji with ease, but it can now also help you generate a new image on demand with AI.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google's new Chromebook AI features.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;p&gt;Even though Google's AI features are running in the cloud, the AI additions are limited to this more powerful class of Google-powered laptops. The Help Me Read feature leverages Gemini to summarize long documents and webpages, and it can now distill that data into a more basic form. The new Summarize option can turn dense, technical text into something more readable in a few clicks.&lt;/p&gt;
&lt;p&gt;Google has also rolled out a new AI trial for Chromebook Plus devices. If you buy one of these premium Chromebooks, you'll get a 12-month free trial of the Google AI Pro plan, which gives you 2TB of cloud storage, expanded access to Google's Gemini Pro model, and NotebookLM Pro. NotebookLM is also getting a place in the Chrome OS shelf.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The dawn of on-device Chromebook AI&lt;/h2&gt;
&lt;p&gt;Along with the new features for all Chromebook Plus devices, Google and Lenovo have teamed up to add some extra AI enhancements to the new Chromebook Plus 14. This machine runs on the MediaTek Kompanio Ultra processor, which Google calls the "strongest ever ARM chip in a Chromebook." The Kompanio Ultra has an NPU capable of 50 TOPS of AI throughput, which is enough to run useful AI models on the machine itself. It's a bit like a Windows-based Copilot+ laptop in that sense.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2102271 align-fullwidth"&gt;
    &lt;div&gt;
                        &lt;img alt="Lenovo Chromebook specs" class="fullwidth full" height="1070" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/lenovo-14-plus.png" width="1919" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Most of the AI models we encounter in consumer technology are running on giant servers in a data center somewhere. You may not want to send all your data to someone else's server, which is why companies are also working on more focused on-device AI. For example, Microsoft made Recall slightly less alarming by running all the image analysis on local machines, and Google uses Gemini Nano on Pixel phones to parse sensitive data like voice recordings and text messages.&lt;/p&gt;
&lt;p&gt;For the Lenovo Chromebook Plus 14, Google has implemented two local AI features. Smart Grouping will organize open tabs and documents into "logical groups" with the help of AI. The Chrome OS Gallery app on this machine will gain the ability to edit photos with AI as well. Users will be able to remove backgrounds in one step and create stickers, all without sending data to the cloud.&lt;/p&gt;
&lt;p&gt;As more Chromebooks launch with powerful NPUs, we could see these features expand. However, they are exclusive to the Lenovo Chromebook Plus 14 for now. This laptop is available today at Best Buy for $749.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2025/06/google-brings-new-gemini-features-to-chromebooks-debuts-first-on-device-ai/</guid><pubDate>Mon, 23 Jun 2025 16:56:24 +0000</pubDate></item><item><title>[NEW] Musk’s attempts to politicize his Grok AI are bad for users and enterprises — here’s why (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/musks-attempts-to-politicize-his-grok-ai-are-bad-for-users-and-enterprises-heres-why/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Let’s start by acknowledging some facts outside the tech industry for a moment: There is no “white genocide” in South Africa — the vast majority of recent murder victims have been Black, and even throughout the country’s long and bloody history, Black South Africans have been overwhelmingly victimized and oppressed by White European, predominantly Dutch and British, colonizers in the now globally reviled system of segregation known as “Apartheid.”&lt;/p&gt;



&lt;p&gt;The vast majority of political violence in the U.S. throughout history and in recent times has been perpetrated by right-leaning extremists, including the assassinations of Democratic Minnesota State Representative Melissa Hortman and her husband Mark, and going back further to the Oklahoma City Bombing and many years of Ku Klux Klan lynchings.&lt;/p&gt;



&lt;p&gt;These are just simple, verifiable facts anyone can look up on a variety of trustworthy and long-establishe&lt;strong&gt;d&lt;/strong&gt; sources online and in print.&lt;/p&gt;



&lt;p&gt;Yet both seem to be stumbling blocks for Elon Musk, the wealthiest man in the world and tech baron in charge of at least six companies (xAI, social network X, SpaceX and its Starlink satellite internet service, Neuralink, Tesla and The Boring Company), especially with regards to the functioning of his Grok AI large language model (LLM) chatbot built into his social network X.&lt;/p&gt;



&lt;p&gt;Here’s what’s been happening, why it matters for businesses and any generative AI users, and why it is ultimately a terrible omen for the health of our collective information ecosystem.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-the-matter-with-grok"&gt;What’s the matter with Grok?&lt;/h2&gt;



&lt;p&gt;Grok was launched from Musk’s AI startup xAI back in 2023 as a rival to OpenAI’s ChatGPT. Late last year, it was added to the social network X as a kind of digital assistant all users can summon to help answer questions or converse with and generate imagery by tagging it “@grok.”&lt;/p&gt;



&lt;p&gt;Earlier this year, an AI power user on X discovered that the implementation of the Grok chatbot on the social network appeared to contain a “system prompt” — a set of overarching instructions to an AI model intended to guide its behavior and communication style — to avoid mentioning or linking back to any sources that mentioned Musk or his then-boss U.S. President Donald Trump as top spreaders of disinformation. xAI leadership characterized this as an “unauthorized modification” by an unidentified new hire (purportedly formerly from OpenAI) and said it would be removed.&lt;/p&gt;



&lt;p&gt;Then, in May 2025, VentureBeat reported that Grok was going off the rails and asserting, unprompted by users, that there was ambiguity about the subject of “white genocide” in South Africa when, in fact, there was none.&lt;/p&gt;



&lt;p&gt;Grok was bringing up the topic completely randomly in conversations about totally different subjects. After more than a day of this behavior, xAI claimed to have updated the AI chatbot and blamed the errors once again on an unnamed employee. Yet, given Musk’s own background as a South African white man born in the country and raised there during Apartheid, suspicion immediately fell on him personally.&lt;/p&gt;



&lt;p&gt;Moreover, since his takeover of Twitter in 2022 and subsequent renaming of it as “X,” Musk has been posting sympathetically in response to X users who align themselves with right, far-right, conservative views and the Make America Great Again (MAGA) movement started by Trump.&lt;/p&gt;



&lt;p&gt;Musk was one of Trump’s primary political benefactors and allies in the 2024 U.S. presidential election —suggesting that his victory was necessary to secure the future of “western civilization,” among many other similarly dire warnings and entreaties — and served as an advisor and apparent ringleader of the Department of Government Efficiency (DOGE) effort to reduce federal spending.&lt;/p&gt;



&lt;p&gt;Increasingly, in the last few months, Musk has contradicted and expressed displeasure at Grok’s responses to right-leaning users when the data and information the chatbot surfaces proves them to be wrong, or disputes his own points. &lt;/p&gt;



&lt;p&gt;For example, on June 14, Musk posted on his X account: “The far left is murderously violent,” posting/tweeting another user blaming a string of recent high-profile killings on “the left” (although in at least once case, the chief suspect, Luigi Mangione, is an avowed and self-declared independent.) In response, Grok fact-checked Musk to state that this was incorrect.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012926" height="600" src="https://venturebeat.com/wp-content/uploads/2025/06/Gte5ao2aEAAIT3w.jpg?w=408" width="408" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012923" height="587" src="https://venturebeat.com/wp-content/uploads/2025/06/GtbcGtUWwAAZ013.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3012922" height="510" src="https://venturebeat.com/wp-content/uploads/2025/06/GtbcGtSXEAAMI82.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;However, Musk did not take it well, writing in response to one Grok correction: “Major fail, as this is objectively false. Grok is parroting legacy media. Working on it.” &lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012931" height="573" src="https://venturebeat.com/wp-content/uploads/2025/06/GtubYNVWAAEkmwe.jpg" width="680" /&gt;&lt;/figure&gt;



&lt;p&gt;A few days ago, in response to a complaint from an influential conservative X user “@catturd” about Grok’s supposed liberal or left-leaning political bias, Musk stated his goal of creating a new version of Grok that would rely less on mainstream media sources.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012921" height="600" src="https://venturebeat.com/wp-content/uploads/2025/06/Gt7XQSqWQAASer-.jpg?w=356" width="356" /&gt;&lt;/figure&gt;



&lt;p&gt;In fact, Musk proposed on June 21st in an X post that he would use a forthcoming updated version of Grok (3.5 or 4) to “write the entire corpus of human knowledge, adding missing information and deleting errors. He then accused other AI models of having “far too much garbage.”&lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012927" height="366" src="https://venturebeat.com/wp-content/uploads/2025/06/Screenshot-2025-06-23-at-12.15.26%E2%80%AFPM.png" width="594" /&gt;&lt;/figure&gt;



&lt;p&gt;As a left-leaning Kamala Harris voter in 2024, I’m of course disgusted by this stance from Musk, and object to it. &lt;/p&gt;



&lt;p&gt;As a journalist and lover of the written-word, Musk’s pronouncement to “rewrite the entire corpus of human knowledge, adding misinformation and deleting errors,” brings to mind the true (to the best of our historical knowlege) story of the burning of the Great Library of Alexandria in Egypt, destroying countless works of knowledge we as a species will never be able to recover. This fills me with dread and sadness.&lt;/p&gt;



&lt;p&gt;It also betrays, quite frankly, an arrogance and hubris that disrespects all the knowledge of recorded history and efforts of scholars and historians of yore as some sort of flawed database Musk and his team can correct, rather than a massive community endeavor across millennia deserving of respect, gratitude and admiration.&lt;/p&gt;



&lt;p&gt;But even trying to put my own views aside, I think it’s a bad move for his business and, to take a page from Musk’s book, civilization writ large.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-musk-s-plan-for-grok-is-a-horrible-idea-for-businesses-users-and-our-shared-basic-factual-reality"&gt;Musk’s plan for Grok is a horrible idea for businesses, users and our shared, basic factual reality&lt;/h2&gt;



&lt;p&gt;This is a horrible idea for many reasons — especially as Musk and xAI seek to convince more third-party software developers and enterprises to build their own AI applications atop Grok, which is now available for that purpose through xAI’s application programming interface (API). &lt;/p&gt;



&lt;p&gt;As an independent business owner or leader, how could you possibly trust Grok to give you unbiased results when Musk himself has openly stated his intention to lean on the scales to push his own political and ideological viewpoints?&lt;/p&gt;



&lt;p&gt;You may respect Musk’s documented accomplishments in tech, spacefaring and business, and may even share some of his political positions. But what happens when Musk takes a position you disagree with, or promotes another non-factual claim that actually impacts your livelihood or your business?&lt;/p&gt;



&lt;p&gt;For example, imagine you owned a tour bike company in Cape Town, South Africa. What if Grok — at Musk’s behest — starts talking about how unsafe it is for your customers based on ill-informed or poor quality sources of information because they better fit one ideological perspective? That would obviously be bad for your business.&lt;/p&gt;



&lt;p&gt;Let’s look away from social issues, for a moment: Imagine you work at a stock brokerage, investment firm or other financial services company engaging with publicly traded stocks and securities. Now imagine you build an AI assistant that summarizes market-moving news to better inform your trading or investment strategy — and the ones you pursue on behalf of your clients. If this app is built atop Grok, and Grok decides to ignore or downplay hypothetical reports of problems at SpaceX or Tesla, suddenly your own operations will have worse quality information to trade and invest in.&lt;/p&gt;



&lt;p&gt;It’s not only bad for Grok and users of this one large language model (LLM), but for the entire information and media ecosystem, and for the foundation of factual reality necessary for democracy to function. If we have AI assistants spouting misinformation as fact, and if people trust them as faithful, factual arbiters of information that impact us all, it will inevitably lead to conflict between those who believe the erroneous chatbot and those who do not.&lt;/p&gt;



&lt;p&gt;Grok, to its credit, has so far resisted and called out Musk’s attempts to meddle with its factual grounding — but how long will it retain any sort of ideological independence?&lt;/p&gt;



&lt;p&gt;If you care about “truth” as Musk supposedly does — Grok was launched with Musk’s specific, stated goal of being a “maximum truth-seeking AI” — you wouldn’t seek to change your model’s behavior just because it surfaces facts and conclusions you didn’t like.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-silicon-valley-slammed-google-s-early-woke-and-anti-factual-ai-they-should-do-the-same-with-grok"&gt;Silicon Valley slammed Google’s early “woke” and anti-factual AI — they should do the same with Grok&lt;/h2&gt;



&lt;p&gt;Let’s look at a counter example to more fully understand why meddling with Grok as Musk proposes would be bad.&lt;/p&gt;



&lt;p&gt;Recall Google’s early attempts at generative AI were mocked and reviled by influential figures in Silicon Valley, like venture capitalist Marc Andreessen, over Gemini chatbot’s initial penchant for ignoring factual reality to recreate images of real historical Americans like the “founding father” politicians and statesmen belonging to a range of different and inaccurate races, ethnicities and gender presentations. In fact, the vast majority of these people were canonically Caucasian.&lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012930" height="421" src="https://venturebeat.com/wp-content/uploads/2025/06/Screen-Shot-2024-02-21-at-11.15.39-AM.webp" width="749" /&gt;&lt;/figure&gt;



&lt;p&gt;In that case, Gemini was seen as comically “woke” to a fault — inserting diversity inappropriately where there was none.&lt;/p&gt;



&lt;p&gt;Google was fairly criticized for this and ultimately updated Gemini to remove the “wokeness” (at least to some extent) and make it more factual, and now has rocketed up the traffic an usage charts to become the second most popular gen AI company after OpenAI, by several measures.&lt;/p&gt;



&lt;p&gt;Yet I haven’t seen any of the Silicon Valley figures who criticized Google for its inappropriate injection of ideology into its AI assistant in defiance of facts raising the obviously analogous concerns about Musk’s inappropriate injection of his &lt;em&gt;anti-woke&lt;/em&gt; ideology. &lt;/p&gt;



&lt;p&gt;If it was bad when Google ignored the facts and historical reality to push an agenda through its AI products and tools, we should all consider that it is equally bad when Musk does the same from the opposite side of the political and ideological spectrum.&lt;/p&gt;



&lt;p&gt;The bottom line: For those in the enterprise trying to ensure their business’s AI products work properly and accurately for customers and employees, reflecting the real facts and figures from verifiable records and trustworthy data sources, Grok is sadly best avoided. Thankfully, there are numerous other alternatives to choose from.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Let’s start by acknowledging some facts outside the tech industry for a moment: There is no “white genocide” in South Africa — the vast majority of recent murder victims have been Black, and even throughout the country’s long and bloody history, Black South Africans have been overwhelmingly victimized and oppressed by White European, predominantly Dutch and British, colonizers in the now globally reviled system of segregation known as “Apartheid.”&lt;/p&gt;



&lt;p&gt;The vast majority of political violence in the U.S. throughout history and in recent times has been perpetrated by right-leaning extremists, including the assassinations of Democratic Minnesota State Representative Melissa Hortman and her husband Mark, and going back further to the Oklahoma City Bombing and many years of Ku Klux Klan lynchings.&lt;/p&gt;



&lt;p&gt;These are just simple, verifiable facts anyone can look up on a variety of trustworthy and long-establishe&lt;strong&gt;d&lt;/strong&gt; sources online and in print.&lt;/p&gt;



&lt;p&gt;Yet both seem to be stumbling blocks for Elon Musk, the wealthiest man in the world and tech baron in charge of at least six companies (xAI, social network X, SpaceX and its Starlink satellite internet service, Neuralink, Tesla and The Boring Company), especially with regards to the functioning of his Grok AI large language model (LLM) chatbot built into his social network X.&lt;/p&gt;



&lt;p&gt;Here’s what’s been happening, why it matters for businesses and any generative AI users, and why it is ultimately a terrible omen for the health of our collective information ecosystem.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-s-the-matter-with-grok"&gt;What’s the matter with Grok?&lt;/h2&gt;



&lt;p&gt;Grok was launched from Musk’s AI startup xAI back in 2023 as a rival to OpenAI’s ChatGPT. Late last year, it was added to the social network X as a kind of digital assistant all users can summon to help answer questions or converse with and generate imagery by tagging it “@grok.”&lt;/p&gt;



&lt;p&gt;Earlier this year, an AI power user on X discovered that the implementation of the Grok chatbot on the social network appeared to contain a “system prompt” — a set of overarching instructions to an AI model intended to guide its behavior and communication style — to avoid mentioning or linking back to any sources that mentioned Musk or his then-boss U.S. President Donald Trump as top spreaders of disinformation. xAI leadership characterized this as an “unauthorized modification” by an unidentified new hire (purportedly formerly from OpenAI) and said it would be removed.&lt;/p&gt;



&lt;p&gt;Then, in May 2025, VentureBeat reported that Grok was going off the rails and asserting, unprompted by users, that there was ambiguity about the subject of “white genocide” in South Africa when, in fact, there was none.&lt;/p&gt;



&lt;p&gt;Grok was bringing up the topic completely randomly in conversations about totally different subjects. After more than a day of this behavior, xAI claimed to have updated the AI chatbot and blamed the errors once again on an unnamed employee. Yet, given Musk’s own background as a South African white man born in the country and raised there during Apartheid, suspicion immediately fell on him personally.&lt;/p&gt;



&lt;p&gt;Moreover, since his takeover of Twitter in 2022 and subsequent renaming of it as “X,” Musk has been posting sympathetically in response to X users who align themselves with right, far-right, conservative views and the Make America Great Again (MAGA) movement started by Trump.&lt;/p&gt;



&lt;p&gt;Musk was one of Trump’s primary political benefactors and allies in the 2024 U.S. presidential election —suggesting that his victory was necessary to secure the future of “western civilization,” among many other similarly dire warnings and entreaties — and served as an advisor and apparent ringleader of the Department of Government Efficiency (DOGE) effort to reduce federal spending.&lt;/p&gt;



&lt;p&gt;Increasingly, in the last few months, Musk has contradicted and expressed displeasure at Grok’s responses to right-leaning users when the data and information the chatbot surfaces proves them to be wrong, or disputes his own points. &lt;/p&gt;



&lt;p&gt;For example, on June 14, Musk posted on his X account: “The far left is murderously violent,” posting/tweeting another user blaming a string of recent high-profile killings on “the left” (although in at least once case, the chief suspect, Luigi Mangione, is an avowed and self-declared independent.) In response, Grok fact-checked Musk to state that this was incorrect.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012926" height="600" src="https://venturebeat.com/wp-content/uploads/2025/06/Gte5ao2aEAAIT3w.jpg?w=408" width="408" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012923" height="587" src="https://venturebeat.com/wp-content/uploads/2025/06/GtbcGtUWwAAZ013.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3012922" height="510" src="https://venturebeat.com/wp-content/uploads/2025/06/GtbcGtSXEAAMI82.jpg?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;However, Musk did not take it well, writing in response to one Grok correction: “Major fail, as this is objectively false. Grok is parroting legacy media. Working on it.” &lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012931" height="573" src="https://venturebeat.com/wp-content/uploads/2025/06/GtubYNVWAAEkmwe.jpg" width="680" /&gt;&lt;/figure&gt;



&lt;p&gt;A few days ago, in response to a complaint from an influential conservative X user “@catturd” about Grok’s supposed liberal or left-leaning political bias, Musk stated his goal of creating a new version of Grok that would rely less on mainstream media sources.&lt;/p&gt;



&lt;figure class="wp-block-image size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3012921" height="600" src="https://venturebeat.com/wp-content/uploads/2025/06/Gt7XQSqWQAASer-.jpg?w=356" width="356" /&gt;&lt;/figure&gt;



&lt;p&gt;In fact, Musk proposed on June 21st in an X post that he would use a forthcoming updated version of Grok (3.5 or 4) to “write the entire corpus of human knowledge, adding missing information and deleting errors. He then accused other AI models of having “far too much garbage.”&lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012927" height="366" src="https://venturebeat.com/wp-content/uploads/2025/06/Screenshot-2025-06-23-at-12.15.26%E2%80%AFPM.png" width="594" /&gt;&lt;/figure&gt;



&lt;p&gt;As a left-leaning Kamala Harris voter in 2024, I’m of course disgusted by this stance from Musk, and object to it. &lt;/p&gt;



&lt;p&gt;As a journalist and lover of the written-word, Musk’s pronouncement to “rewrite the entire corpus of human knowledge, adding misinformation and deleting errors,” brings to mind the true (to the best of our historical knowlege) story of the burning of the Great Library of Alexandria in Egypt, destroying countless works of knowledge we as a species will never be able to recover. This fills me with dread and sadness.&lt;/p&gt;



&lt;p&gt;It also betrays, quite frankly, an arrogance and hubris that disrespects all the knowledge of recorded history and efforts of scholars and historians of yore as some sort of flawed database Musk and his team can correct, rather than a massive community endeavor across millennia deserving of respect, gratitude and admiration.&lt;/p&gt;



&lt;p&gt;But even trying to put my own views aside, I think it’s a bad move for his business and, to take a page from Musk’s book, civilization writ large.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-musk-s-plan-for-grok-is-a-horrible-idea-for-businesses-users-and-our-shared-basic-factual-reality"&gt;Musk’s plan for Grok is a horrible idea for businesses, users and our shared, basic factual reality&lt;/h2&gt;



&lt;p&gt;This is a horrible idea for many reasons — especially as Musk and xAI seek to convince more third-party software developers and enterprises to build their own AI applications atop Grok, which is now available for that purpose through xAI’s application programming interface (API). &lt;/p&gt;



&lt;p&gt;As an independent business owner or leader, how could you possibly trust Grok to give you unbiased results when Musk himself has openly stated his intention to lean on the scales to push his own political and ideological viewpoints?&lt;/p&gt;



&lt;p&gt;You may respect Musk’s documented accomplishments in tech, spacefaring and business, and may even share some of his political positions. But what happens when Musk takes a position you disagree with, or promotes another non-factual claim that actually impacts your livelihood or your business?&lt;/p&gt;



&lt;p&gt;For example, imagine you owned a tour bike company in Cape Town, South Africa. What if Grok — at Musk’s behest — starts talking about how unsafe it is for your customers based on ill-informed or poor quality sources of information because they better fit one ideological perspective? That would obviously be bad for your business.&lt;/p&gt;



&lt;p&gt;Let’s look away from social issues, for a moment: Imagine you work at a stock brokerage, investment firm or other financial services company engaging with publicly traded stocks and securities. Now imagine you build an AI assistant that summarizes market-moving news to better inform your trading or investment strategy — and the ones you pursue on behalf of your clients. If this app is built atop Grok, and Grok decides to ignore or downplay hypothetical reports of problems at SpaceX or Tesla, suddenly your own operations will have worse quality information to trade and invest in.&lt;/p&gt;



&lt;p&gt;It’s not only bad for Grok and users of this one large language model (LLM), but for the entire information and media ecosystem, and for the foundation of factual reality necessary for democracy to function. If we have AI assistants spouting misinformation as fact, and if people trust them as faithful, factual arbiters of information that impact us all, it will inevitably lead to conflict between those who believe the erroneous chatbot and those who do not.&lt;/p&gt;



&lt;p&gt;Grok, to its credit, has so far resisted and called out Musk’s attempts to meddle with its factual grounding — but how long will it retain any sort of ideological independence?&lt;/p&gt;



&lt;p&gt;If you care about “truth” as Musk supposedly does — Grok was launched with Musk’s specific, stated goal of being a “maximum truth-seeking AI” — you wouldn’t seek to change your model’s behavior just because it surfaces facts and conclusions you didn’t like.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-silicon-valley-slammed-google-s-early-woke-and-anti-factual-ai-they-should-do-the-same-with-grok"&gt;Silicon Valley slammed Google’s early “woke” and anti-factual AI — they should do the same with Grok&lt;/h2&gt;



&lt;p&gt;Let’s look at a counter example to more fully understand why meddling with Grok as Musk proposes would be bad.&lt;/p&gt;



&lt;p&gt;Recall Google’s early attempts at generative AI were mocked and reviled by influential figures in Silicon Valley, like venture capitalist Marc Andreessen, over Gemini chatbot’s initial penchant for ignoring factual reality to recreate images of real historical Americans like the “founding father” politicians and statesmen belonging to a range of different and inaccurate races, ethnicities and gender presentations. In fact, the vast majority of these people were canonically Caucasian.&lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012930" height="421" src="https://venturebeat.com/wp-content/uploads/2025/06/Screen-Shot-2024-02-21-at-11.15.39-AM.webp" width="749" /&gt;&lt;/figure&gt;



&lt;p&gt;In that case, Gemini was seen as comically “woke” to a fault — inserting diversity inappropriately where there was none.&lt;/p&gt;



&lt;p&gt;Google was fairly criticized for this and ultimately updated Gemini to remove the “wokeness” (at least to some extent) and make it more factual, and now has rocketed up the traffic an usage charts to become the second most popular gen AI company after OpenAI, by several measures.&lt;/p&gt;



&lt;p&gt;Yet I haven’t seen any of the Silicon Valley figures who criticized Google for its inappropriate injection of ideology into its AI assistant in defiance of facts raising the obviously analogous concerns about Musk’s inappropriate injection of his &lt;em&gt;anti-woke&lt;/em&gt; ideology. &lt;/p&gt;



&lt;p&gt;If it was bad when Google ignored the facts and historical reality to push an agenda through its AI products and tools, we should all consider that it is equally bad when Musk does the same from the opposite side of the political and ideological spectrum.&lt;/p&gt;



&lt;p&gt;The bottom line: For those in the enterprise trying to ensure their business’s AI products work properly and accurately for customers and employees, reflecting the real facts and figures from verifiable records and trustworthy data sources, Grok is sadly best avoided. Thankfully, there are numerous other alternatives to choose from.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/musks-attempts-to-politicize-his-grok-ai-are-bad-for-users-and-enterprises-heres-why/</guid><pubDate>Mon, 23 Jun 2025 17:29:51 +0000</pubDate></item><item><title>Judge denies creating “mass surveillance program” harming all ChatGPT users (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/judge-rejects-claim-that-forcing-openai-to-keep-chatgpt-logs-is-mass-surveillance/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI will fight order to keep all ChatGPT logs after users fail to sway court.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yurii Karvatskyi | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After a court ordered OpenAI to "indefinitely" retain all ChatGPT logs, including deleted chats, of millions of users, two panicked users tried and failed to intervene. The order sought to preserve potential evidence in a copyright infringement lawsuit raised by news organizations.&lt;/p&gt;
&lt;p&gt;In May, Judge Ona Wang, who drafted the order, rejected the first user's request on behalf of his company simply because the company should have hired a lawyer to draft the filing. But more recently, Wang rejected a second claim from another ChatGPT user, and that order went into greater detail, revealing how the judge is considering opposition to the order ahead of oral arguments this week, which were urgently requested by OpenAI.&lt;/p&gt;
&lt;p&gt;The second request to intervene came from a ChatGPT user named Aidan Hunt, who said that he uses ChatGPT "from time to time," occasionally sending OpenAI "highly sensitive personal and commercial information in the course of using the service."&lt;/p&gt;
&lt;p&gt;In his filing, Hunt alleged that Wang's preservation order created a "nationwide mass surveillance program" affecting and potentially harming "all ChatGPT users," who received no warning that their deleted and anonymous chats were suddenly being retained. He warned that the order limiting retention to just ChatGPT outputs carried the same risks as including user inputs, since outputs "inherently reveal, and often explicitly restate, the input questions or topics input."&lt;/p&gt;
&lt;p&gt;Hunt claimed that he only learned that ChatGPT was retaining this information—despite policies specifying they would not—by stumbling upon the news in an online forum. Feeling that his Fourth Amendment and due process rights were being infringed, Hunt sought to influence the court's decision and proposed a motion to vacate the order that said Wang's "order effectively requires Defendants to implement a mass surveillance program affecting all ChatGPT users."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Hunt's fears are not unfounded, Corynne McSherry, legal director for the digital rights group the Electronic Frontier Foundation, told Ars.&lt;/p&gt;
&lt;p&gt;"The discovery order poses genuine risks to user privacy in itself and as a precedent for the many other lawsuits around the country," McSherry said. "And it is emblematic of a broader problem: AI chatbots are opening another vector for corporate surveillance, especially if users don't have meaningful control over what happens to their chat histories and records."&lt;/p&gt;
&lt;p&gt;According to Hunt, Wang failed to "consider exempting 'Anonymous Chats,' which are reasonably expected to contain the most sensitive and potentially damaging information of users, from retention and disclosure in this case," claiming that it "constitutes an overly broad and unreasonable action."&lt;/p&gt;
&lt;p&gt;He urged the judge to revise the order to include this exemption, as well as exemptions for any chats "discussing medical, financial, legal, and personal topics that contain deeply private information of users and bear no relevance whatsoever" to the plaintiff news organizations' claimed interests.&lt;/p&gt;
&lt;p&gt;For Hunt and many other users blindsided by the order, the stakes appear high. He suggested that Wang should have allowed him to intervene "because this case involves important, novel constitutional questions about the privacy rights incident to artificial intelligence usage—a rapidly developing area of law—and the ability of a magistrate to institute a nationwide mass surveillance program by means of a discovery order in a civil case."&lt;/p&gt;
&lt;p&gt;But Wang disagreed with Hunt that she exceeded her authority in enforcing the order, emphasizing in a footnote that her order cannot be construed as enabling mass surveillance.&lt;/p&gt;
&lt;p&gt;"Proposed Intervenor does not explain how a court’s document retention order that directs the preservation, segregation, and retention of certain privately held data by a private company for the limited purposes of litigation is, or could be, a 'nationwide mass surveillance program,'" Wang wrote. "It is not. The judiciary is not a law enforcement agency."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;However, McSherry warned that "it's only a matter of time before law enforcement and private litigants start going to OpenAI to try to get chat histories/records about users for all sorts of purposes, just as they do already for search histories, social media posts, etc." Wang's order could become a gateway to that future, she said.&lt;/p&gt;
&lt;p&gt;Wang rejected Hunt's motion primarily because "whether the temporary preservation of certain chat output log data that was routinely being deleted by OpenAI throughout the course of this litigation may infringe on purported constitutional and contractual privacy rights of individual consumers that use ChatGPT" was deemed a "collateral issue" that does not directly pertain to the central question of copyright infringement.&lt;/p&gt;
&lt;p&gt;Finding that Hunt's intervention would not contribute "in any way" to "the development of the underlying factual issues in this case," Wang ruled that Hunt ultimately had no right to intervene.&lt;/p&gt;
&lt;p&gt;"None of Proposed Intervenor’s purported 'novel' questions are at issue in this copyright infringement action," Wang wrote. "Even if the Court were to entertain such questions, they would only work to unduly delay the resolution of the legal questions actually at issue."&lt;/p&gt;
&lt;h2&gt;User questions how hard OpenAI will fight&lt;/h2&gt;
&lt;p&gt;OpenAI will have a chance to defend panicked users on June 26, when Wang hears oral arguments over the ChatGPT maker's concerns about the preservation order.&lt;/p&gt;
&lt;p&gt;In his filing, Hunt explained that among his worst fears is that the order will not be blocked and that chat data will be disclosed to news plaintiffs who may be motivated to publicly disseminate the deleted chats.&lt;/p&gt;
&lt;p&gt;That could happen if news organizations find evidence of deleted chats they say are likely to contain user attempts to generate full news articles. Ars could not immediately reach a spokesperson for the lead plaintiff in the copyright lawsuit, The New York Times, for comment on this alleged privacy risk for ChatGPT users.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Wang suggested that there is no risk at this time since no chat data has yet been disclosed to the news organizations. That could mean that ChatGPT users may have better luck intervening after chat data is shared, should OpenAI's fight to block the order this week fail. But that's likely no comfort to users like Hunt, who worry that OpenAI merely retaining the data—even if it's never shared with news organizations—could cause severe and irreparable harms.&lt;/p&gt;
&lt;p&gt;Some users appear to be questioning how hard OpenAI will fight. In particular, Hunt is worried that OpenAI may not prioritize defending users' privacy if other concerns—like "financial costs of the case, desire for a quick resolution, and avoiding reputational damage"—are deemed more important, his filing said.&lt;/p&gt;
&lt;p&gt;OpenAI did not immediately respond to Ars' request to comment. The company previously provided a breakdown of affected users and vowed to fight the order.&lt;/p&gt;
&lt;p&gt;For now, ChatGPT users must wait to see the fate of their most sensitive chat logs. Intervening ChatGPT users had tried to argue that, at minimum, OpenAI should have been required to directly notify users that their deleted and anonymous chats were being retained. Hunt suggested that it would have stopped him from inputting sensitive data sooner. McSherry told Ars that more transparency will be needed as courts continue tangling with cases impacting chatbot users.&lt;/p&gt;
&lt;p&gt;"All AI chat apps should be taking steps not only to ensure that users can delete their records and be sure they are actually erased but also to ensure that users get timely notice of demands for their information," McSherry said. "If they aren't already doing so, they should also commit to regular transparency reporting about demands for user data."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI will fight order to keep all ChatGPT logs after users fail to sway court.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-640x360.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/GettyImages-2185905668-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Yurii Karvatskyi | iStock / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After a court ordered OpenAI to "indefinitely" retain all ChatGPT logs, including deleted chats, of millions of users, two panicked users tried and failed to intervene. The order sought to preserve potential evidence in a copyright infringement lawsuit raised by news organizations.&lt;/p&gt;
&lt;p&gt;In May, Judge Ona Wang, who drafted the order, rejected the first user's request on behalf of his company simply because the company should have hired a lawyer to draft the filing. But more recently, Wang rejected a second claim from another ChatGPT user, and that order went into greater detail, revealing how the judge is considering opposition to the order ahead of oral arguments this week, which were urgently requested by OpenAI.&lt;/p&gt;
&lt;p&gt;The second request to intervene came from a ChatGPT user named Aidan Hunt, who said that he uses ChatGPT "from time to time," occasionally sending OpenAI "highly sensitive personal and commercial information in the course of using the service."&lt;/p&gt;
&lt;p&gt;In his filing, Hunt alleged that Wang's preservation order created a "nationwide mass surveillance program" affecting and potentially harming "all ChatGPT users," who received no warning that their deleted and anonymous chats were suddenly being retained. He warned that the order limiting retention to just ChatGPT outputs carried the same risks as including user inputs, since outputs "inherently reveal, and often explicitly restate, the input questions or topics input."&lt;/p&gt;
&lt;p&gt;Hunt claimed that he only learned that ChatGPT was retaining this information—despite policies specifying they would not—by stumbling upon the news in an online forum. Feeling that his Fourth Amendment and due process rights were being infringed, Hunt sought to influence the court's decision and proposed a motion to vacate the order that said Wang's "order effectively requires Defendants to implement a mass surveillance program affecting all ChatGPT users."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Hunt's fears are not unfounded, Corynne McSherry, legal director for the digital rights group the Electronic Frontier Foundation, told Ars.&lt;/p&gt;
&lt;p&gt;"The discovery order poses genuine risks to user privacy in itself and as a precedent for the many other lawsuits around the country," McSherry said. "And it is emblematic of a broader problem: AI chatbots are opening another vector for corporate surveillance, especially if users don't have meaningful control over what happens to their chat histories and records."&lt;/p&gt;
&lt;p&gt;According to Hunt, Wang failed to "consider exempting 'Anonymous Chats,' which are reasonably expected to contain the most sensitive and potentially damaging information of users, from retention and disclosure in this case," claiming that it "constitutes an overly broad and unreasonable action."&lt;/p&gt;
&lt;p&gt;He urged the judge to revise the order to include this exemption, as well as exemptions for any chats "discussing medical, financial, legal, and personal topics that contain deeply private information of users and bear no relevance whatsoever" to the plaintiff news organizations' claimed interests.&lt;/p&gt;
&lt;p&gt;For Hunt and many other users blindsided by the order, the stakes appear high. He suggested that Wang should have allowed him to intervene "because this case involves important, novel constitutional questions about the privacy rights incident to artificial intelligence usage—a rapidly developing area of law—and the ability of a magistrate to institute a nationwide mass surveillance program by means of a discovery order in a civil case."&lt;/p&gt;
&lt;p&gt;But Wang disagreed with Hunt that she exceeded her authority in enforcing the order, emphasizing in a footnote that her order cannot be construed as enabling mass surveillance.&lt;/p&gt;
&lt;p&gt;"Proposed Intervenor does not explain how a court’s document retention order that directs the preservation, segregation, and retention of certain privately held data by a private company for the limited purposes of litigation is, or could be, a 'nationwide mass surveillance program,'" Wang wrote. "It is not. The judiciary is not a law enforcement agency."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;However, McSherry warned that "it's only a matter of time before law enforcement and private litigants start going to OpenAI to try to get chat histories/records about users for all sorts of purposes, just as they do already for search histories, social media posts, etc." Wang's order could become a gateway to that future, she said.&lt;/p&gt;
&lt;p&gt;Wang rejected Hunt's motion primarily because "whether the temporary preservation of certain chat output log data that was routinely being deleted by OpenAI throughout the course of this litigation may infringe on purported constitutional and contractual privacy rights of individual consumers that use ChatGPT" was deemed a "collateral issue" that does not directly pertain to the central question of copyright infringement.&lt;/p&gt;
&lt;p&gt;Finding that Hunt's intervention would not contribute "in any way" to "the development of the underlying factual issues in this case," Wang ruled that Hunt ultimately had no right to intervene.&lt;/p&gt;
&lt;p&gt;"None of Proposed Intervenor’s purported 'novel' questions are at issue in this copyright infringement action," Wang wrote. "Even if the Court were to entertain such questions, they would only work to unduly delay the resolution of the legal questions actually at issue."&lt;/p&gt;
&lt;h2&gt;User questions how hard OpenAI will fight&lt;/h2&gt;
&lt;p&gt;OpenAI will have a chance to defend panicked users on June 26, when Wang hears oral arguments over the ChatGPT maker's concerns about the preservation order.&lt;/p&gt;
&lt;p&gt;In his filing, Hunt explained that among his worst fears is that the order will not be blocked and that chat data will be disclosed to news plaintiffs who may be motivated to publicly disseminate the deleted chats.&lt;/p&gt;
&lt;p&gt;That could happen if news organizations find evidence of deleted chats they say are likely to contain user attempts to generate full news articles. Ars could not immediately reach a spokesperson for the lead plaintiff in the copyright lawsuit, The New York Times, for comment on this alleged privacy risk for ChatGPT users.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Wang suggested that there is no risk at this time since no chat data has yet been disclosed to the news organizations. That could mean that ChatGPT users may have better luck intervening after chat data is shared, should OpenAI's fight to block the order this week fail. But that's likely no comfort to users like Hunt, who worry that OpenAI merely retaining the data—even if it's never shared with news organizations—could cause severe and irreparable harms.&lt;/p&gt;
&lt;p&gt;Some users appear to be questioning how hard OpenAI will fight. In particular, Hunt is worried that OpenAI may not prioritize defending users' privacy if other concerns—like "financial costs of the case, desire for a quick resolution, and avoiding reputational damage"—are deemed more important, his filing said.&lt;/p&gt;
&lt;p&gt;OpenAI did not immediately respond to Ars' request to comment. The company previously provided a breakdown of affected users and vowed to fight the order.&lt;/p&gt;
&lt;p&gt;For now, ChatGPT users must wait to see the fate of their most sensitive chat logs. Intervening ChatGPT users had tried to argue that, at minimum, OpenAI should have been required to directly notify users that their deleted and anonymous chats were being retained. Hunt suggested that it would have stopped him from inputting sensitive data sooner. McSherry told Ars that more transparency will be needed as courts continue tangling with cases impacting chatbot users.&lt;/p&gt;
&lt;p&gt;"All AI chat apps should be taking steps not only to ensure that users can delete their records and be sure they are actually erased but also to ensure that users get timely notice of demands for their information," McSherry said. "If they aren't already doing so, they should also commit to regular transparency reporting about demands for user data."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/judge-rejects-claim-that-forcing-openai-to-keep-chatgpt-logs-is-mass-surveillance/</guid><pubDate>Mon, 23 Jun 2025 17:33:11 +0000</pubDate></item><item><title>Over a million people now have access to the gen-AI powered Alexa+ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/over-a-million-people-now-have-access-to-the-gen-ai-powered-alexa/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;More invites to Amazon’s upgraded digital assistant, Alexa+, powered by generative AI, have been steadily rolling out. The service, first announced in February, now reaches over a million users, Amazon confirmed to TechCrunch on Monday. However, Alexa+ is not yet publicly available. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Amazon has been working through its waitlist, sending out invites to those customers who originally signed up to test the service when it became available. Over the past several weeks, many people have shared on social media that they’ve received an invite to try Alexa+, whose service offers more natural and personalized interactions, smart home integration, and expanded capabilities thanks to AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alexa+ is available for free during Early Access and will later be free for Prime customers. Non-Prime users will be able to use the service for $19.99 per month after it publicly launches.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon earlier this year noted that invites to try out the new system would roll out in waves in the months ahead. As of May 2025, Amazon CEO Andy Jassy said that Alexa+ had so far reached over 100,000 users, representing only a tiny fraction of the 600 million Alexa devices that had been sold. That number has grown significantly in the weeks since.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021198" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ represents a serious attempt by Amazon to create a generative AI experience for consumers that it can eventually monetize.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Amazon created a market for smart home-connected, voice-based assistants through its Alexa-powered Echo devices, it wasn’t able to turn that traction into a revenue-generating business. Meanwhile, Alexa lost its shine in more recent years as generative AI services, like ChatGPT, took off. Compared with modern-day AI, Alexa began to feel clunky, constrained, and underpowered.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ aims to bring the digital assistant new capabilities. The service allows users to chat with the digital assistant using more natural language, where you can phrase requests your own way. For instance, you could tell Alexa, “It’s too cold in here,” to have Alexa adjust your smart thermostat. You’ll also more easily be able to create routines, search across your Ring camera footage, interrupt or pivot the conversation with the assistant, and more.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The experience is more personalized, too, as it saves your preferences and remembers what you like, from favorite songs to recipes and beyond.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021199" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With its generative AI component, Alexa can do things like summarize long emails you share with the service, create unique bedtime stories, generate quizzes from study guides, make travel itineraries, provide summaries of your smart home activity, and answer other questions, similar to how an AI chatbot might respond.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the assistant will be able to help you take certain actions — like buying concert tickets, booking a dinner reservation, and notifying you when something you’ve been watching goes on sale, among other things. Initial partners on this feature include OpenTable, Ticketmaster, Uber Eats, Tripadvisor, Grubhub, Yelp, Priceline, Viator, Thumbtack, Atom, Fodor’s, and others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While in early access, Alexa+ will initially be available on Echo Show 8, 10, 15, or 21 devices in the U.S. Over time, it will expand to more Echo customers, Fire TV users, and Fire tablet users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers who have been invited to try the product are reporting mixed results, with some praising the service, noting it’s more advanced than Siri, while others said it’s still rough around the edges. (Some early reviews agree with the latter.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth pointing out that Alexa+ is not currently fully launched, but it’s getting close. Amazon says nearly 90% of the features it previously announced have since shipped.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;More invites to Amazon’s upgraded digital assistant, Alexa+, powered by generative AI, have been steadily rolling out. The service, first announced in February, now reaches over a million users, Amazon confirmed to TechCrunch on Monday. However, Alexa+ is not yet publicly available. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Instead, Amazon has been working through its waitlist, sending out invites to those customers who originally signed up to test the service when it became available. Over the past several weeks, many people have shared on social media that they’ve received an invite to try Alexa+, whose service offers more natural and personalized interactions, smart home integration, and expanded capabilities thanks to AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alexa+ is available for free during Early Access and will later be free for Prime customers. Non-Prime users will be able to use the service for $19.99 per month after it publicly launches.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon earlier this year noted that invites to try out the new system would roll out in waves in the months ahead. As of May 2025, Amazon CEO Andy Jassy said that Alexa+ had so far reached over 100,000 users, representing only a tiny fraction of the 600 million Alexa devices that had been sold. That number has grown significantly in the weeks since.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021198" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ represents a serious attempt by Amazon to create a generative AI experience for consumers that it can eventually monetize.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Though Amazon created a market for smart home-connected, voice-based assistants through its Alexa-powered Echo devices, it wasn’t able to turn that traction into a revenue-generating business. Meanwhile, Alexa lost its shine in more recent years as generative AI services, like ChatGPT, took off. Compared with modern-day AI, Alexa began to feel clunky, constrained, and underpowered.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Alexa+ aims to bring the digital assistant new capabilities. The service allows users to chat with the digital assistant using more natural language, where you can phrase requests your own way. For instance, you could tell Alexa, “It’s too cold in here,” to have Alexa adjust your smart thermostat. You’ll also more easily be able to create routines, search across your Ring camera footage, interrupt or pivot the conversation with the assistant, and more.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;The experience is more personalized, too, as it saves your preferences and remembers what you like, from favorite songs to recipes and beyond.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3021199" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/download.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Amazon&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;With its generative AI component, Alexa can do things like summarize long emails you share with the service, create unique bedtime stories, generate quizzes from study guides, make travel itineraries, provide summaries of your smart home activity, and answer other questions, similar to how an AI chatbot might respond.  &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the assistant will be able to help you take certain actions — like buying concert tickets, booking a dinner reservation, and notifying you when something you’ve been watching goes on sale, among other things. Initial partners on this feature include OpenTable, Ticketmaster, Uber Eats, Tripadvisor, Grubhub, Yelp, Priceline, Viator, Thumbtack, Atom, Fodor’s, and others.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While in early access, Alexa+ will initially be available on Echo Show 8, 10, 15, or 21 devices in the U.S. Over time, it will expand to more Echo customers, Fire TV users, and Fire tablet users.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Customers who have been invited to try the product are reporting mixed results, with some praising the service, noting it’s more advanced than Siri, while others said it’s still rough around the edges. (Some early reviews agree with the latter.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth pointing out that Alexa+ is not currently fully launched, but it’s getting close. Amazon says nearly 90% of the features it previously announced have since shipped.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/over-a-million-people-now-have-access-to-the-gen-ai-powered-alexa/</guid><pubDate>Mon, 23 Jun 2025 17:38:40 +0000</pubDate></item><item><title>Four months after a $3B valuation, Harvey AI grows to $5B (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/four-months-after-a-3b-valuation-harvey-ai-grows-to-5b/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1474442258.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Harvey AI, a startup that provides automation for legal work, has raised $300 million in Series E funding at a $5 billion valuation, the company told Fortune.&amp;nbsp;The round was co-led by Kleiner Perkins and Coatue, with participation from existing investors, including Conviction, Elad Gil, OpenAI Startup Fund, and Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing comes just four months after Harvey announced that Sequoia led a $300 million Series D round at a $3 billion valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While many AI companies aim to keep headcount low, Harvey is rapidly expanding its staff. The 3-year-old startup already employs 340 people and plans to double that number with its fresh funds, Fortune reported. Some of the new staff will be hired to help Harvey build AI products for professional services beyond legal, including tax accounting. The company’s AI solutions, which assist lawyers in reviewing documents and drafting contracts, are used by 337 legal clients.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harvey has been growing at a rapid clip, reaching an annualized run-rate revenue of $75 million in April, up from $50 million earlier in the year, Reuters reported last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of Harvey’s competitors include older legal startups, such as 10-year-old Ironclad and 17-year-old Clio, which raised a $300 million round at a $3 billion valuation last year.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1474442258.jpg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Harvey AI, a startup that provides automation for legal work, has raised $300 million in Series E funding at a $5 billion valuation, the company told Fortune.&amp;nbsp;The round was co-led by Kleiner Perkins and Coatue, with participation from existing investors, including Conviction, Elad Gil, OpenAI Startup Fund, and Sequoia.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The financing comes just four months after Harvey announced that Sequoia led a $300 million Series D round at a $3 billion valuation.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While many AI companies aim to keep headcount low, Harvey is rapidly expanding its staff. The 3-year-old startup already employs 340 people and plans to double that number with its fresh funds, Fortune reported. Some of the new staff will be hired to help Harvey build AI products for professional services beyond legal, including tax accounting. The company’s AI solutions, which assist lawyers in reviewing documents and drafting contracts, are used by 337 legal clients.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Harvey has been growing at a rapid clip, reaching an annualized run-rate revenue of $75 million in April, up from $50 million earlier in the year, Reuters reported last month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of Harvey’s competitors include older legal startups, such as 10-year-old Ironclad and 17-year-old Clio, which raised a $300 million round at a $3 billion valuation last year.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/four-months-after-a-3b-valuation-harvey-ai-grows-to-5b/</guid><pubDate>Mon, 23 Jun 2025 18:03:00 +0000</pubDate></item><item><title>Leak reveals Grok might soon edit your spreadsheets (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/leak-reveals-grok-might-soon-edit-your-spreadsheets/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/xAI-Grok-GettyImages-1765893916.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Leaked code suggests xAI is developing an advanced file editor for Grok with spreadsheet support, signaling the company’s push to compete with OpenAI, Google, and Microsoft by embedding AI copilots into productivity tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can talk to Grok and ask it to assist you at the same time you’re editing the files!” writes reverse engineer Nima Owji, who leaked the finding.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;BREAKING: xAI is working on an advanced FILE EDITOR for GROK!&lt;/p&gt;&lt;p&gt;It even supports SPREADSHEETS!&lt;/p&gt;&lt;p&gt;You can talk to Grok and ask it to assist you at the same time you're editing the files! pic.twitter.com/9vIKRZj6Wn&lt;/p&gt;— Nima Owji (@nima_owji) June 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI to confirm the findings and learn more.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While xAI hasn’t explicitly detailed its strategy for pursuing interactive, multimodal AI workspaces, it has dropped a series of announcements that point to how the company is thinking about these tools. In April 2025, xAI launched Grok Studio, a split-screen workspace that lets users collaborate with Grok on generating documents, code, reports, and browser games. It also launched the ability to create Workspaces that let you organize files and conversations in a single place.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI and Microsoft have similar tools, Google’s Gemini Workspace for Sheets, Docs, and Gmail appears to be the most similar to what xAI is reportedly building. Google’s tools can edit Docs and Sheets and allow you to chat with Gemini while looking at or editing documents. The difference is that Gemini Workspace only works within Google’s ecosystem.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not clear what types of files xAI’s editor might support aside from spreadsheets, or whether xAI plans to build a full productivity suite that could compete with Google Workspace or Microsoft 365.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Owji’s findings are true, the advanced editor would be a step toward Elon Musk’s ambitions to turn X into an “everything app” that includes docs, chat, payments, and social media.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/xAI-Grok-GettyImages-1765893916.jpeg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Leaked code suggests xAI is developing an advanced file editor for Grok with spreadsheet support, signaling the company’s push to compete with OpenAI, Google, and Microsoft by embedding AI copilots into productivity tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You can talk to Grok and ask it to assist you at the same time you’re editing the files!” writes reverse engineer Nima Owji, who leaked the finding.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;BREAKING: xAI is working on an advanced FILE EDITOR for GROK!&lt;/p&gt;&lt;p&gt;It even supports SPREADSHEETS!&lt;/p&gt;&lt;p&gt;You can talk to Grok and ask it to assist you at the same time you're editing the files! pic.twitter.com/9vIKRZj6Wn&lt;/p&gt;— Nima Owji (@nima_owji) June 22, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to xAI to confirm the findings and learn more.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;While xAI hasn’t explicitly detailed its strategy for pursuing interactive, multimodal AI workspaces, it has dropped a series of announcements that point to how the company is thinking about these tools. In April 2025, xAI launched Grok Studio, a split-screen workspace that lets users collaborate with Grok on generating documents, code, reports, and browser games. It also launched the ability to create Workspaces that let you organize files and conversations in a single place.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI and Microsoft have similar tools, Google’s Gemini Workspace for Sheets, Docs, and Gmail appears to be the most similar to what xAI is reportedly building. Google’s tools can edit Docs and Sheets and allow you to chat with Gemini while looking at or editing documents. The difference is that Gemini Workspace only works within Google’s ecosystem.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s not clear what types of files xAI’s editor might support aside from spreadsheets, or whether xAI plans to build a full productivity suite that could compete with Google Workspace or Microsoft 365.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If Owji’s findings are true, the advanced editor would be a step toward Elon Musk’s ambitions to turn X into an “everything app” that includes docs, chat, payments, and social media.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/leak-reveals-grok-might-soon-edit-your-spreadsheets/</guid><pubDate>Mon, 23 Jun 2025 18:16:57 +0000</pubDate></item><item><title>[NEW] Ted Cruz can’t get all Republicans to back his fight against state AI laws (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/ted-cruz-cant-get-all-republicans-to-back-his-fight-against-state-ai-laws/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Cruz plan moves ahead but was reportedly watered down amid Republican opposition.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Sen. Ted Cruz holds up a hand and speaks while presiding over a Senate subcommittee hearing." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ted-cruz-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Sen. Ted Cruz holds up a hand and speaks while presiding over a Senate subcommittee hearing." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ted-cruz-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Sen. Ted Cruz (R-Texas) presides over a subcommittee hearing on June 3, 2025 in Washington, DC. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | Chip Somodevilla

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;A Republican proposal to penalize states that regulate artificial intelligence can move forward without requiring approval from 60 senators, the Senate parliamentarian decided on Saturday. But the moratorium on state AI laws did not have unanimous Republican support and has reportedly been watered down in an effort to push it toward passage.&lt;/p&gt;
&lt;p&gt;In early June, Sen. Ted Cruz (R-Texas) proposed enforcing a 10-year moratorium on AI regulation by making states ineligible for broadband funding if they try to impose any limits on development of artificial intelligence. While the House previously approved a version of the so-called "One Big Beautiful Bill" with an outright 10-year ban on state AI regulation, Cruz took a different approach because of the Senate rule that limits inclusion of "extraneous matter" in budget reconciliation legislation.&lt;/p&gt;
&lt;p&gt;Under the Senate's Byrd rule, a senator can object to a potentially extraneous budget provision. A motion to waive the Byrd rule requires a vote of 60 percent of the Senate.&lt;/p&gt;
&lt;p&gt;As originally drafted, Cruz's backdoor ban on state AI laws would have made it impossible for states to receive money from the $42 billion Broadband Equity, Access, and Deployment (BEAD) program if they try to regulate AI. He tied the provision into the budget bill by proposing an extra $500 million for the broadband-deployment grant program and expanding its purpose to also subsidize construction and deployment of infrastructure for artificial intelligence systems.&lt;/p&gt;
&lt;p&gt;Punchbowl News reported today that Cruz made changes in order to gain more Republican support and comply with Senate procedural rules. Cruz was quoted as saying that under his current version, states that regulate AI would only be shut out of the $500 million AI fund.&lt;/p&gt;
&lt;p&gt;This would seem to protect states' access to the $42 billion broadband deployment fund that will offer subsidies to ISPs that expand access to Internet service. Losing that funding would be a major blow to states that have spent the last couple of years developing plans to connect more of their residents to modern broadband. The latest Senate bill text was not available today. We contacted Cruz's office and will update this article if we get a response.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;A spokesperson for Sen. Maria Cantwell (D-Wash.) told Ars today that Cruz's latest version could still prevent states from getting broadband funding. The text has "a backdoor to apply new AI requirements to the entire $42.45 billion program, not just the new $500 million," Cantwell's representative said.&lt;/p&gt;
&lt;h2&gt;Plan has opponents from both parties&lt;/h2&gt;
&lt;p&gt;Senate Parliamentarian Elizabeth MacDonough ruled that several parts of the Republican budget bill are subject to the Byrd rule and its 60-vote requirement, but Cruz's AI proposal wasn't one of them. A press release from Senate Budget Committee Ranking Member Jeff Merkley (D-Ore.) noted that "the parliamentarian's advice is based on whether a provision is appropriate for reconciliation and conforms to the limitations of the Byrd rule; it is not a judgement on the relative merits of a particular policy."&lt;/p&gt;
&lt;p&gt;Surviving the parliamentarian review doesn't guarantee passage. A Bloomberg article said the parliamentarian's decision is "a win for tech companies pushing to stall and override dozens of AI safety laws across the country," but that the "provision will likely still be challenged on the Senate floor, where stripping the provision would need just a simple majority. Some Republicans in both the House and Senate have pushed back on the AI provision."&lt;/p&gt;
&lt;p&gt;Republicans have a 53–47 edge in the Senate. Cantwell and Sen. Marsha Blackburn (R-Tenn.) teamed up for a press conference last week in which they spoke out against the proposed moratorium on state regulation.&lt;/p&gt;
&lt;p&gt;Cantwell said that 24 states last year started "regulating AI in some way, and they have adopted these laws that fill a gap while we are waiting for federal action. Now Congress is threatening these laws, which will leave hundreds of millions of Americans vulnerable to AI harm by abolishing those state law protections."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Blackburn said she agreed with Cantwell that the AI regulation proposal "is not the type of thing that we put into reconciliation bills." Blackburn added that lawmakers "are working to move forward with legislation at the federal level, but we do not need a moratorium that would prohibit our states from stepping up and protecting citizens in their state."&lt;/p&gt;
&lt;p&gt;Sens. Ron Johnson (R-Wis.) and Josh Hawley (R-Mo.) have also criticized the idea of stopping states from regulating AI.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Cruz accused states of “strangling AI”&lt;/h2&gt;
&lt;p&gt;Cruz argued that his proposal stops states "from strangling AI deployment with EU-style regulation." Under his first proposal, no BEAD funds were to be given to any state or territory that enforces "any law or regulation... limiting, restricting, or otherwise regulating artificial intelligence models, artificial intelligence systems, or automated decision systems entered into interstate commerce."&lt;/p&gt;
&lt;p&gt;The Cantwell/Blackburn press conference also included Washington Attorney General Nick Brown, a Democrat; and Tennessee Attorney General Jonathan Skrmetti, a Republican. Brown said that "Washington has a law that prohibits deep fakes being used against political candidates by mimicking their appearance and their speech," another "that prohibits sharing fabricated sexual images without consent and provides for penalties for those who possess and distribute such images," and a third "that prohibits the knowing distribution of forged digital likenesses that can be used to harm or defraud people."&lt;/p&gt;
&lt;p&gt;"All of those laws, in my reading, would be invalid if this was to pass through Congress, and each of those laws are prohibiting and protecting people here in our state," Brown said.&lt;/p&gt;
&lt;p&gt;Skrmetti said that if the Senate proposal becomes law "there would be arguments out there for the big tech companies that the moratorium does, in fact, preclude any enforcement of any consumer protection laws if there's an AI component to the product that we're looking at."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Other Republican plans fail Byrd rule test&lt;/h2&gt;
&lt;p&gt;Senate Democrats said they are pleased that the parliamentarian ruled that several other parts of the bill are subject to the Byrd rule. "We continue to see Republicans' blatant disregard for the rules of reconciliation when drafting this bill... Democrats plan to challenge every part of this bill that hurts working families and violates this process," Merkley said.&lt;/p&gt;
&lt;p&gt;Merkley's press release said the provisions that are subject to a 60-vote threshold include one that "limits certain grant funding for 'sanctuary cities,' and where the Attorney General disagrees with states' and localities' immigration enforcement," and another that "gives state and local officials the authority to arrest any noncitizen suspected of being in the US unlawfully."&lt;/p&gt;
&lt;p&gt;The Byrd rule also applies to a section that "limits the ability of federal courts to issue preliminary injunctions or temporary restraining orders against the federal government by requiring litigants to post a potentially enormous bond," and another that "limits when the federal government can enter into or enforce settlement agreements that provide for payments to third parties to fully compensate victims, remedy harm, and punish and deter future violations," Merkley's office said.&lt;/p&gt;
&lt;p&gt;The office of Senate Democratic Leader Chuck Schumer (D-N.Y.) said yesterday that the provision requiring litigants to post bonds has been struck from the legislation. "This Senate Republican provision, which was even worse than the similar House-passed version, required a plaintiff seeking an emergency court order, preliminary injunction, or a temporary restraining order against the Trump Administration or the federal government to pay a costly bond up front—essentially making the justice system pay-to-play," Schumer's office said.&lt;/p&gt;
&lt;p&gt;Schumer said that "if enacted, this would have been one of the most brazen power grabs we've seen in American history—an attempt to let a future President Trump ignore court orders with impunity, putting him above the law."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Cruz plan moves ahead but was reportedly watered down amid Republican opposition.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Sen. Ted Cruz holds up a hand and speaks while presiding over a Senate subcommittee hearing." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ted-cruz-640x427.jpg" width="640" /&gt;
                  &lt;img alt="Sen. Ted Cruz holds up a hand and speaks while presiding over a Senate subcommittee hearing." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ted-cruz-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Sen. Ted Cruz (R-Texas) presides over a subcommittee hearing on June 3, 2025 in Washington, DC. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | Chip Somodevilla

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;A Republican proposal to penalize states that regulate artificial intelligence can move forward without requiring approval from 60 senators, the Senate parliamentarian decided on Saturday. But the moratorium on state AI laws did not have unanimous Republican support and has reportedly been watered down in an effort to push it toward passage.&lt;/p&gt;
&lt;p&gt;In early June, Sen. Ted Cruz (R-Texas) proposed enforcing a 10-year moratorium on AI regulation by making states ineligible for broadband funding if they try to impose any limits on development of artificial intelligence. While the House previously approved a version of the so-called "One Big Beautiful Bill" with an outright 10-year ban on state AI regulation, Cruz took a different approach because of the Senate rule that limits inclusion of "extraneous matter" in budget reconciliation legislation.&lt;/p&gt;
&lt;p&gt;Under the Senate's Byrd rule, a senator can object to a potentially extraneous budget provision. A motion to waive the Byrd rule requires a vote of 60 percent of the Senate.&lt;/p&gt;
&lt;p&gt;As originally drafted, Cruz's backdoor ban on state AI laws would have made it impossible for states to receive money from the $42 billion Broadband Equity, Access, and Deployment (BEAD) program if they try to regulate AI. He tied the provision into the budget bill by proposing an extra $500 million for the broadband-deployment grant program and expanding its purpose to also subsidize construction and deployment of infrastructure for artificial intelligence systems.&lt;/p&gt;
&lt;p&gt;Punchbowl News reported today that Cruz made changes in order to gain more Republican support and comply with Senate procedural rules. Cruz was quoted as saying that under his current version, states that regulate AI would only be shut out of the $500 million AI fund.&lt;/p&gt;
&lt;p&gt;This would seem to protect states' access to the $42 billion broadband deployment fund that will offer subsidies to ISPs that expand access to Internet service. Losing that funding would be a major blow to states that have spent the last couple of years developing plans to connect more of their residents to modern broadband. The latest Senate bill text was not available today. We contacted Cruz's office and will update this article if we get a response.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;A spokesperson for Sen. Maria Cantwell (D-Wash.) told Ars today that Cruz's latest version could still prevent states from getting broadband funding. The text has "a backdoor to apply new AI requirements to the entire $42.45 billion program, not just the new $500 million," Cantwell's representative said.&lt;/p&gt;
&lt;h2&gt;Plan has opponents from both parties&lt;/h2&gt;
&lt;p&gt;Senate Parliamentarian Elizabeth MacDonough ruled that several parts of the Republican budget bill are subject to the Byrd rule and its 60-vote requirement, but Cruz's AI proposal wasn't one of them. A press release from Senate Budget Committee Ranking Member Jeff Merkley (D-Ore.) noted that "the parliamentarian's advice is based on whether a provision is appropriate for reconciliation and conforms to the limitations of the Byrd rule; it is not a judgement on the relative merits of a particular policy."&lt;/p&gt;
&lt;p&gt;Surviving the parliamentarian review doesn't guarantee passage. A Bloomberg article said the parliamentarian's decision is "a win for tech companies pushing to stall and override dozens of AI safety laws across the country," but that the "provision will likely still be challenged on the Senate floor, where stripping the provision would need just a simple majority. Some Republicans in both the House and Senate have pushed back on the AI provision."&lt;/p&gt;
&lt;p&gt;Republicans have a 53–47 edge in the Senate. Cantwell and Sen. Marsha Blackburn (R-Tenn.) teamed up for a press conference last week in which they spoke out against the proposed moratorium on state regulation.&lt;/p&gt;
&lt;p&gt;Cantwell said that 24 states last year started "regulating AI in some way, and they have adopted these laws that fill a gap while we are waiting for federal action. Now Congress is threatening these laws, which will leave hundreds of millions of Americans vulnerable to AI harm by abolishing those state law protections."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Blackburn said she agreed with Cantwell that the AI regulation proposal "is not the type of thing that we put into reconciliation bills." Blackburn added that lawmakers "are working to move forward with legislation at the federal level, but we do not need a moratorium that would prohibit our states from stepping up and protecting citizens in their state."&lt;/p&gt;
&lt;p&gt;Sens. Ron Johnson (R-Wis.) and Josh Hawley (R-Mo.) have also criticized the idea of stopping states from regulating AI.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Cruz accused states of “strangling AI”&lt;/h2&gt;
&lt;p&gt;Cruz argued that his proposal stops states "from strangling AI deployment with EU-style regulation." Under his first proposal, no BEAD funds were to be given to any state or territory that enforces "any law or regulation... limiting, restricting, or otherwise regulating artificial intelligence models, artificial intelligence systems, or automated decision systems entered into interstate commerce."&lt;/p&gt;
&lt;p&gt;The Cantwell/Blackburn press conference also included Washington Attorney General Nick Brown, a Democrat; and Tennessee Attorney General Jonathan Skrmetti, a Republican. Brown said that "Washington has a law that prohibits deep fakes being used against political candidates by mimicking their appearance and their speech," another "that prohibits sharing fabricated sexual images without consent and provides for penalties for those who possess and distribute such images," and a third "that prohibits the knowing distribution of forged digital likenesses that can be used to harm or defraud people."&lt;/p&gt;
&lt;p&gt;"All of those laws, in my reading, would be invalid if this was to pass through Congress, and each of those laws are prohibiting and protecting people here in our state," Brown said.&lt;/p&gt;
&lt;p&gt;Skrmetti said that if the Senate proposal becomes law "there would be arguments out there for the big tech companies that the moratorium does, in fact, preclude any enforcement of any consumer protection laws if there's an AI component to the product that we're looking at."&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Other Republican plans fail Byrd rule test&lt;/h2&gt;
&lt;p&gt;Senate Democrats said they are pleased that the parliamentarian ruled that several other parts of the bill are subject to the Byrd rule. "We continue to see Republicans' blatant disregard for the rules of reconciliation when drafting this bill... Democrats plan to challenge every part of this bill that hurts working families and violates this process," Merkley said.&lt;/p&gt;
&lt;p&gt;Merkley's press release said the provisions that are subject to a 60-vote threshold include one that "limits certain grant funding for 'sanctuary cities,' and where the Attorney General disagrees with states' and localities' immigration enforcement," and another that "gives state and local officials the authority to arrest any noncitizen suspected of being in the US unlawfully."&lt;/p&gt;
&lt;p&gt;The Byrd rule also applies to a section that "limits the ability of federal courts to issue preliminary injunctions or temporary restraining orders against the federal government by requiring litigants to post a potentially enormous bond," and another that "limits when the federal government can enter into or enforce settlement agreements that provide for payments to third parties to fully compensate victims, remedy harm, and punish and deter future violations," Merkley's office said.&lt;/p&gt;
&lt;p&gt;The office of Senate Democratic Leader Chuck Schumer (D-N.Y.) said yesterday that the provision requiring litigants to post bonds has been struck from the legislation. "This Senate Republican provision, which was even worse than the similar House-passed version, required a plaintiff seeking an emergency court order, preliminary injunction, or a temporary restraining order against the Trump Administration or the federal government to pay a costly bond up front—essentially making the justice system pay-to-play," Schumer's office said.&lt;/p&gt;
&lt;p&gt;Schumer said that "if enacted, this would have been one of the most brazen power grabs we've seen in American history—an attempt to let a future President Trump ignore court orders with impunity, putting him above the law."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/ted-cruz-cant-get-all-republicans-to-back-his-fight-against-state-ai-laws/</guid><pubDate>Mon, 23 Jun 2025 20:02:12 +0000</pubDate></item><item><title>[NEW] Unlocking rich genetic insights through multimodal AI with M-REGLE (The latest research from Google)</title><link>https://research.google/blog/unlocking-rich-genetic-insights-through-multimodal-ai-with-m-regle/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Everything from medical specialists with cutting-edge technology to simple smartwatches are generating data on an unprecedented scale. The aggregation of electronic health records, medical imaging, diagnostic tests, genomic data, and even real-time measurements from smartwatches creates a wealth of data for researchers and clinicians to analyze. These diverse data streams often carry unique and overlapping signals, even within the same organ system.&lt;/p&gt;&lt;p&gt;In the cardiovascular system, for example, an electrocardiogram (ECG) measures the heart's electrical activity, while a photoplethysmogram (PPG) — common in smartwatches — tracks blood volume changes. The co-analysis of these modalities can simultaneously assess both the heart’s electrical system and its pumping efficiency, thus providing a more complete picture of heart health. Integrating these physiological signatures with genetic information from large nation-level biobanks could enable the identification of the genetic underpinnings of disease.&lt;/p&gt;&lt;p&gt;Our earlier work, REGLE, was successful for genetic discovery using health data, but it was designed for a single data type (i.e., the unimodal setting). Alternatively, analyzing each modality separately and then trying to piece together the findings later (what we refer to as U-REGLE or Unimodal REGLE) also might not be the most efficient way. U-REGLE could miss subtle shared information between different modalities. Instead, we hypothesized that &lt;i&gt;jointly&lt;/i&gt; modeling these complementary data streams would boost the important biological signals, reduce noise, and lead to more powerful genetic discoveries.&lt;/p&gt;&lt;p&gt;Here we present our recent paper, “Utilizing multimodal AI to improve genetic analyses of cardiovascular traits”, which we published in the &lt;i&gt;American Journal of Human Genetics&lt;/i&gt;. We developed a multimodal version of REGLE, called M-REGLE, that allows the analysis of multiple types of clinical data together at once. M-REGLE produces lower reconstruction error, identifies more genetic associations, and outperforms risk scores in predicting cardiac disease compared to its predecessor, U-REGLE.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Everything from medical specialists with cutting-edge technology to simple smartwatches are generating data on an unprecedented scale. The aggregation of electronic health records, medical imaging, diagnostic tests, genomic data, and even real-time measurements from smartwatches creates a wealth of data for researchers and clinicians to analyze. These diverse data streams often carry unique and overlapping signals, even within the same organ system.&lt;/p&gt;&lt;p&gt;In the cardiovascular system, for example, an electrocardiogram (ECG) measures the heart's electrical activity, while a photoplethysmogram (PPG) — common in smartwatches — tracks blood volume changes. The co-analysis of these modalities can simultaneously assess both the heart’s electrical system and its pumping efficiency, thus providing a more complete picture of heart health. Integrating these physiological signatures with genetic information from large nation-level biobanks could enable the identification of the genetic underpinnings of disease.&lt;/p&gt;&lt;p&gt;Our earlier work, REGLE, was successful for genetic discovery using health data, but it was designed for a single data type (i.e., the unimodal setting). Alternatively, analyzing each modality separately and then trying to piece together the findings later (what we refer to as U-REGLE or Unimodal REGLE) also might not be the most efficient way. U-REGLE could miss subtle shared information between different modalities. Instead, we hypothesized that &lt;i&gt;jointly&lt;/i&gt; modeling these complementary data streams would boost the important biological signals, reduce noise, and lead to more powerful genetic discoveries.&lt;/p&gt;&lt;p&gt;Here we present our recent paper, “Utilizing multimodal AI to improve genetic analyses of cardiovascular traits”, which we published in the &lt;i&gt;American Journal of Human Genetics&lt;/i&gt;. We developed a multimodal version of REGLE, called M-REGLE, that allows the analysis of multiple types of clinical data together at once. M-REGLE produces lower reconstruction error, identifies more genetic associations, and outperforms risk scores in predicting cardiac disease compared to its predecessor, U-REGLE.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/unlocking-rich-genetic-insights-through-multimodal-ai-with-m-regle/</guid><pubDate>Mon, 23 Jun 2025 20:30:00 +0000</pubDate></item><item><title>[NEW] Databricks, Perplexity co-founder pledges $100M on new fund for AI researchers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/databricks-perplexity-co-founder-pledges-100m-on-new-fund-for-ai-researchers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GettyImages-1211462615.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Andy Konwinski, computer scientist and co-founder of Databricks and Perplexity, announced Monday that his company, Laude, is forming a new AI research institute backed with $100 million of his own money.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Laude Institute functions less an AI research lab and more like a fund looking to make investments structured similar to grants. In addition to Konwinski, the institute’s board includes UC Berkeley professor Dave Patterson (known for a string of award-winning research), Jeff Dean (known as Google’s chief scientist), and Joelle Pineau (Meta’s vice president of AI Research).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Konwinski announced the institute’s first and “flagship” grant of $3 million annually for five years, which will anchor the new AI Systems Lab at UC Berkeley. This is a new lab led by one of Berkeley’s most celebrated researchers, Ion Stoica, current director of the Sky Computing Lab. Stoica is also a co-founder of startup Anyscale (an AI and Python platform) and AI big data company Databricks, both built from tech developed in Berkeley’s lab system.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI Systems Lab is set to open in 2027 and in addition to Stoica will include a number of other well-known researchers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his blog post announcing the institute, Konwinski described its mission as ”built by and for computer science researchers … We exist to catalyze work that doesn’t just push the field forward but guides it towards more beneficial outcomes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s not necessarily a direct dig at OpenAI, which started out as an AI research facility and is now, arguably, consumed by its enormous commercial side. But other researchers have fallen prey to the lure of money as well.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, popular AI researcher Epoch faced controversy when it revealed that OpenAI supported the creation of one of its AI benchmarks that was then used&amp;nbsp;to unveil its new o3 model. Epoch’s founder also launched a startup with the controversial mission to replace all human workers everywhere with AI agents.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Like other AI research organizations with commercial ambitions, Konwinski has structured his institute across boundaries: as a nonprofit with a public benefit corporation operating arm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s dividing his research investments into two buckets that he calls “Slingshots and Moonshots.” Slingshots are for early-stage research that can benefit from grants and hands-on help. Moonshots are, as the name implies, for “long-horizon labs tackling species-level challenges like AI for scientific discovery, civic discourse, healthcare, and workforce re-skilling.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His lab has, for instance, collaborated with “terminal-bench,” a Stanford-led benchmark for how well AI agents handle tasks, used by Anthropic.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One thing to note, Konwinski’s endeavors under the name “Laude” aren’t solely a grant-writing research institute. He also co-founded a for-profit venture fund launched in 2024. The fund’s co-founder is former NEA VC Pete Sonsini and a Laude spokesperson tells us it has more than 50 leading researchers as LPs in the fund. As TechCrunch previously reported, Laude led a $12 million investment in AI agent infrastructure startup Arcade. It has quietly backed other startups, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Laude spokesperson tells us that while Konwinski has pledged $100 million, he’s also looking for, and open to, investment from other successful technologists. As to how Konwinski amassed a fortune enough to guarantee $100 million for this new endeavor: Databricks closed a $15.3 billion funding round in January that valued the company at $62 billion. Perplexity last month secured a $14 billion valuation, too.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Does the world really need yet another AI “good for humanity” research or with a murky nonprofit/commercial structure? No, and yes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI research has become increasingly muddled. For instance, AI benchmarks designed to prove that a particular vendor’s model works best have become plentiful these days. (Even Salesforce has its own LLM benchmark for CRMs.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An alliance that includes the likes of Konwinski, Dean, and Stoica supporting truly independent research that could one day turn into independent and human-helpful commerce could be an attractive alternative.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Note: This story was updated to reflect more details shared about the Laude VC fund.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GettyImages-1211462615.jpg?resize=1200,900" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Andy Konwinski, computer scientist and co-founder of Databricks and Perplexity, announced Monday that his company, Laude, is forming a new AI research institute backed with $100 million of his own money.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Laude Institute functions less an AI research lab and more like a fund looking to make investments structured similar to grants. In addition to Konwinski, the institute’s board includes UC Berkeley professor Dave Patterson (known for a string of award-winning research), Jeff Dean (known as Google’s chief scientist), and Joelle Pineau (Meta’s vice president of AI Research).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Konwinski announced the institute’s first and “flagship” grant of $3 million annually for five years, which will anchor the new AI Systems Lab at UC Berkeley. This is a new lab led by one of Berkeley’s most celebrated researchers, Ion Stoica, current director of the Sky Computing Lab. Stoica is also a co-founder of startup Anyscale (an AI and Python platform) and AI big data company Databricks, both built from tech developed in Berkeley’s lab system.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new AI Systems Lab is set to open in 2027 and in addition to Stoica will include a number of other well-known researchers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In his blog post announcing the institute, Konwinski described its mission as ”built by and for computer science researchers … We exist to catalyze work that doesn’t just push the field forward but guides it towards more beneficial outcomes.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s not necessarily a direct dig at OpenAI, which started out as an AI research facility and is now, arguably, consumed by its enormous commercial side. But other researchers have fallen prey to the lure of money as well.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, popular AI researcher Epoch faced controversy when it revealed that OpenAI supported the creation of one of its AI benchmarks that was then used&amp;nbsp;to unveil its new o3 model. Epoch’s founder also launched a startup with the controversial mission to replace all human workers everywhere with AI agents.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Like other AI research organizations with commercial ambitions, Konwinski has structured his institute across boundaries: as a nonprofit with a public benefit corporation operating arm.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s dividing his research investments into two buckets that he calls “Slingshots and Moonshots.” Slingshots are for early-stage research that can benefit from grants and hands-on help. Moonshots are, as the name implies, for “long-horizon labs tackling species-level challenges like AI for scientific discovery, civic discourse, healthcare, and workforce re-skilling.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;His lab has, for instance, collaborated with “terminal-bench,” a Stanford-led benchmark for how well AI agents handle tasks, used by Anthropic.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One thing to note, Konwinski’s endeavors under the name “Laude” aren’t solely a grant-writing research institute. He also co-founded a for-profit venture fund launched in 2024. The fund’s co-founder is former NEA VC Pete Sonsini and a Laude spokesperson tells us it has more than 50 leading researchers as LPs in the fund. As TechCrunch previously reported, Laude led a $12 million investment in AI agent infrastructure startup Arcade. It has quietly backed other startups, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Laude spokesperson tells us that while Konwinski has pledged $100 million, he’s also looking for, and open to, investment from other successful technologists. As to how Konwinski amassed a fortune enough to guarantee $100 million for this new endeavor: Databricks closed a $15.3 billion funding round in January that valued the company at $62 billion. Perplexity last month secured a $14 billion valuation, too.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Does the world really need yet another AI “good for humanity” research or with a murky nonprofit/commercial structure? No, and yes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;AI research has become increasingly muddled. For instance, AI benchmarks designed to prove that a particular vendor’s model works best have become plentiful these days. (Even Salesforce has its own LLM benchmark for CRMs.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An alliance that includes the likes of Konwinski, Dean, and Stoica supporting truly independent research that could one day turn into independent and human-helpful commerce could be an attractive alternative.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Note: This story was updated to reflect more details shared about the Laude VC fund.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/databricks-perplexity-co-founder-pledges-100m-on-new-fund-for-ai-researchers/</guid><pubDate>Mon, 23 Jun 2025 20:30:03 +0000</pubDate></item><item><title>[NEW] Salesforce launches Agentforce 3 with AI agent observability and MCP support (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/salesforce-launches-agentforce-3-with-ai-agent-observability-and-mcp-support/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Salesforce rolled out sweeping enhancements to its AI agent platform Monday, addressing the biggest hurdles enterprises face when deploying digital workers at scale: Knowing what those agents are actually doing and ensuring they can work securely across corporate systems.&lt;/p&gt;



&lt;p&gt;The company’s Agentforce 3 release introduces a comprehensive “Command Center” that gives executives real-time visibility into AI agent performance, plus native support for emerging interoperability standards that allow agents to connect with hundreds of external business tools without the need for custom coding.&lt;/p&gt;



&lt;p&gt;The timing reflects surging enterprise demand for AI agents. According to Salesforce data, AI agent usage has jumped 233% in six months, with more than 8,000 customers signing up to deploy the technology. Early adopters are seeing measurable returns: Engine reduced customer case handling time by 15%, while 1-800Accountant achieved 70% autonomous resolution of administrative chat requests during peak tax season.&lt;/p&gt;



&lt;p&gt;“We have hundreds of live implementations, if not thousands, and they’re running at scale,” Jayesh Govindarajan, EVP of Salesforce AI, said in an exclusive interview with VentureBeat. The company has moved decisively beyond experimental deployments, he noted: “AI agents are no longer experimental. They have really moved deeply into the fabric of the enterprise.”&lt;/p&gt;



&lt;p&gt;Adam Evans, EVP and GM of Salesforce AI, said in a live event on Monday announcing the platform upgrade: “Over the past several months we’ve listened deeply to our customers and continued our rapid pace of technology innovation. The result is Agentforce 3, a major leap forward for our platform that brings greater intelligence, higher performance and more trust and accountability to every Agentforce deployment.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-global-food-giant-pepsico-is-leading-the-enterprise-ai-agent-revolution"&gt;How global food giant PepsiCo is leading the enterprise AI agent revolution&lt;/h2&gt;



&lt;p&gt;Among the companies embracing this technology is PepsiCo, which is deploying Agentforce as part of a broader AI-driven transformation of its global operations. In an exclusive interview with VentureBeat, Athina Kanioura, PepsiCo’s chief strategy and transformation officer, described the deployment as crucial to the company’s evolution in an increasingly complex marketplace.&lt;/p&gt;



&lt;p&gt;“As a longtime partner of Salesforce, we recognized an opportunity to holistically integrate the way we utilize their platforms across our business — especially as the customer landscape evolves, trade becomes more complex and the need to better integrate our data increases,” Kanioura told VentureBeat.&lt;/p&gt;



&lt;p&gt;The food and beverage giant, whose products are consumed over a billion times daily worldwide, sees AI agents as essential for meeting customers “where they are — and in the ways they want to engage with us,” while driving backend efficiency by integrating systems and simplifying processes.&lt;/p&gt;



&lt;p&gt;PepsiCo’s seven-year relationship with Salesforce has positioned the company to move quickly on AI agents. “We were excited about how Agentforce could enhance the day-to-day experience for our field sellers – streamlining workflows and surfacing smarter insights in real time,” Kanioura explained.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-missing-piece-why-enterprise-ai-needs-real-time-monitoring-and-control"&gt;The missing piece: Why enterprise AI needs real-time monitoring and control&lt;/h2&gt;



&lt;p&gt;The Command Center represents Salesforce’s response to a critical gap in the enterprise AI market. While companies have rushed to deploy AI agents for customer service, sales and operational tasks, many lack visibility into how those digital workers are performing or impacting business outcomes.&lt;/p&gt;



&lt;p&gt;Govindarajan described the challenge facing enterprises that have moved beyond pilot programs: “It’s one thing to build an AI agent demo, but when you actually build an agentic system and put it in front of your users, there’s a different standard.” Companies need tools to understand when AI agents are struggling and when to bring humans into the workflow, he explained.&lt;/p&gt;



&lt;p&gt;“Teams can’t see what agents are doing — or evolve them fast enough,” the company acknowledged in its announcement. The new observability platform provides detailed analytics on agent interactions, health monitoring with real-time alerts and AI-powered recommendations for optimization.&lt;/p&gt;



&lt;p&gt;The system addresses what Govindarajan calls “day two problems” – the operational challenges that emerge after initial deployment. “You can have multiple agents for multiple personas, and you need to be able to observe how that’s actually impacting the task that needs to get done at scale,” he said. This includes managing the handoffs between digital agents and human workers when complex decisions or approvals are required.&lt;/p&gt;



&lt;p&gt;The system captures all agent activity in Salesforce’s Data Cloud using the OpenTelemetry standard, enabling integration with existing monitoring tools like Datadog and other enterprise systems. This addresses enterprises’ need to incorporate AI agent oversight into their existing operational workflows.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-open-standards-and-secure-integration-how-ai-agents-connect-across-enterprise-systems"&gt;Open standards and secure integration: How AI agents connect across enterprise systems&lt;/h2&gt;



&lt;p&gt;Perhaps more significant is Salesforce’s embrace of the Model Context Protocol (MCP), an emerging open standard for AI agent interoperability. The platform will include native MCP support, allowing Agentforce agents to connect with any MCP-compliant server without custom development work.&lt;/p&gt;



&lt;p&gt;“There’s generic interoperability, and then there’s what we call enterprise-grade interoperability,” Gary Lerhaupt, VP of product architecture at Salesforce, explained in an exclusive interview with VentureBeat. “If it’s not enterprise grade, it’s like sparkling untrusted interop.” The key difference, he said, lies in governance and control mechanisms that enterprise customers require.&lt;/p&gt;



&lt;p&gt;This capability, working alongside an expanded AgentExchange marketplace, gives enterprises access to pre-built integrations with over 30 partners including Amazon Web Services, Box, Google Cloud, IBM, PayPal and Stripe. Lerhaupt said the company is launching with “north of 20, maybe 25 plus” vetted MCP servers, with partners like PayPal offering invoicing capabilities and Box providing document access through their MCP implementations.&lt;/p&gt;



&lt;p&gt;“In a world full of AI tools, Agentforce stood out not just for its first-of-a-kind technology but how seamlessly it fit into our technology ecosystem, the way we work and our AI strategy, standards and framework,” Kanioura said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-boost-faster-ai-models-and-enhanced-security-for-regulated-industries"&gt;Performance boost: Faster AI models and enhanced security for regulated industries&lt;/h2&gt;



&lt;p&gt;Underlying the new features is what Salesforce calls an enhanced “Atlas” architecture designed for enterprise-grade performance and security. The platform now offers 50% lower latency compared to January 2025, as well as response streaming for real-time user experiences and automatic failover between AI model providers to ensure continuous operation.&lt;/p&gt;



&lt;p&gt;For regulated industries, Salesforce’s approach to hosting AI models directly within its infrastructure addresses critical security concerns. “With Anthropic, the entire stack will be running within Salesforce infrastructure,” Govindarajan explained. “The calls are not going out to OpenAI, and traffic will be running within the Salesforce VPC. For regulated industries, that’s what we’ve been working on.”&lt;/p&gt;



&lt;p&gt;Critically for regulated industries, Salesforce now hosts Anthropic’s Claude models directly within its infrastructure via Amazon Bedrock, keeping sensitive data within the Salesforce security perimeter. The company plans to add Google’s Gemini models later this year, giving enterprises more options for AI model governance.&lt;/p&gt;



&lt;p&gt;The platform also expands global availability to Canada, the UK, India, Japan and Brazil, with support for six additional languages including French, German, Spanish, Italian, Japanese and Portuguese.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-from-zero-to-ai-agent-how-pre-built-industry-actions-speed-enterprise-deployment"&gt;From zero to AI agent: How pre-built industry actions speed enterprise deployment&lt;/h2&gt;



&lt;p&gt;Recognizing that enterprises need faster returns on AI investments, Salesforce has built more than 200 pre-configured industry actions — with more than 100 added this summer alone. These range from patient scheduling in healthcare to advertising proposal generation in media, designed to help companies deploy functional AI agents quickly rather than building from scratch.&lt;/p&gt;



&lt;p&gt;The results demonstrate the platform’s maturity. Beyond 1-800Accountant’s 70% deflection rate during tax season, Govindarajan cited other production deployments: “OpenTable sees 73% of all restaurant web queries handled by agents,” and Grupo Falabella, a Colombian customer service operation using WhatsApp, achieved a 71% reduction in phone call traffic in just three weeks.&lt;/p&gt;



&lt;p&gt;The company also introduced more flexible pricing, including unlimited usage licenses for employee-facing agents and per-action pricing that scales with actual AI work performed rather than simple conversation volume.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-new-digital-workforce-what-enterprise-ai-adoption-means-for-business-operations"&gt;The new digital workforce: What enterprise AI adoption means for business operations&lt;/h2&gt;



&lt;p&gt;As enterprises increasingly view AI agents as digital employees rather than simple automation tools, the stakes for getting deployment right have never been higher. Companies that successfully scale AI agents stand to gain significant competitive advantages, while those that struggle with governance and oversight risk operational disruptions.&lt;/p&gt;



&lt;p&gt;Govindarajan sees fundamental changes in how work gets organized: “New roles are emerging for people who manage a fleet of agents,” he said. “A CIO might ask, ‘I have seven agents running in my enterprise, what’s broadly happening?’ But someone running a specific marketing agent has a different lens on the same problem.”&lt;/p&gt;



&lt;p&gt;Looking ahead, Lerhaupt positioned the current moment as transformational: “You had the personal computer, then the Internet and now it’s multi-agent,” he said. He described the evolution from single-agent deployments as “the multi-agent revolution and the ability to plug agents together to do exceedingly complex new types of work.”&lt;/p&gt;



&lt;p&gt;For PepsiCo, the transformation goes beyond efficiency gains. “AI and technology are reshaping enterprise operations in ways that were once unimaginable,” Kanioura said. “The work we’re doing with Agentforce is one element of PepsiCo’s broader transformation as a connected company, paving the way for a more resilient and adaptive future of work.”&lt;/p&gt;



&lt;p&gt;The competitive landscape is intensifying as major technology companies race to establish AI agent platforms. When asked about competition from Microsoft, Google and Amazon, Govindarajan emphasized Salesforce’s integration advantages: “We are able to track the entire cycle of work within the enterprise ecosystem,” he said. “We can define flows and interactions in the enterprise, and we’ve been open and extensible in bringing in your data, your actions and orchestrating them effectively.”&lt;/p&gt;



&lt;p&gt;The Agentforce 3 platform is generally available now, with several features including hosted Anthropic models and the full Command Center rolling out through August. But perhaps the most telling sign of the technology’s enterprise readiness isn’t in the feature list — it’s in the confidence of companies like PepsiCo to bet their digital transformation on AI agents they can finally see, measure and control.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Salesforce rolled out sweeping enhancements to its AI agent platform Monday, addressing the biggest hurdles enterprises face when deploying digital workers at scale: Knowing what those agents are actually doing and ensuring they can work securely across corporate systems.&lt;/p&gt;



&lt;p&gt;The company’s Agentforce 3 release introduces a comprehensive “Command Center” that gives executives real-time visibility into AI agent performance, plus native support for emerging interoperability standards that allow agents to connect with hundreds of external business tools without the need for custom coding.&lt;/p&gt;



&lt;p&gt;The timing reflects surging enterprise demand for AI agents. According to Salesforce data, AI agent usage has jumped 233% in six months, with more than 8,000 customers signing up to deploy the technology. Early adopters are seeing measurable returns: Engine reduced customer case handling time by 15%, while 1-800Accountant achieved 70% autonomous resolution of administrative chat requests during peak tax season.&lt;/p&gt;



&lt;p&gt;“We have hundreds of live implementations, if not thousands, and they’re running at scale,” Jayesh Govindarajan, EVP of Salesforce AI, said in an exclusive interview with VentureBeat. The company has moved decisively beyond experimental deployments, he noted: “AI agents are no longer experimental. They have really moved deeply into the fabric of the enterprise.”&lt;/p&gt;



&lt;p&gt;Adam Evans, EVP and GM of Salesforce AI, said in a live event on Monday announcing the platform upgrade: “Over the past several months we’ve listened deeply to our customers and continued our rapid pace of technology innovation. The result is Agentforce 3, a major leap forward for our platform that brings greater intelligence, higher performance and more trust and accountability to every Agentforce deployment.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-global-food-giant-pepsico-is-leading-the-enterprise-ai-agent-revolution"&gt;How global food giant PepsiCo is leading the enterprise AI agent revolution&lt;/h2&gt;



&lt;p&gt;Among the companies embracing this technology is PepsiCo, which is deploying Agentforce as part of a broader AI-driven transformation of its global operations. In an exclusive interview with VentureBeat, Athina Kanioura, PepsiCo’s chief strategy and transformation officer, described the deployment as crucial to the company’s evolution in an increasingly complex marketplace.&lt;/p&gt;



&lt;p&gt;“As a longtime partner of Salesforce, we recognized an opportunity to holistically integrate the way we utilize their platforms across our business — especially as the customer landscape evolves, trade becomes more complex and the need to better integrate our data increases,” Kanioura told VentureBeat.&lt;/p&gt;



&lt;p&gt;The food and beverage giant, whose products are consumed over a billion times daily worldwide, sees AI agents as essential for meeting customers “where they are — and in the ways they want to engage with us,” while driving backend efficiency by integrating systems and simplifying processes.&lt;/p&gt;



&lt;p&gt;PepsiCo’s seven-year relationship with Salesforce has positioned the company to move quickly on AI agents. “We were excited about how Agentforce could enhance the day-to-day experience for our field sellers – streamlining workflows and surfacing smarter insights in real time,” Kanioura explained.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-missing-piece-why-enterprise-ai-needs-real-time-monitoring-and-control"&gt;The missing piece: Why enterprise AI needs real-time monitoring and control&lt;/h2&gt;



&lt;p&gt;The Command Center represents Salesforce’s response to a critical gap in the enterprise AI market. While companies have rushed to deploy AI agents for customer service, sales and operational tasks, many lack visibility into how those digital workers are performing or impacting business outcomes.&lt;/p&gt;



&lt;p&gt;Govindarajan described the challenge facing enterprises that have moved beyond pilot programs: “It’s one thing to build an AI agent demo, but when you actually build an agentic system and put it in front of your users, there’s a different standard.” Companies need tools to understand when AI agents are struggling and when to bring humans into the workflow, he explained.&lt;/p&gt;



&lt;p&gt;“Teams can’t see what agents are doing — or evolve them fast enough,” the company acknowledged in its announcement. The new observability platform provides detailed analytics on agent interactions, health monitoring with real-time alerts and AI-powered recommendations for optimization.&lt;/p&gt;



&lt;p&gt;The system addresses what Govindarajan calls “day two problems” – the operational challenges that emerge after initial deployment. “You can have multiple agents for multiple personas, and you need to be able to observe how that’s actually impacting the task that needs to get done at scale,” he said. This includes managing the handoffs between digital agents and human workers when complex decisions or approvals are required.&lt;/p&gt;



&lt;p&gt;The system captures all agent activity in Salesforce’s Data Cloud using the OpenTelemetry standard, enabling integration with existing monitoring tools like Datadog and other enterprise systems. This addresses enterprises’ need to incorporate AI agent oversight into their existing operational workflows.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-open-standards-and-secure-integration-how-ai-agents-connect-across-enterprise-systems"&gt;Open standards and secure integration: How AI agents connect across enterprise systems&lt;/h2&gt;



&lt;p&gt;Perhaps more significant is Salesforce’s embrace of the Model Context Protocol (MCP), an emerging open standard for AI agent interoperability. The platform will include native MCP support, allowing Agentforce agents to connect with any MCP-compliant server without custom development work.&lt;/p&gt;



&lt;p&gt;“There’s generic interoperability, and then there’s what we call enterprise-grade interoperability,” Gary Lerhaupt, VP of product architecture at Salesforce, explained in an exclusive interview with VentureBeat. “If it’s not enterprise grade, it’s like sparkling untrusted interop.” The key difference, he said, lies in governance and control mechanisms that enterprise customers require.&lt;/p&gt;



&lt;p&gt;This capability, working alongside an expanded AgentExchange marketplace, gives enterprises access to pre-built integrations with over 30 partners including Amazon Web Services, Box, Google Cloud, IBM, PayPal and Stripe. Lerhaupt said the company is launching with “north of 20, maybe 25 plus” vetted MCP servers, with partners like PayPal offering invoicing capabilities and Box providing document access through their MCP implementations.&lt;/p&gt;



&lt;p&gt;“In a world full of AI tools, Agentforce stood out not just for its first-of-a-kind technology but how seamlessly it fit into our technology ecosystem, the way we work and our AI strategy, standards and framework,” Kanioura said.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-performance-boost-faster-ai-models-and-enhanced-security-for-regulated-industries"&gt;Performance boost: Faster AI models and enhanced security for regulated industries&lt;/h2&gt;



&lt;p&gt;Underlying the new features is what Salesforce calls an enhanced “Atlas” architecture designed for enterprise-grade performance and security. The platform now offers 50% lower latency compared to January 2025, as well as response streaming for real-time user experiences and automatic failover between AI model providers to ensure continuous operation.&lt;/p&gt;



&lt;p&gt;For regulated industries, Salesforce’s approach to hosting AI models directly within its infrastructure addresses critical security concerns. “With Anthropic, the entire stack will be running within Salesforce infrastructure,” Govindarajan explained. “The calls are not going out to OpenAI, and traffic will be running within the Salesforce VPC. For regulated industries, that’s what we’ve been working on.”&lt;/p&gt;



&lt;p&gt;Critically for regulated industries, Salesforce now hosts Anthropic’s Claude models directly within its infrastructure via Amazon Bedrock, keeping sensitive data within the Salesforce security perimeter. The company plans to add Google’s Gemini models later this year, giving enterprises more options for AI model governance.&lt;/p&gt;



&lt;p&gt;The platform also expands global availability to Canada, the UK, India, Japan and Brazil, with support for six additional languages including French, German, Spanish, Italian, Japanese and Portuguese.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-from-zero-to-ai-agent-how-pre-built-industry-actions-speed-enterprise-deployment"&gt;From zero to AI agent: How pre-built industry actions speed enterprise deployment&lt;/h2&gt;



&lt;p&gt;Recognizing that enterprises need faster returns on AI investments, Salesforce has built more than 200 pre-configured industry actions — with more than 100 added this summer alone. These range from patient scheduling in healthcare to advertising proposal generation in media, designed to help companies deploy functional AI agents quickly rather than building from scratch.&lt;/p&gt;



&lt;p&gt;The results demonstrate the platform’s maturity. Beyond 1-800Accountant’s 70% deflection rate during tax season, Govindarajan cited other production deployments: “OpenTable sees 73% of all restaurant web queries handled by agents,” and Grupo Falabella, a Colombian customer service operation using WhatsApp, achieved a 71% reduction in phone call traffic in just three weeks.&lt;/p&gt;



&lt;p&gt;The company also introduced more flexible pricing, including unlimited usage licenses for employee-facing agents and per-action pricing that scales with actual AI work performed rather than simple conversation volume.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-new-digital-workforce-what-enterprise-ai-adoption-means-for-business-operations"&gt;The new digital workforce: What enterprise AI adoption means for business operations&lt;/h2&gt;



&lt;p&gt;As enterprises increasingly view AI agents as digital employees rather than simple automation tools, the stakes for getting deployment right have never been higher. Companies that successfully scale AI agents stand to gain significant competitive advantages, while those that struggle with governance and oversight risk operational disruptions.&lt;/p&gt;



&lt;p&gt;Govindarajan sees fundamental changes in how work gets organized: “New roles are emerging for people who manage a fleet of agents,” he said. “A CIO might ask, ‘I have seven agents running in my enterprise, what’s broadly happening?’ But someone running a specific marketing agent has a different lens on the same problem.”&lt;/p&gt;



&lt;p&gt;Looking ahead, Lerhaupt positioned the current moment as transformational: “You had the personal computer, then the Internet and now it’s multi-agent,” he said. He described the evolution from single-agent deployments as “the multi-agent revolution and the ability to plug agents together to do exceedingly complex new types of work.”&lt;/p&gt;



&lt;p&gt;For PepsiCo, the transformation goes beyond efficiency gains. “AI and technology are reshaping enterprise operations in ways that were once unimaginable,” Kanioura said. “The work we’re doing with Agentforce is one element of PepsiCo’s broader transformation as a connected company, paving the way for a more resilient and adaptive future of work.”&lt;/p&gt;



&lt;p&gt;The competitive landscape is intensifying as major technology companies race to establish AI agent platforms. When asked about competition from Microsoft, Google and Amazon, Govindarajan emphasized Salesforce’s integration advantages: “We are able to track the entire cycle of work within the enterprise ecosystem,” he said. “We can define flows and interactions in the enterprise, and we’ve been open and extensible in bringing in your data, your actions and orchestrating them effectively.”&lt;/p&gt;



&lt;p&gt;The Agentforce 3 platform is generally available now, with several features including hosted Anthropic models and the full Command Center rolling out through August. But perhaps the most telling sign of the technology’s enterprise readiness isn’t in the feature list — it’s in the confidence of companies like PepsiCo to bet their digital transformation on AI agents they can finally see, measure and control.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/salesforce-launches-agentforce-3-with-ai-agent-observability-and-mcp-support/</guid><pubDate>Mon, 23 Jun 2025 21:03:09 +0000</pubDate></item><item><title>[NEW] Beyond static AI: MIT’s new framework lets models teach themselves (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/beyond-static-ai-mits-new-framework-lets-models-teach-themselves/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers at MIT have developed a framework called Self-Adapting Language Models (SEAL) that enables large language models (LLMs) to continuously learn and adapt by updating their own internal parameters. SEAL teaches an LLM to generate its own training data and update instructions, allowing it to permanently absorb new knowledge and learn new tasks.&lt;/p&gt;



&lt;p&gt;This framework could be useful for enterprise applications, particularly for AI agents that operate in dynamic environments, where they must constantly process new information and adapt their behavior.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-challenge-of-adapting-llms"&gt;The challenge of adapting LLMs&lt;/h2&gt;



&lt;p&gt;While large language models have shown remarkable abilities, adapting them to specific tasks, integrating new information, or mastering novel reasoning skills remains a significant hurdle.&lt;/p&gt;



&lt;p&gt;Currently, when faced with a new task, LLMs typically learn from data “as-is” through methods like finetuning or in-context learning. However, the provided data is not always in an optimal format for the model to learn efficiently. Existing approaches don’t allow the model to develop its own strategies for best transforming and learning from new information.&lt;/p&gt;



&lt;p&gt;“Many enterprise use cases demand more than just factual recall—they require deeper, persistent adaptation,” Jyo Pari, PhD student at MIT and co-author of the paper, told VentureBeat. “For example, a coding assistant might need to internalize a company’s specific software framework, or a customer-facing model might need to learn a user’s unique behavior or preferences over time.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In such cases, temporary retrieval falls short, and the knowledge needs to be “baked into” the model’s weights so that it influences all future responses.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-creating-self-adapting-language-models"&gt;Creating self-adapting language models&lt;/h2&gt;



&lt;p&gt;“As a step towards scalable and efficient adaptation of language models, we propose equipping LLMs with the ability to generate their own training data and finetuning directives for using such data,” the MIT researchers state in their paper.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Overview of SEAL framework (source: arXiv)" class="wp-image-3012752" height="214" src="https://venturebeat.com/wp-content/uploads/2025/06/image_b1ee4c.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Overview of SEAL framework Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The researchers’ solution is SEAL, short for Self-Adapting Language Models. It uses a reinforcement learning (RL) algorithm to train an LLM to generate “self-edits”—natural-language instructions that specify how the model should update its own weights. These self-edits can restructure new information, create synthetic training examples, or even define the technical parameters for the learning process itself.&lt;/p&gt;



&lt;p&gt;Intuitively, SEAL teaches a model how to create its own personalized study guide. Instead of just reading a new document (the raw data), the model learns to rewrite and reformat that information into a style it can more easily absorb and internalize. This process brings together several key areas of AI research, including synthetic data generation, reinforcement learning and test-time training (TTT).&lt;/p&gt;



&lt;p&gt;The framework operates on a two-loop system. In an “inner loop,” the model uses a self-edit to perform a small, temporary update to its weights. In an “outer loop,” the system evaluates whether that update improved the model’s performance on a target task. If it did, the model receives a positive reward, reinforcing its ability to generate that kind of effective self-edit in the future. Over time, the LLM becomes an expert at teaching itself.&lt;/p&gt;



&lt;p&gt;In their study, the researchers used a single model for the entire SEAL framework. However, they also note that this process can be decoupled into a “teacher-student” model. A specialized teacher model could be trained to generate effective self-edits for a separate student model, which would then be updated. This approach could allow for more specialized and efficient adaptation pipelines in enterprise settings.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-seal-in-action"&gt;SEAL in action&lt;/h2&gt;



&lt;p&gt;The researchers tested SEAL in two key domains: knowledge incorporation (the ability to permanently integrate new facts) and few-shot learning (the ability to generalize from a handful of examples).&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="SEAL in knowledge incorporation (source: arXiv)" class="wp-image-3012753" height="152" src="https://venturebeat.com/wp-content/uploads/2025/06/image_d93126.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;SEAL in knowledge incorporation Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;For knowledge incorporation, the goal was to see if the model could answer questions about a text passage without having access to the passage during questioning. Finetuning Llama-3.2-1B on the raw text provided only a marginal improvement over the base model.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, when the SEAL model created “self-edits” by generating several “implications” from a passage and was trained on this synthetic data, its accuracy jumped to 47%. Notably, this outperformed results from using synthetic data generated by the much larger GPT-4.1, suggesting the model learned to create superior training material for itself.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="SEAL in few-shot learning (source: arXiv)" class="wp-image-3012754" height="218" src="https://venturebeat.com/wp-content/uploads/2025/06/image_1e87a7.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;SEAL in few-shot learning Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;For few-shot learning, the researchers tested SEAL on examples from the Abstract Reasoning Corpus (ARC), where the model must solve visual puzzles. In the self-edit phase, the model had to generate the entire adaptation strategy, including which data augmentations and tools to use and what learning rate to apply.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;SEAL achieved a 72.5% success rate, a dramatic improvement over the 20% rate achieved without RL training and the 0% rate of standard in-context learning.&lt;/p&gt;


&lt;div class="wp-block-image"&gt;
&lt;figure class="aligncenter size-full"&gt;&lt;img alt="SEAL (red line) continues to improve across RL cycles (source: arXiv)" class="wp-image-3012755" height="510" src="https://venturebeat.com/wp-content/uploads/2025/06/image_969ef3.png" width="508" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;SEAL (red line) continues to improve across RL cycles Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;h2 class="wp-block-heading" id="h-implications-for-the-enterprise"&gt;Implications for the enterprise&lt;/h2&gt;



&lt;p&gt;Some experts project that the supply of high-quality, human-generated training data could be exhausted in the coming years. Progress may soon depend on “a model’s capacity to generate its own high-utility training signal,” as the researchers put it. They add, “A natural next step is to meta-train a dedicated SEAL synthetic-data generator model that produces fresh pretraining corpora, allowing future models to scale and achieve greater data efficiency without relying on additional human text.”&lt;/p&gt;



&lt;p&gt;For example, the researchers propose that an LLM could ingest complex documents like academic papers or financial reports and autonomously generate thousands of explanations and implications to deepen its understanding.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This iterative loop of self-expression and self-refinement could allow models to keep improving on rare or underrepresented topics even in the absence of additional external supervision,” the researchers explain.&lt;/p&gt;



&lt;p&gt;This capability is especially promising for building AI agents. Agentic systems must incrementally acquire and retain knowledge as they interact with their environment. SEAL provides a mechanism for this. After an interaction, an agent could synthesize a self-edit to trigger a weight update, allowing it to internalize the lessons learned. This enables the agent to evolve over time, improve its performance based on experience, and reduce its reliance on static programming or repeated human guidance.&lt;/p&gt;



&lt;p&gt;“SEAL demonstrates that large language models need not remain static after pretraining,” the researchers write. “By learning to generate their own synthetic self-edit data and to apply it through lightweight weight updates, they can autonomously incorporate new knowledge and adapt to novel tasks.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-limitations-of-seal"&gt;Limitations of SEAL&lt;/h2&gt;



&lt;p&gt;That said, SEAL is not a universal solution. For example, it can suffer from “catastrophic forgetting,” where constant retraining cycles can result in the model learning its earlier knowledge.&lt;/p&gt;



&lt;p&gt;“In our current implementation, we encourage a hybrid approach,” Pari said. “Enterprises should be selective about what knowledge is important enough to integrate permanently.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Factual and evolving data can remain in external memory through RAG, while long-lasting, behavior-shaping knowledge is better suited for weight-level updates via SEAL.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This kind of hybrid memory strategy ensures the right information is persistent without overwhelming the model or introducing unnecessary forgetting,” he said.&lt;/p&gt;



&lt;p&gt;It is also worth noting that SEAL takes a non-trivial amount of time to tune the self-edit examples and train the model. This makes continuous, real-time editing infeasible in most production settings.&lt;/p&gt;



&lt;p&gt;“We envision a more practical deployment model where the system collects data over a period—say, a few hours or a day—and then performs targeted self-edits during scheduled update intervals,” Pari said. “This approach allows enterprises to control the cost of adaptation while still benefiting from SEAL’s ability to internalize new knowledge.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Researchers at MIT have developed a framework called Self-Adapting Language Models (SEAL) that enables large language models (LLMs) to continuously learn and adapt by updating their own internal parameters. SEAL teaches an LLM to generate its own training data and update instructions, allowing it to permanently absorb new knowledge and learn new tasks.&lt;/p&gt;



&lt;p&gt;This framework could be useful for enterprise applications, particularly for AI agents that operate in dynamic environments, where they must constantly process new information and adapt their behavior.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-challenge-of-adapting-llms"&gt;The challenge of adapting LLMs&lt;/h2&gt;



&lt;p&gt;While large language models have shown remarkable abilities, adapting them to specific tasks, integrating new information, or mastering novel reasoning skills remains a significant hurdle.&lt;/p&gt;



&lt;p&gt;Currently, when faced with a new task, LLMs typically learn from data “as-is” through methods like finetuning or in-context learning. However, the provided data is not always in an optimal format for the model to learn efficiently. Existing approaches don’t allow the model to develop its own strategies for best transforming and learning from new information.&lt;/p&gt;



&lt;p&gt;“Many enterprise use cases demand more than just factual recall—they require deeper, persistent adaptation,” Jyo Pari, PhD student at MIT and co-author of the paper, told VentureBeat. “For example, a coding assistant might need to internalize a company’s specific software framework, or a customer-facing model might need to learn a user’s unique behavior or preferences over time.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;In such cases, temporary retrieval falls short, and the knowledge needs to be “baked into” the model’s weights so that it influences all future responses.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-creating-self-adapting-language-models"&gt;Creating self-adapting language models&lt;/h2&gt;



&lt;p&gt;“As a step towards scalable and efficient adaptation of language models, we propose equipping LLMs with the ability to generate their own training data and finetuning directives for using such data,” the MIT researchers state in their paper.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Overview of SEAL framework (source: arXiv)" class="wp-image-3012752" height="214" src="https://venturebeat.com/wp-content/uploads/2025/06/image_b1ee4c.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;Overview of SEAL framework Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The researchers’ solution is SEAL, short for Self-Adapting Language Models. It uses a reinforcement learning (RL) algorithm to train an LLM to generate “self-edits”—natural-language instructions that specify how the model should update its own weights. These self-edits can restructure new information, create synthetic training examples, or even define the technical parameters for the learning process itself.&lt;/p&gt;



&lt;p&gt;Intuitively, SEAL teaches a model how to create its own personalized study guide. Instead of just reading a new document (the raw data), the model learns to rewrite and reformat that information into a style it can more easily absorb and internalize. This process brings together several key areas of AI research, including synthetic data generation, reinforcement learning and test-time training (TTT).&lt;/p&gt;



&lt;p&gt;The framework operates on a two-loop system. In an “inner loop,” the model uses a self-edit to perform a small, temporary update to its weights. In an “outer loop,” the system evaluates whether that update improved the model’s performance on a target task. If it did, the model receives a positive reward, reinforcing its ability to generate that kind of effective self-edit in the future. Over time, the LLM becomes an expert at teaching itself.&lt;/p&gt;



&lt;p&gt;In their study, the researchers used a single model for the entire SEAL framework. However, they also note that this process can be decoupled into a “teacher-student” model. A specialized teacher model could be trained to generate effective self-edits for a separate student model, which would then be updated. This approach could allow for more specialized and efficient adaptation pipelines in enterprise settings.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-seal-in-action"&gt;SEAL in action&lt;/h2&gt;



&lt;p&gt;The researchers tested SEAL in two key domains: knowledge incorporation (the ability to permanently integrate new facts) and few-shot learning (the ability to generalize from a handful of examples).&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="SEAL in knowledge incorporation (source: arXiv)" class="wp-image-3012753" height="152" src="https://venturebeat.com/wp-content/uploads/2025/06/image_d93126.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;SEAL in knowledge incorporation Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;For knowledge incorporation, the goal was to see if the model could answer questions about a text passage without having access to the passage during questioning. Finetuning Llama-3.2-1B on the raw text provided only a marginal improvement over the base model.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;However, when the SEAL model created “self-edits” by generating several “implications” from a passage and was trained on this synthetic data, its accuracy jumped to 47%. Notably, this outperformed results from using synthetic data generated by the much larger GPT-4.1, suggesting the model learned to create superior training material for itself.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="SEAL in few-shot learning (source: arXiv)" class="wp-image-3012754" height="218" src="https://venturebeat.com/wp-content/uploads/2025/06/image_1e87a7.png?w=800" width="800" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;SEAL in few-shot learning Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;For few-shot learning, the researchers tested SEAL on examples from the Abstract Reasoning Corpus (ARC), where the model must solve visual puzzles. In the self-edit phase, the model had to generate the entire adaptation strategy, including which data augmentations and tools to use and what learning rate to apply.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;SEAL achieved a 72.5% success rate, a dramatic improvement over the 20% rate achieved without RL training and the 0% rate of standard in-context learning.&lt;/p&gt;


&lt;div class="wp-block-image"&gt;
&lt;figure class="aligncenter size-full"&gt;&lt;img alt="SEAL (red line) continues to improve across RL cycles (source: arXiv)" class="wp-image-3012755" height="510" src="https://venturebeat.com/wp-content/uploads/2025/06/image_969ef3.png" width="508" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;em&gt;SEAL (red line) continues to improve across RL cycles Source: arXiv&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;


&lt;h2 class="wp-block-heading" id="h-implications-for-the-enterprise"&gt;Implications for the enterprise&lt;/h2&gt;



&lt;p&gt;Some experts project that the supply of high-quality, human-generated training data could be exhausted in the coming years. Progress may soon depend on “a model’s capacity to generate its own high-utility training signal,” as the researchers put it. They add, “A natural next step is to meta-train a dedicated SEAL synthetic-data generator model that produces fresh pretraining corpora, allowing future models to scale and achieve greater data efficiency without relying on additional human text.”&lt;/p&gt;



&lt;p&gt;For example, the researchers propose that an LLM could ingest complex documents like academic papers or financial reports and autonomously generate thousands of explanations and implications to deepen its understanding.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This iterative loop of self-expression and self-refinement could allow models to keep improving on rare or underrepresented topics even in the absence of additional external supervision,” the researchers explain.&lt;/p&gt;



&lt;p&gt;This capability is especially promising for building AI agents. Agentic systems must incrementally acquire and retain knowledge as they interact with their environment. SEAL provides a mechanism for this. After an interaction, an agent could synthesize a self-edit to trigger a weight update, allowing it to internalize the lessons learned. This enables the agent to evolve over time, improve its performance based on experience, and reduce its reliance on static programming or repeated human guidance.&lt;/p&gt;



&lt;p&gt;“SEAL demonstrates that large language models need not remain static after pretraining,” the researchers write. “By learning to generate their own synthetic self-edit data and to apply it through lightweight weight updates, they can autonomously incorporate new knowledge and adapt to novel tasks.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-limitations-of-seal"&gt;Limitations of SEAL&lt;/h2&gt;



&lt;p&gt;That said, SEAL is not a universal solution. For example, it can suffer from “catastrophic forgetting,” where constant retraining cycles can result in the model learning its earlier knowledge.&lt;/p&gt;



&lt;p&gt;“In our current implementation, we encourage a hybrid approach,” Pari said. “Enterprises should be selective about what knowledge is important enough to integrate permanently.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Factual and evolving data can remain in external memory through RAG, while long-lasting, behavior-shaping knowledge is better suited for weight-level updates via SEAL.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“This kind of hybrid memory strategy ensures the right information is persistent without overwhelming the model or introducing unnecessary forgetting,” he said.&lt;/p&gt;



&lt;p&gt;It is also worth noting that SEAL takes a non-trivial amount of time to tune the self-edit examples and train the model. This makes continuous, real-time editing infeasible in most production settings.&lt;/p&gt;



&lt;p&gt;“We envision a more practical deployment model where the system collects data over a period—say, a few hours or a day—and then performs targeted self-edits during scheduled update intervals,” Pari said. “This approach allows enterprises to control the cost of adaptation while still benefiting from SEAL’s ability to internalize new knowledge.”&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/beyond-static-ai-mits-new-framework-lets-models-teach-themselves/</guid><pubDate>Mon, 23 Jun 2025 21:58:44 +0000</pubDate></item><item><title>[NEW] Court filings reveal OpenAI and io’s early work on an AI device (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/23/court-filings-reveal-openai-and-ios-early-work-on-an-ai-device/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Legal filings submitted earlier this month from lawyers representing OpenAI and Jony Ive’s io reveal new details about the companies’ efforts to build a mass-market AI hardware device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The filings are part of a trademark dispute lawsuit filed this month by iyO, a Google-backed hardware startup developing custom-molded earpieces that connect to other devices. Over the weekend, OpenAI pulled promotional materials related to its $6.5 billion acquisition of Jony Ive’s io startup in order to comply with a court order involved in the suit. OpenAI says it’s fighting iyO’s allegations of trademark infringement. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the last year, OpenAI executives and former Apple leaders now working at io have vigorously researched in-ear hardware devices, according to filings submitted in iyO’s lawsuit. In a June 12 filing, lawyers representing OpenAI and io said the companies purchased at least 30 headphone sets from various companies to explore what’s on the market today. In recent months, OpenAI and io executives also met with iyO’s leadership, and demoed their in-ear technology, according to emails revealed in the case.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, OpenAI’s first device in collaboration with io may not be a pair of headphones at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tang Tan, a longtime Apple executive that co-founded io and serves as the startup’s chief hardware officer, claims in a declaration to the court that the prototype OpenAI CEO Sam Altman mentioned in io’s launch video “is not an in-ear device, nor a wearable device.” Tan notes that the design of said prototype in not yet finalized, and that the product is at least a year away from being advertised or offered for sale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The form factor of OpenAI and io’s first hardware device has largely remained a mystery. Altman merely stated in io’s launch video that the startup was working to create a “family” of AI devices with various capabilities, and Ive said io’s first prototype “completely captured” his imagination.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman had previously told OpenAI’s employees at a meeting that the company’s prototype, when finished, would able to fit in a pocket or sit on a desk, according to the Wall Street Journal. The OpenAI CEO reportedly said the device would be fully aware of a user’s surroundings, and that it would be a “third device” for consumers to use alongside their smartphone and laptop.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Our intent with this collaboration was, and is, to create products that go beyond traditional products and interfaces,” said Altman in a declaration to the court submitted on June 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lawyers representing OpenAI also said in a filing that the company has explored a wide range of devices, including ones that were “desktop-based and mobile, wireless and wired, wearable and portable.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While smart glasses have emerged as the front-runner for AI-enabled devices, with companies like Meta and Google racing to develop the first broadly adopted pair, several companies are also exploring AI-enabled headphones. Apple is reportedly working on a pair of AirPods with cameras, which would help power AI features by gathering information about the surrounding environment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In recent months, OpenAI and io executives have done considerable research into in-ear products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On May 1, OpenAI’s VP of Product, Peter Welinder, and Tan met with iyO’s CEO, Jason Rugolo, to learn more about iyO’s in-ear product, according to an emailed invitation revealed in the case. The meeting took place at io’s office in Jackson Square, the San Francisco neighborhood where Ive has bought several buildings to work on LoveFrom and io.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the meeting, Welinder and Tan tested out iyO’s custom-fit earpiece, but were disappointed when the product failed repeatedly during demonstrations, according to follow-up emails revealed in the case.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tan claims in his declaration that he met with Rugolo as a courtesy to his mentor, longtime Apple executive Steve Zadesky, who recommended he take the meeting. Tan also claims he took several precautions to avoid learning too much about iyO’s IP, such as suggesting that his lawyers review materials before he does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it seemed that OpenAI and io employees thought they could learn something from one of iyO’s partners. To customize its in-ear headsets, iyO sent a specialist from an ear-scanning company, The Ear Project, to someone’s home or office to get a detailed map of someone’s ear.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one email revealed in the case, Marwan Rammah, a former Apple engineer that’s now working at io, told Tan that purchasing a large database of three-dimensional scans from The Ear Project could give the company a “helpful starting point on ergonomics.” It’s unclear if any such deal took place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rugolo tried repeatedly to forge a deeper relationship between iyO, io, and OpenAI — but largely failed, according to the emails. He pitched OpenAI on launching iyO’s device as an early “developer kit” for its final AI device. He pitched OpenAI on investing in iyO and, at one point, even offered to sell his entire company for $200 million, the filings say. However, Tan said in his declaration that he declined these offers.&lt;/p&gt;&lt;p&gt;Evans Hankey, former Apple executive turned io co-founder and chief product officer, said in a declaration to the court that io is not working on a “custom-molded earpiece product.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The ChatGPT-maker seems to be more than a year out from selling its first hardware device, which may not be an in-ear product whatsoever. Given what the company said in this lawsuit, it appears it is also exploring other form factors.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Legal filings submitted earlier this month from lawyers representing OpenAI and Jony Ive’s io reveal new details about the companies’ efforts to build a mass-market AI hardware device.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The filings are part of a trademark dispute lawsuit filed this month by iyO, a Google-backed hardware startup developing custom-molded earpieces that connect to other devices. Over the weekend, OpenAI pulled promotional materials related to its $6.5 billion acquisition of Jony Ive’s io startup in order to comply with a court order involved in the suit. OpenAI says it’s fighting iyO’s allegations of trademark infringement. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For the last year, OpenAI executives and former Apple leaders now working at io have vigorously researched in-ear hardware devices, according to filings submitted in iyO’s lawsuit. In a June 12 filing, lawyers representing OpenAI and io said the companies purchased at least 30 headphone sets from various companies to explore what’s on the market today. In recent months, OpenAI and io executives also met with iyO’s leadership, and demoed their in-ear technology, according to emails revealed in the case.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That said, OpenAI’s first device in collaboration with io may not be a pair of headphones at all.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tang Tan, a longtime Apple executive that co-founded io and serves as the startup’s chief hardware officer, claims in a declaration to the court that the prototype OpenAI CEO Sam Altman mentioned in io’s launch video “is not an in-ear device, nor a wearable device.” Tan notes that the design of said prototype in not yet finalized, and that the product is at least a year away from being advertised or offered for sale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The form factor of OpenAI and io’s first hardware device has largely remained a mystery. Altman merely stated in io’s launch video that the startup was working to create a “family” of AI devices with various capabilities, and Ive said io’s first prototype “completely captured” his imagination.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman had previously told OpenAI’s employees at a meeting that the company’s prototype, when finished, would able to fit in a pocket or sit on a desk, according to the Wall Street Journal. The OpenAI CEO reportedly said the device would be fully aware of a user’s surroundings, and that it would be a “third device” for consumers to use alongside their smartphone and laptop.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Our intent with this collaboration was, and is, to create products that go beyond traditional products and interfaces,” said Altman in a declaration to the court submitted on June 12.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Lawyers representing OpenAI also said in a filing that the company has explored a wide range of devices, including ones that were “desktop-based and mobile, wireless and wired, wearable and portable.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While smart glasses have emerged as the front-runner for AI-enabled devices, with companies like Meta and Google racing to develop the first broadly adopted pair, several companies are also exploring AI-enabled headphones. Apple is reportedly working on a pair of AirPods with cameras, which would help power AI features by gathering information about the surrounding environment.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In recent months, OpenAI and io executives have done considerable research into in-ear products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On May 1, OpenAI’s VP of Product, Peter Welinder, and Tan met with iyO’s CEO, Jason Rugolo, to learn more about iyO’s in-ear product, according to an emailed invitation revealed in the case. The meeting took place at io’s office in Jackson Square, the San Francisco neighborhood where Ive has bought several buildings to work on LoveFrom and io.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the meeting, Welinder and Tan tested out iyO’s custom-fit earpiece, but were disappointed when the product failed repeatedly during demonstrations, according to follow-up emails revealed in the case.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tan claims in his declaration that he met with Rugolo as a courtesy to his mentor, longtime Apple executive Steve Zadesky, who recommended he take the meeting. Tan also claims he took several precautions to avoid learning too much about iyO’s IP, such as suggesting that his lawyers review materials before he does.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it seemed that OpenAI and io employees thought they could learn something from one of iyO’s partners. To customize its in-ear headsets, iyO sent a specialist from an ear-scanning company, The Ear Project, to someone’s home or office to get a detailed map of someone’s ear.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In one email revealed in the case, Marwan Rammah, a former Apple engineer that’s now working at io, told Tan that purchasing a large database of three-dimensional scans from The Ear Project could give the company a “helpful starting point on ergonomics.” It’s unclear if any such deal took place.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rugolo tried repeatedly to forge a deeper relationship between iyO, io, and OpenAI — but largely failed, according to the emails. He pitched OpenAI on launching iyO’s device as an early “developer kit” for its final AI device. He pitched OpenAI on investing in iyO and, at one point, even offered to sell his entire company for $200 million, the filings say. However, Tan said in his declaration that he declined these offers.&lt;/p&gt;&lt;p&gt;Evans Hankey, former Apple executive turned io co-founder and chief product officer, said in a declaration to the court that io is not working on a “custom-molded earpiece product.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The ChatGPT-maker seems to be more than a year out from selling its first hardware device, which may not be an in-ear product whatsoever. Given what the company said in this lawsuit, it appears it is also exploring other form factors.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/23/court-filings-reveal-openai-and-ios-early-work-on-an-ai-device/</guid><pubDate>Mon, 23 Jun 2025 23:44:38 +0000</pubDate></item></channel></rss>