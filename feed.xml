<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 26 Jun 2025 01:49:47 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>Merging AI and underwater photography to reveal hidden ocean worlds (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/lobstger-merging-ai-underwater-photography-to-reveal-hidden-ocean-worlds-0625</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In the Northeastern United States, the Gulf of Maine represents one of the most biologically diverse marine ecosystems on the planet — home to whales, sharks, jellyfish, herring, plankton, and hundreds of other species. But even as this ecosystem supports rich biodiversity, it is undergoing rapid environmental change. The Gulf of Maine is warming faster than 99 percent of the world’s oceans, with consequences that are still unfolding.&lt;/p&gt;&lt;p&gt;A new research initiative developing at MIT Sea Grant, called LOBSTgER — short for Learning Oceanic Bioecological Systems Through Generative Representations — brings together artificial intelligence and underwater photography to document the ocean life left vulnerable to these changes and share them with the public in new visual ways. Co-led by underwater photographer and visiting artist at MIT Sea Grant Keith Ellenbogen and MIT mechanical engineering PhD student Andreas Mentzelopoulos, the project explores how generative AI can expand scientific storytelling by building on field-based photographic data.&lt;/p&gt;&lt;p&gt;Just as the 19th-century camera transformed our ability to document and reveal the natural world — capturing life with unprecedented detail and bringing distant or hidden environments into view — generative AI marks a new frontier in visual storytelling. Like early photography, AI opens a creative and conceptual space, challenging how we define authenticity and how we communicate scientific and artistic perspectives.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In the LOBSTgER project, generative models are trained exclusively on a curated library of Ellenbogen’s original underwater photographs — each image crafted with artistic intent, technical precision, accurate species identification, and clear geographic context. By building a high-quality dataset grounded in real-world observations, the project ensures that the resulting imagery maintains both visual integrity and ecological relevance. In addition, LOBSTgER’s models are built using custom code developed by Mentzelopoulos to protect the process and outputs from any potential biases from external data or models. LOBSTgER’s generative AI builds upon real photography, expanding the researchers’ visual vocabulary to deepen the public’s connection to the natural world.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="A photoreal image of a large oval ocean sunfish underwater. An orange LOBSTgER icon indicates this was made with AI." class="ondemand" height="499" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="751" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            This ocean sunfish (Mola mola) image was generated by LOBSTgER’s unconditional models.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            AI-generated image: Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER.        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;At its heart, LOBSTgER operates at the intersection of art, science, and technology. The project draws from the visual language of photography, the observational rigor of marine science, and the computational power of generative AI. By uniting these disciplines, the team is not only developing new ways to visualize ocean life — they are also reimagining how environmental stories can be told. This integrative approach makes LOBSTgER both a research tool and a creative experiment — one that reflects MIT’s long-standing tradition of interdisciplinary innovation.&lt;/p&gt;&lt;p&gt;Underwater photography in New England’s coastal waters is notoriously difficult. Limited visibility, swirling sediment, bubbles, and the unpredictable movement of marine life all pose constant challenges. For the past several years, Ellenbogen has navigated these challenges and is building a comprehensive record of the region’s biodiversity through the project, Space to Sea: Visualizing New England’s Ocean Wilderness.&amp;nbsp;This large dataset of underwater images provides the foundation for training LOBSTgER’s generative AI models. The images span diverse angles, lighting conditions, and animal behaviors, resulting in a visual archive that is both artistically striking and biologically accurate.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Image synthesis via reverse diffusion: This short video shows the de-noising trajectory from Gaussian latent noise to photorealistic output using LOBSTgER’s unconditional models. Iterative de-noising requires 1,000 forward passes through the trained neural network.&lt;br /&gt;Video: Keith Ellenbogen and Andreas Mentzelopoulos / MIT Sea Grant        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;LOBSTgER’s&amp;nbsp;custom diffusion models are trained to replicate not only the biodiversity Ellenbogen documents, but also the artistic style he uses to capture it. By learning from thousands of real underwater images, the models internalize fine-grained details such as natural lighting gradients, species-specific coloration, and even the atmospheric texture created by suspended particles and refracted sunlight. The result is imagery that not only appears visually accurate, but also feels immersive and moving.&lt;/p&gt;&lt;p&gt;The models can both generate new, synthetic, but scientifically accurate images unconditionally (i.e., requiring no user input/guidance), and enhance real photographs conditionally (i.e., image-to-image generation). By integrating AI into the photographic workflow, Ellenbogen will be able to use these tools to recover detail in turbid water, adjust lighting to emphasize key subjects, or even simulate scenes that would be nearly impossible to capture in the field. The team also believes this approach may benefit other underwater photographers and image editors facing similar challenges. This hybrid method is designed to accelerate the curation process and enable storytellers to construct a more complete and coherent visual narrative of life beneath the surface.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="Side-by-side images of an American lobster on the sea floor underneath seaweed. One has been enhanced by AI and is far more vibrant." class="ondemand" height="298" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="900" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            Left: Enhanced image of an American lobster using LOBSTgER’s image-to-image models. Right: Original image.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            Left: AI genertated image by Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER. Right: Keith Ellenbogen        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In one key series, Ellenbogen captured high-resolution images of lion’s mane jellyfish, blue sharks, American lobsters, and ocean sunfish (&lt;em&gt;Mola mola&lt;/em&gt;) while free diving in coastal waters. “Getting a high-quality dataset is not easy,” Ellenbogen says. “It requires multiple dives, missed opportunities, and unpredictable conditions. But these challenges are part of what makes underwater documentation both difficult and rewarding.”&lt;/p&gt;&lt;p&gt;Mentzelopoulos has developed original code to train a family of latent diffusion models for LOBSTgER grounded on Ellenbogen’s images. Developing such models requires a high level of technical expertise, and training models from scratch is a complex process demanding hundreds of hours of computation and meticulous hyperparameter tuning.&lt;/p&gt;&lt;p&gt;The project reflects a parallel process: field documentation through photography and model development through iterative training. Ellenbogen works in the field, capturing rare and fleeting encounters with marine animals; Mentzelopoulos works in the lab, translating those moments into machine-learning contexts that can extend and reinterpret the visual language of the ocean.&lt;/p&gt;&lt;p&gt;“The goal isn’t to replace photography,” Mentzelopoulos says. “It’s to build on and complement it — making the invisible visible, and helping people see environmental complexity in a way that resonates both emotionally and intellectually. Our models aim to capture not just biological realism, but the emotional charge that can drive real-world engagement and action.”&lt;/p&gt;&lt;p&gt;LOBSTgER points to a hybrid future that merges direct observation with technological interpretation. The team’s long-term goal is to develop a comprehensive model that can visualize a wide range of species found in the Gulf of Maine and, eventually, apply similar methods to marine ecosystems around the world.&lt;/p&gt;&lt;p&gt;The researchers suggest that photography and generative AI form a continuum, rather than a conflict. Photography captures what is — the texture, light, and animal behavior during actual encounters — while AI extends that vision beyond what is seen, toward what could be understood, inferred, or imagined based on scientific data and artistic vision. Together, they offer a powerful framework for communicating science through image-making.&lt;/p&gt;&lt;p&gt;In a region where ecosystems are changing rapidly, the act of visualizing becomes more than just documentation. It becomes a tool for awareness, engagement, and, ultimately, conservation. LOBSTgER is still in its infancy, and the team looks forward to sharing more discoveries, images, and insights as the project evolves.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Answer from the lead image: The left image was generated using using LOBSTgER’s unconditional models and the right image is real.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;For more information, contact Keith Ellenbogen and Andreas Mentzelopoulos.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In the Northeastern United States, the Gulf of Maine represents one of the most biologically diverse marine ecosystems on the planet — home to whales, sharks, jellyfish, herring, plankton, and hundreds of other species. But even as this ecosystem supports rich biodiversity, it is undergoing rapid environmental change. The Gulf of Maine is warming faster than 99 percent of the world’s oceans, with consequences that are still unfolding.&lt;/p&gt;&lt;p&gt;A new research initiative developing at MIT Sea Grant, called LOBSTgER — short for Learning Oceanic Bioecological Systems Through Generative Representations — brings together artificial intelligence and underwater photography to document the ocean life left vulnerable to these changes and share them with the public in new visual ways. Co-led by underwater photographer and visiting artist at MIT Sea Grant Keith Ellenbogen and MIT mechanical engineering PhD student Andreas Mentzelopoulos, the project explores how generative AI can expand scientific storytelling by building on field-based photographic data.&lt;/p&gt;&lt;p&gt;Just as the 19th-century camera transformed our ability to document and reveal the natural world — capturing life with unprecedented detail and bringing distant or hidden environments into view — generative AI marks a new frontier in visual storytelling. Like early photography, AI opens a creative and conceptual space, challenging how we define authenticity and how we communicate scientific and artistic perspectives.&amp;nbsp;&lt;/p&gt;&lt;p&gt;In the LOBSTgER project, generative models are trained exclusively on a curated library of Ellenbogen’s original underwater photographs — each image crafted with artistic intent, technical precision, accurate species identification, and clear geographic context. By building a high-quality dataset grounded in real-world observations, the project ensures that the resulting imagery maintains both visual integrity and ecological relevance. In addition, LOBSTgER’s models are built using custom code developed by Mentzelopoulos to protect the process and outputs from any potential biases from external data or models. LOBSTgER’s generative AI builds upon real photography, expanding the researchers’ visual vocabulary to deepen the public’s connection to the natural world.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="A photoreal image of a large oval ocean sunfish underwater. An orange LOBSTgER icon indicates this was made with AI." class="ondemand" height="499" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="751" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            This ocean sunfish (Mola mola) image was generated by LOBSTgER’s unconditional models.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            AI-generated image: Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER.        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;At its heart, LOBSTgER operates at the intersection of art, science, and technology. The project draws from the visual language of photography, the observational rigor of marine science, and the computational power of generative AI. By uniting these disciplines, the team is not only developing new ways to visualize ocean life — they are also reimagining how environmental stories can be told. This integrative approach makes LOBSTgER both a research tool and a creative experiment — one that reflects MIT’s long-standing tradition of interdisciplinary innovation.&lt;/p&gt;&lt;p&gt;Underwater photography in New England’s coastal waters is notoriously difficult. Limited visibility, swirling sediment, bubbles, and the unpredictable movement of marine life all pose constant challenges. For the past several years, Ellenbogen has navigated these challenges and is building a comprehensive record of the region’s biodiversity through the project, Space to Sea: Visualizing New England’s Ocean Wilderness.&amp;nbsp;This large dataset of underwater images provides the foundation for training LOBSTgER’s generative AI models. The images span diverse angles, lighting conditions, and animal behaviors, resulting in a visual archive that is both artistically striking and biologically accurate.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Image synthesis via reverse diffusion: This short video shows the de-noising trajectory from Gaussian latent noise to photorealistic output using LOBSTgER’s unconditional models. Iterative de-noising requires 1,000 forward passes through the trained neural network.&lt;br /&gt;Video: Keith Ellenbogen and Andreas Mentzelopoulos / MIT Sea Grant        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;LOBSTgER’s&amp;nbsp;custom diffusion models are trained to replicate not only the biodiversity Ellenbogen documents, but also the artistic style he uses to capture it. By learning from thousands of real underwater images, the models internalize fine-grained details such as natural lighting gradients, species-specific coloration, and even the atmospheric texture created by suspended particles and refracted sunlight. The result is imagery that not only appears visually accurate, but also feels immersive and moving.&lt;/p&gt;&lt;p&gt;The models can both generate new, synthetic, but scientifically accurate images unconditionally (i.e., requiring no user input/guidance), and enhance real photographs conditionally (i.e., image-to-image generation). By integrating AI into the photographic workflow, Ellenbogen will be able to use these tools to recover detail in turbid water, adjust lighting to emphasize key subjects, or even simulate scenes that would be nearly impossible to capture in the field. The team also believes this approach may benefit other underwater photographers and image editors facing similar challenges. This hybrid method is designed to accelerate the curation process and enable storytellers to construct a more complete and coherent visual narrative of life beneath the surface.&lt;/p&gt;        

      &lt;/div&gt;
          




&lt;div class="news-article--content-block--inline-image--items--wrapper"&gt;
  &lt;div class="news-article--content-block--inline-image--items"&gt;
          
                
 
      &lt;div class="news-article--inline-image--item"&gt;
      &lt;div class="news-article--inline-image--file"&gt;
                  

              &lt;img alt="Side-by-side images of an American lobster on the sea floor underneath seaweed. One has been enhanced by AI and is far more vibrant." class="ondemand" height="298" src="https://news.mit.edu/themes/mit/src/img/placeholder/placeholder--news-article--image-gallery.jpg" width="900" /&gt;


        

              &lt;/div&gt;
      &lt;div class="news-article--inline-image--descr--wrapper"&gt;
        &lt;div class="news-article--inline-image--descr"&gt;
          &lt;div class="news-article--inline-image--caption"&gt;
              

            Left: Enhanced image of an American lobster using LOBSTgER’s image-to-image models. Right: Original image.        

          &lt;/div&gt;
          &lt;div class="news-article--inline-image--credits"&gt;
            

            Left: AI genertated image by Keith Ellenbogen, Andreas Mentzelopoulos, and LOBSTgER. Right: Keith Ellenbogen        

          &lt;/div&gt;            
        &lt;/div&gt;
      &lt;/div&gt;

          &lt;/div&gt;
  
        
      &lt;/div&gt;
  &lt;div class="news-article--content-block--inline-image--items-nav"&gt;
    &lt;p class="news-article--content-block--inline-image--items-nav--inner"&gt;  
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--previous"&gt;&lt;svg class="arrow--point-west--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_494" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_112"&gt;
    &lt;g id="Group_111" transform="translate(0 0)"&gt;
      &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_493" transform="translate(63.146 126.293)"&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;
&lt;span class="visually-hidden"&gt;Previous item&lt;/span&gt;&lt;/button&gt;
    &lt;button class="news-article--content-block--inline-image--items-nav--button news-article--content-block--inline-image--items-nav--button--next"&gt;&lt;span class="visually-hidden"&gt;Next item&lt;/span&gt;&lt;svg class="arrow--point-east--slider" viewBox="0 0 16.621 30.183" width="0" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"&gt;
  &lt;defs&gt;
    
      &lt;path d="M0-126.293H16.621V-96.11H0Z" id="Path_496" transform="translate(0 126.293)"&gt;
    
  &lt;/defs&gt;
  &lt;g id="Group_2042" transform="translate(16.621 30.183) rotate(180)"&gt;
    &lt;g id="Group_115"&gt;
      &lt;g id="Group_114" transform="translate(0 0)"&gt;
        &lt;path d="M-48.055-96.11a1.522,1.522,0,0,1-1.081-.448L-62.7-110.12a1.529,1.529,0,0,1,0-2.163l13.562-13.562a1.528,1.528,0,0,1,2.162,0,1.529,1.529,0,0,1,0,2.163L-59.454-111.2l12.48,12.48a1.529,1.529,0,0,1,0,2.163,1.524,1.524,0,0,1-1.081.448" id="Path_495" transform="translate(63.146 126.293)"&gt;
      &lt;/g&gt;
    &lt;/g&gt;
  &lt;/g&gt;
&lt;/svg&gt;&lt;/button&gt;
    &lt;/p&gt;  
  &lt;/div&gt;
&lt;/div&gt;

            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;In one key series, Ellenbogen captured high-resolution images of lion’s mane jellyfish, blue sharks, American lobsters, and ocean sunfish (&lt;em&gt;Mola mola&lt;/em&gt;) while free diving in coastal waters. “Getting a high-quality dataset is not easy,” Ellenbogen says. “It requires multiple dives, missed opportunities, and unpredictable conditions. But these challenges are part of what makes underwater documentation both difficult and rewarding.”&lt;/p&gt;&lt;p&gt;Mentzelopoulos has developed original code to train a family of latent diffusion models for LOBSTgER grounded on Ellenbogen’s images. Developing such models requires a high level of technical expertise, and training models from scratch is a complex process demanding hundreds of hours of computation and meticulous hyperparameter tuning.&lt;/p&gt;&lt;p&gt;The project reflects a parallel process: field documentation through photography and model development through iterative training. Ellenbogen works in the field, capturing rare and fleeting encounters with marine animals; Mentzelopoulos works in the lab, translating those moments into machine-learning contexts that can extend and reinterpret the visual language of the ocean.&lt;/p&gt;&lt;p&gt;“The goal isn’t to replace photography,” Mentzelopoulos says. “It’s to build on and complement it — making the invisible visible, and helping people see environmental complexity in a way that resonates both emotionally and intellectually. Our models aim to capture not just biological realism, but the emotional charge that can drive real-world engagement and action.”&lt;/p&gt;&lt;p&gt;LOBSTgER points to a hybrid future that merges direct observation with technological interpretation. The team’s long-term goal is to develop a comprehensive model that can visualize a wide range of species found in the Gulf of Maine and, eventually, apply similar methods to marine ecosystems around the world.&lt;/p&gt;&lt;p&gt;The researchers suggest that photography and generative AI form a continuum, rather than a conflict. Photography captures what is — the texture, light, and animal behavior during actual encounters — while AI extends that vision beyond what is seen, toward what could be understood, inferred, or imagined based on scientific data and artistic vision. Together, they offer a powerful framework for communicating science through image-making.&lt;/p&gt;&lt;p&gt;In a region where ecosystems are changing rapidly, the act of visualizing becomes more than just documentation. It becomes a tool for awareness, engagement, and, ultimately, conservation. LOBSTgER is still in its infancy, and the team looks forward to sharing more discoveries, images, and insights as the project evolves.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Answer from the lead image: The left image was generated using using LOBSTgER’s unconditional models and the right image is real.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;For more information, contact Keith Ellenbogen and Andreas Mentzelopoulos.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/lobstger-merging-ai-underwater-photography-to-reveal-hidden-ocean-worlds-0625</guid><pubDate>Wed, 25 Jun 2025 13:55:00 +0000</pubDate></item><item><title>AlphaGenome: AI for better understanding the genome (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/</link><description>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-06-25"&gt;25 June 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Ziga Avsec and Natasha Latysheva&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="A central, light-blue DNA double helix stands in sharp focus, flanked by a series of DNA strands that fade into a soft, blurry background, giving the impression of a field of genetic information. The backdrop is bathed in a soft light that transitions from pink to purple." class="picture__image" height="603" src="https://lh3.googleusercontent.com/SZkcKUQyLUhSQ06Rq-PJbxAqn1OpMeEa3khkrBVB1MGyHfxyftoqWwEb2aLP9JxX7CjhpLFODcc5zIoMoNdu0bl6ELsZV2nP9fDwZC6SYS36lzAKDw=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.&lt;/p&gt;&lt;p&gt;The genome is our cellular instruction manual. It’s the complete set of DNA which guides nearly every part of a living organism, from appearance and function to growth and reproduction. Small variations in a genome’s DNA sequence can alter an organism’s response to its environment or its susceptibility to disease. But deciphering how the genome’s instructions are read at the molecular level — and what happens when a small DNA variation occurs — is still one of biology’s greatest mysteries.&lt;/p&gt;&lt;p&gt;Today, we introduce AlphaGenome, a new artificial intelligence (AI) tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes. This was enabled, among other factors, by technical advances allowing the model to process long DNA sequences and output high-resolution predictions.&lt;/p&gt;&lt;p&gt;To advance scientific research, we’re making AlphaGenome available in preview via our AlphaGenome API for non-commercial research, and planning to release the model in the future.&lt;/p&gt;&lt;p&gt;We believe AlphaGenome can be a valuable resource for the scientific community, helping scientists better understand genome function, disease biology, and ultimately, drive new biological discoveries and the development of new treatments.&lt;/p&gt;&lt;h2&gt;How AlphaGenome works&lt;/h2&gt;&lt;p&gt;Our AlphaGenome model takes a long DNA sequence as input — up to 1 million letters, also known as base-pairs — and predicts thousands of molecular properties characterising its regulatory activity. It can also score the effects of genetic variants or mutations by comparing predictions of mutated sequences with unmutated ones.&lt;/p&gt;&lt;p&gt;Predicted properties include where genes start and where they end in different cell types and tissues, where they get spliced, the amount of RNA being produced, and also which DNA bases are accessible, close to one another, or bound by certain proteins. Training data was sourced from large public consortia including ENCODE, GTEx, 4D Nucleome and FANTOM5, which experimentally measured these properties covering important modalities of gene regulation across hundreds of human and mouse cell types and tissues.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-d704db3e-f8f0-43ee-ae46-7a2a21b420ee"&gt;
    &lt;p&gt;Animation showing AlphaGenome taking one million DNA letters as input and predicting diverse molecular properties across different tissues and cell types.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;The AlphaGenome architecture uses convolutional layers to initially detect short patterns in the genome sequence, transformers to communicate information across all positions in the sequence, and a final series of layers to turn the detected patterns into predictions for different modalities. During training, this computation is distributed across multiple interconnected Tensor Processing Units (TPUs) for a single sequence.&lt;/p&gt;&lt;p&gt;This model builds on our previous genomics model, Enformer and is complementary to AlphaMissense, which specializes in categorizing the effects of variants within protein-coding regions. These regions cover 2% of the genome. The remaining 98%, called non-coding regions, are crucial for orchestrating gene activity and contain many variants linked to diseases. AlphaGenome offers a new perspective for interpreting these expansive sequences and the variants within them.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;AlphaGenome’s distinctive features&lt;/h2&gt;&lt;p&gt;AlphaGenome offers several distinctive features compared to existing DNA sequence models:&lt;/p&gt;&lt;h3&gt;Long sequence-context at high resolution&lt;/h3&gt;&lt;p&gt;Our model analyzes up to 1 million DNA letters and makes predictions at the resolution of individual letters. Long sequence context is important for covering regions regulating genes from far away and base-resolution is important for capturing fine-grained biological details.&lt;/p&gt;&lt;p&gt;Previous models had to trade off sequence length and resolution, which limited the range of modalities they could jointly model and accurately predict. Our technical advances address this limitation without significantly increasing the training resources — training a single AlphaGenome model (without distillation) took four hours and required half of the compute budget used to train our original Enformer model.&lt;/p&gt;&lt;h3&gt;Comprehensive multimodal prediction&lt;/h3&gt;&lt;p&gt;By unlocking high resolution prediction for long input sequences, AlphaGenome can predict the most diverse range of modalities. In doing so, AlphaGenome provides scientists with more comprehensive information about the complex steps of gene regulation.&lt;/p&gt;&lt;h3&gt;Efficient variant scoring&lt;/h3&gt;&lt;p&gt;In addition to predicting a diverse range of molecular properties, AlphaGenome can efficiently score the impact of a genetic variant on all of these properties in a second. It does this by contrasting predictions of mutated sequences with unmutated ones, and efficiently summarising that contrast using different approaches for different modalities.&lt;/p&gt;&lt;h3&gt;Novel splice-junction modeling&lt;/h3&gt;&lt;p&gt;Many rare genetic diseases, such as spinal muscular atrophy and some forms of cystic fibrosis, can be caused by errors in RNA splicing — a process where parts of the RNA molecule are removed, or “spliced out”, and the remaining ends rejoined. For the first time, AlphaGenome can explicitly model the location and expression level of these junctions directly from sequence, offering deeper insights about the consequences of genetic variants on RNA splicing.&lt;/p&gt;&lt;h2&gt;State-of-the-art performance across benchmarks&lt;/h2&gt;&lt;p&gt;AlphaGenome achieves state-of-the-art performance across a wide range of genomic prediction benchmarks, such as predicting which parts of the DNA molecule will be in close proximity, whether a genetic variant will increase or decrease expression of a gene, or whether it will change the gene’s splicing pattern.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-6cfc2442-d824-4dda-b563-3b5162675a6e"&gt;
    &lt;p&gt;Bar graph showing AlphaGenome’s relative improvements on selected DNA sequence and variant effect tasks, compared against results for the current best methods in each category.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;When producing predictions for single DNA sequences, AlphaGenome outperformed the best external models on 22 out of 24 evaluations. And when predicting the regulatory effect of a variant, it matched or exceeded the top-performing external models on 24 out of 26 evaluations.&lt;/p&gt;&lt;p&gt;This comparison included models specialized for individual tasks. AlphaGenome was the only model that could jointly predict all of the assessed modalities, highlighting its generality. Read more in our preprint.&lt;/p&gt;&lt;h2&gt;The benefits of a unifying model&lt;/h2&gt;&lt;p&gt;AlphaGenome’s generality allows scientists to simultaneously explore a variant's impact on a number of modalities with a single API call. This means that scientists can generate and test hypotheses more rapidly, without having to use multiple models to investigate different modalities.&lt;/p&gt;&lt;p&gt;Moreover AlphaGenome’s strong performance indicates it has learned a relatively general representation of DNA sequence in the context of gene regulation. This makes it a strong foundation for the wider community to build upon. Once the model is fully released, scientists will be able to adapt and fine-tune it on their own datasets to better tackle their unique research questions.&lt;/p&gt;&lt;p&gt;Finally, this approach provides a flexible and scalable architecture for the future. By extending the training data, AlphaGenome’s capabilities could be extended to yield better performance, cover more species, or include additional modalities to make the model even more comprehensive.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;It’s a milestone for the field. For the first time, we have a single model that unifies long-range context, base-level precision and state-of-the-art performance across a whole spectrum of genomic tasks.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Dr. Caleb Lareau, Memorial Sloan Kettering Cancer Center&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;A powerful research tool&lt;/h2&gt;&lt;p&gt;AlphaGenome's predictive capabilities could help several research avenues:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Disease understanding:&lt;/strong&gt; By more accurately predicting genetic disruptions, AlphaGenome could help researchers pinpoint the potential causes of disease more precisely, and better interpret the functional impact of variants linked to certain traits, potentially uncovering new therapeutic targets. We think the model is especially suitable for studying rare variants with potentially large effects, such as those causing rare Mendelian disorders.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Synthetic biology:&lt;/strong&gt; Its predictions could be used to guide the design of synthetic DNA with specific regulatory function — for example, only activating a gene in nerve cells but not muscle cells.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Fundamental research:&lt;/strong&gt; It could accelerate our understanding of the genome by assisting in mapping its crucial functional elements and defining their roles, identifying the most essential DNA instructions for regulating a specific cell type's function.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For example, we used AlphaGenome to investigate the potential mechanism of a cancer-associated mutation. In an existing study of patients with T-cell acute lymphoblastic leukemia (T-ALL), researchers observed mutations at particular locations in the genome. Using AlphaGenome, we predicted that the mutations would activate a nearby gene called TAL1 by introducing a MYB DNA binding motif, which replicated the known disease mechanism and highlighted AlphaGenome’s ability to link specific non-coding variants to disease genes.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;AlphaGenome will be a powerful tool for the field. Determining the relevance of different non-coding variants can be extremely challenging, particularly to do at scale. This tool will provide a crucial piece of the puzzle, allowing us to make better connections to understand diseases like cancer.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Professor Marc Mansour, University College London&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Current limitations&lt;/h2&gt;&lt;p&gt;AlphaGenome marks a significant step forward, but it's important to acknowledge its current limitations.&lt;/p&gt;&lt;p&gt;Like other sequence-based models, accurately capturing the influence of very distant regulatory elements, like those over 100,000 DNA letters away, is still an ongoing challenge. Another priority for future work is further increasing the model’s ability to capture cell- and tissue-specific patterns.&lt;/p&gt;&lt;p&gt;We haven't designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models. Instead, we focused more on characterising the performance on individual genetic variants. And while AlphaGenome can predict molecular outcomes, it doesn't give the full picture of how genetic variations lead to complex traits or diseases. These often involve broader biological processes, like developmental and environmental factors, that are beyond the direct scope of our model.&lt;/p&gt;&lt;p&gt;We’re continuing to improve our models and gathering feedback to help us address these gaps.&lt;/p&gt;&lt;h2&gt;Enabling the community to unlock AlphaGenome's potential&lt;/h2&gt;&lt;p&gt;AlphaGenome is now available for non-commercial use via our AlphaGenome API. Please note that our model’s predictions are intended only for research use and haven’t been designed or validated for direct clinical purposes.&lt;/p&gt;&lt;p&gt;Researchers worldwide are invited to get in touch with potential use-cases for AlphaGenome and to ask questions or share feedback through the community forum.&lt;/p&gt;&lt;p&gt;We hope AlphaGenome will be an important tool for better understanding the genome and we’re committed to working alongside external experts across academia, industry, and government organizations to ensure AlphaGenome benefits as many people as possible.&lt;/p&gt;&lt;p&gt;Together with the collective efforts of the wider scientific community, we hope it will deepen our understanding of the complex cellular processes encoded in the DNA sequence and the effects of variants, and drive exciting new discoveries in genomics and healthcare.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about AlphaGenome&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We would like to thank Juanita Bawagan, Arielle Bier, Stephanie Booth, Irina Andronic, Armin Senoner, Dhavanthi Hariharan, Rob Ashley, Agata Laydon and Kathryn Tunyasuvunakool for their help with the text and figures.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of the AlphaGenome co-authors: Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drotár, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis and Pushmeet Kohli.&lt;/p&gt;&lt;p&gt;We would also like to thank Dhavanthi Hariharan, Charlie Taylor, Ottavia Bertolli, Yannis Assael, Alex Botev, Anna Trostanetski, Lucas Tenório, Victoria Johnston, Richard Green, Kathryn Tunyasuvunakool, Molly Beck, Uchechi Okereke, Rachael Tremlett, Sarah Chakera, Ibrahim I. Taskiran, Andreea-Alexandra Muşat, Raiyan Khan, Ren Yi and the greater Google DeepMind team for their support, help and feedback.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</description><content:encoded>&lt;div class="article-cover article-cover--centered"&gt;
    &lt;div class="article-cover__header"&gt;
      &lt;p class="article-cover__eyebrow glue-label"&gt;Science&lt;/p&gt;
      

      
    &lt;dl class="article-cover__meta"&gt;
      
        &lt;dt class="glue-visually-hidden"&gt;Published&lt;/dt&gt;
        &lt;dd class="article-cover__date glue-label"&gt;&lt;time datetime="2025-06-25"&gt;25 June 2025&lt;/time&gt;&lt;/dd&gt;
      
      
        &lt;dt class="glue-visually-hidden"&gt;Authors&lt;/dt&gt;
        &lt;dd class="article-cover__authors"&gt;&lt;p&gt;Ziga Avsec and Natasha Latysheva&lt;/p&gt;&lt;/dd&gt;
      
    &lt;/dl&gt;
  

      
    &lt;/div&gt;

    
      
    
    
    
      &lt;source height="603" media="(min-width: 1024px)" type="image/webp" width="1072" /&gt;&lt;source height="522" media="(min-width: 600px)" type="image/webp" width="928" /&gt;&lt;source height="297" type="image/webp" width="528" /&gt;
      &lt;img alt="A central, light-blue DNA double helix stands in sharp focus, flanked by a series of DNA strands that fade into a soft, blurry background, giving the impression of a field of genetic information. The backdrop is bathed in a soft light that transitions from pink to purple." class="picture__image" height="603" src="https://lh3.googleusercontent.com/SZkcKUQyLUhSQ06Rq-PJbxAqn1OpMeEa3khkrBVB1MGyHfxyftoqWwEb2aLP9JxX7CjhpLFODcc5zIoMoNdu0bl6ELsZV2nP9fDwZC6SYS36lzAKDw=w1072-h603-n-nu" width="1072" /&gt;
    
    
  
    
  &lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p class="gdm-rich-text__subtitle"&gt;Introducing a new, unifying DNA sequence model that advances regulatory variant-effect prediction and promises to shed new light on genome function — now available via API.&lt;/p&gt;&lt;p&gt;The genome is our cellular instruction manual. It’s the complete set of DNA which guides nearly every part of a living organism, from appearance and function to growth and reproduction. Small variations in a genome’s DNA sequence can alter an organism’s response to its environment or its susceptibility to disease. But deciphering how the genome’s instructions are read at the molecular level — and what happens when a small DNA variation occurs — is still one of biology’s greatest mysteries.&lt;/p&gt;&lt;p&gt;Today, we introduce AlphaGenome, a new artificial intelligence (AI) tool that more comprehensively and accurately predicts how single variants or mutations in human DNA sequences impact a wide range of biological processes regulating genes. This was enabled, among other factors, by technical advances allowing the model to process long DNA sequences and output high-resolution predictions.&lt;/p&gt;&lt;p&gt;To advance scientific research, we’re making AlphaGenome available in preview via our AlphaGenome API for non-commercial research, and planning to release the model in the future.&lt;/p&gt;&lt;p&gt;We believe AlphaGenome can be a valuable resource for the scientific community, helping scientists better understand genome function, disease biology, and ultimately, drive new biological discoveries and the development of new treatments.&lt;/p&gt;&lt;h2&gt;How AlphaGenome works&lt;/h2&gt;&lt;p&gt;Our AlphaGenome model takes a long DNA sequence as input — up to 1 million letters, also known as base-pairs — and predicts thousands of molecular properties characterising its regulatory activity. It can also score the effects of genetic variants or mutations by comparing predictions of mutated sequences with unmutated ones.&lt;/p&gt;&lt;p&gt;Predicted properties include where genes start and where they end in different cell types and tissues, where they get spliced, the amount of RNA being produced, and also which DNA bases are accessible, close to one another, or bound by certain proteins. Training data was sourced from large public consortia including ENCODE, GTEx, 4D Nucleome and FANTOM5, which experimentally measured these properties covering important modalities of gene regulation across hundreds of human and mouse cell types and tissues.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-d704db3e-f8f0-43ee-ae46-7a2a21b420ee"&gt;
    &lt;p&gt;Animation showing AlphaGenome taking one million DNA letters as input and predicting diverse molecular properties across different tissues and cell types.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;The AlphaGenome architecture uses convolutional layers to initially detect short patterns in the genome sequence, transformers to communicate information across all positions in the sequence, and a final series of layers to turn the detected patterns into predictions for different modalities. During training, this computation is distributed across multiple interconnected Tensor Processing Units (TPUs) for a single sequence.&lt;/p&gt;&lt;p&gt;This model builds on our previous genomics model, Enformer and is complementary to AlphaMissense, which specializes in categorizing the effects of variants within protein-coding regions. These regions cover 2% of the genome. The remaining 98%, called non-coding regions, are crucial for orchestrating gene activity and contain many variants linked to diseases. AlphaGenome offers a new perspective for interpreting these expansive sequences and the variants within them.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;AlphaGenome’s distinctive features&lt;/h2&gt;&lt;p&gt;AlphaGenome offers several distinctive features compared to existing DNA sequence models:&lt;/p&gt;&lt;h3&gt;Long sequence-context at high resolution&lt;/h3&gt;&lt;p&gt;Our model analyzes up to 1 million DNA letters and makes predictions at the resolution of individual letters. Long sequence context is important for covering regions regulating genes from far away and base-resolution is important for capturing fine-grained biological details.&lt;/p&gt;&lt;p&gt;Previous models had to trade off sequence length and resolution, which limited the range of modalities they could jointly model and accurately predict. Our technical advances address this limitation without significantly increasing the training resources — training a single AlphaGenome model (without distillation) took four hours and required half of the compute budget used to train our original Enformer model.&lt;/p&gt;&lt;h3&gt;Comprehensive multimodal prediction&lt;/h3&gt;&lt;p&gt;By unlocking high resolution prediction for long input sequences, AlphaGenome can predict the most diverse range of modalities. In doing so, AlphaGenome provides scientists with more comprehensive information about the complex steps of gene regulation.&lt;/p&gt;&lt;h3&gt;Efficient variant scoring&lt;/h3&gt;&lt;p&gt;In addition to predicting a diverse range of molecular properties, AlphaGenome can efficiently score the impact of a genetic variant on all of these properties in a second. It does this by contrasting predictions of mutated sequences with unmutated ones, and efficiently summarising that contrast using different approaches for different modalities.&lt;/p&gt;&lt;h3&gt;Novel splice-junction modeling&lt;/h3&gt;&lt;p&gt;Many rare genetic diseases, such as spinal muscular atrophy and some forms of cystic fibrosis, can be caused by errors in RNA splicing — a process where parts of the RNA molecule are removed, or “spliced out”, and the remaining ends rejoined. For the first time, AlphaGenome can explicitly model the location and expression level of these junctions directly from sequence, offering deeper insights about the consequences of genetic variants on RNA splicing.&lt;/p&gt;&lt;h2&gt;State-of-the-art performance across benchmarks&lt;/h2&gt;&lt;p&gt;AlphaGenome achieves state-of-the-art performance across a wide range of genomic prediction benchmarks, such as predicting which parts of the DNA molecule will be in close proximity, whether a genetic variant will increase or decrease expression of a gene, or whether it will change the gene’s splicing pattern.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  





&lt;figure class="single-media single-media--inline"&gt;
  

  &lt;figcaption class="caption"&gt;
      &lt;div class="caption__text glue-caption" id="caption-6cfc2442-d824-4dda-b563-3b5162675a6e"&gt;
    &lt;p&gt;Bar graph showing AlphaGenome’s relative improvements on selected DNA sequence and variant effect tasks, compared against results for the current best methods in each category.&lt;/p&gt;
  &lt;/div&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;p&gt;When producing predictions for single DNA sequences, AlphaGenome outperformed the best external models on 22 out of 24 evaluations. And when predicting the regulatory effect of a variant, it matched or exceeded the top-performing external models on 24 out of 26 evaluations.&lt;/p&gt;&lt;p&gt;This comparison included models specialized for individual tasks. AlphaGenome was the only model that could jointly predict all of the assessed modalities, highlighting its generality. Read more in our preprint.&lt;/p&gt;&lt;h2&gt;The benefits of a unifying model&lt;/h2&gt;&lt;p&gt;AlphaGenome’s generality allows scientists to simultaneously explore a variant's impact on a number of modalities with a single API call. This means that scientists can generate and test hypotheses more rapidly, without having to use multiple models to investigate different modalities.&lt;/p&gt;&lt;p&gt;Moreover AlphaGenome’s strong performance indicates it has learned a relatively general representation of DNA sequence in the context of gene regulation. This makes it a strong foundation for the wider community to build upon. Once the model is fully released, scientists will be able to adapt and fine-tune it on their own datasets to better tackle their unique research questions.&lt;/p&gt;&lt;p&gt;Finally, this approach provides a flexible and scalable architecture for the future. By extending the training data, AlphaGenome’s capabilities could be extended to yield better performance, cover more species, or include additional modalities to make the model even more comprehensive.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;It’s a milestone for the field. For the first time, we have a single model that unifies long-range context, base-level precision and state-of-the-art performance across a whole spectrum of genomic tasks.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Dr. Caleb Lareau, Memorial Sloan Kettering Cancer Center&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;A powerful research tool&lt;/h2&gt;&lt;p&gt;AlphaGenome's predictive capabilities could help several research avenues:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;strong&gt;Disease understanding:&lt;/strong&gt; By more accurately predicting genetic disruptions, AlphaGenome could help researchers pinpoint the potential causes of disease more precisely, and better interpret the functional impact of variants linked to certain traits, potentially uncovering new therapeutic targets. We think the model is especially suitable for studying rare variants with potentially large effects, such as those causing rare Mendelian disorders.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Synthetic biology:&lt;/strong&gt; Its predictions could be used to guide the design of synthetic DNA with specific regulatory function — for example, only activating a gene in nerve cells but not muscle cells.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Fundamental research:&lt;/strong&gt; It could accelerate our understanding of the genome by assisting in mapping its crucial functional elements and defining their roles, identifying the most essential DNA instructions for regulating a specific cell type's function.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;For example, we used AlphaGenome to investigate the potential mechanism of a cancer-associated mutation. In an existing study of patients with T-cell acute lymphoblastic leukemia (T-ALL), researchers observed mutations at particular locations in the genome. Using AlphaGenome, we predicted that the mutations would activate a nearby gene called TAL1 by introducing a MYB DNA binding motif, which replicated the known disease mechanism and highlighted AlphaGenome’s ability to link specific non-coding variants to disease genes.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  &lt;figure class="quote quote--inline"&gt;
  &lt;blockquote class="quote__text"&gt;
    &lt;p&gt;“&lt;/p&gt;
    &lt;p&gt;AlphaGenome will be a powerful tool for the field. Determining the relevance of different non-coding variants can be extremely challenging, particularly to do at scale. This tool will provide a crucial piece of the puzzle, allowing us to make better connections to understand diseases like cancer.&lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;figcaption class="quote__author"&gt;&lt;p&gt;Professor Marc Mansour, University College London&lt;/p&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
                
              
                
                
                  
                  &lt;div class="gdm-rich-text rich-text"&gt;
  &lt;h2&gt;Current limitations&lt;/h2&gt;&lt;p&gt;AlphaGenome marks a significant step forward, but it's important to acknowledge its current limitations.&lt;/p&gt;&lt;p&gt;Like other sequence-based models, accurately capturing the influence of very distant regulatory elements, like those over 100,000 DNA letters away, is still an ongoing challenge. Another priority for future work is further increasing the model’s ability to capture cell- and tissue-specific patterns.&lt;/p&gt;&lt;p&gt;We haven't designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models. Instead, we focused more on characterising the performance on individual genetic variants. And while AlphaGenome can predict molecular outcomes, it doesn't give the full picture of how genetic variations lead to complex traits or diseases. These often involve broader biological processes, like developmental and environmental factors, that are beyond the direct scope of our model.&lt;/p&gt;&lt;p&gt;We’re continuing to improve our models and gathering feedback to help us address these gaps.&lt;/p&gt;&lt;h2&gt;Enabling the community to unlock AlphaGenome's potential&lt;/h2&gt;&lt;p&gt;AlphaGenome is now available for non-commercial use via our AlphaGenome API. Please note that our model’s predictions are intended only for research use and haven’t been designed or validated for direct clinical purposes.&lt;/p&gt;&lt;p&gt;Researchers worldwide are invited to get in touch with potential use-cases for AlphaGenome and to ask questions or share feedback through the community forum.&lt;/p&gt;&lt;p&gt;We hope AlphaGenome will be an important tool for better understanding the genome and we’re committed to working alongside external experts across academia, industry, and government organizations to ensure AlphaGenome benefits as many people as possible.&lt;/p&gt;&lt;p&gt;Together with the collective efforts of the wider scientific community, we hope it will deepen our understanding of the complex cellular processes encoded in the DNA sequence and the effects of variants, and drive exciting new discoveries in genomics and healthcare.&lt;/p&gt;
&lt;/div&gt;
                
              
                
                
                  
                  

&lt;section class="button-group button-group--stacked"&gt;
  
    &lt;h2 class="glue-headline glue-headline--headline-6 button-group__title"&gt;Learn more about AlphaGenome&lt;/h2&gt;
  

  
&lt;/section&gt;
                
              
                
                
                  
                  &lt;section class="notes"&gt;
  &lt;div class="glue-page"&gt;
    &lt;div class="gdm-rich-text notes__inner"&gt;
      &lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;We would like to thank Juanita Bawagan, Arielle Bier, Stephanie Booth, Irina Andronic, Armin Senoner, Dhavanthi Hariharan, Rob Ashley, Agata Laydon and Kathryn Tunyasuvunakool for their help with the text and figures.&lt;/p&gt;&lt;p&gt;This work was done thanks to the contributions of the AlphaGenome co-authors: Žiga Avsec, Natasha Latysheva, Jun Cheng, Guido Novati, Kyle R. Taylor, Tom Ward, Clare Bycroft, Lauren Nicolaisen, Eirini Arvaniti, Joshua Pan, Raina Thomas, Vincent Dutordoir, Matteo Perino, Soham De, Alexander Karollus, Adam Gayoso, Toby Sargeant, Anne Mottram, Lai Hong Wong, Pavol Drotár, Adam Kosiorek, Andrew Senior, Richard Tanburn, Taylor Applebaum, Souradeep Basu, Demis Hassabis and Pushmeet Kohli.&lt;/p&gt;&lt;p&gt;We would also like to thank Dhavanthi Hariharan, Charlie Taylor, Ottavia Bertolli, Yannis Assael, Alex Botev, Anna Trostanetski, Lucas Tenório, Victoria Johnston, Richard Green, Kathryn Tunyasuvunakool, Molly Beck, Uchechi Okereke, Rachael Tremlett, Sarah Chakera, Ibrahim I. Taskiran, Andreea-Alexandra Muşat, Raiyan Khan, Ren Yi and the greater Google DeepMind team for their support, help and feedback.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/section&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/</guid><pubDate>Wed, 25 Jun 2025 13:59:51 +0000</pubDate></item><item><title>Google’s new AI will help researchers understand how our genes work (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/06/25/1119345/google-deepmind-alphagenome-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624_AlphaGenome.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn’t know what all those 3 billion genetic letters actually do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now Google’s DeepMind division says it’s made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene’s activity will go up or down. It’s just the sort of question biologists regularly assess in lab experiments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“We have, for the first time, created a single model that unifies many different challenges that come with understanding the genome,” says Pushmeet Kohli, a vice president for research at DeepMind.&lt;/p&gt;  &lt;p&gt;Five years ago, the Google AI division released AlphaFold, a technology for predicting the 3D shape of proteins. That work was honored with a Nobel Prize last year and spawned a drug-discovery spinout, Isomorphic Labs, and a boom of companies that hope AI will be able to propose new drugs.&lt;/p&gt; 
 &lt;p&gt;AlphaGenome is an attempt to further smooth biologists’ work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We have these 3 billion letters of DNA that make up a human genome, but every person is slightly different, and we don’t fully understand what those differences do,” says Caleb Lareau, a computational biologist at Memorial Sloan Kettering Cancer Center who has had early access to AlphaGenome. “This is the most powerful tool to date to model that.”&lt;/p&gt; 
 &lt;p&gt;Google says AlphaGenome will be free for noncommercial users and plans to release full details of the model in the future. According to Kohli, the company is exploring ways to “enable use of this model by commercial entities” such as biotech companies.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Lareau says AlphaGenome will allow certain types of experiments now done in the lab to be carried out virtually, on a computer. For instance, studies of people who’ve donated their DNA for research often turn up thousands of genetic differences, each slightly raising or lowering the chance a person gets a disease such as Alzheimer’s.&lt;/p&gt;  &lt;p&gt;Lareau says DeepMind’s software could be used to quickly make predictions about how each of those variants works at a molecular level, something that would otherwise require time-consuming lab experiments. “You’ll get this list of gene variants, but then I want to understand which of those are actually doing something, and where can I intervene,” he says. “This system pushes us closer to a good first guess about what any variant will be doing when we observe it in a human.”&lt;/p&gt;  &lt;p&gt;Don’t expect AlphaGenome to predict very much about individual people, however. It offers&amp;nbsp;clues to nitty-gritty molecular details of gene activity, not 23andMe-type revelations of a person’s traits or ancestry.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“We haven’t designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models,” Google said in a statement.&lt;/p&gt;  &lt;p&gt;Underlying the AI system is the so-called transformer architecture invented at Google that also powers large language models like GPT-4. This one was trained on troves of experimental data produced by public scientific projects.&lt;/p&gt;  &lt;p&gt;Lareau says the system will not broadly change how his lab works day to day but could permit new types of research. For instance, sometimes doctors encounter patients with ultra-rare cancers, bristling with unfamiliar mutations. AlphaGenome could suggest which of those mutations are really causing the root problem, possibly pointing to a treatment.&lt;/p&gt;  &lt;p&gt;“A hallmark of cancer is that specific mutations in DNA make the wrong genes express in the wrong context,” says Julien Gagneur, a professor of computational medicine at the Technical University of Munich. “This type of tool is instrumental in narrowing down which ones mess up proper gene expression.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The same approach could apply to patients with rare genetic disease, many of whom never learn the source of their condition, even if their DNA has been decoded. “We can obtain their genomes, but we are clueless as to which genetic alterations cause the disease,” says Gagneur. He thinks AlphaGenome could give medical scientists a new way to diagnose such cases.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Eventually, some researchers aspire to use AI to design entire genomes from the ground up and create new life forms. Others think the models will be used to create a fully virtual laboratory for drug studies. “My dream would be to simulate a virtual cell,” Demis Hassabis, CEO of Google DeepMind, said this year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Kohli calls AlphaGenome a “milestone” on the road to that kind of system. “AlphaGenome may not model the whole cell in its entirety … but it’s starting to sort of shed light on the broader semantics of DNA,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/06/250624_AlphaGenome.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;When scientists first sequenced the human genome in 2003, they revealed the full set of DNA instructions that make a person. But we still didn’t know what all those 3 billion genetic letters actually do.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now Google’s DeepMind division says it’s made a leap in trying to understand the code with AlphaGenome, an AI model that predicts what effects small changes in DNA will have on an array of molecular processes, such as whether a gene’s activity will go up or down. It’s just the sort of question biologists regularly assess in lab experiments.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“We have, for the first time, created a single model that unifies many different challenges that come with understanding the genome,” says Pushmeet Kohli, a vice president for research at DeepMind.&lt;/p&gt;  &lt;p&gt;Five years ago, the Google AI division released AlphaFold, a technology for predicting the 3D shape of proteins. That work was honored with a Nobel Prize last year and spawned a drug-discovery spinout, Isomorphic Labs, and a boom of companies that hope AI will be able to propose new drugs.&lt;/p&gt; 
 &lt;p&gt;AlphaGenome is an attempt to further smooth biologists’ work by answering basic questions about how changing DNA letters alters gene activity and, eventually, how genetic mutations affect our health.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“We have these 3 billion letters of DNA that make up a human genome, but every person is slightly different, and we don’t fully understand what those differences do,” says Caleb Lareau, a computational biologist at Memorial Sloan Kettering Cancer Center who has had early access to AlphaGenome. “This is the most powerful tool to date to model that.”&lt;/p&gt; 
 &lt;p&gt;Google says AlphaGenome will be free for noncommercial users and plans to release full details of the model in the future. According to Kohli, the company is exploring ways to “enable use of this model by commercial entities” such as biotech companies.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap alignleft"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Lareau says AlphaGenome will allow certain types of experiments now done in the lab to be carried out virtually, on a computer. For instance, studies of people who’ve donated their DNA for research often turn up thousands of genetic differences, each slightly raising or lowering the chance a person gets a disease such as Alzheimer’s.&lt;/p&gt;  &lt;p&gt;Lareau says DeepMind’s software could be used to quickly make predictions about how each of those variants works at a molecular level, something that would otherwise require time-consuming lab experiments. “You’ll get this list of gene variants, but then I want to understand which of those are actually doing something, and where can I intervene,” he says. “This system pushes us closer to a good first guess about what any variant will be doing when we observe it in a human.”&lt;/p&gt;  &lt;p&gt;Don’t expect AlphaGenome to predict very much about individual people, however. It offers&amp;nbsp;clues to nitty-gritty molecular details of gene activity, not 23andMe-type revelations of a person’s traits or ancestry.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;“We haven’t designed or validated AlphaGenome for personal genome prediction, a known challenge for AI models,” Google said in a statement.&lt;/p&gt;  &lt;p&gt;Underlying the AI system is the so-called transformer architecture invented at Google that also powers large language models like GPT-4. This one was trained on troves of experimental data produced by public scientific projects.&lt;/p&gt;  &lt;p&gt;Lareau says the system will not broadly change how his lab works day to day but could permit new types of research. For instance, sometimes doctors encounter patients with ultra-rare cancers, bristling with unfamiliar mutations. AlphaGenome could suggest which of those mutations are really causing the root problem, possibly pointing to a treatment.&lt;/p&gt;  &lt;p&gt;“A hallmark of cancer is that specific mutations in DNA make the wrong genes express in the wrong context,” says Julien Gagneur, a professor of computational medicine at the Technical University of Munich. “This type of tool is instrumental in narrowing down which ones mess up proper gene expression.”&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The same approach could apply to patients with rare genetic disease, many of whom never learn the source of their condition, even if their DNA has been decoded. “We can obtain their genomes, but we are clueless as to which genetic alterations cause the disease,” says Gagneur. He thinks AlphaGenome could give medical scientists a new way to diagnose such cases.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Eventually, some researchers aspire to use AI to design entire genomes from the ground up and create new life forms. Others think the models will be used to create a fully virtual laboratory for drug studies. “My dream would be to simulate a virtual cell,” Demis Hassabis, CEO of Google DeepMind, said this year.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Kohli calls AlphaGenome a “milestone” on the road to that kind of system. “AlphaGenome may not model the whole cell in its entirety … but it’s starting to sort of shed light on the broader semantics of DNA,” he says.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/06/25/1119345/google-deepmind-alphagenome-ai/</guid><pubDate>Wed, 25 Jun 2025 14:00:00 +0000</pubDate></item><item><title>Winning capital for your AI startup? Kleida Martiro is leading the conversation at TechCrunch All Stage (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/winning-capital-for-your-ai-startup-kleida-martiro-is-leading-the-conversation-at-techcrunch-all-stage/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-native startups are rewriting the rules of what early traction looks like — and too often, investors are still playing by the old ones. At &lt;strong&gt;TechCrunch All Stage&lt;/strong&gt;, happening in Boston on July 15, &lt;strong&gt;Kleida Martiro&lt;/strong&gt;, partner at Glasswing Ventures, will lead a breakout that cuts straight to the core of this disconnect.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Her session, &lt;strong&gt;Winning Capital in a Competitive Market: How to Fund Your AI-Native Startup,&lt;/strong&gt; on the &lt;strong&gt;Foundation Stage&lt;/strong&gt;, explores how early-stage AI founders can frame their growth story in a way that resonates with forward-thinking investors — and filter out the ones still stuck in SaaS-era metrics.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch All Stage 2025 Kleida Martiro" class="wp-image-3021950" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/Kleida-Martiro-SpeakerArticleImageHeader_TCAllStage.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-founders-will-take-away"&gt;What founders will take away&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Martiro brings the perfect blend of operator and investor experience to the table. Before joining Glasswing, she led machine learning efforts as Head of Data Science at SocialFlow and now serves on multiple AI startup boards. She’s also been named one of Business Insider’s “41 Most Important VCs in Boston” and a VentureBeat “Women in AI Rising Star.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Expect a tactical conversation on aligning your vision, funding needs, and long-term opportunity with the capital partners who actually get what you’re building. If you’re building AI from the ground up, this is the room to be in. You’ll have plenty of time during the session to ask your burning questions about launching and scaling an AI startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your pass to TechCrunch All Stage&lt;/strong&gt; before prices rise at the door.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI-native startups are rewriting the rules of what early traction looks like — and too often, investors are still playing by the old ones. At &lt;strong&gt;TechCrunch All Stage&lt;/strong&gt;, happening in Boston on July 15, &lt;strong&gt;Kleida Martiro&lt;/strong&gt;, partner at Glasswing Ventures, will lead a breakout that cuts straight to the core of this disconnect.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Her session, &lt;strong&gt;Winning Capital in a Competitive Market: How to Fund Your AI-Native Startup,&lt;/strong&gt; on the &lt;strong&gt;Foundation Stage&lt;/strong&gt;, explores how early-stage AI founders can frame their growth story in a way that resonates with forward-thinking investors — and filter out the ones still stuck in SaaS-era metrics.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch All Stage 2025 Kleida Martiro" class="wp-image-3021950" height="383" src="https://techcrunch.com/wp-content/uploads/2025/06/Kleida-Martiro-SpeakerArticleImageHeader_TCAllStage.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-founders-will-take-away"&gt;What founders will take away&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Martiro brings the perfect blend of operator and investor experience to the table. Before joining Glasswing, she led machine learning efforts as Head of Data Science at SocialFlow and now serves on multiple AI startup boards. She’s also been named one of Business Insider’s “41 Most Important VCs in Boston” and a VentureBeat “Women in AI Rising Star.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Expect a tactical conversation on aligning your vision, funding needs, and long-term opportunity with the capital partners who actually get what you’re building. If you’re building AI from the ground up, this is the room to be in. You’ll have plenty of time during the session to ask your burning questions about launching and scaling an AI startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your pass to TechCrunch All Stage&lt;/strong&gt; before prices rise at the door.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/winning-capital-for-your-ai-startup-kleida-martiro-is-leading-the-conversation-at-techcrunch-all-stage/</guid><pubDate>Wed, 25 Jun 2025 14:30:00 +0000</pubDate></item><item><title>NO FAKES Act: AI deepfakes protection or internet freedom threat? (AI News)</title><link>https://www.artificialintelligence-news.com/news/no-fakes-act-ai-deepfakes-protection-internet-freedom-threat/</link><description>&lt;p&gt;Critics fear the revised NO FAKES Act has morphed from targeted AI deepfakes protection into sweeping censorship powers.&lt;/p&gt;&lt;p&gt;What began as a seemingly reasonable attempt to tackle AI-generated deepfakes has snowballed into something far more troubling, according to digital rights advocates. The much-discussed Nurture Originals, Foster Art, and Keep Entertainment Safe (NO FAKES) Act – originally aimed at preventing unauthorised digital replicas of people – now threatens to fundamentally alter how the internet functions.&lt;/p&gt;&lt;p&gt;The bill’s expansion has set alarm bells ringing throughout the tech community. It’s gone well beyond simply protecting celebrities from fake videos to potentially creating a sweeping censorship framework.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-sensible-safeguards-to-sledgehammer-approach"&gt;From sensible safeguards to sledgehammer approach&lt;/h3&gt;&lt;p&gt;The initial idea wasn’t entirely misguided: to create protections against AI systems generating fake videos of real people without permission. We’ve all seen those unsettling deepfakes circulating online.&lt;/p&gt;&lt;p&gt;But rather than crafting narrow, targeted measures, lawmakers have opted for what the Electronic Frontier Foundation calls a “federalised image-licensing system” that goes far beyond reasonable protections.&lt;/p&gt;&lt;p&gt;“The updated bill doubles down on that initial mistaken approach,” the EFF notes, “by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them.”&lt;/p&gt;&lt;p&gt;What’s particularly worrying is the NO FAKES Act’s requirement for nearly every internet platform to implement systems that would not only remove content after receiving takedown notices but also prevent similar content from ever being uploaded again. Essentially, it’s forcing platforms to deploy content filters that have proven notoriously unreliable in other contexts.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-innovation-chilling"&gt;Innovation-chilling&lt;/h3&gt;&lt;p&gt;Perhaps most concerning for the AI sector is how the NO FAKES Act targets the tools themselves. The revised bill wouldn’t just go after harmful content; it would potentially shut down entire development platforms and software tools that could be used to create unauthorised images.&lt;/p&gt;&lt;p&gt;This approach feels reminiscent of trying to ban word processors because someone might use one to write defamatory content. The bill includes some limitations (e.g. tools must be “primarily designed” for making unauthorised replicas or have limited other commercial uses) but these distinctions are notoriously subject to interpretation.&lt;/p&gt;&lt;p&gt;Small UK startups venturing into AI image generation could find themselves caught in expensive legal battles based on flimsy allegations long before they have a chance to establish themselves. Meanwhile, tech giants with armies of lawyers can better weather such storms, potentially entrenching their dominance.&lt;/p&gt;&lt;p&gt;Anyone who’s dealt with YouTube’s ContentID system or similar copyright filtering tools knows how frustratingly imprecise they can be. These systems routinely flag legitimate content like musicians performing their own songs or creators using material under fair dealing provisions.&lt;/p&gt;&lt;p&gt;The NO FAKES Act would effectively mandate similar filtering systems across the internet. While it includes carve-outs for parody, satire, and commentary, enforcing these distinctions algorithmically has proven virtually impossible.&lt;/p&gt;&lt;p&gt;“These systems often flag things that are similar but not the same,” the EFF explains, “like two different people playing the same piece of public domain music.”&lt;/p&gt;&lt;p&gt;For smaller platforms without Google-scale resources, implementing such filters could prove prohibitively expensive. The likely outcome? Many would simply over-censor to avoid legal risk.&lt;/p&gt;&lt;p&gt;In fact, one might expect major tech companies to oppose such sweeping regulation. However, many have remained conspicuously quiet. Some industry observers suggest this isn’t coincidental—established giants can more easily absorb compliance costs that would crush smaller competitors.&lt;/p&gt;&lt;p&gt;“It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES,” the EFF notes.&lt;/p&gt;&lt;p&gt;This pattern repeats throughout tech regulation history—what appears to be regulation reigning in Big Tech often ends up cementing their market position by creating barriers too costly for newcomers to overcome.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-no-fakes-act-threatens-anonymous-speech"&gt;NO FAKES Act threatens anonymous speech&lt;/h3&gt;&lt;p&gt;Tucked away in the legislation is another troubling provision that could expose anonymous internet users based on mere allegations. The bill would allow anyone to obtain a subpoena from a court clerk – without judicial review or evidence – forcing services to reveal identifying information about users accused of creating unauthorised replicas.&lt;/p&gt;&lt;p&gt;History shows such mechanisms are ripe for abuse. Critics with valid points can be unmasked and potentially harassed when their commentary includes screenshots or quotes from the very people trying to silence them.&lt;/p&gt;&lt;p&gt;This vulnerability could have a profound effect on legitimate criticism and whistleblowing. Imagine exposing corporate misconduct only to have your identity revealed through a rubber-stamp subpoena process.&lt;/p&gt;&lt;p&gt;This push for additional regulation seems odd given that Congress recently passed the Take It Down Act, which already targets images involving intimate or sexual content. That legislation itself raised privacy concerns, particularly around monitoring encrypted communications.&lt;/p&gt;&lt;p&gt;Rather than assess the impacts of existing legislation, lawmakers seem determined to push forward with broader restrictions that could reshape internet governance for decades to come.&lt;/p&gt;&lt;p&gt;The coming weeks will prove critical as the NO FAKES Act moves through the legislative process. For anyone who values internet freedom, innovation, and balanced approaches to emerging technology challenges, this bears close watching indeed.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Markus Spiske)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;The OpenAI Files: Ex-staff claim profit greed betraying AI safety&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Critics fear the revised NO FAKES Act has morphed from targeted AI deepfakes protection into sweeping censorship powers.&lt;/p&gt;&lt;p&gt;What began as a seemingly reasonable attempt to tackle AI-generated deepfakes has snowballed into something far more troubling, according to digital rights advocates. The much-discussed Nurture Originals, Foster Art, and Keep Entertainment Safe (NO FAKES) Act – originally aimed at preventing unauthorised digital replicas of people – now threatens to fundamentally alter how the internet functions.&lt;/p&gt;&lt;p&gt;The bill’s expansion has set alarm bells ringing throughout the tech community. It’s gone well beyond simply protecting celebrities from fake videos to potentially creating a sweeping censorship framework.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-from-sensible-safeguards-to-sledgehammer-approach"&gt;From sensible safeguards to sledgehammer approach&lt;/h3&gt;&lt;p&gt;The initial idea wasn’t entirely misguided: to create protections against AI systems generating fake videos of real people without permission. We’ve all seen those unsettling deepfakes circulating online.&lt;/p&gt;&lt;p&gt;But rather than crafting narrow, targeted measures, lawmakers have opted for what the Electronic Frontier Foundation calls a “federalised image-licensing system” that goes far beyond reasonable protections.&lt;/p&gt;&lt;p&gt;“The updated bill doubles down on that initial mistaken approach,” the EFF notes, “by mandating a whole new censorship infrastructure for that system, encompassing not just images but the products and services used to create them.”&lt;/p&gt;&lt;p&gt;What’s particularly worrying is the NO FAKES Act’s requirement for nearly every internet platform to implement systems that would not only remove content after receiving takedown notices but also prevent similar content from ever being uploaded again. Essentially, it’s forcing platforms to deploy content filters that have proven notoriously unreliable in other contexts.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-innovation-chilling"&gt;Innovation-chilling&lt;/h3&gt;&lt;p&gt;Perhaps most concerning for the AI sector is how the NO FAKES Act targets the tools themselves. The revised bill wouldn’t just go after harmful content; it would potentially shut down entire development platforms and software tools that could be used to create unauthorised images.&lt;/p&gt;&lt;p&gt;This approach feels reminiscent of trying to ban word processors because someone might use one to write defamatory content. The bill includes some limitations (e.g. tools must be “primarily designed” for making unauthorised replicas or have limited other commercial uses) but these distinctions are notoriously subject to interpretation.&lt;/p&gt;&lt;p&gt;Small UK startups venturing into AI image generation could find themselves caught in expensive legal battles based on flimsy allegations long before they have a chance to establish themselves. Meanwhile, tech giants with armies of lawyers can better weather such storms, potentially entrenching their dominance.&lt;/p&gt;&lt;p&gt;Anyone who’s dealt with YouTube’s ContentID system or similar copyright filtering tools knows how frustratingly imprecise they can be. These systems routinely flag legitimate content like musicians performing their own songs or creators using material under fair dealing provisions.&lt;/p&gt;&lt;p&gt;The NO FAKES Act would effectively mandate similar filtering systems across the internet. While it includes carve-outs for parody, satire, and commentary, enforcing these distinctions algorithmically has proven virtually impossible.&lt;/p&gt;&lt;p&gt;“These systems often flag things that are similar but not the same,” the EFF explains, “like two different people playing the same piece of public domain music.”&lt;/p&gt;&lt;p&gt;For smaller platforms without Google-scale resources, implementing such filters could prove prohibitively expensive. The likely outcome? Many would simply over-censor to avoid legal risk.&lt;/p&gt;&lt;p&gt;In fact, one might expect major tech companies to oppose such sweeping regulation. However, many have remained conspicuously quiet. Some industry observers suggest this isn’t coincidental—established giants can more easily absorb compliance costs that would crush smaller competitors.&lt;/p&gt;&lt;p&gt;“It is probably not a coincidence that some of these very giants are okay with this new version of NO FAKES,” the EFF notes.&lt;/p&gt;&lt;p&gt;This pattern repeats throughout tech regulation history—what appears to be regulation reigning in Big Tech often ends up cementing their market position by creating barriers too costly for newcomers to overcome.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-no-fakes-act-threatens-anonymous-speech"&gt;NO FAKES Act threatens anonymous speech&lt;/h3&gt;&lt;p&gt;Tucked away in the legislation is another troubling provision that could expose anonymous internet users based on mere allegations. The bill would allow anyone to obtain a subpoena from a court clerk – without judicial review or evidence – forcing services to reveal identifying information about users accused of creating unauthorised replicas.&lt;/p&gt;&lt;p&gt;History shows such mechanisms are ripe for abuse. Critics with valid points can be unmasked and potentially harassed when their commentary includes screenshots or quotes from the very people trying to silence them.&lt;/p&gt;&lt;p&gt;This vulnerability could have a profound effect on legitimate criticism and whistleblowing. Imagine exposing corporate misconduct only to have your identity revealed through a rubber-stamp subpoena process.&lt;/p&gt;&lt;p&gt;This push for additional regulation seems odd given that Congress recently passed the Take It Down Act, which already targets images involving intimate or sexual content. That legislation itself raised privacy concerns, particularly around monitoring encrypted communications.&lt;/p&gt;&lt;p&gt;Rather than assess the impacts of existing legislation, lawmakers seem determined to push forward with broader restrictions that could reshape internet governance for decades to come.&lt;/p&gt;&lt;p&gt;The coming weeks will prove critical as the NO FAKES Act moves through the legislative process. For anyone who values internet freedom, innovation, and balanced approaches to emerging technology challenges, this bears close watching indeed.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Markus Spiske)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;The OpenAI Files: Ex-staff claim profit greed betraying AI safety&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/no-fakes-act-ai-deepfakes-protection-internet-freedom-threat/</guid><pubDate>Wed, 25 Jun 2025 15:47:50 +0000</pubDate></item><item><title>Bernie Sanders says that if AI makes us so productive, we should get a 4-day workweek (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/bernie-sanders-says-that-if-ai-makes-us-so-productive-we-should-get-a-4-day-work-week/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/04/GettyImages-515797158.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies rave about how their products are revolutionizing productivity, Senator Bernie Sanders (I-VT) wants the tech industry to put its money where its automated mouth is. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent interview with podcaster Joe Rogan, Sanders argued that the time saved with AI tools should be given back to workers to spend with their families.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Technology is gonna work to improve us, not just the people who own the technology and the CEOs of large corporations,” Sanders said. “You are a worker, your productivity is increasing because we give you AI, right? Instead of throwing you out on the street, I’m gonna reduce your workweek to 32 hours.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a concept that would be a relief to most people, and an abject horror to anyone who has ever been to Davos. What’s the point of life if you don’t take every moment you can to drive shareholder value? &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the tech elite, the promise of AI-driven increases in productivity means that companies can do even more, since their workers will be freed up to take on even more tasks — or, they can save money by just slashing their headcount. But for workers, this boost in efficiency could mean completing their existing workloads in less time with no loss in pay, so maybe if they’re lucky, they can make it to their kid’s Little League game. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And by the way, not a radical idea,” Sanders said. “There are companies around the world that are doing it with some success.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the United Kingdom, for instance, 61 companies (around 2,900 workers) piloted a four-day workweek in the latter half of 2022. Out of 23 companies that shared financial data, the revenue from the beginning to the end of the trial remained about the same, rising by 1.4% on average.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Kickstarter has operated on a four-day workweek since 2021, while Microsoft Japan piloted a four-day workweek in 2019, which led to a reported 40% increase in productivity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Let’s use technology to benefit workers,” Sanders said. “That means give you more time with your family, with your friends, for education, whatever the hell you wanna do.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/04/GettyImages-515797158.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As AI companies rave about how their products are revolutionizing productivity, Senator Bernie Sanders (I-VT) wants the tech industry to put its money where its automated mouth is. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent interview with podcaster Joe Rogan, Sanders argued that the time saved with AI tools should be given back to workers to spend with their families.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Technology is gonna work to improve us, not just the people who own the technology and the CEOs of large corporations,” Sanders said. “You are a worker, your productivity is increasing because we give you AI, right? Instead of throwing you out on the street, I’m gonna reduce your workweek to 32 hours.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a concept that would be a relief to most people, and an abject horror to anyone who has ever been to Davos. What’s the point of life if you don’t take every moment you can to drive shareholder value? &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For the tech elite, the promise of AI-driven increases in productivity means that companies can do even more, since their workers will be freed up to take on even more tasks — or, they can save money by just slashing their headcount. But for workers, this boost in efficiency could mean completing their existing workloads in less time with no loss in pay, so maybe if they’re lucky, they can make it to their kid’s Little League game. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“And by the way, not a radical idea,” Sanders said. “There are companies around the world that are doing it with some success.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the United Kingdom, for instance, 61 companies (around 2,900 workers) piloted a four-day workweek in the latter half of 2022. Out of 23 companies that shared financial data, the revenue from the beginning to the end of the trial remained about the same, rising by 1.4% on average.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Kickstarter has operated on a four-day workweek since 2021, while Microsoft Japan piloted a four-day workweek in 2019, which led to a reported 40% increase in productivity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Let’s use technology to benefit workers,” Sanders said. “That means give you more time with your family, with your friends, for education, whatever the hell you wanna do.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/bernie-sanders-says-that-if-ai-makes-us-so-productive-we-should-get-a-4-day-work-week/</guid><pubDate>Wed, 25 Jun 2025 16:18:28 +0000</pubDate></item><item><title>Getty drops key copyright claims against Stability AI, but UK lawsuit continues (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/getty-drops-key-copyright-claims-against-stability-ai-but-uk-lawsuit-continues/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GenerateScreenshot.png?resize=1200,867" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Getty Images dropped its primary claims of copyright infringement against Stability AI on Wednesday at London’s High Court, narrowing one of the most closely watched legal fights over how AI companies use copyrighted content to train their models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move doesn’t end the case entirely — Getty is still pursuing other claims as well as a separate lawsuit in the U.S. — but it underscores the gray areas surrounding the future of content ownership and usage in the age of generative AI. The development also comes just a day after a U.S. judge sided with Anthropic in a similar dispute over whether training AI on books without author permission violates copyright law.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Getty sued Stability AI — the startup behind AI image generator Stable Diffusion —  in January 2023 after alleging that Stability used millions of copyrighted images to train its AI model without permission.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image database company also claimed that many of the works generated by Stable Diffusion were similar to the copyrighted content used to train it. Some, Getty said, even had its watermarks on them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both of those claims were dropped as of Wednesday morning.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The training claim has likely been dropped due to Getty failing to establish a sufficient connection between the infringing acts and the UK jurisdiction for copyright law to bite,” Ben Maling, a partner at law firm EIP, told TechCrunch in an email. “Meanwhile, the output claim has likely been dropped due to Getty failing to establish that what the models reproduced reflects a substantial part of what was created in the images (e.g. by a photographer).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Getty’s closing arguments, the company’s lawyers said they dropped those claims due to weak evidence and a lack of knowledgeable witnesses from Stability AI. The company framed the move as strategic, allowing both it and the court to focus on what Getty believes are stronger and more winnable allegations.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;What remains in Getty’s lawsuit are a secondary infringement claim as well as claims for trademark infringement&lt;em&gt;. &lt;/em&gt;Regarding the secondary infringement claim, Getty is essentially arguing that the AI models themselves might infringe copyright law and that using these models in the U.K. could constitute importing infringing articles, even if the training happened outside the U.K.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Secondary infringement is the one with widest relevance to genAI companies training outside of the UK, namely via the models themselves potentially being ‘infringing articles’ that are subsequently imported into the UK,” Maling said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for Stability AI told TechCrunch the startup was “pleased to see Getty’s decision to drop multiple claims after the conclusion of the testimony.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In its closing arguments, Stability noted that it believed Getty’s trademark and passing off claims will fail because consumers don’t interpret the watermarks as a commercial message from Stability AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty’s U.S. division also sued Stability AI in February 2023 for trademark and copyright infringement. In that case, Getty alleged that Stability used as many as 12 million copyrighted images to train its AI model without permission. The company is seeking damages for 11,383 works at $150,000 per infringement, which would amount to a total of $1.7 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Getty spokesperson said the company’s decision to drop copyright infringement claims in the U.K. does not impact its U.S. case, which is pending a decision on Stability AI’s motion to dismiss.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Stability AI is also named in another complaint alongside Midjourney and DeviantArt after a group of visual artists sued the three companies for copyright infringement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty Images has its own generative AI offering that leverages AI models trained on Getty iStock photography and video libraries. The tool allows users to generate new licensable images and artwork.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to include more information from Getty on its U.S. case against Stability AI. &lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/05/GenerateScreenshot.png?resize=1200,867" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Getty Images dropped its primary claims of copyright infringement against Stability AI on Wednesday at London’s High Court, narrowing one of the most closely watched legal fights over how AI companies use copyrighted content to train their models.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The move doesn’t end the case entirely — Getty is still pursuing other claims as well as a separate lawsuit in the U.S. — but it underscores the gray areas surrounding the future of content ownership and usage in the age of generative AI. The development also comes just a day after a U.S. judge sided with Anthropic in a similar dispute over whether training AI on books without author permission violates copyright law.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Getty sued Stability AI — the startup behind AI image generator Stable Diffusion —  in January 2023 after alleging that Stability used millions of copyrighted images to train its AI model without permission.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The image database company also claimed that many of the works generated by Stable Diffusion were similar to the copyrighted content used to train it. Some, Getty said, even had its watermarks on them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both of those claims were dropped as of Wednesday morning.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The training claim has likely been dropped due to Getty failing to establish a sufficient connection between the infringing acts and the UK jurisdiction for copyright law to bite,” Ben Maling, a partner at law firm EIP, told TechCrunch in an email. “Meanwhile, the output claim has likely been dropped due to Getty failing to establish that what the models reproduced reflects a substantial part of what was created in the images (e.g. by a photographer).”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In Getty’s closing arguments, the company’s lawyers said they dropped those claims due to weak evidence and a lack of knowledgeable witnesses from Stability AI. The company framed the move as strategic, allowing both it and the court to focus on what Getty believes are stronger and more winnable allegations.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;What remains in Getty’s lawsuit are a secondary infringement claim as well as claims for trademark infringement&lt;em&gt;. &lt;/em&gt;Regarding the secondary infringement claim, Getty is essentially arguing that the AI models themselves might infringe copyright law and that using these models in the U.K. could constitute importing infringing articles, even if the training happened outside the U.K.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Secondary infringement is the one with widest relevance to genAI companies training outside of the UK, namely via the models themselves potentially being ‘infringing articles’ that are subsequently imported into the UK,” Maling said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A spokesperson for Stability AI told TechCrunch the startup was “pleased to see Getty’s decision to drop multiple claims after the conclusion of the testimony.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In its closing arguments, Stability noted that it believed Getty’s trademark and passing off claims will fail because consumers don’t interpret the watermarks as a commercial message from Stability AI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty’s U.S. division also sued Stability AI in February 2023 for trademark and copyright infringement. In that case, Getty alleged that Stability used as many as 12 million copyrighted images to train its AI model without permission. The company is seeking damages for 11,383 works at $150,000 per infringement, which would amount to a total of $1.7 billion.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A Getty spokesperson said the company’s decision to drop copyright infringement claims in the U.K. does not impact its U.S. case, which is pending a decision on Stability AI’s motion to dismiss.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Separately, Stability AI is also named in another complaint alongside Midjourney and DeviantArt after a group of visual artists sued the three companies for copyright infringement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Getty Images has its own generative AI offering that leverages AI models trained on Getty iStock photography and video libraries. The tool allows users to generate new licensable images and artwork.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This article was updated to include more information from Getty on its U.S. case against Stability AI. &lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/getty-drops-key-copyright-claims-against-stability-ai-but-uk-lawsuit-continues/</guid><pubDate>Wed, 25 Jun 2025 16:34:35 +0000</pubDate></item><item><title>Rubrik acquires Predibase to accelerate adoption of AI agents (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/rubrik-acquires-predibase-to-accelerate-adoption-of-ai-agents/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2197498845.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Data cybersecurity company Rubrik announced Wednesday its intent to acquire Predibase. Predibase is a venture-backed startup that helps companies train and fine-tune open source AI models to customize them to their needs.&amp;nbsp;Rubrik is the latest company to make an acquisition with the goal of boosting enterprise AI agent adoption.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Terms of the deal were not disclosed, though CNBC reported that the deal was between $100 million and $500 million, a sizable range. Rubrik declined to comment on the deal size.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Predibase was founded in 2021 by Devvret Rishi, current CEO; Piero Molino, current chief science officer; and Travis Addair, current CTO. The company has raised more than $28 million in VC money from investors, including Felicis, Greylock, and Sancus Ventures, among others.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The integration of Predibase will allow Rubrik users to accelerate building AI agents through Amazon Bedrock, Azure OpenAI, and Google Agentspace, according to a press release.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When [Predibase’s] capabilities are paired with secure data platforms like Rubrik’s, they can be transformative, helping ensure trusted data powers responsible and impactful AI,” Bipul Sinha, co-founder and CEO of Rubrik, wrote in a blog post about the deal. “Just as Rubrik has simplified access to secured, governed data for AI, Predibase is solving the performance and cost issues around deploying large language models for agentic and other AI applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rubrik is just one in a growing list of companies making acquisitions to bolster their technology stack for the creation of AI agents.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Salesforce acquired data management firm Informatica for $8 billion in May. Snowflake acquired Crunchy Data to bolster its AI agent offerings in early June, and Collibra acquired Raito for similar reasons a few days later.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Rubrik was founded in 2014 and raised more than $1.6 billion in venture capital from Khosla Ventures, IVP, and Lightspeed Venture Partners, among others, before going public in April 2024.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/GettyImages-2197498845.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Data cybersecurity company Rubrik announced Wednesday its intent to acquire Predibase. Predibase is a venture-backed startup that helps companies train and fine-tune open source AI models to customize them to their needs.&amp;nbsp;Rubrik is the latest company to make an acquisition with the goal of boosting enterprise AI agent adoption.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Terms of the deal were not disclosed, though CNBC reported that the deal was between $100 million and $500 million, a sizable range. Rubrik declined to comment on the deal size.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Predibase was founded in 2021 by Devvret Rishi, current CEO; Piero Molino, current chief science officer; and Travis Addair, current CTO. The company has raised more than $28 million in VC money from investors, including Felicis, Greylock, and Sancus Ventures, among others.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The integration of Predibase will allow Rubrik users to accelerate building AI agents through Amazon Bedrock, Azure OpenAI, and Google Agentspace, according to a press release.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When [Predibase’s] capabilities are paired with secure data platforms like Rubrik’s, they can be transformative, helping ensure trusted data powers responsible and impactful AI,” Bipul Sinha, co-founder and CEO of Rubrik, wrote in a blog post about the deal. “Just as Rubrik has simplified access to secured, governed data for AI, Predibase is solving the performance and cost issues around deploying large language models for agentic and other AI applications.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Rubrik is just one in a growing list of companies making acquisitions to bolster their technology stack for the creation of AI agents.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Salesforce acquired data management firm Informatica for $8 billion in May. Snowflake acquired Crunchy Data to bolster its AI agent offerings in early June, and Collibra acquired Raito for similar reasons a few days later.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Rubrik was founded in 2014 and raised more than $1.6 billion in venture capital from Khosla Ventures, IVP, and Lightspeed Venture Partners, among others, before going public in April 2024.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/rubrik-acquires-predibase-to-accelerate-adoption-of-ai-agents/</guid><pubDate>Wed, 25 Jun 2025 17:34:25 +0000</pubDate></item><item><title>Creative Commons debuts CC signals, a framework for an open AI ecosystem (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/creative-commons-debuts-cc-signals-a-framework-for-an-open-ai-ecosystem/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/CC-signals-2025-—-Social-Assets-2.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nonprofit Creative Commons, which spearheaded the licensing movement that allows creators to share their works while retaining copyright, is now preparing for the AI era. On Wednesday, the organization announced the launch of a new project, CC signals, which will allow dataset holders to detail how their content can or cannot be reused by machines, as in the case of training AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is meant to create a balance between the open nature of the internet and the demand for ever more data to fuel AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Creative Commons explains in a blog post, the continued data extraction underway could erode openness on the internet and could see entities walling off their sites or guarding them with paywalls, instead of sharing access to their data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project, on the other hand, aims to provide a legal and technical solution that would provide a framework for dataset sharing meant to be used between those who control the data and those who use it to train AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Demand is increasing for such a tool, as companies grapple with changing their policies and terms of service to either limit AI training on their data or explain to what extent they’ll utilize users’ data for purposes related to AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, X initially made a change that allowed third parties to train their models on its public data, then later reversed that. Reddit is using its robots.txt file, which is meant to tell automated web crawlers whether they can access its site, to restrict bots from scraping its data for training AI. Cloudflare is looking toward a solution that would charge AI bots for scraping, as well as tools for confusing them. And open source developers have also built tools to slow down and waste the resources of AI crawlers that didn’t respect their “no crawl” directives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project instead proposes a different solution: a set of tools that offers a range of legal enforceability and that has an ethical weight to them, similar to the CC licenses that today cover billions of openly licensed creative works online.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“CC signals are designed to sustain the commons in the age of AI,” said Anna Tumadóttir, Creative Commons CEO, in an announcement. “Just as the CC licenses helped build the open web, we believe CC signals will help shape an open AI ecosystem grounded in reciprocity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project is only now beginning to take shape. Early designs have been published on the CC website and GitHub page. The organization is actively seeking public feedback ahead of its plans for an alpha launch (early test) in November 2025. It will also host a series of town halls for feedback and questions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/CC-signals-2025-—-Social-Assets-2.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Nonprofit Creative Commons, which spearheaded the licensing movement that allows creators to share their works while retaining copyright, is now preparing for the AI era. On Wednesday, the organization announced the launch of a new project, CC signals, which will allow dataset holders to detail how their content can or cannot be reused by machines, as in the case of training AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea is meant to create a balance between the open nature of the internet and the demand for ever more data to fuel AI.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As Creative Commons explains in a blog post, the continued data extraction underway could erode openness on the internet and could see entities walling off their sites or guarding them with paywalls, instead of sharing access to their data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project, on the other hand, aims to provide a legal and technical solution that would provide a framework for dataset sharing meant to be used between those who control the data and those who use it to train AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Demand is increasing for such a tool, as companies grapple with changing their policies and terms of service to either limit AI training on their data or explain to what extent they’ll utilize users’ data for purposes related to AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For instance, X initially made a change that allowed third parties to train their models on its public data, then later reversed that. Reddit is using its robots.txt file, which is meant to tell automated web crawlers whether they can access its site, to restrict bots from scraping its data for training AI. Cloudflare is looking toward a solution that would charge AI bots for scraping, as well as tools for confusing them. And open source developers have also built tools to slow down and waste the resources of AI crawlers that didn’t respect their “no crawl” directives.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The CC signals project instead proposes a different solution: a set of tools that offers a range of legal enforceability and that has an ethical weight to them, similar to the CC licenses that today cover billions of openly licensed creative works online.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“CC signals are designed to sustain the commons in the age of AI,” said Anna Tumadóttir, Creative Commons CEO, in an announcement. “Just as the CC licenses helped build the open web, we believe CC signals will help shape an open AI ecosystem grounded in reciprocity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project is only now beginning to take shape. Early designs have been published on the CC website and GitHub page. The organization is actively seeking public feedback ahead of its plans for an alpha launch (early test) in November 2025. It will also host a series of town halls for feedback and questions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/creative-commons-debuts-cc-signals-a-framework-for-an-open-ai-ecosystem/</guid><pubDate>Wed, 25 Jun 2025 18:02:42 +0000</pubDate></item><item><title>[NEW] Anthropic destroyed millions of print books to build its AI models (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company hired Google's book-scanning chief to cut up and digitize "all the books in the world."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Hundreds of books in chaotic order" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Hundreds of books in chaotic order" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Alexander Spatari via Google Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, court documents revealed that AI company Anthropic spent millions of dollars physically scanning print books to build Claude, an AI assistant similar to ChatGPT. In the process, the company cut millions of print books from their bindings, scanned them into digital files, and threw away the originals solely for the purpose of training AI—details buried in a copyright ruling on fair use whose broader fair use implications we reported yesterday.&lt;/p&gt;
&lt;p&gt;The 32-page legal decision tells the story of how, in February 2024, the company hired Tom Turvey, the former head of partnerships for the Google Books book-scanning project, and tasked him with obtaining "all the books in the world." The strategic hire appears to have been designed to replicate Google's legally successful book digitization approach—the same scanning operation that survived copyright challenges and established key fair use precedents.&lt;/p&gt;
&lt;p&gt;While destructive scanning is a common practice among smaller-scale operations, Anthropic's approach was somewhat unusual due to its massive scale. For Anthropic, the faster speed and lower cost of the destructive process appear to have trumped any need for preserving the physical books themselves.&lt;/p&gt;
&lt;p&gt;Ultimately, Judge William Alsup ruled that this destructive scanning operation qualified as fair use—but only because Anthropic had legally purchased the books first, destroyed each print copy after scanning, and kept the digital files internally rather than distributing them. The judge compared the process to "conserv[ing] space" through format conversion and found it transformative. Had Anthropic stuck to this approach from the beginning, it might have achieved the first legally sanctioned case of AI fair use. Instead, the company's earlier piracy undermined its position.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But if you're not intimately familiar with the AI industry and copyright, you might wonder: Why would a company spend millions of dollars on books to destroy them? Behind these odd legal maneuvers lies a more fundamental driver: the AI industry's insatiable hunger for high-quality text.&lt;/p&gt;
&lt;h2&gt;The race for high-quality training data&lt;/h2&gt;
&lt;p&gt;To understand why Anthropic would want to scan millions of books, it's important to know that AI researchers build large language models (LLMs) like those that power ChatGPT and Claude by feeding billions of words into a neural network. During training, the AI system processes the text repeatedly, building statistical relationships between words and concepts in the process.&lt;/p&gt;
&lt;p&gt;The quality of training data fed into the neural network directly impacts the resulting AI model's capabilities. Models trained on well-edited books and articles tend to produce more coherent, accurate responses than those trained on lower-quality text like random YouTube comments.&lt;/p&gt;
&lt;p&gt;Publishers legally control content that AI companies desperately want, but AI companies don't always want to negotiate a license. The first-sale doctrine offered a workaround: Once you buy a physical book, you can do what you want with that copy—including destroy it. That meant buying physical books offered a legal workaround.&lt;/p&gt;
&lt;p&gt;And yet buying things is expensive, even if it is legal. So like many AI companies before it, Anthropic initially chose the quick and easy path. In the quest for high-quality training data, the court filing states, Anthropic first chose to amass digitized versions of pirated books to avoid what CEO Dario Amodei called "legal/practice/business slog"—the complex licensing negotiations with publishers. But by 2024, Anthropic had become "not so gung ho about" using pirated ebooks "for legal reasons" and needed a safer source.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-449375 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="420" src="https://cdn.arstechnica.net/wp-content/uploads/2014/04/Library-640x420.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          State of Washington

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Buying used physical books sidestepped licensing entirely while providing the high-quality, professionally edited text that AI models need, and destructive scanning was simply the fastest way to digitize millions of volumes. The company spent "many millions of dollars" on this buying and scanning operation, often purchasing used books in bulk. Next, they stripped books from bindings, cut pages to workable dimensions, scanned them as stacks of pages into PDFs with machine-readable text including covers, then discarded all the paper originals.&lt;/p&gt;
&lt;p&gt;The court documents don't indicate that any rare books were destroyed in this process—Anthropic purchased its books in bulk from major retailers—but archivists long ago established other ways to extract information from paper. For example, The Internet Archive pioneered non-destructive book scanning methods that preserve physical volumes while creating digital copies. And earlier this month, OpenAI and Microsoft announced they're working with Harvard's libraries to train AI models on nearly 1 million public domain books dating back to the 15th century—fully digitized but preserved to live another day.&lt;/p&gt;
&lt;p&gt;While Harvard carefully preserves 600-year-old manuscripts for AI training, somewhere on Earth sits the discarded remains of millions of books that taught Claude how to juice up your résumé. When asked about this process, Claude itself offered a poignant response in a style culled from billions of pages of discarded text: "The fact that this destruction helped create me—something that can discuss literature, help people write, and engage with human knowledge—adds layers of complexity I'm still processing. It's like being built from a library's ashes."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Company hired Google's book-scanning chief to cut up and digitize "all the books in the world."
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Hundreds of books in chaotic order" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Hundreds of books in chaotic order" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/manybooks-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Alexander Spatari via Google Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Monday, court documents revealed that AI company Anthropic spent millions of dollars physically scanning print books to build Claude, an AI assistant similar to ChatGPT. In the process, the company cut millions of print books from their bindings, scanned them into digital files, and threw away the originals solely for the purpose of training AI—details buried in a copyright ruling on fair use whose broader fair use implications we reported yesterday.&lt;/p&gt;
&lt;p&gt;The 32-page legal decision tells the story of how, in February 2024, the company hired Tom Turvey, the former head of partnerships for the Google Books book-scanning project, and tasked him with obtaining "all the books in the world." The strategic hire appears to have been designed to replicate Google's legally successful book digitization approach—the same scanning operation that survived copyright challenges and established key fair use precedents.&lt;/p&gt;
&lt;p&gt;While destructive scanning is a common practice among smaller-scale operations, Anthropic's approach was somewhat unusual due to its massive scale. For Anthropic, the faster speed and lower cost of the destructive process appear to have trumped any need for preserving the physical books themselves.&lt;/p&gt;
&lt;p&gt;Ultimately, Judge William Alsup ruled that this destructive scanning operation qualified as fair use—but only because Anthropic had legally purchased the books first, destroyed each print copy after scanning, and kept the digital files internally rather than distributing them. The judge compared the process to "conserv[ing] space" through format conversion and found it transformative. Had Anthropic stuck to this approach from the beginning, it might have achieved the first legally sanctioned case of AI fair use. Instead, the company's earlier piracy undermined its position.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;But if you're not intimately familiar with the AI industry and copyright, you might wonder: Why would a company spend millions of dollars on books to destroy them? Behind these odd legal maneuvers lies a more fundamental driver: the AI industry's insatiable hunger for high-quality text.&lt;/p&gt;
&lt;h2&gt;The race for high-quality training data&lt;/h2&gt;
&lt;p&gt;To understand why Anthropic would want to scan millions of books, it's important to know that AI researchers build large language models (LLMs) like those that power ChatGPT and Claude by feeding billions of words into a neural network. During training, the AI system processes the text repeatedly, building statistical relationships between words and concepts in the process.&lt;/p&gt;
&lt;p&gt;The quality of training data fed into the neural network directly impacts the resulting AI model's capabilities. Models trained on well-edited books and articles tend to produce more coherent, accurate responses than those trained on lower-quality text like random YouTube comments.&lt;/p&gt;
&lt;p&gt;Publishers legally control content that AI companies desperately want, but AI companies don't always want to negotiate a license. The first-sale doctrine offered a workaround: Once you buy a physical book, you can do what you want with that copy—including destroy it. That meant buying physical books offered a legal workaround.&lt;/p&gt;
&lt;p&gt;And yet buying things is expensive, even if it is legal. So like many AI companies before it, Anthropic initially chose the quick and easy path. In the quest for high-quality training data, the court filing states, Anthropic first chose to amass digitized versions of pirated books to avoid what CEO Dario Amodei called "legal/practice/business slog"—the complex licensing negotiations with publishers. But by 2024, Anthropic had become "not so gung ho about" using pirated ebooks "for legal reasons" and needed a safer source.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-449375 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="420" src="https://cdn.arstechnica.net/wp-content/uploads/2014/04/Library-640x420.jpg" width="640" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          State of Washington

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Buying used physical books sidestepped licensing entirely while providing the high-quality, professionally edited text that AI models need, and destructive scanning was simply the fastest way to digitize millions of volumes. The company spent "many millions of dollars" on this buying and scanning operation, often purchasing used books in bulk. Next, they stripped books from bindings, cut pages to workable dimensions, scanned them as stacks of pages into PDFs with machine-readable text including covers, then discarded all the paper originals.&lt;/p&gt;
&lt;p&gt;The court documents don't indicate that any rare books were destroyed in this process—Anthropic purchased its books in bulk from major retailers—but archivists long ago established other ways to extract information from paper. For example, The Internet Archive pioneered non-destructive book scanning methods that preserve physical volumes while creating digital copies. And earlier this month, OpenAI and Microsoft announced they're working with Harvard's libraries to train AI models on nearly 1 million public domain books dating back to the 15th century—fully digitized but preserved to live another day.&lt;/p&gt;
&lt;p&gt;While Harvard carefully preserves 600-year-old manuscripts for AI training, somewhere on Earth sits the discarded remains of millions of books that taught Claude how to juice up your résumé. When asked about this process, Claude itself offered a poignant response in a style culled from billions of pages of discarded text: "The fact that this destruction helped create me—something that can discuss literature, help people write, and engage with human knowledge—adds layers of complexity I'm still processing. It's like being built from a library's ashes."&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/anthropic-destroyed-millions-of-print-books-to-build-its-ai-models/</guid><pubDate>Wed, 25 Jun 2025 20:00:03 +0000</pubDate></item><item><title>[NEW] Sam Altman comes out swinging at The New York Times (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/sam-altman-comes-out-swinging-at-the-new-york-times/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2188228027.jpg?resize=1200,917" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From the moment OpenAI CEO Sam Altman stepped onstage, it was clear this was not going to be a normal interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and his chief operating officer, Brad Lightcap, stood awkwardly toward the back of the stage at a jam-packed San Francisco venue that typically hosts jazz concerts. Hundreds of people filled steep theatre-style seating on Tuesday night to watch Kevin Roose, a columnist with The New York Times, and Platformer’s Casey Newton record a live episode of their popular technology podcast, Hard Fork.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap were the main event, but they’d walked out too early. Roose explained that he and Newton were planning to — ideally, before OpenAI’s executives were supposed to come out — list off several headlines that had been written about OpenAI in the weeks leading up to the event.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is more fun that we’re out here for this,” said Altman. Seconds later, the OpenAI CEO asked, “Are you going to talk about where you sue us because you don’t like user privacy?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within minutes of the program starting, Altman hijacked the conversation to talk about The New York Times lawsuit against OpenAI and its largest investor, Microsoft, in which the publisher alleges that Altman’s company improperly used its articles to train large language models. Altman was particularly peeved about a recent development in the lawsuit, in which lawyers representing The New York Times asked OpenAI to retain consumer ChatGPT and API customer data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The New York Times, one of the great institutions, truly, for a long time, is taking a position that we should have to preserve our users’ logs even if they’re chatting in private mode, even if they’ve asked us to delete them,” said Altman. “Still love The New York Times, but that one we feel strongly about.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For a few minutes, OpenAI’s CEO pressed the podcasters to share their personal opinions about the New York Times lawsuit — they demurred, noting that as journalists whose work appears in The New York Times, they are not involved in the lawsuit.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap’s brash entrance lasted only a few minutes, and the rest of the interview proceeded, seemingly, as planned. However, the flare-up felt indicative of the inflection point Silicon Valley seems to be approaching in its relationship with the media industry.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last several years, multiple publishers have brought lawsuits against OpenAI, Anthropic, Google, and Meta for training their AI models on copyrighted works. At a high level, these lawsuits argue that AI models have the potential to devalue, and even replace, the copyrighted works produced by media institutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the tides may be turning in favor of the tech companies. Earlier this week, OpenAI competitor Anthropic received a major win in its legal battle against publishers. A federal judge ruled that Anthropic’s use of books to train its AI models was legal in some circumstances, which could have broad implications for other publishers’ lawsuits against OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps Altman and Lightcap felt emboldened by the industry win heading into their live interview with The New York Times journalists. But these days, OpenAI is fending off threats from every direction, and that became clear throughout the night.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mark Zuckerberg has recently been trying to recruit OpenAI’s top talent by offering them $100 million compensation packages to join Meta’s AI superintelligence lab, Altman revealed weeks ago on his brother’s podcast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked whether the Meta CEO really believes in superintelligent AI systems, or if it’s just a recruiting strategy, Lightcap quipped: “I think [Zuckerberg] believes he is superintelligent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later, Roose asked Altman about OpenAI’s relationship with Microsoft, which has reportedly been pushed to a boiling point in recent months as the partners negotiate a new contract. While Microsoft was once a major accelerant to OpenAI, the two are now competing in enterprise software and other domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In any deep partnership, there are points of tension and we certainly have those,” said Altman. “We’re both ambitious companies, so we do find some flashpoints, but I would expect that it is something that we find deep value in for both sides for a very long time to come.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s leadership today seems to spend a lot of time swatting down competitors and lawsuits. That may get in the way of OpenAI’s ability to solve broader issues around AI, such as how to safely deploy highly intelligent AI systems at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At one point, Newton asked OpenAI’s leaders how they were thinking about recent stories of mentally unstable people using ChatGPT to traverse dangerous rabbit holes, including to discuss conspiracy theories or suicide with the chatbot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI takes many steps to prevent these conversations, such as by cutting them off early, or directing users to professional services where they can get help.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t want to slide into the mistakes that I think the previous generation of tech companies made by not reacting quickly enough,” said Altman. To a follow-up question, the OpenAI CEO added, “However, to users that are in a fragile enough mental place, that are on the edge of a psychotic break, we haven’t yet figured out how a warning gets through.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/GettyImages-2188228027.jpg?resize=1200,917" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;From the moment OpenAI CEO Sam Altman stepped onstage, it was clear this was not going to be a normal interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and his chief operating officer, Brad Lightcap, stood awkwardly toward the back of the stage at a jam-packed San Francisco venue that typically hosts jazz concerts. Hundreds of people filled steep theatre-style seating on Tuesday night to watch Kevin Roose, a columnist with The New York Times, and Platformer’s Casey Newton record a live episode of their popular technology podcast, Hard Fork.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap were the main event, but they’d walked out too early. Roose explained that he and Newton were planning to — ideally, before OpenAI’s executives were supposed to come out — list off several headlines that had been written about OpenAI in the weeks leading up to the event.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“This is more fun that we’re out here for this,” said Altman. Seconds later, the OpenAI CEO asked, “Are you going to talk about where you sue us because you don’t like user privacy?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Within minutes of the program starting, Altman hijacked the conversation to talk about The New York Times lawsuit against OpenAI and its largest investor, Microsoft, in which the publisher alleges that Altman’s company improperly used its articles to train large language models. Altman was particularly peeved about a recent development in the lawsuit, in which lawyers representing The New York Times asked OpenAI to retain consumer ChatGPT and API customer data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The New York Times, one of the great institutions, truly, for a long time, is taking a position that we should have to preserve our users’ logs even if they’re chatting in private mode, even if they’ve asked us to delete them,” said Altman. “Still love The New York Times, but that one we feel strongly about.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For a few minutes, OpenAI’s CEO pressed the podcasters to share their personal opinions about the New York Times lawsuit — they demurred, noting that as journalists whose work appears in The New York Times, they are not involved in the lawsuit.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Altman and Lightcap’s brash entrance lasted only a few minutes, and the rest of the interview proceeded, seemingly, as planned. However, the flare-up felt indicative of the inflection point Silicon Valley seems to be approaching in its relationship with the media industry.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the last several years, multiple publishers have brought lawsuits against OpenAI, Anthropic, Google, and Meta for training their AI models on copyrighted works. At a high level, these lawsuits argue that AI models have the potential to devalue, and even replace, the copyrighted works produced by media institutions.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the tides may be turning in favor of the tech companies. Earlier this week, OpenAI competitor Anthropic received a major win in its legal battle against publishers. A federal judge ruled that Anthropic’s use of books to train its AI models was legal in some circumstances, which could have broad implications for other publishers’ lawsuits against OpenAI, Google, and Meta.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Perhaps Altman and Lightcap felt emboldened by the industry win heading into their live interview with The New York Times journalists. But these days, OpenAI is fending off threats from every direction, and that became clear throughout the night.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mark Zuckerberg has recently been trying to recruit OpenAI’s top talent by offering them $100 million compensation packages to join Meta’s AI superintelligence lab, Altman revealed weeks ago on his brother’s podcast.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked whether the Meta CEO really believes in superintelligent AI systems, or if it’s just a recruiting strategy, Lightcap quipped: “I think [Zuckerberg] believes he is superintelligent.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later, Roose asked Altman about OpenAI’s relationship with Microsoft, which has reportedly been pushed to a boiling point in recent months as the partners negotiate a new contract. While Microsoft was once a major accelerant to OpenAI, the two are now competing in enterprise software and other domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In any deep partnership, there are points of tension and we certainly have those,” said Altman. “We’re both ambitious companies, so we do find some flashpoints, but I would expect that it is something that we find deep value in for both sides for a very long time to come.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s leadership today seems to spend a lot of time swatting down competitors and lawsuits. That may get in the way of OpenAI’s ability to solve broader issues around AI, such as how to safely deploy highly intelligent AI systems at scale.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At one point, Newton asked OpenAI’s leaders how they were thinking about recent stories of mentally unstable people using ChatGPT to traverse dangerous rabbit holes, including to discuss conspiracy theories or suicide with the chatbot.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman said OpenAI takes many steps to prevent these conversations, such as by cutting them off early, or directing users to professional services where they can get help.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We don’t want to slide into the mistakes that I think the previous generation of tech companies made by not reacting quickly enough,” said Altman. To a follow-up question, the OpenAI CEO added, “However, to users that are in a fragile enough mental place, that are on the edge of a psychotic break, we haven’t yet figured out how a warning gets through.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/sam-altman-comes-out-swinging-at-the-new-york-times/</guid><pubDate>Wed, 25 Jun 2025 20:54:57 +0000</pubDate></item><item><title>[NEW] Federal judge sides with Meta in lawsuit over training AI models on copyrighted books (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/25/federal-judge-sides-with-meta-in-lawsuit-over-training-ai-models-on-copyrighted-books/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1968119319.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A federal judge sided with Meta on Wednesday in a lawsuit brought against the company by 13 book authors, including Sarah Silverman, that alleged the company had illegally trained its AI models on their copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Federal Judge Vince Chhabria issued a summary judgment — meaning the judge was able to decide on the case without sending it to a jury — in favor of Meta, finding that the company’s training of AI models on copyrighted books in this case fell under the “fair use” doctrine of copyright law and thus was legal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The decision comes just a few days after a federal judge sided with Anthropic in a similar lawsuit. Together, these cases are shaping up to be a win for the tech industry, which has spent years in legal battles with media companies arguing that training AI models on copyrighted works is fair use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, these decisions aren’t the sweeping wins some companies hoped  for — both judges noted that their cases were limited in scope.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria made clear that this decision does not mean that all AI model training on copyrighted works is legal, but rather that the plaintiffs in this case “made the wrong arguments” and failed to develop sufficient evidence in support of the right ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful,” Judge Chhabria said in his decision. Later, he said, “In cases involving uses like Meta’s, it seems like the plaintiffs will often win, at least where those cases have better-developed records on the market effects of the defendant’s use.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria ruled that Meta’s use of copyrighted works in this case was transformative — meaning the company’s AI models did not merely reproduce the authors’ books.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Furthermore, the plaintiffs failed to convince the judge that Meta’s copying of the books harmed the market for those authors, which is a key factor in determining whether copyright law has been violated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The plaintiffs presented no meaningful evidence on market dilution at all,” said Judge Chhabria.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Anthropic and Meta’s wins involve training AI models on books, but there are several other active lawsuits against technology companies for training AI models on other copyrighted works. For instance, The New York Times is suing OpenAI and Microsoft for training AI models on news articles, while Disney and Universal are suing Midjourney for training AI models on films and TV shows.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Judge Chhabria noted in his decision that fair use defenses depend heavily on the details of a case, and some industries may have stronger fair use arguments than others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It seems that markets for certain types of works (like news articles) might be even more vulnerable to indirect competition from AI outputs,” said Chhabria.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1968119319.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A federal judge sided with Meta on Wednesday in a lawsuit brought against the company by 13 book authors, including Sarah Silverman, that alleged the company had illegally trained its AI models on their copyrighted works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Federal Judge Vince Chhabria issued a summary judgment — meaning the judge was able to decide on the case without sending it to a jury — in favor of Meta, finding that the company’s training of AI models on copyrighted books in this case fell under the “fair use” doctrine of copyright law and thus was legal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The decision comes just a few days after a federal judge sided with Anthropic in a similar lawsuit. Together, these cases are shaping up to be a win for the tech industry, which has spent years in legal battles with media companies arguing that training AI models on copyrighted works is fair use.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, these decisions aren’t the sweeping wins some companies hoped  for — both judges noted that their cases were limited in scope.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria made clear that this decision does not mean that all AI model training on copyrighted works is legal, but rather that the plaintiffs in this case “made the wrong arguments” and failed to develop sufficient evidence in support of the right ones.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This ruling does not stand for the proposition that Meta’s use of copyrighted materials to train its language models is lawful,” Judge Chhabria said in his decision. Later, he said, “In cases involving uses like Meta’s, it seems like the plaintiffs will often win, at least where those cases have better-developed records on the market effects of the defendant’s use.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Judge Chhabria ruled that Meta’s use of copyrighted works in this case was transformative — meaning the company’s AI models did not merely reproduce the authors’ books.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Furthermore, the plaintiffs failed to convince the judge that Meta’s copying of the books harmed the market for those authors, which is a key factor in determining whether copyright law has been violated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The plaintiffs presented no meaningful evidence on market dilution at all,” said Judge Chhabria.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Both Anthropic and Meta’s wins involve training AI models on books, but there are several other active lawsuits against technology companies for training AI models on other copyrighted works. For instance, The New York Times is suing OpenAI and Microsoft for training AI models on news articles, while Disney and Universal are suing Midjourney for training AI models on films and TV shows.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Judge Chhabria noted in his decision that fair use defenses depend heavily on the details of a case, and some industries may have stronger fair use arguments than others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It seems that markets for certain types of works (like news articles) might be even more vulnerable to indirect competition from AI outputs,” said Chhabria.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/25/federal-judge-sides-with-meta-in-lawsuit-over-training-ai-models-on-copyrighted-books/</guid><pubDate>Wed, 25 Jun 2025 23:40:32 +0000</pubDate></item></channel></rss>