<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 18 Jun 2025 01:48:52 +0000</lastBuildDate><item><title>[NEW]  ()</title><link>https://www.wired.com/feed/category/artificial-intelligence/rss</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://www.wired.com/feed/category/artificial-intelligence/rss</guid></item><item><title>Hexagon Taps NVIDIA Robotics and AI Software to Build and Deploy AEON, a New Humanoid (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/hexagon-robotics-ai-software-aeon-humanoid/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;As a global labor shortage leaves 50 million positions unfilled across industries like manufacturing and logistics, Hexagon — a global leader in measurement technologies — is developing humanoid robots that can lend a helping hand.&lt;/p&gt;
&lt;p&gt;Industrial sectors depend on skilled workers to perform a variety of error-prone tasks, including operating high-precision scanners for reality capture — the process of capturing digital data to replicate the real world in simulation.&lt;/p&gt;
&lt;p&gt;At the Hexagon LIVE Global conference, Hexagon’s robotics division today unveiled AEON — a new humanoid robot built in collaboration with NVIDIA that’s engineered to perform a wide range of industrial applications, from manipulation and asset inspection to reality capture and operator support. Hexagon plans to deploy AEON across automotive, transportation, aerospace, manufacturing, warehousing and logistics.&lt;/p&gt;
&lt;p&gt;Future use cases for AEON include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Reality capture&lt;/b&gt;, which involves automatic planning and then scanning of assets, industrial spaces and environments to generate 3D models. The captured data is then used for advanced visualization and collaboration in the Hexagon Digital Reality (HxDR) platform powering Hexagon Reality Cloud Studio (RCS).&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Manipulation &lt;/b&gt;tasks, such as sorting and moving parts in various industrial and manufacturing settings.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Part inspection&lt;/b&gt;, which includes checking parts for defects or ensuring adherence to specifications.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Industrial operations&lt;/b&gt;, including highly dexterous technical tasks like machinery operations, teleoperation and scanning parts using high-end scanners.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“The age of general-purpose robotics has arrived, due to technological advances in simulation and physical AI,” said Deepu Talla, vice president of robotics and edge AI at NVIDIA. “Hexagon’s new AEON humanoid embodies the integration of NVIDIA’s three-computer robotics platform and is making a significant leap forward in addressing industry-critical challenges.”&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="size-large wp-image-82402 alignnone" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/hexagon-car-door-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Using NVIDIA’s Three Computers to Develop AEON&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To build AEON, Hexagon used NVIDIA’s three computers for developing and deploying physical AI systems. They include AI supercomputers to train and fine-tune powerful foundation models; the NVIDIA Omniverse platform, running on NVIDIA OVX servers, for testing and optimizing these models in simulation environments using real and physically based synthetic data; and NVIDIA IGX Thor robotic computers to run the models.&lt;/p&gt;
&lt;p&gt;Hexagon is exploring using NVIDIA accelerated computing to post-train the NVIDIA Isaac GR00T N1.5 open foundation model to improve robot reasoning and policies, and tapping Isaac GR00T-Mimic to generate vast amounts of synthetic motion data from a few human demonstrations.&lt;/p&gt;
&lt;p&gt;AEON learns many of its skills through simulations powered by the NVIDIA Isaac platform. Hexagon uses NVIDIA Isaac Sim, a reference robotic simulation application built on Omniverse, to simulate complex robot actions like navigation, locomotion and manipulation. These skills are then refined using reinforcement learning in NVIDIA Isaac Lab, an open-source framework for robot learning.&lt;/p&gt;

&lt;p&gt;This simulation-first approach enabled Hexagon to fast-track its robotic development, allowing AEON to master core locomotion skills in just 2-3 weeks — rather than 5-6 months — before real-world deployment.&lt;/p&gt;
&lt;p&gt;In addition, AEON taps into NVIDIA Jetson Orin onboard computers to autonomously move, navigate and perform its tasks in real time, enhancing its speed and accuracy while operating in complex and dynamic environments. Hexagon is also planning to upgrade AEON with NVIDIA IGX Thor to enable functional safety for collaborative operation.&lt;/p&gt;
&lt;p&gt;“Our goal with AEON was to design an intelligent, autonomous humanoid that addresses the real-world challenges industrial leaders have shared with us over the past months,” said Arnaud Robert, president of Hexagon’s robotics division. “By leveraging NVIDIA’s full-stack robotics and simulation platforms, we were able to deliver a best-in-class humanoid that combines advanced mechatronics, multimodal sensor fusion and real-time AI.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Data Comes to Life Through Reality Capture and Omniverse Integration&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AEON will be piloted in factories and warehouses to scan everything from small precision parts and automotive components to large assembly lines and storage areas.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-large wp-image-82408" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/hexagon-aeon-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Captured data comes to life in RCS, a platform that allows users to collaborate, visualize and share reality-capture data by tapping into HxDR and NVIDIA Omniverse running in the cloud. This removes the constraint of local infrastructure.&lt;/p&gt;
&lt;p&gt;“Digital twins offer clear advantages, but adoption has been challenging in several industries,” said Lucas Heinzle, vice president of research and development at Hexagon’s robotics division. “AEON’s sophisticated sensor suite enables the integration of reality data capture with NVIDIA Omniverse, streamlining workflows for our customers and moving us closer to making digital twins a mainstream tool for collaboration and innovation.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;AEON’s Next Steps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;By adopting the OpenUSD framework and developing on Omniverse, Hexagon can generate high-fidelity digital twins from scanned data — establishing a data flywheel to continuously train AEON.&lt;/p&gt;
&lt;p&gt;This latest work with Hexagon is helping shape the future of physical AI — delivering scalable, efficient solutions to address the challenges faced by industries that depend on capturing real-world data.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch the &lt;/i&gt;&lt;i&gt;Hexagon LIVE keynote&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;explore presentations&lt;/i&gt;&lt;i&gt; and read more about AEON.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;All imagery courtesy of Hexagon.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;As a global labor shortage leaves 50 million positions unfilled across industries like manufacturing and logistics, Hexagon — a global leader in measurement technologies — is developing humanoid robots that can lend a helping hand.&lt;/p&gt;
&lt;p&gt;Industrial sectors depend on skilled workers to perform a variety of error-prone tasks, including operating high-precision scanners for reality capture — the process of capturing digital data to replicate the real world in simulation.&lt;/p&gt;
&lt;p&gt;At the Hexagon LIVE Global conference, Hexagon’s robotics division today unveiled AEON — a new humanoid robot built in collaboration with NVIDIA that’s engineered to perform a wide range of industrial applications, from manipulation and asset inspection to reality capture and operator support. Hexagon plans to deploy AEON across automotive, transportation, aerospace, manufacturing, warehousing and logistics.&lt;/p&gt;
&lt;p&gt;Future use cases for AEON include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Reality capture&lt;/b&gt;, which involves automatic planning and then scanning of assets, industrial spaces and environments to generate 3D models. The captured data is then used for advanced visualization and collaboration in the Hexagon Digital Reality (HxDR) platform powering Hexagon Reality Cloud Studio (RCS).&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Manipulation &lt;/b&gt;tasks, such as sorting and moving parts in various industrial and manufacturing settings.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Part inspection&lt;/b&gt;, which includes checking parts for defects or ensuring adherence to specifications.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Industrial operations&lt;/b&gt;, including highly dexterous technical tasks like machinery operations, teleoperation and scanning parts using high-end scanners.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“The age of general-purpose robotics has arrived, due to technological advances in simulation and physical AI,” said Deepu Talla, vice president of robotics and edge AI at NVIDIA. “Hexagon’s new AEON humanoid embodies the integration of NVIDIA’s three-computer robotics platform and is making a significant leap forward in addressing industry-critical challenges.”&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="size-large wp-image-82402 alignnone" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/hexagon-car-door-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Using NVIDIA’s Three Computers to Develop AEON&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;To build AEON, Hexagon used NVIDIA’s three computers for developing and deploying physical AI systems. They include AI supercomputers to train and fine-tune powerful foundation models; the NVIDIA Omniverse platform, running on NVIDIA OVX servers, for testing and optimizing these models in simulation environments using real and physically based synthetic data; and NVIDIA IGX Thor robotic computers to run the models.&lt;/p&gt;
&lt;p&gt;Hexagon is exploring using NVIDIA accelerated computing to post-train the NVIDIA Isaac GR00T N1.5 open foundation model to improve robot reasoning and policies, and tapping Isaac GR00T-Mimic to generate vast amounts of synthetic motion data from a few human demonstrations.&lt;/p&gt;
&lt;p&gt;AEON learns many of its skills through simulations powered by the NVIDIA Isaac platform. Hexagon uses NVIDIA Isaac Sim, a reference robotic simulation application built on Omniverse, to simulate complex robot actions like navigation, locomotion and manipulation. These skills are then refined using reinforcement learning in NVIDIA Isaac Lab, an open-source framework for robot learning.&lt;/p&gt;

&lt;p&gt;This simulation-first approach enabled Hexagon to fast-track its robotic development, allowing AEON to master core locomotion skills in just 2-3 weeks — rather than 5-6 months — before real-world deployment.&lt;/p&gt;
&lt;p&gt;In addition, AEON taps into NVIDIA Jetson Orin onboard computers to autonomously move, navigate and perform its tasks in real time, enhancing its speed and accuracy while operating in complex and dynamic environments. Hexagon is also planning to upgrade AEON with NVIDIA IGX Thor to enable functional safety for collaborative operation.&lt;/p&gt;
&lt;p&gt;“Our goal with AEON was to design an intelligent, autonomous humanoid that addresses the real-world challenges industrial leaders have shared with us over the past months,” said Arnaud Robert, president of Hexagon’s robotics division. “By leveraging NVIDIA’s full-stack robotics and simulation platforms, we were able to deliver a best-in-class humanoid that combines advanced mechatronics, multimodal sensor fusion and real-time AI.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Data Comes to Life Through Reality Capture and Omniverse Integration&amp;nbsp;&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;AEON will be piloted in factories and warehouses to scan everything from small precision parts and automotive components to large assembly lines and storage areas.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-large wp-image-82408" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/06/hexagon-aeon-1680x945.jpg" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;Captured data comes to life in RCS, a platform that allows users to collaborate, visualize and share reality-capture data by tapping into HxDR and NVIDIA Omniverse running in the cloud. This removes the constraint of local infrastructure.&lt;/p&gt;
&lt;p&gt;“Digital twins offer clear advantages, but adoption has been challenging in several industries,” said Lucas Heinzle, vice president of research and development at Hexagon’s robotics division. “AEON’s sophisticated sensor suite enables the integration of reality data capture with NVIDIA Omniverse, streamlining workflows for our customers and moving us closer to making digital twins a mainstream tool for collaboration and innovation.”&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;AEON’s Next Steps&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;By adopting the OpenUSD framework and developing on Omniverse, Hexagon can generate high-fidelity digital twins from scanned data — establishing a data flywheel to continuously train AEON.&lt;/p&gt;
&lt;p&gt;This latest work with Hexagon is helping shape the future of physical AI — delivering scalable, efficient solutions to address the challenges faced by industries that depend on capturing real-world data.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch the &lt;/i&gt;&lt;i&gt;Hexagon LIVE keynote&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;explore presentations&lt;/i&gt;&lt;i&gt; and read more about AEON.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;All imagery courtesy of Hexagon.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/hexagon-robotics-ai-software-aeon-humanoid/</guid><pubDate>Tue, 17 Jun 2025 15:10:55 +0000</pubDate></item><item><title>Elon Musk’s xAI is reportedly seeking a $4.3B equity raise (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/elon-musks-xai-is-reportedly-seeking-a-4-3b-equity-raise/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198697259.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk’s startup xAI is trying to raise a $4.3 billion equity investment, according to a report from Bloomberg. This equity funding would be in addition to the $5 billion that Musk is allegedly trying to raise in debt funding for the combined entity of X and xAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company appears to be raising money again after landing a $6 billion cash infusion in December, because it has already spent much of its money.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;xAI makes Grok, the AI chatbot that’s embedded inside the social network X, as well as the image generator Aurora. The technology that powers these products is notoriously resource-intensive, which could be contributing to the rate at which the company is spending money.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-2198697259.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Elon Musk’s startup xAI is trying to raise a $4.3 billion equity investment, according to a report from Bloomberg. This equity funding would be in addition to the $5 billion that Musk is allegedly trying to raise in debt funding for the combined entity of X and xAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company appears to be raising money again after landing a $6 billion cash infusion in December, because it has already spent much of its money.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;xAI makes Grok, the AI chatbot that’s embedded inside the social network X, as well as the image generator Aurora. The technology that powers these products is notoriously resource-intensive, which could be contributing to the rate at which the company is spending money.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/elon-musks-xai-is-reportedly-seeking-a-4-3b-equity-raise/</guid><pubDate>Tue, 17 Jun 2025 15:26:47 +0000</pubDate></item><item><title>How AI chatbots keep people coming back (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/how-ai-chatbots-keep-people-coming-back/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1364050120-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed alignwide is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Chatbots are increasingly looking to keep people chatting, using familiar tactics that we’ve already seen lead to negative consequences. Sycophancy can make AI chatbots respond in a way that’s overly agreeable or flattering. And while having a digital hype person might not seem like a dangerous thing, it is actually a tactic used by tech companies to keep users talking with their bots and returning to their platforms.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/06/GettyImages-1364050120-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed alignwide is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Chatbots are increasingly looking to keep people chatting, using familiar tactics that we’ve already seen lead to negative consequences. Sycophancy can make AI chatbots respond in a way that’s overly agreeable or flattering. And while having a digital hype person might not seem like a dangerous thing, it is actually a tactic used by tech companies to keep users talking with their bots and returning to their platforms.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/how-ai-chatbots-keep-people-coming-back/</guid><pubDate>Tue, 17 Jun 2025 15:39:00 +0000</pubDate></item><item><title>Ren Zhengfei: China’s AI future and Huawei’s long game (AI News)</title><link>https://www.artificialintelligence-news.com/news/ren-zhengfei-china-ai-future-huawei-long-game/</link><description>&lt;p&gt;Ask Huawei CEO Ren Zhengfei for his take on AI in China and the mountain of difficulties facing his company, and you get surprising answers.&lt;/p&gt;&lt;p&gt;“I haven’t thought about it,” says Ren, in a Q&amp;amp;A with Chinese media outlet People’s Daily. “It’s useless to think about it.”&lt;/p&gt;&lt;p&gt;In a world obsessed with five-year plans and crisis management, his advice is almost jarring in its simplicity: “Don’t think about the difficulties. Just do it and move forward step by step.”&lt;/p&gt;&lt;p&gt;This isn’t just a personal mantra; it’s the blueprint for how Huawei is navigating a storm of international sanctions and blockades. It’s a quiet determination that ripples through all his answers.&lt;/p&gt;&lt;p&gt;When the conversation shifts to Huawei’s advanced Ascend AI chips, he is almost brutally honest. He doesn’t boast. In fact, he believes the hype has gotten ahead of reality.&lt;/p&gt;&lt;p&gt;“The United States has exaggerated Huawei’s achievements. Huawei is not that great yet,” he admits, noting that their best chips are still a generation behind.&lt;/p&gt;&lt;p&gt;So what do you do when you can’t buy the best tools? According to Ren, you get smarter with the ones you have. He explains that Huawei is leaning on its brilliance in software and mathematics to close the hardware gap in AI and beyond.&lt;/p&gt;&lt;p&gt;“We use mathematics to make up for physics,” he says, describing a strategy of using code and linking chips together in powerful clusters to achieve results that can compete with the very best. Ingenuity born from necessity.&lt;/p&gt;&lt;p&gt;This grounded perspective applies to people as much as it does to products. In an age of relentless corporate promotion, Ren is wary of the spotlight. “We are also under a lot of pressure when people praise us,” he reveals. “We will be more sober when people criticise us.”&lt;/p&gt;&lt;p&gt;He sees criticism of Huawei not as an attack, but as a gift from the people who actually use their products. It’s a sign of a healthy relationship. His focus remains unwavering: “Don’t care about praise or criticism, but care about whether you can do well.”&lt;/p&gt;&lt;p&gt;But the real heart of Ren’s vision, the idea that truly animates him, lies in something much deeper and slower than the next product cycle: basic scientific research. He speaks about it with the passion of a philosopher, arguing it is the very soul of progress.&lt;/p&gt;&lt;p&gt;“If we do not do basic research, we will have no roots,” he warns. “Even if the leaves are lush and flourishing, they will fall down when the wind blows.”&lt;/p&gt;&lt;p&gt;For Huawei, these are not just poetic words. They are backed by huge investment. Out of an annual R&amp;amp;D budget of 180 billion yuan (around $25 billion) a full third of it – 60 billion yuan (~$8.34 billion) – is poured into theoretical research. This is money spent without the expectation of an immediate return, a long-term bet on the power of human curiosity. It’s an investment in a future that may be decades away.&lt;/p&gt;&lt;p&gt;Looking toward that future, Ren sees AI as a monumental shift not just for Huawei but for humanity. He believes China is well-positioned for this new era, not just because of its technology, but because of its powerful infrastructure and, most importantly, its people.&lt;/p&gt;&lt;p&gt;Ren imagines a future where the real breakthroughs in AI won’t just come from programmers in tech giants like Huawei, but from experts in every field – doctors, engineers, and even miners – using AI to solve real-world problems.&lt;/p&gt;&lt;p&gt;His optimism is infectious. He recalls an op-ed by New York Times columnist Thomas L. Friedman who departed China and published an article earlier this year with a title that requires no further explanation: ‘I Just Saw the Future. It Was Not in America.’&lt;/p&gt;&lt;p&gt;Ren Zhengfei seems to be a leader who has found a sense of calm in the eye of the storm. His focus is not on the shifting political winds, but on the slow, steady work of building something with deep roots, ready for whatever the future holds. Step by patient step.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image credit: European Union under CC BY 4.0 license. Image cropped for effect.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Hugging Face partners with Groq for ultra-fast AI model inference&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Ask Huawei CEO Ren Zhengfei for his take on AI in China and the mountain of difficulties facing his company, and you get surprising answers.&lt;/p&gt;&lt;p&gt;“I haven’t thought about it,” says Ren, in a Q&amp;amp;A with Chinese media outlet People’s Daily. “It’s useless to think about it.”&lt;/p&gt;&lt;p&gt;In a world obsessed with five-year plans and crisis management, his advice is almost jarring in its simplicity: “Don’t think about the difficulties. Just do it and move forward step by step.”&lt;/p&gt;&lt;p&gt;This isn’t just a personal mantra; it’s the blueprint for how Huawei is navigating a storm of international sanctions and blockades. It’s a quiet determination that ripples through all his answers.&lt;/p&gt;&lt;p&gt;When the conversation shifts to Huawei’s advanced Ascend AI chips, he is almost brutally honest. He doesn’t boast. In fact, he believes the hype has gotten ahead of reality.&lt;/p&gt;&lt;p&gt;“The United States has exaggerated Huawei’s achievements. Huawei is not that great yet,” he admits, noting that their best chips are still a generation behind.&lt;/p&gt;&lt;p&gt;So what do you do when you can’t buy the best tools? According to Ren, you get smarter with the ones you have. He explains that Huawei is leaning on its brilliance in software and mathematics to close the hardware gap in AI and beyond.&lt;/p&gt;&lt;p&gt;“We use mathematics to make up for physics,” he says, describing a strategy of using code and linking chips together in powerful clusters to achieve results that can compete with the very best. Ingenuity born from necessity.&lt;/p&gt;&lt;p&gt;This grounded perspective applies to people as much as it does to products. In an age of relentless corporate promotion, Ren is wary of the spotlight. “We are also under a lot of pressure when people praise us,” he reveals. “We will be more sober when people criticise us.”&lt;/p&gt;&lt;p&gt;He sees criticism of Huawei not as an attack, but as a gift from the people who actually use their products. It’s a sign of a healthy relationship. His focus remains unwavering: “Don’t care about praise or criticism, but care about whether you can do well.”&lt;/p&gt;&lt;p&gt;But the real heart of Ren’s vision, the idea that truly animates him, lies in something much deeper and slower than the next product cycle: basic scientific research. He speaks about it with the passion of a philosopher, arguing it is the very soul of progress.&lt;/p&gt;&lt;p&gt;“If we do not do basic research, we will have no roots,” he warns. “Even if the leaves are lush and flourishing, they will fall down when the wind blows.”&lt;/p&gt;&lt;p&gt;For Huawei, these are not just poetic words. They are backed by huge investment. Out of an annual R&amp;amp;D budget of 180 billion yuan (around $25 billion) a full third of it – 60 billion yuan (~$8.34 billion) – is poured into theoretical research. This is money spent without the expectation of an immediate return, a long-term bet on the power of human curiosity. It’s an investment in a future that may be decades away.&lt;/p&gt;&lt;p&gt;Looking toward that future, Ren sees AI as a monumental shift not just for Huawei but for humanity. He believes China is well-positioned for this new era, not just because of its technology, but because of its powerful infrastructure and, most importantly, its people.&lt;/p&gt;&lt;p&gt;Ren imagines a future where the real breakthroughs in AI won’t just come from programmers in tech giants like Huawei, but from experts in every field – doctors, engineers, and even miners – using AI to solve real-world problems.&lt;/p&gt;&lt;p&gt;His optimism is infectious. He recalls an op-ed by New York Times columnist Thomas L. Friedman who departed China and published an article earlier this year with a title that requires no further explanation: ‘I Just Saw the Future. It Was Not in America.’&lt;/p&gt;&lt;p&gt;Ren Zhengfei seems to be a leader who has found a sense of calm in the eye of the storm. His focus is not on the shifting political winds, but on the slow, steady work of building something with deep roots, ready for whatever the future holds. Step by patient step.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Image credit: European Union under CC BY 4.0 license. Image cropped for effect.)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Hugging Face partners with Groq for ultra-fast AI model inference&lt;/strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-11874" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2022/04/ai-expo-world-728x-90-01.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is co-located with other leading events including Intelligent Automation Conference, BlockX, Digital Transformation Week, and Cyber Security &amp;amp; Cloud Expo.&lt;/p&gt;&lt;p&gt;Explore other upcoming enterprise technology events and webinars powered by TechForge here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/ren-zhengfei-china-ai-future-huawei-long-game/</guid><pubDate>Tue, 17 Jun 2025 15:59:17 +0000</pubDate></item><item><title>New methods boost reasoning in small and large language models (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/new-methods-boost-reasoning-in-small-and-large-language-models/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="The image shows a diagram illustrating the relationship between mathematical statements in natural language and formal language. On the left, there is a blue box labeled " background="background" class="wp-image-1142121" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/NewMethods-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Artificial intelligence is advancing across a wide range of fields, with one of the most important developments being its growing capacity for reasoning. This capability could help AI becomes a reliable partner in critical domains like scientific research and healthcare.&lt;/p&gt;



&lt;p&gt;To support this progress, we’ve identified three primary strategies to strengthen reasoning capabilities in both small and large language models: improve architectural design to boost performance in smaller models; incorporate mathematical reasoning techniques to increase reliability; and build stronger generalization capabilities to enable reasoning across a variety of fields.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="smarter-reasoning-in-smaller-models"&gt;Smarter reasoning in smaller models&lt;/h2&gt;



&lt;p&gt;While language models trained on broad world knowledge hold great potential, they lack the ability to learn continuously and refine their understanding. This limitation becomes especially pronounced in smaller models, where limited capacity makes strong reasoning even harder.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;on-demand event&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum Episode 4&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum-episode-4"&gt;Learn about the latest multimodal AI models, advanced benchmarks for AI evaluation and model self-improvement, and an entirely new kind of computer for AI inference and hard optimization. &lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;The problem stems from how current language models operate. They rely on fast, pattern recognition-based responses that break down in complex scenarios. In contrast, people use deliberate, step-by-step reasoning, test different approaches, and evaluate outcomes. To address this gap, we’re building methods to enable stronger reasoning in smaller systems.&lt;/p&gt;



&lt;p&gt;rStar-Math is a method that uses Monte Carlo Tree Search (MCTS) to simulate deeper, more methodical reasoning in smaller models. It uses a three-step, self-improving cycle:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Problem decomposition&lt;/strong&gt; breaks down complex mathematical problems into manageable steps, creating a thorough and accurate course of reasoning.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Process preference model (PPM)&lt;/strong&gt; trains small models to predict reward labels for each step, improving process-level supervision.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt; applies a four-round, self-improvement cycle in which updated strategy models and PPMs guide MCTS to improve performance.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;When tested on four small language models ranging from 1.5 billion to 7 billion parameters, rStar-Math achieved an average accuracy of 53% on the American Invitational Mathematics Examination (AIME)—performance that places it among the top 20% of high school competitors in the US.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: A three-part diagram illustrating the rStar-Math framework. (a) Shows an MCTS-driven reasoning tree with Q-values and answer verification using PPM or Python; correct and incorrect steps are marked. (b) Depicts how Q-value filtering constructs per-step preference pairs from partial to full solutions. (c) Outlines four rounds of self-evolution, alternating between SLM and PPM improvements using terminal-guided and PPM-augmented MCTS." class="wp-image-1141894" height="322" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-1.png" width="1049" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. The rStar-Math framework &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Logic-RL is a reinforcement learning framework that strengthens logical reasoning through a practical system prompt and a structured reward function. By training models on logic puzzles, Logic-RL grants rewards only when both the reasoning process and the final answer meet strict formatting requirements. This prevents shortcuts and promotes analytical rigor.&lt;/p&gt;



&lt;p&gt;Language models trained with Logic-RL demonstrate strong performance beyond logic puzzles, generalizing effectively to mathematical competition problems. On the AIME and AMC (American Mathematics Competitions) datasets, 7-billion-parameter models improved accuracy by 125% and 38%, respectively, compared with baseline models.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="building-reliable-mathematical-reasoning"&gt;Building reliable mathematical reasoning&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Mathematics poses a unique challenge for language models, which often struggle to meet its precision and rigor using natural language. To address this, we’re creating formal and symbolic methods to enable language models to adopt structured mathematical tools. The goal is to convert language model outputs into code based on the fundamental rules of arithmetic, like 1 + 1 = 2, allowing us to systematically verify accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;LIPS (LLM-based Inequality Prover with Symbolic Reasoning) is a system that combines LLMs’ pattern recognition capabilities with symbolic reasoning. LIPS draws on the strategies participants in math competitions use in order to distinguish between tasks best suited to symbolic solvers (e.g., scaling) and those better handled by language models (e.g., rewriting). On 161 Olympiad-level problems, LIPS achieved state-of-the-art results without additional training data.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: A three-part diagram showing the LIPS framework for inequality proof generation. On the left, a current inequality problem is transformed into new inequality subproblems via tactic generation using symbolic-based and LLM-generated rewriting methods. In the center, these new goals are filtered and ranked using LLM and symbolic methods. On the right, a ranked sequence of inequalities forms a complete proof, applying named tactics like Cauchy-Schwarz, AM-GM, and LLM simplification, ending with the original inequality verified." class="wp-image-1141898" height="426" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-2.png" width="1295" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. An overview of LIPS&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;However, translating natural-language math problems into precise, machine-readable formats is a challenge. Our goal is to bridge the gap between the one-pass success rate, where the top-ranked generated result is correct, and the k-pass success rate, where at least one of the top &lt;em&gt;k&lt;/em&gt; generated results is correct.&lt;/p&gt;



&lt;p&gt;We developed a new framework using two evaluation methods. &lt;strong&gt;Symbolic equivalence&lt;/strong&gt; checks whether outputs are logically identical, while &lt;strong&gt;semantic consistency&lt;/strong&gt; uses embedding similarity to detect subtle differences missed by symbolic checks.&lt;/p&gt;



&lt;p&gt;When we evaluated this approach on the MATH and miniF2F datasets, which include problems from various math competitions, it improved accuracy by up to 1.35 times over baseline methods.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: A flowchart illustrating the autoformalization framework. On the left, a natural language math statement is converted into a formal language theorem via an " class="wp-image-1141897" for="for" height="377" multiple="multiple" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-3.png" width="1271" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. An overview of the auto-formalization framework&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;To address the shortage of high-quality training data, we developed a neuro-symbolic framework that automatically generates diverse, well-structured math problems. Symbolic solvers create the problems, while language models translate them into natural language. This approach not only broadens training resources but also supports more effective instruction and evaluation of mathematical reasoning in language models.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4: A flowchart illustrating the neuro-symbolic data generation framework. It begins with a natural language math problem about a sandbox's perimeter. This is formalized into symbolic assertions, then mutated while preserving structure. The formal problem is solved and informalized into a new natural language Q&amp;amp;A about a garden's dimensions. The process continues with further mutation to generate problems of varying difficulty—examples include an easy question about a rectangle’s width and a medium one involving expressions for area." class="wp-image-1142036" height="404" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-4.png" width="1255" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. An overview of the neuro-symbolic data generation framework &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="boosting-generalization-across-domains"&gt;Boosting generalization across domains&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;A key indicator of advanced AI is its ability to generalize—the ability to transfer reasoning skills across different domains. We found that training language models on math data significantly improved performance in coding, science, and other areas, revealing unexpected cross-domain benefits.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This discovery motivated us to develop Chain-of-Reasoning (CoR), an approach that unifies reasoning across natural language, code, and symbolic forms. CoR lets models blend these formats using natural language to frame context, code for precise calculations, and symbolic representations for abstraction. By adjusting prompts, CoR adapts both reasoning depth and paradigm diversity to match specific problem requirements.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Tests of CoR across five math datasets showed its ability to tackle both computational and proof-based problems, demonstrating strong general mathematical problem-solving skills.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5: Diagram illustrating three reasoning paradigms: (a) Single-paradigm reasoning, where all reasoning steps use the same medium (e.g., natural language, algorithms, or symbols); (b) Tool-integrated single-paradigm reasoning, where natural language drives reasoning, but code is used to solve specific sub-problems, with results reintegrated into the language-based reasoning; (c) CoR (multi-paradigm) reasoning framework, which enables reasoning across different paradigms with varying depths to handle diverse problem types, supported by examples." class="wp-image-1141896" height="493" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-5.png" width="1183" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. CoR’s reasoning process under different types of methods&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Current language models often rely on domain-specific solutions, limiting their flexibility across different types of problems. To move beyond this constraint, we developed Critical Plan Step Learning (CPL), an approach focused on high-level abstract planning that teaches models to identify key knowledge, break down problems, and make strategic decisions.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The technique draws on how people solve problems, by breaking them down, identifying key information, and recalling relevant knowledge—strategies we want language models to learn.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;CPL combines two key components: &lt;strong&gt;plan-based MCTS&lt;/strong&gt;, which searches multi-step solution paths and constructs planning trees, and &lt;strong&gt;step-APO&lt;/strong&gt;, which learns preferences for strong intermediate steps while filtering out weak ones. This combination enhances reasoning and improves generalization across tasks, moving AI systems closer to the flexible thinking that characterizes human intelligence.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6: Illustration of CPL. Left: Plans represent abstract thinking for problem-solving, which allows for better generalization, whereas task-specific solutions often limit it. Right: CPL searches within the action space on high-level abstract plans using MCTS and obtains advantage estimates for step-level preferences. CPL can then identify and learn critical steps that provide a distinct advantage over others." class="wp-image-1141895" height="585" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-6.png" width="1238" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Overview of the CPL framework&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead-next-steps-in-ai-reasoning"&gt;Looking ahead: Next steps in AI reasoning&lt;/h2&gt;



&lt;p&gt;From building reliable math solvers to unifying reasoning approaches, researchers are redefining how language models approach complex tasks. Their work sets the stage for more capable and versatile AI systems—applicable to education, science, healthcare, and beyond. Despite these advances, hallucinations and imprecise logic continue to pose risks in critical fields like medicine and scientific research, where accuracy is essential.&lt;/p&gt;



&lt;p&gt;These challenges are driving the team’s exploration of additional tools and frameworks to improve language model reasoning. This includes AutoVerus for automated proof generation in Rust code, SAFE for addressing data scarcity in Rust formal verification, and Alchemy, which uses symbolic mutation to improve neural theorem proving.&lt;/p&gt;



&lt;p&gt;Together, these technologies represent important progress toward building trustworthy, high-performing reasoning models and signal a broader shift toward addressing some of AI’s current limitations.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="The image shows a diagram illustrating the relationship between mathematical statements in natural language and formal language. On the left, there is a blue box labeled " background="background" class="wp-image-1142121" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/NewMethods-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Artificial intelligence is advancing across a wide range of fields, with one of the most important developments being its growing capacity for reasoning. This capability could help AI becomes a reliable partner in critical domains like scientific research and healthcare.&lt;/p&gt;



&lt;p&gt;To support this progress, we’ve identified three primary strategies to strengthen reasoning capabilities in both small and large language models: improve architectural design to boost performance in smaller models; incorporate mathematical reasoning techniques to increase reliability; and build stronger generalization capabilities to enable reasoning across a variety of fields.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="smarter-reasoning-in-smaller-models"&gt;Smarter reasoning in smaller models&lt;/h2&gt;



&lt;p&gt;While language models trained on broad world knowledge hold great potential, they lack the ability to learn continuously and refine their understanding. This limitation becomes especially pronounced in smaller models, where limited capacity makes strong reasoning even harder.&lt;/p&gt;




	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;on-demand event&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;Microsoft Research Forum Episode 4&lt;/h2&gt;
				
								&lt;p class="large" id="microsoft-research-forum-episode-4"&gt;Learn about the latest multimodal AI models, advanced benchmarks for AI evaluation and model self-improvement, and an entirely new kind of computer for AI inference and hard optimization. &lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	&lt;p&gt;The problem stems from how current language models operate. They rely on fast, pattern recognition-based responses that break down in complex scenarios. In contrast, people use deliberate, step-by-step reasoning, test different approaches, and evaluate outcomes. To address this gap, we’re building methods to enable stronger reasoning in smaller systems.&lt;/p&gt;



&lt;p&gt;rStar-Math is a method that uses Monte Carlo Tree Search (MCTS) to simulate deeper, more methodical reasoning in smaller models. It uses a three-step, self-improving cycle:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Problem decomposition&lt;/strong&gt; breaks down complex mathematical problems into manageable steps, creating a thorough and accurate course of reasoning.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Process preference model (PPM)&lt;/strong&gt; trains small models to predict reward labels for each step, improving process-level supervision.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Iterative refinement&lt;/strong&gt; applies a four-round, self-improvement cycle in which updated strategy models and PPMs guide MCTS to improve performance.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;When tested on four small language models ranging from 1.5 billion to 7 billion parameters, rStar-Math achieved an average accuracy of 53% on the American Invitational Mathematics Examination (AIME)—performance that places it among the top 20% of high school competitors in the US.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 1: A three-part diagram illustrating the rStar-Math framework. (a) Shows an MCTS-driven reasoning tree with Q-values and answer verification using PPM or Python; correct and incorrect steps are marked. (b) Depicts how Q-value filtering constructs per-step preference pairs from partial to full solutions. (c) Outlines four rounds of self-evolution, alternating between SLM and PPM improvements using terminal-guided and PPM-augmented MCTS." class="wp-image-1141894" height="322" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-1.png" width="1049" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. The rStar-Math framework &lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Logic-RL is a reinforcement learning framework that strengthens logical reasoning through a practical system prompt and a structured reward function. By training models on logic puzzles, Logic-RL grants rewards only when both the reasoning process and the final answer meet strict formatting requirements. This prevents shortcuts and promotes analytical rigor.&lt;/p&gt;



&lt;p&gt;Language models trained with Logic-RL demonstrate strong performance beyond logic puzzles, generalizing effectively to mathematical competition problems. On the AIME and AMC (American Mathematics Competitions) datasets, 7-billion-parameter models improved accuracy by 125% and 38%, respectively, compared with baseline models.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="building-reliable-mathematical-reasoning"&gt;Building reliable mathematical reasoning&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;Mathematics poses a unique challenge for language models, which often struggle to meet its precision and rigor using natural language. To address this, we’re creating formal and symbolic methods to enable language models to adopt structured mathematical tools. The goal is to convert language model outputs into code based on the fundamental rules of arithmetic, like 1 + 1 = 2, allowing us to systematically verify accuracy.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;LIPS (LLM-based Inequality Prover with Symbolic Reasoning) is a system that combines LLMs’ pattern recognition capabilities with symbolic reasoning. LIPS draws on the strategies participants in math competitions use in order to distinguish between tasks best suited to symbolic solvers (e.g., scaling) and those better handled by language models (e.g., rewriting). On 161 Olympiad-level problems, LIPS achieved state-of-the-art results without additional training data.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 2: A three-part diagram showing the LIPS framework for inequality proof generation. On the left, a current inequality problem is transformed into new inequality subproblems via tactic generation using symbolic-based and LLM-generated rewriting methods. In the center, these new goals are filtered and ranked using LLM and symbolic methods. On the right, a ranked sequence of inequalities forms a complete proof, applying named tactics like Cauchy-Schwarz, AM-GM, and LLM simplification, ending with the original inequality verified." class="wp-image-1141898" height="426" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-2.png" width="1295" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 2. An overview of LIPS&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;However, translating natural-language math problems into precise, machine-readable formats is a challenge. Our goal is to bridge the gap between the one-pass success rate, where the top-ranked generated result is correct, and the k-pass success rate, where at least one of the top &lt;em&gt;k&lt;/em&gt; generated results is correct.&lt;/p&gt;



&lt;p&gt;We developed a new framework using two evaluation methods. &lt;strong&gt;Symbolic equivalence&lt;/strong&gt; checks whether outputs are logically identical, while &lt;strong&gt;semantic consistency&lt;/strong&gt; uses embedding similarity to detect subtle differences missed by symbolic checks.&lt;/p&gt;



&lt;p&gt;When we evaluated this approach on the MATH and miniF2F datasets, which include problems from various math competitions, it improved accuracy by up to 1.35 times over baseline methods.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 3: A flowchart illustrating the autoformalization framework. On the left, a natural language math statement is converted into a formal language theorem via an " class="wp-image-1141897" for="for" height="377" multiple="multiple" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-3.png" width="1271" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 3. An overview of the auto-formalization framework&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;To address the shortage of high-quality training data, we developed a neuro-symbolic framework that automatically generates diverse, well-structured math problems. Symbolic solvers create the problems, while language models translate them into natural language. This approach not only broadens training resources but also supports more effective instruction and evaluation of mathematical reasoning in language models.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 4: A flowchart illustrating the neuro-symbolic data generation framework. It begins with a natural language math problem about a sandbox's perimeter. This is formalized into symbolic assertions, then mutated while preserving structure. The formal problem is solved and informalized into a new natural language Q&amp;amp;A about a garden's dimensions. The process continues with further mutation to generate problems of varying difficulty—examples include an easy question about a rectangle’s width and a medium one involving expressions for area." class="wp-image-1142036" height="404" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-4.png" width="1255" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 4. An overview of the neuro-symbolic data generation framework &lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="boosting-generalization-across-domains"&gt;Boosting generalization across domains&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;A key indicator of advanced AI is its ability to generalize—the ability to transfer reasoning skills across different domains. We found that training language models on math data significantly improved performance in coding, science, and other areas, revealing unexpected cross-domain benefits.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This discovery motivated us to develop Chain-of-Reasoning (CoR), an approach that unifies reasoning across natural language, code, and symbolic forms. CoR lets models blend these formats using natural language to frame context, code for precise calculations, and symbolic representations for abstraction. By adjusting prompts, CoR adapts both reasoning depth and paradigm diversity to match specific problem requirements.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Tests of CoR across five math datasets showed its ability to tackle both computational and proof-based problems, demonstrating strong general mathematical problem-solving skills.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 5: Diagram illustrating three reasoning paradigms: (a) Single-paradigm reasoning, where all reasoning steps use the same medium (e.g., natural language, algorithms, or symbols); (b) Tool-integrated single-paradigm reasoning, where natural language drives reasoning, but code is used to solve specific sub-problems, with results reintegrated into the language-based reasoning; (c) CoR (multi-paradigm) reasoning framework, which enables reasoning across different paradigms with varying depths to handle diverse problem types, supported by examples." class="wp-image-1141896" height="493" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-5.png" width="1183" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 5. CoR’s reasoning process under different types of methods&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;Current language models often rely on domain-specific solutions, limiting their flexibility across different types of problems. To move beyond this constraint, we developed Critical Plan Step Learning (CPL), an approach focused on high-level abstract planning that teaches models to identify key knowledge, break down problems, and make strategic decisions.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;The technique draws on how people solve problems, by breaking them down, identifying key information, and recalling relevant knowledge—strategies we want language models to learn.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;CPL combines two key components: &lt;strong&gt;plan-based MCTS&lt;/strong&gt;, which searches multi-step solution paths and constructs planning trees, and &lt;strong&gt;step-APO&lt;/strong&gt;, which learns preferences for strong intermediate steps while filtering out weak ones. This combination enhances reasoning and improves generalization across tasks, moving AI systems closer to the flexible thinking that characterizes human intelligence.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figure 6: Illustration of CPL. Left: Plans represent abstract thinking for problem-solving, which allows for better generalization, whereas task-specific solutions often limit it. Right: CPL searches within the action space on high-level abstract plans using MCTS and obtains advantage estimates for step-level preferences. CPL can then identify and learn critical steps that provide a distinct advantage over others." class="wp-image-1141895" height="585" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/enhancing-llm-reasoning-abilities-6.png" width="1238" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 6. Overview of the CPL framework&lt;/figcaption&gt;&lt;/figure&gt;



&lt;h2 class="wp-block-heading" id="looking-ahead-next-steps-in-ai-reasoning"&gt;Looking ahead: Next steps in AI reasoning&lt;/h2&gt;



&lt;p&gt;From building reliable math solvers to unifying reasoning approaches, researchers are redefining how language models approach complex tasks. Their work sets the stage for more capable and versatile AI systems—applicable to education, science, healthcare, and beyond. Despite these advances, hallucinations and imprecise logic continue to pose risks in critical fields like medicine and scientific research, where accuracy is essential.&lt;/p&gt;



&lt;p&gt;These challenges are driving the team’s exploration of additional tools and frameworks to improve language model reasoning. This includes AutoVerus for automated proof generation in Rust code, SAFE for addressing data scarcity in Rust formal verification, and Alchemy, which uses symbolic mutation to improve neural theorem proving.&lt;/p&gt;



&lt;p&gt;Together, these technologies represent important progress toward building trustworthy, high-performing reasoning models and signal a broader shift toward addressing some of AI’s current limitations.&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;&lt;!-- promos injected --&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/new-methods-boost-reasoning-in-small-and-large-language-models/</guid><pubDate>Tue, 17 Jun 2025 16:00:00 +0000</pubDate></item><item><title>We’re expanding our Gemini 2.5 family of models (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/</link><description>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Blue and black futuristic illustration with Gemini 2.5 logo in the middle" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_20.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
              







            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;We designed Gemini 2.5 to be a family of hybrid reasoning models that provide amazing performance, while also being at the Pareto Frontier of cost and speed. Today, we’re taking the next step with our 2.5 Pro and Flash models by releasing them as stable and generally available. And we’re bringing you 2.5 Flash-Lite in preview — our most cost-efficient and fastest 2.5 model yet.&lt;/p&gt;&lt;h2&gt;Making 2.5 Flash and 2.5 Pro generally available&lt;/h2&gt;&lt;p&gt;Thanks to all of your feedback, today we’re releasing stable versions of 2.5 Flash and Pro, so you can build production applications with confidence. Developers like Spline and Rooms and organizations like Snap and SmartBear have already been using the latest versions in-production for the last few weeks.&lt;/p&gt;&lt;h2&gt;Introducing Gemini 2.5 Flash-Lite&lt;/h2&gt;&lt;p&gt;We’re also introducing a preview of the new Gemini 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet. You can start building with the preview version now, and we’re looking forward to your feedback.&lt;/p&gt;&lt;p&gt;2.5 Flash Lite has all-around higher quality than 2.0 Flash-Lite on coding, math, science, reasoning and multimodal benchmarks. It excels at high-volume, latency-sensitive tasks like translation and classification, with lower latency than 2.0 Flash-Lite and 2.0 Flash on a broad sample of prompts. It comes with the same capabilities that make Gemini 2.5 helpful, including the ability to turn thinking on at different budgets, connecting to tools like Google Search and code execution, multimodal input, and a 1 million-token context length.&lt;/p&gt;&lt;p&gt;See more details about our 2.5 family of models in the latest Gemini technical report.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    







  
  
    &lt;div&gt;
      &lt;img alt="Gemini 2.5 Flash Lite benchmarks table" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_2-5_benchmarks_margin_light2x_1.gif" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;The preview of Gemini 2.5 Flash-Lite is now available in Google AI Studio and Vertex AI, alongside the stable versions of 2.5 Flash and Pro. Both 2.5 Flash and Pro are also accessible in the Gemini app. We’ve also brought custom versions of 2.5 Flash-Lite and Flash to Search.&lt;/p&gt;&lt;p&gt;We can’t wait to see what you continue to build with Gemini 2.5.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini Models


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</description><content:encoded>&lt;div class="article-image-hero"&gt;
  &lt;div class="article-image-hero__container"&gt;
    &lt;figure class="article-image--full-aspect article-module"&gt;
      &lt;div class="aspect-ratio-image"&gt;
        &lt;div class="aspect-ratio-image__container"&gt;
          &lt;img alt="Blue and black futuristic illustration with Gemini 2.5 logo in the middle" class="aspect-ratio-image__image uni-progressive-image--blur" height="150px" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/2.5_bundle_keyword_blog_header_20.width-2200.format-webp.webp" width="360px" /&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      
    &lt;/figure&gt;
  &lt;/div&gt;
&lt;/div&gt;&lt;div class="uni-content uni-blog-article-container article-container__content
                      
                      "&gt;

            
              







            

            
            
&lt;!--article text--&gt;

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;We designed Gemini 2.5 to be a family of hybrid reasoning models that provide amazing performance, while also being at the Pareto Frontier of cost and speed. Today, we’re taking the next step with our 2.5 Pro and Flash models by releasing them as stable and generally available. And we’re bringing you 2.5 Flash-Lite in preview — our most cost-efficient and fastest 2.5 model yet.&lt;/p&gt;&lt;h2&gt;Making 2.5 Flash and 2.5 Pro generally available&lt;/h2&gt;&lt;p&gt;Thanks to all of your feedback, today we’re releasing stable versions of 2.5 Flash and Pro, so you can build production applications with confidence. Developers like Spline and Rooms and organizations like Snap and SmartBear have already been using the latest versions in-production for the last few weeks.&lt;/p&gt;&lt;h2&gt;Introducing Gemini 2.5 Flash-Lite&lt;/h2&gt;&lt;p&gt;We’re also introducing a preview of the new Gemini 2.5 Flash-Lite, our most cost-efficient and fastest 2.5 model yet. You can start building with the preview version now, and we’re looking forward to your feedback.&lt;/p&gt;&lt;p&gt;2.5 Flash Lite has all-around higher quality than 2.0 Flash-Lite on coding, math, science, reasoning and multimodal benchmarks. It excels at high-volume, latency-sensitive tasks like translation and classification, with lower latency than 2.0 Flash-Lite and 2.0 Flash on a broad sample of prompts. It comes with the same capabilities that make Gemini 2.5 helpful, including the ability to turn thinking on at different budgets, connecting to tools like Google Search and code execution, multimodal input, and a 1 million-token context length.&lt;/p&gt;&lt;p&gt;See more details about our 2.5 family of models in the latest Gemini technical report.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  

  
    







  
  
    &lt;div&gt;
      &lt;img alt="Gemini 2.5 Flash Lite benchmarks table" src="https://storage.googleapis.com/gweb-uniblog-publish-prod/original_images/gemini_2-5_benchmarks_margin_light2x_1.gif" /&gt;
    &lt;/div&gt;
  



  

  
    &lt;div class="module--text module--text__article"&gt;
      &lt;div class="uni-paragraph article-paragraph"&gt;
        &lt;div class="rich-text"&gt;&lt;p&gt;The preview of Gemini 2.5 Flash-Lite is now available in Google AI Studio and Vertex AI, alongside the stable versions of 2.5 Flash and Pro. Both 2.5 Flash and Pro are also accessible in the Gemini app. We’ve also brought custom versions of 2.5 Flash-Lite and Flash to Search.&lt;/p&gt;&lt;p&gt;We can’t wait to see what you continue to build with Gemini 2.5.&lt;/p&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  


            
            

            
              


&lt;div class="
    uni-blog-article-tags
    article-tags
    
  "&gt;
  &lt;div class="uni-blog-article-tags__wrapper"&gt;
    &lt;span class="uni-blog-article-tags__label uni-eyebrow"&gt;POSTED IN:&lt;/span&gt;
  &lt;/div&gt;
  &lt;nav class="uni-blog-article-tags__container uni-click-tracker"&gt;
    &lt;ul class="uni-blog-article-tags__tags-list"&gt;
    
      &lt;li&gt;
        
        
        


  


Gemini Models


  


      &lt;/li&gt;
    

    
      &lt;li&gt;
        
        
        


  


AI


  


      &lt;/li&gt;
    
      &lt;li&gt;
        
        
        


  


Google DeepMind


  


      &lt;/li&gt;
    
    &lt;/ul&gt;
  &lt;/nav&gt;
&lt;/div&gt;

            
          &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/were-expanding-our-gemini-25-family-of-models/</guid><pubDate>Tue, 17 Jun 2025 16:01:13 +0000</pubDate></item><item><title>Gemini 2.5: Updates to our family of thinking models (Google DeepMind Blog)</title><link>https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/</link><description>&lt;div&gt;
    &lt;p&gt;Today we are excited to share updates across the board to our Gemini 2.5 model family:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Gemini 2.5 Pro is generally available and stable (no changes from the 06-05 preview)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Gemini 2.5 Flash is generally available and stable (no changes from the 05-20 preview, see pricing updates below)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Gemini 2.5 Flash-Lite is now available in preview&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. Each model has control over the thinking budget, giving developers the ability to choose when and how much the model “thinks” before generating a response.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Overview of our family of Gemini 2.5 thinking models" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_family_1-1__dark_dev.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Overview of our family of Gemini 2.5 thinking models
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;h2 id="introducing-gemini-2.5-flash-lite"&gt;Introducing Gemini 2.5 Flash-Lite&lt;/h2&gt;&lt;p&gt;Today, we’re introducing 2.5 Flash-Lite in preview with the lowest latency and cost in the 2.5 model family. It’s designed as a cost-effective upgrade from our previous 1.5 and 2.0 Flash models. It also offers better performance across most evals, and lower time to first token while also achieving higher tokens per second decode. This model is great for high throughput tasks like classification or summarization at scale.&lt;/p&gt;&lt;p&gt;Gemini 2.5 Flash-Lite is a reasoning model, which allows for dynamic control of the thinking budget with an API parameter. Because Flash-Lite is optimized for cost and speed, “thinking” is off by default, unlike our other models. 2.5 Flash-Lite also supports all of our native tools like Grounding with Google Search, Code Execution, and URL Context in addition to function calling.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Benchmarks for Gemini 2.5 Flash-Lite" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_flash-lite2-0_flash_benchmarks_margi.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Benchmarks for Gemini 2.5 Flash-Lite
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;h2 id="updates-to-gemini-2.5-flash-and-pricing"&gt;Updates to Gemini 2.5 Flash and pricing&lt;/h2&gt;&lt;p&gt;Over the last year, our research teams have continued to push the pareto frontier with our Flash model series. When 2.5 Flash was initially announced, we had not yet finalized the capabilities for 2.5 Flash-Lite. We also launched with a “thinking” and “non-thinking price”, which led to developer confusion.&lt;/p&gt;&lt;p&gt;With the stable version of Gemini 2.5 Flash rolling out (which is the same 05-20 model preview we made available at Google I/O), and the incredible performance of 2.5 Flash, we are updating the pricing for 2.5 Flash:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;$0.30 / 1M input tokens (*up from $0.15 input)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;$2.50 / 1M output tokens (*down from $3.50 output)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;We removed the thinking vs. non-thinking price difference&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;We kept a single price tier regardless of input token size&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While we strive to maintain consistent pricing between preview and stable releases to minimize disruption, this is a specific adjustment reflecting Flash’s exceptional value, still offering the best cost-per-intelligence available.&lt;/p&gt;&lt;p&gt;And with Gemini 2.5 Flash-Lite, we now have an even lower cost option (with or without thinking) for cost and latency sensitive use cases that require less model intelligence.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Pricing updates for our Gemini Flash family" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_flash-lite_pricing_margin_dark.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Pricing updates for our Gemini Flash family
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;p&gt;If you are using the Gemini 2.5 Flash Preview 04-17 , the existing preview pricing will remain in effect until its planned deprecation on July 15, 2025, at which point that model endpoint will be turned off. You can transition to the generally available model “gemini-2.5-flash”, or switch to 2.5 Flash-Lite Preview as a lower cost option.&lt;/p&gt;&lt;h2 id="continued-growth-of-gemini-2.5-pro"&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;Continued growth of Gemini 2.5 Pro&lt;/h2&gt;&lt;p&gt;The growth and demand for Gemini 2.5 Pro continues to be the steepest of any of our models we have ever seen. To allow more customers to build on this model in production, we are making the 06-05 version of the model stable, with the same pareto frontier price point as before.&lt;/p&gt;&lt;p&gt;We expect that cases where you need the highest intelligence and most capabilities are where you will see Pro shine, like coding and agentic tasks. Gemini 2.5 Pro is at the heart of many of the most loved developer tools.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Top developer tools using Gemini 2.5 Pro, featuring Cursor, Bolt, Cline, Cognition, Windsurf, GitHub, Lovable, Replit, and Zed Industries" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini-2-5-pro-top-developer-tools.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Top developer tools using Gemini 2.5 Pro
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;p&gt;If you are using 2.5 Pro Preview 05-06, the model will remain available until June 19, 2025 and then will be turned off. If you are using 2.5 Pro Preview 06-05, you can simply update your model string to “gemini-2.5-pro”.&lt;/p&gt;&lt;p&gt;We can’t wait to see even more domains benefit from the intelligence of 2.5 Pro and look forward to sharing more about scaling beyond Pro in the near future.&lt;/p&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;
    &lt;p&gt;Today we are excited to share updates across the board to our Gemini 2.5 model family:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Gemini 2.5 Pro is generally available and stable (no changes from the 06-05 preview)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Gemini 2.5 Flash is generally available and stable (no changes from the 05-20 preview, see pricing updates below)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;Gemini 2.5 Flash-Lite is now available in preview&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Gemini 2.5 models are thinking models, capable of reasoning through their thoughts before responding, resulting in enhanced performance and improved accuracy. Each model has control over the thinking budget, giving developers the ability to choose when and how much the model “thinks” before generating a response.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Overview of our family of Gemini 2.5 thinking models" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_family_1-1__dark_dev.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Overview of our family of Gemini 2.5 thinking models
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;h2 id="introducing-gemini-2.5-flash-lite"&gt;Introducing Gemini 2.5 Flash-Lite&lt;/h2&gt;&lt;p&gt;Today, we’re introducing 2.5 Flash-Lite in preview with the lowest latency and cost in the 2.5 model family. It’s designed as a cost-effective upgrade from our previous 1.5 and 2.0 Flash models. It also offers better performance across most evals, and lower time to first token while also achieving higher tokens per second decode. This model is great for high throughput tasks like classification or summarization at scale.&lt;/p&gt;&lt;p&gt;Gemini 2.5 Flash-Lite is a reasoning model, which allows for dynamic control of the thinking budget with an API parameter. Because Flash-Lite is optimized for cost and speed, “thinking” is off by default, unlike our other models. 2.5 Flash-Lite also supports all of our native tools like Grounding with Google Search, Code Execution, and URL Context in addition to function calling.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Benchmarks for Gemini 2.5 Flash-Lite" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_flash-lite2-0_flash_benchmarks_margi.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Benchmarks for Gemini 2.5 Flash-Lite
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;h2 id="updates-to-gemini-2.5-flash-and-pricing"&gt;Updates to Gemini 2.5 Flash and pricing&lt;/h2&gt;&lt;p&gt;Over the last year, our research teams have continued to push the pareto frontier with our Flash model series. When 2.5 Flash was initially announced, we had not yet finalized the capabilities for 2.5 Flash-Lite. We also launched with a “thinking” and “non-thinking price”, which led to developer confusion.&lt;/p&gt;&lt;p&gt;With the stable version of Gemini 2.5 Flash rolling out (which is the same 05-20 model preview we made available at Google I/O), and the incredible performance of 2.5 Flash, we are updating the pricing for 2.5 Flash:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;$0.30 / 1M input tokens (*up from $0.15 input)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;$2.50 / 1M output tokens (*down from $3.50 output)&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;We removed the thinking vs. non-thinking price difference&lt;/li&gt;&lt;/ul&gt;&lt;ul&gt;&lt;li&gt;We kept a single price tier regardless of input token size&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;While we strive to maintain consistent pricing between preview and stable releases to minimize disruption, this is a specific adjustment reflecting Flash’s exceptional value, still offering the best cost-per-intelligence available.&lt;/p&gt;&lt;p&gt;And with Gemini 2.5 Flash-Lite, we now have an even lower cost option (with or without thinking) for cost and latency sensitive use cases that require less model intelligence.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Pricing updates for our Gemini Flash family" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/gemini_2-5_flash-lite_pricing_margin_dark.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Pricing updates for our Gemini Flash family
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;p&gt;If you are using the Gemini 2.5 Flash Preview 04-17 , the existing preview pricing will remain in effect until its planned deprecation on July 15, 2025, at which point that model endpoint will be turned off. You can transition to the generally available model “gemini-2.5-flash”, or switch to 2.5 Flash-Lite Preview as a lower cost option.&lt;/p&gt;&lt;h2 id="continued-growth-of-gemini-2.5-pro"&gt;&lt;b&gt;&lt;br /&gt;&lt;/b&gt;Continued growth of Gemini 2.5 Pro&lt;/h2&gt;&lt;p&gt;The growth and demand for Gemini 2.5 Pro continues to be the steepest of any of our models we have ever seen. To allow more customers to build on this model in production, we are making the 06-05 version of the model stable, with the same pareto frontier price point as before.&lt;/p&gt;&lt;p&gt;We expect that cases where you need the highest intelligence and most capabilities are where you will see Pro shine, like coding and agentic tasks. Gemini 2.5 Pro is at the heart of many of the most loved developer tools.&lt;/p&gt;
&lt;/div&gt;&lt;div&gt;
        &lt;div class="image-wrapper"&gt;
            
                &lt;img alt="Top developer tools using Gemini 2.5 Pro, featuring Cursor, Bolt, Cline, Cognition, Windsurf, GitHub, Lovable, Replit, and Zed Industries" class="regular-image" src="https://storage.googleapis.com/gweb-developer-goog-blog-assets/images/Gemini-2-5-pro-top-developer-tools.original.png" /&gt;
            
            
                
                    &lt;p&gt;
                        Top developer tools using Gemini 2.5 Pro
                    &lt;/p&gt;
                
            
        &lt;/div&gt;
    &lt;/div&gt;&lt;div&gt;
    &lt;p&gt;If you are using 2.5 Pro Preview 05-06, the model will remain available until June 19, 2025 and then will be turned off. If you are using 2.5 Pro Preview 06-05, you can simply update your model string to “gemini-2.5-pro”.&lt;/p&gt;&lt;p&gt;We can’t wait to see even more domains benefit from the intelligence of 2.5 Pro and look forward to sharing more about scaling beyond Pro in the near future.&lt;/p&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://deepmind.google/discover/blog/gemini-25-updates-to-our-family-of-thinking-models/</guid><pubDate>Tue, 17 Jun 2025 16:03:39 +0000</pubDate></item><item><title>OpenAI weighs “nuclear option” of antitrust complaint against Microsoft (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/openai-weighs-nuclear-option-of-antitrust-complaint-against-microsoft/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        WSJ report says OpenAI mulling federal complaint as Microsoft stalls restructuring plan.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI / Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI executives have discussed filing an antitrust complaint with US regulators against Microsoft, the company's largest investor, The Wall Street Journal reported Monday, marking a dramatic escalation in tensions between the two long-term AI partners. OpenAI, which develops ChatGPT, has reportedly considered seeking a federal regulatory review of the terms of its contract with Microsoft for potential antitrust law violations, according to people familiar with the matter.&lt;/p&gt;
&lt;p&gt;The potential antitrust complaint would likely argue that Microsoft is using its dominant position in cloud services and contractual leverage to suppress competition, according to insiders who described it as a "nuclear option," the WSJ reports.&lt;/p&gt;
&lt;p&gt;The move could unravel one of the most important business partnerships in the AI industry—a relationship that started with a $1 billion investment by Microsoft in 2019 and has grown to include billions more in funding, along with Microsoft's exclusive rights to host OpenAI models on its Azure cloud platform.&lt;/p&gt;
&lt;p&gt;The friction centers on OpenAI's efforts to transition from its current nonprofit structure into a public benefit corporation, a conversion that needs Microsoft's approval to complete. The two companies have not been able to agree on details after months of negotiations, sources told Reuters. OpenAI's existing for-profit arm would become a Delaware-based public benefit corporation under the proposed restructuring.&lt;/p&gt;
&lt;p&gt;The companies are discussing revising the terms of Microsoft's investment, including the future equity stake it will hold in OpenAI. According to The Information, OpenAI wants Microsoft to hold a 33 percent stake in a restructured unit in exchange for foregoing rights to future profits. The AI company also wants to modify existing clauses that give Microsoft exclusive rights to host OpenAI models in its cloud.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The restructuring debate attracted criticism from multiple quarters. Elon Musk alleges that OpenAI violated contract provisions by prioritizing profit over the public good in its push to advance AI and has sued to block the conversion. In December, Meta Platforms also asked California's attorney general to block OpenAI's conversion to a for-profit company.&lt;/p&gt;
&lt;h2&gt;Partnership strained as OpenAI sought new cloud providers&lt;/h2&gt;
&lt;p&gt;Last year, the US Federal Trade Commission under the Biden administration began examining the Microsoft-OpenAI partnership in more detail. Microsoft's $13 billion investment in OpenAI prompted competitors like Google to argue that the deal harmed competition by saddling rivals with extra costs and blocking them from hosting OpenAI's latest models themselves, along with what were then Microsoft's exclusive rights to host OpenAI models on its Azure cloud platform.&lt;/p&gt;
&lt;p&gt;However, Microsoft is no longer OpenAI's exclusive cloud provider as of January, and earlier this month, Reuters reported that OpenAI plans to add Alphabet's Google Cloud service to meet its growing needs for computing capacity, marking a notable collaboration between two AI rivals. The shift came alongside OpenAI's participation in the Stargate Project, a new company that intends to invest $500 billion over the next four years in building new AI infrastructure for OpenAI in the United States.&lt;/p&gt;
&lt;p&gt;Meanwhile, Microsoft and OpenAI have maintained a public front of cooperation. "Talks are ongoing and we are optimistic we will continue to build together for years to come," they said in a joint statement to Reuters. In January, a Microsoft blog post noted that "key elements" of the partnership remain in place for the duration of their contract through 2030, including revenue-sharing arrangements. Microsoft has the rights to OpenAI IP (including models and infrastructure) for use within products like Copilot through 2030.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        WSJ report says OpenAI mulling federal complaint as Microsoft stalls restructuring plan.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="absolute inset-0 w-full h-full object-cover hidden" height="169" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-300x169.jpg" width="300" /&gt;
                  &lt;img alt="The OpenAI logo superimposed over a Microsoft logo background" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/07/openai_microsoft_3-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Benj Edwards / OpenAI / Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;OpenAI executives have discussed filing an antitrust complaint with US regulators against Microsoft, the company's largest investor, The Wall Street Journal reported Monday, marking a dramatic escalation in tensions between the two long-term AI partners. OpenAI, which develops ChatGPT, has reportedly considered seeking a federal regulatory review of the terms of its contract with Microsoft for potential antitrust law violations, according to people familiar with the matter.&lt;/p&gt;
&lt;p&gt;The potential antitrust complaint would likely argue that Microsoft is using its dominant position in cloud services and contractual leverage to suppress competition, according to insiders who described it as a "nuclear option," the WSJ reports.&lt;/p&gt;
&lt;p&gt;The move could unravel one of the most important business partnerships in the AI industry—a relationship that started with a $1 billion investment by Microsoft in 2019 and has grown to include billions more in funding, along with Microsoft's exclusive rights to host OpenAI models on its Azure cloud platform.&lt;/p&gt;
&lt;p&gt;The friction centers on OpenAI's efforts to transition from its current nonprofit structure into a public benefit corporation, a conversion that needs Microsoft's approval to complete. The two companies have not been able to agree on details after months of negotiations, sources told Reuters. OpenAI's existing for-profit arm would become a Delaware-based public benefit corporation under the proposed restructuring.&lt;/p&gt;
&lt;p&gt;The companies are discussing revising the terms of Microsoft's investment, including the future equity stake it will hold in OpenAI. According to The Information, OpenAI wants Microsoft to hold a 33 percent stake in a restructured unit in exchange for foregoing rights to future profits. The AI company also wants to modify existing clauses that give Microsoft exclusive rights to host OpenAI models in its cloud.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The restructuring debate attracted criticism from multiple quarters. Elon Musk alleges that OpenAI violated contract provisions by prioritizing profit over the public good in its push to advance AI and has sued to block the conversion. In December, Meta Platforms also asked California's attorney general to block OpenAI's conversion to a for-profit company.&lt;/p&gt;
&lt;h2&gt;Partnership strained as OpenAI sought new cloud providers&lt;/h2&gt;
&lt;p&gt;Last year, the US Federal Trade Commission under the Biden administration began examining the Microsoft-OpenAI partnership in more detail. Microsoft's $13 billion investment in OpenAI prompted competitors like Google to argue that the deal harmed competition by saddling rivals with extra costs and blocking them from hosting OpenAI's latest models themselves, along with what were then Microsoft's exclusive rights to host OpenAI models on its Azure cloud platform.&lt;/p&gt;
&lt;p&gt;However, Microsoft is no longer OpenAI's exclusive cloud provider as of January, and earlier this month, Reuters reported that OpenAI plans to add Alphabet's Google Cloud service to meet its growing needs for computing capacity, marking a notable collaboration between two AI rivals. The shift came alongside OpenAI's participation in the Stargate Project, a new company that intends to invest $500 billion over the next four years in building new AI infrastructure for OpenAI in the United States.&lt;/p&gt;
&lt;p&gt;Meanwhile, Microsoft and OpenAI have maintained a public front of cooperation. "Talks are ongoing and we are optimistic we will continue to build together for years to come," they said in a joint statement to Reuters. In January, a Microsoft blog post noted that "key elements" of the partnership remain in place for the duration of their contract through 2030, including revenue-sharing arrangements. Microsoft has the rights to OpenAI IP (including models and infrastructure) for use within products like Copilot through 2030.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/openai-weighs-nuclear-option-of-antitrust-complaint-against-microsoft/</guid><pubDate>Tue, 17 Jun 2025 16:26:28 +0000</pubDate></item><item><title>AI Safety Newsletter #57: The RAISE Act (AI Safety Newsletter)</title><link>https://newsletter.safe.ai/p/ai-safety-newsletter-57-the-raise</link><description>&lt;p&gt;&lt;span&gt;Welcome to the AI Safety Newsletter by the &lt;/span&gt;&lt;a href="https://www.safe.ai/" rel="rel"&gt;Center for AI Safety&lt;/a&gt;&lt;span&gt;. We discuss developments in AI and AI safety. No technical background required.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this edition: The New York Legislature passes an act regulating frontier AI—but it may not be signed into law for some time.&lt;/p&gt;&lt;p&gt;&lt;span&gt;Listen to the AI Safety Newsletter for free on &lt;/span&gt;&lt;a href="https://spotify.link/E6lHa1ij2Cb" rel="rel"&gt;Spotify&lt;/a&gt;&lt;span&gt; or &lt;/span&gt;&lt;a href="https://podcasts.apple.com/us/podcast/ai-safety-newsletter/id1702875110" rel="rel"&gt;Apple Podcasts&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;New York may soon become the first state to regulate frontier AI systems. On June 12, the state’s legislature &lt;/span&gt;&lt;a href="https://www.senatorgounardes.nyc/raise-act-release" rel="rel"&gt;passed&lt;/a&gt;&lt;span&gt; the Responsible AI Safety and Education (RAISE) Act. If New York Governor Kathy Hochul signs it into law, the &lt;/span&gt;&lt;a href="https://www.nysenate.gov/legislation/bills/2025/S6953/amendment/B" rel="rel"&gt;RAISE Act&lt;/a&gt;&lt;span&gt; will be the most significant state AI legislation in the U.S.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;New York’s RAISE Act imposes four guardrails on frontier labs: &lt;/strong&gt;&lt;span&gt;developers must publish a safety plan, hold back unreasonably risky models, disclose major incidents, and face penalties for non-compliance.&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Publish and maintain a safety plan.&lt;/strong&gt;&lt;span&gt; Before deployment, developers must post a redacted “safety and security protocol,” transmit the plan to both the attorney general and the Division of Homeland Security and Emergency Services, keep the unredacted version—plus all supporting test data—for five years, and review the plan each year.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Withhold any model that presents an “unreasonable risk of critical harm.”&lt;/strong&gt;&lt;span&gt; Developers must delay their release and work to reduce risk if evaluations show the system poses an unreasonable risk of causing at least 100 deaths or $1 billion in damage through weapons of mass destruction or automated criminal activity.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Report safety incidents within seventy-two hours.&lt;/strong&gt;&lt;span&gt; If developers discover the theft of model weights, evidence of dangerous autonomous behavior, or other events that demonstrably raises the risk of critical harm, they must report their discovery to state officials within three days.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Penalties for non-compliance.&lt;/strong&gt;&lt;span&gt; The NY attorney general may seek up to $10 million for a first violation and $30 million for subsequent violations.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;The RAISE Act only regulates the largest developers.&lt;/strong&gt;&lt;span&gt; Mirroring California’s SB 1047—&lt;/span&gt;&lt;a href="https://www.npr.org/2024/09/20/nx-s1-5119792/newsom-ai-bill-california-sb1047-tech#:~:text=How%20Memphis%20became%20a%20battleground,growth%20for%20early%2Dstage%20companies." rel="rel"&gt;vetoed by&lt;/a&gt;&lt;span&gt; Governor Gavin Newsom in 2024—the Act covers any model costing at least $100 million in compute.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Obligations fall on developers that have trained at least one frontier model and spent a cumulative $100 million on such training—and on anyone who later buys the model’s full intellectual-property rights. Accredited colleges are exempt when conducting academic research, but commercial spin-outs are not. These carve-outs serve to focus the legal burden onto the handful of firms capable of creating catastrophic harms.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;While New York acts, the U.S. Congress weighs a federal moratorium on state AI regulation.&lt;/strong&gt;&lt;span&gt; The “One Big Beautiful Bill Act,” the budget reconciliation package the U.S. House of Representatives approved on May 22, contained a &lt;/span&gt;&lt;a href="https://apnews.com/article/ai-regulation-state-moratorium-congress-39d1c8a0758ffe0242283bb82f66d51a" rel="rel"&gt;10‑year federal moratorium&lt;/a&gt;&lt;span&gt; on “any law or regulation” that “restricts, governs or conditions” the design, deployment, or use of AI systems.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The moratorium was originally unlikely to pass the Senate’s Byrd Rule, which prohibits policy provisions from being included in budget reconciliation bills. The Senate Commerce Committee, chaired by Cruz, recently &lt;/span&gt;&lt;a href="https://www.politico.com/live-updates/2025/06/05/congress/senate-commerce-megabill-frees-spectrum-ties-bead-to-ai-moratorium-00391136" rel="rel"&gt;revised&lt;/a&gt;&lt;span&gt; the moratorium such that it would be a prerequisite for states to receive billions in federal broadband expansion funds. This change could potentially bypass the Byrd rule.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;However, the proposed moratorium has drawn &lt;/span&gt;&lt;a href="https://x.com/RepMTG/status/1930650431253827806?t=rK_HvP4W2eb3qIB-FikMjw" rel="rel"&gt;criticism&lt;/a&gt;&lt;span&gt; from some Republican lawmakers—including the &lt;/span&gt;&lt;a href="https://www.politico.com/newsletters/future-pulse/2025/06/16/state-ai-laws-could-get-a-reprieve-00408596" rel="rel"&gt;House Freedom Caucus&lt;/a&gt;&lt;span&gt;—who may be crucial to its survival. A &lt;/span&gt;&lt;a href="https://www.commonsensemedia.org/press-releases/new-poll-reveals-strong-bipartisan-opposition-to-proposed-ban-on-state-ai-laws" rel="rel"&gt;recent poll found&lt;/a&gt;&lt;span&gt; that proposal appears to be unpopular with the party’s base, with 50 percent of Republican voters saying they opposed the moratorium compared to 30 percent saying they supported it. Last week, a bipartisan group of 260 state legislators also wrote &lt;/span&gt;&lt;a href="https://ari.us/state-lawmakers-urge-congress-to-drop-ai-law-preemption/" rel="rel"&gt;a letter&lt;/a&gt;&lt;span&gt; to congress opposing the moratorium.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The RAISE Act isn’t law yet&lt;/strong&gt;&lt;span&gt;. Although both chambers have passed the bill, they have not yet delivered it to Governor Kathy Hochul—a step lawmakers can take at any point during 2025.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaa39fa0-a05c-4785-9130-ab331a0e0e34_1600x427.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="389" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaa39fa0-a05c-4785-9130-ab331a0e0e34_1600x427.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;figcaption class="image-caption"&gt;&lt;em&gt;&lt;span&gt;A diagram depicting the bill’s current status. &lt;/span&gt;&lt;a href="https://www.nysenate.gov/legislation/bills/2025/S6953/amendment/B" rel="rel"&gt;Source&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;Once the bill is finally sent, Hochul will have up to 30 days to sign it, veto it, or negotiate “chapter amendments,” the back-and-forth revisions governors often use to tweak language before giving final approval. Until that clock starts, the measure sits in limbo, and its ultimate shape—possibly even its survival—remains an open question.&lt;/p&gt;&lt;p&gt;Government&lt;/p&gt;&lt;p&gt;Industry&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Google &lt;/span&gt;&lt;a href="https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/" rel="rel"&gt;released&lt;/a&gt;&lt;span&gt; an upgraded preview of Gemini 2.5 Pro, which scores the highest on most benchmarks.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Sam Altman wrote a &lt;/span&gt;&lt;a href="https://blog.samaltman.com/the-gentle-singularity" rel="rel"&gt;new blog post&lt;/a&gt;&lt;span&gt; discussing how an intelligence recursion would be rapid but “gentle”: "If we can do a decade’s worth of research in a year, or a month, then the rate of progress will obviously be quite different."&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Meta &lt;/span&gt;&lt;a href="https://apnews.com/article/meta-ai-superintelligence-agi-scale-alexandr-wang-4b55aabf7ea018e38ffdccb66e37cf26" rel="rel"&gt;invested&lt;/a&gt;&lt;span&gt; $14.3 billion in Scale AI and hired its CEO Alexandr Wang to run a new superintelligence team.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Civil Society&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;David “davidad” Dalrymple &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/ai-grid-blackouts-guarantees" rel="rel"&gt;argues&lt;/a&gt;&lt;span&gt; that in order to fulfill the potential of AI in safety-critical domains like energy grids, we need to develop more robust, mathematical guarantees of safety.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kevin Frazier &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/options-for-ai-liability" rel="rel"&gt;writes&lt;/a&gt;&lt;span&gt; about how in the absence of federal legislation, the burden of managing AI risks has fallen to judges and state legislators—actors lacking the tools needed to ensure consistency, enforceability, or fairness.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vanessa Bates Ramirez &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/ai-friends-openai-study" rel="rel"&gt;writes&lt;/a&gt;&lt;span&gt; that, while AI is increasingly being used for emotional support, research from OpenAI and MIT raises concerns that it may leave some users feeling even worse.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Nora Ammann and Sarah Hastings-Woodhouse &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/ai-arms-race-assurance-technologies" rel="rel"&gt;discuss&lt;/a&gt;&lt;span&gt; how assurance technologies could help de-escalate an AI arms race.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Epoch AI &lt;/span&gt;&lt;a href="https://epoch.ai/data/ai-supercomputers?view=map#explore-the-data" rel="rel"&gt;released&lt;/a&gt;&lt;span&gt; a dataset that maps the world’s largest AI supercomputers.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Yoshua Bengio launched &lt;/span&gt;&lt;a href="https://lawzero.org/en" rel="rel"&gt;LawZero&lt;/a&gt;&lt;span&gt;, a nonprofit advancing “safe-by-design” AI.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;See also: &lt;/span&gt;&lt;a href="https://x.com/ai_risks?lang=en" rel="rel"&gt;CAIS’ X account&lt;/a&gt;&lt;span&gt;, our paper on &lt;/span&gt;&lt;a href="https://www.nationalsecurity.ai/" rel="rel"&gt;superintelligence strategy&lt;/a&gt;&lt;span&gt;, our &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/" rel="rel"&gt;AI safety course&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="http://ai-frontiers.org/" rel="rel"&gt;AI Frontiers&lt;/a&gt;&lt;span&gt;, a new platform for expert commentary and analysis.&lt;/span&gt;&lt;/p&gt;&lt;p class="button-wrapper"&gt;&lt;a class="button primary" href="https://newsletter.safe.ai/p/ai-safety-newsletter-57-the-raise?utm_source=substack&amp;amp;utm_medium=email&amp;amp;utm_content=share&amp;amp;action=share" rel="rel"&gt;&lt;span&gt;Share&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</description><content:encoded>&lt;p&gt;&lt;span&gt;Welcome to the AI Safety Newsletter by the &lt;/span&gt;&lt;a href="https://www.safe.ai/" rel="rel"&gt;Center for AI Safety&lt;/a&gt;&lt;span&gt;. We discuss developments in AI and AI safety. No technical background required.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;In this edition: The New York Legislature passes an act regulating frontier AI—but it may not be signed into law for some time.&lt;/p&gt;&lt;p&gt;&lt;span&gt;Listen to the AI Safety Newsletter for free on &lt;/span&gt;&lt;a href="https://spotify.link/E6lHa1ij2Cb" rel="rel"&gt;Spotify&lt;/a&gt;&lt;span&gt; or &lt;/span&gt;&lt;a href="https://podcasts.apple.com/us/podcast/ai-safety-newsletter/id1702875110" rel="rel"&gt;Apple Podcasts&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;New York may soon become the first state to regulate frontier AI systems. On June 12, the state’s legislature &lt;/span&gt;&lt;a href="https://www.senatorgounardes.nyc/raise-act-release" rel="rel"&gt;passed&lt;/a&gt;&lt;span&gt; the Responsible AI Safety and Education (RAISE) Act. If New York Governor Kathy Hochul signs it into law, the &lt;/span&gt;&lt;a href="https://www.nysenate.gov/legislation/bills/2025/S6953/amendment/B" rel="rel"&gt;RAISE Act&lt;/a&gt;&lt;span&gt; will be the most significant state AI legislation in the U.S.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;New York’s RAISE Act imposes four guardrails on frontier labs: &lt;/strong&gt;&lt;span&gt;developers must publish a safety plan, hold back unreasonably risky models, disclose major incidents, and face penalties for non-compliance.&lt;/span&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Publish and maintain a safety plan.&lt;/strong&gt;&lt;span&gt; Before deployment, developers must post a redacted “safety and security protocol,” transmit the plan to both the attorney general and the Division of Homeland Security and Emergency Services, keep the unredacted version—plus all supporting test data—for five years, and review the plan each year.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Withhold any model that presents an “unreasonable risk of critical harm.”&lt;/strong&gt;&lt;span&gt; Developers must delay their release and work to reduce risk if evaluations show the system poses an unreasonable risk of causing at least 100 deaths or $1 billion in damage through weapons of mass destruction or automated criminal activity.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Report safety incidents within seventy-two hours.&lt;/strong&gt;&lt;span&gt; If developers discover the theft of model weights, evidence of dangerous autonomous behavior, or other events that demonstrably raises the risk of critical harm, they must report their discovery to state officials within three days.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;Penalties for non-compliance.&lt;/strong&gt;&lt;span&gt; The NY attorney general may seek up to $10 million for a first violation and $30 million for subsequent violations.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;The RAISE Act only regulates the largest developers.&lt;/strong&gt;&lt;span&gt; Mirroring California’s SB 1047—&lt;/span&gt;&lt;a href="https://www.npr.org/2024/09/20/nx-s1-5119792/newsom-ai-bill-california-sb1047-tech#:~:text=How%20Memphis%20became%20a%20battleground,growth%20for%20early%2Dstage%20companies." rel="rel"&gt;vetoed by&lt;/a&gt;&lt;span&gt; Governor Gavin Newsom in 2024—the Act covers any model costing at least $100 million in compute.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;Obligations fall on developers that have trained at least one frontier model and spent a cumulative $100 million on such training—and on anyone who later buys the model’s full intellectual-property rights. Accredited colleges are exempt when conducting academic research, but commercial spin-outs are not. These carve-outs serve to focus the legal burden onto the handful of firms capable of creating catastrophic harms.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;While New York acts, the U.S. Congress weighs a federal moratorium on state AI regulation.&lt;/strong&gt;&lt;span&gt; The “One Big Beautiful Bill Act,” the budget reconciliation package the U.S. House of Representatives approved on May 22, contained a &lt;/span&gt;&lt;a href="https://apnews.com/article/ai-regulation-state-moratorium-congress-39d1c8a0758ffe0242283bb82f66d51a" rel="rel"&gt;10‑year federal moratorium&lt;/a&gt;&lt;span&gt; on “any law or regulation” that “restricts, governs or conditions” the design, deployment, or use of AI systems.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;The moratorium was originally unlikely to pass the Senate’s Byrd Rule, which prohibits policy provisions from being included in budget reconciliation bills. The Senate Commerce Committee, chaired by Cruz, recently &lt;/span&gt;&lt;a href="https://www.politico.com/live-updates/2025/06/05/congress/senate-commerce-megabill-frees-spectrum-ties-bead-to-ai-moratorium-00391136" rel="rel"&gt;revised&lt;/a&gt;&lt;span&gt; the moratorium such that it would be a prerequisite for states to receive billions in federal broadband expansion funds. This change could potentially bypass the Byrd rule.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;span&gt;However, the proposed moratorium has drawn &lt;/span&gt;&lt;a href="https://x.com/RepMTG/status/1930650431253827806?t=rK_HvP4W2eb3qIB-FikMjw" rel="rel"&gt;criticism&lt;/a&gt;&lt;span&gt; from some Republican lawmakers—including the &lt;/span&gt;&lt;a href="https://www.politico.com/newsletters/future-pulse/2025/06/16/state-ai-laws-could-get-a-reprieve-00408596" rel="rel"&gt;House Freedom Caucus&lt;/a&gt;&lt;span&gt;—who may be crucial to its survival. A &lt;/span&gt;&lt;a href="https://www.commonsensemedia.org/press-releases/new-poll-reveals-strong-bipartisan-opposition-to-proposed-ban-on-state-ai-laws" rel="rel"&gt;recent poll found&lt;/a&gt;&lt;span&gt; that proposal appears to be unpopular with the party’s base, with 50 percent of Republican voters saying they opposed the moratorium compared to 30 percent saying they supported it. Last week, a bipartisan group of 260 state legislators also wrote &lt;/span&gt;&lt;a href="https://ari.us/state-lawmakers-urge-congress-to-drop-ai-law-preemption/" rel="rel"&gt;a letter&lt;/a&gt;&lt;span&gt; to congress opposing the moratorium.&lt;/span&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;The RAISE Act isn’t law yet&lt;/strong&gt;&lt;span&gt;. Although both chambers have passed the bill, they have not yet delivered it to Governor Kathy Hochul—a step lawmakers can take at any point during 2025.&lt;/span&gt;&lt;/p&gt;&lt;div class="captioned-image-container"&gt;&lt;figure&gt;&lt;a class="image-link image2 is-viewable-img" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaa39fa0-a05c-4785-9130-ab331a0e0e34_1600x427.png" rel="rel" target="_blank"&gt;&lt;div class="image2-inset"&gt;&lt;source type="image/webp" /&gt;&lt;img alt="alt" class="sizing-normal" height="389" src="https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faaa39fa0-a05c-4785-9130-ab331a0e0e34_1600x427.png" width="1456" /&gt;&lt;/div&gt;&lt;/a&gt;&lt;figcaption class="image-caption"&gt;&lt;em&gt;&lt;span&gt;A diagram depicting the bill’s current status. &lt;/span&gt;&lt;a href="https://www.nysenate.gov/legislation/bills/2025/S6953/amendment/B" rel="rel"&gt;Source&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;/div&gt;&lt;p&gt;Once the bill is finally sent, Hochul will have up to 30 days to sign it, veto it, or negotiate “chapter amendments,” the back-and-forth revisions governors often use to tweak language before giving final approval. Until that clock starts, the measure sits in limbo, and its ultimate shape—possibly even its survival—remains an open question.&lt;/p&gt;&lt;p&gt;Government&lt;/p&gt;&lt;p&gt;Industry&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Google &lt;/span&gt;&lt;a href="https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/" rel="rel"&gt;released&lt;/a&gt;&lt;span&gt; an upgraded preview of Gemini 2.5 Pro, which scores the highest on most benchmarks.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Sam Altman wrote a &lt;/span&gt;&lt;a href="https://blog.samaltman.com/the-gentle-singularity" rel="rel"&gt;new blog post&lt;/a&gt;&lt;span&gt; discussing how an intelligence recursion would be rapid but “gentle”: "If we can do a decade’s worth of research in a year, or a month, then the rate of progress will obviously be quite different."&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Meta &lt;/span&gt;&lt;a href="https://apnews.com/article/meta-ai-superintelligence-agi-scale-alexandr-wang-4b55aabf7ea018e38ffdccb66e37cf26" rel="rel"&gt;invested&lt;/a&gt;&lt;span&gt; $14.3 billion in Scale AI and hired its CEO Alexandr Wang to run a new superintelligence team.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Civil Society&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;David “davidad” Dalrymple &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/ai-grid-blackouts-guarantees" rel="rel"&gt;argues&lt;/a&gt;&lt;span&gt; that in order to fulfill the potential of AI in safety-critical domains like energy grids, we need to develop more robust, mathematical guarantees of safety.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Kevin Frazier &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/options-for-ai-liability" rel="rel"&gt;writes&lt;/a&gt;&lt;span&gt; about how in the absence of federal legislation, the burden of managing AI risks has fallen to judges and state legislators—actors lacking the tools needed to ensure consistency, enforceability, or fairness.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Vanessa Bates Ramirez &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/ai-friends-openai-study" rel="rel"&gt;writes&lt;/a&gt;&lt;span&gt; that, while AI is increasingly being used for emotional support, research from OpenAI and MIT raises concerns that it may leave some users feeling even worse.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Nora Ammann and Sarah Hastings-Woodhouse &lt;/span&gt;&lt;a href="https://ai-frontiers.org/articles/ai-arms-race-assurance-technologies" rel="rel"&gt;discuss&lt;/a&gt;&lt;span&gt; how assurance technologies could help de-escalate an AI arms race.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Epoch AI &lt;/span&gt;&lt;a href="https://epoch.ai/data/ai-supercomputers?view=map#explore-the-data" rel="rel"&gt;released&lt;/a&gt;&lt;span&gt; a dataset that maps the world’s largest AI supercomputers.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;span&gt;Yoshua Bengio launched &lt;/span&gt;&lt;a href="https://lawzero.org/en" rel="rel"&gt;LawZero&lt;/a&gt;&lt;span&gt;, a nonprofit advancing “safe-by-design” AI.&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;span&gt;See also: &lt;/span&gt;&lt;a href="https://x.com/ai_risks?lang=en" rel="rel"&gt;CAIS’ X account&lt;/a&gt;&lt;span&gt;, our paper on &lt;/span&gt;&lt;a href="https://www.nationalsecurity.ai/" rel="rel"&gt;superintelligence strategy&lt;/a&gt;&lt;span&gt;, our &lt;/span&gt;&lt;a href="https://www.aisafetybook.com/" rel="rel"&gt;AI safety course&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="http://ai-frontiers.org/" rel="rel"&gt;AI Frontiers&lt;/a&gt;&lt;span&gt;, a new platform for expert commentary and analysis.&lt;/span&gt;&lt;/p&gt;&lt;p class="button-wrapper"&gt;&lt;a class="button primary" href="https://newsletter.safe.ai/p/ai-safety-newsletter-57-the-raise?utm_source=substack&amp;amp;utm_medium=email&amp;amp;utm_content=share&amp;amp;action=share" rel="rel"&gt;&lt;span&gt;Share&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://newsletter.safe.ai/p/ai-safety-newsletter-57-the-raise</guid><pubDate>Tue, 17 Jun 2025 16:30:41 +0000</pubDate></item><item><title>Anysphere launches a $200-a-month Cursor AI coding subscription (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/anysphere-launches-a-200-a-month-cursor-ai-coding-subscription/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-1356382582.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anysphere launched a new $200-a-month subscription plan for its popular AI coding tool, Cursor, the company announced in a blog post on Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new plan, Ultra, offers users 20x more usage on AI models from OpenAI, Anthropic, Google DeepMind, and xAI compared to the company’s $20-a-month subscription plan, Pro. Anysphere also says Cursor users on the Ultra plan will get priority access to new features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anysphere CEO Michael Truell said in the blog that the Ultra plan was made possible through multi-year partnerships with AI model providers. In the last several months, OpenAI, Anthropic, and Google DeepMind have similarly rolled out pricier subscription plans — ranging from $100 a month to $250 a month — as part of an effort to capitalize on their power users and offer them increased usage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new, pricy Cursor subscription plan seems poised to add revenue to Anysphere’s already booming business. Earlier this month, Anysphere announced that Cursor had reached $500 million in annualized recurring revenue and is being used by major companies such as Nvidia, Uber, and Adobe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anysphere was one of the fastest companies ever to reach $100 million in ARR, and it doesn’t seem to be slowing down. Based on TechCrunch’s previous reporting, Cursor’s ARR has grown by $200 million since April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the race to develop “vibe-coding” tools is heating up, and many of the AI model providers Cursor relies on are developing their own AI coding products. OpenAI has reportedly acquired Cursor’s competitor, Windsurf, to beef up its offerings of AI coding products. Meanwhile, Anthropic continues to develop Claude Code, its in-house AI coding tool that utilizes its popular AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unclear whether Anysphere can sustain this level of growth without bumping into AI model providers. We’ve seen more competitive tactics emerge in the AI coding space as these businesses have grown. For example, Anthropic recently slashed Windsurf’s direct access to Claude AI models as a means to undercut its largest competitor, OpenAI.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;At the same time, Anysphere has started putting more resources toward developing its own AI models that work alongside models from OpenAI and Anthropic. In May, Anysphere rolled out a new “Tab” AI model, which can suggest code changes across various files.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent interview with TechCrunch, Anthropic co-founder Jared Kaplan said he believed Anthropic would be working with Cursor for a long time. Anysphere’s multi-year partnerships with Anthropic and other AI model providers certainly suggest these relationships aren’t going anywhere just yet. Nonetheless, the competition for users is getting more intense.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/02/GettyImages-1356382582.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anysphere launched a new $200-a-month subscription plan for its popular AI coding tool, Cursor, the company announced in a blog post on Monday.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new plan, Ultra, offers users 20x more usage on AI models from OpenAI, Anthropic, Google DeepMind, and xAI compared to the company’s $20-a-month subscription plan, Pro. Anysphere also says Cursor users on the Ultra plan will get priority access to new features.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anysphere CEO Michael Truell said in the blog that the Ultra plan was made possible through multi-year partnerships with AI model providers. In the last several months, OpenAI, Anthropic, and Google DeepMind have similarly rolled out pricier subscription plans — ranging from $100 a month to $250 a month — as part of an effort to capitalize on their power users and offer them increased usage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new, pricy Cursor subscription plan seems poised to add revenue to Anysphere’s already booming business. Earlier this month, Anysphere announced that Cursor had reached $500 million in annualized recurring revenue and is being used by major companies such as Nvidia, Uber, and Adobe.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anysphere was one of the fastest companies ever to reach $100 million in ARR, and it doesn’t seem to be slowing down. Based on TechCrunch’s previous reporting, Cursor’s ARR has grown by $200 million since April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, the race to develop “vibe-coding” tools is heating up, and many of the AI model providers Cursor relies on are developing their own AI coding products. OpenAI has reportedly acquired Cursor’s competitor, Windsurf, to beef up its offerings of AI coding products. Meanwhile, Anthropic continues to develop Claude Code, its in-house AI coding tool that utilizes its popular AI models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s unclear whether Anysphere can sustain this level of growth without bumping into AI model providers. We’ve seen more competitive tactics emerge in the AI coding space as these businesses have grown. For example, Anthropic recently slashed Windsurf’s direct access to Claude AI models as a means to undercut its largest competitor, OpenAI.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;At the same time, Anysphere has started putting more resources toward developing its own AI models that work alongside models from OpenAI and Anthropic. In May, Anysphere rolled out a new “Tab” AI model, which can suggest code changes across various files.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a recent interview with TechCrunch, Anthropic co-founder Jared Kaplan said he believed Anthropic would be working with Cursor for a long time. Anysphere’s multi-year partnerships with Anthropic and other AI model providers certainly suggest these relationships aren’t going anywhere just yet. Nonetheless, the competition for users is getting more intense.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/anysphere-launches-a-200-a-month-cursor-ai-coding-subscription/</guid><pubDate>Tue, 17 Jun 2025 16:45:37 +0000</pubDate></item><item><title>Intel to lay off up to 20% of Intel Foundry workers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/intel-to-layoff-up-to-20-of-intel-foundry-workers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1831946092-e1732615535581.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Intel will begin a new round of layoffs next month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant plans to lay off 15% to 20% of workers in its Intel Foundry division starting in July, according to an internal memo originally reported on by The Oregonian. Intel Foundry designs, manufactures, and packages semiconductors for external clients.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s unclear how many workers this will directly impact. Intel’s total workforce was 108,900 people as of December 2024, according to the company’s annual regulatory filing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Intel for more information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This news is not a particular shock. Since Intel’s CEO Lip-Bu Tan took the helm in March, he’s indicated several times that the company needs to refocus on its core business units, flatten its organization structure, and return to being an engineering-first company. Rumors of these specific layoffs began swirling in April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tan also told Intel’s customers it would spin off its noncore units at the company’s Intel Vision conference in March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel laid off 15% of its staff, around 15,000 employees, last August.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/04/GettyImages-1831946092-e1732615535581.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Intel will begin a new round of layoffs next month.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The semiconductor giant plans to lay off 15% to 20% of workers in its Intel Foundry division starting in July, according to an internal memo originally reported on by The Oregonian. Intel Foundry designs, manufactures, and packages semiconductors for external clients.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;It’s unclear how many workers this will directly impact. Intel’s total workforce was 108,900 people as of December 2024, according to the company’s annual regulatory filing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to Intel for more information.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This news is not a particular shock. Since Intel’s CEO Lip-Bu Tan took the helm in March, he’s indicated several times that the company needs to refocus on its core business units, flatten its organization structure, and return to being an engineering-first company. Rumors of these specific layoffs began swirling in April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tan also told Intel’s customers it would spin off its noncore units at the company’s Intel Vision conference in March.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Intel laid off 15% of its staff, around 15,000 employees, last August.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/intel-to-layoff-up-to-20-of-intel-foundry-workers/</guid><pubDate>Tue, 17 Jun 2025 17:50:32 +0000</pubDate></item><item><title>Google’s Gemini AI family updated with stable 2.5 Pro, super-efficient 2.5 Flash-Lite (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/06/googles-gemini-ai-family-updated-with-stable-2-5-pro-super-efficient-2-5-flash-lite/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google says Gemini 2.5 is stable and ready for developers to build on.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini 2.5 I/O keynote" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini-2.5-Tulsee-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini 2.5 I/O keynote" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini-2.5-Tulsee-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google's Tulsee Doshi talks Gemini 2.5 at I/O 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google has announced a big expansion of its Gemini AI model family today. After months of tweaking and tuning, the high-power Gemini 2.5 Pro is leaving preview and is ready for developers to build on. Meanwhile, Google is offering a peek at its upcoming high-efficiency model, known as Gemini 2.5 Pro Flash-Lite. Try as it might, Google can't get away from confusing model names.&lt;/p&gt;
&lt;p&gt;Google's AI aspirations have been looking up in 2025 with the debut of Gemini 2.5. These models showed a marked improvement over past versions, making Google more competitive with OpenAI and its popular GPT models. However, we've been inundated with previews and test builds as Google works toward general availability, which means a model is stable enough for long-term development work.&lt;/p&gt;
&lt;p&gt;The 2.5 Flash model left preview at I/O, but Gemini 2.5 Pro lagged behind. Today, Flash is hitting general availability with the 04-17 build. Gemini 2.5 Pro is leaving preview and also reaching general availability, and as predicted, the recently revamped 06-05 build is the winner. This version aimed to address some issues that popped up in the Google I/O build of 2.5 Pro, and it appears to have worked.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2101453 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini final models" class="fullwidth full" height="967" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini-models-june.png" width="1059" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google now has a Gemini model for every task.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;All Gemini 2.5 models include adjustable thinking budgets, making them appealing to developers who want more control over costs. For the most price-sensitive devs, Google is also introducing Gemini 2.5 Flash-Lite, which was previously experimental. This model is now in preview, offering a way to run high-volume AI workloads without incurring significant costs. Compared to 2.5 Flash, it's one-third of the cost for text, image, and video inputs and less than one-sixth of the cost for output tokens. It's unlikely this variant of Gemini will come to the app for regular users, because it's less capable than 2.5 Flash and only makes sense when you're paying by the token.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Additionally, Google Flash and Flash-Lite are coming to search. A Google spokesperson tells Ars that custom versions of these models are now active in AI overviews and AI Mode. Google generally tries to use the model that's the best fit for the query. So, complex AI Mode searches use 2.5 Pro, but something simple could rely on Flash or even Flash-Lite for very basic searches.&lt;/p&gt;
&lt;p&gt;The Flash-Lite preview is available in Google AI Studio and Vertex AI for developers, along with stable versions of Gemini 2.5 Flash and Gemini 2.5 Pro. You won't see any major changes in the Gemini app, as the final 2.5 Pro and 2.5 Flash models were already live in the app. The Pro variant will drop the preview label as Flash did last month, but functionality won't be any different. Free users still have limited access to 2.5 Pro in the app, with paying Pro users getting a higher limit of 100 prompts per day. AI Ultra subscribers have the highest level of access to Gemini 2.5 Pro.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google says Gemini 2.5 is stable and ready for developers to build on.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Gemini 2.5 I/O keynote" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini-2.5-Tulsee-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Gemini 2.5 I/O keynote" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini-2.5-Tulsee-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google's Tulsee Doshi talks Gemini 2.5 at I/O 2025.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Ryan Whitwam

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google has announced a big expansion of its Gemini AI model family today. After months of tweaking and tuning, the high-power Gemini 2.5 Pro is leaving preview and is ready for developers to build on. Meanwhile, Google is offering a peek at its upcoming high-efficiency model, known as Gemini 2.5 Pro Flash-Lite. Try as it might, Google can't get away from confusing model names.&lt;/p&gt;
&lt;p&gt;Google's AI aspirations have been looking up in 2025 with the debut of Gemini 2.5. These models showed a marked improvement over past versions, making Google more competitive with OpenAI and its popular GPT models. However, we've been inundated with previews and test builds as Google works toward general availability, which means a model is stable enough for long-term development work.&lt;/p&gt;
&lt;p&gt;The 2.5 Flash model left preview at I/O, but Gemini 2.5 Pro lagged behind. Today, Flash is hitting general availability with the 04-17 build. Gemini 2.5 Pro is leaving preview and also reaching general availability, and as predicted, the recently revamped 06-05 build is the winner. This version aimed to address some issues that popped up in the Google I/O build of 2.5 Pro, and it appears to have worked.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2101453 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="Gemini final models" class="fullwidth full" height="967" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/Gemini-models-june.png" width="1059" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Google now has a Gemini model for every task.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;All Gemini 2.5 models include adjustable thinking budgets, making them appealing to developers who want more control over costs. For the most price-sensitive devs, Google is also introducing Gemini 2.5 Flash-Lite, which was previously experimental. This model is now in preview, offering a way to run high-volume AI workloads without incurring significant costs. Compared to 2.5 Flash, it's one-third of the cost for text, image, and video inputs and less than one-sixth of the cost for output tokens. It's unlikely this variant of Gemini will come to the app for regular users, because it's less capable than 2.5 Flash and only makes sense when you're paying by the token.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Additionally, Google Flash and Flash-Lite are coming to search. A Google spokesperson tells Ars that custom versions of these models are now active in AI overviews and AI Mode. Google generally tries to use the model that's the best fit for the query. So, complex AI Mode searches use 2.5 Pro, but something simple could rely on Flash or even Flash-Lite for very basic searches.&lt;/p&gt;
&lt;p&gt;The Flash-Lite preview is available in Google AI Studio and Vertex AI for developers, along with stable versions of Gemini 2.5 Flash and Gemini 2.5 Pro. You won't see any major changes in the Gemini app, as the final 2.5 Pro and 2.5 Flash models were already live in the app. The Pro variant will drop the preview label as Flash did last month, but functionality won't be any different. Free users still have limited access to 2.5 Pro in the app, with paying Pro users getting a higher limit of 100 prompts per day. AI Ultra subscribers have the highest level of access to Gemini 2.5 Pro.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/06/googles-gemini-ai-family-updated-with-stable-2-5-pro-super-efficient-2-5-flash-lite/</guid><pubDate>Tue, 17 Jun 2025 17:58:55 +0000</pubDate></item><item><title>[NEW] Toy-maker Mattel accused of planning “reckless” AI social experiment on kids (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/06/mattel-sparks-fear-that-planned-chatgpt-fueled-toys-will-warp-kids/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Mattel faces pressure to be more transparent about OpenAI partnership.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Barbie invading your childhood" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Barbie invading your childhood" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After Mattel and OpenAI announced a partnership that would result in an AI product marketed to kids, a consumer rights advocacy group is warning that the collaboration may endanger children.&lt;/p&gt;
&lt;p&gt;It remains unclear what shape Mattel's first AI product will take. However, on Tuesday, Public Citizen co-President Robert Weissman issued a statement urging more transparency so that parents can prepare for potential risks. Weissman is particularly concerned that ChatGPT-fueled toys could hurt kids in unknown ways.&lt;/p&gt;
&lt;p&gt;"Endowing toys with human-seeming voices that are able to engage in human-like conversations risks inflicting real damage on children," Weissman said. "It may undermine social development, interfere with children’s ability to form peer relationships, pull children away from playtime with peers, and possibly inflict long-term harm."&lt;/p&gt;
&lt;p&gt;One anonymous source told Axios that Mattel's plans for the AI partnership are still in "early stages," so perhaps more will be revealed as Mattel gears up for its first launch. That source suggested that the first product would not be marketed to kids under 13, which some think suggests that Mattel may recognize that exposing younger kids to AI is possibly a step too far at this stage. But more likely, it's due to OpenAI age restrictions on its API, prohibiting use under 13.&lt;/p&gt;
&lt;p&gt;Parents shouldn't be blindsided by new products, Weissman suggested, and some red lines should be drawn before any toy hits the shelves. Perhaps most urgently, "Mattel should announce immediately that it will not incorporate AI technology into children’s toys," Weissman said. "Children do not have the cognitive capacity to distinguish fully between reality and play."&lt;/p&gt;
&lt;p&gt;"Mattel should not leverage its trust with parents to conduct a reckless social experiment on our children by selling toys that incorporate AI," Weissman said.&lt;/p&gt;
&lt;p&gt;OpenAI declined to comment. Mattel did not immediately respond to Ars' request for comment.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI and Mattel defend partnership&lt;/h2&gt;
&lt;p&gt;In Mattel's press release, the toy maker behind brands like Barbie and Hot Wheels remained vague, saying only that the OpenAI deal would "support AI-powered products and experiences based on Mattel’s brands." The company's chief franchise officer, Josh Silverman, said the collaboration would enable Mattel to "reimagine new forms of play," teasing that the first release would be announced by the end of this year. Axios' source suggested it likely wouldn't be sold until 2026.&lt;/p&gt;
&lt;p&gt;OpenAI's statement also glossed over the details, promising "to bring a new dimension of AI-powered innovation and magic to Mattel’s iconic brands."&lt;/p&gt;
&lt;p&gt;Both companies emphasized that safety, privacy, and age-appropriateness would be front of mind in designing Mattel's AI products. OpenAI further claimed that kids would only be exposed to positive experiences through the collaboration, due to Mattel's experience creating kid-friendly products.&lt;/p&gt;
&lt;p&gt;"By tapping into OpenAI’s AI capabilities, Mattel aims to reimagine how fans can experience and interact with its cherished brands, with careful consideration to ensure positive, enriching experiences," OpenAI said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Critics fear Mattel is moving too fast&lt;/h2&gt;
&lt;p&gt;Critics on LinkedIn have noted that while the partnership could have positive impacts on kids—like enhancing learning or inclusivity—AI toys also carry a wide variety of potential risks that families should carefully weigh before buying into any new hyped product.&lt;/p&gt;
&lt;p&gt;In a detailed post, one tech executive, Varundeep Kaur, warned that parents should be thinking about privacy since AI toys may process their kids' "voice data, behavioral patterns, and personal preferences." He suggested Mattel may have set its first AI product's age limit at 13 to avoid running afoul of laws that are stricter when it comes to kids' data. OpenAI has said the collaboration will comply with all safety and privacy regulations.&lt;/p&gt;
&lt;p&gt;Parents should also keep in mind the bias behind the large language models that fuel AI tools like ChatGPT, Kaur said, which "might reproduce subtle stereotypes, biased narratives, or culturally inappropriate content, even unintentionally," that could skew kids' perspectives or social development.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Most obviously, AI models are still prone to hallucination, Kaur noted. And while Mattel's AI toys are "unlikely to cause physical harm," toys giving "inappropriate or bizarre responses" could "be confusing or even unsettling for a child," he said.&lt;/p&gt;
&lt;p&gt;For parents, the emotional ties kids make with AI toys will also need to be monitored, especially since chatbot outputs can be unpredictable. Another LinkedIn user, Adam Dodge—founder of a digital safety company preventing cyber abuse, called EndTab—pointed to a lawsuit where a grieving mom alleged her son committed suicide after interacting with hyper-realistic chatbots.&lt;/p&gt;
&lt;p&gt;Those bots encouraged self-harm and engaged her son in sexualized chats, and Dodge suggested that toy makers are similarly "wading into dangerous new waters with AI" that could possibly "communicate dangerous, sexualized, and harmful responses that put kids at risk."&lt;/p&gt;
&lt;p&gt;"This was inevitable—but wow does it make me cringe," Dodge wrote, noting that Mattel's plan to announce its first product this year seems "fast."&lt;/p&gt;
&lt;p&gt;Dodge said that right now, Mattel and OpenAI are "saying the right things" by emphasizing safety, privacy, and security, but more transparency is needed before parents can rest assured that AI toys are safe.&lt;/p&gt;
&lt;p&gt;AI is "unpredictable, sycophantic, and addictive," Dodge warned. "I don't want to be posting a year from now about how a Hot Wheels car encouraged self-harm or that children are in committed romantic relationships with their AI Barbies."&lt;/p&gt;
&lt;p&gt;Kaur agreed that it's in Mattel's best interest to give parents more information, since "public trust will be vital for widespread adoption." He recommended that the toy maker submit to independent audits and provide parental controls to reassure parents, as well as clearly outline how data is used, where it's stored, who has access to it, and what will happen if their kids' data is breached.&lt;/p&gt;
&lt;p&gt;For Mattel, a bigger legal threat forcing responsible design and appropriate content filtering may come from any unintentional copyright issues arising from using OpenAI models trained on a wide range of intellectual property. Hollywood studios recently sued one AI company for allowing users to generate images of their most popular characters and would likely be just as litigious defending against AI toys emulating their characters.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Mattel faces pressure to be more transparent about OpenAI partnership.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Barbie invading your childhood" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-640x360.jpg" width="640" /&gt;
                  &lt;img alt="AI Barbie invading your childhood" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/06/ai-barbie-hotwheels-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;After Mattel and OpenAI announced a partnership that would result in an AI product marketed to kids, a consumer rights advocacy group is warning that the collaboration may endanger children.&lt;/p&gt;
&lt;p&gt;It remains unclear what shape Mattel's first AI product will take. However, on Tuesday, Public Citizen co-President Robert Weissman issued a statement urging more transparency so that parents can prepare for potential risks. Weissman is particularly concerned that ChatGPT-fueled toys could hurt kids in unknown ways.&lt;/p&gt;
&lt;p&gt;"Endowing toys with human-seeming voices that are able to engage in human-like conversations risks inflicting real damage on children," Weissman said. "It may undermine social development, interfere with children’s ability to form peer relationships, pull children away from playtime with peers, and possibly inflict long-term harm."&lt;/p&gt;
&lt;p&gt;One anonymous source told Axios that Mattel's plans for the AI partnership are still in "early stages," so perhaps more will be revealed as Mattel gears up for its first launch. That source suggested that the first product would not be marketed to kids under 13, which some think suggests that Mattel may recognize that exposing younger kids to AI is possibly a step too far at this stage. But more likely, it's due to OpenAI age restrictions on its API, prohibiting use under 13.&lt;/p&gt;
&lt;p&gt;Parents shouldn't be blindsided by new products, Weissman suggested, and some red lines should be drawn before any toy hits the shelves. Perhaps most urgently, "Mattel should announce immediately that it will not incorporate AI technology into children’s toys," Weissman said. "Children do not have the cognitive capacity to distinguish fully between reality and play."&lt;/p&gt;
&lt;p&gt;"Mattel should not leverage its trust with parents to conduct a reckless social experiment on our children by selling toys that incorporate AI," Weissman said.&lt;/p&gt;
&lt;p&gt;OpenAI declined to comment. Mattel did not immediately respond to Ars' request for comment.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI and Mattel defend partnership&lt;/h2&gt;
&lt;p&gt;In Mattel's press release, the toy maker behind brands like Barbie and Hot Wheels remained vague, saying only that the OpenAI deal would "support AI-powered products and experiences based on Mattel’s brands." The company's chief franchise officer, Josh Silverman, said the collaboration would enable Mattel to "reimagine new forms of play," teasing that the first release would be announced by the end of this year. Axios' source suggested it likely wouldn't be sold until 2026.&lt;/p&gt;
&lt;p&gt;OpenAI's statement also glossed over the details, promising "to bring a new dimension of AI-powered innovation and magic to Mattel’s iconic brands."&lt;/p&gt;
&lt;p&gt;Both companies emphasized that safety, privacy, and age-appropriateness would be front of mind in designing Mattel's AI products. OpenAI further claimed that kids would only be exposed to positive experiences through the collaboration, due to Mattel's experience creating kid-friendly products.&lt;/p&gt;
&lt;p&gt;"By tapping into OpenAI’s AI capabilities, Mattel aims to reimagine how fans can experience and interact with its cherished brands, with careful consideration to ensure positive, enriching experiences," OpenAI said.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Critics fear Mattel is moving too fast&lt;/h2&gt;
&lt;p&gt;Critics on LinkedIn have noted that while the partnership could have positive impacts on kids—like enhancing learning or inclusivity—AI toys also carry a wide variety of potential risks that families should carefully weigh before buying into any new hyped product.&lt;/p&gt;
&lt;p&gt;In a detailed post, one tech executive, Varundeep Kaur, warned that parents should be thinking about privacy since AI toys may process their kids' "voice data, behavioral patterns, and personal preferences." He suggested Mattel may have set its first AI product's age limit at 13 to avoid running afoul of laws that are stricter when it comes to kids' data. OpenAI has said the collaboration will comply with all safety and privacy regulations.&lt;/p&gt;
&lt;p&gt;Parents should also keep in mind the bias behind the large language models that fuel AI tools like ChatGPT, Kaur said, which "might reproduce subtle stereotypes, biased narratives, or culturally inappropriate content, even unintentionally," that could skew kids' perspectives or social development.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Most obviously, AI models are still prone to hallucination, Kaur noted. And while Mattel's AI toys are "unlikely to cause physical harm," toys giving "inappropriate or bizarre responses" could "be confusing or even unsettling for a child," he said.&lt;/p&gt;
&lt;p&gt;For parents, the emotional ties kids make with AI toys will also need to be monitored, especially since chatbot outputs can be unpredictable. Another LinkedIn user, Adam Dodge—founder of a digital safety company preventing cyber abuse, called EndTab—pointed to a lawsuit where a grieving mom alleged her son committed suicide after interacting with hyper-realistic chatbots.&lt;/p&gt;
&lt;p&gt;Those bots encouraged self-harm and engaged her son in sexualized chats, and Dodge suggested that toy makers are similarly "wading into dangerous new waters with AI" that could possibly "communicate dangerous, sexualized, and harmful responses that put kids at risk."&lt;/p&gt;
&lt;p&gt;"This was inevitable—but wow does it make me cringe," Dodge wrote, noting that Mattel's plan to announce its first product this year seems "fast."&lt;/p&gt;
&lt;p&gt;Dodge said that right now, Mattel and OpenAI are "saying the right things" by emphasizing safety, privacy, and security, but more transparency is needed before parents can rest assured that AI toys are safe.&lt;/p&gt;
&lt;p&gt;AI is "unpredictable, sycophantic, and addictive," Dodge warned. "I don't want to be posting a year from now about how a Hot Wheels car encouraged self-harm or that children are in committed romantic relationships with their AI Barbies."&lt;/p&gt;
&lt;p&gt;Kaur agreed that it's in Mattel's best interest to give parents more information, since "public trust will be vital for widespread adoption." He recommended that the toy maker submit to independent audits and provide parental controls to reassure parents, as well as clearly outline how data is used, where it's stored, who has access to it, and what will happen if their kids' data is breached.&lt;/p&gt;
&lt;p&gt;For Mattel, a bigger legal threat forcing responsible design and appropriate content filtering may come from any unintentional copyright issues arising from using OpenAI models trained on a wide range of intellectual property. Hollywood studios recently sued one AI company for allowing users to generate images of their most popular characters and would likely be just as litigious defending against AI toys emulating their characters.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/06/mattel-sparks-fear-that-planned-chatgpt-fueled-toys-will-warp-kids/</guid><pubDate>Tue, 17 Jun 2025 19:22:43 +0000</pubDate></item><item><title>[NEW] Meta is reportedly building AI smart glasses with Prada, too (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/meta-is-reportedly-building-ai-smart-glasses-with-prada-too/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is working on a pair of AI smart glasses with the Italian high fashion brand, Prada, according to a report from CNBC on Tuesday. It’s unclear at this time when Meta’s Prada smart glasses will be publicly announced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The reported Prada collaboration signifies that Meta aims to bring its AI smart glasses technology to more fashion companies outside of its relationship with eyewear giant EssilorLuxottica. Until now, Meta has collaborated closely with EssilorLuxottica and its numerous brands. Prada is not owned by EssilorLuxottica, although the fashion brand has relied on the company to help build its eyewear for decades and the companies just renewed their partnership.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta has already sold millions of Ray-Ban Meta AI smart glasses. Earlier this week, the company teased a collaboration with another EssilorLuxottica brand, Oakley, as Bloomberg previously reported. CNBC reports that those Oakley smart glasses, which could be announced as soon as Friday, may cost around $360.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta is working on a pair of AI smart glasses with the Italian high fashion brand, Prada, according to a report from CNBC on Tuesday. It’s unclear at this time when Meta’s Prada smart glasses will be publicly announced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The reported Prada collaboration signifies that Meta aims to bring its AI smart glasses technology to more fashion companies outside of its relationship with eyewear giant EssilorLuxottica. Until now, Meta has collaborated closely with EssilorLuxottica and its numerous brands. Prada is not owned by EssilorLuxottica, although the fashion brand has relied on the company to help build its eyewear for decades and the companies just renewed their partnership.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Meta has already sold millions of Ray-Ban Meta AI smart glasses. Earlier this week, the company teased a collaboration with another EssilorLuxottica brand, Oakley, as Bloomberg previously reported. CNBC reports that those Oakley smart glasses, which could be announced as soon as Friday, may cost around $360.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/meta-is-reportedly-building-ai-smart-glasses-with-prada-too/</guid><pubDate>Tue, 17 Jun 2025 19:38:07 +0000</pubDate></item><item><title>[NEW] A sounding board for strengthening the student experience (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/sounding-board-for-strengthening-student-experience-0617</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-SCC-UAG.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During his first year at MIT in 2021, Matthew Caren ’25 received an intriguing email inviting students to apply to become members of the MIT Schwarzman College of Computing’s (SCC)&amp;nbsp;Undergraduate Advisory Group (UAG). He immediately shot off an application.&lt;/p&gt;&lt;p&gt;Caren is a jazz musician who majored in computer science and engineering, and minored in music and theater arts. He was drawn to the college because of its focus on the applied intersections between computing, engineering, the arts, and other academic pursuits.&amp;nbsp;Caron eagerly joined the UAG and stayed on it all four years at MIT.&lt;/p&gt;&lt;p&gt;First formed in April 2020, the group brings together a committee of around 25 undergraduate&amp;nbsp;students representing a broad swath of both traditional and&amp;nbsp;blended majors in electrical engineering and computer science (EECS) and other computing-related programs. They advise the college’s leadership on issues, offer constructive feedback, and serve as a sounding board for innovative new ideas.&lt;/p&gt;&lt;p&gt;“The ethos of the UAG is the ethos of the college itself,” Caren explains. “If you very intentionally bring together a bunch of smart, interesting, fun-to-be-around people who are all interested in completely diverse things, you'll get some really cool discussions and interactions out of it.”&lt;/p&gt;&lt;p&gt;Along the way, he’s also made “dear” friends and found true colleagues. In the group’s monthly meetings with SCC dean Dan Huttenlocher and Deputy Dean Asu Ozdaglar, who is also the department head of EECS, UAG members speak openly about challenges in the student experience and offer recommendations to guests from across the Institute, such as faculty who are developing new courses and looking for student input.&lt;/p&gt;&lt;p&gt;“This group is unique in the sense that it’s a direct line of communication to the college’s leadership,” says Caren. “They make time in their insanely busy schedules for us to explain where the holes are, and what students’ needs are, directly from our experiences.”&lt;/p&gt;&lt;p&gt;“The students in the group are keenly interested in computer science and AI, especially how these fields connect with other disciplines. They’re also passionate about MIT and eager to enhance the undergraduate experience. Hearing their perspective is refreshing — their honesty and feedback have been incredibly helpful to me as dean,” says Huttenlocher.&lt;/p&gt;&lt;p&gt;“Meeting with the students each month is a real pleasure. The UAG has been an invaluable space for understanding the student experience more deeply. They engage with computing in diverse ways across MIT, so their input on the curriculum and broader college issues has been insightful,” Ozdaglar says.&lt;/p&gt;&lt;p&gt;UAG program manager Ellen Rushman says that “Asu and Dan have done an amazing job cultivating a space in which students feel safe bringing up things that aren’t positive all the time.” The group’s suggestions are frequently implemented, too.&lt;/p&gt;&lt;p&gt;For example, in 2021, Skidmore, Owings &amp;amp; Merrill, the architects designing the new SCC&amp;nbsp;building, presented their renderings at a UAG meeting to request student feedback. Their original interiors layout offered very few of the hybrid study and meeting booths that are so popular in today’s first floor lobby.&lt;/p&gt;&lt;p&gt;Hearing strong UAG opinions about the sort of open-plan, community-building spaces that students really valued was one of the things that created the change to the current floor plan. “It’s super cool walking into the personalized space and seeing it constantly being in use and always crowded. I actually feel happy when I can’t get a table,” says Caren, who has just ended his tenure as co-chair of the group in preparation for graduation.&lt;/p&gt;&lt;p&gt;Caren’s co-chair, rising senior Julia Schneider, who is double-majoring in artificial intelligence and decision-making and mathematics, joined the UAG as a first-year to understand more about the college’s mission of fostering interdepartmental collaborations.&lt;/p&gt;&lt;p&gt;“Since I am a student in electrical engineering and computer science, but I conduct research in mechanical engineering on robotics, the college’s mission of fostering interdepartmental collaborations and uniting them through computing really spoke to my personal experiences in my first year at MIT,” Schneider says.&lt;/p&gt;&lt;p&gt;During her time on the UAG, members have joined subgroups focused around achieving different programmatic goals of the college, such as curating a public lecture series for the 2025-26 academic year to give MIT students exposure to faculty who conduct research in other disciplines that relate to computing.&lt;/p&gt;&lt;p&gt;At one meeting, after hearing how challenging it is for students to understand all the possible courses to take during their tenure, Schneider and some UAG peers formed a subgroup to find a solution.&lt;/p&gt;&lt;p&gt;The students agreed that some of the best courses they’ve taken at MIT, or pairings of courses that really struck a chord with their interdisciplinary interests, came because they spoke to upperclassmen and got recommendations. “This kind of tribal knowledge doesn’t really permeate to all of MIT,” Schneider explains.&lt;/p&gt;&lt;p&gt;For the last six months, Schneider and the subgroup have been working on a course visualization website,&amp;nbsp;NerdXing,&amp;nbsp;which came out of these discussions.&lt;/p&gt;&lt;p&gt;Guided by Rob Miller, distinguished professor of computer science in EECS, the subgroup used a dataset of EECS course enrollments over the past decade to develop a different type of tool than MIT students typically use, such as CourseRoad and others.&lt;/p&gt;&lt;p&gt;Miller, who regularly attends the UAG meetings in his role as the education officer for the college’s cross-cutting initiative,&amp;nbsp;Common Ground for Computing Education, comments, “the really cool idea here is to help students find paths that were taken by other people who are like them — not just interested in computer science, but maybe also in biology, or music, or economics, or neuroscience.&amp;nbsp;It's very much in the spirit of the College of Computing — applying data-driven computational methods, in support of students with wide-ranging computational interests.”&lt;/p&gt;&lt;p&gt;Opening the NerdXing pilot, which is set to roll out later this spring, Schneider gave a demo. She explains that if you are a computer science (CS) major and would like to create a visual presenting potential courses for you, after you select your major and a class of interest, you can expand a huge graph presenting all the possible courses your CS peers have taken over the past decade.&lt;/p&gt;&lt;p&gt;She clicked on class 18.404 (Theory of Computation) as the starting class of interest, which led to class 6.7900 (Machine Learning), and then unexpectedly to 21M.302 (Harmony and Counterpoint II), an advanced music class.&lt;/p&gt;&lt;p&gt;“You start to see aggregate statistics that tell you how many students took each course, and you can further pare it down to see the most popular courses in CS or follow lines of red dots between courses to see the typical sequence of classes taken.”&lt;/p&gt;&lt;p&gt;By getting granular on the graph, users begin to see classes that they have probably never heard anyone talking about in their program. “I think that one of the reasons you come to MIT is to be able to take cool stuff exactly like this,” says Schneider.&lt;/p&gt;&lt;p&gt;The tool aims to show students how they can choose classes that go far beyond just filling degree requirements. It’s just one example of how UAG is empowering students to strengthen the college and the experiences it offers them.&lt;/p&gt;&lt;p&gt;“We are MIT students. We have the skills to build solutions,” Schneider says. “This group of people not only brings up ways in which things could be better, but we take it into our own hands to fix things.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202505/mit-SCC-UAG.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;During his first year at MIT in 2021, Matthew Caren ’25 received an intriguing email inviting students to apply to become members of the MIT Schwarzman College of Computing’s (SCC)&amp;nbsp;Undergraduate Advisory Group (UAG). He immediately shot off an application.&lt;/p&gt;&lt;p&gt;Caren is a jazz musician who majored in computer science and engineering, and minored in music and theater arts. He was drawn to the college because of its focus on the applied intersections between computing, engineering, the arts, and other academic pursuits.&amp;nbsp;Caron eagerly joined the UAG and stayed on it all four years at MIT.&lt;/p&gt;&lt;p&gt;First formed in April 2020, the group brings together a committee of around 25 undergraduate&amp;nbsp;students representing a broad swath of both traditional and&amp;nbsp;blended majors in electrical engineering and computer science (EECS) and other computing-related programs. They advise the college’s leadership on issues, offer constructive feedback, and serve as a sounding board for innovative new ideas.&lt;/p&gt;&lt;p&gt;“The ethos of the UAG is the ethos of the college itself,” Caren explains. “If you very intentionally bring together a bunch of smart, interesting, fun-to-be-around people who are all interested in completely diverse things, you'll get some really cool discussions and interactions out of it.”&lt;/p&gt;&lt;p&gt;Along the way, he’s also made “dear” friends and found true colleagues. In the group’s monthly meetings with SCC dean Dan Huttenlocher and Deputy Dean Asu Ozdaglar, who is also the department head of EECS, UAG members speak openly about challenges in the student experience and offer recommendations to guests from across the Institute, such as faculty who are developing new courses and looking for student input.&lt;/p&gt;&lt;p&gt;“This group is unique in the sense that it’s a direct line of communication to the college’s leadership,” says Caren. “They make time in their insanely busy schedules for us to explain where the holes are, and what students’ needs are, directly from our experiences.”&lt;/p&gt;&lt;p&gt;“The students in the group are keenly interested in computer science and AI, especially how these fields connect with other disciplines. They’re also passionate about MIT and eager to enhance the undergraduate experience. Hearing their perspective is refreshing — their honesty and feedback have been incredibly helpful to me as dean,” says Huttenlocher.&lt;/p&gt;&lt;p&gt;“Meeting with the students each month is a real pleasure. The UAG has been an invaluable space for understanding the student experience more deeply. They engage with computing in diverse ways across MIT, so their input on the curriculum and broader college issues has been insightful,” Ozdaglar says.&lt;/p&gt;&lt;p&gt;UAG program manager Ellen Rushman says that “Asu and Dan have done an amazing job cultivating a space in which students feel safe bringing up things that aren’t positive all the time.” The group’s suggestions are frequently implemented, too.&lt;/p&gt;&lt;p&gt;For example, in 2021, Skidmore, Owings &amp;amp; Merrill, the architects designing the new SCC&amp;nbsp;building, presented their renderings at a UAG meeting to request student feedback. Their original interiors layout offered very few of the hybrid study and meeting booths that are so popular in today’s first floor lobby.&lt;/p&gt;&lt;p&gt;Hearing strong UAG opinions about the sort of open-plan, community-building spaces that students really valued was one of the things that created the change to the current floor plan. “It’s super cool walking into the personalized space and seeing it constantly being in use and always crowded. I actually feel happy when I can’t get a table,” says Caren, who has just ended his tenure as co-chair of the group in preparation for graduation.&lt;/p&gt;&lt;p&gt;Caren’s co-chair, rising senior Julia Schneider, who is double-majoring in artificial intelligence and decision-making and mathematics, joined the UAG as a first-year to understand more about the college’s mission of fostering interdepartmental collaborations.&lt;/p&gt;&lt;p&gt;“Since I am a student in electrical engineering and computer science, but I conduct research in mechanical engineering on robotics, the college’s mission of fostering interdepartmental collaborations and uniting them through computing really spoke to my personal experiences in my first year at MIT,” Schneider says.&lt;/p&gt;&lt;p&gt;During her time on the UAG, members have joined subgroups focused around achieving different programmatic goals of the college, such as curating a public lecture series for the 2025-26 academic year to give MIT students exposure to faculty who conduct research in other disciplines that relate to computing.&lt;/p&gt;&lt;p&gt;At one meeting, after hearing how challenging it is for students to understand all the possible courses to take during their tenure, Schneider and some UAG peers formed a subgroup to find a solution.&lt;/p&gt;&lt;p&gt;The students agreed that some of the best courses they’ve taken at MIT, or pairings of courses that really struck a chord with their interdisciplinary interests, came because they spoke to upperclassmen and got recommendations. “This kind of tribal knowledge doesn’t really permeate to all of MIT,” Schneider explains.&lt;/p&gt;&lt;p&gt;For the last six months, Schneider and the subgroup have been working on a course visualization website,&amp;nbsp;NerdXing,&amp;nbsp;which came out of these discussions.&lt;/p&gt;&lt;p&gt;Guided by Rob Miller, distinguished professor of computer science in EECS, the subgroup used a dataset of EECS course enrollments over the past decade to develop a different type of tool than MIT students typically use, such as CourseRoad and others.&lt;/p&gt;&lt;p&gt;Miller, who regularly attends the UAG meetings in his role as the education officer for the college’s cross-cutting initiative,&amp;nbsp;Common Ground for Computing Education, comments, “the really cool idea here is to help students find paths that were taken by other people who are like them — not just interested in computer science, but maybe also in biology, or music, or economics, or neuroscience.&amp;nbsp;It's very much in the spirit of the College of Computing — applying data-driven computational methods, in support of students with wide-ranging computational interests.”&lt;/p&gt;&lt;p&gt;Opening the NerdXing pilot, which is set to roll out later this spring, Schneider gave a demo. She explains that if you are a computer science (CS) major and would like to create a visual presenting potential courses for you, after you select your major and a class of interest, you can expand a huge graph presenting all the possible courses your CS peers have taken over the past decade.&lt;/p&gt;&lt;p&gt;She clicked on class 18.404 (Theory of Computation) as the starting class of interest, which led to class 6.7900 (Machine Learning), and then unexpectedly to 21M.302 (Harmony and Counterpoint II), an advanced music class.&lt;/p&gt;&lt;p&gt;“You start to see aggregate statistics that tell you how many students took each course, and you can further pare it down to see the most popular courses in CS or follow lines of red dots between courses to see the typical sequence of classes taken.”&lt;/p&gt;&lt;p&gt;By getting granular on the graph, users begin to see classes that they have probably never heard anyone talking about in their program. “I think that one of the reasons you come to MIT is to be able to take cool stuff exactly like this,” says Schneider.&lt;/p&gt;&lt;p&gt;The tool aims to show students how they can choose classes that go far beyond just filling degree requirements. It’s just one example of how UAG is empowering students to strengthen the college and the experiences it offers them.&lt;/p&gt;&lt;p&gt;“We are MIT students. We have the skills to build solutions,” Schneider says. “This group of people not only brings up ways in which things could be better, but we take it into our own hands to fix things.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/sounding-board-for-strengthening-student-experience-0617</guid><pubDate>Tue, 17 Jun 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] Unpacking the bias of large language models (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/unpacking-large-language-model-bias-0617</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-transform-bias-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Research has shown that large language models (LLMs) tend to overemphasize information at the beginning and end of a document or conversation, while neglecting the middle.&lt;/p&gt;&lt;p&gt;This “position bias” means that, if a lawyer is using an LLM-powered virtual assistant to retrieve a certain phrase in a 30-page affidavit, the LLM is more likely to find the right text if it is on the initial or final pages.&lt;/p&gt;&lt;p&gt;MIT researchers have discovered the mechanism behind this phenomenon.&lt;/p&gt;&lt;p&gt;They created a theoretical framework to study how information flows through the machine-learning architecture that forms the backbone of LLMs. They found that certain design choices which control how the model processes input data can cause position bias.&lt;/p&gt;&lt;p&gt;Their experiments revealed that model architectures, particularly those affecting how information is spread across input words within the model, can give rise to or intensify position bias, and that training data also contribute to the problem.&lt;/p&gt;&lt;p&gt;In addition to pinpointing the origins of position bias, their framework can be used to diagnose and correct it in future model designs.&lt;/p&gt;&lt;p&gt;This could lead to more reliable chatbots that stay on topic during long conversations, medical AI systems that reason more fairly when handling a trove of patient data, and code assistants that pay closer attention to all parts of a program.&lt;/p&gt;&lt;p&gt;“These models are black boxes, so as an LLM user, you probably don’t know that position bias can cause your model to be inconsistent. You just feed it your documents in whatever order you want and expect it to work. But by understanding the underlying mechanism of these black-box models better, we can improve them by addressing these limitations,” says Xinyi Wu, a graduate student in the MIT Institute for Data, Systems, and Society (IDSS) and the Laboratory for Information and Decision Systems (LIDS), and first author of a paper on this research.&lt;/p&gt;&lt;p&gt;Her co-authors include Yifei Wang, an MIT postdoc; and senior authors Stefanie Jegelka, an associate professor of electrical engineering and computer science (EECS) and a member of IDSS and the Computer Science and Artificial Intelligence Laboratory (CSAIL); and Ali Jadbabaie, professor and head of the Department of Civil and Environmental Engineering, a core faculty member of IDSS, and a principal investigator in LIDS. The research will be presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Analyzing attention&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLMs like Claude, Llama, and GPT-4 are powered by a type of neural network architecture known as a transformer. Transformers are designed to process sequential data, encoding a sentence into chunks called tokens and then learning the relationships between tokens to predict what words comes next.&lt;/p&gt;&lt;p&gt;These models have gotten very good at this because of the attention mechanism, which uses interconnected layers of data processing nodes to make sense of context by allowing tokens to selectively focus on, or attend to, related tokens.&lt;/p&gt;&lt;p&gt;But if every token can attend to every other token in a 30-page document, that quickly becomes computationally intractable. So, when engineers build transformer models, they often employ attention masking techniques which limit the words a token can attend to.&lt;/p&gt;&lt;p&gt;For instance, a causal mask only allows words to attend to those that came before it.&lt;/p&gt;&lt;p&gt;Engineers also use positional encodings to help the model understand the location of each word in a sentence, improving performance.&lt;/p&gt;&lt;p&gt;The MIT researchers built a graph-based theoretical framework to explore how these modeling choices, attention masks and positional encodings, could affect position bias.&lt;/p&gt;&lt;p&gt;“Everything is coupled and tangled within the attention mechanism, so it is very hard to study. Graphs are a flexible language to describe the dependent relationship among words within the attention mechanism and trace them across multiple layers,” Wu says.&lt;/p&gt;&lt;p&gt;Their theoretical analysis suggested that causal masking gives the model an inherent bias toward the beginning of an input, even when that bias doesn’t exist in the data.&lt;/p&gt;&lt;p&gt;If the earlier words are relatively unimportant for a sentence’s meaning, causal masking can cause the transformer to pay more attention to its beginning anyway.&lt;/p&gt;&lt;p&gt;“While it is often true that earlier words and later words in a sentence are more important, if an LLM is used on a task that is not natural language generation, like ranking or information retrieval, these biases can be extremely harmful,” Wu says.&lt;/p&gt;&lt;p&gt;As a model grows, with additional layers of attention mechanism, this bias is amplified because earlier parts of the input are used more frequently in the model’s reasoning process.&lt;/p&gt;&lt;p&gt;They also found that using positional encodings to link words more strongly to nearby words can mitigate position bias. The technique refocuses the model’s attention in the right place, but its effect can be diluted in models with more attention layers.&lt;/p&gt;&lt;p&gt;And these design choices are only one cause of position bias — some can come from training data the model uses to learn how to prioritize words in a sequence.&lt;/p&gt;&lt;p&gt;“If you know your data are biased in a certain way, then you should also finetune your model on top of adjusting your modeling choices,” Wu says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lost in the middle&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After they’d established a theoretical framework, the researchers performed experiments in which they systematically varied the position of the correct answer in text sequences for an information retrieval task.&lt;/p&gt;&lt;p&gt;The experiments showed a “lost-in-the-middle” phenomenon, where retrieval accuracy followed a U-shaped pattern. Models performed best if the right answer was located at the beginning of the sequence. Performance declined the closer it got to the middle before rebounding a bit if the correct answer was near the end.&lt;/p&gt;&lt;p&gt;Ultimately, their work suggests that using a different masking technique, removing extra layers from the attention mechanism, or strategically employing positional encodings could reduce position bias and improve a model’s accuracy.&lt;/p&gt;&lt;p&gt;“By doing a combination of theory and experiments, we were able to look at the consequences of model design choices that weren’t clear at the time. If you want to use a model in high-stakes applications, you must know when it will work, when it won’t, and why,” Jadbabaie says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to further explore the effects of positional encodings and study how position bias could be strategically exploited in certain applications.&lt;/p&gt;&lt;p&gt;“These researchers offer a rare theoretical lens into the attention mechanism at the heart of the transformer model. They provide a compelling analysis that clarifies longstanding quirks in transformer behavior, showing that attention mechanisms, especially with causal masks, inherently bias models toward the beginning of sequences. The paper achieves the best of both worlds — mathematical clarity paired with insights that reach into the guts of real-world systems,” says Amin Saberi, professor and director of the Stanford University Center for Computational Market Design, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research is supported, in part, by the U.S. Office of Naval Research, the National Science Foundation, and an Alexander von Humboldt Professorship.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-transform-bias-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Research has shown that large language models (LLMs) tend to overemphasize information at the beginning and end of a document or conversation, while neglecting the middle.&lt;/p&gt;&lt;p&gt;This “position bias” means that, if a lawyer is using an LLM-powered virtual assistant to retrieve a certain phrase in a 30-page affidavit, the LLM is more likely to find the right text if it is on the initial or final pages.&lt;/p&gt;&lt;p&gt;MIT researchers have discovered the mechanism behind this phenomenon.&lt;/p&gt;&lt;p&gt;They created a theoretical framework to study how information flows through the machine-learning architecture that forms the backbone of LLMs. They found that certain design choices which control how the model processes input data can cause position bias.&lt;/p&gt;&lt;p&gt;Their experiments revealed that model architectures, particularly those affecting how information is spread across input words within the model, can give rise to or intensify position bias, and that training data also contribute to the problem.&lt;/p&gt;&lt;p&gt;In addition to pinpointing the origins of position bias, their framework can be used to diagnose and correct it in future model designs.&lt;/p&gt;&lt;p&gt;This could lead to more reliable chatbots that stay on topic during long conversations, medical AI systems that reason more fairly when handling a trove of patient data, and code assistants that pay closer attention to all parts of a program.&lt;/p&gt;&lt;p&gt;“These models are black boxes, so as an LLM user, you probably don’t know that position bias can cause your model to be inconsistent. You just feed it your documents in whatever order you want and expect it to work. But by understanding the underlying mechanism of these black-box models better, we can improve them by addressing these limitations,” says Xinyi Wu, a graduate student in the MIT Institute for Data, Systems, and Society (IDSS) and the Laboratory for Information and Decision Systems (LIDS), and first author of a paper on this research.&lt;/p&gt;&lt;p&gt;Her co-authors include Yifei Wang, an MIT postdoc; and senior authors Stefanie Jegelka, an associate professor of electrical engineering and computer science (EECS) and a member of IDSS and the Computer Science and Artificial Intelligence Laboratory (CSAIL); and Ali Jadbabaie, professor and head of the Department of Civil and Environmental Engineering, a core faculty member of IDSS, and a principal investigator in LIDS. The research will be presented at the International Conference on Machine Learning.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Analyzing attention&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;LLMs like Claude, Llama, and GPT-4 are powered by a type of neural network architecture known as a transformer. Transformers are designed to process sequential data, encoding a sentence into chunks called tokens and then learning the relationships between tokens to predict what words comes next.&lt;/p&gt;&lt;p&gt;These models have gotten very good at this because of the attention mechanism, which uses interconnected layers of data processing nodes to make sense of context by allowing tokens to selectively focus on, or attend to, related tokens.&lt;/p&gt;&lt;p&gt;But if every token can attend to every other token in a 30-page document, that quickly becomes computationally intractable. So, when engineers build transformer models, they often employ attention masking techniques which limit the words a token can attend to.&lt;/p&gt;&lt;p&gt;For instance, a causal mask only allows words to attend to those that came before it.&lt;/p&gt;&lt;p&gt;Engineers also use positional encodings to help the model understand the location of each word in a sentence, improving performance.&lt;/p&gt;&lt;p&gt;The MIT researchers built a graph-based theoretical framework to explore how these modeling choices, attention masks and positional encodings, could affect position bias.&lt;/p&gt;&lt;p&gt;“Everything is coupled and tangled within the attention mechanism, so it is very hard to study. Graphs are a flexible language to describe the dependent relationship among words within the attention mechanism and trace them across multiple layers,” Wu says.&lt;/p&gt;&lt;p&gt;Their theoretical analysis suggested that causal masking gives the model an inherent bias toward the beginning of an input, even when that bias doesn’t exist in the data.&lt;/p&gt;&lt;p&gt;If the earlier words are relatively unimportant for a sentence’s meaning, causal masking can cause the transformer to pay more attention to its beginning anyway.&lt;/p&gt;&lt;p&gt;“While it is often true that earlier words and later words in a sentence are more important, if an LLM is used on a task that is not natural language generation, like ranking or information retrieval, these biases can be extremely harmful,” Wu says.&lt;/p&gt;&lt;p&gt;As a model grows, with additional layers of attention mechanism, this bias is amplified because earlier parts of the input are used more frequently in the model’s reasoning process.&lt;/p&gt;&lt;p&gt;They also found that using positional encodings to link words more strongly to nearby words can mitigate position bias. The technique refocuses the model’s attention in the right place, but its effect can be diluted in models with more attention layers.&lt;/p&gt;&lt;p&gt;And these design choices are only one cause of position bias — some can come from training data the model uses to learn how to prioritize words in a sequence.&lt;/p&gt;&lt;p&gt;“If you know your data are biased in a certain way, then you should also finetune your model on top of adjusting your modeling choices,” Wu says.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Lost in the middle&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;After they’d established a theoretical framework, the researchers performed experiments in which they systematically varied the position of the correct answer in text sequences for an information retrieval task.&lt;/p&gt;&lt;p&gt;The experiments showed a “lost-in-the-middle” phenomenon, where retrieval accuracy followed a U-shaped pattern. Models performed best if the right answer was located at the beginning of the sequence. Performance declined the closer it got to the middle before rebounding a bit if the correct answer was near the end.&lt;/p&gt;&lt;p&gt;Ultimately, their work suggests that using a different masking technique, removing extra layers from the attention mechanism, or strategically employing positional encodings could reduce position bias and improve a model’s accuracy.&lt;/p&gt;&lt;p&gt;“By doing a combination of theory and experiments, we were able to look at the consequences of model design choices that weren’t clear at the time. If you want to use a model in high-stakes applications, you must know when it will work, when it won’t, and why,” Jadbabaie says.&lt;/p&gt;&lt;p&gt;In the future, the researchers want to further explore the effects of positional encodings and study how position bias could be strategically exploited in certain applications.&lt;/p&gt;&lt;p&gt;“These researchers offer a rare theoretical lens into the attention mechanism at the heart of the transformer model. They provide a compelling analysis that clarifies longstanding quirks in transformer behavior, showing that attention mechanisms, especially with causal masks, inherently bias models toward the beginning of sequences. The paper achieves the best of both worlds — mathematical clarity paired with insights that reach into the guts of real-world systems,” says Amin Saberi, professor and director of the Stanford University Center for Computational Market Design, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research is supported, in part, by the U.S. Office of Naval Research, the National Science Foundation, and an Alexander von Humboldt Professorship.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/unpacking-large-language-model-bias-0617</guid><pubDate>Tue, 17 Jun 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] Amazon expects to reduce corporate jobs due to AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/amazon-expects-to-reduce-corporate-jobs-due-to-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2201505679.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy is betting that generative AI will change how the company thinks about its workforce in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said that as the company continues to roll out more AI agents, and thus change how the company’s work is done, he expects Amazon will reduce the number of corporate jobs needed in the future, according to a memo that was first covered by CNBC.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs,” Jassy wrote in the memo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the size of this future reduction of workforce is hard to estimate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A recent survey from the World Economic Forum found that potential reductions in workforce due to AI may already be happening. The survey found that 40% of employers plan to cut staff that are doing roles that can be automated by AI.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2201505679.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Amazon CEO Andy Jassy is betting that generative AI will change how the company thinks about its workforce in the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Jassy said that as the company continues to roll out more AI agents, and thus change how the company’s work is done, he expects Amazon will reduce the number of corporate jobs needed in the future, according to a memo that was first covered by CNBC.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs,” Jassy wrote in the memo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that the size of this future reduction of workforce is hard to estimate.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A recent survey from the World Economic Forum found that potential reductions in workforce due to AI may already be happening. The survey found that 40% of employers plan to cut staff that are doing roles that can be automated by AI.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/amazon-expects-to-reduce-corporate-jobs-due-to-ai/</guid><pubDate>Tue, 17 Jun 2025 20:21:23 +0000</pubDate></item><item><title>[NEW] Combining technology, education, and human connection to improve online learning (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/caitlin-morris-combines-tech-education-human-connection-improve-online-learning-0617</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/mit-Caitlin-Morris.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT Morningside Academy for Design (MAD) Fellow&amp;nbsp;Caitlin Morris is an architect, artist,&amp;nbsp;researcher, and educator who has studied psychology and used online learning tools to teach herself coding and other skills. She’s a soft-spoken observer, with a keen interest in how people use space and respond to their environments.&amp;nbsp;Combining her observational skills with active community engagement, she works at the intersection of technology, education, and human connection to improve digital learning platforms.&lt;/p&gt;&lt;p&gt;Morris grew up in rural upstate New York in a family of makers. She learned to sew, cook, and build things with wood at a young age. One of her earlier memories is of a small handsaw she made — with the help of her father, a professional carpenter. It had wooden handles on both sides to make sawing easier for her.&lt;/p&gt;&lt;p&gt;Later, when she needed to learn something, she’d turn to project-based communities, rather than books. She taught herself to code late at night, taking advantage of community-oriented platforms where people answer questions and post sketches, allowing her to see the code behind the objects people made.&lt;/p&gt;&lt;p&gt;“For me, that was this huge, wake-up moment of feeling like there was a path to expression that was not a traditional computer-science classroom,” she says. “I think that’s partly why I feel so passionate about what I’m doing now. That was the big transformation: having that community available in this really personal, project-based way.”&lt;/p&gt;&lt;p&gt;Subsequently, Morris has become involved in community-based learning in diverse ways: She’s a co-organizer of the MIT Media Lab’s Festival of Learning; she leads creative coding community meetups; and she’s been active in the open-source software community development.&lt;/p&gt;&lt;p&gt;“My years of organizing learning and making communities — both in person and online — have shown me firsthand how powerful social interaction can be for motivation and curiosity,” Morris said. “My research is really about identifying which elements of that social magic are most essential, so we can design digital environments that better support those dynamics.”&lt;/p&gt;&lt;p&gt;Even in her artwork, Morris sometimes works with a collective. She’s contributed to the creation of about 10 large art installations that combine movement, sound, imagery, lighting, and other technologies to immerse the visitor in an experience evoking some aspect of nature, such as flowing water, birds in flight, or crowd kinetics. These marvelous installations are commanding and calming at the same time, possibly because they focus the mind, eye, and sometimes the ear.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            MIT graduate student and MAD Fellow Caitlin Morris contributed concept design, design development, electrical design and engineering, firmware development, and fabrication to “Diffusion Choir,” an installation from the artist collaborative Hypersonic, as well as Sosolimited and Plebian Design.&lt;br /&gt;Video: Hypersonic        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;She did much of this work with New York-based Hypersonic, a company of artists and technologists specializing in large kinetic installations in public spaces. Before that, she earned a BS in psychology and a BS in architectural building sciences from Rensselaer Polytechnic Institute, then an MFA in design and technology from the Parsons School of Design at The New School.&lt;/p&gt;&lt;p&gt;During, in between, after, and sometimes concurrently, she taught design, coding, and other technologies at the high school, undergraduate, and graduate-student levels.&lt;/p&gt;&lt;p&gt;“I think what kind of got me hooked on teaching was that the way I learned as a child was not the same as in the classroom,” Morris explains. “And I later saw this in many of my students. I got the feeling that the normal way of learning things was not working for them. And they thought it was their fault. They just didn’t really feel welcome within the traditional education model.”&lt;/p&gt;&lt;p&gt;Morris says that when she worked with those students, tossing aside tradition and instead saying — “You know, we’re just going to do this animation. Or we’re going to make this design or this website or these graphics, and we’re going to approach it in this totally different way” — she saw people “kind of unlock and be like, ‘Oh my gosh. I never thought I could do that.’&lt;/p&gt;&lt;p&gt;“For me, that was the hook, that’s the magic of it. Because I was coming from that experience of having to figure out those unlock mechanisms for myself, it was really exciting to be able to share them with other people, those unlock moments.”&lt;/p&gt;&lt;p&gt;For her doctoral work with the MIT Media Lab’s Fluid Interfaces Group, she’s focusing on the personal space and emotional gaps associated with learning, particularly online and AI-assisted learning. This research builds on her experience increasing human connection in both physical and virtual learning environments.&lt;/p&gt;&lt;p&gt;“I’m developing a framework that combines AI-driven behavioral analysis with human expert assessment to study social learning dynamics,” she says. “My research investigates how social interaction patterns influence curiosity development and intrinsic motivation in learning, with particular focus on understanding how these dynamics differ between real peers and AI-supported environments.”&lt;/p&gt;&lt;p&gt;The first step in her research is determining which elements of social interaction are not replaceable by an AI-based digital tutor. Following that assessment, her goal is to build a prototype platform for experiential learning.&lt;/p&gt;&lt;p&gt;“I’m creating tools that can simultaneously track observable behaviors — like physical actions, language cues, and interaction patterns — while capturing learners’ subjective experiences through reflection and interviews,” Morris explains. “This approach helps connect what people do with how they feel about their learning experience.&lt;/p&gt;&lt;p&gt;“I aim to make two primary contributions: first, analysis tools for studying social learning dynamics; and second, prototype tools that demonstrate practical approaches for supporting social curiosity in digital learning environments. These contributions could help bridge the gap between the efficiency of digital platforms and the rich social interaction that occurs in effective in-person learning.”&lt;/p&gt;&lt;p&gt;Her goals make Morris a perfect fit for the MIT MAD Fellowship. One statement in MAD’s mission is: “Breaking away from traditional education, we foster creativity, critical thinking, making, and collaboration, exploring a range of dynamic approaches to prepare students for complex, real-world challenges.”&lt;/p&gt;&lt;p&gt;Morris wants to help community organizations deal with the rapid AI-powered changes in education, once she finishes her doctorate in 2026. “What should we do with this ‘physical space versus virtual space’ divide?” she asks. That is the space currently captivating Morris’s thoughts.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/mit-Caitlin-Morris.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;MIT Morningside Academy for Design (MAD) Fellow&amp;nbsp;Caitlin Morris is an architect, artist,&amp;nbsp;researcher, and educator who has studied psychology and used online learning tools to teach herself coding and other skills. She’s a soft-spoken observer, with a keen interest in how people use space and respond to their environments.&amp;nbsp;Combining her observational skills with active community engagement, she works at the intersection of technology, education, and human connection to improve digital learning platforms.&lt;/p&gt;&lt;p&gt;Morris grew up in rural upstate New York in a family of makers. She learned to sew, cook, and build things with wood at a young age. One of her earlier memories is of a small handsaw she made — with the help of her father, a professional carpenter. It had wooden handles on both sides to make sawing easier for her.&lt;/p&gt;&lt;p&gt;Later, when she needed to learn something, she’d turn to project-based communities, rather than books. She taught herself to code late at night, taking advantage of community-oriented platforms where people answer questions and post sketches, allowing her to see the code behind the objects people made.&lt;/p&gt;&lt;p&gt;“For me, that was this huge, wake-up moment of feeling like there was a path to expression that was not a traditional computer-science classroom,” she says. “I think that’s partly why I feel so passionate about what I’m doing now. That was the big transformation: having that community available in this really personal, project-based way.”&lt;/p&gt;&lt;p&gt;Subsequently, Morris has become involved in community-based learning in diverse ways: She’s a co-organizer of the MIT Media Lab’s Festival of Learning; she leads creative coding community meetups; and she’s been active in the open-source software community development.&lt;/p&gt;&lt;p&gt;“My years of organizing learning and making communities — both in person and online — have shown me firsthand how powerful social interaction can be for motivation and curiosity,” Morris said. “My research is really about identifying which elements of that social magic are most essential, so we can design digital environments that better support those dynamics.”&lt;/p&gt;&lt;p&gt;Even in her artwork, Morris sometimes works with a collective. She’s contributed to the creation of about 10 large art installations that combine movement, sound, imagery, lighting, and other technologies to immerse the visitor in an experience evoking some aspect of nature, such as flowing water, birds in flight, or crowd kinetics. These marvelous installations are commanding and calming at the same time, possibly because they focus the mind, eye, and sometimes the ear.&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        
        &lt;div class="news-article--inline-video--caption"&gt;
      

            MIT graduate student and MAD Fellow Caitlin Morris contributed concept design, design development, electrical design and engineering, firmware development, and fabrication to “Diffusion Choir,” an installation from the artist collaborative Hypersonic, as well as Sosolimited and Plebian Design.&lt;br /&gt;Video: Hypersonic        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;She did much of this work with New York-based Hypersonic, a company of artists and technologists specializing in large kinetic installations in public spaces. Before that, she earned a BS in psychology and a BS in architectural building sciences from Rensselaer Polytechnic Institute, then an MFA in design and technology from the Parsons School of Design at The New School.&lt;/p&gt;&lt;p&gt;During, in between, after, and sometimes concurrently, she taught design, coding, and other technologies at the high school, undergraduate, and graduate-student levels.&lt;/p&gt;&lt;p&gt;“I think what kind of got me hooked on teaching was that the way I learned as a child was not the same as in the classroom,” Morris explains. “And I later saw this in many of my students. I got the feeling that the normal way of learning things was not working for them. And they thought it was their fault. They just didn’t really feel welcome within the traditional education model.”&lt;/p&gt;&lt;p&gt;Morris says that when she worked with those students, tossing aside tradition and instead saying — “You know, we’re just going to do this animation. Or we’re going to make this design or this website or these graphics, and we’re going to approach it in this totally different way” — she saw people “kind of unlock and be like, ‘Oh my gosh. I never thought I could do that.’&lt;/p&gt;&lt;p&gt;“For me, that was the hook, that’s the magic of it. Because I was coming from that experience of having to figure out those unlock mechanisms for myself, it was really exciting to be able to share them with other people, those unlock moments.”&lt;/p&gt;&lt;p&gt;For her doctoral work with the MIT Media Lab’s Fluid Interfaces Group, she’s focusing on the personal space and emotional gaps associated with learning, particularly online and AI-assisted learning. This research builds on her experience increasing human connection in both physical and virtual learning environments.&lt;/p&gt;&lt;p&gt;“I’m developing a framework that combines AI-driven behavioral analysis with human expert assessment to study social learning dynamics,” she says. “My research investigates how social interaction patterns influence curiosity development and intrinsic motivation in learning, with particular focus on understanding how these dynamics differ between real peers and AI-supported environments.”&lt;/p&gt;&lt;p&gt;The first step in her research is determining which elements of social interaction are not replaceable by an AI-based digital tutor. Following that assessment, her goal is to build a prototype platform for experiential learning.&lt;/p&gt;&lt;p&gt;“I’m creating tools that can simultaneously track observable behaviors — like physical actions, language cues, and interaction patterns — while capturing learners’ subjective experiences through reflection and interviews,” Morris explains. “This approach helps connect what people do with how they feel about their learning experience.&lt;/p&gt;&lt;p&gt;“I aim to make two primary contributions: first, analysis tools for studying social learning dynamics; and second, prototype tools that demonstrate practical approaches for supporting social curiosity in digital learning environments. These contributions could help bridge the gap between the efficiency of digital platforms and the rich social interaction that occurs in effective in-person learning.”&lt;/p&gt;&lt;p&gt;Her goals make Morris a perfect fit for the MIT MAD Fellowship. One statement in MAD’s mission is: “Breaking away from traditional education, we foster creativity, critical thinking, making, and collaboration, exploring a range of dynamic approaches to prepare students for complex, real-world challenges.”&lt;/p&gt;&lt;p&gt;Morris wants to help community organizations deal with the rapid AI-powered changes in education, once she finishes her doctorate in 2026. “What should we do with this ‘physical space versus virtual space’ divide?” she asks. That is the space currently captivating Morris’s thoughts.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/caitlin-morris-combines-tech-education-human-connection-improve-online-learning-0617</guid><pubDate>Tue, 17 Jun 2025 20:25:00 +0000</pubDate></item><item><title>[NEW] Google’s Gemini panicked when playing Pokémon (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/googles-gemini-panicked-when-playing-pokemon/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI companies are battling to dominate the industry, but sometimes they’re also battling in Pokémon gyms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Google and Anthropic both study how their latest AI models navigate early Pokémon games, the results can be as amusing as they are enlightening — and this time, Google DeepMind has written in a report that Gemini 2.5 Pro resorts to panic when its Pokémon are close to death. This can cause the AI’s performance to experience “qualitatively observable degradation in the model’s reasoning capability,” according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI benchmarking — or, the process of comparing the performance of different AI models — is a dubious art that often provides little context for the actual capabilities of a given model. But some researchers think that studying how AI models play video games could be useful (or, at the very least, kind of funny). &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Over the last several months, two developers unaffiliated with Google and Anthropic have set up respective Twitch streams called “Gemini Plays Pokémon” and “Claude Plays Pokémon,” where anyone can watch in real time as an AI tries to navigate a children’s video game from over 25 years ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each stream displays the AI’s “reasoning” process — or, a natural language translation of how the AI evaluates a problem and arrives at a response — giving us insight into the way that these models work. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3019676" height="539" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-17-at-3.43.39PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While the progress of these AI models is impressive, they are still not very good at playing Pokémon. It takes hundreds of hours for Gemini to reason through a game that a child could complete in exponentially less time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s interesting about watching an AI navigate a Pokémon game is not so much about its time of completion, but rather how it behaves along the way.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Over the course of the playthrough, Gemini 2.5 Pro gets into various situations which cause the model to simulate ‘panic,’” the report says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This state of “panic” can result in the model’s performance getting worse, as the AI may suddenly stop using certain tools at its disposal for a stretch of gameplay. While AI does not think or experience emotion, its actions mimic the way in which a human might make poor, hasty decisions when under stress — a fascinating, yet unsettling response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This behavior has occurred in enough separate instances that the members of the Twitch chat have actively noticed when it is occurring,” the report says.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Claude has also exhibited some curious behaviors in its journeys across Kanto. In one instance, the AI picked up on the pattern that when all of its Pokémon run out of health, the player character will “white out” and return to a Pokémon Center.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Claude got stuck in the Mt. Moon cave, it erroneously hypothesized that if it intentionally got all of its Pokémon to faint, then it would be transported across the cave to the Pokémon Center in the next town.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, that isn’t how the game works. When all of your Pokémon die, you return to whatever Pokémon Center you used most recently, rather than the nearest geographically. Viewers watched on in horror as the AI essentially tried to kill itself in the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite its shortcomings, there are a few ways in which the AI can outperform human players. As of the release of Gemini 2.5 Pro, the AI is able to solve puzzles with impressive accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With some human assistance, the AI created agentic tools — prompted instances of Gemini 2.5 Pro geared toward specific tasks — to solve the game’s boulder puzzles and find efficient routes to reach a destination.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With only a prompt describing boulder physics and a description of how to verify a valid path, Gemini 2.5 Pro is able to one-shot some of these complex boulder puzzles, which are required to progress through Victory Road,” the report says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Gemini 2.5 Pro did a lot of the work in creating these tools on its own, Google theorizes that the current model may be capable of creating these tools without human intervention. Who knows, maybe Gemini will therapize itself into creating a “don’t panic” module.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI companies are battling to dominate the industry, but sometimes they’re also battling in Pokémon gyms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As Google and Anthropic both study how their latest AI models navigate early Pokémon games, the results can be as amusing as they are enlightening — and this time, Google DeepMind has written in a report that Gemini 2.5 Pro resorts to panic when its Pokémon are close to death. This can cause the AI’s performance to experience “qualitatively observable degradation in the model’s reasoning capability,” according to the report.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI benchmarking — or, the process of comparing the performance of different AI models — is a dubious art that often provides little context for the actual capabilities of a given model. But some researchers think that studying how AI models play video games could be useful (or, at the very least, kind of funny). &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Over the last several months, two developers unaffiliated with Google and Anthropic have set up respective Twitch streams called “Gemini Plays Pokémon” and “Claude Plays Pokémon,” where anyone can watch in real time as an AI tries to navigate a children’s video game from over 25 years ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Each stream displays the AI’s “reasoning” process — or, a natural language translation of how the AI evaluates a problem and arrives at a response — giving us insight into the way that these models work. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3019676" height="539" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-17-at-3.43.39PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;While the progress of these AI models is impressive, they are still not very good at playing Pokémon. It takes hundreds of hours for Gemini to reason through a game that a child could complete in exponentially less time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s interesting about watching an AI navigate a Pokémon game is not so much about its time of completion, but rather how it behaves along the way.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;“Over the course of the playthrough, Gemini 2.5 Pro gets into various situations which cause the model to simulate ‘panic,’” the report says. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This state of “panic” can result in the model’s performance getting worse, as the AI may suddenly stop using certain tools at its disposal for a stretch of gameplay. While AI does not think or experience emotion, its actions mimic the way in which a human might make poor, hasty decisions when under stress — a fascinating, yet unsettling response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This behavior has occurred in enough separate instances that the members of the Twitch chat have actively noticed when it is occurring,” the report says.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Claude has also exhibited some curious behaviors in its journeys across Kanto. In one instance, the AI picked up on the pattern that when all of its Pokémon run out of health, the player character will “white out” and return to a Pokémon Center.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Claude got stuck in the Mt. Moon cave, it erroneously hypothesized that if it intentionally got all of its Pokémon to faint, then it would be transported across the cave to the Pokémon Center in the next town.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, that isn’t how the game works. When all of your Pokémon die, you return to whatever Pokémon Center you used most recently, rather than the nearest geographically. Viewers watched on in horror as the AI essentially tried to kill itself in the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite its shortcomings, there are a few ways in which the AI can outperform human players. As of the release of Gemini 2.5 Pro, the AI is able to solve puzzles with impressive accuracy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;With some human assistance, the AI created agentic tools — prompted instances of Gemini 2.5 Pro geared toward specific tasks — to solve the game’s boulder puzzles and find efficient routes to reach a destination.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With only a prompt describing boulder physics and a description of how to verify a valid path, Gemini 2.5 Pro is able to one-shot some of these complex boulder puzzles, which are required to progress through Victory Road,” the report says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Gemini 2.5 Pro did a lot of the work in creating these tools on its own, Google theorizes that the current model may be capable of creating these tools without human intervention. Who knows, maybe Gemini will therapize itself into creating a “don’t panic” module.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/googles-gemini-panicked-when-playing-pokemon/</guid><pubDate>Tue, 17 Jun 2025 20:53:19 +0000</pubDate></item><item><title>[NEW] OpenAI’s $200M DoD contract could squeeze frenemy Microsoft (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/openais-200m-dod-contract-could-squeeze-frenemy-microsoft/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1930518491.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI said Monday the U.S. Department of Defense granted it a contract for up to $200 million to help the agency identify and build prototype systems that use its frontier models for administrative tasks and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI provides a few examples of possible tasking, such as helping service members get healthcare, streamlining data on various programs, and “supporting proactive cyber defense.” The company also said that “All use cases must be consistent with OpenAI’s usage policies and guidelines.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The DoD’s announcement used slightly more straightforward wording. It says, “Under this award, the performer will develop prototype frontier AI capabilities to address critical national security challenges in both warfighting and enterprise domains.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that reference to war-fighting applies to the weapons themselves or just other areas associated with wars, like paperwork, remains to be seen. OpenAI’s guidelines do forbid individual users to use ChatGPT or its APIs to develop or use weapons. However, OpenAI deleted the explicit prohibitions of “military and warfare” in its terms of service back in January 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given how heavily some powerful people in Silicon Valley have warned of the dangers of China’s advanced LLM models, it’s not surprising the DoD wants to use OpenAI for whatever purposes it wants. For instance, Marc Andreessen, co-founder of VC firm Andreessen Horowitz, an OpenAI investor, recently appeared on Jack Altman’s “Uncapped” podcast (Jack is Sam Altman’s brother). Andreessen described the race between China’s AI and the Western world’s models as a “cold war.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, perhaps an equally interesting part of this announcement is what it says about OpenAI’s increasingly strained relationship with its major investor Microsoft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft has thousands of contracts with the federal government worth hundreds of millions of dollars. It has, for decades, been implementing the strict security protocols necessary for the government — especially the DoD — to use its cloud.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;OpenAI announced this deal as part of its broader new “OpenAI for Government” program, which consolidates a number of other programs it uses to sell wares directly to government agencies, including the U.S. National Labs⁠, the Air Force Research Laboratory, NASA, NIH, and the Treasury, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it was only in April that Microsoft announced the DoD had approved its Azure OpenAI Service for all classified levels. Now the DoD is also going straight to the source. From Microsoft’s perspective: Ouch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft declined to comment and OpenAI did not respond to a request for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1930518491.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI said Monday the U.S. Department of Defense granted it a contract for up to $200 million to help the agency identify and build prototype systems that use its frontier models for administrative tasks and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI provides a few examples of possible tasking, such as helping service members get healthcare, streamlining data on various programs, and “supporting proactive cyber defense.” The company also said that “All use cases must be consistent with OpenAI’s usage policies and guidelines.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The DoD’s announcement used slightly more straightforward wording. It says, “Under this award, the performer will develop prototype frontier AI capabilities to address critical national security challenges in both warfighting and enterprise domains.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether that reference to war-fighting applies to the weapons themselves or just other areas associated with wars, like paperwork, remains to be seen. OpenAI’s guidelines do forbid individual users to use ChatGPT or its APIs to develop or use weapons. However, OpenAI deleted the explicit prohibitions of “military and warfare” in its terms of service back in January 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Given how heavily some powerful people in Silicon Valley have warned of the dangers of China’s advanced LLM models, it’s not surprising the DoD wants to use OpenAI for whatever purposes it wants. For instance, Marc Andreessen, co-founder of VC firm Andreessen Horowitz, an OpenAI investor, recently appeared on Jack Altman’s “Uncapped” podcast (Jack is Sam Altman’s brother). Andreessen described the race between China’s AI and the Western world’s models as a “cold war.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, perhaps an equally interesting part of this announcement is what it says about OpenAI’s increasingly strained relationship with its major investor Microsoft.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft has thousands of contracts with the federal government worth hundreds of millions of dollars. It has, for decades, been implementing the strict security protocols necessary for the government — especially the DoD — to use its cloud.&amp;nbsp;&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;OpenAI announced this deal as part of its broader new “OpenAI for Government” program, which consolidates a number of other programs it uses to sell wares directly to government agencies, including the U.S. National Labs⁠, the Air Force Research Laboratory, NASA, NIH, and the Treasury, according to the company.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it was only in April that Microsoft announced the DoD had approved its Azure OpenAI Service for all classified levels. Now the DoD is also going straight to the source. From Microsoft’s perspective: Ouch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Microsoft declined to comment and OpenAI did not respond to a request for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/openais-200m-dod-contract-could-squeeze-frenemy-microsoft/</guid><pubDate>Tue, 17 Jun 2025 21:09:34 +0000</pubDate></item><item><title>[NEW] OpenAI moves forward with GPT-4.5 deprecation in API, triggering developer anguish and confusion (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/openai-moves-forward-with-gpt-4-5-deprecation-in-api-triggering-developer-anguish-and-confusion/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Word spread quickly across the machine learning and AI community on the social network X yesterday: OpenAI was sending developers an email notifying them that the company would be removing one of its largest and most powerful large language models (LLMs), GPT-4.5 Preview, from the official OpenAI application programming interface (API) &lt;strong&gt;on July 14, 2025.&lt;/strong&gt; &lt;/p&gt;



&lt;p&gt;However, as an OpenAI spokesperson told VentureBeat via email, GPT-4.5 Preview will remain an option to individual ChatGPT users through the dropdown model selector menu at the top of the application. &lt;/p&gt;



&lt;p&gt;But it stil means that any third-party developers who had built applications or workflows atop GPT-4.5 preview (we’ll call it GPT-4.5 from here on out for simplicity’s sake, since a full GPT-4.5 was never made available on this platform), now need to switch over to another one of OpenAI’s nearly 40 (!!!) different model offerings still available through the API. &lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012431" height="507" src="https://venturebeat.com/wp-content/uploads/2025/06/Gtl4zbCXIAAGnLj.jpg" width="633" /&gt;&lt;/figure&gt;



&lt;p&gt;The news quickly spread on X, where developers and AI enthusiasts posted reactions ranging from disappointment to confusion. &lt;/p&gt;



&lt;p&gt;Some described GPT-4.5 as a daily tool in their workflow, praising its tone and reliability. Others questioned the rationale behind launching the model in the first place if it was going to be short-lived.&lt;/p&gt;



&lt;p&gt;“This is sad — GPT-4.5 is one of my fav models,” wrote @BumrahBachi. &lt;/p&gt;



&lt;p&gt;Ben Hyak, the co-founder of AI observability and performance monitoring platform Raindrop.AI, called the move “tragic,” adding: “o3 + 4.5 are the models I use the most everyday.” &lt;/p&gt;



&lt;p&gt;Another user, @flowersslop asked bluntly, “what was the purpose of this model all along?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-deprecation-had-been-planned-since-april"&gt;Deprecation had been planned since April&lt;/h2&gt;



&lt;p&gt;Despite the strong reaction, OpenAI had in fact already announced the plan to deprecate GPT-4.5 Preview back in April 2025 during the launch of GPT-4.1. &lt;/p&gt;



&lt;p&gt;At that time, the company stated that developers would have three months to transition away from 4.5. OpenAI framed the model as an experimental offering that provided insights for future development, and said it would carry forward learnings from GPT-4.5 into future iterations — particularly in areas like creativity and writing nuance.&lt;/p&gt;



&lt;p&gt;In a follow-up response to VentureBeat, OpenAI communications confirmed that the June email was simply a scheduled reminder and that there are currently no plans to remove GPT-4.5 from ChatGPT subscriptions, where the model remains available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-speculation-on-cost-and-model-strategy"&gt;Community speculation on cost and model strategy&lt;/h2&gt;



&lt;p&gt;Still, the developer-facing deprecation leaves a gap for some users, especially those who had built workflows or products around GPT-4.5’s specific characteristics. &lt;/p&gt;



&lt;p&gt;Some in the community speculated that high compute costs might have influenced the move, noting that similar changes had occurred with prior models. &lt;/p&gt;



&lt;p&gt;Others referenced recent API pricing updates, including a major reduction in cost for GPT-3.5 (internally referred to as o3), which is now priced 80% lower than before.&lt;/p&gt;



&lt;p&gt;User @chatgpt21 commented that GPT-4.5 is the “best non reasoning model for open ai on all benchmarks and it’s obvious,” and predicted that OpenAI will “they add test time compute it will blow o3 out of the water. In order to scale TTC you need to scale pre training”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-end-of-the-road-for-gpt-4-5-via-api-developers-encouraged-to-migrate-to-gpt-4-1"&gt;The end of the road for GPT-4.5 via API — developers encouraged to migrate to GPT-4.1&lt;/h2&gt;



&lt;p&gt;OpenAI has directed developers to its online forum for questions about migrating to GPT-4.1 or other models. With the API shutdown for GPT-4.5 Preview set for mid-July, teams relying on the model now have less than a month to complete that transition.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Word spread quickly across the machine learning and AI community on the social network X yesterday: OpenAI was sending developers an email notifying them that the company would be removing one of its largest and most powerful large language models (LLMs), GPT-4.5 Preview, from the official OpenAI application programming interface (API) &lt;strong&gt;on July 14, 2025.&lt;/strong&gt; &lt;/p&gt;



&lt;p&gt;However, as an OpenAI spokesperson told VentureBeat via email, GPT-4.5 Preview will remain an option to individual ChatGPT users through the dropdown model selector menu at the top of the application. &lt;/p&gt;



&lt;p&gt;But it stil means that any third-party developers who had built applications or workflows atop GPT-4.5 preview (we’ll call it GPT-4.5 from here on out for simplicity’s sake, since a full GPT-4.5 was never made available on this platform), now need to switch over to another one of OpenAI’s nearly 40 (!!!) different model offerings still available through the API. &lt;/p&gt;



&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="alt" class="wp-image-3012431" height="507" src="https://venturebeat.com/wp-content/uploads/2025/06/Gtl4zbCXIAAGnLj.jpg" width="633" /&gt;&lt;/figure&gt;



&lt;p&gt;The news quickly spread on X, where developers and AI enthusiasts posted reactions ranging from disappointment to confusion. &lt;/p&gt;



&lt;p&gt;Some described GPT-4.5 as a daily tool in their workflow, praising its tone and reliability. Others questioned the rationale behind launching the model in the first place if it was going to be short-lived.&lt;/p&gt;



&lt;p&gt;“This is sad — GPT-4.5 is one of my fav models,” wrote @BumrahBachi. &lt;/p&gt;



&lt;p&gt;Ben Hyak, the co-founder of AI observability and performance monitoring platform Raindrop.AI, called the move “tragic,” adding: “o3 + 4.5 are the models I use the most everyday.” &lt;/p&gt;



&lt;p&gt;Another user, @flowersslop asked bluntly, “what was the purpose of this model all along?”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-deprecation-had-been-planned-since-april"&gt;Deprecation had been planned since April&lt;/h2&gt;



&lt;p&gt;Despite the strong reaction, OpenAI had in fact already announced the plan to deprecate GPT-4.5 Preview back in April 2025 during the launch of GPT-4.1. &lt;/p&gt;



&lt;p&gt;At that time, the company stated that developers would have three months to transition away from 4.5. OpenAI framed the model as an experimental offering that provided insights for future development, and said it would carry forward learnings from GPT-4.5 into future iterations — particularly in areas like creativity and writing nuance.&lt;/p&gt;



&lt;p&gt;In a follow-up response to VentureBeat, OpenAI communications confirmed that the June email was simply a scheduled reminder and that there are currently no plans to remove GPT-4.5 from ChatGPT subscriptions, where the model remains available.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-community-speculation-on-cost-and-model-strategy"&gt;Community speculation on cost and model strategy&lt;/h2&gt;



&lt;p&gt;Still, the developer-facing deprecation leaves a gap for some users, especially those who had built workflows or products around GPT-4.5’s specific characteristics. &lt;/p&gt;



&lt;p&gt;Some in the community speculated that high compute costs might have influenced the move, noting that similar changes had occurred with prior models. &lt;/p&gt;



&lt;p&gt;Others referenced recent API pricing updates, including a major reduction in cost for GPT-3.5 (internally referred to as o3), which is now priced 80% lower than before.&lt;/p&gt;



&lt;p&gt;User @chatgpt21 commented that GPT-4.5 is the “best non reasoning model for open ai on all benchmarks and it’s obvious,” and predicted that OpenAI will “they add test time compute it will blow o3 out of the water. In order to scale TTC you need to scale pre training”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-end-of-the-road-for-gpt-4-5-via-api-developers-encouraged-to-migrate-to-gpt-4-1"&gt;The end of the road for GPT-4.5 via API — developers encouraged to migrate to GPT-4.1&lt;/h2&gt;



&lt;p&gt;OpenAI has directed developers to its online forum for questions about migrating to GPT-4.1 or other models. With the API shutdown for GPT-4.5 Preview set for mid-July, teams relying on the model now have less than a month to complete that transition.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-moves-forward-with-gpt-4-5-deprecation-in-api-triggering-developer-anguish-and-confusion/</guid><pubDate>Tue, 17 Jun 2025 21:52:29 +0000</pubDate></item><item><title>[NEW] Google launches production-ready Gemini 2.5 AI models to challenge OpenAI’s enterprise dominance (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/google-launches-production-ready-gemini-2-5-ai-models-to-challenge-openais-enterprise-dominance/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google moved decisively to strengthen its position in the artificial intelligence arms race Monday, declaring its most powerful Gemini 2.5 models ready for enterprise production while unveiling a new ultra-efficient variant designed to undercut competitors on cost and speed.&lt;/p&gt;



&lt;p&gt;The Alphabet subsidiary promoted two of its flagship AI models—Gemini 2.5 Pro and Gemini 2.5 Flash—from experimental preview status to general availability, signaling the company’s confidence that the technology can handle mission-critical business applications. Google simultaneously introduced Gemini 2.5 Flash-Lite, positioning it as the most cost-effective option in its model lineup for high-volume tasks.&lt;/p&gt;



&lt;p&gt;The announcements represent Google’s most assertive challenge yet to OpenAI’s market leadership, offering enterprises a comprehensive suite of AI tools spanning from premium reasoning capabilities to budget-conscious automation. The move comes as businesses increasingly demand production-ready AI systems that can scale reliably across their operations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-google-finally-moved-its-most-powerful-ai-models-from-preview-to-production-status"&gt;Why Google finally moved its most powerful AI models from preview to production status&lt;/h2&gt;



&lt;p&gt;Google’s decision to graduate these models from preview reflects mounting pressure to match OpenAI’s rapid deployment of consumer and enterprise AI tools. While OpenAI has dominated headlines with ChatGPT and its GPT-4 family, Google has pursued a more cautious approach, extensively testing models before declaring them production-ready.&lt;/p&gt;



&lt;p&gt;“The momentum of the Gemini 2.5 era continues to build,” wrote Jason Gelman, Director of Product Management for Vertex AI, in a blog post announcing the updates. The language suggests Google views this moment as pivotal in establishing its AI platform’s credibility among enterprise buyers.&lt;/p&gt;



&lt;p&gt;The timing appears strategic. Google released these updates just weeks after OpenAI faced scrutiny over the safety and reliability of its latest models, creating an opening for Google to position itself as the more stable, enterprise-focused alternative.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-gemini-s-thinking-capabilities-give-enterprises-more-control-over-ai-decision-making"&gt;How Gemini’s ‘thinking’ capabilities give enterprises more control over AI decision-making&lt;/h2&gt;



&lt;p&gt;What distinguishes Google’s approach is its emphasis on “reasoning” or “thinking” capabilities — a technical architecture that allows models to process problems more deliberately before responding. Unlike traditional language models that generate responses immediately, Gemini 2.5 models can spend additional computational resources working through complex problems step-by-step.&lt;/p&gt;



&lt;p&gt;This “thinking budget” gives developers unprecedented control over AI behavior. They can instruct models to think longer for complex reasoning tasks or respond quickly for simple queries, optimizing both accuracy and cost. The feature addresses a critical enterprise need: predictable AI behavior that can be tuned for specific business requirements.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Pro, positioned as Google’s most capable model, excels at complex reasoning, advanced code generation, and multimodal understanding. It can process up to one million tokens of context—roughly equivalent to 750,000 words — enabling it to analyze entire codebases or lengthy documents in a single session.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Flash strikes a balance between capability and efficiency, designed for high-throughput enterprise tasks like large-scale document summarization and responsive chat applications. The newly introduced Flash-Lite variant sacrifices some intelligence for dramatic cost savings, targeting use cases like classification and translation where speed and volume matter more than sophisticated reasoning.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-companies-like-snap-and-smartbear-are-already-using-gemini-2-5-in-mission-critical-applications"&gt;Major companies like Snap and SmartBear are already using Gemini 2.5 in mission-critical applications&lt;/h2&gt;



&lt;p&gt;Several major companies have already integrated these models into production systems, suggesting Google’s confidence in their stability isn’t misplaced. Snap Inc. uses Gemini 2.5 Pro to power spatial intelligence features in its AR glasses, translating 2D image coordinates into 3D space for augmented reality applications.&lt;/p&gt;



&lt;p&gt;SmartBear, which provides software testing tools, leverages Gemini 2.5 Flash to translate manual test scripts into automated tests. “The ROI is multifaceted,” said Fitz Nowlan, the company’s VP of AI, describing how the technology accelerates testing velocity while reducing costs.&lt;/p&gt;



&lt;p&gt;Healthcare technology company Connective Health uses the models to extract vital medical information from complex free-text records — a task requiring both accuracy and reliability given the life-or-death nature of medical data. The company’s success with these applications suggests Google’s models have achieved the reliability threshold necessary for regulated industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-google-s-new-ai-pricing-strategy-targets-both-premium-and-budget-conscious-enterprise-customers"&gt;Google’s new AI pricing strategy targets both premium and budget-conscious enterprise customers&lt;/h2&gt;



&lt;p&gt;Google’s pricing decisions signal its determination to compete aggressively across market segments. The company raised prices for Gemini 2.5 Flash input tokens from $0.15 to $0.30 per million tokens while reducing output token costs from $3.50 to $2.50 per million tokens. This restructuring benefits applications that generate lengthy responses — a common enterprise use case.&lt;/p&gt;



&lt;p&gt;More significantly, Google eliminated the previous distinction between “thinking” and “non-thinking” pricing that had confused developers. The simplified pricing structure removes a barrier to adoption while making cost prediction easier for enterprise buyers.&lt;/p&gt;



&lt;p&gt;Flash-Lite’s introduction at $0.10 per million input tokens and $0.40 per million output tokens creates a new bottom tier designed to capture price-sensitive workloads. This pricing positions Google to compete with smaller AI providers who have gained traction by offering basic models at extremely low costs.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-google-s-three-tier-model-lineup-means-for-the-competitive-ai-landscape"&gt;What Google’s three-tier model lineup means for the competitive AI landscape&lt;/h2&gt;



&lt;p&gt;The simultaneous release of three production-ready models across different performance tiers represents a sophisticated market segmentation strategy. Google appears to be borrowing from the traditional software industry playbook: offer good, better, and best options to capture customers across budget ranges while providing upgrade paths as needs evolve.&lt;/p&gt;



&lt;p&gt;This approach contrasts sharply with OpenAI’s strategy of pushing users toward its most capable (and expensive) models. Google’s willingness to offer genuinely low-cost alternatives could disrupt the market’s pricing dynamics, particularly for high-volume applications where cost per interaction matters more than peak performance.&lt;/p&gt;



&lt;p&gt;The technical capabilities also position Google advantageously for enterprise sales cycles. The million-token context length enables use cases—like analyzing entire legal contracts or processing comprehensive financial reports — that competing models cannot handle effectively. For large enterprises with complex document processing needs, this capability difference could prove decisive.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-google-s-enterprise-focused-approach-differs-from-openai-s-consumer-first-strategy"&gt;How Google’s enterprise-focused approach differs from OpenAI’s consumer-first strategy&lt;/h2&gt;



&lt;p&gt;These releases occur against the backdrop of intensifying AI competition across multiple fronts. While consumer attention focuses on chatbot interfaces, the real business value—and revenue potential—lies in enterprise applications that can automate complex workflows and augment human decision-making.&lt;/p&gt;



&lt;p&gt;Google’s emphasis on production readiness and enterprise features suggests the company has learned from earlier AI deployment challenges. Previous Google AI launches sometimes felt premature or disconnected from real business needs. The extensive preview period for Gemini 2.5 models, combined with early enterprise partnerships, indicates a more mature approach to product development.&lt;/p&gt;



&lt;p&gt;The technical architecture choices also reflect lessons learned from the broader industry. The “thinking” capability addresses criticism that AI models make decisions too quickly, without sufficient consideration of complex factors. By making this reasoning process controllable and transparent, Google positions its models as more trustworthy for high-stakes business applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprises-need-to-know-about-choosing-between-competing-ai-platforms"&gt;What enterprises need to know about choosing between competing AI platforms&lt;/h2&gt;



&lt;p&gt;Google’s aggressive positioning of the Gemini 2.5 family sets up 2025 as a pivotal year for enterprise AI adoption. With production-ready models spanning performance and cost requirements, Google has eliminated many of the technical and economic barriers that previously limited enterprise AI deployment.&lt;/p&gt;



&lt;p&gt;The real test will come as businesses integrate these tools into critical workflows. Early enterprise adopters report promising results, but broader market validation requires months of production use across diverse industries and applications.&lt;/p&gt;



&lt;p&gt;For technical decision makers, Google’s announcement creates both opportunity and complexity. The range of model options enables more precise matching of capabilities to requirements, but also demands more sophisticated evaluation and deployment strategies. Organizations must now consider not just whether to adopt AI, but which specific models and configurations best serve their unique needs.&lt;/p&gt;



&lt;p&gt;The stakes extend beyond individual company decisions. As AI becomes integral to business operations across industries, the choice of AI platform increasingly determines competitive advantage. Enterprise buyers face a critical inflection point: commit to a single AI provider’s ecosystem or maintain costly multi-vendor strategies as the technology matures.&lt;/p&gt;



&lt;p&gt;Google wants to become the enterprise standard for AI—a position that could prove extraordinarily valuable as AI adoption accelerates. The company that created the search engine now wants to create the intelligence engine that powers every business decision.&lt;/p&gt;



&lt;p&gt;After years of watching OpenAI capture headlines and market share, Google has finally stopped talking about the future of AI and started selling it.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Google moved decisively to strengthen its position in the artificial intelligence arms race Monday, declaring its most powerful Gemini 2.5 models ready for enterprise production while unveiling a new ultra-efficient variant designed to undercut competitors on cost and speed.&lt;/p&gt;



&lt;p&gt;The Alphabet subsidiary promoted two of its flagship AI models—Gemini 2.5 Pro and Gemini 2.5 Flash—from experimental preview status to general availability, signaling the company’s confidence that the technology can handle mission-critical business applications. Google simultaneously introduced Gemini 2.5 Flash-Lite, positioning it as the most cost-effective option in its model lineup for high-volume tasks.&lt;/p&gt;



&lt;p&gt;The announcements represent Google’s most assertive challenge yet to OpenAI’s market leadership, offering enterprises a comprehensive suite of AI tools spanning from premium reasoning capabilities to budget-conscious automation. The move comes as businesses increasingly demand production-ready AI systems that can scale reliably across their operations.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-google-finally-moved-its-most-powerful-ai-models-from-preview-to-production-status"&gt;Why Google finally moved its most powerful AI models from preview to production status&lt;/h2&gt;



&lt;p&gt;Google’s decision to graduate these models from preview reflects mounting pressure to match OpenAI’s rapid deployment of consumer and enterprise AI tools. While OpenAI has dominated headlines with ChatGPT and its GPT-4 family, Google has pursued a more cautious approach, extensively testing models before declaring them production-ready.&lt;/p&gt;



&lt;p&gt;“The momentum of the Gemini 2.5 era continues to build,” wrote Jason Gelman, Director of Product Management for Vertex AI, in a blog post announcing the updates. The language suggests Google views this moment as pivotal in establishing its AI platform’s credibility among enterprise buyers.&lt;/p&gt;



&lt;p&gt;The timing appears strategic. Google released these updates just weeks after OpenAI faced scrutiny over the safety and reliability of its latest models, creating an opening for Google to position itself as the more stable, enterprise-focused alternative.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-gemini-s-thinking-capabilities-give-enterprises-more-control-over-ai-decision-making"&gt;How Gemini’s ‘thinking’ capabilities give enterprises more control over AI decision-making&lt;/h2&gt;



&lt;p&gt;What distinguishes Google’s approach is its emphasis on “reasoning” or “thinking” capabilities — a technical architecture that allows models to process problems more deliberately before responding. Unlike traditional language models that generate responses immediately, Gemini 2.5 models can spend additional computational resources working through complex problems step-by-step.&lt;/p&gt;



&lt;p&gt;This “thinking budget” gives developers unprecedented control over AI behavior. They can instruct models to think longer for complex reasoning tasks or respond quickly for simple queries, optimizing both accuracy and cost. The feature addresses a critical enterprise need: predictable AI behavior that can be tuned for specific business requirements.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Pro, positioned as Google’s most capable model, excels at complex reasoning, advanced code generation, and multimodal understanding. It can process up to one million tokens of context—roughly equivalent to 750,000 words — enabling it to analyze entire codebases or lengthy documents in a single session.&lt;/p&gt;



&lt;p&gt;Gemini 2.5 Flash strikes a balance between capability and efficiency, designed for high-throughput enterprise tasks like large-scale document summarization and responsive chat applications. The newly introduced Flash-Lite variant sacrifices some intelligence for dramatic cost savings, targeting use cases like classification and translation where speed and volume matter more than sophisticated reasoning.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-major-companies-like-snap-and-smartbear-are-already-using-gemini-2-5-in-mission-critical-applications"&gt;Major companies like Snap and SmartBear are already using Gemini 2.5 in mission-critical applications&lt;/h2&gt;



&lt;p&gt;Several major companies have already integrated these models into production systems, suggesting Google’s confidence in their stability isn’t misplaced. Snap Inc. uses Gemini 2.5 Pro to power spatial intelligence features in its AR glasses, translating 2D image coordinates into 3D space for augmented reality applications.&lt;/p&gt;



&lt;p&gt;SmartBear, which provides software testing tools, leverages Gemini 2.5 Flash to translate manual test scripts into automated tests. “The ROI is multifaceted,” said Fitz Nowlan, the company’s VP of AI, describing how the technology accelerates testing velocity while reducing costs.&lt;/p&gt;



&lt;p&gt;Healthcare technology company Connective Health uses the models to extract vital medical information from complex free-text records — a task requiring both accuracy and reliability given the life-or-death nature of medical data. The company’s success with these applications suggests Google’s models have achieved the reliability threshold necessary for regulated industries.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-google-s-new-ai-pricing-strategy-targets-both-premium-and-budget-conscious-enterprise-customers"&gt;Google’s new AI pricing strategy targets both premium and budget-conscious enterprise customers&lt;/h2&gt;



&lt;p&gt;Google’s pricing decisions signal its determination to compete aggressively across market segments. The company raised prices for Gemini 2.5 Flash input tokens from $0.15 to $0.30 per million tokens while reducing output token costs from $3.50 to $2.50 per million tokens. This restructuring benefits applications that generate lengthy responses — a common enterprise use case.&lt;/p&gt;



&lt;p&gt;More significantly, Google eliminated the previous distinction between “thinking” and “non-thinking” pricing that had confused developers. The simplified pricing structure removes a barrier to adoption while making cost prediction easier for enterprise buyers.&lt;/p&gt;



&lt;p&gt;Flash-Lite’s introduction at $0.10 per million input tokens and $0.40 per million output tokens creates a new bottom tier designed to capture price-sensitive workloads. This pricing positions Google to compete with smaller AI providers who have gained traction by offering basic models at extremely low costs.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-google-s-three-tier-model-lineup-means-for-the-competitive-ai-landscape"&gt;What Google’s three-tier model lineup means for the competitive AI landscape&lt;/h2&gt;



&lt;p&gt;The simultaneous release of three production-ready models across different performance tiers represents a sophisticated market segmentation strategy. Google appears to be borrowing from the traditional software industry playbook: offer good, better, and best options to capture customers across budget ranges while providing upgrade paths as needs evolve.&lt;/p&gt;



&lt;p&gt;This approach contrasts sharply with OpenAI’s strategy of pushing users toward its most capable (and expensive) models. Google’s willingness to offer genuinely low-cost alternatives could disrupt the market’s pricing dynamics, particularly for high-volume applications where cost per interaction matters more than peak performance.&lt;/p&gt;



&lt;p&gt;The technical capabilities also position Google advantageously for enterprise sales cycles. The million-token context length enables use cases—like analyzing entire legal contracts or processing comprehensive financial reports — that competing models cannot handle effectively. For large enterprises with complex document processing needs, this capability difference could prove decisive.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-google-s-enterprise-focused-approach-differs-from-openai-s-consumer-first-strategy"&gt;How Google’s enterprise-focused approach differs from OpenAI’s consumer-first strategy&lt;/h2&gt;



&lt;p&gt;These releases occur against the backdrop of intensifying AI competition across multiple fronts. While consumer attention focuses on chatbot interfaces, the real business value—and revenue potential—lies in enterprise applications that can automate complex workflows and augment human decision-making.&lt;/p&gt;



&lt;p&gt;Google’s emphasis on production readiness and enterprise features suggests the company has learned from earlier AI deployment challenges. Previous Google AI launches sometimes felt premature or disconnected from real business needs. The extensive preview period for Gemini 2.5 models, combined with early enterprise partnerships, indicates a more mature approach to product development.&lt;/p&gt;



&lt;p&gt;The technical architecture choices also reflect lessons learned from the broader industry. The “thinking” capability addresses criticism that AI models make decisions too quickly, without sufficient consideration of complex factors. By making this reasoning process controllable and transparent, Google positions its models as more trustworthy for high-stakes business applications.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-enterprises-need-to-know-about-choosing-between-competing-ai-platforms"&gt;What enterprises need to know about choosing between competing AI platforms&lt;/h2&gt;



&lt;p&gt;Google’s aggressive positioning of the Gemini 2.5 family sets up 2025 as a pivotal year for enterprise AI adoption. With production-ready models spanning performance and cost requirements, Google has eliminated many of the technical and economic barriers that previously limited enterprise AI deployment.&lt;/p&gt;



&lt;p&gt;The real test will come as businesses integrate these tools into critical workflows. Early enterprise adopters report promising results, but broader market validation requires months of production use across diverse industries and applications.&lt;/p&gt;



&lt;p&gt;For technical decision makers, Google’s announcement creates both opportunity and complexity. The range of model options enables more precise matching of capabilities to requirements, but also demands more sophisticated evaluation and deployment strategies. Organizations must now consider not just whether to adopt AI, but which specific models and configurations best serve their unique needs.&lt;/p&gt;



&lt;p&gt;The stakes extend beyond individual company decisions. As AI becomes integral to business operations across industries, the choice of AI platform increasingly determines competitive advantage. Enterprise buyers face a critical inflection point: commit to a single AI provider’s ecosystem or maintain costly multi-vendor strategies as the technology matures.&lt;/p&gt;



&lt;p&gt;Google wants to become the enterprise standard for AI—a position that could prove extraordinarily valuable as AI adoption accelerates. The company that created the search engine now wants to create the intelligence engine that powers every business decision.&lt;/p&gt;



&lt;p&gt;After years of watching OpenAI capture headlines and market share, Google has finally stopped talking about the future of AI and started selling it.&lt;/p&gt;




&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/google-launches-production-ready-gemini-2-5-ai-models-to-challenge-openais-enterprise-dominance/</guid><pubDate>Tue, 17 Jun 2025 21:55:56 +0000</pubDate></item><item><title>[NEW] Sam Altman says Meta tried and failed to poach OpenAI’s talent with $100M offers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/sam-altman-says-meta-tried-and-failed-to-poach-openais-talent-with-100m-offers/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1535376729-e1731897472270.jpg?resize=1200,798" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta CEO Mark Zuckerberg has been on something of a hiring spree lately, trying to staff up Meta’s new superintelligence team with top-tier AI researchers from competing labs. To work on a team led by former Scale AI CEO Alexandr Wang and at a desk physically near Zuckerberg, Meta has reportedly offered employees from OpenAI and Google DeepMind compensation packages worth upwards of $100 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO Sam Altman confirmed those reports on a podcast with his brother, Jack Altman, which was published on Tuesday. However, the OpenAI CEO noted that Zuckerberg’s recruiting efforts have been largely unsuccessful and made sure to throw a few more digs at Meta in the process.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“[Meta has] started making these, like, giant offers to a lot of people on our team,” Sam Altman said on the podcast. “You know, like, $100 million signing bonuses, more than that [in] compensation per year […] I’m really happy that, at least so far, none of our best people have decided to take him up on that.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO said he believed his employees made the assessment that OpenAI had a better chance of achieving AGI and may one day be the more valuable company. He also said he believes Meta’s focus on high compensation packages for employees, rather than the mission of delivering AGI, would likely not create a great culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta reportedly tried to poach one of OpenAI’s lead researchers, Noam Brown, as well as Google’s AI architect, Koray Kavukcuoglu. However, both efforts were unsuccessful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sam Altman went on to say he believes OpenAI’s culture of innovation has been a major key to its success, and that Meta’s “current AI efforts have not worked as well as they hoped.” The OpenAI CEO said he respects many things about Meta but noted he doesn’t “think they’re a company that’s great at innovation.” Later in the podcast, Altman said he believes it’s not enough for companies to catch up on AI — they have to truly innovate to stay ahead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO’s comments highlight some of the challenges that Meta has to overcome in order to build out a successful AI superintelligence lab. Besides bringing on Wang, Meta announced last week that it invested significantly in Wang’s former company, Scale AI. The company has also reportedly nabbed a few star AI researchers, such as Google DeepMind’s Jack Rae and Sesame AI’s Johan Schalkwyk. But there’s more work ahead.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;In the coming year, Meta will have to staff up its new AI team while OpenAI, Anthropic, and Google DeepMind operate at full speed. In the coming months, OpenAI is expected to release an open AI model that’s likely to set Meta back in the AI race even further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later on in the podcast, Sam Altman described an AI-powered social media feed that seems likely to encroach on Meta’s apps. The OpenAI CEO said he’s curious about exploring a social media app that uses AI to deliver custom feeds based on what users want, rather than the default, algorithmic feed that exists on traditional social media apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is reportedly working on a social networking app internally. Meanwhile, Meta is experimenting with an AI-powered social network through its Meta AI app. However, it seems that some users are confused by the Meta AI app and have shared some hyperpersonal chats with the broader world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI-powered social networks take off remains to be seen. In the meantime, Zuckerberg and Sam Altman seem poised to butt heads over the AI talent race.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/11/GettyImages-1535376729-e1731897472270.jpg?resize=1200,798" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta CEO Mark Zuckerberg has been on something of a hiring spree lately, trying to staff up Meta’s new superintelligence team with top-tier AI researchers from competing labs. To work on a team led by former Scale AI CEO Alexandr Wang and at a desk physically near Zuckerberg, Meta has reportedly offered employees from OpenAI and Google DeepMind compensation packages worth upwards of $100 million.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI CEO Sam Altman confirmed those reports on a podcast with his brother, Jack Altman, which was published on Tuesday. However, the OpenAI CEO noted that Zuckerberg’s recruiting efforts have been largely unsuccessful and made sure to throw a few more digs at Meta in the process.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“[Meta has] started making these, like, giant offers to a lot of people on our team,” Sam Altman said on the podcast. “You know, like, $100 million signing bonuses, more than that [in] compensation per year […] I’m really happy that, at least so far, none of our best people have decided to take him up on that.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO said he believed his employees made the assessment that OpenAI had a better chance of achieving AGI and may one day be the more valuable company. He also said he believes Meta’s focus on high compensation packages for employees, rather than the mission of delivering AGI, would likely not create a great culture.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta reportedly tried to poach one of OpenAI’s lead researchers, Noam Brown, as well as Google’s AI architect, Koray Kavukcuoglu. However, both efforts were unsuccessful.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sam Altman went on to say he believes OpenAI’s culture of innovation has been a major key to its success, and that Meta’s “current AI efforts have not worked as well as they hoped.” The OpenAI CEO said he respects many things about Meta but noted he doesn’t “think they’re a company that’s great at innovation.” Later in the podcast, Altman said he believes it’s not enough for companies to catch up on AI — they have to truly innovate to stay ahead.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The OpenAI CEO’s comments highlight some of the challenges that Meta has to overcome in order to build out a successful AI superintelligence lab. Besides bringing on Wang, Meta announced last week that it invested significantly in Wang’s former company, Scale AI. The company has also reportedly nabbed a few star AI researchers, such as Google DeepMind’s Jack Rae and Sesame AI’s Johan Schalkwyk. But there’s more work ahead.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;In the coming year, Meta will have to staff up its new AI team while OpenAI, Anthropic, and Google DeepMind operate at full speed. In the coming months, OpenAI is expected to release an open AI model that’s likely to set Meta back in the AI race even further.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Later on in the podcast, Sam Altman described an AI-powered social media feed that seems likely to encroach on Meta’s apps. The OpenAI CEO said he’s curious about exploring a social media app that uses AI to deliver custom feeds based on what users want, rather than the default, algorithmic feed that exists on traditional social media apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is reportedly working on a social networking app internally. Meanwhile, Meta is experimenting with an AI-powered social network through its Meta AI app. However, it seems that some users are confused by the Meta AI app and have shared some hyperpersonal chats with the broader world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Whether AI-powered social networks take off remains to be seen. In the meantime, Zuckerberg and Sam Altman seem poised to butt heads over the AI talent race.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/sam-altman-says-meta-tried-and-failed-to-poach-openais-talent-with-100m-offers/</guid><pubDate>Tue, 17 Jun 2025 22:31:28 +0000</pubDate></item><item><title>[NEW] The Interpretable AI playbook: What Anthropic’s research means for your enterprise LLM strategy (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/the-interpretable-ai-playbook-what-anthropics-research-means-for-your-enterprise-llm-strategy/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic CEO Dario Amodei made an urgent push in April for the need to understand how AI models think.&lt;/p&gt;



&lt;p&gt;This comes at a crucial time. As Anthropic battles in global AI rankings, it’s important to note what sets it apart from other top AI labs. Since its founding in 2021, when seven OpenAI employees broke off over concerns about AI safety, Anthropic has built AI models that adhere to a set of human-valued principles, a system they call Constitutional AI. These principles ensure that models are “helpful, honest and harmless” and generally act in the best interests of society. At the same time, Anthropic’s research arm is diving deep to understand how its models think about the world, and &lt;em&gt;why&lt;/em&gt; they produce helpful (and sometimes harmful) answers.&lt;/p&gt;



&lt;p&gt;Anthropic’s flagship model, Claude 3.7 Sonnet, dominated coding benchmarks when it launched in February, proving that AI models can excel at both performance and safety. And the recent release of Claude 4.0 Opus and Sonnet again puts Claude at the top of coding benchmarks. However, in today’s rapid and hyper-competitive AI market, Anthropic’s rivals like Google’s Gemini 2.5 Pro and Open AI’s o3 have their own impressive showings for coding prowess, while they’re already dominating Claude at math, creative writing and overall reasoning across many languages.&lt;/p&gt;



&lt;p&gt;If Amodei’s thoughts are any indication, Anthropic is planning for the future of AI and its implications in critical fields like medicine, psychology and law, where model safety and human values are imperative. And it shows: Anthropic is the leading AI lab that focuses strictly on developing “interpretable” AI, which are models that let us understand, to some degree of certainty, what the model is thinking and how it arrives at a particular conclusion.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amazon and Google have already invested billions of dollars in Anthropic even as they build their own AI models, so perhaps Anthropic’s competitive advantage is still budding. Interpretable models, as Anthropic suggests, could significantly reduce the long-term operational costs associated with debugging, auditing and mitigating risks in complex AI deployments.&lt;/p&gt;



&lt;p&gt;Sayash Kapoor, an AI safety researcher, suggests that while interpretability is valuable, it is just one of many tools for managing AI risk. In his view, “interpretability is neither necessary nor sufficient” to ensure models behave safely — it matters most when paired with filters, verifiers and human-centered design. This more expansive view sees interpretability as part of a larger ecosystem of control strategies, particularly in real-world AI deployments where models are components in broader decision-making systems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-need-for-interpretable-ai"&gt;The need for interpretable AI&lt;/h2&gt;



&lt;p&gt;Until recently, many thought AI was still years from advancements like those that are now helping Claude, Gemini and ChatGPT boast exceptional market adoption. While these models are already pushing the frontiers of human knowledge, their widespread use is attributable to just how good they are at solving a wide range of practical problems that require creative problem-solving or detailed analysis. As models are put to the task on increasingly critical problems, it is important that they produce accurate answers.&lt;/p&gt;



&lt;p&gt;Amodei fears that when an AI responds to a prompt, “we have no idea… why it chooses certain words over others, or why it occasionally makes a mistake despite usually being accurate.” Such errors — hallucinations of inaccurate information, or responses that do not align with human values — will hold AI models back from reaching their full potential. Indeed, we’ve seen many examples of AI continuing to struggle with hallucinations and unethical behavior.&lt;/p&gt;



&lt;p&gt;For Amodei, the best way to solve these problems is to understand how an AI thinks: “Our inability to understand models’ internal mechanisms means that we cannot meaningfully predict such [harmful] behaviors, and therefore struggle to rule them out … If instead it were possible to look inside models, we might be able to systematically block all jailbreaks, and also characterize what dangerous knowledge the models have.”&lt;/p&gt;



&lt;p&gt;Amodei also sees the opacity of current models as a barrier to deploying AI models in “high-stakes financial or safety-critical settings, because we can’t fully set the limits on their behavior, and a small number of mistakes could be very harmful.” In decision-making that affects humans directly, like medical diagnosis or mortgage assessments, legal regulations require AI to explain its decisions.&lt;/p&gt;



&lt;p&gt;Imagine a financial institution using a large language model (LLM) for fraud detection — interpretability could mean explaining a denied loan application to a customer as required by law. Or a manufacturing firm optimizing supply chains — understanding why an AI suggests a particular supplier could unlock efficiencies and prevent unforeseen bottlenecks.&lt;/p&gt;



&lt;p&gt;Because of this, Amodei explains, “Anthropic is doubling down on interpretability, and we have a goal of getting to ‘interpretability can reliably detect most model problems’ by 2027.”&lt;/p&gt;



&lt;p&gt;To that end, Anthropic recently participated in a $50 million investment in Goodfire, an AI research lab making breakthrough progress on AI “brain scans.” Their model inspection platform, Ember, is an agnostic tool that identifies learned concepts within models and lets users manipulate them. In a recent demo, the company showed how Ember can recognize individual visual concepts within an image generation AI and then let users &lt;em&gt;paint&lt;/em&gt; these concepts on a canvas to generate new images that follow the user’s design.&lt;/p&gt;



&lt;p&gt;Anthropic’s investment in Ember hints at the fact that developing interpretable models is difficult enough that Anthropic does not have the manpower to achieve interpretability on their own. Creative interpretable models requires new toolchains and skilled developers to build them&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-broader-context-an-ai-researcher-s-perspective"&gt;Broader context: An AI researcher’s perspective&lt;/h2&gt;



&lt;p&gt;To break down Amodei’s perspective and add much-needed context, VentureBeat interviewed Kapoor an AI safety researcher at Princeton. Kapoor co-authored the book &lt;em&gt;AI Snake Oil&lt;/em&gt;, a critical examination of exaggerated claims surrounding the capabilities of leading AI models. He is also a co-author of “&lt;em&gt;AI as Normal Technology&lt;/em&gt;,” in which he advocates for treating AI as a standard, transformational tool like the internet or electricity, and promotes a realistic perspective on its integration into everyday systems.&lt;/p&gt;



&lt;p&gt;Kapoor doesn’t dispute that interpretability is valuable. However, he’s skeptical of treating it as the central pillar of AI alignment. “It’s not a silver bullet,” Kapoor told VentureBeat. Many of the most effective safety techniques, such as post-response filtering, don’t require opening up the model at all, he said.&lt;/p&gt;



&lt;p&gt;He also warns against what researchers call the “fallacy of inscrutability” — the idea that if we don’t fully understand a system’s internals, we can’t use or regulate it responsibly. In practice, full transparency isn’t how most technologies are evaluated. What matters is whether a system performs reliably under real conditions.&lt;/p&gt;



&lt;p&gt;This isn’t the first time Amodei has warned about the risks of AI outpacing our understanding. In his October 2024 post, “Machines of Loving Grace,” he sketched out a vision of increasingly capable models that could take meaningful real-world actions (and maybe double our lifespans).&lt;/p&gt;



&lt;p&gt;According to Kapoor, there’s an important distinction to be made here between a model’s &lt;em&gt;capability&lt;/em&gt; and its &lt;em&gt;power&lt;/em&gt;. Model capabilities are undoubtedly increasing rapidly, and they may soon develop enough intelligence to find solutions for many complex problems challenging humanity today. But a model is only as powerful as the interfaces we provide it to interact with the real world, including where and how models are deployed.&lt;/p&gt;



&lt;p&gt;Amodei has separately argued that the U.S. should maintain a lead in AI development, in part through export controls that limit access to powerful models. The idea is that authoritarian governments might use frontier AI systems irresponsibly — or seize the geopolitical and economic edge that comes with deploying them first.&lt;/p&gt;



&lt;p&gt;For Kapoor, “Even the biggest proponents of export controls agree that it will give us at most a year or two.” He thinks we should treat AI as a “normal technology” like electricity or the internet. While revolutionary, it took decades for both technologies to be fully realized throughout society. Kapoor thinks it’s the same for AI: The best way to maintain geopolitical edge is to focus on the “long game” of transforming industries to use AI effectively.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-others-critiquing-amodei"&gt;Others critiquing Amodei&lt;/h2&gt;



&lt;p&gt;Kapoor isn’t the only one critiquing Amodei’s stance. Last week at VivaTech in Paris, Jansen Huang, CEO of Nvidia, declared his disagreement with Amodei’s views. Huang questioned whether the authority to develop AI should be limited to a few powerful entities like Anthropic. He said: “If you want things to be done safely and responsibly, you do it in the open … Don’t do it in a dark room and tell me it’s safe.” &lt;/p&gt;



&lt;p&gt;In response, Anthropic stated: “Dario has never claimed that ‘only Anthropic’ can build safe and powerful AI. As the public record will show, Dario has advocated for a national transparency standard for AI developers (including Anthropic) so the public and policymakers are aware of the models’ capabilities and risks and can prepare accordingly.”&lt;/p&gt;



&lt;p&gt;It’s also worth noting that Anthropic isn’t alone in its pursuit of interpretability: Google’s DeepMind interpretability team, led by Neel Nanda, has also made serious contributions to interpretability research.&lt;/p&gt;



&lt;p&gt;Ultimately, top AI labs and researchers are providing strong evidence that interpretability could be a key differentiator in the competitive AI market. Enterprises that prioritize interpretability early may gain a significant competitive edge by building more trusted, compliant, and adaptable AI systems.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Anthropic CEO Dario Amodei made an urgent push in April for the need to understand how AI models think.&lt;/p&gt;



&lt;p&gt;This comes at a crucial time. As Anthropic battles in global AI rankings, it’s important to note what sets it apart from other top AI labs. Since its founding in 2021, when seven OpenAI employees broke off over concerns about AI safety, Anthropic has built AI models that adhere to a set of human-valued principles, a system they call Constitutional AI. These principles ensure that models are “helpful, honest and harmless” and generally act in the best interests of society. At the same time, Anthropic’s research arm is diving deep to understand how its models think about the world, and &lt;em&gt;why&lt;/em&gt; they produce helpful (and sometimes harmful) answers.&lt;/p&gt;



&lt;p&gt;Anthropic’s flagship model, Claude 3.7 Sonnet, dominated coding benchmarks when it launched in February, proving that AI models can excel at both performance and safety. And the recent release of Claude 4.0 Opus and Sonnet again puts Claude at the top of coding benchmarks. However, in today’s rapid and hyper-competitive AI market, Anthropic’s rivals like Google’s Gemini 2.5 Pro and Open AI’s o3 have their own impressive showings for coding prowess, while they’re already dominating Claude at math, creative writing and overall reasoning across many languages.&lt;/p&gt;



&lt;p&gt;If Amodei’s thoughts are any indication, Anthropic is planning for the future of AI and its implications in critical fields like medicine, psychology and law, where model safety and human values are imperative. And it shows: Anthropic is the leading AI lab that focuses strictly on developing “interpretable” AI, which are models that let us understand, to some degree of certainty, what the model is thinking and how it arrives at a particular conclusion.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Amazon and Google have already invested billions of dollars in Anthropic even as they build their own AI models, so perhaps Anthropic’s competitive advantage is still budding. Interpretable models, as Anthropic suggests, could significantly reduce the long-term operational costs associated with debugging, auditing and mitigating risks in complex AI deployments.&lt;/p&gt;



&lt;p&gt;Sayash Kapoor, an AI safety researcher, suggests that while interpretability is valuable, it is just one of many tools for managing AI risk. In his view, “interpretability is neither necessary nor sufficient” to ensure models behave safely — it matters most when paired with filters, verifiers and human-centered design. This more expansive view sees interpretability as part of a larger ecosystem of control strategies, particularly in real-world AI deployments where models are components in broader decision-making systems.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-need-for-interpretable-ai"&gt;The need for interpretable AI&lt;/h2&gt;



&lt;p&gt;Until recently, many thought AI was still years from advancements like those that are now helping Claude, Gemini and ChatGPT boast exceptional market adoption. While these models are already pushing the frontiers of human knowledge, their widespread use is attributable to just how good they are at solving a wide range of practical problems that require creative problem-solving or detailed analysis. As models are put to the task on increasingly critical problems, it is important that they produce accurate answers.&lt;/p&gt;



&lt;p&gt;Amodei fears that when an AI responds to a prompt, “we have no idea… why it chooses certain words over others, or why it occasionally makes a mistake despite usually being accurate.” Such errors — hallucinations of inaccurate information, or responses that do not align with human values — will hold AI models back from reaching their full potential. Indeed, we’ve seen many examples of AI continuing to struggle with hallucinations and unethical behavior.&lt;/p&gt;



&lt;p&gt;For Amodei, the best way to solve these problems is to understand how an AI thinks: “Our inability to understand models’ internal mechanisms means that we cannot meaningfully predict such [harmful] behaviors, and therefore struggle to rule them out … If instead it were possible to look inside models, we might be able to systematically block all jailbreaks, and also characterize what dangerous knowledge the models have.”&lt;/p&gt;



&lt;p&gt;Amodei also sees the opacity of current models as a barrier to deploying AI models in “high-stakes financial or safety-critical settings, because we can’t fully set the limits on their behavior, and a small number of mistakes could be very harmful.” In decision-making that affects humans directly, like medical diagnosis or mortgage assessments, legal regulations require AI to explain its decisions.&lt;/p&gt;



&lt;p&gt;Imagine a financial institution using a large language model (LLM) for fraud detection — interpretability could mean explaining a denied loan application to a customer as required by law. Or a manufacturing firm optimizing supply chains — understanding why an AI suggests a particular supplier could unlock efficiencies and prevent unforeseen bottlenecks.&lt;/p&gt;



&lt;p&gt;Because of this, Amodei explains, “Anthropic is doubling down on interpretability, and we have a goal of getting to ‘interpretability can reliably detect most model problems’ by 2027.”&lt;/p&gt;



&lt;p&gt;To that end, Anthropic recently participated in a $50 million investment in Goodfire, an AI research lab making breakthrough progress on AI “brain scans.” Their model inspection platform, Ember, is an agnostic tool that identifies learned concepts within models and lets users manipulate them. In a recent demo, the company showed how Ember can recognize individual visual concepts within an image generation AI and then let users &lt;em&gt;paint&lt;/em&gt; these concepts on a canvas to generate new images that follow the user’s design.&lt;/p&gt;



&lt;p&gt;Anthropic’s investment in Ember hints at the fact that developing interpretable models is difficult enough that Anthropic does not have the manpower to achieve interpretability on their own. Creative interpretable models requires new toolchains and skilled developers to build them&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-broader-context-an-ai-researcher-s-perspective"&gt;Broader context: An AI researcher’s perspective&lt;/h2&gt;



&lt;p&gt;To break down Amodei’s perspective and add much-needed context, VentureBeat interviewed Kapoor an AI safety researcher at Princeton. Kapoor co-authored the book &lt;em&gt;AI Snake Oil&lt;/em&gt;, a critical examination of exaggerated claims surrounding the capabilities of leading AI models. He is also a co-author of “&lt;em&gt;AI as Normal Technology&lt;/em&gt;,” in which he advocates for treating AI as a standard, transformational tool like the internet or electricity, and promotes a realistic perspective on its integration into everyday systems.&lt;/p&gt;



&lt;p&gt;Kapoor doesn’t dispute that interpretability is valuable. However, he’s skeptical of treating it as the central pillar of AI alignment. “It’s not a silver bullet,” Kapoor told VentureBeat. Many of the most effective safety techniques, such as post-response filtering, don’t require opening up the model at all, he said.&lt;/p&gt;



&lt;p&gt;He also warns against what researchers call the “fallacy of inscrutability” — the idea that if we don’t fully understand a system’s internals, we can’t use or regulate it responsibly. In practice, full transparency isn’t how most technologies are evaluated. What matters is whether a system performs reliably under real conditions.&lt;/p&gt;



&lt;p&gt;This isn’t the first time Amodei has warned about the risks of AI outpacing our understanding. In his October 2024 post, “Machines of Loving Grace,” he sketched out a vision of increasingly capable models that could take meaningful real-world actions (and maybe double our lifespans).&lt;/p&gt;



&lt;p&gt;According to Kapoor, there’s an important distinction to be made here between a model’s &lt;em&gt;capability&lt;/em&gt; and its &lt;em&gt;power&lt;/em&gt;. Model capabilities are undoubtedly increasing rapidly, and they may soon develop enough intelligence to find solutions for many complex problems challenging humanity today. But a model is only as powerful as the interfaces we provide it to interact with the real world, including where and how models are deployed.&lt;/p&gt;



&lt;p&gt;Amodei has separately argued that the U.S. should maintain a lead in AI development, in part through export controls that limit access to powerful models. The idea is that authoritarian governments might use frontier AI systems irresponsibly — or seize the geopolitical and economic edge that comes with deploying them first.&lt;/p&gt;



&lt;p&gt;For Kapoor, “Even the biggest proponents of export controls agree that it will give us at most a year or two.” He thinks we should treat AI as a “normal technology” like electricity or the internet. While revolutionary, it took decades for both technologies to be fully realized throughout society. Kapoor thinks it’s the same for AI: The best way to maintain geopolitical edge is to focus on the “long game” of transforming industries to use AI effectively.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-others-critiquing-amodei"&gt;Others critiquing Amodei&lt;/h2&gt;



&lt;p&gt;Kapoor isn’t the only one critiquing Amodei’s stance. Last week at VivaTech in Paris, Jansen Huang, CEO of Nvidia, declared his disagreement with Amodei’s views. Huang questioned whether the authority to develop AI should be limited to a few powerful entities like Anthropic. He said: “If you want things to be done safely and responsibly, you do it in the open … Don’t do it in a dark room and tell me it’s safe.” &lt;/p&gt;



&lt;p&gt;In response, Anthropic stated: “Dario has never claimed that ‘only Anthropic’ can build safe and powerful AI. As the public record will show, Dario has advocated for a national transparency standard for AI developers (including Anthropic) so the public and policymakers are aware of the models’ capabilities and risks and can prepare accordingly.”&lt;/p&gt;



&lt;p&gt;It’s also worth noting that Anthropic isn’t alone in its pursuit of interpretability: Google’s DeepMind interpretability team, led by Neel Nanda, has also made serious contributions to interpretability research.&lt;/p&gt;



&lt;p&gt;Ultimately, top AI labs and researchers are providing strong evidence that interpretability could be a key differentiator in the competitive AI market. Enterprises that prioritize interpretability early may gain a significant competitive edge by building more trusted, compliant, and adaptable AI systems.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/the-interpretable-ai-playbook-what-anthropics-research-means-for-your-enterprise-llm-strategy/</guid><pubDate>Tue, 17 Jun 2025 23:01:08 +0000</pubDate></item><item><title>[NEW] Police shut down Cluely’s party, the ‘cheat at everything’ startup (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/17/police-shut-down-cluelys-party-the-cheat-at-everything-startup/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Cluely-party-video.png?resize=1200,772" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The latest San Francisco startup culture drama happened on Monday night. And it centered around “the most legendary party that never happened,” Cluely founder and CEO Roy Lee tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cluely had hoped to throw an after-party for a Y Combinator event occurring on Monday and Tuesday called AI Startup School. The event drew crowds thanks to scheduled speakers like Sam Altman, Satya Nadella, and Elon Musk.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cluely is an AI startup born of controversy and rage-bait comedy marketing. True to form, Lee posted a satirical video on X advertising his after-party. It shows him camped out by the famed Y Combinator sign — the one all the YC founders take selfies with. (Cluely is not a YC startup.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tweet advertised the party to his more than 100,000 followers and said to DM for an invite. Lee tells TechCrunch that he didn’t actually send invites out to the hordes. “We only invited friends and friends of friends,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it became&lt;em&gt; the&lt;/em&gt; party, and people shared the details. When it was set to begin, so many people were standing outside the venue that the lines wrapped around blocks. “It just blew up way out of proportion,” Lee says. What looked like 2,000 people showed up, he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A party that big might have gotten out of control, but it didn’t get the chance. The lines were blocking traffic, so the cops showed up and shut it down. “Cluely’s aura is just too strong!”&amp;nbsp;Lee was heard shouting outside as the cops busted it up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would have been the most legendary party in tech history. And I would argue that the reputation of this story might just make it the most legendary party that never happened,” Lee tells TechCrunch, simultaneously proud and bummed.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Lee became known in San Francisco when he posted a viral tweet on X saying he was suspended by Columbia University after he and his co-founder developed an AI tool to cheat on job interviews for software engineers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They turned that tool into a startup that offers a hidden in-browser window that can’t be viewed by an interviewer or proctor. The startup also went viral for its marketing that promised to help people “cheat on everything.” In April, Cluely raised a $5.3 million seed round, and its marketing is now a little less in-your-face: “Everything you need. Before you ask.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The party and its demise by law enforcement naturally became the subject of jokes, memes, and inventive rumors. Lee’s explanation of the crowds outside is perhaps more dull than what some people imagined.&amp;nbsp;After the cops showed, “We did some cleanup, but the drinks are all there waiting for the next party,” he promises.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/06/Cluely-party-video.png?resize=1200,772" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The latest San Francisco startup culture drama happened on Monday night. And it centered around “the most legendary party that never happened,” Cluely founder and CEO Roy Lee tells TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Cluely had hoped to throw an after-party for a Y Combinator event occurring on Monday and Tuesday called AI Startup School. The event drew crowds thanks to scheduled speakers like Sam Altman, Satya Nadella, and Elon Musk.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Cluely is an AI startup born of controversy and rage-bait comedy marketing. True to form, Lee posted a satirical video on X advertising his after-party. It shows him camped out by the famed Y Combinator sign — the one all the YC founders take selfies with. (Cluely is not a YC startup.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tweet advertised the party to his more than 100,000 followers and said to DM for an invite. Lee tells TechCrunch that he didn’t actually send invites out to the hordes. “We only invited friends and friends of friends,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it became&lt;em&gt; the&lt;/em&gt; party, and people shared the details. When it was set to begin, so many people were standing outside the venue that the lines wrapped around blocks. “It just blew up way out of proportion,” Lee says. What looked like 2,000 people showed up, he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A party that big might have gotten out of control, but it didn’t get the chance. The lines were blocking traffic, so the cops showed up and shut it down. “Cluely’s aura is just too strong!”&amp;nbsp;Lee was heard shouting outside as the cops busted it up.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It would have been the most legendary party in tech history. And I would argue that the reputation of this story might just make it the most legendary party that never happened,” Lee tells TechCrunch, simultaneously proud and bummed.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Lee became known in San Francisco when he posted a viral tweet on X saying he was suspended by Columbia University after he and his co-founder developed an AI tool to cheat on job interviews for software engineers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;They turned that tool into a startup that offers a hidden in-browser window that can’t be viewed by an interviewer or proctor. The startup also went viral for its marketing that promised to help people “cheat on everything.” In April, Cluely raised a $5.3 million seed round, and its marketing is now a little less in-your-face: “Everything you need. Before you ask.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The party and its demise by law enforcement naturally became the subject of jokes, memes, and inventive rumors. Lee’s explanation of the crowds outside is perhaps more dull than what some people imagined.&amp;nbsp;After the cops showed, “We did some cleanup, but the drinks are all there waiting for the next party,” he promises.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/17/police-shut-down-cluelys-party-the-cheat-at-everything-startup/</guid><pubDate>Tue, 17 Jun 2025 23:09:59 +0000</pubDate></item></channel></rss>