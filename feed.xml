<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 12 Nov 2025 18:30:39 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>The global race for the AI app layer is still on (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/11/the-global-race-for-the-ai-app-layer-is-still-on/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The U.S. is far ahead of Europe in the race for large AI models — but the picture is different for the application layer, with emerging category leaders such as Lovable, the vibe-coding startup, and Synthesia, which makes AI-generated video for the enterprise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s the conclusion made by global VC firm Accel in its 2025 Globalscape report, which focuses on the AI and cloud market. Worth noting: Accel is an investor in both Lovable and Synthesia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Accel, European and Israeli cloud and AI applications have raised 66 cents for every dollar raised by their American counterparts in 2025 so far. “When we started this report 10 years ago, Europe was one tenth of the U.S.,” Accel partner Philippe Botteri told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066813" height="382" src="https://techcrunch.com/wp-content/uploads/2025/11/Accel-2025-Globalscape-slide.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Courtesy of Accel&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Botteri says the ratio has increased because the region has developed an ecosystem of founders and investors “who really understand how to build great software companies, and that flywheel has been running for 10 years.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also a reminder that Europeans and Israelis can do more than just staff Big Tech AI labs — an observation also shared by Jonathan Userovici, a Paris-based general partner at Headline. “Across every vertical, from legal and healthcare to manufacturing and marketing, we’re seeing founders who combine world-class technical talent with deep market expertise,” Userovici told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This aligns with the findings of the AI Europe 100 report published by Headline earlier this year, which curated AI-native application startups around Europe that the firm sees as having “the potential to become tomorrow’s winners in Europe” thanks to a combination of growth velocity, team, and tech advancement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That growth velocity is one of the key differences that Accel sees between this AI wave and previous ones. A new breed of AI-native applications has reached $100 million in annual recurring revenue in a matter of years, a feat that used to take decades.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“They’re growing faster than anything we’ve seen in the past, and they’re doing this with an incredible level of efficiency, meaning that revenue per headcount is the highest we’ve ever seen for software companies,” Botteri said. “And that’s happening on both sides of the [Atlantic] ocean.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, he noted that “existing cloud software companies are not going away.” Accel’s Public Cloud Index is up 25% year-over-year, and these players are “all adding agentic capabilities to their products.” As for private companies, some are integrating AI so deeply that they can be considered AI-native, he argued, naming Accel portfolio company Doctolib as an example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Europe has kept high hopes for homegrown foundation model companies like Mistral AI, Accel’s outlook for European model companies is less sunny. But Botteri didn’t dismiss it entirely as a space for future leaders to emerge. It could still happen for smaller models but “it is not a very target-rich environment,” he added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In contrast, VCs are actively competing for investment opportunities in the AI application layer, despite recurring questions about defensibility. According to Botteri, there is still defensibility in building a product-centric offering with fast adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another false dichotomy is the thought that there is no space outside of models and applications. “We see that most of the market today is chasing models, compute and applications, and we think that data is undervalued at the moment,” said Lotan Levkowitz, a managing partner at Israeli VC firm Grove Ventures. “We strongly believe that companies focused on proprietary data and data flywheels are indeed very lucrative.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The U.S. is far ahead of Europe in the race for large AI models — but the picture is different for the application layer, with emerging category leaders such as Lovable, the vibe-coding startup, and Synthesia, which makes AI-generated video for the enterprise. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That’s the conclusion made by global VC firm Accel in its 2025 Globalscape report, which focuses on the AI and cloud market. Worth noting: Accel is an investor in both Lovable and Synthesia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Accel, European and Israeli cloud and AI applications have raised 66 cents for every dollar raised by their American counterparts in 2025 so far. “When we started this report 10 years ago, Europe was one tenth of the U.S.,” Accel partner Philippe Botteri told TechCrunch.&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066813" height="382" src="https://techcrunch.com/wp-content/uploads/2025/11/Accel-2025-Globalscape-slide.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Courtesy of Accel&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Botteri says the ratio has increased because the region has developed an ecosystem of founders and investors “who really understand how to build great software companies, and that flywheel has been running for 10 years.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s also a reminder that Europeans and Israelis can do more than just staff Big Tech AI labs — an observation also shared by Jonathan Userovici, a Paris-based general partner at Headline. “Across every vertical, from legal and healthcare to manufacturing and marketing, we’re seeing founders who combine world-class technical talent with deep market expertise,” Userovici told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This aligns with the findings of the AI Europe 100 report published by Headline earlier this year, which curated AI-native application startups around Europe that the firm sees as having “the potential to become tomorrow’s winners in Europe” thanks to a combination of growth velocity, team, and tech advancement.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That growth velocity is one of the key differences that Accel sees between this AI wave and previous ones. A new breed of AI-native applications has reached $100 million in annual recurring revenue in a matter of years, a feat that used to take decades.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“They’re growing faster than anything we’ve seen in the past, and they’re doing this with an incredible level of efficiency, meaning that revenue per headcount is the highest we’ve ever seen for software companies,” Botteri said. “And that’s happening on both sides of the [Atlantic] ocean.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, he noted that “existing cloud software companies are not going away.” Accel’s Public Cloud Index is up 25% year-over-year, and these players are “all adding agentic capabilities to their products.” As for private companies, some are integrating AI so deeply that they can be considered AI-native, he argued, naming Accel portfolio company Doctolib as an example.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Europe has kept high hopes for homegrown foundation model companies like Mistral AI, Accel’s outlook for European model companies is less sunny. But Botteri didn’t dismiss it entirely as a space for future leaders to emerge. It could still happen for smaller models but “it is not a very target-rich environment,” he added.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In contrast, VCs are actively competing for investment opportunities in the AI application layer, despite recurring questions about defensibility. According to Botteri, there is still defensibility in building a product-centric offering with fast adoption.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another false dichotomy is the thought that there is no space outside of models and applications. “We see that most of the market today is chasing models, compute and applications, and we think that data is undervalued at the moment,” said Lotan Levkowitz, a managing partner at Israeli VC firm Grove Ventures. “We strongly believe that companies focused on proprietary data and data flywheels are indeed very lucrative.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/11/the-global-race-for-the-ai-app-layer-is-still-on/</guid><pubDate>Wed, 12 Nov 2025 07:01:00 +0000</pubDate></item><item><title>Google reveals its own version of Apple’s AI cloud (AI News)</title><link>https://www.artificialintelligence-news.com/news/google-reveals-its-own-version-of-apple-ai-cloud/</link><description>&lt;p&gt;Google has rolled out Private AI Compute, a new cloud-based processing system designed to bring the privacy of on-device AI to the cloud. The platform aims to give users faster, more capable AI experiences without compromising data security. It combines Google’s most advanced Gemini models with strict privacy safeguards, reflecting the company’s ongoing effort to make AI both powerful and responsible.&lt;/p&gt;&lt;p&gt;The feature closely resembles Apple’s Private Cloud Compute, signalling how major tech firms are rethinking privacy in the age of large-scale AI. Both companies are trying to balance two competing needs — the huge computing power required to run advanced AI models and users’ expectations for data privacy.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-google-built-private-ai-compute"&gt;Why Google built Private AI Compute&lt;/h3&gt;&lt;p&gt;As AI systems get smarter, they’re also becoming more personal. What started as tools that completed simple tasks or answered direct questions are now systems that can anticipate user needs, suggest actions, and handle complex processes in real time. That kind of intelligence demands a level of reasoning and computation that often exceeds what’s possible on a single device.&lt;/p&gt;&lt;p&gt;Private AI Compute bridges that gap. It lets Gemini models in the cloud process data faster and more efficiently while ensuring that sensitive information remains private and inaccessible to anyone else — not even Google engineers. Google describes it as combining the power of cloud AI with the security users expect from local processing.&lt;/p&gt;&lt;p&gt;In practical terms, this means you could get quicker responses, smarter suggestions, and more personalised results without your personal data ever leaving your control.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-private-ai-compute-keeps-data-secure"&gt;How Private AI Compute keeps data secure&lt;/h3&gt;&lt;p&gt;Google claims the new platform is based on the same principles that underpin its broader AI and privacy strategy: giving users control, maintaining security, and earning trust. The system acts as a protected computing environment, isolating data so it can be processed safely and privately.&lt;/p&gt;&lt;p&gt;It uses a multi-layered design centred on three key components:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Unified Google tech stack:&lt;/strong&gt; Private AI Compute runs entirely on Google’s own infrastructure, powered by custom Tensor Processing Units (TPUs). It’s secured through Titanium Intelligence Enclaves (TIE), which create an additional layer of protection for data processed in the cloud.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Encrypted connections:&lt;/strong&gt; Before data is sent for processing, remote attestation and encryption verify that it’s connecting to a trusted, hardware-secured environment. Once inside this sealed cloud space, information stays private to the user.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Zero access assurance:&lt;/strong&gt; Google says the system is designed so that no one — not even the company itself — can access the data processed within Private AI Compute.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This design builds on Google’s Secure AI Framework (SAIF), AI Principles, and Privacy Principles, which outline how the company develops and deploys AI responsibly.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-users-can-expect"&gt;What users can expect&lt;/h3&gt;&lt;p&gt;Private AI Compute also improves the performance of AI features that are already running on devices. Magic Cue on the Pixel 10 can now offer more relevant and timely suggestions by leveraging cloud-level processing power. Similarly, the Recorder app can use the system to summarise transcriptions across a wider range of languages — something that would be difficult to do entirely on-device.&lt;/p&gt;&lt;p&gt;These examples hint at what’s ahead. With Private AI Compute, Google can deliver AI experiences that combine the privacy of local models with the intelligence of cloud-based ones. It’s an approach that could eventually apply to everything from personal assistants and photo organisation to productivity and accessibility tools.&lt;/p&gt;&lt;p&gt;Google calls this launch “just the beginning.” The company says Private AI Compute opens the door to a new generation of AI tools that are both more capable and more private. As AI becomes increasingly woven into everyday tasks, users are demanding greater transparency and control over how their data is used — and Google appears to be positioning this technology as part of that answer.&lt;/p&gt;&lt;p&gt;For those interested in the technical details, Google has published a technical brief explaining how Private AI Compute works and how it fits into the company’s larger vision for responsible AI development.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Solen Feyissa)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple plans big Siri update with help from Google AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110522" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Google has rolled out Private AI Compute, a new cloud-based processing system designed to bring the privacy of on-device AI to the cloud. The platform aims to give users faster, more capable AI experiences without compromising data security. It combines Google’s most advanced Gemini models with strict privacy safeguards, reflecting the company’s ongoing effort to make AI both powerful and responsible.&lt;/p&gt;&lt;p&gt;The feature closely resembles Apple’s Private Cloud Compute, signalling how major tech firms are rethinking privacy in the age of large-scale AI. Both companies are trying to balance two competing needs — the huge computing power required to run advanced AI models and users’ expectations for data privacy.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-why-google-built-private-ai-compute"&gt;Why Google built Private AI Compute&lt;/h3&gt;&lt;p&gt;As AI systems get smarter, they’re also becoming more personal. What started as tools that completed simple tasks or answered direct questions are now systems that can anticipate user needs, suggest actions, and handle complex processes in real time. That kind of intelligence demands a level of reasoning and computation that often exceeds what’s possible on a single device.&lt;/p&gt;&lt;p&gt;Private AI Compute bridges that gap. It lets Gemini models in the cloud process data faster and more efficiently while ensuring that sensitive information remains private and inaccessible to anyone else — not even Google engineers. Google describes it as combining the power of cloud AI with the security users expect from local processing.&lt;/p&gt;&lt;p&gt;In practical terms, this means you could get quicker responses, smarter suggestions, and more personalised results without your personal data ever leaving your control.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-how-private-ai-compute-keeps-data-secure"&gt;How Private AI Compute keeps data secure&lt;/h3&gt;&lt;p&gt;Google claims the new platform is based on the same principles that underpin its broader AI and privacy strategy: giving users control, maintaining security, and earning trust. The system acts as a protected computing environment, isolating data so it can be processed safely and privately.&lt;/p&gt;&lt;p&gt;It uses a multi-layered design centred on three key components:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;&lt;strong&gt;Unified Google tech stack:&lt;/strong&gt; Private AI Compute runs entirely on Google’s own infrastructure, powered by custom Tensor Processing Units (TPUs). It’s secured through Titanium Intelligence Enclaves (TIE), which create an additional layer of protection for data processed in the cloud.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Encrypted connections:&lt;/strong&gt; Before data is sent for processing, remote attestation and encryption verify that it’s connecting to a trusted, hardware-secured environment. Once inside this sealed cloud space, information stays private to the user.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Zero access assurance:&lt;/strong&gt; Google says the system is designed so that no one — not even the company itself — can access the data processed within Private AI Compute.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This design builds on Google’s Secure AI Framework (SAIF), AI Principles, and Privacy Principles, which outline how the company develops and deploys AI responsibly.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-users-can-expect"&gt;What users can expect&lt;/h3&gt;&lt;p&gt;Private AI Compute also improves the performance of AI features that are already running on devices. Magic Cue on the Pixel 10 can now offer more relevant and timely suggestions by leveraging cloud-level processing power. Similarly, the Recorder app can use the system to summarise transcriptions across a wider range of languages — something that would be difficult to do entirely on-device.&lt;/p&gt;&lt;p&gt;These examples hint at what’s ahead. With Private AI Compute, Google can deliver AI experiences that combine the privacy of local models with the intelligence of cloud-based ones. It’s an approach that could eventually apply to everything from personal assistants and photo organisation to productivity and accessibility tools.&lt;/p&gt;&lt;p&gt;Google calls this launch “just the beginning.” The company says Private AI Compute opens the door to a new generation of AI tools that are both more capable and more private. As AI becomes increasingly woven into everyday tasks, users are demanding greater transparency and control over how their data is used — and Google appears to be positioning this technology as part of that answer.&lt;/p&gt;&lt;p&gt;For those interested in the technical details, Google has published a technical brief explaining how Private AI Compute works and how it fits into the company’s larger vision for responsible AI development.&lt;/p&gt;&lt;p&gt;&lt;em&gt;(Photo by Solen Feyissa)&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: Apple plans big Siri update with help from Google AI&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-110522" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-3.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/google-reveals-its-own-version-of-apple-ai-cloud/</guid><pubDate>Wed, 12 Nov 2025 09:00:00 +0000</pubDate></item><item><title>Figma bets on India to expand beyond design (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/figma-bets-on-india-to-expand-beyond-design/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Figma is expanding its presence in India by setting up a local office and hiring Indian talent as it seeks to deepen ties with one of its largest user communities and make a broader push to better win over developers alongside the designers who already rely on the platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2012 by Dylan Field and Evan Wallace, Figma broke through by offering a browser-based interface at a time when most designers were still tied to desktop software. The approach was initially met with skepticism, but the platform eventually became a go-to collaboration tool for UX and product teams. Now, the company is looking to replicate that trajectory with developers — and sees India as a key market to accelerate that evolution.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India has one of the world’s largest developer communities — an advantage already recognized by tech giants such as Microsoft, which counts nearly 22 million Indian developers on GitHub. As much as 33% of Figma’s users globally are developers, and the company has been rolling out features aimed at bridging design and engineering workflows. However, Figma still faces a perception challenge: many Indian developers continue to see Figma primarily as a design tool rather than a platform for end-to-end product creation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has such a large population of developers who might not currently think of Figma as their tool, and that’s the thing that we want to do,” said Abhishek Mathur, VP of Engineering at Figma, in an interview. “A lot of it is being done by the community, but we want to be part of that activity as well — and share our story of enabling developers to be more than just writing code.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, Figma opened a new office in Bengaluru, India, as part of its continuing expansion outside the U.S. The San Francisco-headquartered company already has offices in Tokyo, Singapore, London, Paris, Berlin, Sydney, and São Paulo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Figma had been supporting users in India remotely through its Singapore team. The company now recognizes the value of establishing a local presence, as its user base and community activity in the country have continued to expand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has always been a global hub of innovation, and particularly, for Figma, international markets are a big part of usage,” Mathur told TechCrunch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As much as 85% of Figma’s overall usage is international, and India is its second-largest user base after the U.S., Mathur noted. The company said it was serving users in 85% of India’s 28 official states as of the third quarter of 2025. As of September, more than 40% of the top 100 companies listed on the Bombay Stock Exchange were Figma customers, it added.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figma's users in Indian states" class="wp-image-3066895" height="1421" src="https://techcrunch.com/wp-content/uploads/2025/11/figma-india-users-map_51bb09.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Figma&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Figma counts 13 million weekly active users worldwide. The company did not share specific user numbers for India, though Mathur described the country as “a very large portion” of its base. Its India community, called Friends of Figma, alone includes more than 25,000 members.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, Figma introduced a new range of AI-powered features designed to extend its software’s value beyond design teams, positioning it in competition not just with Adobe and Canva, but also with AI coding platforms such as Replit and Lovable. One of those features, Figma Make, allows users to generate working web applications from natural-language prompts and collaborate on both design and code within the same workspace.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mathur said India has been the largest market for Figma Make, with users in the country generating over 800,000 prototypes so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma also sees increased adoption among developers in India, particularly for its dev mode, which debuted in 2023 to help developers quickly translate designs into code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The first spectrum of imagination to production is what we are seeing in terms of differences between India and the rest of the globe,” Mathur said. “The usage patterns are similar, but the scale of operations in some of the things is very challenging.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma’s Bengaluru office will initially focus on strengthening the company’s sales and marketing operations in the country. Its users in India include consumer-facing startups such as CRED, Groww, Fynd, Swiggy, and Zomato, as well as IT services giants including Infosys and TCS and consumer companies such as Airtel, CARS24, and Myntra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2024, Figma generated about half of its revenue from markets outside the U.S., and Mathur described India as an “important market” for the company, though he did not disclose its specific contribution to global revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s user base is already influencing Figma’s product development. For instance, feedback from its community in India led the company to introduce improved code-export options that produce higher-quality code — a direct response to requests from Indian users seeking better output.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to continue to do events, understand and work with our customers — small to large — and as time progresses, we might add other possibilities as well,” Mathur said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Figma is expanding its presence in India by setting up a local office and hiring Indian talent as it seeks to deepen ties with one of its largest user communities and make a broader push to better win over developers alongside the designers who already rely on the platform.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founded in 2012 by Dylan Field and Evan Wallace, Figma broke through by offering a browser-based interface at a time when most designers were still tied to desktop software. The approach was initially met with skepticism, but the platform eventually became a go-to collaboration tool for UX and product teams. Now, the company is looking to replicate that trajectory with developers — and sees India as a key market to accelerate that evolution.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;India has one of the world’s largest developer communities — an advantage already recognized by tech giants such as Microsoft, which counts nearly 22 million Indian developers on GitHub. As much as 33% of Figma’s users globally are developers, and the company has been rolling out features aimed at bridging design and engineering workflows. However, Figma still faces a perception challenge: many Indian developers continue to see Figma primarily as a design tool rather than a platform for end-to-end product creation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has such a large population of developers who might not currently think of Figma as their tool, and that’s the thing that we want to do,” said Abhishek Mathur, VP of Engineering at Figma, in an interview. “A lot of it is being done by the community, but we want to be part of that activity as well — and share our story of enabling developers to be more than just writing code.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;On Wednesday, Figma opened a new office in Bengaluru, India, as part of its continuing expansion outside the U.S. The San Francisco-headquartered company already has offices in Tokyo, Singapore, London, Paris, Berlin, Sydney, and São Paulo.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Until now, Figma had been supporting users in India remotely through its Singapore team. The company now recognizes the value of establishing a local presence, as its user base and community activity in the country have continued to expand.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“India has always been a global hub of innovation, and particularly, for Figma, international markets are a big part of usage,” Mathur told TechCrunch.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As much as 85% of Figma’s overall usage is international, and India is its second-largest user base after the U.S., Mathur noted. The company said it was serving users in 85% of India’s 28 official states as of the third quarter of 2025. As of September, more than 40% of the top 100 companies listed on the Bombay Stock Exchange were Figma customers, it added.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Figma's users in Indian states" class="wp-image-3066895" height="1421" src="https://techcrunch.com/wp-content/uploads/2025/11/figma-india-users-map_51bb09.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Figma&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Figma counts 13 million weekly active users worldwide. The company did not share specific user numbers for India, though Mathur described the country as “a very large portion” of its base. Its India community, called Friends of Figma, alone includes more than 25,000 members.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In May, Figma introduced a new range of AI-powered features designed to extend its software’s value beyond design teams, positioning it in competition not just with Adobe and Canva, but also with AI coding platforms such as Replit and Lovable. One of those features, Figma Make, allows users to generate working web applications from natural-language prompts and collaborate on both design and code within the same workspace.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Mathur said India has been the largest market for Figma Make, with users in the country generating over 800,000 prototypes so far.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma also sees increased adoption among developers in India, particularly for its dev mode, which debuted in 2023 to help developers quickly translate designs into code.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The first spectrum of imagination to production is what we are seeing in terms of differences between India and the rest of the globe,” Mathur said. “The usage patterns are similar, but the scale of operations in some of the things is very challenging.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Figma’s Bengaluru office will initially focus on strengthening the company’s sales and marketing operations in the country. Its users in India include consumer-facing startups such as CRED, Groww, Fynd, Swiggy, and Zomato, as well as IT services giants including Infosys and TCS and consumer companies such as Airtel, CARS24, and Myntra.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In 2024, Figma generated about half of its revenue from markets outside the U.S., and Mathur described India as an “important market” for the company, though he did not disclose its specific contribution to global revenue.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s user base is already influencing Figma’s product development. For instance, feedback from its community in India led the company to introduce improved code-export options that produce higher-quality code — a direct response to requests from Indian users seeking better output.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We want to continue to do events, understand and work with our customers — small to large — and as time progresses, we might add other possibilities as well,” Mathur said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/figma-bets-on-india-to-expand-beyond-design/</guid><pubDate>Wed, 12 Nov 2025 10:00:00 +0000</pubDate></item><item><title>Improving VMware migration workflows with agentic AI (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/12/1124919/improving-vmware-migration-workflows-with-agentic-ai/</link><description>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;EPAM&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For years, many chief information officers (CIOs) looked at VMware-to-cloud migrations with a wary pragmatism. Manually mapping dependencies and rewriting legacy apps mid-flight was not an enticing, low-lift proposition for enterprise IT teams.&lt;/p&gt;  &lt;p&gt;But the calculus for such decisions has changed dramatically in a short period of time. Following recent VMware licensing changes, organizations are seeing greater uncertainty around the platform’s future. At the same time, cloud-native innovation is accelerating. According to the &lt;strong&gt;CNCF’s 2024 Annual Survey,&lt;/strong&gt; 89% of organizations have already adopted at least some cloud-native techniques, and the share of companies reporting nearly all development and deployment as cloud-native grew sharply from 2023 to 2024 (20% to 24%). And market research firm &lt;strong&gt;IDC reports&lt;/strong&gt; that cloud providers have become top strategic partners for generative AI initiatives.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1124925" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/EPAM-Landing-Page-Card-1.png" /&gt;&lt;/figure&gt;    &lt;p&gt;This is all happening amid escalating pressure to innovate faster and more cost-effectively to meet the demands of an AI-first future. As enterprises prepare for that inevitability, they are facing compute demands that are difficult, if not prohibitively expensive, to maintain exclusively on-premises.&lt;/p&gt;  &lt;p&gt;Download the full article.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;section class="sponsoredModule__wrapper--c8f6fcf4edb2dcd3a940a2824bb850dc sponsoredModule__minimalist--f63b84a37007076f51d0ebb0dc1af42f"&gt;&lt;p class="sponsoredModule__intro--e69c514244f1e38617e4ec5ea754fb7f"&gt;&lt;span&gt;In partnership with&lt;/span&gt;EPAM&lt;/p&gt;&lt;span class="image__wrapper--373a87c0cefdc42b3a8bd26457571412"&gt;&lt;span class=" lazy-load-image-background opacity"&gt;&lt;span class="image__img--e1a73f503bf0f4a3d2504e1d64ea29cb imgLazyLoaded"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;figcaption class="image__meta--16eb0f8dde685315ba1d77ae67c89391"&gt;&lt;/figcaption&gt;&lt;/section&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For years, many chief information officers (CIOs) looked at VMware-to-cloud migrations with a wary pragmatism. Manually mapping dependencies and rewriting legacy apps mid-flight was not an enticing, low-lift proposition for enterprise IT teams.&lt;/p&gt;  &lt;p&gt;But the calculus for such decisions has changed dramatically in a short period of time. Following recent VMware licensing changes, organizations are seeing greater uncertainty around the platform’s future. At the same time, cloud-native innovation is accelerating. According to the &lt;strong&gt;CNCF’s 2024 Annual Survey,&lt;/strong&gt; 89% of organizations have already adopted at least some cloud-native techniques, and the share of companies reporting nearly all development and deployment as cloud-native grew sharply from 2023 to 2024 (20% to 24%). And market research firm &lt;strong&gt;IDC reports&lt;/strong&gt; that cloud providers have become top strategic partners for generative AI initiatives.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1124925" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/EPAM-Landing-Page-Card-1.png" /&gt;&lt;/figure&gt;    &lt;p&gt;This is all happening amid escalating pressure to innovate faster and more cost-effectively to meet the demands of an AI-first future. As enterprises prepare for that inevitability, they are facing compute demands that are difficult, if not prohibitively expensive, to maintain exclusively on-premises.&lt;/p&gt;  &lt;p&gt;Download the full article.&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This content was researched, designed, and written by human writers, editors, analysts, and illustrators. This includes the writing of surveys and collection of data for surveys. AI tools that may have been used were limited to secondary production processes that passed thorough human review.&lt;/em&gt;&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/12/1124919/improving-vmware-migration-workflows-with-agentic-ai/</guid><pubDate>Wed, 12 Nov 2025 10:11:15 +0000</pubDate></item><item><title>MMCTAgent: Enabling multimodal reasoning over large video and image collections (Microsoft Research)</title><link>https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/</link><description>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-purple gradient background: the first icon shows an image/photo; the second icon depicts a computer monitor with vertical bars; the third icon displays three connected circles with user silhouettes." class="wp-image-1153930" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MMCTAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Modern multimodal AI models can recognize objects, describe scenes, and answer questions about images and short video clips, but they struggle with long-form and large-scale visual data, where real-world reasoning requires moving beyond object recognition and short-clip analysis.&lt;/p&gt;



&lt;p&gt;Real-world reasoning increasingly involves analyzing long-form video content, where context spans minutes or hours, far beyond the context limits of most models. It also entails querying across massive multimodal libraries of videos, images, and transcripts, where finding and integrating relevant evidence requires more than retrieval—it requires strategic reasoning. Existing models typically perform single-pass inference, producing one-shot answers. This limits their ability to handle tasks that require temporal reasoning, cross-modal grounding, and iterative refinement.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="mmctagent"&gt;MMCTAgent&lt;/h2&gt;



&lt;p&gt;To meet these challenges, we developed the Multi-modal Critical Thinking Agent, or MMCTAgent, for structured reasoning over long-form video and image data, available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and featured on Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;Built on AutoGen, Microsoft’s open-source multi-agent system, MMCTAgent provides multimodal question-answering with a Planner–Critic architecture. This design enables planning, reflection, and tool-based reasoning, bridging perception and deliberation in multimodal tasks. It links language, vision, and temporal understanding, transforming static multimodal tasks into dynamic reasoning workflows.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Unlike conventional models that produce one-shot answers, MMCTAgent has modality-specific agents, including ImageAgent and VideoAgent, which include tools like get_relevant_query_frames() or object_detection-tool(). These agents perform&amp;nbsp;deliberate, iterative reasoning—selecting the right tools for each modality, evaluating intermediate results, and refining conclusions through a Critic loop. This enables MMCTAgent to analyze complex queries across long videos and large image libraries with explainability, extensibility, and scalability.&lt;/p&gt;







	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;The AI Revolution in Medicine, Revisited&lt;/h2&gt;
				
								&lt;p class="large" id="the-ai-revolution-in-medicine-revisited"&gt;Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="how-mmctagent-works"&gt;How MMCTAgent works&lt;/h2&gt;



&lt;p&gt;MMCTAgent integrates two coordinated agents, Planner and Critic, orchestrated through AutoGen. The Planner agent decomposes a user query, identifies the appropriate reasoning tools, performs multimodal operations, and drafts a preliminary answer. The Critic agent reviews the Planner’s reasoning chain, validates evidence alignment, and refines or revises the response for factual accuracy and consistency.&lt;/p&gt;



&lt;p&gt;This iterative reasoning loop enables MMCTAgent to improve its answers through structured self-evaluation—bringing reflection into AI reasoning. A key strength of MMCTAgent lies in its modular extensibility. Developers can easily integrate new, domain-specific tools—such as medical image analyzers, industrial inspection models, or specialized retrieval modules—by adding them to ImageQnATools or VideoQnATools. This design makes MMCTAgent adaptable across domains.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="videoagent-from-ingestion-to-long-form-multimodal-reasoning"&gt;VideoAgent: From ingestion to long-form multimodal reasoning&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback.&amp;nbsp;" class="wp-image-1155366" height="8455" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/MMCT_UPDATED_FINAL_FINAL.png" width="14353" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The VideoAgent extends this architecture to long-form video reasoning. It operates in two connected phases: library creation (ingestion) and query-time reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-1-video-ingestion-and-library-creation"&gt;Phase 1 – Video ingestion and library creation&lt;/h4&gt;



&lt;p&gt;Before reasoning, long-form videos undergo an ingestion pipeline that aligns multimodal information for retrieval and understanding:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Transcription &lt;/strong&gt;and&lt;strong&gt; translation&lt;/strong&gt;: Converts audio to text and, if multilingual, translates transcripts into a consistent language&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Key-frame identification&lt;/strong&gt;: Extracts representative frames marking major visual or scene changes&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Semantic chunking &lt;/strong&gt;and&lt;strong&gt; chapter generation&lt;/strong&gt;: Combines transcript segments and visual summaries into coherent, semantically segmented chapters with associated key frames. Inspired by Microsoft’s Deep Video Discovery agentic search tool, this step also extracts detailed descriptions of objects, on-screen text, and characters present within each video segment, integrating these insights directly into the corresponding chapters.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multimodal embedding creation&lt;/strong&gt;: Generates image embeddings for key frames, linking them to their corresponding transcript and chapter data&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;All structured metadata, including transcripts, visual summaries, chapters, and embeddings, is indexed in the Multimodal Knowledgebase using Azure AI Search&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which forms the foundation for scalable semantic retrieval and downstream reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-2-video-question-answering-and-reasoning"&gt;Phase 2 – Video question answering and reasoning&lt;/h4&gt;



&lt;p&gt;When a user submits a query, the VideoAgent retrieves, analyzes, and reasons across the indexed video content using specialized planner and critic tools.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools-1"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;get_video_analysis&lt;/strong&gt;: Finds the most relevant video, provides a summary, and lists detected objects&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt;: Retrieves contextual information and relevant chapters from the Azure AI Search index&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_relevant_frames&lt;/strong&gt;: Selects key frames most relevant to the user query&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;query_frame&lt;/strong&gt;: Performs detailed visual and textual reasoning over selected frames&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt; and &lt;strong&gt;get_relevant_frames&lt;/strong&gt; work in tandem to ensure that reasoning begins from the most semantically relevant evidence&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tools-1"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Evaluates the reasoning output for temporal alignment, factual accuracy, and coherence between visual and textual modalities&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This two-phase design, which involves&amp;nbsp;structured ingestion followed by agentic reasoning, enables MMCTAgent to deliver accurate, interpretable insights for long information-dense videos.&amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="imageagent-structured-reasoning-for-static-visuals"&gt;ImageAgent: Structured reasoning for static visuals&lt;/h3&gt;



&lt;p&gt;While the VideoAgent handles temporal reasoning across long-form videos, the ImageAgent applies the same Planner–Critic paradigm to static visual analysis. It performs modular, tool-based reasoning over images, combining perception tools for recognition, detection, and optical character recognition with language-based reasoning for interpretation and explanation.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;vit_tool&lt;/strong&gt;: Leverages Vision Transformer (ViT) or Vision Languague Model (VLM) for high-level visual understanding and description&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;recog_tool&lt;/strong&gt;: Performs scene, face, and object recognition&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;object_detection_tool&lt;/strong&gt;: Localizes and labels entities within an image&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;ocr_tool&lt;/strong&gt;: Extracts embedded text from visual elements&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tool"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Validates the Planner’s conclusions for factual alignment and consistency, refining the final response&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This lightweight ImageAgent provides fine-grained, explainable reasoning over image collections—supporting visual question answering, content inspection, and multimodal retrieval—while maintaining architectural symmetry with the VideoAgent.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation-results"&gt;Evaluation Results&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;To assess the effectiveness of MMCTAgent, we evaluated both the ImageAgent and VideoAgent with multiple base LLM models and a range of benchmark datasets and real-world scenarios. Some key results are presented here.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Image Datasets&lt;/th&gt;&lt;th&gt;GPT-4V&lt;/th&gt;&lt;th&gt;MMCT with GPT-4V&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;th&gt;GPT-5&lt;/th&gt;&lt;th&gt;MMCT with GPT-5&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;MM-Vet [1]&lt;/td&gt;&lt;td&gt;60.20&lt;/td&gt;&lt;td&gt;74.24&lt;/td&gt;&lt;td&gt;77.98&lt;/td&gt;&lt;td&gt;79.36&lt;/td&gt;&lt;td&gt;80.51&lt;/td&gt;&lt;td&gt;81.65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MMMU [2]&lt;/td&gt;&lt;td&gt;56.80&lt;/td&gt;&lt;td&gt;63.57&lt;/td&gt;&lt;td&gt;69.10&lt;/td&gt;&lt;td&gt;73.00&lt;/td&gt;&lt;td&gt;84.20&lt;/td&gt;&lt;td&gt;85.44&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Video Datasets&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;VideoMME [3]&lt;/td&gt;&lt;td&gt;72.10&lt;/td&gt;&lt;td&gt;76.70&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;MMCTAgent enhances base model performance by augmenting their capabilities with appropriate tools such as object detection and optical character recognition (OCR) for weaker models, or domain-specific tools for stronger models, thereby leading to substantial improvements. For example, integrating these tools raised GPT-4V’s accuracy from 60.20% to 74.24% on MM-Vet dataset. Additionally, the configurable Critic agent provides additional validation, which is especially valuable in critical domains. The additional evaluation results are available here&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="takeaways-and-next-steps"&gt;Takeaways and next steps&lt;/h2&gt;



&lt;p&gt;MMCTAgent demonstrates a scalable agentic approach to multimodal reasoning with a Planner–Critic architecture. Its unified multimodal design supports both image and video pipelines, while the extensible toolchain enables rapid integration of domain-specific tools and capabilities. It provides Azure-native deployment and supports configurability within the broader open-source ecosystem.&lt;/p&gt;



&lt;p&gt;Looking ahead, we aim to improve efficiency and adaptability in retrieval and reasoning workflows, and to extend MMCTAgent’s applications beyond current agricultural evaluations, exploring new real-world domains through initiatives like Project Gecko to advance the creation of accessible, innovative multimodal applications for people around the globe.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;



&lt;p&gt;We would like to thank our team members for their valuable contributions to this work: Aman Patkar, Ogbemi Ekwejunor-Etchie, Somnath Kumar, Soumya De, and Yash Gadhia. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[1] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. “MM-VET: Evaluating large multimodal models for integrated capabilities”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[2] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. “MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[3] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. “Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis”, 2024.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</description><content:encoded>&lt;figure class="wp-block-image size-full"&gt;&lt;img alt="Three white icons on a blue-to-purple gradient background: the first icon shows an image/photo; the second icon depicts a computer monitor with vertical bars; the third icon displays three connected circles with user silhouettes." class="wp-image-1153930" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MMCTAgent-BlogHeroFeature-1400x788-1.jpg" width="1400" /&gt;&lt;/figure&gt;



&lt;p&gt;Modern multimodal AI models can recognize objects, describe scenes, and answer questions about images and short video clips, but they struggle with long-form and large-scale visual data, where real-world reasoning requires moving beyond object recognition and short-clip analysis.&lt;/p&gt;



&lt;p&gt;Real-world reasoning increasingly involves analyzing long-form video content, where context spans minutes or hours, far beyond the context limits of most models. It also entails querying across massive multimodal libraries of videos, images, and transcripts, where finding and integrating relevant evidence requires more than retrieval—it requires strategic reasoning. Existing models typically perform single-pass inference, producing one-shot answers. This limits their ability to handle tasks that require temporal reasoning, cross-modal grounding, and iterative refinement.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="mmctagent"&gt;MMCTAgent&lt;/h2&gt;



&lt;p&gt;To meet these challenges, we developed the Multi-modal Critical Thinking Agent, or MMCTAgent, for structured reasoning over long-form video and image data, available on GitHub&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt; and featured on Azure AI Foundry Labs&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;p&gt;Built on AutoGen, Microsoft’s open-source multi-agent system, MMCTAgent provides multimodal question-answering with a Planner–Critic architecture. This design enables planning, reflection, and tool-based reasoning, bridging perception and deliberation in multimodal tasks. It links language, vision, and temporal understanding, transforming static multimodal tasks into dynamic reasoning workflows.&amp;nbsp;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Unlike conventional models that produce one-shot answers, MMCTAgent has modality-specific agents, including ImageAgent and VideoAgent, which include tools like get_relevant_query_frames() or object_detection-tool(). These agents perform&amp;nbsp;deliberate, iterative reasoning—selecting the right tools for each modality, evaluating intermediate results, and refining conclusions through a Critic loop. This enables MMCTAgent to analyze complex queries across long videos and large image libraries with explainability, extensibility, and scalability.&lt;/p&gt;







	&lt;div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide"&gt;
		

		&lt;p class="msr-promo__label text-gray-800 text-center text-uppercase"&gt;
		&lt;span class="px-4 bg-white display-inline-block font-weight-semibold small"&gt;PODCAST SERIES&lt;/span&gt;
	&lt;/p&gt;
	
	&lt;div class="row pt-3 pb-4 align-items-center"&gt;
						
			
			&lt;div class="msr-promo__content p-3 px-5 col-12 col-md"&gt;

									&lt;h2 class="h4"&gt;The AI Revolution in Medicine, Revisited&lt;/h2&gt;
				
								&lt;p class="large" id="the-ai-revolution-in-medicine-revisited"&gt;Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.&lt;/p&gt;
				
								
							&lt;/div&gt;&lt;!--/.msr-promo__content--&gt;
	&lt;/div&gt;&lt;!--/.msr-promo__inner-wrap--&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;	&lt;/div&gt;&lt;!--/.msr-promo--&gt;
	


&lt;h2 class="wp-block-heading" id="how-mmctagent-works"&gt;How MMCTAgent works&lt;/h2&gt;



&lt;p&gt;MMCTAgent integrates two coordinated agents, Planner and Critic, orchestrated through AutoGen. The Planner agent decomposes a user query, identifies the appropriate reasoning tools, performs multimodal operations, and drafts a preliminary answer. The Critic agent reviews the Planner’s reasoning chain, validates evidence alignment, and refines or revises the response for factual accuracy and consistency.&lt;/p&gt;



&lt;p&gt;This iterative reasoning loop enables MMCTAgent to improve its answers through structured self-evaluation—bringing reflection into AI reasoning. A key strength of MMCTAgent lies in its modular extensibility. Developers can easily integrate new, domain-specific tools—such as medical image analyzers, industrial inspection models, or specialized retrieval modules—by adding them to ImageQnATools or VideoQnATools. This design makes MMCTAgent adaptable across domains.&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="videoagent-from-ingestion-to-long-form-multimodal-reasoning"&gt;VideoAgent: From ingestion to long-form multimodal reasoning&lt;/h3&gt;



&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback.&amp;nbsp;" class="wp-image-1155366" height="8455" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/MMCT_UPDATED_FINAL_FINAL.png" width="14353" /&gt;&lt;figcaption class="wp-element-caption"&gt;Figure 1. MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p&gt;The VideoAgent extends this architecture to long-form video reasoning. It operates in two connected phases: library creation (ingestion) and query-time reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-1-video-ingestion-and-library-creation"&gt;Phase 1 – Video ingestion and library creation&lt;/h4&gt;



&lt;p&gt;Before reasoning, long-form videos undergo an ingestion pipeline that aligns multimodal information for retrieval and understanding:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Transcription &lt;/strong&gt;and&lt;strong&gt; translation&lt;/strong&gt;: Converts audio to text and, if multilingual, translates transcripts into a consistent language&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Key-frame identification&lt;/strong&gt;: Extracts representative frames marking major visual or scene changes&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Semantic chunking &lt;/strong&gt;and&lt;strong&gt; chapter generation&lt;/strong&gt;: Combines transcript segments and visual summaries into coherent, semantically segmented chapters with associated key frames. Inspired by Microsoft’s Deep Video Discovery agentic search tool, this step also extracts detailed descriptions of objects, on-screen text, and characters present within each video segment, integrating these insights directly into the corresponding chapters.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Multimodal embedding creation&lt;/strong&gt;: Generates image embeddings for key frames, linking them to their corresponding transcript and chapter data&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;All structured metadata, including transcripts, visual summaries, chapters, and embeddings, is indexed in the Multimodal Knowledgebase using Azure AI Search&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;, which forms the foundation for scalable semantic retrieval and downstream reasoning.&lt;/p&gt;



&lt;hr class="wp-block-separator has-alpha-channel-opacity" /&gt;



&lt;h4 class="wp-block-heading" id="phase-2-video-question-answering-and-reasoning"&gt;Phase 2 – Video question answering and reasoning&lt;/h4&gt;



&lt;p&gt;When a user submits a query, the VideoAgent retrieves, analyzes, and reasons across the indexed video content using specialized planner and critic tools.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools-1"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;get_video_analysis&lt;/strong&gt;: Finds the most relevant video, provides a summary, and lists detected objects&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt;: Retrieves contextual information and relevant chapters from the Azure AI Search index&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_relevant_frames&lt;/strong&gt;: Selects key frames most relevant to the user query&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;query_frame&lt;/strong&gt;: Performs detailed visual and textual reasoning over selected frames&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;get_context&lt;/strong&gt; and &lt;strong&gt;get_relevant_frames&lt;/strong&gt; work in tandem to ensure that reasoning begins from the most semantically relevant evidence&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tools-1"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Evaluates the reasoning output for temporal alignment, factual accuracy, and coherence between visual and textual modalities&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This two-phase design, which involves&amp;nbsp;structured ingestion followed by agentic reasoning, enables MMCTAgent to deliver accurate, interpretable insights for long information-dense videos.&amp;nbsp;&lt;/p&gt;



&lt;h3 class="wp-block-heading" id="imageagent-structured-reasoning-for-static-visuals"&gt;ImageAgent: Structured reasoning for static visuals&lt;/h3&gt;



&lt;p&gt;While the VideoAgent handles temporal reasoning across long-form videos, the ImageAgent applies the same Planner–Critic paradigm to static visual analysis. It performs modular, tool-based reasoning over images, combining perception tools for recognition, detection, and optical character recognition with language-based reasoning for interpretation and explanation.&lt;/p&gt;



&lt;h5 class="wp-block-heading" id="planner-tools"&gt;Planner tools&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;vit_tool&lt;/strong&gt;: Leverages Vision Transformer (ViT) or Vision Languague Model (VLM) for high-level visual understanding and description&amp;nbsp;&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;recog_tool&lt;/strong&gt;: Performs scene, face, and object recognition&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;object_detection_tool&lt;/strong&gt;: Localizes and labels entities within an image&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;ocr_tool&lt;/strong&gt;: Extracts embedded text from visual elements&lt;/li&gt;
&lt;/ul&gt;



&lt;h5 class="wp-block-heading" id="critic-tool"&gt;Critic tool&lt;/h5&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;critic_tool&lt;/strong&gt;: Validates the Planner’s conclusions for factual alignment and consistency, refining the final response&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This lightweight ImageAgent provides fine-grained, explainable reasoning over image collections—supporting visual question answering, content inspection, and multimodal retrieval—while maintaining architectural symmetry with the VideoAgent.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="evaluation-results"&gt;Evaluation Results&amp;nbsp;&lt;/h2&gt;



&lt;p&gt;To assess the effectiveness of MMCTAgent, we evaluated both the ImageAgent and VideoAgent with multiple base LLM models and a range of benchmark datasets and real-world scenarios. Some key results are presented here.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Image Datasets&lt;/th&gt;&lt;th&gt;GPT-4V&lt;/th&gt;&lt;th&gt;MMCT with GPT-4V&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;th&gt;GPT-5&lt;/th&gt;&lt;th&gt;MMCT with GPT-5&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;MM-Vet [1]&lt;/td&gt;&lt;td&gt;60.20&lt;/td&gt;&lt;td&gt;74.24&lt;/td&gt;&lt;td&gt;77.98&lt;/td&gt;&lt;td&gt;79.36&lt;/td&gt;&lt;td&gt;80.51&lt;/td&gt;&lt;td&gt;81.65&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;MMMU [2]&lt;/td&gt;&lt;td&gt;56.80&lt;/td&gt;&lt;td&gt;63.57&lt;/td&gt;&lt;td&gt;69.10&lt;/td&gt;&lt;td&gt;73.00&lt;/td&gt;&lt;td&gt;84.20&lt;/td&gt;&lt;td&gt;85.44&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;figure class="wp-block-table"&gt;&lt;table class="has-fixed-layout"&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Video Datasets&lt;/th&gt;&lt;th&gt;GPT4o&lt;/th&gt;&lt;th&gt;MMCT with GPT-4o&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;VideoMME [3]&lt;/td&gt;&lt;td&gt;72.10&lt;/td&gt;&lt;td&gt;76.70&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;p&gt;MMCTAgent enhances base model performance by augmenting their capabilities with appropriate tools such as object detection and optical character recognition (OCR) for weaker models, or domain-specific tools for stronger models, thereby leading to substantial improvements. For example, integrating these tools raised GPT-4V’s accuracy from 60.20% to 74.24% on MM-Vet dataset. Additionally, the configurable Critic agent provides additional validation, which is especially valuable in critical domains. The additional evaluation results are available here&lt;span class="sr-only"&gt; (opens in new tab)&lt;/span&gt;.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="takeaways-and-next-steps"&gt;Takeaways and next steps&lt;/h2&gt;



&lt;p&gt;MMCTAgent demonstrates a scalable agentic approach to multimodal reasoning with a Planner–Critic architecture. Its unified multimodal design supports both image and video pipelines, while the extensible toolchain enables rapid integration of domain-specific tools and capabilities. It provides Azure-native deployment and supports configurability within the broader open-source ecosystem.&lt;/p&gt;



&lt;p&gt;Looking ahead, we aim to improve efficiency and adaptability in retrieval and reasoning workflows, and to extend MMCTAgent’s applications beyond current agricultural evaluations, exploring new real-world domains through initiatives like Project Gecko to advance the creation of accessible, innovative multimodal applications for people around the globe.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;



&lt;p&gt;We would like to thank our team members for their valuable contributions to this work: Aman Patkar, Ogbemi Ekwejunor-Etchie, Somnath Kumar, Soumya De, and Yash Gadhia. &lt;/p&gt;



&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;strong&gt;&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[1] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. “MM-VET: Evaluating large multimodal models for integrated capabilities”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[2] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. “MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI”, 2023.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;[3] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. “Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis”, 2024.&amp;nbsp;&lt;/p&gt;
&lt;span class="sr-only" id="label-external-link"&gt;Opens in a new tab&lt;/span&gt;</content:encoded><guid isPermaLink="false">https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/</guid><pubDate>Wed, 12 Nov 2025 12:00:20 +0000</pubDate></item><item><title>[NEW] Microsoft-backed Veir is bringing superconductors to data centers (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/microsoft-backed-veir-targets-data-centers-for-its-megawatt-class-superconductors/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1195233690.jpg?resize=1200,775" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Power demands of data centers have grown from tens to 200 kilowatts in just a few years, a pace that has data center developers scrambling to design future facilities that can handle the load. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the next couple of years, it’s going to be 600 kilowatts, and then we’re going to a megawatt,” Tim Heidel, CEO of Veir, told TechCrunch. “We’re speaking to folks that are now trying to wrap their heads around the architecture for how you design data centers that have multi-megawatt racks.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At those scales, even the low-voltage cables that bring power to the racks start to take up too much space and generate too much heat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To rein that in, Veir has adapted its superconducting electrical cables to bring them inside the data center. The Microsoft-backed startup’s first product will be a cable system capable of carrying 3 megawatts of low-voltage electricity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To demonstrate the technology, Veir built a simulated data center near its headquarters in Massachusetts. The cables will be piloted in data centers next year in advance of an expected 2027 commercial launch, Heidel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Superconductors are a class of materials that can conduct electricity with zero loss of energy. The only hitch is that they need to be cooled well below freezing temperatures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Veir had previously focused on using superconductors to improve capacity on long-distance transmission lines. But utilities are cautious and tend to be slow to adopt new technology. While there’s still a good chance utilities will eventually tap superconductors for high-demand transmission lines, that transition is a bit further in the future.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The pace at which the data center community is moving, evolving, growing, scaling, and tackling challenges is far higher than the transmission community,” Heidel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Veir has been in talks with data centers for years. Recently, the tenor of those conversations changed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We were seeing a lot of folks saying, ‘Oh this grid interconnection problem is a real thing, and we got to figure out how to solve that.’ But then a handful of potential customers started turning around and saying, we actually have really hard problems to solve on our campuses and inside of our buildings,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup took the same core technology that it had developed for transmission lines and adapted it to the low-voltage needs of data centers. Veir buys the superconductors from the same suppliers, and they’re wrapped in a jacket to contain the liquid nitrogen coolant that keeps the material at -196˚ C (-321˚ F). Termination boxes sit at the end of those cables to transition from superconductors to copper cables.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re really a systems integrator that builds the cooling systems, manufactures the cables, puts the whole system together in order to deliver an enormous amount of power in a small space,” Heidel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result are cables that require 20 times less space than copper while carrying power five times farther, Veir said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The AI and data center community is desperate to find solutions today and is desperate to stay ahead. There’s a tremendous amount of competitive pressure to stay at the forefront,” Heidel said.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/02/GettyImages-1195233690.jpg?resize=1200,775" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Power demands of data centers have grown from tens to 200 kilowatts in just a few years, a pace that has data center developers scrambling to design future facilities that can handle the load. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“In the next couple of years, it’s going to be 600 kilowatts, and then we’re going to a megawatt,” Tim Heidel, CEO of Veir, told TechCrunch. “We’re speaking to folks that are now trying to wrap their heads around the architecture for how you design data centers that have multi-megawatt racks.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;At those scales, even the low-voltage cables that bring power to the racks start to take up too much space and generate too much heat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To rein that in, Veir has adapted its superconducting electrical cables to bring them inside the data center. The Microsoft-backed startup’s first product will be a cable system capable of carrying 3 megawatts of low-voltage electricity.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To demonstrate the technology, Veir built a simulated data center near its headquarters in Massachusetts. The cables will be piloted in data centers next year in advance of an expected 2027 commercial launch, Heidel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Superconductors are a class of materials that can conduct electricity with zero loss of energy. The only hitch is that they need to be cooled well below freezing temperatures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Veir had previously focused on using superconductors to improve capacity on long-distance transmission lines. But utilities are cautious and tend to be slow to adopt new technology. While there’s still a good chance utilities will eventually tap superconductors for high-demand transmission lines, that transition is a bit further in the future.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“The pace at which the data center community is moving, evolving, growing, scaling, and tackling challenges is far higher than the transmission community,” Heidel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Veir has been in talks with data centers for years. Recently, the tenor of those conversations changed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We were seeing a lot of folks saying, ‘Oh this grid interconnection problem is a real thing, and we got to figure out how to solve that.’ But then a handful of potential customers started turning around and saying, we actually have really hard problems to solve on our campuses and inside of our buildings,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup took the same core technology that it had developed for transmission lines and adapted it to the low-voltage needs of data centers. Veir buys the superconductors from the same suppliers, and they’re wrapped in a jacket to contain the liquid nitrogen coolant that keeps the material at -196˚ C (-321˚ F). Termination boxes sit at the end of those cables to transition from superconductors to copper cables.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re really a systems integrator that builds the cooling systems, manufactures the cables, puts the whole system together in order to deliver an enormous amount of power in a small space,” Heidel said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result are cables that require 20 times less space than copper while carrying power five times farther, Veir said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The AI and data center community is desperate to find solutions today and is desperate to stay ahead. There’s a tremendous amount of competitive pressure to stay at the forefront,” Heidel said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/microsoft-backed-veir-targets-data-centers-for-its-megawatt-class-superconductors/</guid><pubDate>Wed, 12 Nov 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] The Download: how to survive a conspiracy theory, and moldy cities (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/11/12/1127881/the-download-how-to-survive-a-conspiracy-theory-and-moldy-cities/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What it’s like to be in the middle of a conspiracy theory (according to a conspiracy theory expert)&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Mike Rothschild is a journalist and an expert on the growth and impact of conspiracy theories and disinformation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;It’s something of a familiar cycle by now: Tragedy hits; rampant misinformation and conspiracy theories follow. It’s often even more acute in the case of a natural disaster, when conspiracy theories about what “really” caused the calamity run right into culture-war-driven climate change denialism. Put together, these theories obscure real causes while elevating fake ones.&lt;/p&gt;&lt;p&gt;I’ve studied these ideas extensively, having spent the last 10 years writing about conspiracy theories and disinformation as a journalist and researcher. I’ve covered everything from the rise of QAnon to whether Donald Trump faked his assassination attempt. I’ve written three books, testified to Congress, and even written a report for the January 6th Committee.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Still, I’d never lived it. Not until my house in Altadena, California, burned down. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s series “The New Conspiracy Age,” on how the present boom in conspiracy theories is reshaping science and technology. Check out &lt;/strong&gt;&lt;strong&gt;the rest of the series here&lt;/strong&gt;&lt;strong&gt;. It’s also featured in this week’s MIT Technology Review Narrated podcast, which we publish each week on &lt;/strong&gt;&lt;strong&gt;Spotify&lt;/strong&gt;&lt;strong&gt; and &lt;/strong&gt;&lt;strong&gt;Apple Podcasts&lt;/strong&gt;&lt;strong&gt;.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’d like to hear more from Mike, he’ll be joining our features editor Amanda Silverman and executive editor Niall Firth for a subscriber-exclusive Roundtable conversation exploring how we can survive in the age of conspiracies. It’s at 1pm ET on Thursday November 20—&lt;/strong&gt;&lt;strong&gt;register now to join us&lt;/strong&gt;&lt;strong&gt;!&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;This startup thinks slime mold can help us design better cities&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;It is a yellow blob with no brain, yet some researchers believe a curious organism known as slime mold could help us build more resilient cities.&lt;/p&gt;&lt;p&gt;Humans have been building cities for 6,000 years, but slime mold has been around for 600 million. The team behind a new startup called Mireta wants to translate the organism’s biological superpowers into algorithms that might help improve transit times, alleviate congestion, and minimize climate-related disruptions in cities worldwide. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Elissaveta M. Brandon&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about our bodies. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 US government officials are skipping COP30&lt;/strong&gt;&lt;br /&gt;And American corporate executives are following their lead. (NYT $)&lt;br /&gt;+ &lt;em&gt;Protestors stormed the climate talks in Brazil. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Gavin Newsom took aim at Donald Trump’s climate policies onstage. &lt;/em&gt;(FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The UK may assess AI models for their ability to generate CSAM&lt;/strong&gt;&lt;br /&gt;Its government has suggested amending a legal bill to enable the tests. (BBC)&lt;br /&gt;+ &lt;em&gt;US investigators are using AI to detect child abuse images made by AI. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Google is suing a group of Chinese hackers&lt;/strong&gt;&lt;br /&gt;It claims they’re selling software to enable criminal scams. (FT $)&lt;br /&gt;+ &lt;em&gt;The group allegedly sends colossal text message phishing attacks. &lt;/em&gt;(CBS News)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;4 A major ‘cryptoqueen’ criminal has been jailed&lt;br /&gt;Qian Zhimin used money stolen from Chinese pensioners to buy cryptocurrency now worth billions. (BBC)&lt;br /&gt;+ &lt;em&gt;She defrauded her victims through an elaborate ponzi scheme. &lt;/em&gt;(CNN)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Carbon capture’s creators fear it’s being misused&lt;/strong&gt;&lt;br /&gt;Overreliance on the method could breed overconfidence and cause countries to delay reducing emissions. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Big Tech’s big bet on a controversial carbon removal tactic. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;6 The UK will use AI to phase out animal testing&lt;br /&gt;&lt;/strong&gt;3D bioprinted human tissues could also help to speed up the process. (The Guardian)&lt;br /&gt;+ &lt;em&gt;But the AI boom is looking increasingly precarious. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Louisiana is dealing with a whooping cough outbreak&lt;br /&gt;&lt;/strong&gt;Two infants have died to date from the wholly preventative disease. (Undark)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Here’s how ordinary people use ChatGPT&lt;br /&gt;&lt;/strong&gt;Emotional support and discussions crop up regularly.(WP $)&lt;br /&gt;+ &lt;em&gt;It’s surprisingly easy to stumble into a relationship with an AI chatbot. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Inside the search for lost continents&lt;/strong&gt;&lt;br /&gt;A newly-discovered mechanism is shedding light on why they may have vanished. (404 Media)&lt;br /&gt;+ &lt;em&gt;How environmental DNA is giving scientists a new way to understand our world. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 AI is taking Gen Z’s entry-level jobs&lt;/strong&gt;&lt;br /&gt;Especially in traditionally graduate-friendly consultancies. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;What the Industrial Revolution can teach us about how to handle AI. &lt;/em&gt;(Knowable Magazine)&lt;br /&gt;+ &lt;em&gt;America’s corporate boards are stumbling in the dark. &lt;/em&gt;(WSJ $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"We can’t eat money.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Nato, an Indigenous leader from the Tupinamba community, tells Reuters why they are protesting at the COP30 climate summit in Brazil against any potential sale of their land.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127883" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_892899.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How K-pop fans are shaping elections around the globe&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Back in the early ‘90s, Korean pop music, known as K-pop, was largely conserved to its native South Korea. It’s since exploded around the globe into an international phenomenon, emphasizing choreography and elaborate performance.&lt;/p&gt;&lt;p&gt;It’s made bands like Girls Generation, EXO, BTS, and Blackpink into household names, and inspired a special brand of particularly fierce devotion in their fans.&lt;/p&gt;&lt;p&gt;Now, those same fandoms have learned how to use their digital skills to advocate for social change and pursue political goals—organizing acts of civil resistance, donating generously to charity, and even foiling white supremacist attempts to spread hate speech. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Soo Youn&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ These sucker fish are having the time of their lives hitching a ride on a whale.&lt;br /&gt;+ Next time you fly, ditch the WiFi. I know I will.&lt;br /&gt;+ I love this colossal interactive gif.&lt;br /&gt;+ The hottest scent in perfumery right now? Smelling like a robot, apparently.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;What it’s like to be in the middle of a conspiracy theory (according to a conspiracy theory expert)&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;em&gt;—Mike Rothschild is a journalist and an expert on the growth and impact of conspiracy theories and disinformation.&lt;/em&gt;&lt;/p&gt;&lt;p&gt;It’s something of a familiar cycle by now: Tragedy hits; rampant misinformation and conspiracy theories follow. It’s often even more acute in the case of a natural disaster, when conspiracy theories about what “really” caused the calamity run right into culture-war-driven climate change denialism. Put together, these theories obscure real causes while elevating fake ones.&lt;/p&gt;&lt;p&gt;I’ve studied these ideas extensively, having spent the last 10 years writing about conspiracy theories and disinformation as a journalist and researcher. I’ve covered everything from the rise of QAnon to whether Donald Trump faked his assassination attempt. I’ve written three books, testified to Congress, and even written a report for the January 6th Committee.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Still, I’d never lived it. Not until my house in Altadena, California, burned down. Read the full story.&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;This story is part of MIT Technology Review’s series “The New Conspiracy Age,” on how the present boom in conspiracy theories is reshaping science and technology. Check out &lt;/strong&gt;&lt;strong&gt;the rest of the series here&lt;/strong&gt;&lt;strong&gt;. It’s also featured in this week’s MIT Technology Review Narrated podcast, which we publish each week on &lt;/strong&gt;&lt;strong&gt;Spotify&lt;/strong&gt;&lt;strong&gt; and &lt;/strong&gt;&lt;strong&gt;Apple Podcasts&lt;/strong&gt;&lt;strong&gt;.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;If you’d like to hear more from Mike, he’ll be joining our features editor Amanda Silverman and executive editor Niall Firth for a subscriber-exclusive Roundtable conversation exploring how we can survive in the age of conspiracies. It’s at 1pm ET on Thursday November 20—&lt;/strong&gt;&lt;strong&gt;register now to join us&lt;/strong&gt;&lt;strong&gt;!&lt;/strong&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;This startup thinks slime mold can help us design better cities&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;It is a yellow blob with no brain, yet some researchers believe a curious organism known as slime mold could help us build more resilient cities.&lt;/p&gt;&lt;p&gt;Humans have been building cities for 6,000 years, but slime mold has been around for 600 million. The team behind a new startup called Mireta wants to translate the organism’s biological superpowers into algorithms that might help improve transit times, alleviate congestion, and minimize climate-related disruptions in cities worldwide. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Elissaveta M. Brandon&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;This story is from the latest print issue of MIT Technology Review magazine, which is full of fascinating stories about our bodies. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;1 US government officials are skipping COP30&lt;/strong&gt;&lt;br /&gt;And American corporate executives are following their lead. (NYT $)&lt;br /&gt;+ &lt;em&gt;Protestors stormed the climate talks in Brazil. &lt;/em&gt;(The Guardian)&lt;br /&gt;+ &lt;em&gt;Gavin Newsom took aim at Donald Trump’s climate policies onstage. &lt;/em&gt;(FT $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;2 The UK may assess AI models for their ability to generate CSAM&lt;/strong&gt;&lt;br /&gt;Its government has suggested amending a legal bill to enable the tests. (BBC)&lt;br /&gt;+ &lt;em&gt;US investigators are using AI to detect child abuse images made by AI. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 Google is suing a group of Chinese hackers&lt;/strong&gt;&lt;br /&gt;It claims they’re selling software to enable criminal scams. (FT $)&lt;br /&gt;+ &lt;em&gt;The group allegedly sends colossal text message phishing attacks. &lt;/em&gt;(CBS News)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;4 A major ‘cryptoqueen’ criminal has been jailed&lt;br /&gt;Qian Zhimin used money stolen from Chinese pensioners to buy cryptocurrency now worth billions. (BBC)&lt;br /&gt;+ &lt;em&gt;She defrauded her victims through an elaborate ponzi scheme. &lt;/em&gt;(CNN)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;5 Carbon capture’s creators fear it’s being misused&lt;/strong&gt;&lt;br /&gt;Overreliance on the method could breed overconfidence and cause countries to delay reducing emissions. (Bloomberg $)&lt;br /&gt;+ &lt;em&gt;Big Tech’s big bet on a controversial carbon removal tactic. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;strong&gt;6 The UK will use AI to phase out animal testing&lt;br /&gt;&lt;/strong&gt;3D bioprinted human tissues could also help to speed up the process. (The Guardian)&lt;br /&gt;+ &lt;em&gt;But the AI boom is looking increasingly precarious. &lt;/em&gt;(WSJ $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 Louisiana is dealing with a whooping cough outbreak&lt;br /&gt;&lt;/strong&gt;Two infants have died to date from the wholly preventative disease. (Undark)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;8 Here’s how ordinary people use ChatGPT&lt;br /&gt;&lt;/strong&gt;Emotional support and discussions crop up regularly.(WP $)&lt;br /&gt;+ &lt;em&gt;It’s surprisingly easy to stumble into a relationship with an AI chatbot. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9 Inside the search for lost continents&lt;/strong&gt;&lt;br /&gt;A newly-discovered mechanism is shedding light on why they may have vanished. (404 Media)&lt;br /&gt;+ &lt;em&gt;How environmental DNA is giving scientists a new way to understand our world. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;10 AI is taking Gen Z’s entry-level jobs&lt;/strong&gt;&lt;br /&gt;Especially in traditionally graduate-friendly consultancies. (NY Mag $)&lt;br /&gt;+ &lt;em&gt;What the Industrial Revolution can teach us about how to handle AI. &lt;/em&gt;(Knowable Magazine)&lt;br /&gt;+ &lt;em&gt;America’s corporate boards are stumbling in the dark. &lt;/em&gt;(WSJ $)&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;"We can’t eat money.”&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;—Nato, an Indigenous leader from the Tupinamba community, tells Reuters why they are protesting at the COP30 climate summit in Brazil against any potential sale of their land.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="alt" class="wp-image-1127883" src="https://wp.technologyreview.com/wp-content/uploads/2025/11/image_892899.png" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;How K-pop fans are shaping elections around the globe&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Back in the early ‘90s, Korean pop music, known as K-pop, was largely conserved to its native South Korea. It’s since exploded around the globe into an international phenomenon, emphasizing choreography and elaborate performance.&lt;/p&gt;&lt;p&gt;It’s made bands like Girls Generation, EXO, BTS, and Blackpink into household names, and inspired a special brand of particularly fierce devotion in their fans.&lt;/p&gt;&lt;p&gt;Now, those same fandoms have learned how to use their digital skills to advocate for social change and pursue political goals—organizing acts of civil resistance, donating generously to charity, and even foiling white supremacist attempts to spread hate speech. Read the full story.&lt;/p&gt;&lt;p&gt;&lt;em&gt;—Soo Youn&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ These sucker fish are having the time of their lives hitching a ride on a whale.&lt;br /&gt;+ Next time you fly, ditch the WiFi. I know I will.&lt;br /&gt;+ I love this colossal interactive gif.&lt;br /&gt;+ The hottest scent in perfumery right now? Smelling like a robot, apparently.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/11/12/1127881/the-download-how-to-survive-a-conspiracy-theory-and-moldy-cities/</guid><pubDate>Wed, 12 Nov 2025 13:10:00 +0000</pubDate></item><item><title>[NEW] Fei-Fei Li’s World Labs speeds up the world model race with Marble, its first commercial product (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/fei-fei-lis-world-labs-speeds-up-the-world-model-race-with-marble-its-first-commercial-product/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;World Labs, the startup founded by AI pioneer Fei-Fei Li, is launching its first commercial world model product. Marble is now available via freemium and paid tiers that let users turn text prompts, photos, videos, 3D layouts, or panoramas into editable, downloadable 3D environments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the generative world model, first released in limited beta preview two months ago, comes a little over a year after World Labs came out of stealth with $230 million in funding, and puts the startup ahead of competitors building world models. World models are AI systems that generate an internal representation of an environment, and can be used to predict future outcomes and plan actions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Startups like Decart and Odyssey have released free demos, and Google’s Genie is still in limited research preview. Marble differs from these — and even World Labs’ own real-time model, RTFM — because it creates persistent, downloadable 3D environments rather than generating worlds on-the-fly as you explore. This, the company says, results in less morphing or inconsistency, and lets users export worlds as Gaussian splats, meshes, or videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Marble is also the first model of its kind to offer AI-native editing tools and a hybrid 3D editor that lets users block out spatial structures before AI fills in the visual details.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066967" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/worldlabs.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“This is a brand new category of model that’s generating 3D worlds, and this is something that’s going to get better over time. It’s something we’ve already improved quite a lot,” Justin Johnson, co-founder of World Labs, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last December, World Labs showed how its early models could generate interactive 3D scenes based on a single image. While impressive, the somewhat cartoonish scenes weren’t fully explorable since movements were limited to a small area, and there were occasional rendering errors.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In my trial of the beta preview, I found Marble generated impressive worlds from image prompts alone — from game-like environments to photorealistic versions of my living room. Scenes morphed at the edges, though that’s apparently been improved in today’s launch. That said, a world I’d generated in the beta using a single prompt looked better and matched my intent more closely than the same prompt does now.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;I haven’t yet tested the editing features, though Johnson says they make Marble practical for near-term gaming, VFX, and virtual reality (VR) projects.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One of our main themes for Marble going forward is creative control,” Johnson said. “There should always be a quick pathway to generate something, but you should be able to dive even deeper and get a lot of control over the things that you’re generating. You don’t want the machine to just take the wheel and pull all that creativity away from you.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066968" height="325" src="https://techcrunch.com/wp-content/uploads/2025/11/World-Labs-Marble-input-output-pipeline.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Marble’s input to output pipeline&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marble’s take on creative control starts with input flexibility. The beta only accepted single images, forcing the model to invent unseen details for a 360-degree view. With the full launch, users can now upload multiple images or short clips to show a space from different angles and have the model generate fairly realistic digital twins.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Then we have Chisel, an experimental 3D editor that lets users block out coarse spatial layouts (think walls, boxes, or planes) and then add text prompts to guide the visual style. Marble generates the world, decoupling structure from style — similar to how HTML provides the structure of a website and CSS adds in color. Unlike text-based editing, Chisel lets you directly manipulate objects.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066966" height="379" src="https://techcrunch.com/wp-content/uploads/2025/11/world-labs-coarse-3d-chisel.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Marble’s Chisel feature decouples structure from style&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“I can just go in there and grab the 3D block that represents the couch and move it somewhere else,” Johnson said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature that gives you more editing control is the ability to expand a world.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you generate a world, you can expand it up to once,” Johnson said. “When you move to a piece of the world that’s starting to break apart, you can basically tell the model to expand there or generate more world in the vicinity of where you currently are, and then it can add more detail in that region.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users who want to create extremely large spaces can combine multiple worlds with “composer mode.” Johnson demonstrated this for me with two worlds he had already built — a room made of cheese with grape chairs, and another of a futuristic meeting room in space.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-path-to-spatial-intelligence"&gt;The path to spatial intelligence&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066972" height="433" src="https://techcrunch.com/wp-content/uploads/2025/11/Untitled-design.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Space ship environment created in Marble with text prompt overlayed (Note how the lights are realistically reflected in the hub’s walls)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs/TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marble is available via four subscription tiers: Free (four generations from text, image, or panorama), Standard ($20/month, 12 generations plus multi-image/video input and advanced editing), Pro ($35/month, 25 generations with scene expansion and commercial rights), and Max ($95/month, all features and 75 generations).&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Johnson thinks the initial use cases for Marble will be gaming, visual effects for film, and virtual reality.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Game developers have mixed feelings about the tech. A recent Game Developers Conference survey found a third of respondents believed generative AI has a negative impact on the games industry — 12% more than the survey indicated year earlier. Intellectual property theft, energy consumption, and a decrease in quality from AI-generated content were among the top concerns aired. And last year, a Wired investigation found game studios like Activision Blizzard are using AI to cut corners and combat attrition.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In gaming, Johnson sees developers using Marble to generate background environments and ambient spaces and then importing those assets into game engines like Unity or Unreal Engine to add interactive elements, logic, and code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not designed to replace the entire existing pipeline for gaming, but to just give you assets that you can drop into that pipeline,” he said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For VFX work, Marble sidesteps the inconsistency and poor camera control that plague AI video generators, per Johnson. Its 3D assets let artists stage scenes and control camera movements with frame-perfect precision, he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Johnson said World Labs isn’t focusing on virtual reality (VR) applications right now, he noted the industry is “starved for content” and excited about the launch. Marble is already compatible with the Vision Pro and Quest 3 VR headsets, and every generated world can be viewed in VR today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Marble may also have potential use cases for robotics. Johnson noted that unlike image and video generation, robotics doesn’t have the benefit of a large repository of training data. But with generators like Marble, it becomes easier to simulate training environments.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a recent manifesto by Fei-Fei Li, CEO and co-founder of World Labs, Marble represents the first step toward creating “a truly spatially intelligent world model.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Li believes “the next generation of world models will enable machines to achieve spatial intelligence on an entirely new level.” If large language models can teach machines to read and write, Li hopes systems like Marble can teach them to see and build. She says the ability to understand how things exist and interact in three-dimensional spaces can eventually help machines make breakthroughs beyond gaming and robotics, and even into science and medicine.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our dreams of truly intelligent machines will not be complete without spatial intelligence,” Li wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;i&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;&lt;/i&gt;rebecca.bellan@techcrunch.com &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at&amp;nbsp;@rebeccabellan.491&lt;/em&gt; &lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;World Labs, the startup founded by AI pioneer Fei-Fei Li, is launching its first commercial world model product. Marble is now available via freemium and paid tiers that let users turn text prompts, photos, videos, 3D layouts, or panoramas into editable, downloadable 3D environments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of the generative world model, first released in limited beta preview two months ago, comes a little over a year after World Labs came out of stealth with $230 million in funding, and puts the startup ahead of competitors building world models. World models are AI systems that generate an internal representation of an environment, and can be used to predict future outcomes and plan actions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Startups like Decart and Odyssey have released free demos, and Google’s Genie is still in limited research preview. Marble differs from these — and even World Labs’ own real-time model, RTFM — because it creates persistent, downloadable 3D environments rather than generating worlds on-the-fly as you explore. This, the company says, results in less morphing or inconsistency, and lets users export worlds as Gaussian splats, meshes, or videos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Marble is also the first model of its kind to offer AI-native editing tools and a hybrid 3D editor that lets users block out spatial structures before AI fills in the visual details.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066967" height="383" src="https://techcrunch.com/wp-content/uploads/2025/11/worldlabs.gif?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“This is a brand new category of model that’s generating 3D worlds, and this is something that’s going to get better over time. It’s something we’ve already improved quite a lot,” Justin Johnson, co-founder of World Labs, told TechCrunch.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last December, World Labs showed how its early models could generate interactive 3D scenes based on a single image. While impressive, the somewhat cartoonish scenes weren’t fully explorable since movements were limited to a small area, and there were occasional rendering errors.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In my trial of the beta preview, I found Marble generated impressive worlds from image prompts alone — from game-like environments to photorealistic versions of my living room. Scenes morphed at the edges, though that’s apparently been improved in today’s launch. That said, a world I’d generated in the beta using a single prompt looked better and matched my intent more closely than the same prompt does now.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;I haven’t yet tested the editing features, though Johnson says they make Marble practical for near-term gaming, VFX, and virtual reality (VR) projects.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“One of our main themes for Marble going forward is creative control,” Johnson said. “There should always be a quick pathway to generate something, but you should be able to dive even deeper and get a lot of control over the things that you’re generating. You don’t want the machine to just take the wheel and pull all that creativity away from you.”&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066968" height="325" src="https://techcrunch.com/wp-content/uploads/2025/11/World-Labs-Marble-input-output-pipeline.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Marble’s input to output pipeline&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marble’s take on creative control starts with input flexibility. The beta only accepted single images, forcing the model to invent unseen details for a 360-degree view. With the full launch, users can now upload multiple images or short clips to show a space from different angles and have the model generate fairly realistic digital twins.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Then we have Chisel, an experimental 3D editor that lets users block out coarse spatial layouts (think walls, boxes, or planes) and then add text prompts to guide the visual style. Marble generates the world, decoupling structure from style — similar to how HTML provides the structure of a website and CSS adds in color. Unlike text-based editing, Chisel lets you directly manipulate objects.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066966" height="379" src="https://techcrunch.com/wp-content/uploads/2025/11/world-labs-coarse-3d-chisel.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Marble’s Chisel feature decouples structure from style&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“I can just go in there and grab the 3D block that represents the couch and move it somewhere else,” Johnson said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another new feature that gives you more editing control is the ability to expand a world.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Once you generate a world, you can expand it up to once,” Johnson said. “When you move to a piece of the world that’s starting to break apart, you can basically tell the model to expand there or generate more world in the vicinity of where you currently are, and then it can add more detail in that region.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users who want to create extremely large spaces can combine multiple worlds with “composer mode.” Johnson demonstrated this for me with two worlds he had already built — a room made of cheese with grape chairs, and another of a futuristic meeting room in space.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-the-path-to-spatial-intelligence"&gt;The path to spatial intelligence&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3066972" height="433" src="https://techcrunch.com/wp-content/uploads/2025/11/Untitled-design.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Space ship environment created in Marble with text prompt overlayed (Note how the lights are realistically reflected in the hub’s walls)&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;World Labs/TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Marble is available via four subscription tiers: Free (four generations from text, image, or panorama), Standard ($20/month, 12 generations plus multi-image/video input and advanced editing), Pro ($35/month, 25 generations with scene expansion and commercial rights), and Max ($95/month, all features and 75 generations).&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Johnson thinks the initial use cases for Marble will be gaming, visual effects for film, and virtual reality.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Game developers have mixed feelings about the tech. A recent Game Developers Conference survey found a third of respondents believed generative AI has a negative impact on the games industry — 12% more than the survey indicated year earlier. Intellectual property theft, energy consumption, and a decrease in quality from AI-generated content were among the top concerns aired. And last year, a Wired investigation found game studios like Activision Blizzard are using AI to cut corners and combat attrition.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;In gaming, Johnson sees developers using Marble to generate background environments and ambient spaces and then importing those assets into game engines like Unity or Unreal Engine to add interactive elements, logic, and code.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s not designed to replace the entire existing pipeline for gaming, but to just give you assets that you can drop into that pipeline,” he said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For VFX work, Marble sidesteps the inconsistency and poor camera control that plague AI video generators, per Johnson. Its 3D assets let artists stage scenes and control camera movements with frame-perfect precision, he said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Johnson said World Labs isn’t focusing on virtual reality (VR) applications right now, he noted the industry is “starved for content” and excited about the launch. Marble is already compatible with the Vision Pro and Quest 3 VR headsets, and every generated world can be viewed in VR today.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Marble may also have potential use cases for robotics. Johnson noted that unlike image and video generation, robotics doesn’t have the benefit of a large repository of training data. But with generators like Marble, it becomes easier to simulate training environments.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;According to a recent manifesto by Fei-Fei Li, CEO and co-founder of World Labs, Marble represents the first step toward creating “a truly spatially intelligent world model.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Li believes “the next generation of world models will enable machines to achieve spatial intelligence on an entirely new level.” If large language models can teach machines to read and write, Li hopes systems like Marble can teach them to see and build. She says the ability to understand how things exist and interact in three-dimensional spaces can eventually help machines make breakthroughs beyond gaming and robotics, and even into science and medicine.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Our dreams of truly intelligent machines will not be complete without spatial intelligence,” Li wrote.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;i&gt;Got a sensitive tip or confidential documents? We’re reporting on the inner workings of the AI industry — from the companies shaping its future to the people impacted by their decisions. Reach out to Rebecca Bellan at&amp;nbsp;&lt;/i&gt;rebecca.bellan@techcrunch.com &lt;em&gt;or Russell Brandom at russell.brandom@techcrunch.com. For secure communication, you can contact them via Signal at&amp;nbsp;@rebeccabellan.491&lt;/em&gt; &lt;em&gt;and russellbrandom.49.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/fei-fei-lis-world-labs-speeds-up-the-world-model-race-with-marble-its-first-commercial-product/</guid><pubDate>Wed, 12 Nov 2025 13:44:01 +0000</pubDate></item><item><title>[NEW] How Deductive AI saved DoorDash 1,000 engineering hours by automating software debugging (AI | VentureBeat)</title><link>https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating</link><description>[unable to retrieve full-text content]&lt;p&gt;As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: &lt;a href="https://algocademy.com/blog/why-debugging-takes-longer-than-writing-the-actual-code/"&gt;&lt;u&gt;Engineers are drowning in debugging work&lt;/u&gt;&lt;/a&gt;, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&amp;#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deductive.ai/"&gt;&lt;u&gt;Deductive AI&lt;/u&gt;&lt;/a&gt;, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by &lt;a href="https://www.crv.com/"&gt;&lt;u&gt;CRV&lt;/u&gt;&lt;/a&gt;, with participation from &lt;a href="https://www.databricks.com/databricks-ventures"&gt;&lt;u&gt;Databricks Ventures&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.thomvest.com/"&gt;&lt;u&gt;Thomvest Ventures&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.primeset.com/"&gt;&lt;u&gt;PrimeSet&lt;/u&gt;&lt;/a&gt;, to commercialize what it calls &amp;quot;&lt;a href="https://www.deductive.ai/product"&gt;&lt;u&gt;AI SRE agents&lt;/u&gt;&lt;/a&gt;&amp;quot; that can diagnose and help fix software failures at machine speed.&lt;/p&gt;&lt;p&gt;The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.&lt;/p&gt;&lt;p&gt;&amp;quot;The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&amp;#x27;s made of a million other needles, it&amp;#x27;s constantly reshuffling itself, and is on fire — and every second you don&amp;#x27;t find it equals lost revenue,&amp;quot; said Sameer Agarwal, Deductive&amp;#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.&lt;/p&gt;&lt;p&gt;Deductive&amp;#x27;s system builds what the company calls a &amp;quot;knowledge graph&amp;quot; that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.&lt;/p&gt;&lt;p&gt;The technology has already shown measurable impact at some of the world&amp;#x27;s most demanding production environments. &lt;a href="https://www.deductive.ai/blogs/how-doordash-powers-a-reliable-high-performance-ad-platform-with-deductive-ai"&gt;&lt;u&gt;DoorDash&amp;#x27;s advertising platform&lt;/u&gt;&lt;/a&gt;, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,&amp;quot; said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. &amp;quot;Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt; estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact &amp;quot;in millions of dollars,&amp;quot; according to Ansari. At location intelligence company &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI-generated code is creating a debugging crisis&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The timing of Deductive&amp;#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/karpathy/status/1886192184808149383?lang=en"&gt;&lt;u&gt;Vibe coding&lt;/u&gt;&lt;/a&gt;,&amp;quot; a term popularized by AI researcher &lt;a href="https://karpathy.ai/"&gt;&lt;u&gt;Andrej Karpathy&lt;/u&gt;&lt;/a&gt;, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as &amp;quot;redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns&amp;quot; that accumulate over time.&lt;/p&gt;&lt;p&gt;&amp;quot;Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,&amp;quot; Agarwal told Venturebeat. &amp;quot;In many ways, we now need AI to help clean up the mess that AI itself is creating.&amp;quot;&lt;/p&gt;&lt;p&gt;The claim that engineers spend roughly half their time on debugging isn&amp;#x27;t hyperbole. The Association for Computing Machinery reports that developers spend &lt;a href="https://queue.acm.org/detail.cfm?id=3404974"&gt;&lt;u&gt;35% to 50% of their time validating and debugging software&lt;/u&gt;&lt;/a&gt;. More recently, &lt;a href="https://www.harness.io/blog/announcing-harness-ai"&gt;&lt;u&gt;Harness&amp;#x27;s State of Software Delivery 2025&lt;/u&gt;&lt;/a&gt; report found that 67% of developers are spending more time debugging AI-generated code.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;ve seen world-class engineers spending half of their time debugging instead of building,&amp;quot; said Rakesh Kothari, Deductive&amp;#x27;s co-founder and CEO. &amp;quot;And as vibe coding generates new code at a rate we&amp;#x27;ve never seen, this problem is only going to get worse.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Deductive&amp;#x27;s AI agents actually investigate production failures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Deductive&amp;#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://newrelic.com/"&gt;&lt;u&gt;New Relic&lt;/u&gt;&lt;/a&gt;. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls &amp;quot;code-aware reasoning&amp;quot;—the ability to understand not just that something broke, but why the code behaves the way it does.&lt;/p&gt;&lt;p&gt;&amp;quot;Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,&amp;quot; Agarwal explained. &amp;quot;These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.&amp;quot;&lt;/p&gt;&lt;p&gt;The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.&lt;/p&gt;&lt;p&gt;When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.&lt;/p&gt;&lt;p&gt;The critical difference from rule-based automation is Deductive&amp;#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.&lt;/p&gt;&lt;p&gt;&amp;quot;Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,&amp;quot; Agarwal said. &amp;quot;It learns how to think through problems, not just point them out.&amp;quot;&lt;/p&gt;&lt;p&gt;At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&amp;#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.&lt;/p&gt;&lt;p&gt;&amp;quot;Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,&amp;quot; Ansari said. &amp;quot;Deductive was able to explain not just what changed, but how and why it impacted production behavior.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The company keeps humans in the loop—for now&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While Deductive&amp;#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.&lt;/p&gt;&lt;p&gt;&amp;quot;While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,&amp;quot; Agarwal said. &amp;quot;We believe maintaining a human in the loop is essential for trust, transparency and operational safety.&amp;quot;&lt;/p&gt;&lt;p&gt;However, he acknowledged that &amp;quot;over time, we do think that deeper automation will come and how humans operate in the loop will evolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Databricks and ThoughtSpot veterans bet on reasoning over observability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The founding team brings deep expertise from building some of Silicon Valley&amp;#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created &lt;a href="https://arxiv.org/abs/1203.5485"&gt;&lt;u&gt;BlinkDB&lt;/u&gt;&lt;/a&gt;, an influential system for approximate query processing. He was among the first engineers at &lt;a href="https://www.databricks.com/"&gt;&lt;u&gt;Databricks&lt;/u&gt;&lt;/a&gt;, where he helped build &lt;a href="https://docs.databricks.com/gcp/en/spark/?scid=701Vp000004h4b1IAA&amp;amp;utm_medium=paid+search&amp;amp;utm_source=google&amp;amp;utm_campaign=23156677199&amp;amp;utm_adgroup=189768475320&amp;amp;utm_content=aimax&amp;amp;utm_offer=aimax&amp;amp;utm_ad=779965794184&amp;amp;utm_term=apache%20iceberg%20spark&amp;amp;gad_source=1&amp;amp;gad_campaignid=23156677199&amp;amp;gbraid=0AAAAABYBeAhSzbWjXCf1Ok8HU2XzyuNAb&amp;amp;gclid=CjwKCAiA_dDIBhB6EiwAvzc1cN1MnT40-rmesA_-YwBm870Sksy-DQYqWaR9mqQLAIQjzo7yRJpIfBoC7GsQAvD_BwE"&gt;&lt;u&gt;Apache Spark&lt;/u&gt;&lt;/a&gt;. Kothari was an early engineer at &lt;a href="https://www.thoughtspot.com/"&gt;&lt;u&gt;ThoughtSpot&lt;/u&gt;&lt;/a&gt;, where he led teams focused on distributed query processing and large-scale system optimization.&lt;/p&gt;&lt;p&gt;The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&amp;#x27;s &lt;a href="https://www.crv.com/team/max-gazor"&gt;&lt;u&gt;Max Gazor&lt;/u&gt;&lt;/a&gt;, the round included participation from &lt;a href="https://sequoiacap.com/podcast/training-data-ion-stoica/"&gt;&lt;u&gt;Ion Stoica&lt;/u&gt;&lt;/a&gt;, founder of Databricks and Anyscale; &lt;a href="https://www.thoughtspot.com/author/ajeet-singh"&gt;&lt;u&gt;Ajeet Singh&lt;/u&gt;&lt;/a&gt;, founder of Nutanix and ThoughtSpot; and &lt;a href="http://bensigelman.org/"&gt;&lt;u&gt;Ben Sigelman&lt;/u&gt;&lt;/a&gt;, founder of Lightstep.&lt;/p&gt;&lt;p&gt;Rather than competing with platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.pagerduty.com/"&gt;&lt;u&gt;PagerDuty&lt;/u&gt;&lt;/a&gt;, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.&lt;/p&gt;&lt;p&gt;The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&amp;#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.&lt;/p&gt;&lt;p&gt;With fresh capital and early customer traction at companies like &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://kumo.ai/"&gt;&lt;u&gt;Kumo AI&lt;/u&gt;&lt;/a&gt;, Deductive plans to expand its team and deepen the system&amp;#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.&lt;/p&gt;&lt;p&gt;DoorDash&amp;#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: &amp;quot;Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.&amp;quot;&lt;/p&gt;&lt;p&gt;In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;As software systems grow more complex and AI tools generate code faster than ever, a fundamental problem is getting worse: &lt;a href="https://algocademy.com/blog/why-debugging-takes-longer-than-writing-the-actual-code/"&gt;&lt;u&gt;Engineers are drowning in debugging work&lt;/u&gt;&lt;/a&gt;, spending up to half their time hunting down the causes of software failures instead of building new products. The challenge has become so acute that it&amp;#x27;s creating a new category of tooling — AI agents that can diagnose production failures in minutes instead of hours.&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.deductive.ai/"&gt;&lt;u&gt;Deductive AI&lt;/u&gt;&lt;/a&gt;, a startup emerging from stealth mode Wednesday, believes it has found a solution by applying reinforcement learning — the same technology that powers game-playing AI systems — to the messy, high-stakes world of production software incidents. The company announced it has raised $7.5 million in seed funding led by &lt;a href="https://www.crv.com/"&gt;&lt;u&gt;CRV&lt;/u&gt;&lt;/a&gt;, with participation from &lt;a href="https://www.databricks.com/databricks-ventures"&gt;&lt;u&gt;Databricks Ventures&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://www.thomvest.com/"&gt;&lt;u&gt;Thomvest Ventures&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://www.primeset.com/"&gt;&lt;u&gt;PrimeSet&lt;/u&gt;&lt;/a&gt;, to commercialize what it calls &amp;quot;&lt;a href="https://www.deductive.ai/product"&gt;&lt;u&gt;AI SRE agents&lt;/u&gt;&lt;/a&gt;&amp;quot; that can diagnose and help fix software failures at machine speed.&lt;/p&gt;&lt;p&gt;The pitch resonates with a growing frustration inside engineering organizations: Modern observability tools can show that something broke, but they rarely explain why. When a production system fails at 3 a.m., engineers still face hours of manual detective work, cross-referencing logs, metrics, deployment histories, and code changes across dozens of interconnected services to identify the root cause.&lt;/p&gt;&lt;p&gt;&amp;quot;The complexities and inter-dependencies of modern infrastructure means that investigating the root cause of an outage or incident can feel like searching for a needle in a haystack, except the haystack is the size of a football field, it&amp;#x27;s made of a million other needles, it&amp;#x27;s constantly reshuffling itself, and is on fire — and every second you don&amp;#x27;t find it equals lost revenue,&amp;quot; said Sameer Agarwal, Deductive&amp;#x27;s co-founder and chief technology officer, in an exclusive interview with VentureBeat.&lt;/p&gt;&lt;p&gt;Deductive&amp;#x27;s system builds what the company calls a &amp;quot;knowledge graph&amp;quot; that maps relationships across codebases, telemetry data, engineering discussions, and internal documentation. When an incident occurs, multiple AI agents work together to form hypotheses, test them against live system evidence, and converge on a root cause — mimicking the investigative workflow of experienced site reliability engineers, but completing the process in minutes rather than hours.&lt;/p&gt;&lt;p&gt;The technology has already shown measurable impact at some of the world&amp;#x27;s most demanding production environments. &lt;a href="https://www.deductive.ai/blogs/how-doordash-powers-a-reliable-high-performance-ad-platform-with-deductive-ai"&gt;&lt;u&gt;DoorDash&amp;#x27;s advertising platform&lt;/u&gt;&lt;/a&gt;, which runs real-time auctions that must complete in under 100 milliseconds, has integrated Deductive into its incident response workflow. The company has set an ambitious 2026 goal of resolving production incidents within 10 minutes.&lt;/p&gt;&lt;p&gt;&amp;quot;Our Ads Platform operates at a pace where manual, slow-moving investigations are no longer viable. Every minute of downtime directly affects company revenue,&amp;quot; said Shahrooz Ansari, Senior Director of Engineering at DoorDash, in an interview with VentureBeat. &amp;quot;Deductive has become a critical extension of our team, rapidly synthesizing signals across dozens of services and surfacing the insights that matter—within minutes.&amp;quot;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt; estimates that Deductive has root-caused approximately 100 production incidents over the past few months, translating to more than 1,000 hours of annual engineering productivity and a revenue impact &amp;quot;in millions of dollars,&amp;quot; according to Ansari. At location intelligence company &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, Deductive reduced the time to diagnose Apache Spark job failures by 90% —t urning a process that previously took hours or days into one that completes in under 10 minutes — while generating over $275,000 in annual savings.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why AI-generated code is creating a debugging crisis&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The timing of Deductive&amp;#x27;s launch reflects a brewing tension in software development: AI coding assistants are enabling engineers to generate code faster than ever, but the resulting software is often harder to understand and maintain.&lt;/p&gt;&lt;p&gt;&amp;quot;&lt;a href="https://x.com/karpathy/status/1886192184808149383?lang=en"&gt;&lt;u&gt;Vibe coding&lt;/u&gt;&lt;/a&gt;,&amp;quot; a term popularized by AI researcher &lt;a href="https://karpathy.ai/"&gt;&lt;u&gt;Andrej Karpathy&lt;/u&gt;&lt;/a&gt;, refers to using natural-language prompts to generate code through AI assistants. While these tools accelerate development, they can introduce what Agarwal describes as &amp;quot;redundancies, breaks in architectural boundaries, assumptions, or ignored design patterns&amp;quot; that accumulate over time.&lt;/p&gt;&lt;p&gt;&amp;quot;Most AI-generated code still introduces redundancies, breaks architectural boundaries, makes assumptions, or ignores established design patterns,&amp;quot; Agarwal told Venturebeat. &amp;quot;In many ways, we now need AI to help clean up the mess that AI itself is creating.&amp;quot;&lt;/p&gt;&lt;p&gt;The claim that engineers spend roughly half their time on debugging isn&amp;#x27;t hyperbole. The Association for Computing Machinery reports that developers spend &lt;a href="https://queue.acm.org/detail.cfm?id=3404974"&gt;&lt;u&gt;35% to 50% of their time validating and debugging software&lt;/u&gt;&lt;/a&gt;. More recently, &lt;a href="https://www.harness.io/blog/announcing-harness-ai"&gt;&lt;u&gt;Harness&amp;#x27;s State of Software Delivery 2025&lt;/u&gt;&lt;/a&gt; report found that 67% of developers are spending more time debugging AI-generated code.&lt;/p&gt;&lt;p&gt;&amp;quot;We&amp;#x27;ve seen world-class engineers spending half of their time debugging instead of building,&amp;quot; said Rakesh Kothari, Deductive&amp;#x27;s co-founder and CEO. &amp;quot;And as vibe coding generates new code at a rate we&amp;#x27;ve never seen, this problem is only going to get worse.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How Deductive&amp;#x27;s AI agents actually investigate production failures&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Deductive&amp;#x27;s technical approach differs substantially from the AI features being added to existing observability platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://newrelic.com/"&gt;&lt;u&gt;New Relic&lt;/u&gt;&lt;/a&gt;. Most of those systems use large language models to summarize data or identify correlations, but they lack what Agarwal calls &amp;quot;code-aware reasoning&amp;quot;—the ability to understand not just that something broke, but why the code behaves the way it does.&lt;/p&gt;&lt;p&gt;&amp;quot;Most enterprises use multiple observability tools across different teams and services, so no vendor has a single holistic view of how their systems behave, fail, and recover—nor are they able to pair that with an understanding of the code that defines system behavior,&amp;quot; Agarwal explained. &amp;quot;These are key ingredients to resolving software incidents and it is exactly the gap Deductive fills.&amp;quot;&lt;/p&gt;&lt;p&gt;The system connects to existing infrastructure using read-only API access to observability platforms, code repositories, incident management tools, and chat systems. It then continuously builds and updates its knowledge graph, mapping dependencies between services and tracking deployment histories.&lt;/p&gt;&lt;p&gt;When an alert fires, Deductive launches what the company describes as a multi-agent investigation. Different agents specialize in different aspects of the problem: one might analyze recent code changes, another examines trace data, while a third correlates the timing of the incident with recent deployments. The agents share findings and iteratively refine their hypotheses.&lt;/p&gt;&lt;p&gt;The critical difference from rule-based automation is Deductive&amp;#x27;s use of reinforcement learning. The system learns from every incident which investigative steps led to correct diagnoses and which were dead ends. When engineers provide feedback, the system incorporates that signal into its learning model.&lt;/p&gt;&lt;p&gt;&amp;quot;Each time it observes an investigation, it learns which steps, data sources, and decisions led to the right outcome,&amp;quot; Agarwal said. &amp;quot;It learns how to think through problems, not just point them out.&amp;quot;&lt;/p&gt;&lt;p&gt;At DoorDash, a recent latency spike in an API initially appeared to be an isolated service issue. Deductive&amp;#x27;s investigation revealed that the root cause was actually timeout errors from a downstream machine learning platform undergoing a deployment. The system connected these dots by analyzing log volumes, traces, and deployment metadata across multiple services.&lt;/p&gt;&lt;p&gt;&amp;quot;Without Deductive, our team would have had to manually correlate the latency spike across all logs, traces, and deployment histories,&amp;quot; Ansari said. &amp;quot;Deductive was able to explain not just what changed, but how and why it impacted production behavior.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The company keeps humans in the loop—for now&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;While Deductive&amp;#x27;s technology could theoretically push fixes directly to production systems, the company has deliberately chosen to keep humans in the loop—at least for now.&lt;/p&gt;&lt;p&gt;&amp;quot;While our system is capable of deeper automation and could push fixes to production, currently, we recommend precise fixes and mitigations that engineers can review, validate, and apply,&amp;quot; Agarwal said. &amp;quot;We believe maintaining a human in the loop is essential for trust, transparency and operational safety.&amp;quot;&lt;/p&gt;&lt;p&gt;However, he acknowledged that &amp;quot;over time, we do think that deeper automation will come and how humans operate in the loop will evolve.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Databricks and ThoughtSpot veterans bet on reasoning over observability&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The founding team brings deep expertise from building some of Silicon Valley&amp;#x27;s most successful data infrastructure platforms. Agarwal earned his Ph.D. at UC Berkeley, where he created &lt;a href="https://arxiv.org/abs/1203.5485"&gt;&lt;u&gt;BlinkDB&lt;/u&gt;&lt;/a&gt;, an influential system for approximate query processing. He was among the first engineers at &lt;a href="https://www.databricks.com/"&gt;&lt;u&gt;Databricks&lt;/u&gt;&lt;/a&gt;, where he helped build &lt;a href="https://docs.databricks.com/gcp/en/spark/?scid=701Vp000004h4b1IAA&amp;amp;utm_medium=paid+search&amp;amp;utm_source=google&amp;amp;utm_campaign=23156677199&amp;amp;utm_adgroup=189768475320&amp;amp;utm_content=aimax&amp;amp;utm_offer=aimax&amp;amp;utm_ad=779965794184&amp;amp;utm_term=apache%20iceberg%20spark&amp;amp;gad_source=1&amp;amp;gad_campaignid=23156677199&amp;amp;gbraid=0AAAAABYBeAhSzbWjXCf1Ok8HU2XzyuNAb&amp;amp;gclid=CjwKCAiA_dDIBhB6EiwAvzc1cN1MnT40-rmesA_-YwBm870Sksy-DQYqWaR9mqQLAIQjzo7yRJpIfBoC7GsQAvD_BwE"&gt;&lt;u&gt;Apache Spark&lt;/u&gt;&lt;/a&gt;. Kothari was an early engineer at &lt;a href="https://www.thoughtspot.com/"&gt;&lt;u&gt;ThoughtSpot&lt;/u&gt;&lt;/a&gt;, where he led teams focused on distributed query processing and large-scale system optimization.&lt;/p&gt;&lt;p&gt;The investor syndicate reflects both the technical credibility and market opportunity. Beyond CRV&amp;#x27;s &lt;a href="https://www.crv.com/team/max-gazor"&gt;&lt;u&gt;Max Gazor&lt;/u&gt;&lt;/a&gt;, the round included participation from &lt;a href="https://sequoiacap.com/podcast/training-data-ion-stoica/"&gt;&lt;u&gt;Ion Stoica&lt;/u&gt;&lt;/a&gt;, founder of Databricks and Anyscale; &lt;a href="https://www.thoughtspot.com/author/ajeet-singh"&gt;&lt;u&gt;Ajeet Singh&lt;/u&gt;&lt;/a&gt;, founder of Nutanix and ThoughtSpot; and &lt;a href="http://bensigelman.org/"&gt;&lt;u&gt;Ben Sigelman&lt;/u&gt;&lt;/a&gt;, founder of Lightstep.&lt;/p&gt;&lt;p&gt;Rather than competing with platforms like &lt;a href="https://www.datadoghq.com/"&gt;&lt;u&gt;Datadog&lt;/u&gt;&lt;/a&gt; or &lt;a href="https://www.pagerduty.com/"&gt;&lt;u&gt;PagerDuty&lt;/u&gt;&lt;/a&gt;, Deductive positions itself as a complementary layer that sits on top of existing tools. The pricing model reflects this: Instead of charging based on data volume, Deductive charges based on the number of incidents investigated, plus a base platform fee.&lt;/p&gt;&lt;p&gt;The company offers both cloud-hosted and self-hosted deployment options and emphasizes that it doesn&amp;#x27;t store customer data on its servers or use it to train models for other customers — a critical assurance given the proprietary nature of both code and production system behavior.&lt;/p&gt;&lt;p&gt;With fresh capital and early customer traction at companies like &lt;a href="https://www.doordash.com/"&gt;&lt;u&gt;DoorDash&lt;/u&gt;&lt;/a&gt;, &lt;a href="https://foursquare.com/"&gt;&lt;u&gt;Foursquare&lt;/u&gt;&lt;/a&gt;, and &lt;a href="https://kumo.ai/"&gt;&lt;u&gt;Kumo AI&lt;/u&gt;&lt;/a&gt;, Deductive plans to expand its team and deepen the system&amp;#x27;s reasoning capabilities from reactive incident analysis to proactive prevention. The near-term vision: helping teams predict problems before they occur.&lt;/p&gt;&lt;p&gt;DoorDash&amp;#x27;s Ansari offers a pragmatic endorsement of where the technology stands today: &amp;quot;Investigations that were previously manual and time-consuming are now automated, allowing engineers to shift their energy toward prevention, business impact, and innovation.&amp;quot;&lt;/p&gt;&lt;p&gt;In an industry where every second of downtime translates to lost revenue, that shift from firefighting to building increasingly looks less like a luxury and more like table stakes.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/how-deductive-ai-saved-doordash-1-000-engineering-hours-by-automating</guid><pubDate>Wed, 12 Nov 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Faster Than a Click: Hyperlink Agent Search Now Available on NVIDIA RTX PCs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-nexa-hyperlink-local-agent/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Large language model (LLM)-based AI assistants are powerful productivity tools, but without the right context and information, they can struggle to provide nuanced, relevant answers. While most LLM-based chat apps allow users to supply a few files for context, they often don’t have access to all the information buried across slides, notes, PDFs and images in a user’s PC.&lt;/p&gt;
&lt;p&gt;Nexa.ai’s Hyperlink is a local AI agent that addresses this challenge. It can quickly index thousands of files, understand the intent of a user’s question and provide contextual, tailored insights.&lt;/p&gt;
&lt;p&gt;A new version of the app, available today, includes accelerations for NVIDIA RTX AI PCs, tripling retrieval-augmented generation indexing speed. For example, a dense 1GB folder that would previously take almost 15 minutes to index can now be ready for search in just four to five minutes. In addition, LLM inference is accelerated by 2x for faster responses to user queries.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87266"&gt;&lt;img alt="alt" class="size-large wp-image-87266" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/Nexa-Benchmarks-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87266"&gt;Hyperlink on NVIDIA RTX AI PCs delivers up to 3x faster indexing and 2x faster LLM inference. Benchmarked on an RTX 5090 using a test dataset; indexing measured as total index time, inference measured in tokens per second.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Turn Local Data Into Instant Intelligence&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Hyperlink uses generative AI to search thousands of files for the right information, understanding the intent and context of a user’s query, rather than merely matching keywords.&lt;/p&gt;
&lt;p&gt;To do this, it creates a searchable index of all local files a user indicates — whether a small folder or every single file on a computer. Users can describe what they’re looking for in natural language and find relevant content across documents, slides, PDFs and images.&lt;/p&gt;
&lt;p&gt;For example, if a user asks for help with a “Sci-Fi book report comparing themes between two novels,” Hyperlink can find the relevant information — even if it’s saved in a file named “Lit_Homework_Final.docx.”&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Combining search with the reasoning capabilities of RTX-accelerated LLMs, Hyperlink then answers questions based on insights from a user’s files. It connects ideas across sources, identifies relationships between documents and generates well-reasoned answers with clear citations.&lt;/p&gt;
&lt;p&gt;All user data stays on the device and is kept private. This means personal files never leave the computer, so users don’t have to worry about sensitive information being sent to the cloud. They get the benefits of powerful AI without sacrificing control or peace of mind.&lt;/p&gt;
&lt;p&gt;Hyperlink is already being adopted by professionals, students and creators to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Prepare for meetings&lt;/b&gt;: Summarize key discussion points across notes and transcripts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Analyze reports&lt;/b&gt;: Get well-researched answers, citing key data points from across industry reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Create content: &lt;/b&gt;Compile writing or video ideas from years of saved notes and drafts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Study smarter: &lt;/b&gt;Review a key concept while cramming for a test, searching through lecture notes, slides and tutorials — all at once.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Organize receipts: &lt;/b&gt;Sort scanned documents and automatically complete expense reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Debug faster:&lt;/b&gt; Search across documentation and comments in code to resolve errors or version conflicts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download the Hyperlink app to start experimenting with AI search on RTX PCs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;Large language model (LLM)-based AI assistants are powerful productivity tools, but without the right context and information, they can struggle to provide nuanced, relevant answers. While most LLM-based chat apps allow users to supply a few files for context, they often don’t have access to all the information buried across slides, notes, PDFs and images in a user’s PC.&lt;/p&gt;
&lt;p&gt;Nexa.ai’s Hyperlink is a local AI agent that addresses this challenge. It can quickly index thousands of files, understand the intent of a user’s question and provide contextual, tailored insights.&lt;/p&gt;
&lt;p&gt;A new version of the app, available today, includes accelerations for NVIDIA RTX AI PCs, tripling retrieval-augmented generation indexing speed. For example, a dense 1GB folder that would previously take almost 15 minutes to index can now be ready for search in just four to five minutes. In addition, LLM inference is accelerated by 2x for faster responses to user queries.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_87266"&gt;&lt;img alt="alt" class="size-large wp-image-87266" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/Nexa-Benchmarks-1680x945.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-87266"&gt;Hyperlink on NVIDIA RTX AI PCs delivers up to 3x faster indexing and 2x faster LLM inference. Benchmarked on an RTX 5090 using a test dataset; indexing measured as total index time, inference measured in tokens per second.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Turn Local Data Into Instant Intelligence&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Hyperlink uses generative AI to search thousands of files for the right information, understanding the intent and context of a user’s query, rather than merely matching keywords.&lt;/p&gt;
&lt;p&gt;To do this, it creates a searchable index of all local files a user indicates — whether a small folder or every single file on a computer. Users can describe what they’re looking for in natural language and find relevant content across documents, slides, PDFs and images.&lt;/p&gt;
&lt;p&gt;For example, if a user asks for help with a “Sci-Fi book report comparing themes between two novels,” Hyperlink can find the relevant information — even if it’s saved in a file named “Lit_Homework_Final.docx.”&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Combining search with the reasoning capabilities of RTX-accelerated LLMs, Hyperlink then answers questions based on insights from a user’s files. It connects ideas across sources, identifies relationships between documents and generates well-reasoned answers with clear citations.&lt;/p&gt;
&lt;p&gt;All user data stays on the device and is kept private. This means personal files never leave the computer, so users don’t have to worry about sensitive information being sent to the cloud. They get the benefits of powerful AI without sacrificing control or peace of mind.&lt;/p&gt;
&lt;p&gt;Hyperlink is already being adopted by professionals, students and creators to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Prepare for meetings&lt;/b&gt;: Summarize key discussion points across notes and transcripts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Analyze reports&lt;/b&gt;: Get well-researched answers, citing key data points from across industry reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Create content: &lt;/b&gt;Compile writing or video ideas from years of saved notes and drafts.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Study smarter: &lt;/b&gt;Review a key concept while cramming for a test, searching through lecture notes, slides and tutorials — all at once.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Organize receipts: &lt;/b&gt;Sort scanned documents and automatically complete expense reports.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Debug faster:&lt;/b&gt; Search across documentation and comments in code to resolve errors or version conflicts.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Download the Hyperlink app to start experimenting with AI search on RTX PCs.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;. Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-nexa-hyperlink-local-agent/</guid><pubDate>Wed, 12 Nov 2025 14:00:21 +0000</pubDate></item><item><title>[NEW] Differentially private machine learning at scale with JAX-Privacy (The latest research from Google)</title><link>https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.&lt;/p&gt;&lt;p&gt;That’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation, just-in-time compilation, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax, which simplifies the implementation of neural network architectures, and Optax, which implements state-of-the-art optimizers.&lt;/p&gt;&lt;p&gt;Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.&lt;/p&gt;&lt;p&gt;Today, we are proud to announce the release of JAX-Privacy 1.0. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.&lt;/p&gt;&lt;p&gt;That’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation, just-in-time compilation, and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax, which simplifies the implementation of neural network architectures, and Optax, which implements state-of-the-art optimizers.&lt;/p&gt;&lt;p&gt;Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training. It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.&lt;/p&gt;&lt;p&gt;Today, we are proud to announce the release of JAX-Privacy 1.0. Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/</guid><pubDate>Wed, 12 Nov 2025 15:32:00 +0000</pubDate></item><item><title>[NEW] Anthropic announces $50 billion data center plan (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/anthropic-announces-50-billion-data-center-plan/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-2154160973-e1723115200227.jpg?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday said it had signed an ambitious new data center partnership with U.K.-based neocloud provider Fluidstack, committing $50 billion to building facilities across the U.S. to meet its growing compute needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data centers will be located in Texas and New York, and come online throughout 2026. The company described the sites as “custom built for Anthropic with a focus on maximizing efficiency for our workloads.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re getting closer to AI that can accelerate scientific discovery and help solve complex problems in ways that weren’t possible before,” Anthropic’s CEO and co-founder Dario Amodei (pictured above) said in a statement. “Realizing that potential requires infrastructure that can support continued development at the frontier.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of the intense compute demands of Anthropic’s Claude family of models, the company is already engaged in significant cloud partnerships with both Google and Amazon (which is also an investor). But this is the company’s first major effort to build custom infrastructure. The $50 billion outlay, while large, is in line with the company’s internal revenue projections, which reportedly see Anthropic reaching $70 billion in revenue and $17 billion in positive cash flow by 2028.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While $50 billion represents a massive commitment in both cash and compute power, it is nonetheless dwarfed by similar projects from Anthropic’s competitors. Meta has committed to building $600 billion worth of data centers over the next three years, while the Stargate partnership between SoftBank, OpenAI and Oracle has already planned $500 billion in infrastructure spending. The spending has fueled concerns about an AI bubble due to flagging demand or even misallocated spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project marks a major success for Fluidstack, a relatively young neocloud company that has become a vendor of choice in the AI building boom. Founded in 2017, the company was named in February as the primary partner for a 1 gigawatt AI project backed by the French government, which represented more than $11 billion in spending. According to Forbes, the company already has partnerships in place with Meta, Black Forest Labs, and France’s Mistral.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fluidstack was also one of the first third-party vendors to receive Google’s custom-built TPUs, a major vote of confidence for the company.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-2154160973-e1723115200227.jpg?resize=1200,673" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic on Wednesday said it had signed an ambitious new data center partnership with U.K.-based neocloud provider Fluidstack, committing $50 billion to building facilities across the U.S. to meet its growing compute needs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The data centers will be located in Texas and New York, and come online throughout 2026. The company described the sites as “custom built for Anthropic with a focus on maximizing efficiency for our workloads.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“We’re getting closer to AI that can accelerate scientific discovery and help solve complex problems in ways that weren’t possible before,” Anthropic’s CEO and co-founder Dario Amodei (pictured above) said in a statement. “Realizing that potential requires infrastructure that can support continued development at the frontier.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Because of the intense compute demands of Anthropic’s Claude family of models, the company is already engaged in significant cloud partnerships with both Google and Amazon (which is also an investor). But this is the company’s first major effort to build custom infrastructure. The $50 billion outlay, while large, is in line with the company’s internal revenue projections, which reportedly see Anthropic reaching $70 billion in revenue and $17 billion in positive cash flow by 2028.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While $50 billion represents a massive commitment in both cash and compute power, it is nonetheless dwarfed by similar projects from Anthropic’s competitors. Meta has committed to building $600 billion worth of data centers over the next three years, while the Stargate partnership between SoftBank, OpenAI and Oracle has already planned $500 billion in infrastructure spending. The spending has fueled concerns about an AI bubble due to flagging demand or even misallocated spending.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The project marks a major success for Fluidstack, a relatively young neocloud company that has become a vendor of choice in the AI building boom. Founded in 2017, the company was named in February as the primary partner for a 1 gigawatt AI project backed by the French government, which represented more than $11 billion in spending. According to Forbes, the company already has partnerships in place with Meta, Black Forest Labs, and France’s Mistral.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Fluidstack was also one of the first third-party vendors to receive Google’s custom-built TPUs, a major vote of confidence for the company.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/anthropic-announces-50-billion-data-center-plan/</guid><pubDate>Wed, 12 Nov 2025 15:52:47 +0000</pubDate></item><item><title>[NEW] AI data startup WisdomAI has raised another $50M, led by Kleiner, Nvidia (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/12/ai-data-startup-wisdomai-has-raised-another-50m-led-by-kleiner-nvidia/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/WisdomAI-cofounders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;WisdomAI, the new AI data analytics startup from Rubrik co-founder Soham Mazumdar, has landed a fresh $50 million Series A led by Kleiner Perkins with participation from new investor NVentures (Nvidia’s venture capital arm).&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This round comes roughly six months after the startup announced a seed round of $23 million led by Coatue.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;WisdomAI offers AI-driven data analytics that can answer business questions from structured, unstructured, and even “dirty” data, meaning data that hasn’t been cleaned of typos or errors.&amp;nbsp;A business user simply asks questions in natural language, such as, “How many customers do I have in my pipeline and what’s preventing them from closing this quarter?”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the company is using a clever method to eliminate the LLM hallucination problem. WisdomAI is not using LLMs to write answers to questions. Instead, LLMs are only used to write the query —  the part that will go out to a data warehouse to retrieve data. So if the LLM hallucinates, it will simply write an ineffective query rather than inventing false answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;WisdomAI has written its own logic that it calls the “enterprise context layer,” which studies the customer data to understand it. All of the startup’s co-founders worked with Mazumdar at data security company Rubrik, giving them deep experience with enterprise storage warehouses. (Mazumdar left Rubrik in 2023.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since formally launching in late 2024, WisdomAI has grown from two enterprise customers to around 40 enterprise customers, CEO Mazumdar says. WisdomAI counts such companies like Descope, ConocoPhillips, Cisco, and Patreon as customers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mazumdar said the young company is also growing usage within its enterprise customers. Some customers have doubled usage within two months. Another customer started with 10 seats and expanded to 450, which is almost everyone in the company, he said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the last six months, WisdomAI has also added an agentic feature that will alert users in real time to important changes in situations they are monitoring.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I have created an agent which is watching our product usage metrics, our ticket information,” Mazumdar says, adding that it took him about five minutes to create it. But rather than sending him a daily or, even, hourly report on usage and helpdesk tickets, it alerts him “when something interesting happens,” he describes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think that’s the magic with analytics. It’s always been a static report, but we are making it dynamic. We are making it proactive,” he said.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/05/WisdomAI-cofounders.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;WisdomAI, the new AI data analytics startup from Rubrik co-founder Soham Mazumdar, has landed a fresh $50 million Series A led by Kleiner Perkins with participation from new investor NVentures (Nvidia’s venture capital arm).&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This round comes roughly six months after the startup announced a seed round of $23 million led by Coatue.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;WisdomAI offers AI-driven data analytics that can answer business questions from structured, unstructured, and even “dirty” data, meaning data that hasn’t been cleaned of typos or errors.&amp;nbsp;A business user simply asks questions in natural language, such as, “How many customers do I have in my pipeline and what’s preventing them from closing this quarter?”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But the company is using a clever method to eliminate the LLM hallucination problem. WisdomAI is not using LLMs to write answers to questions. Instead, LLMs are only used to write the query —  the part that will go out to a data warehouse to retrieve data. So if the LLM hallucinates, it will simply write an ineffective query rather than inventing false answers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;WisdomAI has written its own logic that it calls the “enterprise context layer,” which studies the customer data to understand it. All of the startup’s co-founders worked with Mazumdar at data security company Rubrik, giving them deep experience with enterprise storage warehouses. (Mazumdar left Rubrik in 2023.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since formally launching in late 2024, WisdomAI has grown from two enterprise customers to around 40 enterprise customers, CEO Mazumdar says. WisdomAI counts such companies like Descope, ConocoPhillips, Cisco, and Patreon as customers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mazumdar said the young company is also growing usage within its enterprise customers. Some customers have doubled usage within two months. Another customer started with 10 seats and expanded to 450, which is almost everyone in the company, he said.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;In the last six months, WisdomAI has also added an agentic feature that will alert users in real time to important changes in situations they are monitoring.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I have created an agent which is watching our product usage metrics, our ticket information,” Mazumdar says, adding that it took him about five minutes to create it. But rather than sending him a daily or, even, hourly report on usage and helpdesk tickets, it alerts him “when something interesting happens,” he describes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think that’s the magic with analytics. It’s always been a static report, but we are making it dynamic. We are making it proactive,” he said.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/12/ai-data-startup-wisdomai-has-raised-another-50m-led-by-kleiner-nvidia/</guid><pubDate>Wed, 12 Nov 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] NVIDIA Wins Every MLPerf Training v5.1 Benchmark (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/mlperf-training-benchmark-blackwell-ultra/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In the age of AI reasoning, training smarter, more capable models is critical to scaling intelligence. Delivering the massive performance to meet this new age requires breakthroughs across GPUs, CPUs, NICs, scale-up and scale-out networking, system architectures, and mountains of software and algorithms.&lt;/p&gt;
&lt;p&gt;In MLPerf Training v5.1 — the latest round in a long-running series of industry-standard tests of AI training performance — NVIDIA swept all seven tests, delivering the fastest time to train across large language models (LLMs), image generation, recommender systems, computer vision and graph neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87247 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-wins-every-mlperf-training-benchmark-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA was also the only platform to submit results on every test, underscoring the rich programmability of NVIDIA GPUs, and the maturity and versatility of its CUDA software stack.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Ultra Doubles Down&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The GB300 NVL72 rack-scale system, powered by the NVIDIA Blackwell Ultra GPU architecture, made its debut in MLPerf Training this round, following a record-setting showing in the most recent MLPerf Inference round.&lt;/p&gt;
&lt;p&gt;Compared with the prior-generation Hopper architecture, the Blackwell Ultra-based GB300 NVL72 delivered more than 4x the Llama 3.1 405B pretraining and nearly 5x the Llama 2 70B LoRA fine-tuning performance using the same number of GPUs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87238 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-blackwell-ultra-delivers-large-training-leap-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;These gains were fueled by Blackwell Ultra’s architectural improvements — including new Tensor Cores that offer 15 petaflops of NVFP4 AI compute, twice the attention-layer compute and 279GB of HBM3e memory — as well as new training methods that tapped into the architecture’s enormous NVFP4 compute performance.&lt;/p&gt;
&lt;p&gt;Connecting multiple GB300 NVL72 systems, the NVIDIA Quantum-X800 InfiniBand platform — the industry’s first end-to-end 800 Gb/s scale-up networking platform — also made its MLPerf debut, doubling scale-out networking bandwidth compared with the prior generation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Performance Unlocked: NVFP4 Accelerates LLM Training&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Key to the outstanding results this round was performing calculations using NVFP4 precision — a first in the history of MLPerf Training.&lt;/p&gt;
&lt;p&gt;One way to increase compute performance is to build an architecture capable of performing computations on data represented with fewer bits, and then to perform those calculations at a faster rate. However, lower precision means less information is available in each calculation. This means using low-precision calculations in the training process calls for careful design decisions to keep results accurate.&lt;/p&gt;
&lt;p&gt;NVIDIA teams innovated at every layer of the stack to adopt FP4 precision for LLM training. The NVIDIA Blackwell GPU can perform FP4 calculations — including the NVIDIA-designed NVFP4 format as well as other FP4 variants — at double the rate of FP8. Blackwell Ultra boosts that to 3x, enabling the GPUs to deliver substantially greater AI compute performance.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only platform to date that has submitted MLPerf Training results with calculations performed using FP4 precision while meeting the benchmark’s strict accuracy requirements.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Scales to New Heights&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA set a new Llama 3.1 405B time-to-train record of just 10 minutes, powered by more than 5,000 Blackwell GPUs working together efficiently. This entry was 2.7x faster than the best Blackwell-based result submitted in the prior round, resulting from efficient scaling to more than twice the number of GPUs, as well as the use of NVFP4 precision to dramatically increase the effective performance of each Blackwell GPU.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87244 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-blackwell-delivers-massive-boost-for-large-scale-training-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;To illustrate the performance increase per GPU, NVIDIA submitted results this round using 2,560 Blackwell GPUs, achieving a time to train of 18.79 minutes — 45% faster than the submission last round using 2,496 GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;New Benchmarks, New Records&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA also set performance records on the two new benchmarks added this round: Llama 3.1 8B and FLUX.1.&lt;/p&gt;
&lt;p&gt;Llama 3.1 8B — a compact yet highly capable LLM — replaced the long-running BERT-large model, adding a modern, smaller LLM to the benchmark suite. NVIDIA submitted results with up to 512 Blackwell Ultra GPUs, setting the bar at 5.2 minutes to train.&lt;/p&gt;
&lt;p&gt;In addition, FLUX.1 — a state-of-the-art image generation model — replaced Stable Diffusion v2, with only the NVIDIA platform submitting results on the benchmark. NVIDIA submitted results using 1,152 Blackwell GPUs, setting a record time to train of 12.5 minutes.&lt;/p&gt;
&lt;p&gt;NVIDIA continued to hold records on the existing graph neural network, object detection and recommender system tests.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;A Broad and Deep Partner Ecosystem&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA ecosystem participated extensively this round, with compelling submissions from 15 organizations including ASUSTeK, Dell Technologies, Giga Computing, Hewlett Packard Enterprise, Krai, Lambda, Lenovo, Nebius, Quanta Cloud Technology, Supermicro, University of Florida, Verda (formerly DataCrunch) and Wiwynn.&lt;/p&gt;
&lt;p&gt;NVIDIA is innovating at a one-year rhythm, driving significant and rapid performance increases across pretraining, post-training and inference — paving the way to new levels of intelligence and accelerating AI adoption.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See more NVIDIA performance data on the &lt;/i&gt;&lt;i&gt;Data Center Deep Learning Product Performance Hub&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Performance Explorer&lt;/i&gt;&lt;i&gt; pages.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;In the age of AI reasoning, training smarter, more capable models is critical to scaling intelligence. Delivering the massive performance to meet this new age requires breakthroughs across GPUs, CPUs, NICs, scale-up and scale-out networking, system architectures, and mountains of software and algorithms.&lt;/p&gt;
&lt;p&gt;In MLPerf Training v5.1 — the latest round in a long-running series of industry-standard tests of AI training performance — NVIDIA swept all seven tests, delivering the fastest time to train across large language models (LLMs), image generation, recommender systems, computer vision and graph neural networks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87247 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-wins-every-mlperf-training-benchmark-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA was also the only platform to submit results on every test, underscoring the rich programmability of NVIDIA GPUs, and the maturity and versatility of its CUDA software stack.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Ultra Doubles Down&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The GB300 NVL72 rack-scale system, powered by the NVIDIA Blackwell Ultra GPU architecture, made its debut in MLPerf Training this round, following a record-setting showing in the most recent MLPerf Inference round.&lt;/p&gt;
&lt;p&gt;Compared with the prior-generation Hopper architecture, the Blackwell Ultra-based GB300 NVL72 delivered more than 4x the Llama 3.1 405B pretraining and nearly 5x the Llama 2 70B LoRA fine-tuning performance using the same number of GPUs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87238 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-blackwell-ultra-delivers-large-training-leap-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;These gains were fueled by Blackwell Ultra’s architectural improvements — including new Tensor Cores that offer 15 petaflops of NVFP4 AI compute, twice the attention-layer compute and 279GB of HBM3e memory — as well as new training methods that tapped into the architecture’s enormous NVFP4 compute performance.&lt;/p&gt;
&lt;p&gt;Connecting multiple GB300 NVL72 systems, the NVIDIA Quantum-X800 InfiniBand platform — the industry’s first end-to-end 800 Gb/s scale-up networking platform — also made its MLPerf debut, doubling scale-out networking bandwidth compared with the prior generation.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Performance Unlocked: NVFP4 Accelerates LLM Training&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Key to the outstanding results this round was performing calculations using NVFP4 precision — a first in the history of MLPerf Training.&lt;/p&gt;
&lt;p&gt;One way to increase compute performance is to build an architecture capable of performing computations on data represented with fewer bits, and then to perform those calculations at a faster rate. However, lower precision means less information is available in each calculation. This means using low-precision calculations in the training process calls for careful design decisions to keep results accurate.&lt;/p&gt;
&lt;p&gt;NVIDIA teams innovated at every layer of the stack to adopt FP4 precision for LLM training. The NVIDIA Blackwell GPU can perform FP4 calculations — including the NVIDIA-designed NVFP4 format as well as other FP4 variants — at double the rate of FP8. Blackwell Ultra boosts that to 3x, enabling the GPUs to deliver substantially greater AI compute performance.&lt;/p&gt;
&lt;p&gt;NVIDIA is the only platform to date that has submitted MLPerf Training results with calculations performed using FP4 precision while meeting the benchmark’s strict accuracy requirements.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;NVIDIA Blackwell Scales to New Heights&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA set a new Llama 3.1 405B time-to-train record of just 10 minutes, powered by more than 5,000 Blackwell GPUs working together efficiently. This entry was 2.7x faster than the best Blackwell-based result submitted in the prior round, resulting from efficient scaling to more than twice the number of GPUs, as well as the use of NVFP4 precision to dramatically increase the effective performance of each Blackwell GPU.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-87244 size-large" height="945" src="https://blogs.nvidia.com/wp-content/uploads/2025/11/nvidia-blackwell-delivers-massive-boost-for-large-scale-training-1680x945.png" width="1680" /&gt;&lt;/p&gt;
&lt;p&gt;To illustrate the performance increase per GPU, NVIDIA submitted results this round using 2,560 Blackwell GPUs, achieving a time to train of 18.79 minutes — 45% faster than the submission last round using 2,496 GPUs.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;New Benchmarks, New Records&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;NVIDIA also set performance records on the two new benchmarks added this round: Llama 3.1 8B and FLUX.1.&lt;/p&gt;
&lt;p&gt;Llama 3.1 8B — a compact yet highly capable LLM — replaced the long-running BERT-large model, adding a modern, smaller LLM to the benchmark suite. NVIDIA submitted results with up to 512 Blackwell Ultra GPUs, setting the bar at 5.2 minutes to train.&lt;/p&gt;
&lt;p&gt;In addition, FLUX.1 — a state-of-the-art image generation model — replaced Stable Diffusion v2, with only the NVIDIA platform submitting results on the benchmark. NVIDIA submitted results using 1,152 Blackwell GPUs, setting a record time to train of 12.5 minutes.&lt;/p&gt;
&lt;p&gt;NVIDIA continued to hold records on the existing graph neural network, object detection and recommender system tests.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;A Broad and Deep Partner Ecosystem&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The NVIDIA ecosystem participated extensively this round, with compelling submissions from 15 organizations including ASUSTeK, Dell Technologies, Giga Computing, Hewlett Packard Enterprise, Krai, Lambda, Lenovo, Nebius, Quanta Cloud Technology, Supermicro, University of Florida, Verda (formerly DataCrunch) and Wiwynn.&lt;/p&gt;
&lt;p&gt;NVIDIA is innovating at a one-year rhythm, driving significant and rapid performance increases across pretraining, post-training and inference — paving the way to new levels of intelligence and accelerating AI adoption.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See more NVIDIA performance data on the &lt;/i&gt;&lt;i&gt;Data Center Deep Learning Product Performance Hub&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Performance Explorer&lt;/i&gt;&lt;i&gt; pages.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/mlperf-training-benchmark-blackwell-ultra/</guid><pubDate>Wed, 12 Nov 2025 16:00:35 +0000</pubDate></item><item><title>[NEW] Baidu ERNIE multimodal AI beats GPT and Gemini in benchmarks (AI News)</title><link>https://www.artificialintelligence-news.com/news/baidu-ernie-multimodal-ai-gpt-and-gemini-benchmarks/</link><description>&lt;p&gt;Baidu’s latest ERNIE model, a super-efficient multimodal AI, is beating GPT and Gemini on key benchmarks and targets enterprise data often ignored by text-focused models.&lt;/p&gt;&lt;p&gt;For many businesses, valuable insights are locked in engineering schematics, factory-floor video feeds, medical scans, and logistics dashboards. Baidu’s new model, ERNIE-4.5-VL-28B-A3B-Thinking, is designed to fill this gap.&lt;/p&gt;&lt;p&gt;What’s interesting to enterprise architects is not just its multimodal capability, but its architecture. It’s described as a “lightweight” model, activating only three billion parameters during operation. This approach targets the high inference costs that often stall AI-scaling projects. Baidu is betting on efficiency as a path to adoption, training the system as a foundation for “multimodal agents” that can reason and act, not just perceive.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-complex-visual-data-analysis-capabilities-supported-by-ai-benchmarks"&gt;Complex visual data analysis capabilities supported by AI benchmarks&lt;/h3&gt;&lt;p&gt;Baidu’s multimodal ERNIE AI model excels at handling dense, non-text data. For example, it can interpret a “Peak Time Reminder” chart to find optimal visiting hours, a task that reflects the resource-scheduling challenges in logistics or retail.&lt;/p&gt;&lt;p&gt;ERNIE 4.5 also shows capability in technical domains, like solving a bridge circuit diagram by applying Ohm’s and Kirchhoff’s laws. For R&amp;amp;D and engineering arms, a future assistant could validate designs or explain complex schematics to new hires.&lt;/p&gt;&lt;p&gt;This capability is supported by Baidu’s benchmarks, which show ERNIE-4.5-VL-28B-A3B-Thinking outperforming competitors like GPT-5-High and Gemini 2.5 Pro on some key tests:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;MathVista: ERNIE (82.5) vs Gemini (82.3) and GPT (81.3)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;ChartQA: ERNIE (87.1) vs Gemini (76.3) and GPT (78.2)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;VLMs Are Blind: ERNIE (77.3) vs Gemini (76.5) and GPT (69.6)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s worth noting, of course, that AI benchmarks provide a guide but can be flawed. Always perform internal tests for your needs before deploying any AI model for mission-critical applications.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-baidu-shifts-from-perception-to-automation-with-its-latest-ernie-ai-model"&gt;Baidu shifts from perception to automation with its latest ERNIE AI model&lt;/h3&gt;&lt;p&gt;The primary hurdle for enterprise AI is moving from perception (“what is this?”) to automation (“what now?”). ERNIE 4.5 claims to address this by integrating visual grounding with tool use.&lt;/p&gt;&lt;p&gt;Asking the multimodal AI to find all people wearing suits in an image and return their coordinates in JSON format works. The model generates the structured data, a function easily transferable to a production line for visual inspection or to a system auditing site images for safety compliance.&lt;/p&gt;&lt;p&gt;The model also manages external tools and can autonomously zoom in on a photograph to read small text. If it faces an unknown object, it can trigger an image search to identify it. This represents a less passive form of AI that could power an agent to not only flag a data centre error, but also zoom in on the code, search the internal knowledge base, and suggest the fix.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-unlocking-business-intelligence-with-multimodal-ai"&gt;Unlocking business intelligence with multimodal AI&lt;/h3&gt;&lt;p&gt;Baidu’s latest ERNIE AI model also targets corporate video archives from training sessions and meetings to security footage. It can extract all on-screen subtitles and map them to their precise timestamps.&lt;/p&gt;&lt;p&gt;It also demonstrates temporal awareness, finding specific scenes (like those “filmed on a bridge”) by analysing visual cues. The clear end-goal is making vast video libraries searchable, allowing an employee to find the exact moment a specific topic was discussed in a two-hour webinar they may have dozed off a couple of times during.&lt;/p&gt;&lt;p&gt;Baidu provides deployment guidance for several paths, including transformers, vLLM, and FastDeploy. However, the hardware requirements are a major barrier. A single-card deployment needs 80GB of GPU memory. This is not a tool for casual experimentation, but for organisations with existing and high-performance AI infrastructure.&lt;/p&gt;&lt;p&gt;For those with the hardware, Baidu’s ERNIEKit toolkit allows fine-tuning on proprietary data; a necessity for most high-value use cases. Baidu is providing its latest ERNIE AI model with an Apache 2.0 licence that permits commercial use, which is essential for adoption.&lt;/p&gt;&lt;p&gt;The market is finally moving toward multimodal AI that can see, read, and act within a specific business context, and the benchmarks suggest it’s doing so with impressive capability. The immediate task is to identify high-value visual reasoning jobs within your own operation and weigh them against the substantial hardware and governance costs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Wiz: Security lapses emerge amid the global AI race&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Baidu’s latest ERNIE model, a super-efficient multimodal AI, is beating GPT and Gemini on key benchmarks and targets enterprise data often ignored by text-focused models.&lt;/p&gt;&lt;p&gt;For many businesses, valuable insights are locked in engineering schematics, factory-floor video feeds, medical scans, and logistics dashboards. Baidu’s new model, ERNIE-4.5-VL-28B-A3B-Thinking, is designed to fill this gap.&lt;/p&gt;&lt;p&gt;What’s interesting to enterprise architects is not just its multimodal capability, but its architecture. It’s described as a “lightweight” model, activating only three billion parameters during operation. This approach targets the high inference costs that often stall AI-scaling projects. Baidu is betting on efficiency as a path to adoption, training the system as a foundation for “multimodal agents” that can reason and act, not just perceive.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-complex-visual-data-analysis-capabilities-supported-by-ai-benchmarks"&gt;Complex visual data analysis capabilities supported by AI benchmarks&lt;/h3&gt;&lt;p&gt;Baidu’s multimodal ERNIE AI model excels at handling dense, non-text data. For example, it can interpret a “Peak Time Reminder” chart to find optimal visiting hours, a task that reflects the resource-scheduling challenges in logistics or retail.&lt;/p&gt;&lt;p&gt;ERNIE 4.5 also shows capability in technical domains, like solving a bridge circuit diagram by applying Ohm’s and Kirchhoff’s laws. For R&amp;amp;D and engineering arms, a future assistant could validate designs or explain complex schematics to new hires.&lt;/p&gt;&lt;p&gt;This capability is supported by Baidu’s benchmarks, which show ERNIE-4.5-VL-28B-A3B-Thinking outperforming competitors like GPT-5-High and Gemini 2.5 Pro on some key tests:&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;MathVista: ERNIE (82.5) vs Gemini (82.3) and GPT (81.3)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;ChartQA: ERNIE (87.1) vs Gemini (76.3) and GPT (78.2)&lt;/li&gt;&lt;/ul&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;VLMs Are Blind: ERNIE (77.3) vs Gemini (76.5) and GPT (69.6)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;It’s worth noting, of course, that AI benchmarks provide a guide but can be flawed. Always perform internal tests for your needs before deploying any AI model for mission-critical applications.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-baidu-shifts-from-perception-to-automation-with-its-latest-ernie-ai-model"&gt;Baidu shifts from perception to automation with its latest ERNIE AI model&lt;/h3&gt;&lt;p&gt;The primary hurdle for enterprise AI is moving from perception (“what is this?”) to automation (“what now?”). ERNIE 4.5 claims to address this by integrating visual grounding with tool use.&lt;/p&gt;&lt;p&gt;Asking the multimodal AI to find all people wearing suits in an image and return their coordinates in JSON format works. The model generates the structured data, a function easily transferable to a production line for visual inspection or to a system auditing site images for safety compliance.&lt;/p&gt;&lt;p&gt;The model also manages external tools and can autonomously zoom in on a photograph to read small text. If it faces an unknown object, it can trigger an image search to identify it. This represents a less passive form of AI that could power an agent to not only flag a data centre error, but also zoom in on the code, search the internal knowledge base, and suggest the fix.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-unlocking-business-intelligence-with-multimodal-ai"&gt;Unlocking business intelligence with multimodal AI&lt;/h3&gt;&lt;p&gt;Baidu’s latest ERNIE AI model also targets corporate video archives from training sessions and meetings to security footage. It can extract all on-screen subtitles and map them to their precise timestamps.&lt;/p&gt;&lt;p&gt;It also demonstrates temporal awareness, finding specific scenes (like those “filmed on a bridge”) by analysing visual cues. The clear end-goal is making vast video libraries searchable, allowing an employee to find the exact moment a specific topic was discussed in a two-hour webinar they may have dozed off a couple of times during.&lt;/p&gt;&lt;p&gt;Baidu provides deployment guidance for several paths, including transformers, vLLM, and FastDeploy. However, the hardware requirements are a major barrier. A single-card deployment needs 80GB of GPU memory. This is not a tool for casual experimentation, but for organisations with existing and high-performance AI infrastructure.&lt;/p&gt;&lt;p&gt;For those with the hardware, Baidu’s ERNIEKit toolkit allows fine-tuning on proprietary data; a necessity for most high-value use cases. Baidu is providing its latest ERNIE AI model with an Apache 2.0 licence that permits commercial use, which is essential for adoption.&lt;/p&gt;&lt;p&gt;The market is finally moving toward multimodal AI that can see, read, and act within a specific business context, and the benchmarks suggest it’s doing so with impressive capability. The immediate task is to identify high-value visual reasoning jobs within your own operation and weigh them against the substantial hardware and governance costs.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Wiz: Security lapses emerge amid the global AI race&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110077" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-10.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/baidu-ernie-multimodal-ai-gpt-and-gemini-benchmarks/</guid><pubDate>Wed, 12 Nov 2025 16:09:44 +0000</pubDate></item><item><title>[NEW] Meta’s star AI scientist Yann LeCun plans to leave for own startup (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/metas-star-ai-scientist-yann-lecun-plans-to-leave-for-own-startup/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI pioneer reportedly frustrated with Meta’s shift from research to rapid product releases.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="205" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-300x205.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Yann LeCun, Vice President and Chief AI Scientist for Meta Platforms, as seen in September 2023.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Photo by Kevin Dietsch/Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Meta’s chief AI scientist and Turing Award winner Yann LeCun plans to leave the company to launch his own startup focused on a different type of AI called “world models,” the Financial Times reported. The French-US scientist has reportedly told associates he will depart in the coming months and is already in early talks to raise funds for the new venture. The departure comes as CEO Mark Zuckerberg radically overhauled Meta’s AI operations after deciding the company had fallen behind rivals such as OpenAI and Google.&lt;/p&gt;
&lt;p&gt;World models are hypothetical AI systems that some AI engineers expect to develop an internal “understanding” of the physical world by learning from video and spatial data rather than text alone. Unlike current large language models (such as the kind that power ChatGPT) that predict the next segment of data in a sequence, world models would ideally simulate cause-and-effect scenarios, understand physics, and enable machines to reason and plan more like animals do. LeCun has said this architecture could take a decade to fully develop.&lt;/p&gt;
&lt;p&gt;While some AI experts believe that Transformer-based AI models—such as large language models, video synthesis models, and interactive world synthesis models—have emergently modeled physics or absorbed the structural rules of the physical world from training data examples, the evidence so far generally points to sophisticated pattern-matching rather than a base understanding of how the physical world actually works.&lt;/p&gt;
&lt;p&gt;LeCun’s planned exit is the latest in a string of leadership reshuffles at Meta during what has been a tumultuous year for the company. A key turning point was the disappointing launch and benchmark-gaming controversy of the AI language model Llama 4 in April, which many in the industry saw as a flop when it performed worse than the most advanced offerings from Google, OpenAI, and Anthropic. Meanwhile, the Meta AI chatbot has failed to gain traction with consumers and suffered controversies and setbacks over its interactions with children.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;LeCun founded Meta’s Fundamental AI Research lab, known as FAIR, in 2013 and has served as the company’s chief AI scientist ever since. He is one of three researchers who won the 2018 Turing Award for pioneering work on deep learning and convolutional neural networks. After leaving Meta, LeCun will remain a professor at New York University, where he has taught since 2003.&lt;/p&gt;
&lt;p&gt;LeCun has previously argued that large language models like Llama that Zuckerberg has put at the center of his strategy are useful, but they will never be able to reason and plan like humans, increasingly appearing to contradict his boss’s grandiose AI vision for developing “superintelligence.”&lt;/p&gt;
&lt;p&gt;For example, in May 2024, when an OpenAI researcher discussed the need to control ultra-intelligent AI, LeCun responded on X by writing that before urgently figuring out how to control AI systems much smarter than humans, researchers need to have the beginning of a hint of a design for a system smarter than a house cat.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2104309 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="569" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/zuckmetaverse-1024x569.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Mark Zuckerberg once believed the “metaverse” was the future and renamed his company because of it.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Facebook

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Within FAIR, LeCun has instead focused on developing world models that can truly plan and reason. Over the past year, though, Meta’s AI research groups have seen growing tension and mass layoffs as Zuckerberg has shifted the company’s AI strategy away from long-term research and toward the rapid deployment of commercial products.&lt;/p&gt;
&lt;p&gt;Over the summer, Zuckerberg hired Alexandr Wang to lead a new superintelligence team at Meta, paying $14.3 billion to hire the 28-year-old founder of data-labeling startup Scale AI and acquire a 49 percent interest in his company. LeCun, who had previously reported to Chief Product Officer Chris Cox, now reports to Wang, which seems like a sharp rebuke of LeCun’s approach to AI.&lt;/p&gt;
&lt;p&gt;Zuckerberg also personally handpicked an exclusive team called TBD Lab to accelerate the development of the next iteration of large language models, luring staff from rivals such as OpenAI and Google with astonishingly large $100 to $250 million pay packages. As a result, Zuckerberg has come under growing pressure from Wall Street to show that his multibillion-dollar investment in becoming an AI leader will pay off and boost revenue. But if it turns out like his previous pivot to the metaverse, Zuckerberg’s latest bet could prove equally expensive and unfruitful.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        AI pioneer reportedly frustrated with Meta’s shift from research to rapid product releases.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="205" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-300x205.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2024/11/GettyImages-1691376215-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Yann LeCun, Vice President and Chief AI Scientist for Meta Platforms, as seen in September 2023.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Photo by Kevin Dietsch/Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Meta’s chief AI scientist and Turing Award winner Yann LeCun plans to leave the company to launch his own startup focused on a different type of AI called “world models,” the Financial Times reported. The French-US scientist has reportedly told associates he will depart in the coming months and is already in early talks to raise funds for the new venture. The departure comes as CEO Mark Zuckerberg radically overhauled Meta’s AI operations after deciding the company had fallen behind rivals such as OpenAI and Google.&lt;/p&gt;
&lt;p&gt;World models are hypothetical AI systems that some AI engineers expect to develop an internal “understanding” of the physical world by learning from video and spatial data rather than text alone. Unlike current large language models (such as the kind that power ChatGPT) that predict the next segment of data in a sequence, world models would ideally simulate cause-and-effect scenarios, understand physics, and enable machines to reason and plan more like animals do. LeCun has said this architecture could take a decade to fully develop.&lt;/p&gt;
&lt;p&gt;While some AI experts believe that Transformer-based AI models—such as large language models, video synthesis models, and interactive world synthesis models—have emergently modeled physics or absorbed the structural rules of the physical world from training data examples, the evidence so far generally points to sophisticated pattern-matching rather than a base understanding of how the physical world actually works.&lt;/p&gt;
&lt;p&gt;LeCun’s planned exit is the latest in a string of leadership reshuffles at Meta during what has been a tumultuous year for the company. A key turning point was the disappointing launch and benchmark-gaming controversy of the AI language model Llama 4 in April, which many in the industry saw as a flop when it performed worse than the most advanced offerings from Google, OpenAI, and Anthropic. Meanwhile, the Meta AI chatbot has failed to gain traction with consumers and suffered controversies and setbacks over its interactions with children.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A different approach to AI&lt;/h2&gt;
&lt;p&gt;LeCun founded Meta’s Fundamental AI Research lab, known as FAIR, in 2013 and has served as the company’s chief AI scientist ever since. He is one of three researchers who won the 2018 Turing Award for pioneering work on deep learning and convolutional neural networks. After leaving Meta, LeCun will remain a professor at New York University, where he has taught since 2003.&lt;/p&gt;
&lt;p&gt;LeCun has previously argued that large language models like Llama that Zuckerberg has put at the center of his strategy are useful, but they will never be able to reason and plan like humans, increasingly appearing to contradict his boss’s grandiose AI vision for developing “superintelligence.”&lt;/p&gt;
&lt;p&gt;For example, in May 2024, when an OpenAI researcher discussed the need to control ultra-intelligent AI, LeCun responded on X by writing that before urgently figuring out how to control AI systems much smarter than humans, researchers need to have the beginning of a hint of a design for a system smarter than a house cat.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2104309 align-center"&gt;
    &lt;div&gt;
                        &lt;img alt="alt" class="center large" height="569" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/zuckmetaverse-1024x569.jpg" width="1024" /&gt;
                  &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Mark Zuckerberg once believed the “metaverse” was the future and renamed his company because of it.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Facebook

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Within FAIR, LeCun has instead focused on developing world models that can truly plan and reason. Over the past year, though, Meta’s AI research groups have seen growing tension and mass layoffs as Zuckerberg has shifted the company’s AI strategy away from long-term research and toward the rapid deployment of commercial products.&lt;/p&gt;
&lt;p&gt;Over the summer, Zuckerberg hired Alexandr Wang to lead a new superintelligence team at Meta, paying $14.3 billion to hire the 28-year-old founder of data-labeling startup Scale AI and acquire a 49 percent interest in his company. LeCun, who had previously reported to Chief Product Officer Chris Cox, now reports to Wang, which seems like a sharp rebuke of LeCun’s approach to AI.&lt;/p&gt;
&lt;p&gt;Zuckerberg also personally handpicked an exclusive team called TBD Lab to accelerate the development of the next iteration of large language models, luring staff from rivals such as OpenAI and Google with astonishingly large $100 to $250 million pay packages. As a result, Zuckerberg has come under growing pressure from Wall Street to show that his multibillion-dollar investment in becoming an AI leader will pay off and boost revenue. But if it turns out like his previous pivot to the metaverse, Zuckerberg’s latest bet could prove equally expensive and unfruitful.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/metas-star-ai-scientist-yann-lecun-plans-to-leave-for-own-startup/</guid><pubDate>Wed, 12 Nov 2025 17:14:16 +0000</pubDate></item><item><title>[NEW] What startups want from OpenAI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/what-startups-want-from-openai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54885185707_36953b477e_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Marc Manara, OpenAI’s head of startups, says the reality of AI has advanced far beyond ideas and experiments. AI-native companies are hitting $200 million in annual recurring revenue, and product cycles have shrunk from two-week sprints to single days.&amp;nbsp;And OpenAI is helping. &lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Russell Brandom sits down with Manara at TechCrunch Disrupt 2025 to explore how OpenAI is serving the startups building on its platform.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The shift from two-week sprints to one-day development cycles, and what that means for how startups should structure their engineering teams&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why some startups are customizing models for specific tasks in healthcare, finance, and other verticals that seemed out of reach&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where AI still&amp;nbsp;hasn’t&amp;nbsp;fully integrated with companies, and why longer-horizon autonomous tasks&amp;nbsp;remain&amp;nbsp;the next frontier for both models and startups&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/11/54885185707_36953b477e_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;Marc Manara, OpenAI’s head of startups, says the reality of AI has advanced far beyond ideas and experiments. AI-native companies are hitting $200 million in annual recurring revenue, and product cycles have shrunk from two-week sprints to single days.&amp;nbsp;And OpenAI is helping. &lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Today on TechCrunch’s&amp;nbsp;Equity&amp;nbsp;podcast, Russell Brandom sits down with Manara at TechCrunch Disrupt 2025 to explore how OpenAI is serving the startups building on its platform.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;The shift from two-week sprints to one-day development cycles, and what that means for how startups should structure their engineering teams&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Why some startups are customizing models for specific tasks in healthcare, finance, and other verticals that seemed out of reach&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Where AI still&amp;nbsp;hasn’t&amp;nbsp;fully integrated with companies, and why longer-horizon autonomous tasks&amp;nbsp;remain&amp;nbsp;the next frontier for both models and startups&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on&amp;nbsp;X&amp;nbsp;and&amp;nbsp;Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/what-startups-want-from-openai/</guid><pubDate>Wed, 12 Nov 2025 17:18:48 +0000</pubDate></item><item><title>[NEW] OpenAI fights order to hand over 20 million private ChatGPT conversations (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/11/openai-fights-order-to-hand-over-20-million-private-chatgpt-conversations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI: NYT wants evidence of ChatGPT users trying to get around news paywall.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-640x427.jpg" width="640" /&gt;
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | alexsl

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI wants a court to reverse a ruling forcing the ChatGPT maker to give 20 million user chats to The New York Times and other news plaintiffs that sued it over alleged copyright infringement. Although OpenAI previously offered 20 million user chats as a counter to the NYT’s demand for 120 million, the AI company says a court order requiring production of the chats is too broad.&lt;/p&gt;
&lt;p&gt;“The logs at issue here are c&lt;em&gt;omplete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT,” OpenAI said today in a filing in US District Court for the Southern District of New York. “Disclosure of those logs is thus much more likely to expose private information [than individual prompt-output pairs], in the same way that eavesdropping on an entire conversation reveals more private information than a 5-second conversation fragment.”&lt;/p&gt;
&lt;p&gt;OpenAI’s filing said that “more than 99.99%” of the chats “have &lt;em&gt;nothing to do&lt;/em&gt; with this case.” It asked the district court to “vacate the order and order News Plaintiffs to respond to OpenAI’s proposal for identifying relevant logs.” OpenAI could also seek review in a federal court of appeals.&lt;/p&gt;
&lt;p&gt;OpenAI posted a message on its website to users today saying that “The New York Times is demanding that we turn over 20 million of your private ChatGPT conversations” in order to “find examples of you using ChatGPT to try to get around their paywall.”&lt;/p&gt;
&lt;p&gt;ChatGPT users concerned about privacy have more to worry about than the NYT case. For example, ChatGPT conversations have been found in Google search results and the Google Search Console tool that developers can use to monitor search traffic. OpenAI today said it plans to develop “advanced security features designed to keep your data private, including client-side encryption for your messages with ChatGPT. ”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI: AI chats should be treated like private emails&lt;/h2&gt;
&lt;p&gt;OpenAI’s court filing argues that the chat log production should be narrowed based on the relevance of chats to the case.&lt;/p&gt;
&lt;p&gt;“OpenAI is unaware of any court ordering wholesale production of personal information at this scale,” the filing said. “This sets a dangerous precedent: it suggests that anyone who files a lawsuit against an AI company can demand production of tens of millions of conversations without first narrowing for relevance. This is not how discovery works in other cases: courts do not allow plaintiffs suing Google to dig through the private emails of tens of millions of Gmail users irrespective of their relevance. And it is not how discovery should work for generative AI tools either.”&lt;/p&gt;
&lt;p&gt;A November 7 order by US Magistrate Judge Ona Wang sided with the NYT, saying that OpenAI must “produce the 20 million de-identified Consumer ChatGPT Logs to News Plaintiffs by November 14, 2025, or within 7 days of completing the de-identification process.” Wang ruled that the production must go forward even though the parties don’t agree on whether the logs must be produced in full:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Whether or not the parties had reached agreement to produce the 20 million Consumer ChatGPT Logs &lt;u&gt;in whole&lt;/u&gt;—which the parties vehemently dispute—such production here is appropriate. OpenAI has failed to explain how its consumers’ privacy rights are not adequately protected by: (1) the existing protective order in this multidistrict litigation or (2) OpenAI’s exhaustive de-identification of all of the 20 million Consumer ChatGPT Logs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI’s filing today said the court order “did not acknowledge OpenAI’s sworn witness declaration explaining that the de-identification process is not intended to remove information that is non-identifying but may nonetheless be private, like a Washington Post reporter’s hypothetical use of ChatGPT to assist in the preparation of a news article.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Chats stored under legal hold&lt;/h2&gt;
&lt;p&gt;The 20 million chats consist of a random sampling of ChatGPT conversations from December 2022 to November 2024 and do not include chats of business customers, OpenAI said in the message on its website.&lt;/p&gt;
&lt;p&gt;“We presented several privacy-preserving options to The Times, including targeted searches over the sample (&lt;em&gt;e.g.&lt;/em&gt;, to search for chats that might include text from a New York Times article so they only receive the conversations relevant to their claims), as well as high-level data classifying how ChatGPT was used in the sample. These were rejected by The Times,” OpenAI said.&lt;/p&gt;
&lt;p&gt;The chats are stored in a secure system that is “protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations,” OpenAI said. The NYT “would be legally obligated at this time to not make any data public outside the court process,” and OpenAI said it will fight any attempts to make the user conversations public.&lt;/p&gt;
&lt;p&gt;An NYT filing on October 30 accused OpenAI of defying prior agreements “by refusing to produce even a small sample of the billions of model outputs that its conduct has put in issue in this case.” The filing continued:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Immediate production of the output log sample is essential to stay on track for the February 26, 2026, discovery deadline. OpenAI’s proposal to run searches on this small subset of its model outputs on Plaintiffs’ behalf is as inefficient as it is inadequate to allow Plaintiffs to fairly analyze how “real world” users interact with a core product at the center of this litigation. Plaintiffs cannot reasonably conduct expert analyses about how OpenAI’s models function in its core consumer-facing product, how retrieval augmented generation (“RAG”) functions to deliver news content, how consumers interact with that product, and the frequency of hallucinations without access to the model outputs themselves.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI said the NYT’s discovery requests were initially limited to logs “related to Times content” and that it has “been working to satisfy those requests by sampling conversation logs. Towards the end of that process, News Plaintiffs filed a motion with a new demand: that instead of finding and producing logs that are ‘related to Times content,’ OpenAI should hand over the entire 20 million-log sample ‘via hard drive.'”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI disputes judge’s reasoning&lt;/h2&gt;
&lt;p&gt;The November 7 order cited a California case, &lt;em&gt;Concord Music Group, Inc. v. Anthropic PBC&lt;/em&gt;, in which US District Magistrate Judge Susan van Keulen ordered the production of 5 million records. OpenAI consistently relied on van Keulen’s use of a sample-size formula “in support of its previous proposed methodology for conversation data sampling, but fails to explain why Judge [van] Keulen’s subsequent order directing production of the entire 5-million record sample to the plaintiff in that case is not similarly instructive here,” Wang wrote.&lt;/p&gt;
&lt;p&gt;OpenAI’s filing today said the company was never given an opportunity to explain why &lt;em&gt;Concord&lt;/em&gt; shouldn’t apply in this case because the news plaintiffs did not reference it in their motion.&lt;/p&gt;
&lt;p&gt;“The cited &lt;em&gt;Concord&lt;/em&gt; order was not about whether wholesale production of the sample was appropriate; it was about the mechanism through which Anthropic would effectuate an &lt;em&gt;already agreed-upon&lt;/em&gt; production,” OpenAI wrote. “Nothing about that order suggests that Judge van Keulen would have ordered wholesale production had Anthropic raised the privacy concerns that OpenAI has raised throughout this case.”&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Concord&lt;/em&gt; logs were just prompt-output pairs, “&lt;em&gt;i.e.&lt;/em&gt;, a single user prompt followed by a single model output,” OpenAI wrote. “The logs at issue here are &lt;em&gt;complete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT.” That could result in “up to 80 million prompt-output pairs,” OpenAI said.&lt;/p&gt;
&lt;p&gt;We contacted The New York Times about OpenAI’s filing and will update this article if it provides any comment.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        OpenAI: NYT wants evidence of ChatGPT users trying to get around news paywall.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-640x427.jpg" width="640" /&gt;
                  &lt;img alt="A person's finger tapping a ChatGPT app icon on an iPhone screen that also shows icons for DeepSeek, Gemini, Copilot, Grok, and Claude." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/chatgpt-app-icon-1152x648-1762971088.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images | alexsl

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;OpenAI wants a court to reverse a ruling forcing the ChatGPT maker to give 20 million user chats to The New York Times and other news plaintiffs that sued it over alleged copyright infringement. Although OpenAI previously offered 20 million user chats as a counter to the NYT’s demand for 120 million, the AI company says a court order requiring production of the chats is too broad.&lt;/p&gt;
&lt;p&gt;“The logs at issue here are c&lt;em&gt;omplete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT,” OpenAI said today in a filing in US District Court for the Southern District of New York. “Disclosure of those logs is thus much more likely to expose private information [than individual prompt-output pairs], in the same way that eavesdropping on an entire conversation reveals more private information than a 5-second conversation fragment.”&lt;/p&gt;
&lt;p&gt;OpenAI’s filing said that “more than 99.99%” of the chats “have &lt;em&gt;nothing to do&lt;/em&gt; with this case.” It asked the district court to “vacate the order and order News Plaintiffs to respond to OpenAI’s proposal for identifying relevant logs.” OpenAI could also seek review in a federal court of appeals.&lt;/p&gt;
&lt;p&gt;OpenAI posted a message on its website to users today saying that “The New York Times is demanding that we turn over 20 million of your private ChatGPT conversations” in order to “find examples of you using ChatGPT to try to get around their paywall.”&lt;/p&gt;
&lt;p&gt;ChatGPT users concerned about privacy have more to worry about than the NYT case. For example, ChatGPT conversations have been found in Google search results and the Google Search Console tool that developers can use to monitor search traffic. OpenAI today said it plans to develop “advanced security features designed to keep your data private, including client-side encryption for your messages with ChatGPT. ”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI: AI chats should be treated like private emails&lt;/h2&gt;
&lt;p&gt;OpenAI’s court filing argues that the chat log production should be narrowed based on the relevance of chats to the case.&lt;/p&gt;
&lt;p&gt;“OpenAI is unaware of any court ordering wholesale production of personal information at this scale,” the filing said. “This sets a dangerous precedent: it suggests that anyone who files a lawsuit against an AI company can demand production of tens of millions of conversations without first narrowing for relevance. This is not how discovery works in other cases: courts do not allow plaintiffs suing Google to dig through the private emails of tens of millions of Gmail users irrespective of their relevance. And it is not how discovery should work for generative AI tools either.”&lt;/p&gt;
&lt;p&gt;A November 7 order by US Magistrate Judge Ona Wang sided with the NYT, saying that OpenAI must “produce the 20 million de-identified Consumer ChatGPT Logs to News Plaintiffs by November 14, 2025, or within 7 days of completing the de-identification process.” Wang ruled that the production must go forward even though the parties don’t agree on whether the logs must be produced in full:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Whether or not the parties had reached agreement to produce the 20 million Consumer ChatGPT Logs &lt;u&gt;in whole&lt;/u&gt;—which the parties vehemently dispute—such production here is appropriate. OpenAI has failed to explain how its consumers’ privacy rights are not adequately protected by: (1) the existing protective order in this multidistrict litigation or (2) OpenAI’s exhaustive de-identification of all of the 20 million Consumer ChatGPT Logs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI’s filing today said the court order “did not acknowledge OpenAI’s sworn witness declaration explaining that the de-identification process is not intended to remove information that is non-identifying but may nonetheless be private, like a Washington Post reporter’s hypothetical use of ChatGPT to assist in the preparation of a news article.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Chats stored under legal hold&lt;/h2&gt;
&lt;p&gt;The 20 million chats consist of a random sampling of ChatGPT conversations from December 2022 to November 2024 and do not include chats of business customers, OpenAI said in the message on its website.&lt;/p&gt;
&lt;p&gt;“We presented several privacy-preserving options to The Times, including targeted searches over the sample (&lt;em&gt;e.g.&lt;/em&gt;, to search for chats that might include text from a New York Times article so they only receive the conversations relevant to their claims), as well as high-level data classifying how ChatGPT was used in the sample. These were rejected by The Times,” OpenAI said.&lt;/p&gt;
&lt;p&gt;The chats are stored in a secure system that is “protected under legal hold, meaning it can’t be accessed or used for purposes other than meeting legal obligations,” OpenAI said. The NYT “would be legally obligated at this time to not make any data public outside the court process,” and OpenAI said it will fight any attempts to make the user conversations public.&lt;/p&gt;
&lt;p&gt;An NYT filing on October 30 accused OpenAI of defying prior agreements “by refusing to produce even a small sample of the billions of model outputs that its conduct has put in issue in this case.” The filing continued:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Immediate production of the output log sample is essential to stay on track for the February 26, 2026, discovery deadline. OpenAI’s proposal to run searches on this small subset of its model outputs on Plaintiffs’ behalf is as inefficient as it is inadequate to allow Plaintiffs to fairly analyze how “real world” users interact with a core product at the center of this litigation. Plaintiffs cannot reasonably conduct expert analyses about how OpenAI’s models function in its core consumer-facing product, how retrieval augmented generation (“RAG”) functions to deliver news content, how consumers interact with that product, and the frequency of hallucinations without access to the model outputs themselves.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;OpenAI said the NYT’s discovery requests were initially limited to logs “related to Times content” and that it has “been working to satisfy those requests by sampling conversation logs. Towards the end of that process, News Plaintiffs filed a motion with a new demand: that instead of finding and producing logs that are ‘related to Times content,’ OpenAI should hand over the entire 20 million-log sample ‘via hard drive.'”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;OpenAI disputes judge’s reasoning&lt;/h2&gt;
&lt;p&gt;The November 7 order cited a California case, &lt;em&gt;Concord Music Group, Inc. v. Anthropic PBC&lt;/em&gt;, in which US District Magistrate Judge Susan van Keulen ordered the production of 5 million records. OpenAI consistently relied on van Keulen’s use of a sample-size formula “in support of its previous proposed methodology for conversation data sampling, but fails to explain why Judge [van] Keulen’s subsequent order directing production of the entire 5-million record sample to the plaintiff in that case is not similarly instructive here,” Wang wrote.&lt;/p&gt;
&lt;p&gt;OpenAI’s filing today said the company was never given an opportunity to explain why &lt;em&gt;Concord&lt;/em&gt; shouldn’t apply in this case because the news plaintiffs did not reference it in their motion.&lt;/p&gt;
&lt;p&gt;“The cited &lt;em&gt;Concord&lt;/em&gt; order was not about whether wholesale production of the sample was appropriate; it was about the mechanism through which Anthropic would effectuate an &lt;em&gt;already agreed-upon&lt;/em&gt; production,” OpenAI wrote. “Nothing about that order suggests that Judge van Keulen would have ordered wholesale production had Anthropic raised the privacy concerns that OpenAI has raised throughout this case.”&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Concord&lt;/em&gt; logs were just prompt-output pairs, “&lt;em&gt;i.e.&lt;/em&gt;, a single user prompt followed by a single model output,” OpenAI wrote. “The logs at issue here are &lt;em&gt;complete conversations&lt;/em&gt;: each log in the 20 million sample represents a complete exchange of multiple prompt-output pairs between a user and ChatGPT.” That could result in “up to 80 million prompt-output pairs,” OpenAI said.&lt;/p&gt;
&lt;p&gt;We contacted The New York Times about OpenAI’s filing and will update this article if it provides any comment.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/11/openai-fights-order-to-hand-over-20-million-private-chatgpt-conversations/</guid><pubDate>Wed, 12 Nov 2025 18:27:27 +0000</pubDate></item></channel></rss>