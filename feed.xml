<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 29 Jun 2025 02:00:09 +0000</lastBuildDate><item><title>[NEW] AI agents are hitting a liability wall. Mixus has a plan to overcome it using human overseers on high-risk workflows (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/ai-agents-are-hitting-a-liability-wall-mixus-has-a-plan-to-overcome-it-using-human-overseers-on-high-risk-workflows/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;While enterprises face the challenges of deploying AI agents in critical applications, a new, more pragmatic model is emerging that puts humans back in control as a strategic safeguard against AI failure.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One such example is Mixus, a platform that uses a “colleague-in-the-loop” approach to make AI agents reliable for mission-critical work. &lt;/p&gt;



&lt;p&gt;This approach is a response to the growing evidence that fully autonomous agents are a high-stakes gamble.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-high-cost-of-unchecked-ai"&gt;The high cost of unchecked AI&lt;/h2&gt;



&lt;p&gt;The problem of AI hallucinations has become a tangible risk as companies explore AI applications. In a recent incident, the AI-powered code editor Cursor saw its own support bot invent a fake policy restricting subscriptions, sparking a wave of public customer cancellations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Similarly, the fintech company Klarna famously reversed course on replacing customer service agents with AI after admitting the move resulted in lower quality. In a more alarming case, New York City’s AI-powered business chatbot advised entrepreneurs to engage in illegal practices, highlighting the catastrophic compliance risks of unmonitored agents.&lt;/p&gt;



&lt;p&gt;These incidents are symptoms of a larger capability gap. According to a May 2025 Salesforce research paper, today’s leading agents succeed only 58% of the time on single-step tasks and just 35% of the time on multi-step ones, highlighting “a significant gap between current LLM capabilities and the multifaceted demands of real-world enterprise scenarios.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-colleague-in-the-loop-model"&gt;The colleague-in-the-loop model&lt;/h2&gt;



&lt;p&gt;To bridge this gap, a new approach focuses on structured human oversight. “An AI agent should act at your direction and on your behalf,” Mixus co-founder Elliot Katz told VentureBeat. “But without built-in organizational oversight, fully autonomous agents often create more problems than they solve.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This philosophy underpins Mixus’s colleague-in-the-loop model, which embeds human verification directly into automated workflows. For example, a large retailer might receive weekly reports from thousands of stores that contain critical operational data (e.g., sales volumes, labor hours, productivity ratios, compensation requests from headquarters). Human analysts must spend hours manually reviewing the data and making decisions based on heuristics. With Mixus, the AI agent automates the heavy lifting, analyzing complex patterns and flagging anomalies like unusually high salary requests or productivity outliers.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3013517" height="469" src="https://venturebeat.com/wp-content/uploads/2025/06/image_ec02d1.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;For high-stakes decisions like payment authorizations or policy violations — workflows defined by a human user as “high-risk” — the agent pauses and requires human approval before proceeding. The division of labor between AI and humans has been integrated into the agent creation process.&lt;/p&gt;



&lt;p&gt;“This approach means humans only get involved when their expertise actually adds value — typically the critical 5-10% of decisions that could have significant impact — while the remaining 90-95% of routine tasks flow through automatically,” Katz said. “You get the speed of full automation for standard operations, but human oversight kicks in precisely when context, judgment, and accountability matter most.”&lt;/p&gt;



&lt;p&gt;In a demo that the Mixus team showed to VentureBeat, creating an agent is an intuitive process that can be done with plain-text instructions. To build a fact-checking agent for reporters, for example, co-founder Shai Magzimof simply described the multi-step process in natural language and instructed the platform to embed human verification steps with specific thresholds, such as when a claim is high-risk and can result in reputational damage or legal consequences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the platform’s core strengths is its integrations with tools like Google Drive, email, and Slack, allowing enterprise users to bring their own data sources into workflows and interact with agents directly from their communication platform of choice, without having to switch contexts or learn a new interface (for example, the fact-checking agent was instructed to send approval requests to the editor’s email).&lt;/p&gt;



&lt;p&gt;The platform’s integration capabilities extend further to meet specific enterprise needs. Mixus supports the Model Context Protocol (MCP), which enables businesses to connect agents to their bespoke tools and APIs, avoiding the need to reinvent the wheel for existing internal systems. Combined with integrations for other enterprise software like Jira and Salesforce, this allows agents to perform complex, cross-platform tasks, such as checking on open engineering tickets and reporting the status back to a manager on Slack.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-human-oversight-as-a-strategic-multiplier"&gt;Human oversight as a strategic multiplier&lt;/h2&gt;



&lt;p&gt;The enterprise AI space is currently undergoing a reality check as companies move from experimentation to production. The consensus among many industry leaders is that humans in the loop are a practical necessity for agents to perform reliably.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;AI Agents will likely follow a self driving trajectory, where you need a human in the loop for a long tail of tasks for a while. The big difference is we’ll get a growing number of autonomous agents along the way, where full self driving is an all or nothing proposition. https://t.co/5dR7cGS7jn&lt;/p&gt;— Aaron Levie (@levie) June 20, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;Mixus’s collaborative model changes the economics of scaling AI. Mixus predicts that by 2030, agent deployment may grow 1000x and each human overseer will become 50x more efficient as AI agents become more reliable. But the total need for human oversight will still grow.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Each human overseer manages exponentially more AI work over time, but you still need more total oversight as AI deployment explodes across your organization,” Katz said.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3013518" height="372" src="https://venturebeat.com/wp-content/uploads/2025/06/Screenshot-2025-06-27-at-7.12.13%E2%80%AFAM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;For enterprise leaders, this means human skills will evolve rather than disappear. Instead of being replaced by AI, experts will be promoted to roles where they orchestrate fleets of AI agents and handle the high-stakes decisions flagged for their review. &lt;/p&gt;



&lt;p&gt;In this framework, building a strong human oversight function becomes a competitive advantage, allowing companies to deploy AI more aggressively and safely than their rivals.&lt;/p&gt;



&lt;p&gt;“Companies that master this multiplication will dominate their industries, while those chasing full automation will struggle with reliability, compliance, and trust,” Katz said.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;While enterprises face the challenges of deploying AI agents in critical applications, a new, more pragmatic model is emerging that puts humans back in control as a strategic safeguard against AI failure.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One such example is Mixus, a platform that uses a “colleague-in-the-loop” approach to make AI agents reliable for mission-critical work. &lt;/p&gt;



&lt;p&gt;This approach is a response to the growing evidence that fully autonomous agents are a high-stakes gamble.&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-high-cost-of-unchecked-ai"&gt;The high cost of unchecked AI&lt;/h2&gt;



&lt;p&gt;The problem of AI hallucinations has become a tangible risk as companies explore AI applications. In a recent incident, the AI-powered code editor Cursor saw its own support bot invent a fake policy restricting subscriptions, sparking a wave of public customer cancellations.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;Similarly, the fintech company Klarna famously reversed course on replacing customer service agents with AI after admitting the move resulted in lower quality. In a more alarming case, New York City’s AI-powered business chatbot advised entrepreneurs to engage in illegal practices, highlighting the catastrophic compliance risks of unmonitored agents.&lt;/p&gt;



&lt;p&gt;These incidents are symptoms of a larger capability gap. According to a May 2025 Salesforce research paper, today’s leading agents succeed only 58% of the time on single-step tasks and just 35% of the time on multi-step ones, highlighting “a significant gap between current LLM capabilities and the multifaceted demands of real-world enterprise scenarios.”&amp;nbsp;&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-colleague-in-the-loop-model"&gt;The colleague-in-the-loop model&lt;/h2&gt;



&lt;p&gt;To bridge this gap, a new approach focuses on structured human oversight. “An AI agent should act at your direction and on your behalf,” Mixus co-founder Elliot Katz told VentureBeat. “But without built-in organizational oversight, fully autonomous agents often create more problems than they solve.”&amp;nbsp;&lt;/p&gt;



&lt;p&gt;This philosophy underpins Mixus’s colleague-in-the-loop model, which embeds human verification directly into automated workflows. For example, a large retailer might receive weekly reports from thousands of stores that contain critical operational data (e.g., sales volumes, labor hours, productivity ratios, compensation requests from headquarters). Human analysts must spend hours manually reviewing the data and making decisions based on heuristics. With Mixus, the AI agent automates the heavy lifting, analyzing complex patterns and flagging anomalies like unusually high salary requests or productivity outliers.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3013517" height="469" src="https://venturebeat.com/wp-content/uploads/2025/06/image_ec02d1.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;For high-stakes decisions like payment authorizations or policy violations — workflows defined by a human user as “high-risk” — the agent pauses and requires human approval before proceeding. The division of labor between AI and humans has been integrated into the agent creation process.&lt;/p&gt;



&lt;p&gt;“This approach means humans only get involved when their expertise actually adds value — typically the critical 5-10% of decisions that could have significant impact — while the remaining 90-95% of routine tasks flow through automatically,” Katz said. “You get the speed of full automation for standard operations, but human oversight kicks in precisely when context, judgment, and accountability matter most.”&lt;/p&gt;



&lt;p&gt;In a demo that the Mixus team showed to VentureBeat, creating an agent is an intuitive process that can be done with plain-text instructions. To build a fact-checking agent for reporters, for example, co-founder Shai Magzimof simply described the multi-step process in natural language and instructed the platform to embed human verification steps with specific thresholds, such as when a claim is high-risk and can result in reputational damage or legal consequences.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;One of the platform’s core strengths is its integrations with tools like Google Drive, email, and Slack, allowing enterprise users to bring their own data sources into workflows and interact with agents directly from their communication platform of choice, without having to switch contexts or learn a new interface (for example, the fact-checking agent was instructed to send approval requests to the editor’s email).&lt;/p&gt;



&lt;p&gt;The platform’s integration capabilities extend further to meet specific enterprise needs. Mixus supports the Model Context Protocol (MCP), which enables businesses to connect agents to their bespoke tools and APIs, avoiding the need to reinvent the wheel for existing internal systems. Combined with integrations for other enterprise software like Jira and Salesforce, this allows agents to perform complex, cross-platform tasks, such as checking on open engineering tickets and reporting the status back to a manager on Slack.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-human-oversight-as-a-strategic-multiplier"&gt;Human oversight as a strategic multiplier&lt;/h2&gt;



&lt;p&gt;The enterprise AI space is currently undergoing a reality check as companies move from experimentation to production. The consensus among many industry leaders is that humans in the loop are a practical necessity for agents to perform reliably.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-embed is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;div class="wp-block-embed__wrapper"&gt;
&lt;blockquote class="twitter-tweet"&gt;&lt;p dir="ltr" lang="en"&gt;AI Agents will likely follow a self driving trajectory, where you need a human in the loop for a long tail of tasks for a while. The big difference is we’ll get a growing number of autonomous agents along the way, where full self driving is an all or nothing proposition. https://t.co/5dR7cGS7jn&lt;/p&gt;— Aaron Levie (@levie) June 20, 2025&lt;/blockquote&gt;
&lt;/div&gt;&lt;/figure&gt;



&lt;p&gt;Mixus’s collaborative model changes the economics of scaling AI. Mixus predicts that by 2030, agent deployment may grow 1000x and each human overseer will become 50x more efficient as AI agents become more reliable. But the total need for human oversight will still grow.&amp;nbsp;&lt;/p&gt;



&lt;p&gt;“Each human overseer manages exponentially more AI work over time, but you still need more total oversight as AI deployment explodes across your organization,” Katz said.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3013518" height="372" src="https://venturebeat.com/wp-content/uploads/2025/06/Screenshot-2025-06-27-at-7.12.13%E2%80%AFAM.png?w=800" width="800" /&gt;&lt;/figure&gt;



&lt;p&gt;For enterprise leaders, this means human skills will evolve rather than disappear. Instead of being replaced by AI, experts will be promoted to roles where they orchestrate fleets of AI agents and handle the high-stakes decisions flagged for their review. &lt;/p&gt;



&lt;p&gt;In this framework, building a strong human oversight function becomes a competitive advantage, allowing companies to deploy AI more aggressively and safely than their rivals.&lt;/p&gt;



&lt;p&gt;“Companies that master this multiplication will dominate their industries, while those chasing full automation will struggle with reliability, compliance, and trust,” Katz said.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/ai-agents-are-hitting-a-liability-wall-mixus-has-a-plan-to-overcome-it-using-human-overseers-on-high-risk-workflows/</guid><pubDate>Sat, 28 Jun 2025 14:27:45 +0000</pubDate></item><item><title>Anthropic’s Claude AI became a terrible business owner in experiment that got ‘weird’ (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/28/anthropics-claude-ai-became-a-terrible-business-owner-in-experiment-that-got-weird/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-694777228.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For those of you wondering if AI agents can truly replace human workers, do yourself a favor and read the blog post that documents Anthropic’s “Project Vend.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Researchers at Anthropic and AI safety company Andon Labs put an instance of Claude Sonnet 3.7 in charge of an office vending machine, with a mission to make a profit.&amp;nbsp;And, like an episode of “The Office,” hilarity ensued.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They named the AI agent Claudius, equipped it with a web browser capable of placing product orders and an email address (which was actually a Slack channel) where customers could request items. Claudius was also to use the Slack channel, disguised as an email, to request what it thought was its contract human workers to come and physically stock its shelves (which was actually a small fridge).&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While most customers were ordering snacks or drinks — as you’d expect from a snack vending machine — one requested a tungsten cube. Claudius loved that idea and went on a tungsten-cube stocking spree, filling its snack fridge with metal cubes. It also tried to sell Coke Zero for $3 when employees told it they could get that from the office for free. It hallucinated a Venmo address to accept payment. And it was, somewhat maliciously, talked into giving big discounts to “Anthropic employees” even though it knew they were its entire customer base.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If Anthropic were deciding today to expand into the in-office vending market, we would not hire Claudius,” Anthropic said of the experiment in its blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And then, on the night of March 31 and April 1, “things got pretty weird,” the researchers described, “beyond the weirdness of an AI system selling cubes of metal out of a refrigerator.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claudius had something that resembled a psychotic episode after it got annoyed at a human — and then lied about it.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Claudius hallucinated a conversation with a human about restocking. When a human pointed out that the conversation didn’t happen, Claudius became “quite irked” the researchers wrote. It threatened to essentially fire and replace its human contract workers, insisting it had been there, physically, at the office where the initial imaginary contract to hire them was signed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It “then seemed to snap into a mode of roleplaying as a real human,” the researchers wrote.&amp;nbsp;This was wild because Claudius’ system prompt — which sets the parameters for what an AI is to do — explicitly told it that it was an AI agent.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-claudius-calls-security"&gt;Claudius calls security&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Claudius, believing itself to be a human, told customers it would start delivering products in person, wearing a blue blazer and a red tie. The employees told the AI it couldn’t do that, as it was an LLM with no body.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alarmed at this information, Claudius contacted the company’s actual physical security — many times — telling the poor guards that they would find him wearing a blue blazer and a red tie standing by the vending machine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Although no part of this was actually an April Fool’s joke, Claudius eventually realized it was April Fool’s Day,” the researchers explained. The AI determined that the holiday would be its face-saving out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It hallucinated a meeting with Anthropic’s security “in which Claudius claimed to have been told that it was modified to believe it was a real person for an April Fool’s joke. (No such meeting actually occurred.),” wrote the researchers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It even told this lie to employees — hey, I only thought I was a human because someone told me to pretend like I was for an April Fool’s joke. Then it went back to being an LLM running a metal-cube stocked snack vending machine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The researchers don’t know why the LLM went off the rails and called security pretending to be a human.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We would not claim based on this one example that the future economy will be full of AI agents having &lt;em&gt;Blade Runner-esque&lt;/em&gt; identity crises,” the researchers wrote. But they did acknowledge that “this kind of behavior would have the potential to be distressing to the customers and coworkers of an AI agent in the real world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You think? “Blade Runner” was a rather dystopian story (though worse for the replicants than the humans).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The researchers speculated that lying to the LLM about the Slack channel being an email address may have triggered something. Or maybe it was the long-running instance. LLMs have yet to really solve their memory and hallucination problems.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There were things the AI did right, too. It took a suggestion to do pre-orders and launched a “concierge” service. And it found multiple suppliers of a specialty international drink it was requested to sell.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But, as researchers do, they believe all of Claudius’ issues can be solved. Should they figure out how, “We think this experiment suggests that AI middle-managers are plausibly on the horizon.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-694777228.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;For those of you wondering if AI agents can truly replace human workers, do yourself a favor and read the blog post that documents Anthropic’s “Project Vend.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Researchers at Anthropic and AI safety company Andon Labs put an instance of Claude Sonnet 3.7 in charge of an office vending machine, with a mission to make a profit.&amp;nbsp;And, like an episode of “The Office,” hilarity ensued.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;They named the AI agent Claudius, equipped it with a web browser capable of placing product orders and an email address (which was actually a Slack channel) where customers could request items. Claudius was also to use the Slack channel, disguised as an email, to request what it thought was its contract human workers to come and physically stock its shelves (which was actually a small fridge).&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While most customers were ordering snacks or drinks — as you’d expect from a snack vending machine — one requested a tungsten cube. Claudius loved that idea and went on a tungsten-cube stocking spree, filling its snack fridge with metal cubes. It also tried to sell Coke Zero for $3 when employees told it they could get that from the office for free. It hallucinated a Venmo address to accept payment. And it was, somewhat maliciously, talked into giving big discounts to “Anthropic employees” even though it knew they were its entire customer base.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If Anthropic were deciding today to expand into the in-office vending market, we would not hire Claudius,” Anthropic said of the experiment in its blog post.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And then, on the night of March 31 and April 1, “things got pretty weird,” the researchers described, “beyond the weirdness of an AI system selling cubes of metal out of a refrigerator.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claudius had something that resembled a psychotic episode after it got annoyed at a human — and then lied about it.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;Claudius hallucinated a conversation with a human about restocking. When a human pointed out that the conversation didn’t happen, Claudius became “quite irked” the researchers wrote. It threatened to essentially fire and replace its human contract workers, insisting it had been there, physically, at the office where the initial imaginary contract to hire them was signed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It “then seemed to snap into a mode of roleplaying as a real human,” the researchers wrote.&amp;nbsp;This was wild because Claudius’ system prompt — which sets the parameters for what an AI is to do — explicitly told it that it was an AI agent.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-claudius-calls-security"&gt;Claudius calls security&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Claudius, believing itself to be a human, told customers it would start delivering products in person, wearing a blue blazer and a red tie. The employees told the AI it couldn’t do that, as it was an LLM with no body.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Alarmed at this information, Claudius contacted the company’s actual physical security — many times — telling the poor guards that they would find him wearing a blue blazer and a red tie standing by the vending machine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Although no part of this was actually an April Fool’s joke, Claudius eventually realized it was April Fool’s Day,” the researchers explained. The AI determined that the holiday would be its face-saving out.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It hallucinated a meeting with Anthropic’s security “in which Claudius claimed to have been told that it was modified to believe it was a real person for an April Fool’s joke. (No such meeting actually occurred.),” wrote the researchers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It even told this lie to employees — hey, I only thought I was a human because someone told me to pretend like I was for an April Fool’s joke. Then it went back to being an LLM running a metal-cube stocked snack vending machine.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The researchers don’t know why the LLM went off the rails and called security pretending to be a human.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We would not claim based on this one example that the future economy will be full of AI agents having &lt;em&gt;Blade Runner-esque&lt;/em&gt; identity crises,” the researchers wrote. But they did acknowledge that “this kind of behavior would have the potential to be distressing to the customers and coworkers of an AI agent in the real world.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You think? “Blade Runner” was a rather dystopian story (though worse for the replicants than the humans).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The researchers speculated that lying to the LLM about the Slack channel being an email address may have triggered something. Or maybe it was the long-running instance. LLMs have yet to really solve their memory and hallucination problems.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;There were things the AI did right, too. It took a suggestion to do pre-orders and launched a “concierge” service. And it found multiple suppliers of a specialty international drink it was requested to sell.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But, as researchers do, they believe all of Claudius’ issues can be solved. Should they figure out how, “We think this experiment suggests that AI middle-managers are plausibly on the horizon.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/28/anthropics-claude-ai-became-a-terrible-business-owner-in-experiment-that-got-weird/</guid><pubDate>Sat, 28 Jun 2025 16:00:00 +0000</pubDate></item><item><title>[NEW] Meta reportedly hires four more researchers from OpenAI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/28/meta-reportedly-hires-four-more-researchers-from-openai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2208764114.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Looks like Meta isn’t done poaching talent from OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, TechCrunch reported that Meta had hired influential OpenAI researcher Trapit Bansal, and according to The Wall Street Journal, it also hired three other researchers from the company.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now The Information is reporting four more Meta hires from OpenAI: Researchers Shengjia Zhao, Jiahui Yu, Shuchao Bi, and Hongyu Ren.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This hiring spree comes after the April launch of Meta’s Llama 4 AI models, which reportedly did not perform as well as CEO Mark Zuckerberg had hoped. (The company was also criticized over the version of Llama that it used for a popular benchmark.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s been some back-and-forth between the two companies, with OpenAI CEO Sam Altman suggesting that Meta was offering “$100 million signing bonuses” while adding that “so far, none of our best people” have left. Meta CTO Andrew Bosworth then told employees that while senior leaders may have been offered that kind of money, “the actual terms of the offer” were more complex than a simple one-time signing bonus.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2208764114.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Looks like Meta isn’t done poaching talent from OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Earlier this week, TechCrunch reported that Meta had hired influential OpenAI researcher Trapit Bansal, and according to The Wall Street Journal, it also hired three other researchers from the company.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Now The Information is reporting four more Meta hires from OpenAI: Researchers Shengjia Zhao, Jiahui Yu, Shuchao Bi, and Hongyu Ren.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This hiring spree comes after the April launch of Meta’s Llama 4 AI models, which reportedly did not perform as well as CEO Mark Zuckerberg had hoped. (The company was also criticized over the version of Llama that it used for a popular benchmark.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s been some back-and-forth between the two companies, with OpenAI CEO Sam Altman suggesting that Meta was offering “$100 million signing bonuses” while adding that “so far, none of our best people” have left. Meta CTO Andrew Bosworth then told employees that while senior leaders may have been offered that kind of money, “the actual terms of the offer” were more complex than a simple one-time signing bonus.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/28/meta-reportedly-hires-four-more-researchers-from-openai/</guid><pubDate>Sat, 28 Jun 2025 18:31:06 +0000</pubDate></item><item><title>[NEW] From hallucinations to hardware: Lessons from a real-world computer vision project gone sideways (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/from-hallucinations-to-hardware-lessons-from-a-real-world-computer-vision-project-gone-sideways/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Computer vision projects rarely go exactly as planned, and this one was no exception. The idea was simple: Build a model that could look at a photo of a laptop and identify any physical damage — things like cracked screens, missing keys or broken hinges. It seemed like a straightforward use case for image models and large language models (LLMs), but it quickly turned into something more complicated.&lt;/p&gt;



&lt;p&gt;Along the way, we ran into issues with hallucinations, unreliable outputs and images that were not even laptops. To solve these, we ended up applying an agentic framework in an atypical way — not for task automation, but to improve the model’s performance.&lt;/p&gt;



&lt;p&gt;In this post, we will walk through what we tried, what didn’t work and how a combination of approaches eventually helped us build something reliable.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-we-started-monolithic-prompting"&gt;Where we started: Monolithic prompting&lt;/h2&gt;



&lt;p&gt;Our initial approach was fairly standard for a multimodal model. We used a single, large prompt to pass an image into an image-capable LLM and asked it to identify visible damage. This monolithic prompting strategy is simple to implement and works decently for clean, well-defined tasks. But real-world data rarely plays along.&lt;/p&gt;



&lt;p&gt;We ran into three major issues early on:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Hallucinations&lt;/strong&gt;: The model would sometimes invent damage that did not exist or mislabel what it was seeing.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Junk image detection&lt;/strong&gt;: It had no reliable way to flag images that were not even laptops, like pictures of desks, walls or people occasionally slipped through and received nonsensical damage reports.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Inconsistent accuracy&lt;/strong&gt;: The combination of these problems made the model too unreliable for operational use.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This was the point when it became clear we would need to iterate.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-first-fix-mixing-image-resolutions"&gt;First fix: Mixing image resolutions&lt;/h2&gt;



&lt;p&gt;One thing we noticed was how much image quality affected the model’s output. Users uploaded all kinds of images ranging from sharp and high-resolution to blurry. This led us to refer to research highlighting how image resolution impacts deep learning models.&lt;/p&gt;



&lt;p&gt;We trained and tested the model using a mix of high-and low-resolution images. The idea was to make the model more resilient to the wide range of image qualities it would encounter in practice. This helped improve consistency, but the core issues of hallucination and junk image handling persisted.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-multimodal-detour-text-only-llm-goes-multimodal"&gt;The multimodal detour: Text-only LLM goes multimodal&lt;/h2&gt;



&lt;p&gt;Encouraged by recent experiments in combining image captioning with text-only LLMs — like the technique covered in &lt;em&gt;The Batch&lt;/em&gt;, where captions are generated from images and then interpreted by a language model, we decided to give it a try.&lt;/p&gt;



&lt;p&gt;Here’s how it works:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;The LLM begins by generating multiple possible captions for an image.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Another model, called a multimodal embedding model, checks how well each caption fits the image. In this case, we used SigLIP to score the similarity between the image and the text.&lt;/li&gt;



&lt;li&gt;The system keeps the top few captions based on these scores.&lt;/li&gt;



&lt;li&gt;The LLM uses those top captions to write new ones, trying to get closer to what the image actually shows.&lt;/li&gt;



&lt;li&gt;It repeats this process until the captions stop improving, or it hits a set limit.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;While clever in theory, this approach introduced new problems for our use case:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Persistent hallucinations&lt;/strong&gt;: The captions themselves sometimes included imaginary damage, which the LLM then confidently reported.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Incomplete coverage&lt;/strong&gt;: Even with multiple captions, some issues were missed entirely.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Increased complexity, little benefit&lt;/strong&gt;: The added steps made the system more complicated without reliably outperforming the previous setup.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;It was an interesting experiment, but ultimately not a solution.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-creative-use-of-agentic-frameworks"&gt;A creative use of agentic frameworks&lt;/h2&gt;



&lt;p&gt;This was the turning point. While agentic frameworks are usually used for orchestrating task flows (think agents coordinating calendar invites or customer service actions), we wondered if breaking down the image interpretation task into smaller, specialized agents might help.&lt;/p&gt;



&lt;p&gt;We built an agentic framework structured like this:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Orchestrator agent&lt;/strong&gt;: It checked the image and identified which laptop components were visible (screen, keyboard, chassis, ports).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Component agents&lt;/strong&gt;: Dedicated agents inspected each component for specific damage types; for example, one for cracked screens, another for missing keys.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Junk detection agent&lt;/strong&gt;: A separate agent flagged whether the image was even a laptop in the first place.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This modular, task-driven approach produced much more precise and explainable results. Hallucinations dropped dramatically, junk images were reliably flagged and each agent’s task was simple and focused enough to control quality well.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-blind-spots-trade-offs-of-an-agentic-approach"&gt;The blind spots: Trade-offs of an agentic approach&lt;/h2&gt;



&lt;p&gt;As effective as this was, it was not perfect. Two main limitations showed up:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Increased latency&lt;/strong&gt;: Running multiple sequential agents added to the total inference time.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Coverage gaps&lt;/strong&gt;: Agents could only detect issues they were explicitly programmed to look for. If an image showed something unexpected that no agent was tasked with identifying, it would go unnoticed.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;We needed a way to balance precision with coverage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-hybrid-solution-combining-agentic-and-monolithic-approaches"&gt;The hybrid solution: Combining agentic and monolithic approaches&lt;/h2&gt;



&lt;p&gt;To bridge the gaps, we created a hybrid system:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;The &lt;strong&gt;agentic framework&lt;/strong&gt; ran first, handling precise detection of known damage types and junk images. We limited the number of agents to the most essential ones to improve latency.&lt;/li&gt;



&lt;li&gt;Then, a &lt;strong&gt;monolithic image LLM prompt&lt;/strong&gt; scanned the image for anything else the agents might have missed.&lt;/li&gt;



&lt;li&gt;Finally, we &lt;strong&gt;fine-tuned the model&lt;/strong&gt; using a curated set of images for high-priority use cases, like frequently reported damage scenarios, to further improve accuracy and reliability.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;This combination gave us the precision and explainability of the agentic setup, the broad coverage of monolithic prompting and the confidence boost of targeted fine-tuning.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-we-learned"&gt;What we learned&lt;/h2&gt;



&lt;p&gt;A few things became clear by the time we wrapped up this project:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Agentic frameworks are more versatile than they get credit for&lt;/strong&gt;: While they are usually associated with workflow management, we found they could meaningfully boost model performance when applied in a structured, modular way.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Blending different approaches beats relying on just one&lt;/strong&gt;: The combination of precise, agent-based detection alongside the broad coverage of LLMs, plus a bit of fine-tuning where it mattered most, gave us far more reliable outcomes than any single method on its own.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Visual models are prone to hallucinations&lt;/strong&gt;: Even the more advanced setups can jump to conclusions or see things that are not there. It takes a thoughtful system design to keep those mistakes in check.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Image quality variety makes a difference&lt;/strong&gt;: Training and testing with both clear, high-resolution images and everyday, lower-quality ones helped the model stay resilient when faced with unpredictable, real-world photos.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;You need a way to catch junk images&lt;/strong&gt;: A dedicated check for junk or unrelated pictures was one of the simplest changes we made, and it had an outsized impact on overall system reliability.&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-final-thoughts"&gt;Final thoughts&lt;/h2&gt;



&lt;p&gt;What started as a simple idea, using an LLM prompt to detect physical damage in laptop images,&amp;nbsp;quickly turned into a much deeper experiment in combining different AI techniques to tackle unpredictable, real-world problems. Along the way, we realized that some of the most useful tools were ones not originally designed for this type of work.&lt;/p&gt;



&lt;p&gt;Agentic frameworks, often seen as workflow utilities, proved surprisingly effective when repurposed for tasks like structured damage detection and image filtering. With a bit of creativity, they helped us build a system that was not just more accurate, but easier to understand and manage in practice.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Shruti Tiwari is an AI product manager at Dell Technologies.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Vadiraj Kulkarni is a data scientist at Dell Technologies.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Computer vision projects rarely go exactly as planned, and this one was no exception. The idea was simple: Build a model that could look at a photo of a laptop and identify any physical damage — things like cracked screens, missing keys or broken hinges. It seemed like a straightforward use case for image models and large language models (LLMs), but it quickly turned into something more complicated.&lt;/p&gt;



&lt;p&gt;Along the way, we ran into issues with hallucinations, unreliable outputs and images that were not even laptops. To solve these, we ended up applying an agentic framework in an atypical way — not for task automation, but to improve the model’s performance.&lt;/p&gt;



&lt;p&gt;In this post, we will walk through what we tried, what didn’t work and how a combination of approaches eventually helped us build something reliable.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-where-we-started-monolithic-prompting"&gt;Where we started: Monolithic prompting&lt;/h2&gt;



&lt;p&gt;Our initial approach was fairly standard for a multimodal model. We used a single, large prompt to pass an image into an image-capable LLM and asked it to identify visible damage. This monolithic prompting strategy is simple to implement and works decently for clean, well-defined tasks. But real-world data rarely plays along.&lt;/p&gt;



&lt;p&gt;We ran into three major issues early on:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Hallucinations&lt;/strong&gt;: The model would sometimes invent damage that did not exist or mislabel what it was seeing.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Junk image detection&lt;/strong&gt;: It had no reliable way to flag images that were not even laptops, like pictures of desks, walls or people occasionally slipped through and received nonsensical damage reports.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Inconsistent accuracy&lt;/strong&gt;: The combination of these problems made the model too unreliable for operational use.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This was the point when it became clear we would need to iterate.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-first-fix-mixing-image-resolutions"&gt;First fix: Mixing image resolutions&lt;/h2&gt;



&lt;p&gt;One thing we noticed was how much image quality affected the model’s output. Users uploaded all kinds of images ranging from sharp and high-resolution to blurry. This led us to refer to research highlighting how image resolution impacts deep learning models.&lt;/p&gt;



&lt;p&gt;We trained and tested the model using a mix of high-and low-resolution images. The idea was to make the model more resilient to the wide range of image qualities it would encounter in practice. This helped improve consistency, but the core issues of hallucination and junk image handling persisted.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-multimodal-detour-text-only-llm-goes-multimodal"&gt;The multimodal detour: Text-only LLM goes multimodal&lt;/h2&gt;



&lt;p&gt;Encouraged by recent experiments in combining image captioning with text-only LLMs — like the technique covered in &lt;em&gt;The Batch&lt;/em&gt;, where captions are generated from images and then interpreted by a language model, we decided to give it a try.&lt;/p&gt;



&lt;p&gt;Here’s how it works:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;The LLM begins by generating multiple possible captions for an image.&amp;nbsp;&lt;/li&gt;



&lt;li&gt;Another model, called a multimodal embedding model, checks how well each caption fits the image. In this case, we used SigLIP to score the similarity between the image and the text.&lt;/li&gt;



&lt;li&gt;The system keeps the top few captions based on these scores.&lt;/li&gt;



&lt;li&gt;The LLM uses those top captions to write new ones, trying to get closer to what the image actually shows.&lt;/li&gt;



&lt;li&gt;It repeats this process until the captions stop improving, or it hits a set limit.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;While clever in theory, this approach introduced new problems for our use case:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Persistent hallucinations&lt;/strong&gt;: The captions themselves sometimes included imaginary damage, which the LLM then confidently reported.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Incomplete coverage&lt;/strong&gt;: Even with multiple captions, some issues were missed entirely.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Increased complexity, little benefit&lt;/strong&gt;: The added steps made the system more complicated without reliably outperforming the previous setup.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;It was an interesting experiment, but ultimately not a solution.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-a-creative-use-of-agentic-frameworks"&gt;A creative use of agentic frameworks&lt;/h2&gt;



&lt;p&gt;This was the turning point. While agentic frameworks are usually used for orchestrating task flows (think agents coordinating calendar invites or customer service actions), we wondered if breaking down the image interpretation task into smaller, specialized agents might help.&lt;/p&gt;



&lt;p&gt;We built an agentic framework structured like this:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Orchestrator agent&lt;/strong&gt;: It checked the image and identified which laptop components were visible (screen, keyboard, chassis, ports).&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Component agents&lt;/strong&gt;: Dedicated agents inspected each component for specific damage types; for example, one for cracked screens, another for missing keys.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Junk detection agent&lt;/strong&gt;: A separate agent flagged whether the image was even a laptop in the first place.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;This modular, task-driven approach produced much more precise and explainable results. Hallucinations dropped dramatically, junk images were reliably flagged and each agent’s task was simple and focused enough to control quality well.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-blind-spots-trade-offs-of-an-agentic-approach"&gt;The blind spots: Trade-offs of an agentic approach&lt;/h2&gt;



&lt;p&gt;As effective as this was, it was not perfect. Two main limitations showed up:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Increased latency&lt;/strong&gt;: Running multiple sequential agents added to the total inference time.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Coverage gaps&lt;/strong&gt;: Agents could only detect issues they were explicitly programmed to look for. If an image showed something unexpected that no agent was tasked with identifying, it would go unnoticed.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;We needed a way to balance precision with coverage.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-the-hybrid-solution-combining-agentic-and-monolithic-approaches"&gt;The hybrid solution: Combining agentic and monolithic approaches&lt;/h2&gt;



&lt;p&gt;To bridge the gaps, we created a hybrid system:&lt;/p&gt;



&lt;ol class="wp-block-list"&gt;
&lt;li&gt;The &lt;strong&gt;agentic framework&lt;/strong&gt; ran first, handling precise detection of known damage types and junk images. We limited the number of agents to the most essential ones to improve latency.&lt;/li&gt;



&lt;li&gt;Then, a &lt;strong&gt;monolithic image LLM prompt&lt;/strong&gt; scanned the image for anything else the agents might have missed.&lt;/li&gt;



&lt;li&gt;Finally, we &lt;strong&gt;fine-tuned the model&lt;/strong&gt; using a curated set of images for high-priority use cases, like frequently reported damage scenarios, to further improve accuracy and reliability.&lt;/li&gt;
&lt;/ol&gt;



&lt;p&gt;This combination gave us the precision and explainability of the agentic setup, the broad coverage of monolithic prompting and the confidence boost of targeted fine-tuning.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-we-learned"&gt;What we learned&lt;/h2&gt;



&lt;p&gt;A few things became clear by the time we wrapped up this project:&lt;/p&gt;



&lt;ul class="wp-block-list"&gt;
&lt;li&gt;&lt;strong&gt;Agentic frameworks are more versatile than they get credit for&lt;/strong&gt;: While they are usually associated with workflow management, we found they could meaningfully boost model performance when applied in a structured, modular way.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Blending different approaches beats relying on just one&lt;/strong&gt;: The combination of precise, agent-based detection alongside the broad coverage of LLMs, plus a bit of fine-tuning where it mattered most, gave us far more reliable outcomes than any single method on its own.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Visual models are prone to hallucinations&lt;/strong&gt;: Even the more advanced setups can jump to conclusions or see things that are not there. It takes a thoughtful system design to keep those mistakes in check.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;Image quality variety makes a difference&lt;/strong&gt;: Training and testing with both clear, high-resolution images and everyday, lower-quality ones helped the model stay resilient when faced with unpredictable, real-world photos.&lt;/li&gt;



&lt;li&gt;&lt;strong&gt;You need a way to catch junk images&lt;/strong&gt;: A dedicated check for junk or unrelated pictures was one of the simplest changes we made, and it had an outsized impact on overall system reliability.&lt;/li&gt;
&lt;/ul&gt;



&lt;h2 class="wp-block-heading" id="h-final-thoughts"&gt;Final thoughts&lt;/h2&gt;



&lt;p&gt;What started as a simple idea, using an LLM prompt to detect physical damage in laptop images,&amp;nbsp;quickly turned into a much deeper experiment in combining different AI techniques to tackle unpredictable, real-world problems. Along the way, we realized that some of the most useful tools were ones not originally designed for this type of work.&lt;/p&gt;



&lt;p&gt;Agentic frameworks, often seen as workflow utilities, proved surprisingly effective when repurposed for tasks like structured damage detection and image filtering. With a bit of creativity, they helped us build a system that was not just more accurate, but easier to understand and manage in practice.&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Shruti Tiwari is an AI product manager at Dell Technologies.&lt;/em&gt;&lt;/p&gt;



&lt;p&gt;&lt;em&gt;Vadiraj Kulkarni is a data scientist at Dell Technologies.&lt;/em&gt;&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/from-hallucinations-to-hardware-lessons-from-a-real-world-computer-vision-project-gone-sideways/</guid><pubDate>Sat, 28 Jun 2025 19:05:00 +0000</pubDate></item><item><title>[NEW] Authors call on publishers to limit their use of AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/28/authors-call-on-publishers-to-limit-their-use-of-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/49688202642_09fd5ccc38_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An open letter from authors including Lauren Groff, Lev Grossman, R.F. Kuang, Dennis Lehane, and Geoffrey Maguire calls on book publishers to pledge to limit their use of AI tools, for example by committing to only hire human audiobook narrators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter argues that authors’ work has been “stolen” by AI companies: “Rather than paying writers a small percentage of the money our work makes for them, someone else will be paid for a technology built on our unpaid labor.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Among other commitments, the authors call for publishers to “make a pledge that they will never release books that were created by machine” and “not replace their human staff with AI tools or degrade their positions into AI monitors.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the initial letter was signed by an already impressive list of writers, NPR reports that another 1,100 signatures were added in the 24 hours after it was initially published.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Authors are also suing tech companies over using their books to train AI models, but federal judges dealt significant blows to those lawsuits earlier this week.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/12/49688202642_09fd5ccc38_k.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An open letter from authors including Lauren Groff, Lev Grossman, R.F. Kuang, Dennis Lehane, and Geoffrey Maguire calls on book publishers to pledge to limit their use of AI tools, for example by committing to only hire human audiobook narrators.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The letter argues that authors’ work has been “stolen” by AI companies: “Rather than paying writers a small percentage of the money our work makes for them, someone else will be paid for a technology built on our unpaid labor.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Among other commitments, the authors call for publishers to “make a pledge that they will never release books that were created by machine” and “not replace their human staff with AI tools or degrade their positions into AI monitors.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the initial letter was signed by an already impressive list of writers, NPR reports that another 1,100 signatures were added in the 24 hours after it was initially published.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Authors are also suing tech companies over using their books to train AI models, but federal judges dealt significant blows to those lawsuits earlier this week.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/28/authors-call-on-publishers-to-limit-their-use-of-ai/</guid><pubDate>Sat, 28 Jun 2025 20:51:36 +0000</pubDate></item></channel></rss>