<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 09 Oct 2025 01:38:18 +0000</lastBuildDate><item><title>From courtside to code: Tristan Thompson on AI, sports, and startups at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/tristan-thompson-on-ai-sports-and-startups-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Former NBA champion, fintech entrepreneur, and AI founder Tristan Thompson is bringing a different kind of game to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, taking place October 27-29 at San Francisco’s Moscone West — one that blends athletic grit, entrepreneurial instinct, and cutting-edge AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After more than a decade lighting up the NBA, Thompson has shifted focus from the hardwood to the innovation arena. His latest venture, &lt;strong&gt;TracyAI&lt;/strong&gt;, applies advanced artificial intelligence to sports analytics — turning real-time data into predictive insights that empower athletes, teams, and fans alike.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this fireside chat where sports meets tech. &lt;strong&gt;Register now&lt;/strong&gt; to save up to $444 on your ticket or up to &lt;strong&gt;30% on group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Tristan Thompson" class="wp-image-3055324" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Tristan-Thompson-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-unlocking-the-next-era-of-sports-and-tech"&gt;Unlocking the next era of sports and tech&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At Disrupt, Thompson will unpack how he’s using AI to reimagine performance, strategy, and fan engagement, and why athletes are increasingly shaping the startup ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Thompson’s tech story doesn’t stop there. He currently holds four C-suite roles across the web3 and fintech landscape — including chief digital equity officer at World Mobile, driving affordable community-owned internet access, and chief advisory officer at AxonDAO, an AI-powered medical research platform inspired by his brother’s epilepsy journey. He’s also the co-founder of Basketball Fun, an immersive digital experience redefining how fans connect to the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an outspoken advocate for innovation and inclusion, Thompson is part of a new wave of athlete entrepreneurs using their platforms for impact. His projects have been featured by Forbes, CNBC Crypto World, and CoinDesk, and he recently took the stage at Bitcoin 2025 to discuss athlete-driven innovation in decentralized tech.&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;“Sports taught me discipline, but tech taught me scale. When you merge those worlds, the potential is unlimited.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 class="wp-block-heading" id="h-from-nba-courts-to-ai-innovation-catch-thompson-live-at-disrupt-2025"&gt;From NBA courts to AI innovation — catch Thompson live at Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Lean in on Tristan Thompson’s fireside chat at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, where he’ll explore how AI is transforming the future of sports, startups, and storytelling — and how athletes are becoming the next generation of founders. &lt;strong&gt;Grab your Disrupt 2025 pass&lt;/strong&gt; and join the conversation on the future of AI, venture, and innovation at one of the most anticipated tech events of the year. Bringing a group? &lt;strong&gt;Save up to 30% on bundle passes&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Former NBA champion, fintech entrepreneur, and AI founder Tristan Thompson is bringing a different kind of game to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, taking place October 27-29 at San Francisco’s Moscone West — one that blends athletic grit, entrepreneurial instinct, and cutting-edge AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After more than a decade lighting up the NBA, Thompson has shifted focus from the hardwood to the innovation arena. His latest venture, &lt;strong&gt;TracyAI&lt;/strong&gt;, applies advanced artificial intelligence to sports analytics — turning real-time data into predictive insights that empower athletes, teams, and fans alike.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss this fireside chat where sports meets tech. &lt;strong&gt;Register now&lt;/strong&gt; to save up to $444 on your ticket or up to &lt;strong&gt;30% on group passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Tristan Thompson" class="wp-image-3055324" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Tristan-Thompson-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-unlocking-the-next-era-of-sports-and-tech"&gt;Unlocking the next era of sports and tech&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;At Disrupt, Thompson will unpack how he’s using AI to reimagine performance, strategy, and fan engagement, and why athletes are increasingly shaping the startup ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Thompson’s tech story doesn’t stop there. He currently holds four C-suite roles across the web3 and fintech landscape — including chief digital equity officer at World Mobile, driving affordable community-owned internet access, and chief advisory officer at AxonDAO, an AI-powered medical research platform inspired by his brother’s epilepsy journey. He’s also the co-founder of Basketball Fun, an immersive digital experience redefining how fans connect to the game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As an outspoken advocate for innovation and inclusion, Thompson is part of a new wave of athlete entrepreneurs using their platforms for impact. His projects have been featured by Forbes, CNBC Crypto World, and CoinDesk, and he recently took the stage at Bitcoin 2025 to discuss athlete-driven innovation in decentralized tech.&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;“Sports taught me discipline, but tech taught me scale. When you merge those worlds, the potential is unlimited.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 class="wp-block-heading" id="h-from-nba-courts-to-ai-innovation-catch-thompson-live-at-disrupt-2025"&gt;From NBA courts to AI innovation — catch Thompson live at Disrupt 2025&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Lean in on Tristan Thompson’s fireside chat at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, where he’ll explore how AI is transforming the future of sports, startups, and storytelling — and how athletes are becoming the next generation of founders. &lt;strong&gt;Grab your Disrupt 2025 pass&lt;/strong&gt; and join the conversation on the future of AI, venture, and innovation at one of the most anticipated tech events of the year. Bringing a group? &lt;strong&gt;Save up to 30% on bundle passes&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/tristan-thompson-on-ai-sports-and-startups-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 08 Oct 2025 14:00:00 +0000</pubDate></item><item><title>Google launches extensions system for its command-line coding tool (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/google-launches-extensions-system-for-its-command-line-coding-tool/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-08-at-9.22.17-AM.jpg?resize=1200,578" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, Google officially launched a new feature for its command-line AI system, Gemini CLI, allowing outside companies to integrate directly into the AI product. Called Gemini CLI Extensions, the feature is launching with extensions from Figma, Stripe, and other companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes just two days after OpenAI’s launch of apps in ChatGPT, which also integrated third-party systems into an AI environment. But while app access to ChatGPT is tightly curated, Gemini CLI extensions can be published with no endorsement or participation from Google. Available extensions will be hosted in public repositories on GitHub and installed manually by developers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“That open ecosystem is vital to us,” Taylor Mullen, a senior staff engineer on the project, told TechCrunch. “Everything we’re doing is grounded in a fair ecosystem that anyone can participate in.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first available extension was to Google’s own Nanobanana image generator, which was posted to GitHub last week. Once installed, the extension allows users to generate images directly from the Gemini CLI terminal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Gemini CLI has grown to over one million users, Google says, with usage heavily skewed toward software developers. Notably, Gemini CLI is heavily used in the development and maintenance of its own codebase, closely overseen by product managers, as detailed in a recent TechCrunch interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview, Google’s senior director of product management for developer tools Ryan J. Salva told TechCrunch that the purpose of the new feature was to turn Gemini CLI into “an extensibility platform, a conduit to other tools and instructions that come from elsewhere in your tool chain.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screen-Shot-2025-10-08-at-9.22.17-AM.jpg?resize=1200,578" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Wednesday, Google officially launched a new feature for its command-line AI system, Gemini CLI, allowing outside companies to integrate directly into the AI product. Called Gemini CLI Extensions, the feature is launching with extensions from Figma, Stripe, and other companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The announcement comes just two days after OpenAI’s launch of apps in ChatGPT, which also integrated third-party systems into an AI environment. But while app access to ChatGPT is tightly curated, Gemini CLI extensions can be published with no endorsement or participation from Google. Available extensions will be hosted in public repositories on GitHub and installed manually by developers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“That open ecosystem is vital to us,” Taylor Mullen, a senior staff engineer on the project, told TechCrunch. “Everything we’re doing is grounded in a fair ecosystem that anyone can participate in.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The first available extension was to Google’s own Nanobanana image generator, which was posted to GitHub last week. Once installed, the extension allows users to generate images directly from the Gemini CLI terminal.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Launched in June, Gemini CLI has grown to over one million users, Google says, with usage heavily skewed toward software developers. Notably, Gemini CLI is heavily used in the development and maintenance of its own codebase, closely overseen by product managers, as detailed in a recent TechCrunch interview.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In an interview, Google’s senior director of product management for developer tools Ryan J. Salva told TechCrunch that the purpose of the new feature was to turn Gemini CLI into “an extensibility platform, a conduit to other tools and instructions that come from elsewhere in your tool chain.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/google-launches-extensions-system-for-its-command-line-coding-tool/</guid><pubDate>Wed, 08 Oct 2025 14:00:00 +0000</pubDate></item><item><title>SoftBank bulks up its robotics portfolio with ABB Group’s robotics unit (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/softbank-bulks-up-its-robotics-portfolio-with-abb-groups-robotics-unit/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1980672274.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Japanese investing conglomerate SoftBank Group is buying a robotics company as the financial behemoth says physical AI is its next frontier.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank announced on Wednesday that it has acquired Zurich, Switzerland-based ABB Group’s robotics business unit for $5.375 billion. The deal is subject to regulatory approval; SoftBank predicts the deal will close in mid-to-late 2026 according to a press release. Sami Atiya, the head of the division, will;exit the company once the acquisition is complete, per ABB Group.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ABB’s robotics business&amp;nbsp;employs&amp;nbsp;roughly 7,000 people and&amp;nbsp;sells a variety of robots and equipment, designed for tasks like picking,&amp;nbsp;cleaning,&amp;nbsp;and painting.&amp;nbsp;The company&amp;nbsp;made&amp;nbsp;$2.3 billion&amp;nbsp;in revenue in 2024, representing 7% of ABB’s&amp;nbsp;overall revenue.&amp;nbsp;ABB&amp;nbsp;announced plans to&amp;nbsp;spin out its robotics group in&amp;nbsp;April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank said it hopes to be able to “reignite” sales at ABB’s robotics spinoff. The organization’s 2024 revenue was only $2.3 billion, down from $2.5 billion the previous year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank has been building up its investments and holdings in robotics over the last few years. The entity has invested in companies including more legacy players like AutoStore and startups including Skild AI and Agile Robots.&amp;nbsp; &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm also launched its own robotics platform, SoftBank Robotics Group, in 2014.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“SoftBank’s next frontier is physical AI,” Masayoshi Son, the chairman and CEO of SoftBank said in the company’s press release. “Together with ABB Robotics, we will unite world-class technology and talent under our shared vision to fuse Artificial Super Intelligence and robotics — driving a groundbreaking evolution that will propel humanity forward.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics is one of four focus areas for SoftBank, together with&amp;nbsp;AI chips, AI data&amp;nbsp;centers,&amp;nbsp;and energy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The center of the ‘Information Revolution’ has evolved from personal computers, the internet, and broadband to smartphones, and has now entered a new phase led by artificial intelligence,” SoftBank said in its press release. “In this context, [SoftBank Group] has declared its mission to realize artificial super intelligence (ASI) for the advancement of humanity.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to SoftBank for more information.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/GettyImages-1980672274.jpg?resize=1200,764" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Japanese investing conglomerate SoftBank Group is buying a robotics company as the financial behemoth says physical AI is its next frontier.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank announced on Wednesday that it has acquired Zurich, Switzerland-based ABB Group’s robotics business unit for $5.375 billion. The deal is subject to regulatory approval; SoftBank predicts the deal will close in mid-to-late 2026 according to a press release. Sami Atiya, the head of the division, will;exit the company once the acquisition is complete, per ABB Group.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;ABB’s robotics business&amp;nbsp;employs&amp;nbsp;roughly 7,000 people and&amp;nbsp;sells a variety of robots and equipment, designed for tasks like picking,&amp;nbsp;cleaning,&amp;nbsp;and painting.&amp;nbsp;The company&amp;nbsp;made&amp;nbsp;$2.3 billion&amp;nbsp;in revenue in 2024, representing 7% of ABB’s&amp;nbsp;overall revenue.&amp;nbsp;ABB&amp;nbsp;announced plans to&amp;nbsp;spin out its robotics group in&amp;nbsp;April.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank said it hopes to be able to “reignite” sales at ABB’s robotics spinoff. The organization’s 2024 revenue was only $2.3 billion, down from $2.5 billion the previous year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;SoftBank has been building up its investments and holdings in robotics over the last few years. The entity has invested in companies including more legacy players like AutoStore and startups including Skild AI and Agile Robots.&amp;nbsp; &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The firm also launched its own robotics platform, SoftBank Robotics Group, in 2014.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“SoftBank’s next frontier is physical AI,” Masayoshi Son, the chairman and CEO of SoftBank said in the company’s press release. “Together with ABB Robotics, we will unite world-class technology and talent under our shared vision to fuse Artificial Super Intelligence and robotics — driving a groundbreaking evolution that will propel humanity forward.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Robotics is one of four focus areas for SoftBank, together with&amp;nbsp;AI chips, AI data&amp;nbsp;centers,&amp;nbsp;and energy.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The center of the ‘Information Revolution’ has evolved from personal computers, the internet, and broadband to smartphones, and has now entered a new phase led by artificial intelligence,” SoftBank said in its press release. “In this context, [SoftBank Group] has declared its mission to realize artificial super intelligence (ASI) for the advancement of humanity.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch reached out to SoftBank for more information.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/softbank-bulks-up-its-robotics-portfolio-with-abb-groups-robotics-unit/</guid><pubDate>Wed, 08 Oct 2025 14:10:18 +0000</pubDate></item><item><title>Unveiling the next wave of Startup Battlefield 200 VC judges at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/unveiling-the-next-wave-of-startup-battlefield-200-vc-judges-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;&amp;nbsp;at&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;&amp;nbsp;is&amp;nbsp;almost here, and the pressure has never been greater. With&amp;nbsp;&lt;strong&gt;$100,000&lt;/strong&gt;&amp;nbsp;on the line, the&amp;nbsp;top 20 early-stage startups&amp;nbsp;know they must deliver their&amp;nbsp;very best&amp;nbsp;on the&amp;nbsp;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;from&amp;nbsp;October 27-29 at San Francisco’s Moscone West. Meanwhile, our VC judges will bring sharp questions, deep experience, and a discerning eye to distinguish the promising from the truly exceptional.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founders&amp;nbsp;can gain invaluable insights into what makes a winning pitch and a sustainable startup from these seasoned VCs.&amp;nbsp;Investors&amp;nbsp;can discover pitch-ready, impactful startups to add to their pipeline. Everyone else can&amp;nbsp;witness&amp;nbsp;the intense startup showdown, where the top 20 TechCrunch-vetted startups pitch for the equity-free prize and the coveted Disrupt Cup.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to watch and save up to $444 on your&amp;nbsp;pass, or&amp;nbsp;bring your team or friends and&amp;nbsp;&lt;strong&gt;save up to 30% with bundle passes&lt;/strong&gt;.&amp;nbsp;Don’t&amp;nbsp;miss this nail-biting “World Series” of pitch competitions live.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-next-batch-of-nbsp-battlefield-nbsp-200-judges"&gt;Meet the next batch of&amp;nbsp;Battlefield&amp;nbsp;200 judges&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Today,&amp;nbsp;we’re&amp;nbsp;excited to introduce the&amp;nbsp;fourth wave of judges&amp;nbsp;joining the competition. Only one more reveal&amp;nbsp;remains. Stay tuned&amp;nbsp;to the&amp;nbsp;&lt;strong&gt;Disrupt agenda&lt;/strong&gt;&amp;nbsp;to see&amp;nbsp;who the last five judges will&amp;nbsp;be.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Allison Baum Gates, Sara Ittelson, Miloni Madan Presler, Katelin Holloway, Rinki Sethi" class="wp-image-3055320" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Presler-Gates-Holloway-Ittelson-Sethi_5-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-allison-baum-gates-general-partner-nbsp-sempervirens-nbsp-venture-capital-nbsp"&gt;Allison Baum Gates, General Partner,&amp;nbsp;SemperVirens&amp;nbsp;Venture Capital&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Allison Baum Gates&amp;nbsp;is a general partner at&amp;nbsp;SemperVirens, a venture fund investing in healthcare, fintech, and enterprise SaaS. To date, she has raised hundreds of millions of dollars from LPs in the U.S., Europe, and Asia, and has deployed capital into more than 70 companies, including nine unicorns and 24 exits, and she holds 10 board positions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gates has been featured in major publications such as the Financial Times, Wall Street Journal, Forbes, and TechCrunch, and has taught hundreds of business school students. She is a lecturer in management at Stanford GSB and the author of&amp;nbsp;“Breaking into Venture”&amp;nbsp;(McGraw-Hill, 2023), a comprehensive guide for navigating the VC industry. Her second book,&amp;nbsp;“Beyond the Pitch: The Psychology of Raising VC Funding,” will be released next year. She holds a B.A. in Economics with Honors from Harvard University and is proficient in three languages.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-katelin-holloway-founding-partner-sevensevensix"&gt;Katelin Holloway,&amp;nbsp;Founding Partner, SevenSevenSix&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Katelin Holloway&amp;nbsp;is a founding partner at Alexis Ohanian’s early-stage venture capital firm,&amp;nbsp;SevenSevenSix. A seasoned investor with more than 20 years of operational experience at companies like Pixar and Reddit, Holloway backs transformative startups across diverse sectors. She invests in solutions that expand human potential and resilience, advancing health, exploring new frontiers, driving sustainability, fostering creativity, and enriching the human experience.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At SevenSevenSix, Holloway combines her operational&amp;nbsp;expertise&amp;nbsp;with a deep commitment to supporting founders as they navigate early-stage challenges and scale their visions. She is dedicated to reshaping venture capital by empowering generational entrepreneurs to tackle humanity’s biggest challenges while delivering exceptional returns.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="(L-R) Amanda Silberling, Senior Writer, Digital Culture, TechCrunch, Josh Fabian, Co-founder &amp;amp; CEO, Metafy and Katelin Holloway, Founding Partner, Seven Seven Six speak onstage during TechCrunch Disrupt 2022 on October 20, 2022 in San Francisco, California." class="wp-image-2434494" height="453" src="https://techcrunch.com/wp-content/uploads/2022/11/Amanda-Silberling-Senior-Writer-Digital-Culture-TechCrunch-Josh-Fabian-Co-founder-CEO-Metafy-and-Katelin-Holloway-Founding-Partner-Seven-Seven-Six-speak-onstage-during-TechCrunch-Disrupt-2022-on-October-20-202.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kelly Sullivan / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-miloni-madan-presler-partner-nbsp-institutional-venture-partners-ivp-nbsp"&gt;Miloni Madan Presler, Partner,&amp;nbsp;Institutional Venture Partners (IVP)&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Miloni Madan Presler&amp;nbsp;is passionate about partnering with businesses that innovate with purpose — companies that solve real problems rather than innovate for innovation’s sake. As a general partner at&amp;nbsp;Institutional Venture Partners (IVP), she focuses on supporting founders who build products and solutions that people truly need, not just want. She seeks out companies that bridge gaps in fragmented ecosystems, automate outdated processes, and deliver data-driven platforms capable of transforming industries. Madan&amp;nbsp;is driven by the belief that these companies — and their impact — will endure long beyond any single investment, and&amp;nbsp;it’s&amp;nbsp;this pursuit of lasting change that fuels her commitment and intentionality in every partnership.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-sara-nbsp-ittelson-partner-accel-nbsp"&gt;Sara&amp;nbsp;Ittelson, Partner, Accel&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Sara&amp;nbsp;Ittelson&amp;nbsp;joined&amp;nbsp;Accel&amp;nbsp;in 2022 and focuses on early-stage consumer, enterprise, and AI companies.&amp;nbsp;Before&amp;nbsp;Accel, she was&amp;nbsp;head of&amp;nbsp;strategic&amp;nbsp;partnerships at Faire and previously worked in Global Business Development at Uber.&amp;nbsp;Ittelson&amp;nbsp;is from Chico, California, and graduated from Northwestern University and Stanford.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - SEPTEMBER 19: (L-R) Accel Partner Sara Ittelson, Kindred Ventures Founder &amp;amp; Managing Partner Steve Jang, and Breakthrough Energy Ventures Partner Libby Wayman speak onstage during TechCrunch Disrupt 2023 at Moscone Center on September 19, 2023 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2602892" height="383" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1691159208.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-rinki-sethi-founding-partner-lockstep-nbsp"&gt;Rinki Sethi, Founding Partner, Lockstep&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Rinki Sethi&amp;nbsp;is&amp;nbsp;chief&amp;nbsp;security&amp;nbsp;officer at&amp;nbsp;Upwind Security&amp;nbsp;and&amp;nbsp;founding&amp;nbsp;partner at&amp;nbsp;Lockstep. She previously served as&amp;nbsp;vice&amp;nbsp;president and&amp;nbsp;chief&amp;nbsp;information&amp;nbsp;security&amp;nbsp;officer at Twitter, BILL, and Rubrik and held senior security leadership roles&amp;nbsp;at&amp;nbsp;Palo Alto Networks, IBM,&amp;nbsp;eBay,&amp;nbsp;and Intuit. Sethi brings deep&amp;nbsp;expertise&amp;nbsp;to both the private sector and corporate boards, shaping security strategy at the highest levels.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-now-s-the-time-to-save-on-your-disrupt-pass-nbsp"&gt;Now’s the time to save on your Disrupt pass&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Don’t&amp;nbsp;miss the startup pitch competition of the year, live. With&amp;nbsp;electrifying&amp;nbsp;energy, high stakes, and sharp questions, Startup Battlefield 200 has long been the launchpad for early-stage startups. Brands like Discord, Trello, and Mint got their big break on the Disrupt Stage.&amp;nbsp;&lt;strong&gt;Register here&lt;/strong&gt;&amp;nbsp;to join before ticket&amp;nbsp;prices&amp;nbsp;increase.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Startup Battlefield 200 2023" class="wp-image-3013876" height="453" src="https://techcrunch.com/wp-content/uploads/2025/06/Startup-Battlefield-2023-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;&lt;strong&gt;Startup Battlefield 200&lt;/strong&gt;&amp;nbsp;at&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;&amp;nbsp;is&amp;nbsp;almost here, and the pressure has never been greater. With&amp;nbsp;&lt;strong&gt;$100,000&lt;/strong&gt;&amp;nbsp;on the line, the&amp;nbsp;top 20 early-stage startups&amp;nbsp;know they must deliver their&amp;nbsp;very best&amp;nbsp;on the&amp;nbsp;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&amp;nbsp;from&amp;nbsp;October 27-29 at San Francisco’s Moscone West. Meanwhile, our VC judges will bring sharp questions, deep experience, and a discerning eye to distinguish the promising from the truly exceptional.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Founders&amp;nbsp;can gain invaluable insights into what makes a winning pitch and a sustainable startup from these seasoned VCs.&amp;nbsp;Investors&amp;nbsp;can discover pitch-ready, impactful startups to add to their pipeline. Everyone else can&amp;nbsp;witness&amp;nbsp;the intense startup showdown, where the top 20 TechCrunch-vetted startups pitch for the equity-free prize and the coveted Disrupt Cup.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;to watch and save up to $444 on your&amp;nbsp;pass, or&amp;nbsp;bring your team or friends and&amp;nbsp;&lt;strong&gt;save up to 30% with bundle passes&lt;/strong&gt;.&amp;nbsp;Don’t&amp;nbsp;miss this nail-biting “World Series” of pitch competitions live.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-meet-the-next-batch-of-nbsp-battlefield-nbsp-200-judges"&gt;Meet the next batch of&amp;nbsp;Battlefield&amp;nbsp;200 judges&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Today,&amp;nbsp;we’re&amp;nbsp;excited to introduce the&amp;nbsp;fourth wave of judges&amp;nbsp;joining the competition. Only one more reveal&amp;nbsp;remains. Stay tuned&amp;nbsp;to the&amp;nbsp;&lt;strong&gt;Disrupt agenda&lt;/strong&gt;&amp;nbsp;to see&amp;nbsp;who the last five judges will&amp;nbsp;be.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Allison Baum Gates, Sara Ittelson, Miloni Madan Presler, Katelin Holloway, Rinki Sethi" class="wp-image-3055320" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_Presler-Gates-Holloway-Ittelson-Sethi_5-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-allison-baum-gates-general-partner-nbsp-sempervirens-nbsp-venture-capital-nbsp"&gt;Allison Baum Gates, General Partner,&amp;nbsp;SemperVirens&amp;nbsp;Venture Capital&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Allison Baum Gates&amp;nbsp;is a general partner at&amp;nbsp;SemperVirens, a venture fund investing in healthcare, fintech, and enterprise SaaS. To date, she has raised hundreds of millions of dollars from LPs in the U.S., Europe, and Asia, and has deployed capital into more than 70 companies, including nine unicorns and 24 exits, and she holds 10 board positions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Gates has been featured in major publications such as the Financial Times, Wall Street Journal, Forbes, and TechCrunch, and has taught hundreds of business school students. She is a lecturer in management at Stanford GSB and the author of&amp;nbsp;“Breaking into Venture”&amp;nbsp;(McGraw-Hill, 2023), a comprehensive guide for navigating the VC industry. Her second book,&amp;nbsp;“Beyond the Pitch: The Psychology of Raising VC Funding,” will be released next year. She holds a B.A. in Economics with Honors from Harvard University and is proficient in three languages.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-katelin-holloway-founding-partner-sevensevensix"&gt;Katelin Holloway,&amp;nbsp;Founding Partner, SevenSevenSix&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Katelin Holloway&amp;nbsp;is a founding partner at Alexis Ohanian’s early-stage venture capital firm,&amp;nbsp;SevenSevenSix. A seasoned investor with more than 20 years of operational experience at companies like Pixar and Reddit, Holloway backs transformative startups across diverse sectors. She invests in solutions that expand human potential and resilience, advancing health, exploring new frontiers, driving sustainability, fostering creativity, and enriching the human experience.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;At SevenSevenSix, Holloway combines her operational&amp;nbsp;expertise&amp;nbsp;with a deep commitment to supporting founders as they navigate early-stage challenges and scale their visions. She is dedicated to reshaping venture capital by empowering generational entrepreneurs to tackle humanity’s biggest challenges while delivering exceptional returns.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="(L-R) Amanda Silberling, Senior Writer, Digital Culture, TechCrunch, Josh Fabian, Co-founder &amp;amp; CEO, Metafy and Katelin Holloway, Founding Partner, Seven Seven Six speak onstage during TechCrunch Disrupt 2022 on October 20, 2022 in San Francisco, California." class="wp-image-2434494" height="453" src="https://techcrunch.com/wp-content/uploads/2022/11/Amanda-Silberling-Senior-Writer-Digital-Culture-TechCrunch-Josh-Fabian-Co-founder-CEO-Metafy-and-Katelin-Holloway-Founding-Partner-Seven-Seven-Six-speak-onstage-during-TechCrunch-Disrupt-2022-on-October-20-202.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kelly Sullivan / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-miloni-madan-presler-partner-nbsp-institutional-venture-partners-ivp-nbsp"&gt;Miloni Madan Presler, Partner,&amp;nbsp;Institutional Venture Partners (IVP)&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Miloni Madan Presler&amp;nbsp;is passionate about partnering with businesses that innovate with purpose — companies that solve real problems rather than innovate for innovation’s sake. As a general partner at&amp;nbsp;Institutional Venture Partners (IVP), she focuses on supporting founders who build products and solutions that people truly need, not just want. She seeks out companies that bridge gaps in fragmented ecosystems, automate outdated processes, and deliver data-driven platforms capable of transforming industries. Madan&amp;nbsp;is driven by the belief that these companies — and their impact — will endure long beyond any single investment, and&amp;nbsp;it’s&amp;nbsp;this pursuit of lasting change that fuels her commitment and intentionality in every partnership.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-sara-nbsp-ittelson-partner-accel-nbsp"&gt;Sara&amp;nbsp;Ittelson, Partner, Accel&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Sara&amp;nbsp;Ittelson&amp;nbsp;joined&amp;nbsp;Accel&amp;nbsp;in 2022 and focuses on early-stage consumer, enterprise, and AI companies.&amp;nbsp;Before&amp;nbsp;Accel, she was&amp;nbsp;head of&amp;nbsp;strategic&amp;nbsp;partnerships at Faire and previously worked in Global Business Development at Uber.&amp;nbsp;Ittelson&amp;nbsp;is from Chico, California, and graduated from Northwestern University and Stanford.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="SAN FRANCISCO, CALIFORNIA - SEPTEMBER 19: (L-R) Accel Partner Sara Ittelson, Kindred Ventures Founder &amp;amp; Managing Partner Steve Jang, and Breakthrough Energy Ventures Partner Libby Wayman speak onstage during TechCrunch Disrupt 2023 at Moscone Center on September 19, 2023 in San Francisco, California. (Photo by Kimberly White/Getty Images for TechCrunch)" class="wp-image-2602892" height="383" src="https://techcrunch.com/wp-content/uploads/2023/09/GettyImages-1691159208.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-rinki-sethi-founding-partner-lockstep-nbsp"&gt;Rinki Sethi, Founding Partner, Lockstep&amp;nbsp;&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Rinki Sethi&amp;nbsp;is&amp;nbsp;chief&amp;nbsp;security&amp;nbsp;officer at&amp;nbsp;Upwind Security&amp;nbsp;and&amp;nbsp;founding&amp;nbsp;partner at&amp;nbsp;Lockstep. She previously served as&amp;nbsp;vice&amp;nbsp;president and&amp;nbsp;chief&amp;nbsp;information&amp;nbsp;security&amp;nbsp;officer at Twitter, BILL, and Rubrik and held senior security leadership roles&amp;nbsp;at&amp;nbsp;Palo Alto Networks, IBM,&amp;nbsp;eBay,&amp;nbsp;and Intuit. Sethi brings deep&amp;nbsp;expertise&amp;nbsp;to both the private sector and corporate boards, shaping security strategy at the highest levels.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-now-s-the-time-to-save-on-your-disrupt-pass-nbsp"&gt;Now’s the time to save on your Disrupt pass&amp;nbsp;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Don’t&amp;nbsp;miss the startup pitch competition of the year, live. With&amp;nbsp;electrifying&amp;nbsp;energy, high stakes, and sharp questions, Startup Battlefield 200 has long been the launchpad for early-stage startups. Brands like Discord, Trello, and Mint got their big break on the Disrupt Stage.&amp;nbsp;&lt;strong&gt;Register here&lt;/strong&gt;&amp;nbsp;to join before ticket&amp;nbsp;prices&amp;nbsp;increase.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Startup Battlefield 200 2023" class="wp-image-3013876" height="453" src="https://techcrunch.com/wp-content/uploads/2025/06/Startup-Battlefield-2023-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/unveiling-the-next-wave-of-startup-battlefield-200-vc-judges-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 08 Oct 2025 14:30:00 +0000</pubDate></item><item><title>Google’s virtual try-on shopping tool expands to more countries, now lets you try on shoes (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/googles-virtual-try-on-shopping-tool-expands-to-more-countries-now-lets-you-try-on-shoes/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that its AI feature that lets users virtually try on clothes is expanding to Australia, Canada, and Japan. The tech giant also announced the feature now lets users virtually try on shoes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works by asking users to upload a photo to see how real clothes might look on them. Now, users can visualize how different pairs of shoes would look on them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To virtually try on a pair of shoes, users need to tap on any product listing on Google, select the “Try It On” button, then add a full-length photo of themselves. After a few seconds, they will see the shoes from the listing on a digital version of themselves. Users have the option to save or share the image with others.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055414" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-08-at-10.39.44AM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch comes two months after Google introduced the ability for users to virtually try on clothes using AI. While Google had already offered&amp;nbsp;virtual try-on technology before, the earlier features focused on showing items on a diverse range of models’ bodies. With the new AI feature, the company started allowing users to try clothes on a virtual version of their own body.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been investing in the virtual try-on space in other ways as well. In June, the tech giant launched an&amp;nbsp;experimental app called Doppl&amp;nbsp;that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While both the virtual try-on feature, which now includes shoes, and Doppl are powered by the same generative AI technology, Doppl is designed to let shoppers dive even deeper into virtual try-on, helping them curate their personal style. Plus, Doppl can create AI-generated videos so users can get a better sense of how the outfit would look on them in real life.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google isn’t the only company to launch virtual try-on technology, as both Amazon and Walmart have introduced similar features.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Google announced on Wednesday that its AI feature that lets users virtually try on clothes is expanding to Australia, Canada, and Japan. The tech giant also announced the feature now lets users virtually try on shoes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature works by asking users to upload a photo to see how real clothes might look on them. Now, users can visualize how different pairs of shoes would look on them.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To virtually try on a pair of shoes, users need to tap on any product listing on Google, select the “Try It On” button, then add a full-length photo of themselves. After a few seconds, they will see the shoes from the listing on a digital version of themselves. Users have the option to save or share the image with others.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055414" height="680" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-08-at-10.39.44AM.png?w=652" width="652" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The launch comes two months after Google introduced the ability for users to virtually try on clothes using AI. While Google had already offered&amp;nbsp;virtual try-on technology before, the earlier features focused on showing items on a diverse range of models’ bodies. With the new AI feature, the company started allowing users to try clothes on a virtual version of their own body.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google has been investing in the virtual try-on space in other ways as well. In June, the tech giant launched an&amp;nbsp;experimental app called Doppl&amp;nbsp;that uses AI to visualize how different outfits might look on you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While both the virtual try-on feature, which now includes shoes, and Doppl are powered by the same generative AI technology, Doppl is designed to let shoppers dive even deeper into virtual try-on, helping them curate their personal style. Plus, Doppl can create AI-generated videos so users can get a better sense of how the outfit would look on them in real life.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google isn’t the only company to launch virtual try-on technology, as both Amazon and Walmart have introduced similar features.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/googles-virtual-try-on-shopping-tool-expands-to-more-countries-now-lets-you-try-on-shoes/</guid><pubDate>Wed, 08 Oct 2025 14:49:01 +0000</pubDate></item><item><title>Ganiga will showcase its waste-sorting robots at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/ganiga-will-showcase-its-waste-sorting-robots-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Despite the well-known environmental benefits of recycling, it’s estimated that less than 10% of the world’s plastic gets recycled. Ganiga Innovation looks to bring that percentage up using AI-enabled robotic waste bins.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Italian startup Ganiga built three products to help better manage waste and recycling. The first is a fleet of robotic waste bins, called Hoooly, that use generative AI to determine what is trash and what is recycling and sort the waste accordingly. The second is a smart lid that can be fitted to existing waste bins with the same functionality as its larger bin counterpart.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company also has a software product that allows companies to track the waste they produce; it offers suggestions for how a company can reduce waste production based on its waste data.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga will be showing off its tech as part of this year’s Startup Battlefield competition at TechCrunch Disrupt 2025, which runs October 27 to 29 at San Francisco’s Moscone West.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nicolas Zeoli, the founder and CEO of Ganiga, told TechCrunch that he’s had dreams of building the next great company, like Facebook or Apple, since he was younger.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He decided to focus on waste because he said issues surrounding waste management are very tangible in his native Italy — and it was clear there wasn’t much being done about it.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We all need to reclaim this problem,” Zeoli said. “I read 100 articles on this problem. For example, in one year, only in one year, in the whole world over 100 million tons of plastic is created and only 9% is recycled. This is a very real problem.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Zeoli launched Ganiga in 2021 and built its first prototype in 2022. Zeoli said they decided to focus on building a bin to solve this problem because it gives people a physical place to put waste that can ensure it gets properly recycled and sorted, and because the bins spit out data that can be used for the future.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Waste management is also expensive for companies, Zeoli said. Many organizations, especially in Europe, have ESG mandates to adhere to. Zeoli hopes Hoooly can help companies better track their waste production to help them reduce waste and waste-related costs down the line.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga started selling its bins in 2024 and has since sold more than 120 robots to customers like Google and to multiple airports, including the ones in Bologna, Venice, and Madrid, among others.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zeoli said the company made $500,000 in revenue in 2024 and is already up to $750,000 in just the first nine months of 2025.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also raised $1.5 million in pre-seed funding from investors, including clean tech VC firm NextSTEP and NextEnergy Capital, among others. Ganiga is looking to raise a $3 million seed round.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is gearing up to launch its latest product in November, the Hooolyfood, which is a software product that uses camera images to determine the exact amount of food waste. The company plans to delve into further software-focused products in the future, too, Zeoli said, based on the data their current bins and software is collecting.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga has focused on the European market thus far, but Zeoli said he’s hoping to expand into the U.S.; the company is even thinking of moving its headquarters stateside in 2026.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ganiga is the first startup in all the world to fill one airport with the smart bins,” Zeoli said. “This is important because we don’t target the prototype; we are a product, and we are open to the market.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;If you want to learn from Ganiga firsthand, and see dozens of additional pitches and valuable workshops and make the connections that drive business results, head here to learn more about this year’s Disrupt, taking place October 27 to 29 in San Francisco.&lt;/em&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025" class="wp-image-3048094" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Disrupt_General_Article_No-Anniversary_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Despite the well-known environmental benefits of recycling, it’s estimated that less than 10% of the world’s plastic gets recycled. Ganiga Innovation looks to bring that percentage up using AI-enabled robotic waste bins.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Italian startup Ganiga built three products to help better manage waste and recycling. The first is a fleet of robotic waste bins, called Hoooly, that use generative AI to determine what is trash and what is recycling and sort the waste accordingly. The second is a smart lid that can be fitted to existing waste bins with the same functionality as its larger bin counterpart.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company also has a software product that allows companies to track the waste they produce; it offers suggestions for how a company can reduce waste production based on its waste data.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga will be showing off its tech as part of this year’s Startup Battlefield competition at TechCrunch Disrupt 2025, which runs October 27 to 29 at San Francisco’s Moscone West.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nicolas Zeoli, the founder and CEO of Ganiga, told TechCrunch that he’s had dreams of building the next great company, like Facebook or Apple, since he was younger.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He decided to focus on waste because he said issues surrounding waste management are very tangible in his native Italy — and it was clear there wasn’t much being done about it.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We all need to reclaim this problem,” Zeoli said. “I read 100 articles on this problem. For example, in one year, only in one year, in the whole world over 100 million tons of plastic is created and only 9% is recycled. This is a very real problem.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Zeoli launched Ganiga in 2021 and built its first prototype in 2022. Zeoli said they decided to focus on building a bin to solve this problem because it gives people a physical place to put waste that can ensure it gets properly recycled and sorted, and because the bins spit out data that can be used for the future.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Waste management is also expensive for companies, Zeoli said. Many organizations, especially in Europe, have ESG mandates to adhere to. Zeoli hopes Hoooly can help companies better track their waste production to help them reduce waste and waste-related costs down the line.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga started selling its bins in 2024 and has since sold more than 120 robots to customers like Google and to multiple airports, including the ones in Bologna, Venice, and Madrid, among others.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Zeoli said the company made $500,000 in revenue in 2024 and is already up to $750,000 in just the first nine months of 2025.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has also raised $1.5 million in pre-seed funding from investors, including clean tech VC firm NextSTEP and NextEnergy Capital, among others. Ganiga is looking to raise a $3 million seed round.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company is gearing up to launch its latest product in November, the Hooolyfood, which is a software product that uses camera images to determine the exact amount of food waste. The company plans to delve into further software-focused products in the future, too, Zeoli said, based on the data their current bins and software is collecting.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Ganiga has focused on the European market thus far, but Zeoli said he’s hoping to expand into the U.S.; the company is even thinking of moving its headquarters stateside in 2026.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Ganiga is the first startup in all the world to fill one airport with the smart bins,” Zeoli said. “This is important because we don’t target the prototype; we are a product, and we are open to the market.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;If you want to learn from Ganiga firsthand, and see dozens of additional pitches and valuable workshops and make the connections that drive business results, head here to learn more about this year’s Disrupt, taking place October 27 to 29 in San Francisco.&lt;/em&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025" class="wp-image-3048094" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_Disrupt_General_Article_No-Anniversary_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/ganiga-will-showcase-its-waste-sorting-robots-at-techcrunch-disrupt-2025/</guid><pubDate>Wed, 08 Oct 2025 15:59:45 +0000</pubDate></item><item><title>Vandals deface ads for AI necklaces that listen to all your conversations (AI – Ars Technica)</title><link>https://arstechnica.com/tech-policy/2025/10/vandals-deface-ads-for-ai-necklaces-that-listen-to-all-your-conversations/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Critics attacked subway ads to defend human friends and broadly criticize AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A New York City subway ad takeover promoting an AI friend necklace sparked widespread vandalism.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Gerald Zaffuts | iStock Editorial / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;"AI doesn't care," a vandal scrawled on a New York subway ad promoting a wearable AI pendant called Friend, which was designed to monitor a user's everyday conversations and serve as a companion "who listens, responds, and supports you."&lt;/p&gt;
&lt;p&gt;"Human connection is sacred," the vandal wrote, emphasizing, "AI is &lt;em&gt;not &lt;/em&gt;your friend."&lt;/p&gt;
&lt;p&gt;This act of vandalism is now part of a huge online archive collecting defaced ads that the Friend campaign inspired, as many New Yorkers responded with vitriol to marketing claims that the AI "friend" would never "bail on dinner" or abandon you to ride the subway alone.&lt;/p&gt;
&lt;p&gt;"Friends don't let friends sell their souls," another vandal wrote.&lt;/p&gt;
&lt;p&gt;Others criticizing the AI pendant—which sells for $129 and will be available in Walmart stores soon—used the ads to lob larger political complaints about AI. For example, on an ad suggesting an AI friend would "never leave dirty dishes in the sink," a vandal called out AI data centers like xAI's for "poisoning black communities." Another wrote that "freely giving your personal info to Big Tech won't heal your wounds," urging subway riders to join the backlash against Palantir, which The Guardian reported uses AI to surveil individuals and identify military targets.&lt;/p&gt;
&lt;p&gt;A review of the archive shows that much of the criticism focused on the potential for the pendant to spy on users. "AI surveillance slop," one vandal summed it up, while another defaced an ad promising "I'll binge the entire series with you" to say "I'll steal your info, steal your data, steal your identity."&lt;/p&gt;
&lt;p&gt;According&amp;nbsp;to The New York Times, over the past six weeks, the Friend ads have become "one of the most talked about subway marketing campaigns in recent memory," after 22-year-old Friend founder Avi Schiffmann paid less than $1 million to flood MTA subway cars with ads across New York.&amp;nbsp;Since its rollout in New York, the campaign has spread to Los Angeles, and Chicago will be next, but MTA subway cars were targeted first to drive as much hype as possible, Schiffman confirmed.&lt;/p&gt;
&lt;p&gt;"Only the MTA allows you to buy a full takeover like that," Schiffman told the NYT. "It almost feels illegal."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to backlash over feared surveillance capitalism, critics have accused Schiffman of taking advantage of the loneliness epidemic. Conducting a survey last year, researchers with Harvard Graduate School of Education’s Making Caring Common found that people between "30-44 years of age were the loneliest group." Overall, 73 percent of those surveyed "selected technology as contributing to loneliness in the country."&lt;/p&gt;
&lt;p&gt;But Schiffman rejects these criticisms, telling the NYT that his AI Friend pendant is intended to supplement human friends, not replace them, supposedly helping to raise the "average emotional intelligence" of users "significantly."&lt;/p&gt;
&lt;p&gt;"I don’t view this as dystopian," Schiffman said, suggesting that "the AI friend is a new category of companionship, one that will coexist alongside traditional friends rather than replace them," the NYT reported. "We have a cat and a dog and a child and an adult in the same room," the Friend founder said. "Why not an AI?"&lt;/p&gt;
&lt;p&gt;The MTA has not commented on the controversy, but Victoria Mottesheard—a vice president at Outfront Media, which manages MTA advertising—told the NYT that the Friend campaign blew up because AI "is the conversation of 2025."&lt;/p&gt;
&lt;h2&gt;Website lets anyone deface Friend ads&lt;/h2&gt;
&lt;p&gt;So far, the Friend ads have not yielded significant sales, Schiffman confirmed, telling the NYT that only 3,100 have sold. He expects that society isn't ready for AI companions to be promoted at such a large scale and that his ad campaign will help normalize AI friends.&lt;/p&gt;
&lt;p&gt;In the meantime, critics have rushed to attack Friend on social media, inspiring a website where anyone can vandalize a Friend ad and share it online. That website has received close to 6,000 submissions so far, its creator, Marc Mueller, told the NYT, and visitors can take a tour of these submissions by choosing "ride train to see more" after creating their own vandalized version.&lt;/p&gt;
&lt;p&gt;For visitors to Mueller's site, riding the train displays a carousel documenting backlash to Friend, as well as "performance art" by visitors poking fun at the ads in less serious ways. One example showed a vandalized ad changing "Friend" to "Fries," with a crude illustration of McDonald's French fries, while another transformed the ad into a campaign for "fried chicken."&lt;/p&gt;
&lt;p&gt;Others were seemingly more serious about turning the ad into a warning. One vandal drew a bunch of arrows pointing to the "end" in Friend while turning the pendant into a cry-face emoji, seemingly drawing attention to research on the mental health risks of relying on AI companions—including the alleged suicide risks of products like Character.AI and ChatGPT, which have spawned lawsuits and prompted a Senate hearing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Critics attacked subway ads to defend human friends and broadly criticize AI.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/GettyImages-2182000985-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A New York City subway ad takeover promoting an AI friend necklace sparked widespread vandalism.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Gerald Zaffuts | iStock Editorial / Getty Images Plus

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;"AI doesn't care," a vandal scrawled on a New York subway ad promoting a wearable AI pendant called Friend, which was designed to monitor a user's everyday conversations and serve as a companion "who listens, responds, and supports you."&lt;/p&gt;
&lt;p&gt;"Human connection is sacred," the vandal wrote, emphasizing, "AI is &lt;em&gt;not &lt;/em&gt;your friend."&lt;/p&gt;
&lt;p&gt;This act of vandalism is now part of a huge online archive collecting defaced ads that the Friend campaign inspired, as many New Yorkers responded with vitriol to marketing claims that the AI "friend" would never "bail on dinner" or abandon you to ride the subway alone.&lt;/p&gt;
&lt;p&gt;"Friends don't let friends sell their souls," another vandal wrote.&lt;/p&gt;
&lt;p&gt;Others criticizing the AI pendant—which sells for $129 and will be available in Walmart stores soon—used the ads to lob larger political complaints about AI. For example, on an ad suggesting an AI friend would "never leave dirty dishes in the sink," a vandal called out AI data centers like xAI's for "poisoning black communities." Another wrote that "freely giving your personal info to Big Tech won't heal your wounds," urging subway riders to join the backlash against Palantir, which The Guardian reported uses AI to surveil individuals and identify military targets.&lt;/p&gt;
&lt;p&gt;A review of the archive shows that much of the criticism focused on the potential for the pendant to spy on users. "AI surveillance slop," one vandal summed it up, while another defaced an ad promising "I'll binge the entire series with you" to say "I'll steal your info, steal your data, steal your identity."&lt;/p&gt;
&lt;p&gt;According&amp;nbsp;to The New York Times, over the past six weeks, the Friend ads have become "one of the most talked about subway marketing campaigns in recent memory," after 22-year-old Friend founder Avi Schiffmann paid less than $1 million to flood MTA subway cars with ads across New York.&amp;nbsp;Since its rollout in New York, the campaign has spread to Los Angeles, and Chicago will be next, but MTA subway cars were targeted first to drive as much hype as possible, Schiffman confirmed.&lt;/p&gt;
&lt;p&gt;"Only the MTA allows you to buy a full takeover like that," Schiffman told the NYT. "It almost feels illegal."&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;In addition to backlash over feared surveillance capitalism, critics have accused Schiffman of taking advantage of the loneliness epidemic. Conducting a survey last year, researchers with Harvard Graduate School of Education’s Making Caring Common found that people between "30-44 years of age were the loneliest group." Overall, 73 percent of those surveyed "selected technology as contributing to loneliness in the country."&lt;/p&gt;
&lt;p&gt;But Schiffman rejects these criticisms, telling the NYT that his AI Friend pendant is intended to supplement human friends, not replace them, supposedly helping to raise the "average emotional intelligence" of users "significantly."&lt;/p&gt;
&lt;p&gt;"I don’t view this as dystopian," Schiffman said, suggesting that "the AI friend is a new category of companionship, one that will coexist alongside traditional friends rather than replace them," the NYT reported. "We have a cat and a dog and a child and an adult in the same room," the Friend founder said. "Why not an AI?"&lt;/p&gt;
&lt;p&gt;The MTA has not commented on the controversy, but Victoria Mottesheard—a vice president at Outfront Media, which manages MTA advertising—told the NYT that the Friend campaign blew up because AI "is the conversation of 2025."&lt;/p&gt;
&lt;h2&gt;Website lets anyone deface Friend ads&lt;/h2&gt;
&lt;p&gt;So far, the Friend ads have not yielded significant sales, Schiffman confirmed, telling the NYT that only 3,100 have sold. He expects that society isn't ready for AI companions to be promoted at such a large scale and that his ad campaign will help normalize AI friends.&lt;/p&gt;
&lt;p&gt;In the meantime, critics have rushed to attack Friend on social media, inspiring a website where anyone can vandalize a Friend ad and share it online. That website has received close to 6,000 submissions so far, its creator, Marc Mueller, told the NYT, and visitors can take a tour of these submissions by choosing "ride train to see more" after creating their own vandalized version.&lt;/p&gt;
&lt;p&gt;For visitors to Mueller's site, riding the train displays a carousel documenting backlash to Friend, as well as "performance art" by visitors poking fun at the ads in less serious ways. One example showed a vandalized ad changing "Friend" to "Fries," with a crude illustration of McDonald's French fries, while another transformed the ad into a campaign for "fried chicken."&lt;/p&gt;
&lt;p&gt;Others were seemingly more serious about turning the ad into a warning. One vandal drew a bunch of arrows pointing to the "end" in Friend while turning the pendant into a cry-face emoji, seemingly drawing attention to research on the mental health risks of relying on AI companions—including the alleged suicide risks of products like Character.AI and ChatGPT, which have spawned lawsuits and prompted a Senate hearing.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2025/10/vandals-deface-ads-for-ai-necklaces-that-listen-to-all-your-conversations/</guid><pubDate>Wed, 08 Oct 2025 16:07:29 +0000</pubDate></item><item><title>Zendesk says its new AI agent can solve 80% of support issues (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/zendesk-says-its-new-ai-agent-can-solve-80-of-support-issues/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/04/GettyImages-1082161742.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Zendesk announced Wednesday at its AI summit a string of LLM-driven products meant to reshape the company’s reliance on human technicians.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The center of the new features is an autonomous support agent that Zendesk believes will solve 80% of support issues without human intervention. That system will be supplemented by a co-pilot agent, which will assist human technicians with the remaining 20% of issues, as well as an admin-layer agent, a voice-based agent, and an analytics agent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Shashi Upadhyay, Zendesk’s president of Product, Engineering and AI, the new agents are part of a broader change in the support industry, as AI replaces much of the work that was previously done by humans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The world’s going to shift from software that’s built for human users, to a system where AI actually does most of the work,” Upadhyay told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Independent benchmarks suggest that contemporary AI models are capable of taking on the work. TAU-bench, which was designed to measure a model’s&amp;nbsp;tool-calling ability,&amp;nbsp;includes a scenario in which models have to process a returned product — a close analogue to many support tasks. The current leader, Claude Sonnet 4.5, solves 85% of issues on the test.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After a chaotic investor fight in 2022, Zendesk has made a string of AI acquisitions that laid the groundwork for the current shift. The analytics agent launching today is built directly on the company’s Hyperarc acquisition, which was completed in July. Earlier AI acquisitions include the QA and agentic service system Klaus (acquired in February 2024) and the automation platform Ultimate (acquired the following March).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zendesk has previewed the new system with existing customers, and Upadhyay says the results have been promising.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“For customers that have been using it, consumer satisfaction has been up by five to 10 points,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Large language models have often been deployed for customer support, although rarely at the Zendesk’s scale. Companies from Airbnb to Regal Theaters have already experimented with in-house chatbot support, often contracting directly with foundation model labs. But those systems typically deal with information retrieval rather than more complex troubleshooting or taking self-directed action.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If the new push for AI-based support is successful, the economic implications would be significant. Zendesk’s Resolution Platform already supports nearly 20,000 customers, resolving 4.6 billion tickets each year. Beyond Zendesk, the U.S. employs 2.4 million customer service representatives — with far larger workforces in other countries.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/04/GettyImages-1082161742.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Zendesk announced Wednesday at its AI summit a string of LLM-driven products meant to reshape the company’s reliance on human technicians.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The center of the new features is an autonomous support agent that Zendesk believes will solve 80% of support issues without human intervention. That system will be supplemented by a co-pilot agent, which will assist human technicians with the remaining 20% of issues, as well as an admin-layer agent, a voice-based agent, and an analytics agent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;According to Shashi Upadhyay, Zendesk’s president of Product, Engineering and AI, the new agents are part of a broader change in the support industry, as AI replaces much of the work that was previously done by humans.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The world’s going to shift from software that’s built for human users, to a system where AI actually does most of the work,” Upadhyay told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Independent benchmarks suggest that contemporary AI models are capable of taking on the work. TAU-bench, which was designed to measure a model’s&amp;nbsp;tool-calling ability,&amp;nbsp;includes a scenario in which models have to process a returned product — a close analogue to many support tasks. The current leader, Claude Sonnet 4.5, solves 85% of issues on the test.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After a chaotic investor fight in 2022, Zendesk has made a string of AI acquisitions that laid the groundwork for the current shift. The analytics agent launching today is built directly on the company’s Hyperarc acquisition, which was completed in July. Earlier AI acquisitions include the QA and agentic service system Klaus (acquired in February 2024) and the automation platform Ultimate (acquired the following March).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Zendesk has previewed the new system with existing customers, and Upadhyay says the results have been promising.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“For customers that have been using it, consumer satisfaction has been up by five to 10 points,” he told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Large language models have often been deployed for customer support, although rarely at the Zendesk’s scale. Companies from Airbnb to Regal Theaters have already experimented with in-house chatbot support, often contracting directly with foundation model labs. But those systems typically deal with information retrieval rather than more complex troubleshooting or taking self-directed action.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If the new push for AI-based support is successful, the economic implications would be significant. Zendesk’s Resolution Platform already supports nearly 20,000 customers, resolving 4.6 billion tickets each year. Beyond Zendesk, the U.S. employs 2.4 million customer service representatives — with far larger workforces in other countries.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/zendesk-says-its-new-ai-agent-can-solve-80-of-support-issues/</guid><pubDate>Wed, 08 Oct 2025 17:00:25 +0000</pubDate></item><item><title>Using generative AI to diversify virtual training grounds for robots (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/using-generative-ai-diversify-virtual-training-grounds-robots-1008</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-csail-restaurant.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-19f1f3f3-7fff-ddef-2547-d3fc1f5d3464"&gt;Chatbots like ChatGPT and Claude have experienced a meteoric rise in usage over the past three years because they can help you with a wide range of tasks. Whether you’re writing Shakespearean sonnets, debugging code, or need an answer to an obscure trivia question, artificial intelligence systems seem to have you covered. The source of this versatility? Billions, or even trillions, of textual data points across the internet.&lt;/p&gt;&lt;p dir="ltr"&gt;Those data aren’t enough to teach a robot to be a helpful household or factory assistant, though. To understand how to handle, stack, and place various arrangements of objects across diverse environments, robots need demonstrations. You can think of robot training data as a collection of how-to videos that walk the systems through each motion of a task. Collecting these demonstrations on real robots is time-consuming and not perfectly repeatable, so engineers have created training data by generating simulations with AI (which don’t often reflect real-world physics), or tediously handcrafting each digital environment from scratch.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Toyota Research Institute may have found a way to create the diverse, realistic training grounds robots need. Their “steerable scene generation” approach creates digital scenes of things like kitchens, living rooms, and restaurants that engineers can use to simulate lots of real-world interactions and scenarios. Trained on over 44 million 3D rooms filled with models of objects such as tables and plates, the tool places existing assets in new scenes, then refines each one into a physically accurate, lifelike environment.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation creates these 3D worlds by “steering” a diffusion model — an AI system that generates a visual from random noise — toward a scene you’d find in everyday life. The researchers used this generative system to “in-paint” an environment, filling in particular elements throughout the scene. You can imagine a blank canvas suddenly turning into a kitchen scattered with 3D objects, which are gradually rearranged into a scene that imitates real-world physics. For example, the system ensures that a fork doesn’t pass through a bowl on a table — a common glitch in 3D graphics known as “clipping,” where models overlap or intersect.&lt;/p&gt;&lt;p&gt;How exactly steerable scene generation guides its creation toward realism, however, depends on the strategy you choose. Its main strategy is “Monte Carlo tree search” (MCTS), where the model creates a series of alternative scenes, filling them out in different ways toward a particular objective (like making a scene more physically realistic, or including as many edible items as possible). It’s used by the AI program AlphaGo to beat human opponents in Go (a game similar to chess), as the system considers potential sequences of moves before choosing the most advantageous one.&lt;/p&gt;&lt;p&gt;“We are the first to apply MCTS to scene generation by framing the scene generation task as a sequential decision-making process,” says MIT Department of Electrical Engineering and Computer Science (EECS) PhD student Nicholas Pfaff, who is a CSAIL researcher and a lead author on a&amp;nbsp;paper presenting the work. “We keep building on top of partial scenes to produce better or more desired scenes over time. As a result, MCTS creates scenes that are more complex than what the diffusion model was trained on.”&lt;/p&gt;&lt;p dir="ltr"&gt;In one particularly telling experiment, MCTS added the maximum number of objects to a simple restaurant scene. It featured as many as 34 items on a table, including massive stacks of dim sum dishes, after training on scenes with only 17 objects on average.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation also allows you to generate diverse training scenarios via reinforcement learning — essentially, teaching a diffusion model to fulfill an objective by trial-and-error. After you train on the initial data, your system undergoes a second training stage, where you outline a reward (basically, a desired outcome with a score indicating how close you are to that goal). The model automatically learns to create scenes with higher scores, often producing scenarios that are quite different from those it was trained on.&lt;/p&gt;&lt;p&gt;Users can also prompt the system directly by typing in specific visual descriptions (like “a kitchen with four apples and a bowl on the table”). Then, steerable scene generation can bring your requests to life with precision. For example, the tool accurately followed users’ prompts at rates of 98 percent when building scenes of pantry shelves, and 86 percent for messy breakfast tables. Both marks are at least a 10 percent improvement over comparable methods like&amp;nbsp;“MiDiffusion” and “DiffuScene.”&lt;/p&gt;&lt;p&gt;The system can also complete specific scenes via prompting or light directions (like “come up with a different scene arrangement using the same objects”). You could ask it to place apples on several plates on a kitchen table, for instance, or put board games and books on a shelf. It’s essentially “filling in the blank” by slotting items in empty spaces, but preserving the rest of a scene.&lt;/p&gt;&lt;p dir="ltr"&gt;According to the researchers, the strength of their project lies in its ability to create many scenes that roboticists can actually use. “A key insight from our findings is that it’s OK for the scenes we pre-trained on to not exactly resemble the scenes that we actually want,” says Pfaff. “Using our steering methods, we can move beyond that broad distribution and sample from a ‘better’ one. In other words, generating the diverse, realistic, and task-aligned scenes that we actually want to train our robots in.”&lt;/p&gt;&lt;p dir="ltr"&gt;Such vast scenes became the testing grounds where they could record a virtual robot interacting with different items. The machine carefully placed forks and knives into a cutlery holder, for instance, and rearranged bread onto plates in various 3D settings. Each simulation appeared fluid and realistic, resembling the real-world, adaptable robots steerable scene generation could help train, one day.&lt;/p&gt;&lt;p dir="ltr"&gt;While the system could be an encouraging path forward in generating lots of diverse training data for robots, the researchers say their work is more of a proof of concept. In the future, they’d like to use generative AI to create entirely new objects and scenes, instead of using a fixed library of assets. They also plan to incorporate articulated objects that the robot could open or twist (like cabinets or jars filled with food) to make the scenes even more interactive.&lt;/p&gt;&lt;p dir="ltr"&gt;To make their virtual environments even more realistic, Pfaff and his colleagues may incorporate real-world objects by using a library of objects and scenes pulled from images on the internet and using their previous work on “Scalable Real2Sim.” By expanding how diverse and lifelike AI-constructed robot testing grounds can be, the team hopes to build a community of users that’ll create lots of data, which could then be used as a massive dataset to teach dexterous robots different skills.&lt;/p&gt;&lt;p&gt;“Today, creating realistic scenes for simulation can be quite a challenging endeavor; procedural generation can readily produce a large number of scenes, but they likely won’t be representative of the environments the robot would encounter in the real world. Manually creating bespoke scenes is both time-consuming and expensive,” says Jeremy Binagia, an applied scientist at Amazon Robotics who wasn’t involved in the paper. “Steerable scene generation offers a better approach: train a generative model on a large collection of pre-existing scenes and adapt it (using a strategy such as reinforcement learning) to specific downstream applications. Compared to previous works that leverage an off-the-shelf vision-language model or focus just on arranging objects in a 2D grid, this approach guarantees physical feasibility and considers full 3D translation and rotation, enabling the generation of much more interesting scenes.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Steerable scene generation with post training and inference-time search provides a novel and efficient framework for automating scene generation at scale,” says Toyota Research Institute roboticist Rick Cory SM ’08, PhD ’10, who also wasn’t involved in the paper. “Moreover, it can generate ‘never-before-seen’ scenes that are deemed important for downstream tasks. In the future, combining this framework with vast internet data could unlock an important milestone towards efficient training of robots for deployment in the real world.”&lt;/p&gt;&lt;p&gt;Pfaff wrote the paper with senior author Russ Tedrake, the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT; a senior vice president of large behavior models at the Toyota Research Institute; and CSAIL principal investigator. Other authors were Toyota Research Institute robotics researcher Hongkai Dai SM ’12, PhD ’16; team lead and Senior Research Scientist Sergey Zakharov; and Carnegie Mellon University PhD student Shun Iwase. Their work was supported, in part, by Amazon and the Toyota Research Institute. The researchers presented their work at the Conference on Robot Learning (CoRL) in September.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202509/mit-csail-restaurant.gif" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p dir="ltr" id="docs-internal-guid-19f1f3f3-7fff-ddef-2547-d3fc1f5d3464"&gt;Chatbots like ChatGPT and Claude have experienced a meteoric rise in usage over the past three years because they can help you with a wide range of tasks. Whether you’re writing Shakespearean sonnets, debugging code, or need an answer to an obscure trivia question, artificial intelligence systems seem to have you covered. The source of this versatility? Billions, or even trillions, of textual data points across the internet.&lt;/p&gt;&lt;p dir="ltr"&gt;Those data aren’t enough to teach a robot to be a helpful household or factory assistant, though. To understand how to handle, stack, and place various arrangements of objects across diverse environments, robots need demonstrations. You can think of robot training data as a collection of how-to videos that walk the systems through each motion of a task. Collecting these demonstrations on real robots is time-consuming and not perfectly repeatable, so engineers have created training data by generating simulations with AI (which don’t often reflect real-world physics), or tediously handcrafting each digital environment from scratch.&lt;/p&gt;&lt;p dir="ltr"&gt;Researchers at MIT’s Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Toyota Research Institute may have found a way to create the diverse, realistic training grounds robots need. Their “steerable scene generation” approach creates digital scenes of things like kitchens, living rooms, and restaurants that engineers can use to simulate lots of real-world interactions and scenarios. Trained on over 44 million 3D rooms filled with models of objects such as tables and plates, the tool places existing assets in new scenes, then refines each one into a physically accurate, lifelike environment.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation creates these 3D worlds by “steering” a diffusion model — an AI system that generates a visual from random noise — toward a scene you’d find in everyday life. The researchers used this generative system to “in-paint” an environment, filling in particular elements throughout the scene. You can imagine a blank canvas suddenly turning into a kitchen scattered with 3D objects, which are gradually rearranged into a scene that imitates real-world physics. For example, the system ensures that a fork doesn’t pass through a bowl on a table — a common glitch in 3D graphics known as “clipping,” where models overlap or intersect.&lt;/p&gt;&lt;p&gt;How exactly steerable scene generation guides its creation toward realism, however, depends on the strategy you choose. Its main strategy is “Monte Carlo tree search” (MCTS), where the model creates a series of alternative scenes, filling them out in different ways toward a particular objective (like making a scene more physically realistic, or including as many edible items as possible). It’s used by the AI program AlphaGo to beat human opponents in Go (a game similar to chess), as the system considers potential sequences of moves before choosing the most advantageous one.&lt;/p&gt;&lt;p&gt;“We are the first to apply MCTS to scene generation by framing the scene generation task as a sequential decision-making process,” says MIT Department of Electrical Engineering and Computer Science (EECS) PhD student Nicholas Pfaff, who is a CSAIL researcher and a lead author on a&amp;nbsp;paper presenting the work. “We keep building on top of partial scenes to produce better or more desired scenes over time. As a result, MCTS creates scenes that are more complex than what the diffusion model was trained on.”&lt;/p&gt;&lt;p dir="ltr"&gt;In one particularly telling experiment, MCTS added the maximum number of objects to a simple restaurant scene. It featured as many as 34 items on a table, including massive stacks of dim sum dishes, after training on scenes with only 17 objects on average.&lt;/p&gt;&lt;p dir="ltr"&gt;Steerable scene generation also allows you to generate diverse training scenarios via reinforcement learning — essentially, teaching a diffusion model to fulfill an objective by trial-and-error. After you train on the initial data, your system undergoes a second training stage, where you outline a reward (basically, a desired outcome with a score indicating how close you are to that goal). The model automatically learns to create scenes with higher scores, often producing scenarios that are quite different from those it was trained on.&lt;/p&gt;&lt;p&gt;Users can also prompt the system directly by typing in specific visual descriptions (like “a kitchen with four apples and a bowl on the table”). Then, steerable scene generation can bring your requests to life with precision. For example, the tool accurately followed users’ prompts at rates of 98 percent when building scenes of pantry shelves, and 86 percent for messy breakfast tables. Both marks are at least a 10 percent improvement over comparable methods like&amp;nbsp;“MiDiffusion” and “DiffuScene.”&lt;/p&gt;&lt;p&gt;The system can also complete specific scenes via prompting or light directions (like “come up with a different scene arrangement using the same objects”). You could ask it to place apples on several plates on a kitchen table, for instance, or put board games and books on a shelf. It’s essentially “filling in the blank” by slotting items in empty spaces, but preserving the rest of a scene.&lt;/p&gt;&lt;p dir="ltr"&gt;According to the researchers, the strength of their project lies in its ability to create many scenes that roboticists can actually use. “A key insight from our findings is that it’s OK for the scenes we pre-trained on to not exactly resemble the scenes that we actually want,” says Pfaff. “Using our steering methods, we can move beyond that broad distribution and sample from a ‘better’ one. In other words, generating the diverse, realistic, and task-aligned scenes that we actually want to train our robots in.”&lt;/p&gt;&lt;p dir="ltr"&gt;Such vast scenes became the testing grounds where they could record a virtual robot interacting with different items. The machine carefully placed forks and knives into a cutlery holder, for instance, and rearranged bread onto plates in various 3D settings. Each simulation appeared fluid and realistic, resembling the real-world, adaptable robots steerable scene generation could help train, one day.&lt;/p&gt;&lt;p dir="ltr"&gt;While the system could be an encouraging path forward in generating lots of diverse training data for robots, the researchers say their work is more of a proof of concept. In the future, they’d like to use generative AI to create entirely new objects and scenes, instead of using a fixed library of assets. They also plan to incorporate articulated objects that the robot could open or twist (like cabinets or jars filled with food) to make the scenes even more interactive.&lt;/p&gt;&lt;p dir="ltr"&gt;To make their virtual environments even more realistic, Pfaff and his colleagues may incorporate real-world objects by using a library of objects and scenes pulled from images on the internet and using their previous work on “Scalable Real2Sim.” By expanding how diverse and lifelike AI-constructed robot testing grounds can be, the team hopes to build a community of users that’ll create lots of data, which could then be used as a massive dataset to teach dexterous robots different skills.&lt;/p&gt;&lt;p&gt;“Today, creating realistic scenes for simulation can be quite a challenging endeavor; procedural generation can readily produce a large number of scenes, but they likely won’t be representative of the environments the robot would encounter in the real world. Manually creating bespoke scenes is both time-consuming and expensive,” says Jeremy Binagia, an applied scientist at Amazon Robotics who wasn’t involved in the paper. “Steerable scene generation offers a better approach: train a generative model on a large collection of pre-existing scenes and adapt it (using a strategy such as reinforcement learning) to specific downstream applications. Compared to previous works that leverage an off-the-shelf vision-language model or focus just on arranging objects in a 2D grid, this approach guarantees physical feasibility and considers full 3D translation and rotation, enabling the generation of much more interesting scenes.”&lt;/p&gt;&lt;p dir="ltr"&gt;“Steerable scene generation with post training and inference-time search provides a novel and efficient framework for automating scene generation at scale,” says Toyota Research Institute roboticist Rick Cory SM ’08, PhD ’10, who also wasn’t involved in the paper. “Moreover, it can generate ‘never-before-seen’ scenes that are deemed important for downstream tasks. In the future, combining this framework with vast internet data could unlock an important milestone towards efficient training of robots for deployment in the real world.”&lt;/p&gt;&lt;p&gt;Pfaff wrote the paper with senior author Russ Tedrake, the Toyota Professor of Electrical Engineering and Computer Science, Aeronautics and Astronautics, and Mechanical Engineering at MIT; a senior vice president of large behavior models at the Toyota Research Institute; and CSAIL principal investigator. Other authors were Toyota Research Institute robotics researcher Hongkai Dai SM ’12, PhD ’16; team lead and Senior Research Scientist Sergey Zakharov; and Carnegie Mellon University PhD student Shun Iwase. Their work was supported, in part, by Amazon and the Toyota Research Institute. The researchers presented their work at the Conference on Robot Learning (CoRL) in September.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/using-generative-ai-diversify-virtual-training-grounds-robots-1008</guid><pubDate>Wed, 08 Oct 2025 17:45:00 +0000</pubDate></item><item><title>[NEW] How AI-Powered Wireless Networks Will Revitalize US Global Leadership in Communications (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-6g-telecommunications/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;span&gt;Telecommunication networks are critical infrastructure for every nation, underpinning the global flow of communications and data across billions of smartphones and connected devices. The next generation of these networks, known as 6G, will for the first time be built to support AI traffic — and can be&amp;nbsp;powered by AI from the ground up. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The nations and companies leading this shift, known as AI-native 6G, are poised to lead the global AI economy and lead in the AI era.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Unlike previous network evolutions, the development and deployment of 6G will fundamentally redefine the architecture and functionality of global communications networks to go beyond connectivity. For the first time, AI will enable these networks to sense and infer at the edge, serving hundreds of billions of AI-powered endpoints and supporting AI-driven applications such as autonomous vehicles, precision agriculture and advanced manufacturing. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;How these next-generation networks are designed, deployed and managed has important implications for the economy as well as for national security. Since 6G networks will power billions of devices and run mission-critical applications, it’s more important than ever to determine who builds and operates these networks. &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;NVIDIA is leading a coalition of U.S. companies&lt;/span&gt;&lt;span&gt; committed to developing a high-performance, secure and reliable AI-native 6G solution under the AI-Native Wireless Networks project, aka AI-WIN. By designing a powerful 6G network solution running on American technology, the U.S. has an opportunity to regain global leadership by accelerating the path to AI-native 6G across the nation and worldwide.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span class="TextRun SCXW210273977 BCX4" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun SCXW210273977 BCX4"&gt;From 1G to 6G: The Evolution of Networks and Connected Devices&lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun SCXW210273977 BCX4" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun SCXW210273977 BCX4"&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;span class="EOP SCXW210273977 BCX4"&gt;&amp;nbsp;&lt;/span&gt;&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;As the technology and the types of data being transmitted over networks evolved, each generation of networks — and the standards that guide them — progressed to handle new use cases and complexity.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;While 1G supported analog voice services, 2G introduced text. In the 3G era, the first smartphones were enabled, and internet browsing became mainstream. In 4G, users experienced true mobile broadband — while 5G brought faster data transfers and greater throughput, enabling many telecom operators to offer unlimited data as part of their wireless services.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_85542"&gt;&lt;img alt="alt" class="size-full wp-image-85542" height="226" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/The-evolution-of-5G-Source-OKportal-Technology-2019.png" width="850" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-85542"&gt;The evolution of 5G. Source: OKportal Technology, via ResearchGate&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;Today, the &lt;/span&gt;&lt;span&gt;number of mobile phone subscriptions around the world&lt;/span&gt;&lt;span&gt; exceeds the human population, and AI services are expected to proliferate at all points in the network, including at the edge. To support these billions of devices, the world will need AI-native 6G.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;&lt;span&gt;Five Advantages of AI-Native 6G Development&lt;/span&gt;&lt;/b&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;The convergence of AI and wireless infrastructure will fundamentally reshape the global telecommunications and AI landscape. Building 6G networks on a foundation of AI will make networks more efficient and enable them to bring AI services closer to end users. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Here are five benefits of building AI into 6G infrastructure from the ground up.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Built-In Capability to Deliver AI Services at the Edge&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;Telecommunications networks traditionally delivered voice, data and video.  In the future, 6G networks will also be called upon to support a new kind of traffic: AI traffic.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI built into 6G infrastructure will enable the delivery of AI services through inference at the edge. This means that when a consumer interacts with an AI service, that workload can be handled by edge data centers in the 6G network, as well as centralized data centers when required.&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mobile networks will support data-intensive, AI-powered use cases including autonomous vehicle fleet management, smart glasses, generative AI services and AI agents on phones or other devices, integrated sensing, interactive web and video streaming, collaborative robots, drone detection and other applications that are yet to be invented. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The transmission of this massive increase in uplink and bursty data needs to be resilient, secure and purpose-built.  &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Extreme Efficiency Across Spectrum Usage, Energy Usage and Operations&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;The most important raw material for wireless networks is spectrum: the airwaves over which two-way data transmission takes place.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Since the amount of spectrum available to serve the exponential rise in AI traffic is finite, new, AI-based techniques are needed to optimize the usage of every bit of spectrum.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI-native algorithms deployed with 6G networks will deliver extreme spectrum efficiency, enabling billions of dollars in cost savings while improving user experience for all connected devices. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI-native hardware and software will also enable extreme energy efficiency at scale, reducing the overall operational costs for telcos. And agentic AI will improve and automate network management and operations, greatly improving reliability, resiliency and network performance.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; New Revenue Opportunities for Carriers&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;6G networks built on AI will &lt;/span&gt;&lt;span&gt;create new revenue streams for telecommunications companies. Accelerated computing as the backbone of telecommunications infrastructure will drive new returns on capital investments that extend far beyond providing data connectivity. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;One estimate suggests that telco operators can earn roughly $5 in AI inference revenue from every $1 invested in new AI radio access network (AI-RAN) infrastructure. And, with the network efficiency gains provided by AI, operators will be able to harness excess capacity to run AI workloads concurrently with cellular services.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Software-Defined Infrastructure&amp;nbsp;&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;Wireless networks can now innovate at the pace of AI, without having to make new hardware investments with every generation. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Telecommunications infrastructure today largely consists of single-purpose, hardware-defined systems that are built to deliver voice, video and data services. With this setup, innovation cycles are long and tied to hardware upgrades with each new generation —&amp;nbsp;and companies must still invest in different infrastructure to handle AI computing. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Moving toward software-defined RAN architecture will enable one common infrastructure stack to run mobile wireless services and AI applications, while making it easy to evolve to the next generation with a software upgrade. This will enable a rapid pace of innovation in telecoms, opening the field for many to participate and collectively deliver new capabilities and services for humans, AI agents and machines.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Shifting a network’s underlying infrastructure from single-purpose technology to off-the-shelf, multi-purpose infrastructure is a game changer that can enable telcos to maximize every new dollar spent.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Enhanced Cybersecurity&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;In the 6G era, cybersecurity enters a new dimension where AI is a critical requirement across the network’s topology and deeply embedded into every layer of the technology stack. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;As 6G networks connect billions of IoT devices, AI is essential for real-time threat detection and automated remediation and incident response. AI models can process massive data streams to quickly identify and neutralize attacks, whether they’re occurring on the device, at the network edge or in the cloud.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI-driven cybersecurity can help keep hyper-connected systems — such as self-driving vehicles and&amp;nbsp;massive factories — resilient, secure and adaptive. And to ensure that security, it’s imperative that 6G networks deployed across the world are operated by trusted companies committed to the principles of transparency and openness.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;&lt;span&gt;Meeting the 6G Moment&lt;/span&gt;&lt;/b&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;International standards committees have already started the work to define the upcoming standards for 6G. These standards are set by industry, academia and government experts who agree on technical specifications for each new generation of wireless technology.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;For the past several decades, international competitors have dominated global network deployments. They’re racing to do the same with 6G, building on their existing technology stacks and rapidly integrating AI compute and sensing capabilities to create a fabric for massively distributed intelligence.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;By developing a strong, AI-powered 6G network running on American technology, the U.S. will have a powerful tool to compete worldwide and advance open and transparent technology ecosystems.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;Learn about &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;NVIDIA solutions for 5G and 6G networks&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt; and join the &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;NVIDIA 6G Developer Program&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;.&lt;/span&gt;&lt;/i&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;span&gt;Telecommunication networks are critical infrastructure for every nation, underpinning the global flow of communications and data across billions of smartphones and connected devices. The next generation of these networks, known as 6G, will for the first time be built to support AI traffic — and can be&amp;nbsp;powered by AI from the ground up. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The nations and companies leading this shift, known as AI-native 6G, are poised to lead the global AI economy and lead in the AI era.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Unlike previous network evolutions, the development and deployment of 6G will fundamentally redefine the architecture and functionality of global communications networks to go beyond connectivity. For the first time, AI will enable these networks to sense and infer at the edge, serving hundreds of billions of AI-powered endpoints and supporting AI-driven applications such as autonomous vehicles, precision agriculture and advanced manufacturing. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;How these next-generation networks are designed, deployed and managed has important implications for the economy as well as for national security. Since 6G networks will power billions of devices and run mission-critical applications, it’s more important than ever to determine who builds and operates these networks. &amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;NVIDIA is leading a coalition of U.S. companies&lt;/span&gt;&lt;span&gt; committed to developing a high-performance, secure and reliable AI-native 6G solution under the AI-Native Wireless Networks project, aka AI-WIN. By designing a powerful 6G network solution running on American technology, the U.S. has an opportunity to regain global leadership by accelerating the path to AI-native 6G across the nation and worldwide.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;&lt;span class="TextRun SCXW210273977 BCX4" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun SCXW210273977 BCX4"&gt;From 1G to 6G: The Evolution of Networks and Connected Devices&lt;/span&gt;&lt;/span&gt;&lt;span class="TextRun SCXW210273977 BCX4" lang="EN-US" xml:lang="EN-US"&gt;&lt;span class="NormalTextRun SCXW210273977 BCX4"&gt;&amp;nbsp;&lt;/span&gt;&lt;/span&gt;&lt;span class="EOP SCXW210273977 BCX4"&gt;&amp;nbsp;&lt;/span&gt;&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;As the technology and the types of data being transmitted over networks evolved, each generation of networks — and the standards that guide them — progressed to handle new use cases and complexity.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;While 1G supported analog voice services, 2G introduced text. In the 3G era, the first smartphones were enabled, and internet browsing became mainstream. In 4G, users experienced true mobile broadband — while 5G brought faster data transfers and greater throughput, enabling many telecom operators to offer unlimited data as part of their wireless services.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_85542"&gt;&lt;img alt="alt" class="size-full wp-image-85542" height="226" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/The-evolution-of-5G-Source-OKportal-Technology-2019.png" width="850" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-85542"&gt;The evolution of 5G. Source: OKportal Technology, via ResearchGate&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;span&gt;Today, the &lt;/span&gt;&lt;span&gt;number of mobile phone subscriptions around the world&lt;/span&gt;&lt;span&gt; exceeds the human population, and AI services are expected to proliferate at all points in the network, including at the edge. To support these billions of devices, the world will need AI-native 6G.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;&lt;span&gt;Five Advantages of AI-Native 6G Development&lt;/span&gt;&lt;/b&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;The convergence of AI and wireless infrastructure will fundamentally reshape the global telecommunications and AI landscape. Building 6G networks on a foundation of AI will make networks more efficient and enable them to bring AI services closer to end users. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Here are five benefits of building AI into 6G infrastructure from the ground up.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Built-In Capability to Deliver AI Services at the Edge&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;Telecommunications networks traditionally delivered voice, data and video.  In the future, 6G networks will also be called upon to support a new kind of traffic: AI traffic.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI built into 6G infrastructure will enable the delivery of AI services through inference at the edge. This means that when a consumer interacts with an AI service, that workload can be handled by edge data centers in the 6G network, as well as centralized data centers when required.&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Mobile networks will support data-intensive, AI-powered use cases including autonomous vehicle fleet management, smart glasses, generative AI services and AI agents on phones or other devices, integrated sensing, interactive web and video streaming, collaborative robots, drone detection and other applications that are yet to be invented. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;The transmission of this massive increase in uplink and bursty data needs to be resilient, secure and purpose-built.  &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="2"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Extreme Efficiency Across Spectrum Usage, Energy Usage and Operations&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;The most important raw material for wireless networks is spectrum: the airwaves over which two-way data transmission takes place.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Since the amount of spectrum available to serve the exponential rise in AI traffic is finite, new, AI-based techniques are needed to optimize the usage of every bit of spectrum.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI-native algorithms deployed with 6G networks will deliver extreme spectrum efficiency, enabling billions of dollars in cost savings while improving user experience for all connected devices. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI-native hardware and software will also enable extreme energy efficiency at scale, reducing the overall operational costs for telcos. And agentic AI will improve and automate network management and operations, greatly improving reliability, resiliency and network performance.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; New Revenue Opportunities for Carriers&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;6G networks built on AI will &lt;/span&gt;&lt;span&gt;create new revenue streams for telecommunications companies. Accelerated computing as the backbone of telecommunications infrastructure will drive new returns on capital investments that extend far beyond providing data connectivity. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;One estimate suggests that telco operators can earn roughly $5 in AI inference revenue from every $1 invested in new AI radio access network (AI-RAN) infrastructure. And, with the network efficiency gains provided by AI, operators will be able to harness excess capacity to run AI workloads concurrently with cellular services.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="4"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Software-Defined Infrastructure&amp;nbsp;&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;Wireless networks can now innovate at the pace of AI, without having to make new hardware investments with every generation. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Telecommunications infrastructure today largely consists of single-purpose, hardware-defined systems that are built to deliver voice, video and data services. With this setup, innovation cycles are long and tied to hardware upgrades with each new generation —&amp;nbsp;and companies must still invest in different infrastructure to handle AI computing. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Moving toward software-defined RAN architecture will enable one common infrastructure stack to run mobile wireless services and AI applications, while making it easy to evolve to the next generation with a software upgrade. This will enable a rapid pace of innovation in telecoms, opening the field for many to participate and collectively deliver new capabilities and services for humans, AI agents and machines.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Shifting a network’s underlying infrastructure from single-purpose technology to off-the-shelf, multi-purpose infrastructure is a game changer that can enable telcos to maximize every new dollar spent.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;ol start="5"&gt;
&lt;li&gt;
&lt;h3&gt;&lt;b&gt;&lt;span&gt; Enhanced Cybersecurity&lt;/span&gt;&lt;/b&gt;&lt;/h3&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span&gt;In the 6G era, cybersecurity enters a new dimension where AI is a critical requirement across the network’s topology and deeply embedded into every layer of the technology stack. &lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;As 6G networks connect billions of IoT devices, AI is essential for real-time threat detection and automated remediation and incident response. AI models can process massive data streams to quickly identify and neutralize attacks, whether they’re occurring on the device, at the network edge or in the cloud.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;AI-driven cybersecurity can help keep hyper-connected systems — such as self-driving vehicles and&amp;nbsp;massive factories — resilient, secure and adaptive. And to ensure that security, it’s imperative that 6G networks deployed across the world are operated by trusted companies committed to the principles of transparency and openness.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;&lt;span&gt;Meeting the 6G Moment&lt;/span&gt;&lt;/b&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;International standards committees have already started the work to define the upcoming standards for 6G. These standards are set by industry, academia and government experts who agree on technical specifications for each new generation of wireless technology.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;For the past several decades, international competitors have dominated global network deployments. They’re racing to do the same with 6G, building on their existing technology stacks and rapidly integrating AI compute and sensing capabilities to create a fabric for massively distributed intelligence.&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;By developing a strong, AI-powered 6G network running on American technology, the U.S. will have a powerful tool to compete worldwide and advance open and transparent technology ecosystems.&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;Learn about &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;NVIDIA solutions for 5G and 6G networks&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt; and join the &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;NVIDIA 6G Developer Program&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;.&lt;/span&gt;&lt;/i&gt;&lt;span&gt;&amp;nbsp;&lt;/span&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-6g-telecommunications/</guid><pubDate>Wed, 08 Oct 2025 17:50:51 +0000</pubDate></item><item><title>[NEW] Sora’s downloads in its first week was nearly as big as ChatGPT’s launch (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/soras-downloads-in-its-first-week-was-nearly-as-big-as-chatgpts-launch/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After OpenAI’s video-generating app Sora surged to the No. 1 position on the U.S. App Store, it has now, technically, experienced a bigger first week than ChatGPT on iOS, according to new data from app intelligence provider Appfigures. Its estimates show that Sora saw 627,000 iOS downloads in its first seven days of availability, compared with ChatGPT’s 606,000 iOS downloads during its first week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the fairest comparison, however, because ChatGPT was available only in the U.S. during its first week, while Sora is currently offered in the U.S. and Canada at launch. Still, Appfigures says that Canada contributed about 45,000 installs, so the Sora launch was about 96% of ChatGPT’s launch, if the data had been based on the U.S.&amp;nbsp;numbers only.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This level of consumer adoption is worth noting because Sora remains an invite-only app, while ChatGPT was more publicly available at launch. That makes Sora’s performance more impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its first day, Sora saw 56,000 app installs in short order, bumping the app to become the No. 3 Top Overall app on the U.S. App Store. By Friday, October 3, it reached No. 1. That surge had already put Sora’s debut ahead of other major AI app launches, including Anthropic’s Claude and Microsoft’s Copilot, and put it on par with xAI’s Grok launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A quick scan of social media provides plenty of anecdotes that support Appfigures’ data. Sora videos, which uses the new Sora 2 video model and gives users the ability to generate realistic deepfakes, seem to be everywhere. Users are even creating deepfakes of dead people, a use case that has prompted Zelda Williams, daughter of the late actor Robin Williams, to ask folks to stop sending her AI-generated images of her father.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055495" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/thumbnail_sora-first-week-downloads-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Per Appfigures, the app has seen steady adoption since its first day on the market, September 30, 2025. Its data indicates that daily downloads on iOS hit a high mark of 107,800 downloads on October 1, 2025. It has since seen between lows of 84,400 daily installs (on October 6) and 98,500 daily installs (on October 4).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that’s not quite as high as earlier in the week, it’s still decent numbers for an app that not everyone can yet use.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After OpenAI’s video-generating app Sora surged to the No. 1 position on the U.S. App Store, it has now, technically, experienced a bigger first week than ChatGPT on iOS, according to new data from app intelligence provider Appfigures. Its estimates show that Sora saw 627,000 iOS downloads in its first seven days of availability, compared with ChatGPT’s 606,000 iOS downloads during its first week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This isn’t the fairest comparison, however, because ChatGPT was available only in the U.S. during its first week, while Sora is currently offered in the U.S. and Canada at launch. Still, Appfigures says that Canada contributed about 45,000 installs, so the Sora launch was about 96% of ChatGPT’s launch, if the data had been based on the U.S.&amp;nbsp;numbers only.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This level of consumer adoption is worth noting because Sora remains an invite-only app, while ChatGPT was more publicly available at launch. That makes Sora’s performance more impressive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;During its first day, Sora saw 56,000 app installs in short order, bumping the app to become the No. 3 Top Overall app on the U.S. App Store. By Friday, October 3, it reached No. 1. That surge had already put Sora’s debut ahead of other major AI app launches, including Anthropic’s Claude and Microsoft’s Copilot, and put it on par with xAI’s Grok launch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;A quick scan of social media provides plenty of anecdotes that support Appfigures’ data. Sora videos, which uses the new Sora 2 video model and gives users the ability to generate realistic deepfakes, seem to be everywhere. Users are even creating deepfakes of dead people, a use case that has prompted Zelda Williams, daughter of the late actor Robin Williams, to ask folks to stop sending her AI-generated images of her father.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3055495" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/thumbnail_sora-first-week-downloads-1.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Appfigures&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Per Appfigures, the app has seen steady adoption since its first day on the market, September 30, 2025. Its data indicates that daily downloads on iOS hit a high mark of 107,800 downloads on October 1, 2025. It has since seen between lows of 84,400 daily installs (on October 6) and 98,500 daily installs (on October 4).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While that’s not quite as high as earlier in the week, it’s still decent numbers for an app that not everyone can yet use.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/soras-downloads-in-its-first-week-was-nearly-as-big-as-chatgpts-launch/</guid><pubDate>Wed, 08 Oct 2025 18:12:54 +0000</pubDate></item><item><title>[NEW] Samsung AI researcher's new, open reasoning model TRM outperforms models 10,000X larger — on specific problems (AI | VentureBeat)</title><link>https://venturebeat.com/ai/samsung-ai-researchers-new-open-reasoning-model-trm-outperforms-models-10</link><description>[unable to retrieve full-text content]&lt;p&gt;The trend of AI researchers developing new, &lt;a href="https://www.linkedin.com/pulse/next-big-thing-ai-think-small-models-venturebeat-yyrte/?trackingId=x3X3vTZhTnmwCTUtOWGAug%3D%3D"&gt;small&lt;/a&gt; open source generative models that outperform far larger, proprietary peers continued this week with yet another staggering advancement.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Alexia Jolicoeur-Martineau&lt;/b&gt;, Senior AI Researcher at &lt;b&gt;Samsung&amp;#x27;s Advanced​ Institute of Technology (SAIT)&lt;/b&gt; in Montreal, Canada,&lt;b&gt;​ &lt;/b&gt;has &lt;a href="https://x.com/jm_alexia/status/1975560628657164426"&gt;&lt;b&gt;introduced the Tiny Recursion Model (TRM)&lt;/b&gt;&lt;/a&gt; — a neural network so small it contains just 7 million parameters (internal model settings), yet it competes with or surpasses cutting-edge language models 10,000 times larger in terms of their parameter count, including&lt;b&gt; OpenAI&amp;#x27;s o3-mini and Google&amp;#x27;s Gemini 2.5 Pro,&lt;/b&gt; on some of the toughest reasoning benchmarks in AI research. &lt;/p&gt;&lt;p&gt;The goal is to show that very highly performant new AI models can be created affordably without massive investments in the graphics processing units (GPUs) and power needed to train the larger, multi-trillion parameter flagship models powering many LLM chatbots today. The results were described in a research paper published on open access website arxiv.org, entitled &amp;quot;&lt;a href="https://arxiv.org/abs/2510.04871v1"&gt;&lt;i&gt;Less is More: Recursive Reasoning with Tiny Networks&lt;/i&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;&amp;quot;The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to solve hard tasks is a trap,&amp;quot; wrote Jolicoeur-Martineau on the &lt;a href="https://x.com/jm_alexia/status/1975561300890812793"&gt;social network X&lt;/a&gt;. &amp;quot;Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Jolicoeur-Martineau also added: &amp;quot;With recursive reasoning, it turns out that &amp;#x27;less is more&amp;#x27;. &lt;b&gt;A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.&amp;quot;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TRM&amp;#x27;s code is available now on &lt;a href="https://github.com/SamsungSAILMontreal/TinyRecursiveModels"&gt;Github&lt;/a&gt; under an enterprise-friendly, commercially viable MIT License — meaning anyone from researchers to companies can take, modify it, and deploy it for their own purposes, even commercial applications.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;One Big Caveat&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;However, readers should be aware that TRM was designed specifically to perform well on structured, visual, grid-based problems like Sudoku, mazes, and puzzles on the &lt;a href="https://arcprize.org/arc-agi"&gt;ARC (Abstract and Reasoning Corpus)-AGI benchmark&lt;/a&gt;, the latter which offers tasks that should be easy for humans but difficult for AI models, such sorting colors on a grid based on a prior, but not identical, solution. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Hierarchy to Simplicity&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The TRM architecture represents a radical simplification. &lt;/p&gt;&lt;p&gt;It builds upon a technique called &lt;b&gt;Hierarchical Reasoning Model (HRM)&lt;/b&gt; introduced earlier this year, which showed that small networks could tackle logical puzzles like Sudoku and mazes. &lt;/p&gt;&lt;p&gt;HRM relied on two cooperating networks—one operating at high frequency, the other at low—supported by biologically inspired arguments and mathematical justifications involving fixed-point theorems. Jolicoeur-Martineau found this unnecessarily complicated.&lt;/p&gt;&lt;p&gt;TRM strips these elements away. Instead of two networks, it uses a &lt;b&gt;single two-layer model&lt;/b&gt; that recursively refines its own predictions. &lt;/p&gt;&lt;p&gt;The model begins with an embedded question and an initial answer, represented by variables &lt;b&gt;x&lt;/b&gt;, &lt;b&gt;y&lt;/b&gt;, and &lt;b&gt;z&lt;/b&gt;. Through a series of reasoning steps, it updates its internal latent representation &lt;b&gt;z&lt;/b&gt; and refines the answer &lt;b&gt;y&lt;/b&gt; until it converges on a stable output. Each iteration corrects potential errors from the previous step, yielding a self-improving reasoning process without extra hierarchy or mathematical overhead.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How Recursion Replaces Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core idea behind TRM is that &lt;i&gt;recursion can substitute for depth and size.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;By iteratively reasoning over its own output, the network effectively simulates a much deeper architecture without the associated memory or computational cost. This recursive cycle, run over as many as sixteen supervision steps, allows the model to make progressively better predictions — similar in spirit to how large language models use multi-step “chain-of-thought” reasoning, but achieved here with a compact, feed-forward design.&lt;/p&gt;&lt;p&gt;The simplicity pays off in both efficiency and generalization. The model uses fewer layers, no fixed-point approximations, and no dual-network hierarchy. A lightweight &lt;b&gt;halting mechanism&lt;/b&gt; decides when to stop refining, preventing wasted computation while maintaining accuracy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance That Punches Above Its Weight&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its small footprint, TRM delivers benchmark results that rival or exceed models millions of times larger. In testing, the model achieved:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;87.4% accuracy&lt;/b&gt; on &lt;b&gt;Sudoku-Extreme&lt;/b&gt; (up from 55% for HRM)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;85% accuracy&lt;/b&gt; on &lt;b&gt;Maze-Hard&lt;/b&gt; puzzles&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;45% accuracy&lt;/b&gt; on &lt;b&gt;ARC-AGI-1&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;8% accuracy&lt;/b&gt; on &lt;b&gt;ARC-AGI-2&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These results surpass or closely match performance from several high-end large language models, including &lt;b&gt;DeepSeek R1&lt;/b&gt;, &lt;b&gt;Gemini 2.5 Pro&lt;/b&gt;, and &lt;b&gt;o3-mini&lt;/b&gt;, despite TRM using less than 0.01% of their parameters.&lt;/p&gt;&lt;p&gt;Such results suggest that recursive reasoning, not scale, may be the key to handling abstract and combinatorial reasoning problems — domains where even top-tier generative models often stumble.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Design Philosophy: Less Is More&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;TRM’s success stems from deliberate minimalism. Jolicoeur-Martineau found that reducing complexity led to better generalization. &lt;/p&gt;&lt;p&gt;When the researcher increased layer count or model size, performance declined due to overfitting on small datasets. &lt;/p&gt;&lt;p&gt;By contrast, the two-layer structure, combined with recursive depth and &lt;b&gt;deep supervision&lt;/b&gt;, achieved optimal results.&lt;/p&gt;&lt;p&gt;The model also performed better when self-attention was replaced with a &lt;b&gt;simpler multilayer perceptron&lt;/b&gt; on tasks with small, fixed contexts like Sudoku. &lt;/p&gt;&lt;p&gt;For larger grids, such as ARC puzzles, self-attention remained valuable. These findings underline that model architecture should match data structure and scale rather than default to maximal capacity.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Training Small, Thinking Big&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;TRM is now officially available as &lt;b&gt;open source under an MIT license&lt;/b&gt; on &lt;a href="https://github.com/SamsungSAILMontreal/TinyRecursiveModels"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The repository includes full training and evaluation scripts, dataset builders for Sudoku, Maze, and ARC-AGI, and reference configurations for reproducing the published results. &lt;/p&gt;&lt;p&gt;It also documents compute requirements ranging from a single NVIDIA L40S GPU for Sudoku training to multi-GPU H100 setups for ARC-AGI experiments.&lt;/p&gt;&lt;p&gt;The open release confirms that TRM is designed specifically for &lt;b&gt;structured, grid-based reasoning tasks&lt;/b&gt; rather than general-purpose language modeling. &lt;/p&gt;&lt;p&gt;Each benchmark — Sudoku-Extreme, Maze-Hard, and ARC-AGI — uses small, well-defined input–output grids, aligning with the model’s recursive supervision process. &lt;/p&gt;&lt;p&gt;Training involves substantial data augmentation (such as color permutations and geometric transformations), underscoring that TRM’s efficiency lies in its parameter size rather than total compute demand.&lt;/p&gt;&lt;p&gt;The model’s simplicity and transparency make it more accessible to researchers outside of large corporate labs. Its codebase builds directly on the earlier Hierarchical Reasoning Model framework but removes HRM’s biological analogies, multiple network hierarchies, and fixed-point dependencies. &lt;/p&gt;&lt;p&gt;In doing so, TRM offers a reproducible baseline for exploring recursive reasoning in small models — a counterpoint to the dominant “scale is all you need” philosophy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community Reaction&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of TRM and its open-source codebase prompted an immediate debate among AI researchers and practitioners on X. While many praised the achievement, others questioned how broadly its methods could generalize.&lt;/p&gt;&lt;p&gt;Supporters hailed TRM as proof that small models can outperform giants, calling it “&lt;a href="https://x.com/VraserX/status/1975668454763209169"&gt;10,000× smaller yet smarter&lt;/a&gt;” and a potential step toward architectures that think rather than merely scale. &lt;/p&gt;&lt;p&gt;Critics countered that TRM’s domain is narrow — focused on &lt;b&gt;bounded, grid-based puzzles&lt;/b&gt; — and that its compute savings come mainly from size, not total runtime. &lt;/p&gt;&lt;p&gt;Researcher &lt;a href="https://x.com/ynmncha/status/1975699469607010731"&gt;Yunmin Cha&lt;/a&gt; noted that TRM’s training depends on heavy augmentation and recursive passes, “more compute, same model.” &lt;/p&gt;&lt;p&gt;Cancer geneticist and data scientist&lt;a href="https://x.com/LovedayChey/status/1975907723150590019"&gt; Chey Loveday&lt;/a&gt; stressed that TRM is a &lt;i&gt;solver&lt;/i&gt;, not a chat model or text generator: it excels at structured reasoning but not open-ended language.&lt;/p&gt;&lt;p&gt;Machine learning researcher &lt;a href="https://x.com/rasbt/status/1975922614389408022"&gt;Sebastian Raschka&lt;/a&gt; positioned TRM as an important simplification of HRM rather than a new form of general intelligence. &lt;/p&gt;&lt;p&gt;He described its process as “a two-step loop that updates an internal reasoning state, then refines the answer.”&lt;/p&gt;&lt;p&gt;Several researchers, including &lt;a href="https://x.com/augustinabele/status/1975927654353445022"&gt;Augustin Nabele&lt;/a&gt;, agreed that the model’s strength lies in its clear reasoning structure but noted that future work would need to show transfer to less-constrained problem types.&lt;/p&gt;&lt;p&gt;The consensus emerging online is that TRM may be narrow, but its message is broad: careful recursion, not constant expansion, could drive the next wave of reasoning research.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Looking Ahead&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While TRM currently applies to supervised reasoning tasks, its recursive framework opens several future directions. Jolicoeur-Martineau has suggested exploring &lt;b&gt;generative or multi-answer variants&lt;/b&gt;, where the model could produce multiple possible solutions rather than a single deterministic one. &lt;/p&gt;&lt;p&gt;Another open question involves scaling laws for recursion — determining how far the “less is more” principle can extend as model complexity or data size grows.&lt;/p&gt;&lt;p&gt;Ultimately, the study offers both a practical tool and a conceptual reminder: progress in AI need not depend on ever-larger models. Sometimes, teaching a small network to think carefully — and recursively — can be more powerful than making a large one think once.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;The trend of AI researchers developing new, &lt;a href="https://www.linkedin.com/pulse/next-big-thing-ai-think-small-models-venturebeat-yyrte/?trackingId=x3X3vTZhTnmwCTUtOWGAug%3D%3D"&gt;small&lt;/a&gt; open source generative models that outperform far larger, proprietary peers continued this week with yet another staggering advancement.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Alexia Jolicoeur-Martineau&lt;/b&gt;, Senior AI Researcher at &lt;b&gt;Samsung&amp;#x27;s Advanced​ Institute of Technology (SAIT)&lt;/b&gt; in Montreal, Canada,&lt;b&gt;​ &lt;/b&gt;has &lt;a href="https://x.com/jm_alexia/status/1975560628657164426"&gt;&lt;b&gt;introduced the Tiny Recursion Model (TRM)&lt;/b&gt;&lt;/a&gt; — a neural network so small it contains just 7 million parameters (internal model settings), yet it competes with or surpasses cutting-edge language models 10,000 times larger in terms of their parameter count, including&lt;b&gt; OpenAI&amp;#x27;s o3-mini and Google&amp;#x27;s Gemini 2.5 Pro,&lt;/b&gt; on some of the toughest reasoning benchmarks in AI research. &lt;/p&gt;&lt;p&gt;The goal is to show that very highly performant new AI models can be created affordably without massive investments in the graphics processing units (GPUs) and power needed to train the larger, multi-trillion parameter flagship models powering many LLM chatbots today. The results were described in a research paper published on open access website arxiv.org, entitled &amp;quot;&lt;a href="https://arxiv.org/abs/2510.04871v1"&gt;&lt;i&gt;Less is More: Recursive Reasoning with Tiny Networks&lt;/i&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;p&gt;&amp;quot;The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to solve hard tasks is a trap,&amp;quot; wrote Jolicoeur-Martineau on the &lt;a href="https://x.com/jm_alexia/status/1975561300890812793"&gt;social network X&lt;/a&gt;. &amp;quot;Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction.&amp;quot;&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;Jolicoeur-Martineau also added: &amp;quot;With recursive reasoning, it turns out that &amp;#x27;less is more&amp;#x27;. &lt;b&gt;A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.&amp;quot;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;TRM&amp;#x27;s code is available now on &lt;a href="https://github.com/SamsungSAILMontreal/TinyRecursiveModels"&gt;Github&lt;/a&gt; under an enterprise-friendly, commercially viable MIT License — meaning anyone from researchers to companies can take, modify it, and deploy it for their own purposes, even commercial applications.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;One Big Caveat&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;However, readers should be aware that TRM was designed specifically to perform well on structured, visual, grid-based problems like Sudoku, mazes, and puzzles on the &lt;a href="https://arcprize.org/arc-agi"&gt;ARC (Abstract and Reasoning Corpus)-AGI benchmark&lt;/a&gt;, the latter which offers tasks that should be easy for humans but difficult for AI models, such sorting colors on a grid based on a prior, but not identical, solution. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;From Hierarchy to Simplicity&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The TRM architecture represents a radical simplification. &lt;/p&gt;&lt;p&gt;It builds upon a technique called &lt;b&gt;Hierarchical Reasoning Model (HRM)&lt;/b&gt; introduced earlier this year, which showed that small networks could tackle logical puzzles like Sudoku and mazes. &lt;/p&gt;&lt;p&gt;HRM relied on two cooperating networks—one operating at high frequency, the other at low—supported by biologically inspired arguments and mathematical justifications involving fixed-point theorems. Jolicoeur-Martineau found this unnecessarily complicated.&lt;/p&gt;&lt;p&gt;TRM strips these elements away. Instead of two networks, it uses a &lt;b&gt;single two-layer model&lt;/b&gt; that recursively refines its own predictions. &lt;/p&gt;&lt;p&gt;The model begins with an embedded question and an initial answer, represented by variables &lt;b&gt;x&lt;/b&gt;, &lt;b&gt;y&lt;/b&gt;, and &lt;b&gt;z&lt;/b&gt;. Through a series of reasoning steps, it updates its internal latent representation &lt;b&gt;z&lt;/b&gt; and refines the answer &lt;b&gt;y&lt;/b&gt; until it converges on a stable output. Each iteration corrects potential errors from the previous step, yielding a self-improving reasoning process without extra hierarchy or mathematical overhead.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;How Recursion Replaces Scale&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The core idea behind TRM is that &lt;i&gt;recursion can substitute for depth and size.&lt;/i&gt;&lt;/p&gt;&lt;p&gt;By iteratively reasoning over its own output, the network effectively simulates a much deeper architecture without the associated memory or computational cost. This recursive cycle, run over as many as sixteen supervision steps, allows the model to make progressively better predictions — similar in spirit to how large language models use multi-step “chain-of-thought” reasoning, but achieved here with a compact, feed-forward design.&lt;/p&gt;&lt;p&gt;The simplicity pays off in both efficiency and generalization. The model uses fewer layers, no fixed-point approximations, and no dual-network hierarchy. A lightweight &lt;b&gt;halting mechanism&lt;/b&gt; decides when to stop refining, preventing wasted computation while maintaining accuracy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Performance That Punches Above Its Weight&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Despite its small footprint, TRM delivers benchmark results that rival or exceed models millions of times larger. In testing, the model achieved:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;87.4% accuracy&lt;/b&gt; on &lt;b&gt;Sudoku-Extreme&lt;/b&gt; (up from 55% for HRM)&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;85% accuracy&lt;/b&gt; on &lt;b&gt;Maze-Hard&lt;/b&gt; puzzles&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;45% accuracy&lt;/b&gt; on &lt;b&gt;ARC-AGI-1&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;8% accuracy&lt;/b&gt; on &lt;b&gt;ARC-AGI-2&lt;/b&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These results surpass or closely match performance from several high-end large language models, including &lt;b&gt;DeepSeek R1&lt;/b&gt;, &lt;b&gt;Gemini 2.5 Pro&lt;/b&gt;, and &lt;b&gt;o3-mini&lt;/b&gt;, despite TRM using less than 0.01% of their parameters.&lt;/p&gt;&lt;p&gt;Such results suggest that recursive reasoning, not scale, may be the key to handling abstract and combinatorial reasoning problems — domains where even top-tier generative models often stumble.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Design Philosophy: Less Is More&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;TRM’s success stems from deliberate minimalism. Jolicoeur-Martineau found that reducing complexity led to better generalization. &lt;/p&gt;&lt;p&gt;When the researcher increased layer count or model size, performance declined due to overfitting on small datasets. &lt;/p&gt;&lt;p&gt;By contrast, the two-layer structure, combined with recursive depth and &lt;b&gt;deep supervision&lt;/b&gt;, achieved optimal results.&lt;/p&gt;&lt;p&gt;The model also performed better when self-attention was replaced with a &lt;b&gt;simpler multilayer perceptron&lt;/b&gt; on tasks with small, fixed contexts like Sudoku. &lt;/p&gt;&lt;p&gt;For larger grids, such as ARC puzzles, self-attention remained valuable. These findings underline that model architecture should match data structure and scale rather than default to maximal capacity.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Training Small, Thinking Big&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;TRM is now officially available as &lt;b&gt;open source under an MIT license&lt;/b&gt; on &lt;a href="https://github.com/SamsungSAILMontreal/TinyRecursiveModels"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;The repository includes full training and evaluation scripts, dataset builders for Sudoku, Maze, and ARC-AGI, and reference configurations for reproducing the published results. &lt;/p&gt;&lt;p&gt;It also documents compute requirements ranging from a single NVIDIA L40S GPU for Sudoku training to multi-GPU H100 setups for ARC-AGI experiments.&lt;/p&gt;&lt;p&gt;The open release confirms that TRM is designed specifically for &lt;b&gt;structured, grid-based reasoning tasks&lt;/b&gt; rather than general-purpose language modeling. &lt;/p&gt;&lt;p&gt;Each benchmark — Sudoku-Extreme, Maze-Hard, and ARC-AGI — uses small, well-defined input–output grids, aligning with the model’s recursive supervision process. &lt;/p&gt;&lt;p&gt;Training involves substantial data augmentation (such as color permutations and geometric transformations), underscoring that TRM’s efficiency lies in its parameter size rather than total compute demand.&lt;/p&gt;&lt;p&gt;The model’s simplicity and transparency make it more accessible to researchers outside of large corporate labs. Its codebase builds directly on the earlier Hierarchical Reasoning Model framework but removes HRM’s biological analogies, multiple network hierarchies, and fixed-point dependencies. &lt;/p&gt;&lt;p&gt;In doing so, TRM offers a reproducible baseline for exploring recursive reasoning in small models — a counterpoint to the dominant “scale is all you need” philosophy.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Community Reaction&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;The release of TRM and its open-source codebase prompted an immediate debate among AI researchers and practitioners on X. While many praised the achievement, others questioned how broadly its methods could generalize.&lt;/p&gt;&lt;p&gt;Supporters hailed TRM as proof that small models can outperform giants, calling it “&lt;a href="https://x.com/VraserX/status/1975668454763209169"&gt;10,000× smaller yet smarter&lt;/a&gt;” and a potential step toward architectures that think rather than merely scale. &lt;/p&gt;&lt;p&gt;Critics countered that TRM’s domain is narrow — focused on &lt;b&gt;bounded, grid-based puzzles&lt;/b&gt; — and that its compute savings come mainly from size, not total runtime. &lt;/p&gt;&lt;p&gt;Researcher &lt;a href="https://x.com/ynmncha/status/1975699469607010731"&gt;Yunmin Cha&lt;/a&gt; noted that TRM’s training depends on heavy augmentation and recursive passes, “more compute, same model.” &lt;/p&gt;&lt;p&gt;Cancer geneticist and data scientist&lt;a href="https://x.com/LovedayChey/status/1975907723150590019"&gt; Chey Loveday&lt;/a&gt; stressed that TRM is a &lt;i&gt;solver&lt;/i&gt;, not a chat model or text generator: it excels at structured reasoning but not open-ended language.&lt;/p&gt;&lt;p&gt;Machine learning researcher &lt;a href="https://x.com/rasbt/status/1975922614389408022"&gt;Sebastian Raschka&lt;/a&gt; positioned TRM as an important simplification of HRM rather than a new form of general intelligence. &lt;/p&gt;&lt;p&gt;He described its process as “a two-step loop that updates an internal reasoning state, then refines the answer.”&lt;/p&gt;&lt;p&gt;Several researchers, including &lt;a href="https://x.com/augustinabele/status/1975927654353445022"&gt;Augustin Nabele&lt;/a&gt;, agreed that the model’s strength lies in its clear reasoning structure but noted that future work would need to show transfer to less-constrained problem types.&lt;/p&gt;&lt;p&gt;The consensus emerging online is that TRM may be narrow, but its message is broad: careful recursion, not constant expansion, could drive the next wave of reasoning research.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;Looking Ahead&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;While TRM currently applies to supervised reasoning tasks, its recursive framework opens several future directions. Jolicoeur-Martineau has suggested exploring &lt;b&gt;generative or multi-answer variants&lt;/b&gt;, where the model could produce multiple possible solutions rather than a single deterministic one. &lt;/p&gt;&lt;p&gt;Another open question involves scaling laws for recursion — determining how far the “less is more” principle can extend as model complexity or data size grows.&lt;/p&gt;&lt;p&gt;Ultimately, the study offers both a practical tool and a conceptual reminder: progress in AI need not depend on ever-larger models. Sometimes, teaching a small network to think carefully — and recursively — can be more powerful than making a large one think once.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/samsung-ai-researchers-new-open-reasoning-model-trm-outperforms-models-10</guid><pubDate>Wed, 08 Oct 2025 18:32:00 +0000</pubDate></item><item><title>[NEW] Here’s what Jony Ive and Sam Altman revealed about their secretive AI hardware project at OpenAI’s Dev Day (AI | VentureBeat)</title><link>https://venturebeat.com/ai/heres-what-jony-ive-and-sam-altman-revealed-about-their-secretive-ai</link><description>[unable to retrieve full-text content]&lt;p&gt;In a packed theater at Fort Mason, after a whirlwind keynote of product announcements, &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; CEO Sam Altman sat down with Sir Jony Ive, the legendary designer behind Apple&amp;#x27;s most iconic products. The conversation, held exclusively for the 1,500 developers in attendance and not part of the public livestream, offered the clearest glimpse yet into the philosophy and ambition behind their secretive collaboration to build a new &amp;quot;family&amp;quot; of AI-powered devices.&lt;/p&gt;&lt;p&gt;The partnership, solidified by OpenAI&amp;#x27;s &lt;a href="https://www.nytimes.com/2025/05/21/technology/openai-jony-ive-deal.html"&gt;&lt;u&gt;staggering $6.5 billion acquisition&lt;/u&gt;&lt;/a&gt; of Ive&amp;#x27;s hardware startup Io in May, has been the subject of intense speculation.While concrete product details remained under wraps, the discussion pivoted away from specifications and toward a profound, almost therapeutic mission: to fix our broken relationship with technology.&lt;/p&gt;&lt;p&gt;For nearly 45 minutes, Ive, in his signature thoughtful cadence, articulated a vision that feels like both a continuation of and a repentance for his life&amp;#x27;s work. The man who designed the iPhone, a device that arguably defined the modern era of personal computing, is now on a quest to cure the very anxieties it helped create.&lt;/p&gt;&lt;h2&gt;Jony Ive&amp;#x27;s post-Apple mission, clarified by ChatGPT&lt;/h2&gt;&lt;p&gt;The collaboration, Ive explained, was years in the making, but it was the launch of ChatGPT that provided a sudden, clarifying purpose for his post-Apple design collective, &lt;a href="https://www.lovefrom.com/"&gt;&lt;u&gt;LoveFrom&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;quot;With the launch of ChatGPT, it felt like our purpose for the last six years became clear,&amp;quot; Ive said. &amp;quot;We were starting to develop some ideas for an interface based on the capability of the technology these guys were developing... I&amp;#x27;ve never in my career come across anything vaguely like the affordance, like the capability that we&amp;#x27;re now starting to sense.&amp;quot;&lt;/p&gt;&lt;p&gt;This capability, he argued, demands a fundamental rethinking of the devices we use, which he described as &amp;quot;legacy products&amp;quot; from a bygone era. The core motivation, he stressed, is not about corporate agendas but about a sense of duty to humanity.&lt;/p&gt;&lt;p&gt;&amp;quot;The reason we&amp;#x27;re doing this is we love our species and we want to be useful,&amp;quot; Ive said. &amp;quot;We think that humanity deserves much better than humanity generally is given.&amp;quot;&lt;/p&gt;&lt;h2&gt;An &amp;#x27;obscene understatement&amp;#x27;: Jony Ive&amp;#x27;s quest to cure our tech anxiety&lt;/h2&gt;&lt;p&gt;The most striking theme of the conversation was Ive&amp;#x27;s candid critique of the current state of technology — the very ecosystem he was instrumental in building. He described our current dynamic with our devices as deeply flawed, a problem he now sees AI as the solution to, not an extension of.&lt;/p&gt;&lt;p&gt;&amp;quot;I don&amp;#x27;t think we have an easy relationship with our technology at the moment,&amp;quot; Ive began, before adding, &amp;quot;When I said we have an uncomfortable relationship with our technology, I mean, that&amp;#x27;s the most obscene understatement.&amp;quot;&lt;/p&gt;&lt;p&gt;Instead of chasing productivity, the primary goal for this new family of devices is emotional well-being. It&amp;#x27;s a radical departure from the efficiency-obsessed ethos that dominates Silicon Valley.&lt;/p&gt;&lt;p&gt;When asked about his ambitions for the new devices, Ive prioritized emotional well-being over simple productivity. &amp;quot;I know I should care about productivity, and I do,&amp;quot; he said, but his ultimate goal is that the tools &amp;quot;make us happy and fulfilled, and more peaceful and less anxious, and less disconnected.&amp;quot;&lt;/p&gt;&lt;p&gt;He framed it as a chance to reject the current, fraught relationship people have with their technology. &amp;quot;We have a chance to... absolutely change the situation that we find ourselves in,&amp;quot; he stated. &amp;quot;We don&amp;#x27;t accept this has to be the norm.&amp;quot;&lt;/p&gt;&lt;h2&gt;Buried in brilliance: why &amp;#x27;15 to 20 compelling ideas&amp;#x27; have become Ive&amp;#x27;s biggest challenge&lt;/h2&gt;&lt;p&gt;While the vision is clear, the path is fraught with challenges. Reports have surfaced about &lt;a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3"&gt;&lt;u&gt;technical hurdles and philosophical debates&lt;/u&gt;&lt;/a&gt; delaying the project. Ive himself gave voice to this struggle, admitting the sheer pace of AI&amp;#x27;s progress has been overwhelming. The rapid advancement has generated a torrent of possibilities, making the crucial act of focusing incredibly difficult.&lt;/p&gt;&lt;p&gt;&amp;quot;The momentum is so extraordinary... it has led us to generate 15 to 20 really compelling product ideas. And the challenge is trying to focus,&amp;quot; Ive confessed.&amp;quot;I used to be good at that, and I&amp;#x27;ve lost some confidence, because the choices are, it&amp;#x27;ll be easy if you really knew there were three good ones... it&amp;#x27;s just not like that.&amp;quot;&lt;/p&gt;&lt;p&gt;This admission provides context to reports that the team is grappling with unresolved issues around the device&amp;#x27;s &amp;quot;personality&amp;quot; and computing infrastructure. The goal, according to one source, is to create an AI companion that is &amp;quot;accessible but not intrusive,&amp;quot; avoiding the pitfalls of a &amp;quot;&lt;a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3"&gt;&lt;u&gt;weird AI girlfriend&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;h2&gt;Beyond the screen: Ive&amp;#x27;s design philosophy for an &amp;#x27;inevitable&amp;#x27; AI device&lt;/h2&gt;&lt;p&gt;While no devices were shown, the conversation and prior reports offer clues. The project involves a &amp;quot;&lt;a href="https://fortune.com/2025/10/07/legendary-apple-designer-jony-ive-wants-to-fix-our-relationships-with-technology-openai/"&gt;&lt;u&gt;family of devices&lt;/u&gt;&lt;/a&gt;,&amp;quot; not a single gadget.It will likely be a departure from the screen-centric world we inhabit. Reports suggest a &amp;quot;&lt;a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3"&gt;&lt;u&gt;palm-sized device without a screen&lt;/u&gt;&lt;/a&gt;&amp;quot; that relies on cameras and microphones to perceive its environment.&lt;/p&gt;&lt;p&gt;Ive argued that it would be &amp;quot;absurd&amp;quot; to assume that today&amp;#x27;s breathtaking AI technology should be delivered through &amp;quot;products that are decades old.&amp;quot; The goal is to create something that feels entirely new, yet completely natural.&lt;/p&gt;&lt;p&gt;&amp;quot;It should seem inevitable. It should seem obvious, as if there wasn&amp;#x27;t possibly another rational solution to the problem,&amp;quot; Ive said, echoing a design philosophy often attributed to his time with Steve Jobs.&lt;/p&gt;&lt;p&gt;He also spoke of bringing a sense of joy and whimsy back to technology, pushing back against a culture he feels has become overly serious.&lt;/p&gt;&lt;p&gt;&amp;quot;In terms of the interfaces we design, if we can&amp;#x27;t smile honestly, if it&amp;#x27;s just another deeply serious sort of exclusive thing, I think that would do us all a huge disservice,&amp;quot; he remarked.&lt;/p&gt;&lt;p&gt;The chat concluded without a product reveal, leaving the audience with a philosophical blueprint rather than a technical one. The central narrative is clear: &lt;a href="https://openai.com/sam-and-jony/"&gt;&lt;u&gt;Jony Ive&lt;/u&gt;&lt;/a&gt;, the designer who put a screen in every pocket, is now betting on a screenless future, powered by OpenAI&amp;#x27;s formidable intelligence, to make us all a little less anxious and a little more human.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;In a packed theater at Fort Mason, after a whirlwind keynote of product announcements, &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; CEO Sam Altman sat down with Sir Jony Ive, the legendary designer behind Apple&amp;#x27;s most iconic products. The conversation, held exclusively for the 1,500 developers in attendance and not part of the public livestream, offered the clearest glimpse yet into the philosophy and ambition behind their secretive collaboration to build a new &amp;quot;family&amp;quot; of AI-powered devices.&lt;/p&gt;&lt;p&gt;The partnership, solidified by OpenAI&amp;#x27;s &lt;a href="https://www.nytimes.com/2025/05/21/technology/openai-jony-ive-deal.html"&gt;&lt;u&gt;staggering $6.5 billion acquisition&lt;/u&gt;&lt;/a&gt; of Ive&amp;#x27;s hardware startup Io in May, has been the subject of intense speculation.While concrete product details remained under wraps, the discussion pivoted away from specifications and toward a profound, almost therapeutic mission: to fix our broken relationship with technology.&lt;/p&gt;&lt;p&gt;For nearly 45 minutes, Ive, in his signature thoughtful cadence, articulated a vision that feels like both a continuation of and a repentance for his life&amp;#x27;s work. The man who designed the iPhone, a device that arguably defined the modern era of personal computing, is now on a quest to cure the very anxieties it helped create.&lt;/p&gt;&lt;h2&gt;Jony Ive&amp;#x27;s post-Apple mission, clarified by ChatGPT&lt;/h2&gt;&lt;p&gt;The collaboration, Ive explained, was years in the making, but it was the launch of ChatGPT that provided a sudden, clarifying purpose for his post-Apple design collective, &lt;a href="https://www.lovefrom.com/"&gt;&lt;u&gt;LoveFrom&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;quot;With the launch of ChatGPT, it felt like our purpose for the last six years became clear,&amp;quot; Ive said. &amp;quot;We were starting to develop some ideas for an interface based on the capability of the technology these guys were developing... I&amp;#x27;ve never in my career come across anything vaguely like the affordance, like the capability that we&amp;#x27;re now starting to sense.&amp;quot;&lt;/p&gt;&lt;p&gt;This capability, he argued, demands a fundamental rethinking of the devices we use, which he described as &amp;quot;legacy products&amp;quot; from a bygone era. The core motivation, he stressed, is not about corporate agendas but about a sense of duty to humanity.&lt;/p&gt;&lt;p&gt;&amp;quot;The reason we&amp;#x27;re doing this is we love our species and we want to be useful,&amp;quot; Ive said. &amp;quot;We think that humanity deserves much better than humanity generally is given.&amp;quot;&lt;/p&gt;&lt;h2&gt;An &amp;#x27;obscene understatement&amp;#x27;: Jony Ive&amp;#x27;s quest to cure our tech anxiety&lt;/h2&gt;&lt;p&gt;The most striking theme of the conversation was Ive&amp;#x27;s candid critique of the current state of technology — the very ecosystem he was instrumental in building. He described our current dynamic with our devices as deeply flawed, a problem he now sees AI as the solution to, not an extension of.&lt;/p&gt;&lt;p&gt;&amp;quot;I don&amp;#x27;t think we have an easy relationship with our technology at the moment,&amp;quot; Ive began, before adding, &amp;quot;When I said we have an uncomfortable relationship with our technology, I mean, that&amp;#x27;s the most obscene understatement.&amp;quot;&lt;/p&gt;&lt;p&gt;Instead of chasing productivity, the primary goal for this new family of devices is emotional well-being. It&amp;#x27;s a radical departure from the efficiency-obsessed ethos that dominates Silicon Valley.&lt;/p&gt;&lt;p&gt;When asked about his ambitions for the new devices, Ive prioritized emotional well-being over simple productivity. &amp;quot;I know I should care about productivity, and I do,&amp;quot; he said, but his ultimate goal is that the tools &amp;quot;make us happy and fulfilled, and more peaceful and less anxious, and less disconnected.&amp;quot;&lt;/p&gt;&lt;p&gt;He framed it as a chance to reject the current, fraught relationship people have with their technology. &amp;quot;We have a chance to... absolutely change the situation that we find ourselves in,&amp;quot; he stated. &amp;quot;We don&amp;#x27;t accept this has to be the norm.&amp;quot;&lt;/p&gt;&lt;h2&gt;Buried in brilliance: why &amp;#x27;15 to 20 compelling ideas&amp;#x27; have become Ive&amp;#x27;s biggest challenge&lt;/h2&gt;&lt;p&gt;While the vision is clear, the path is fraught with challenges. Reports have surfaced about &lt;a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3"&gt;&lt;u&gt;technical hurdles and philosophical debates&lt;/u&gt;&lt;/a&gt; delaying the project. Ive himself gave voice to this struggle, admitting the sheer pace of AI&amp;#x27;s progress has been overwhelming. The rapid advancement has generated a torrent of possibilities, making the crucial act of focusing incredibly difficult.&lt;/p&gt;&lt;p&gt;&amp;quot;The momentum is so extraordinary... it has led us to generate 15 to 20 really compelling product ideas. And the challenge is trying to focus,&amp;quot; Ive confessed.&amp;quot;I used to be good at that, and I&amp;#x27;ve lost some confidence, because the choices are, it&amp;#x27;ll be easy if you really knew there were three good ones... it&amp;#x27;s just not like that.&amp;quot;&lt;/p&gt;&lt;p&gt;This admission provides context to reports that the team is grappling with unresolved issues around the device&amp;#x27;s &amp;quot;personality&amp;quot; and computing infrastructure. The goal, according to one source, is to create an AI companion that is &amp;quot;accessible but not intrusive,&amp;quot; avoiding the pitfalls of a &amp;quot;&lt;a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3"&gt;&lt;u&gt;weird AI girlfriend&lt;/u&gt;&lt;/a&gt;.&amp;quot;&lt;/p&gt;&lt;h2&gt;Beyond the screen: Ive&amp;#x27;s design philosophy for an &amp;#x27;inevitable&amp;#x27; AI device&lt;/h2&gt;&lt;p&gt;While no devices were shown, the conversation and prior reports offer clues. The project involves a &amp;quot;&lt;a href="https://fortune.com/2025/10/07/legendary-apple-designer-jony-ive-wants-to-fix-our-relationships-with-technology-openai/"&gt;&lt;u&gt;family of devices&lt;/u&gt;&lt;/a&gt;,&amp;quot; not a single gadget.It will likely be a departure from the screen-centric world we inhabit. Reports suggest a &amp;quot;&lt;a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3"&gt;&lt;u&gt;palm-sized device without a screen&lt;/u&gt;&lt;/a&gt;&amp;quot; that relies on cameras and microphones to perceive its environment.&lt;/p&gt;&lt;p&gt;Ive argued that it would be &amp;quot;absurd&amp;quot; to assume that today&amp;#x27;s breathtaking AI technology should be delivered through &amp;quot;products that are decades old.&amp;quot; The goal is to create something that feels entirely new, yet completely natural.&lt;/p&gt;&lt;p&gt;&amp;quot;It should seem inevitable. It should seem obvious, as if there wasn&amp;#x27;t possibly another rational solution to the problem,&amp;quot; Ive said, echoing a design philosophy often attributed to his time with Steve Jobs.&lt;/p&gt;&lt;p&gt;He also spoke of bringing a sense of joy and whimsy back to technology, pushing back against a culture he feels has become overly serious.&lt;/p&gt;&lt;p&gt;&amp;quot;In terms of the interfaces we design, if we can&amp;#x27;t smile honestly, if it&amp;#x27;s just another deeply serious sort of exclusive thing, I think that would do us all a huge disservice,&amp;quot; he remarked.&lt;/p&gt;&lt;p&gt;The chat concluded without a product reveal, leaving the audience with a philosophical blueprint rather than a technical one. The central narrative is clear: &lt;a href="https://openai.com/sam-and-jony/"&gt;&lt;u&gt;Jony Ive&lt;/u&gt;&lt;/a&gt;, the designer who put a screen in every pocket, is now betting on a screenless future, powered by OpenAI&amp;#x27;s formidable intelligence, to make us all a little less anxious and a little more human.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/heres-what-jony-ive-and-sam-altman-revealed-about-their-secretive-ai</guid><pubDate>Wed, 08 Oct 2025 18:40:00 +0000</pubDate></item><item><title>[NEW] MIT Schwarzman College of Computing and MBZUAI launch international collaboration to shape the future of AI (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/mit-schwarzman-college-computing-mbzuai-launch-collaboration-shape-future-ai-1008</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/mit-mbzuai.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The MIT Schwarzman College of Computing and the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) recently celebrated the launch of the MIT–MBZUAI Collaborative Research Program, a new effort to strengthen the building blocks of artificial intelligence and accelerate its use in pressing scientific and societal challenges.&lt;/p&gt;&lt;p&gt;Under the five-year agreement, faculty, students, and research staff from both institutions will collaborate on fundamental research projects to advance the technological foundations of AI and its applications in three core areas: scientific discovery, human thriving, and the health of the planet.&lt;/p&gt;&lt;p&gt;“Artificial intelligence is transforming nearly every aspect of human endeavor. MIT’s leadership in AI is greatly enriched through collaborations with leading academic institutions in the U.S. and around the world,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science. “Our collaboration with MBZUAI reflects a shared commitment to advancing AI in ways that are responsible, inclusive, and globally impactful. Together, we can explore new horizons in AI and bring broad benefits to society.”&lt;/p&gt;&lt;p&gt;“This agreement will unite the efforts of researchers at two world-class institutions to advance frontier AI research across scientific discovery, human thriving, and the health of the planet. By combining MBZUAI’s focus on foundational models and real-world deployment with MIT’s depth in computing and interdisciplinary innovation, we are creating a transcontinental bridge for discovery. Together, we will not only expand the boundaries of AI science, but also ensure that these breakthroughs are pursued responsibly and applied where they matter most — improving human health, enabling intelligent robotics, and driving sustainable AI at scale,” says Eric Xing, president and university professor at MBZUAI.&lt;/p&gt;&lt;p&gt;Each institution has appointed an academic director to oversee the program on its campus. At MIT, Philip Isola, the Class of 1948 Career Development Professor in the Department of Electrical Engineering and Computer Science, will serve as program lead. At MBZUAI, Le Song, professor of machine learning, will take on the role.&lt;/p&gt;&lt;p&gt;Supported by MBZUAI — the first university dedicated entirely to advancing science through AI, and based in Abu Dhabi, U.A.E. — the collaboration will fund a number of joint research projects per year. The findings will be openly publishable, and each project will be led by a principal investigator from MIT and one from MBZUAI, with project selections made by a steering committee composed of representatives from both institutions.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202510/mit-mbzuai.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;The MIT Schwarzman College of Computing and the Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) recently celebrated the launch of the MIT–MBZUAI Collaborative Research Program, a new effort to strengthen the building blocks of artificial intelligence and accelerate its use in pressing scientific and societal challenges.&lt;/p&gt;&lt;p&gt;Under the five-year agreement, faculty, students, and research staff from both institutions will collaborate on fundamental research projects to advance the technological foundations of AI and its applications in three core areas: scientific discovery, human thriving, and the health of the planet.&lt;/p&gt;&lt;p&gt;“Artificial intelligence is transforming nearly every aspect of human endeavor. MIT’s leadership in AI is greatly enriched through collaborations with leading academic institutions in the U.S. and around the world,” says Dan Huttenlocher, dean of the MIT Schwarzman College of Computing and the Henry Ellis Warren Professor of Electrical Engineering and Computer Science. “Our collaboration with MBZUAI reflects a shared commitment to advancing AI in ways that are responsible, inclusive, and globally impactful. Together, we can explore new horizons in AI and bring broad benefits to society.”&lt;/p&gt;&lt;p&gt;“This agreement will unite the efforts of researchers at two world-class institutions to advance frontier AI research across scientific discovery, human thriving, and the health of the planet. By combining MBZUAI’s focus on foundational models and real-world deployment with MIT’s depth in computing and interdisciplinary innovation, we are creating a transcontinental bridge for discovery. Together, we will not only expand the boundaries of AI science, but also ensure that these breakthroughs are pursued responsibly and applied where they matter most — improving human health, enabling intelligent robotics, and driving sustainable AI at scale,” says Eric Xing, president and university professor at MBZUAI.&lt;/p&gt;&lt;p&gt;Each institution has appointed an academic director to oversee the program on its campus. At MIT, Philip Isola, the Class of 1948 Career Development Professor in the Department of Electrical Engineering and Computer Science, will serve as program lead. At MBZUAI, Le Song, professor of machine learning, will take on the role.&lt;/p&gt;&lt;p&gt;Supported by MBZUAI — the first university dedicated entirely to advancing science through AI, and based in Abu Dhabi, U.A.E. — the collaboration will fund a number of joint research projects per year. The findings will be openly publishable, and each project will be led by a principal investigator from MIT and one from MBZUAI, with project selections made by a steering committee composed of representatives from both institutions.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/mit-schwarzman-college-computing-mbzuai-launch-collaboration-shape-future-ai-1008</guid><pubDate>Wed, 08 Oct 2025 19:10:00 +0000</pubDate></item><item><title>[NEW] OpenAI’s Nick Turley on transforming ChatGPT into an operating system (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/openais-nick-turley-on-transforming-chatgpt-into-an-operating-system/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Nick Turley joined OpenAI in 2022 as the head of ChatGPT, he was tasked with commercializing the company’s research. He has made great strides toward that goal, growing the product to 800 million weekly active users. Now Turley wants to take an even bigger swing: transforming ChatGPT into a new type of operating system full of third-party apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I sat down with Turley this week on the outskirts of San Francisco’s Fort Mason, a former U.S. military post where OpenAI held its third annual developer conference, to discuss how he’s thinking about ChatGPT’s future. You can find a transcript of our conversation at the bottom of this article.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To turn ChatGPT into an operating system, Turley tells me he’s drawing inspiration from web browsers. Over the last decade, browsers have emerged as a new kind of operating system — not in the literal sense like macOS or Windows — because they’ve become the main place people work on computers thanks to a variety of web applications. Turley sees ChatGPT evolving in a similar way: a platform that could change how people interact with software.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is reportedly developing a browser too. Turley doesn’t confirm or deny this, but he does say browsers are “really interesting.” The company is also working with Jony Ive and a team of longtime Apple designers on a family of hardware devices. Given these efforts, it’s easy to see how a ChatGPT operating system full of apps could become a central component of OpenAI’s consumer ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has been chasing this idea for a while. In 2023, the company launched an array of “AI app store” efforts such as ChatGPT plugins and the GPT Store. Those products didn’t exactly take off, but OpenAI seems to have a better approach this time around.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of apps aligns with OpenAI’s desire to turn ChatGPT into an e-commerce destination. Apps from Expedia, DoorDash, and Uber could lead to more transactions in ChatGPT, something OpenAI can now facilitate and capture some of the revenue from. Having a product featured in ChatGPT could be a major source of business for both third parties and OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This might also be OpenAI’s most compelling pitch to developers yet. Third parties can now reach ChatGPT’s 800 million users during their everyday conversations. Apps are part of ChatGPT’s core experience, rather than in a separate store of widgets. Developers can also build more interactive experiences in ChatGPT, beyond just chatbots connected to their company’s data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;However, the business of running an operating system also comes with lots of messy problems, such as how to promote certain apps over others. Turley says OpenAI isn’t ruling out letting some companies pay for their apps to have priority placement in ChatGPT, but the company is figuring out how to do this without hurting the user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Third-party developers likely also want access to ChatGPT user data. In a set of guidelines, OpenAI says app developers must “gather only the minimum data required to perform the tool’s function,” but it’s unclear what that means in practice. Turley says OpenAI may build out new features — such as a partitioned memory in ChatGPT — that could let users give fine-grained data access to developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One standout comment from our conversation was how Turley views ChatGPT as the “delivery vehicle” for OpenAI’s nonprofit mission: to develop and distribute artificial general intelligence (AGI) — highly autonomous AI systems — in a way that benefits humanity. Some OpenAI researchers worry that the company’s consumer business could overpower its nonprofit mission. But according to Turley, ChatGPT is how OpenAI will distribute AGI to the masses. How’s that for a spin?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Here’s my conversation with Nick Turley, head of ChatGPT, which has been edited for clarity and brevity. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3055345" height="678" src="https://techcrunch.com/wp-content/uploads/2025/10/Nick-Turley.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s Head of ChatGPT, Nick Turley.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;How are you thinking about ChatGPT as a platform for other companies?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think we’re gonna look back at ChatGPT in a couple years and feel like the current product is in the command line era. It’s really powerful, but it’s lacking something very important, which is affordances.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the classic operating system world, that’s obvious. We prefer going to Mac or Windows and opening applications versus remembering all the commands. It’s kind of bonkers to me that we’ve scaled the product to 800 million weekly active users with the form factor we have. This is a weird and hard [way] to grow category, and yet it’s growing like crazy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The evolution we’re trying to make over the next few years is one where ChatGPT itself is more like an operating system where you can come and use applications. If you want to write, there’s an app for that. If you want to code, there’s an app for that. If you want to interact with goods and services, there are applications for you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But we can’t build everything ourselves. We’re not going to have a music streaming service, or replicate Coursera’s catalog of educational materials. We’re not going to get into the business that Expedia and Booking.com are in. And for that reason, it makes sense to partner.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s also a whole generation of apps that people are going to build that wouldn’t have been possible previously. The Ubers of the world only exist because of the mobile platform, and I’m really excited about what those might be for ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We also want to give developers, who have been with us since the beginning, access to ChatGPT’s 800 million weekly users. If they’re able to enhance ChatGPT and build real businesses on top of that, it creates more winners in the ecosystem.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Where do you draw inspiration from when building ChatGPT?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can’t go to one spot. I often tell job candidates they need to have first principles thinking, and if they’re gonna try to run a playbook they saw at Meta or Google, you’re actually gonna run out of competitors to copy. When it comes to [ChatGPT] or Sora, there’s just zero precedent. So you kind of have to get your analogies from different places.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think browsers are really interesting because, in some ways, they’ve become the operating system in the last 10 years. How many of us actually use desktop apps? You might use Excel or PowerPoint, but most of what we do actually happens in the browser via application-like things.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I also spent some time looking at the early ads for the [Apple] PowerBook. It’s kind of like ChatGPT where it was this appliance that nobody quite knew everything you could do with it. The ads were literally like “It’s a calculator, it’s an alarm clock.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So there isn’t a single thing you can look at, but it behooves us to learn from history. If you just look at the last 10 years, there might not be the perfect analogous thing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;You mentioned browsers and devices there. How are you thinking about expanding ChatGPT into those form factors?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is the kind of place where you dream big. One category we have covered is productivity, which is effectively ChatGPT. But there are so many other product categories to be built, and they’re all going to change with AI. Entertainment is one, which is why I’m excited about Sora. Social media is another one. Obviously, hardware and access points to the internet are interesting too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You should really think about what we’re building as a family of products and applications that are tied together by your account, personalization, and identity layer. I’m really excited that we’re not boxing ourselves in. Even if we were just the ChatGPT company, there would be infinite things to build, but our ambition on what we can do for people just goes way beyond that.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;I’m interested in hearing how you think the consumer business of OpenAI fits into the nonprofit mission. I’ve heard some people say the consumer business funds the mission. How do you see it?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The OpenAI I joined was a research lab that might ship a demo or two. In fact, my job description at the time was framed to me as “helping commercialize OpenAI technology” — very open ended. At that time, the product existed to bring the research to life so that people actually get it. I think that was true and still is true, as you can see with Sora. The best way to start a grounded discourse on the profoundness of a technology is to ship something.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then we moved from that framing to, okay, maybe the product is more than that. Maybe the product is actually the way we fund the mission. It became evident at some point, even before I got to OpenAI, that this is all going to be very expensive. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But after ChatGPT, we started talking about it a bit differently. Our mission is to ensure that AGI benefits all of humanity, and reaches people. If you combine that with the insight that AGI is probably not this single moment in time, but rather a gradual thing, you have to think of product as the delivery vehicle of the mission. It’s the way you actually benefit people in practice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you look at what these 800 million people are doing every week, ChatGPT is helping them achieve their goals. I don’t know if you saw the guy in the keynote who taught himself to code at 89. That’s insane to me. I talk to ChatGPT users who help their autistic kids by modeling social interactions. I talk to people who are entirely self-taught in a language based on what they do with ChatGPT. Like, that is the mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I don’t think it’s fair to talk about the consumer business as a funding vehicle. Rather, it’s the expression. That is one way in which OpenAI has evolved, to me at least, since I’ve joined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Let’s dive deeper into the apps that were announced today. OpenAI has said that third parties can only take the “minimum amount of data” necessary to run an app in ChatGPT. How are you thinking about user privacy?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From day one, we’re going to ask developers to disclose to users what information they’re requesting. We’re also only going to let [apps] go live if they are reasonable in the data that they request. We published our developer guidelines [at launch] so people won’t be surprised when we reject their app because it doesn’t comply with our stance on privacy.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the next month, we want to build ways for users to give fine-grained access to developers. I think Apple has done a phenomenal job with this, where you can share data just this time, or all the time, etc.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To do that well, we might need some concept of a partitioned memory in ChatGPT, which we’re still thinking through. But we’re really excited about the idea because you might want to keep certain conversations, like health, separate from others, such as music. Users may want to share one, but not the other, with an app. So we’re going to have a lot more to share soon, because it’s actually a combined research and engineering challenge to do this well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The thing that’s uncompromisable for us is transparency. We want users, at all points, to understand what data might be going to a third party, but the controls will come over time as we build them out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;DoorDash and Instacart are two companies that will have apps in ChatGPT soon enough. If I want to order some snacks, how will ChatGPT know which one to go to?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the classic question. The best way to start is you show them both. If you’ve used one of them before, we’ll prioritize that one. If you’ve used both, we’ll ask which one you prefer. We could get more sophisticated over time. You could imagine one of these apps being much higher quality than another. Maybe there would be reason to prioritize one over the other. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We have multiple partners in the same product categories. I think the most graceful and respectful way to handle that is to serve both apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Are you thinking about letting companies pay for their apps to have preferential spot placement in ChatGPT?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is one of the things we’re hoping to do some discovery on with developers. There’s this trade-off. You could try to figure it all out in advance, and roll it out with the announcement, but that probably means you didn’t talk to a lot of people. Or you could delay it, which means everyone’s asking questions and doesn’t know exactly what’s going to happen, but it gives us the ability to actually engage.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We chose the latter just because we know that building this ecosystem is going to be a long game. It’s not going to happen on day one, and therefore it’s better to be thoughtful on what sort of distribution mechanisms are and aren’t fair game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the end of the day, we want a great user experience. So if that would lead to apps [surfacing] that are irrelevant to the user, I don’t think we’d like it. If this was a lever that helped us prioritize apps that are really serious because they’re clearly trying to invest in exposure, it could be a good thing. We have no point of view as of today. It’s certainly something that’s come up with different partners.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;When Nick Turley joined OpenAI in 2022 as the head of ChatGPT, he was tasked with commercializing the company’s research. He has made great strides toward that goal, growing the product to 800 million weekly active users. Now Turley wants to take an even bigger swing: transforming ChatGPT into a new type of operating system full of third-party apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I sat down with Turley this week on the outskirts of San Francisco’s Fort Mason, a former U.S. military post where OpenAI held its third annual developer conference, to discuss how he’s thinking about ChatGPT’s future. You can find a transcript of our conversation at the bottom of this article.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To turn ChatGPT into an operating system, Turley tells me he’s drawing inspiration from web browsers. Over the last decade, browsers have emerged as a new kind of operating system — not in the literal sense like macOS or Windows — because they’ve become the main place people work on computers thanks to a variety of web applications. Turley sees ChatGPT evolving in a similar way: a platform that could change how people interact with software.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is reportedly developing a browser too. Turley doesn’t confirm or deny this, but he does say browsers are “really interesting.” The company is also working with Jony Ive and a team of longtime Apple designers on a family of hardware devices. Given these efforts, it’s easy to see how a ChatGPT operating system full of apps could become a central component of OpenAI’s consumer ecosystem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI has been chasing this idea for a while. In 2023, the company launched an array of “AI app store” efforts such as ChatGPT plugins and the GPT Store. Those products didn’t exactly take off, but OpenAI seems to have a better approach this time around.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The launch of apps aligns with OpenAI’s desire to turn ChatGPT into an e-commerce destination. Apps from Expedia, DoorDash, and Uber could lead to more transactions in ChatGPT, something OpenAI can now facilitate and capture some of the revenue from. Having a product featured in ChatGPT could be a major source of business for both third parties and OpenAI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This might also be OpenAI’s most compelling pitch to developers yet. Third parties can now reach ChatGPT’s 800 million users during their everyday conversations. Apps are part of ChatGPT’s core experience, rather than in a separate store of widgets. Developers can also build more interactive experiences in ChatGPT, beyond just chatbots connected to their company’s data.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;However, the business of running an operating system also comes with lots of messy problems, such as how to promote certain apps over others. Turley says OpenAI isn’t ruling out letting some companies pay for their apps to have priority placement in ChatGPT, but the company is figuring out how to do this without hurting the user experience.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Third-party developers likely also want access to ChatGPT user data. In a set of guidelines, OpenAI says app developers must “gather only the minimum data required to perform the tool’s function,” but it’s unclear what that means in practice. Turley says OpenAI may build out new features — such as a partitioned memory in ChatGPT — that could let users give fine-grained data access to developers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One standout comment from our conversation was how Turley views ChatGPT as the “delivery vehicle” for OpenAI’s nonprofit mission: to develop and distribute artificial general intelligence (AGI) — highly autonomous AI systems — in a way that benefits humanity. Some OpenAI researchers worry that the company’s consumer business could overpower its nonprofit mission. But according to Turley, ChatGPT is how OpenAI will distribute AGI to the masses. How’s that for a spin?&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Here’s my conversation with Nick Turley, head of ChatGPT, which has been edited for clarity and brevity. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="alt" class="wp-image-3055345" height="678" src="https://techcrunch.com/wp-content/uploads/2025/10/Nick-Turley.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;OpenAI’s Head of ChatGPT, Nick Turley.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;OpenAI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;How are you thinking about ChatGPT as a platform for other companies?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think we’re gonna look back at ChatGPT in a couple years and feel like the current product is in the command line era. It’s really powerful, but it’s lacking something very important, which is affordances.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the classic operating system world, that’s obvious. We prefer going to Mac or Windows and opening applications versus remembering all the commands. It’s kind of bonkers to me that we’ve scaled the product to 800 million weekly active users with the form factor we have. This is a weird and hard [way] to grow category, and yet it’s growing like crazy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The evolution we’re trying to make over the next few years is one where ChatGPT itself is more like an operating system where you can come and use applications. If you want to write, there’s an app for that. If you want to code, there’s an app for that. If you want to interact with goods and services, there are applications for you.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But we can’t build everything ourselves. We’re not going to have a music streaming service, or replicate Coursera’s catalog of educational materials. We’re not going to get into the business that Expedia and Booking.com are in. And for that reason, it makes sense to partner.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;There’s also a whole generation of apps that people are going to build that wouldn’t have been possible previously. The Ubers of the world only exist because of the mobile platform, and I’m really excited about what those might be for ChatGPT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We also want to give developers, who have been with us since the beginning, access to ChatGPT’s 800 million weekly users. If they’re able to enhance ChatGPT and build real businesses on top of that, it creates more winners in the ecosystem.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Where do you draw inspiration from when building ChatGPT?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You can’t go to one spot. I often tell job candidates they need to have first principles thinking, and if they’re gonna try to run a playbook they saw at Meta or Google, you’re actually gonna run out of competitors to copy. When it comes to [ChatGPT] or Sora, there’s just zero precedent. So you kind of have to get your analogies from different places.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think browsers are really interesting because, in some ways, they’ve become the operating system in the last 10 years. How many of us actually use desktop apps? You might use Excel or PowerPoint, but most of what we do actually happens in the browser via application-like things.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I also spent some time looking at the early ads for the [Apple] PowerBook. It’s kind of like ChatGPT where it was this appliance that nobody quite knew everything you could do with it. The ads were literally like “It’s a calculator, it’s an alarm clock.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So there isn’t a single thing you can look at, but it behooves us to learn from history. If you just look at the last 10 years, there might not be the perfect analogous thing.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;You mentioned browsers and devices there. How are you thinking about expanding ChatGPT into those form factors?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is the kind of place where you dream big. One category we have covered is productivity, which is effectively ChatGPT. But there are so many other product categories to be built, and they’re all going to change with AI. Entertainment is one, which is why I’m excited about Sora. Social media is another one. Obviously, hardware and access points to the internet are interesting too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;You should really think about what we’re building as a family of products and applications that are tied together by your account, personalization, and identity layer. I’m really excited that we’re not boxing ourselves in. Even if we were just the ChatGPT company, there would be infinite things to build, but our ambition on what we can do for people just goes way beyond that.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;I’m interested in hearing how you think the consumer business of OpenAI fits into the nonprofit mission. I’ve heard some people say the consumer business funds the mission. How do you see it?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The OpenAI I joined was a research lab that might ship a demo or two. In fact, my job description at the time was framed to me as “helping commercialize OpenAI technology” — very open ended. At that time, the product existed to bring the research to life so that people actually get it. I think that was true and still is true, as you can see with Sora. The best way to start a grounded discourse on the profoundness of a technology is to ship something.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then we moved from that framing to, okay, maybe the product is more than that. Maybe the product is actually the way we fund the mission. It became evident at some point, even before I got to OpenAI, that this is all going to be very expensive. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But after ChatGPT, we started talking about it a bit differently. Our mission is to ensure that AGI benefits all of humanity, and reaches people. If you combine that with the insight that AGI is probably not this single moment in time, but rather a gradual thing, you have to think of product as the delivery vehicle of the mission. It’s the way you actually benefit people in practice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If you look at what these 800 million people are doing every week, ChatGPT is helping them achieve their goals. I don’t know if you saw the guy in the keynote who taught himself to code at 89. That’s insane to me. I talk to ChatGPT users who help their autistic kids by modeling social interactions. I talk to people who are entirely self-taught in a language based on what they do with ChatGPT. Like, that is the mission.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I don’t think it’s fair to talk about the consumer business as a funding vehicle. Rather, it’s the expression. That is one way in which OpenAI has evolved, to me at least, since I’ve joined.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Let’s dive deeper into the apps that were announced today. OpenAI has said that third parties can only take the “minimum amount of data” necessary to run an app in ChatGPT. How are you thinking about user privacy?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;From day one, we’re going to ask developers to disclose to users what information they’re requesting. We’re also only going to let [apps] go live if they are reasonable in the data that they request. We published our developer guidelines [at launch] so people won’t be surprised when we reject their app because it doesn’t comply with our stance on privacy.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Over the next month, we want to build ways for users to give fine-grained access to developers. I think Apple has done a phenomenal job with this, where you can share data just this time, or all the time, etc.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To do that well, we might need some concept of a partitioned memory in ChatGPT, which we’re still thinking through. But we’re really excited about the idea because you might want to keep certain conversations, like health, separate from others, such as music. Users may want to share one, but not the other, with an app. So we’re going to have a lot more to share soon, because it’s actually a combined research and engineering challenge to do this well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The thing that’s uncompromisable for us is transparency. We want users, at all points, to understand what data might be going to a third party, but the controls will come over time as we build them out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;DoorDash and Instacart are two companies that will have apps in ChatGPT soon enough. If I want to order some snacks, how will ChatGPT know which one to go to?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is the classic question. The best way to start is you show them both. If you’ve used one of them before, we’ll prioritize that one. If you’ve used both, we’ll ask which one you prefer. We could get more sophisticated over time. You could imagine one of these apps being much higher quality than another. Maybe there would be reason to prioritize one over the other. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We have multiple partners in the same product categories. I think the most graceful and respectful way to handle that is to serve both apps.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Are you thinking about letting companies pay for their apps to have preferential spot placement in ChatGPT?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is one of the things we’re hoping to do some discovery on with developers. There’s this trade-off. You could try to figure it all out in advance, and roll it out with the announcement, but that probably means you didn’t talk to a lot of people. Or you could delay it, which means everyone’s asking questions and doesn’t know exactly what’s going to happen, but it gives us the ability to actually engage.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;We chose the latter just because we know that building this ecosystem is going to be a long game. It’s not going to happen on day one, and therefore it’s better to be thoughtful on what sort of distribution mechanisms are and aren’t fair game.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At the end of the day, we want a great user experience. So if that would lead to apps [surfacing] that are irrelevant to the user, I don’t think we’d like it. If this was a lever that helped us prioritize apps that are really serious because they’re clearly trying to invest in exposure, it could be a good thing. We have no point of view as of today. It’s certainly something that’s come up with different partners.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/openais-nick-turley-on-transforming-chatgpt-into-an-operating-system/</guid><pubDate>Wed, 08 Oct 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] To scale agentic AI, Notion tore down its tech stack and started fresh (AI | VentureBeat)</title><link>https://venturebeat.com/ai/to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh</link><description>[unable to retrieve full-text content]&lt;p&gt;Many organizations would be hesitant to overhaul their tech stack and start from scratch. 

Not &lt;a href="https://www.notion.com/"&gt;Notion&lt;/a&gt;. 

For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale.

Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. 

“Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We&amp;#x27;ve rebuilt a new architecture because workflows are different from agents.”&lt;/p&gt;&lt;h2&gt;Re-orchestrating so models can work autonomously&lt;/h2&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/notion-bets-big-on-integrated-llms-adds-gpt-4-1-and-claude-3-7-to-platform"&gt;Notion&lt;/a&gt; has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. 

In a rapidly evolving AI landscape, the company identified the need to move beyond simpler, task-based workflows to goal-oriented reasoning systems that allow agents to autonomously select, orchestrate, and execute tools across connected environments. &lt;/p&gt;&lt;p&gt;Very quickly, &lt;a href="https://venturebeat.com/ai/teaching-the-model-designing-llm-feedback-loops-that-get-smarter-over-time"&gt;reasoning models&lt;/a&gt; have become “far better” at learning to use tools and follow chain-of-thought (CoT) instructions, Sachs noted. This allows them to be “far more independent” and make multiple decisions within one agentic workflow. “We rebuilt our AI system to play to that,&amp;quot; she said. 

From an engineering perspective, this meant replacing rigid prompt-based flows with a unified orchestration model, Sachs explained. This core model is supported by modular sub-agents that search Notion and the web, query and add to databases and edit content. 

Each agent uses tools contextually; for instance, they can decide whether to search Notion itself, or another platform like Slack. The model will perform successive searches until the relevant information is found. It can then, for instance, convert notes into proposals, create follow-up messages, track tasks, and spot and make updates in knowledge bases. 

In Notion 2.0, the team focused on having AI perform specific tasks, which required them to “think exhaustively” about how to prompt the model, Sachs noted. However, with version 3.0, users can assign tasks to agents, and agents can actually take action and perform multiple tasks concurrently.  

“We reorchestrated it to be self-selecting on the tools, rather than few-shotting, which is explicitly prompting how to go through all these different scenarios,” Sachs explained. The aim is to ensure everything interfaces with AI and that “anything you can do, your Notion agent can do.”&lt;/p&gt;&lt;h2&gt;Bifurcating to isolate hallucinations&lt;/h2&gt;&lt;p&gt;Notion’s philosophy of “better, faster, cheaper,” drives a continuous iteration cycle that balances latency and accuracy through fine-tuned vector embeddings and elastic search optimization. Sachs’ team employs a rigorous evaluation framework that combines deterministic tests, vernacular optimization, human-annotated data and LLMs-as-a-judge, with model-based scoring identifying discrepancies and inaccuracies. 

“By bifurcating the evaluation, we&amp;#x27;re able to identify where the problems come from, and that helps us isolate unnecessary hallucinations,” Sachs explained. Further, making the architecture itself simpler means it’s easier to make changes as models and techniques evolve. 

“We optimize latency and parallel thinking as much as possible,” which leads to “way better accuracy,” Sachs noted. Models are grounded in data from the web and the Notion connected workspace. 

Ultimately, Sachs reported, the investment in rebuilding its architecture has already provided Notion returns in terms of capability and faster rate of change. 

She added, “We are fully open to rebuilding it again, when the next breakthrough happens, if we have to.”&lt;/p&gt;&lt;h2&gt;Understanding contextual latency&lt;/h2&gt;&lt;p&gt;When building and fine-tuning models, it’s important to understand that latency is subjective: AI must provide the most relevant information, not necessarily the most, at the cost of speed. 

“You&amp;#x27;d be surprised at the different ways customers are willing to wait for things and not wait for things,” Sachs said. It makes for an interesting experiment: How slow can you go before people abandon the model?

With pure navigational search, for instance, users may not be as patient; they want answers near-immediately. “If you ask, ‘What&amp;#x27;s two plus two,’ you don&amp;#x27;t want to wait for your agent to be searching everywhere in Slack and JIRA,” Sachs pointed out. 

But the longer the time it&amp;#x27;s given, the more exhaustive a reasoning agent can be. For instance, Notion can perform &lt;a href="https://venturebeat.com/ai/the-anti-chatgpt-thomson-reuters-multi-agent-system-slashes-20-hour-tasks-to"&gt;20 minutes of autonomous work&lt;/a&gt; across hundreds of websites, files and other materials. In these instances, users are more willing to wait, Sachs explained; they allow the model to execute in the background while they attend to other tasks. 

“It&amp;#x27;s a product question,” said Sachs. “How do we set user expectations from the UI? How do we ascertain user expectations on latency?”&lt;/p&gt;&lt;h2&gt;Notion is its biggest user&lt;/h2&gt;&lt;p&gt;Notion understands the importance of using its own product — in fact, its employees are among its biggest power users. 

Sachs explained that teams have active sandboxes that generate training and evaluation data, as well as a “really active” thumbs-up-thumbs-down user feedback loop. Users aren’t shy about saying what they think should be improved or features they’d like to see. 

Sachs emphasized that when a user thumbs down an interaction, they are explicitly giving permission to a human annotator to analyze that interaction in a way that de-anonymizes them as much as possible. 

“We are using our own tool as a company all day, every day, and so we get really fast feedback loops,” said Sachs. “We’re really dogfooding our own product.” 

That said, it’s their own product they’re building, Sachs noted, so they understand that they may have goggles on when it comes to quality and functionality. To balance this out, Notion has trusted &amp;quot;very AI-savvy&amp;quot; design partners who are granted early access to new capabilities and provide important feedback. 

Sachs emphasized that this is just as important as internal prototyping. 

“We&amp;#x27;re all about experimenting in the open, I think you get much richer feedback,” said Sachs. “Because at the end of the day, if we just look at how Notion uses Notion, we&amp;#x27;re not really giving the best experience to our customers.” 

Just as importantly, continuous internal testing allows teams to evaluate progressions and make sure models aren&amp;#x27;t regressing (when accuracy and performance degrades over time). &amp;quot;Everything you&amp;#x27;re doing stays faithful,&amp;quot; Sachs explained. &amp;quot;You know that your latency is within bounds.&amp;quot; &lt;/p&gt;&lt;p&gt;Many companies make the mistake of focusing too intensely on retroactively-focused evans; this makes it difficult for them to understand how or where they&amp;#x27;re improving, Sachs pointed out. Notion considers evals as a &amp;quot;litmus test&amp;quot; of development and forward-looking progression and evals of observability and regression proofing. 

“I think a big mistake a lot of companies make is conflating the two,” said Sachs. “We use them for both purposes; we think about them really differently.”&lt;/p&gt;&lt;h2&gt;Takeaways from Notion&amp;#x27;s journey&lt;/h2&gt;&lt;p&gt;For enterprises, Notion can serve as a blueprint for how to responsibly and dynamically operationalize agentic AI in a connected, permissioned enterprise workspace. 

Sach’s takeaways for other tech leaders: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Don’t be afraid to rebuild when foundational capabilities change; Notion fully re-engineered its architecture to align with reasoning-based models.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Treat latency as contextual: Optimize per use case, rather than universally. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ground all outputs in trustworthy, curated enterprise data to ensure accuracy and trust. 

She advised: “Be willing to make the hard decisions. Be willing to sit at the top of the frontier, so to speak, on what you&amp;#x27;re developing to build the best product you can for your customers.” &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Many organizations would be hesitant to overhaul their tech stack and start from scratch. 

Not &lt;a href="https://www.notion.com/"&gt;Notion&lt;/a&gt;. 

For the 3.0 version of its productivity software (released in September), the company didn’t hesitate to rebuild from the ground up; they recognized that it was necessary, in fact, to support agentic AI at enterprise scale.

Whereas traditional AI-powered workflows involve explicit, step-by-step instructions based on few-shot learning, AI agents powered by advanced reasoning models are thoughtful about tool definition, can identify and comprehend what tools they have at their disposal and plan next steps. 

“Rather than trying to retrofit into what we were building, we wanted to play to the strengths of reasoning models,” Sarah Sachs, Notion’s head of AI modeling, told VentureBeat. “We&amp;#x27;ve rebuilt a new architecture because workflows are different from agents.”&lt;/p&gt;&lt;h2&gt;Re-orchestrating so models can work autonomously&lt;/h2&gt;&lt;p&gt;&lt;a href="https://venturebeat.com/ai/notion-bets-big-on-integrated-llms-adds-gpt-4-1-and-claude-3-7-to-platform"&gt;Notion&lt;/a&gt; has been adopted by 94% of Forbes AI 50 companies, has 100 million total users and counts among its customers OpenAI, Cursor, Figma, Ramp and Vercel. 

In a rapidly evolving AI landscape, the company identified the need to move beyond simpler, task-based workflows to goal-oriented reasoning systems that allow agents to autonomously select, orchestrate, and execute tools across connected environments. &lt;/p&gt;&lt;p&gt;Very quickly, &lt;a href="https://venturebeat.com/ai/teaching-the-model-designing-llm-feedback-loops-that-get-smarter-over-time"&gt;reasoning models&lt;/a&gt; have become “far better” at learning to use tools and follow chain-of-thought (CoT) instructions, Sachs noted. This allows them to be “far more independent” and make multiple decisions within one agentic workflow. “We rebuilt our AI system to play to that,&amp;quot; she said. 

From an engineering perspective, this meant replacing rigid prompt-based flows with a unified orchestration model, Sachs explained. This core model is supported by modular sub-agents that search Notion and the web, query and add to databases and edit content. 

Each agent uses tools contextually; for instance, they can decide whether to search Notion itself, or another platform like Slack. The model will perform successive searches until the relevant information is found. It can then, for instance, convert notes into proposals, create follow-up messages, track tasks, and spot and make updates in knowledge bases. 

In Notion 2.0, the team focused on having AI perform specific tasks, which required them to “think exhaustively” about how to prompt the model, Sachs noted. However, with version 3.0, users can assign tasks to agents, and agents can actually take action and perform multiple tasks concurrently.  

“We reorchestrated it to be self-selecting on the tools, rather than few-shotting, which is explicitly prompting how to go through all these different scenarios,” Sachs explained. The aim is to ensure everything interfaces with AI and that “anything you can do, your Notion agent can do.”&lt;/p&gt;&lt;h2&gt;Bifurcating to isolate hallucinations&lt;/h2&gt;&lt;p&gt;Notion’s philosophy of “better, faster, cheaper,” drives a continuous iteration cycle that balances latency and accuracy through fine-tuned vector embeddings and elastic search optimization. Sachs’ team employs a rigorous evaluation framework that combines deterministic tests, vernacular optimization, human-annotated data and LLMs-as-a-judge, with model-based scoring identifying discrepancies and inaccuracies. 

“By bifurcating the evaluation, we&amp;#x27;re able to identify where the problems come from, and that helps us isolate unnecessary hallucinations,” Sachs explained. Further, making the architecture itself simpler means it’s easier to make changes as models and techniques evolve. 

“We optimize latency and parallel thinking as much as possible,” which leads to “way better accuracy,” Sachs noted. Models are grounded in data from the web and the Notion connected workspace. 

Ultimately, Sachs reported, the investment in rebuilding its architecture has already provided Notion returns in terms of capability and faster rate of change. 

She added, “We are fully open to rebuilding it again, when the next breakthrough happens, if we have to.”&lt;/p&gt;&lt;h2&gt;Understanding contextual latency&lt;/h2&gt;&lt;p&gt;When building and fine-tuning models, it’s important to understand that latency is subjective: AI must provide the most relevant information, not necessarily the most, at the cost of speed. 

“You&amp;#x27;d be surprised at the different ways customers are willing to wait for things and not wait for things,” Sachs said. It makes for an interesting experiment: How slow can you go before people abandon the model?

With pure navigational search, for instance, users may not be as patient; they want answers near-immediately. “If you ask, ‘What&amp;#x27;s two plus two,’ you don&amp;#x27;t want to wait for your agent to be searching everywhere in Slack and JIRA,” Sachs pointed out. 

But the longer the time it&amp;#x27;s given, the more exhaustive a reasoning agent can be. For instance, Notion can perform &lt;a href="https://venturebeat.com/ai/the-anti-chatgpt-thomson-reuters-multi-agent-system-slashes-20-hour-tasks-to"&gt;20 minutes of autonomous work&lt;/a&gt; across hundreds of websites, files and other materials. In these instances, users are more willing to wait, Sachs explained; they allow the model to execute in the background while they attend to other tasks. 

“It&amp;#x27;s a product question,” said Sachs. “How do we set user expectations from the UI? How do we ascertain user expectations on latency?”&lt;/p&gt;&lt;h2&gt;Notion is its biggest user&lt;/h2&gt;&lt;p&gt;Notion understands the importance of using its own product — in fact, its employees are among its biggest power users. 

Sachs explained that teams have active sandboxes that generate training and evaluation data, as well as a “really active” thumbs-up-thumbs-down user feedback loop. Users aren’t shy about saying what they think should be improved or features they’d like to see. 

Sachs emphasized that when a user thumbs down an interaction, they are explicitly giving permission to a human annotator to analyze that interaction in a way that de-anonymizes them as much as possible. 

“We are using our own tool as a company all day, every day, and so we get really fast feedback loops,” said Sachs. “We’re really dogfooding our own product.” 

That said, it’s their own product they’re building, Sachs noted, so they understand that they may have goggles on when it comes to quality and functionality. To balance this out, Notion has trusted &amp;quot;very AI-savvy&amp;quot; design partners who are granted early access to new capabilities and provide important feedback. 

Sachs emphasized that this is just as important as internal prototyping. 

“We&amp;#x27;re all about experimenting in the open, I think you get much richer feedback,” said Sachs. “Because at the end of the day, if we just look at how Notion uses Notion, we&amp;#x27;re not really giving the best experience to our customers.” 

Just as importantly, continuous internal testing allows teams to evaluate progressions and make sure models aren&amp;#x27;t regressing (when accuracy and performance degrades over time). &amp;quot;Everything you&amp;#x27;re doing stays faithful,&amp;quot; Sachs explained. &amp;quot;You know that your latency is within bounds.&amp;quot; &lt;/p&gt;&lt;p&gt;Many companies make the mistake of focusing too intensely on retroactively-focused evans; this makes it difficult for them to understand how or where they&amp;#x27;re improving, Sachs pointed out. Notion considers evals as a &amp;quot;litmus test&amp;quot; of development and forward-looking progression and evals of observability and regression proofing. 

“I think a big mistake a lot of companies make is conflating the two,” said Sachs. “We use them for both purposes; we think about them really differently.”&lt;/p&gt;&lt;h2&gt;Takeaways from Notion&amp;#x27;s journey&lt;/h2&gt;&lt;p&gt;For enterprises, Notion can serve as a blueprint for how to responsibly and dynamically operationalize agentic AI in a connected, permissioned enterprise workspace. 

Sach’s takeaways for other tech leaders: &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;Don’t be afraid to rebuild when foundational capabilities change; Notion fully re-engineered its architecture to align with reasoning-based models.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Treat latency as contextual: Optimize per use case, rather than universally. &lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ground all outputs in trustworthy, curated enterprise data to ensure accuracy and trust. 

She advised: “Be willing to make the hard decisions. Be willing to sit at the top of the frontier, so to speak, on what you&amp;#x27;re developing to build the best product you can for your customers.” &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/to-scale-agentic-ai-notion-tore-down-its-tech-stack-and-started-fresh</guid><pubDate>Wed, 08 Oct 2025 21:08:00 +0000</pubDate></item><item><title>[NEW] Bank of England warns AI stock bubble rivals 2000 dotcom peak (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/10/bank-of-england-warns-ai-stock-bubble-rivals-2000-dotcom-peak/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Central bank says market concentration hasn't been this extreme in 50 years.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Bank of England building in London." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/bank_of_england-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Bank of England building in London." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/bank_of_england-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Bank of England building in London.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Scott E Barbour via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI bubble talk is in the air, and among the chorus of voices warning of an AI-fueled market bubble (which includes OpenAI CEO Sam Altman and Amazon's Jeff Bezos) is the Bank of England, which warned on Wednesday that global financial markets could face a sharp correction if investor sentiment turns negative on AI.&lt;/p&gt;
&lt;p&gt;The UK central bank said US stock valuations resemble those seen near the peak of the dotcom bubble on some measures, with AI-focused companies making up an unprecedented portion of market value.&lt;/p&gt;
&lt;p&gt;In its quarterly report derived from a meeting of its Financial Policy Committee that took place last week, BoE wrote that "the risk of a sharp market correction has increased." Reuters notes that it's the BoE's strongest warning to date about potential AI-driven market declines. The committee, chaired by Governor Andrew Bailey, said spillover risks to Britain's financial system from such a shock were "material."&lt;/p&gt;
&lt;p&gt;The warning comes as the S&amp;amp;P 500 hit a record high on Tuesday, up 14 percent year to date. The BoE noted in its report that 30 percent of the S&amp;amp;P 500's valuation comes from just five companies at the top, which is the most concentrated the index has been in 50 years. These companies include chipmaker Nvidia, Microsoft, Apple, Amazon, and Facebook parent Meta, all of which have invested substantially in AI development.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Share valuations based on past earnings have also reached their highest levels since the dotcom bubble 25 years ago, though the BoE noted they appear less extreme when based on investors' expectations for future profits. "This, when combined with increasing concentration within market indices, leaves equity markets particularly exposed should expectations around the impact of AI become less optimistic," the central bank said.&lt;/p&gt;
&lt;h2&gt;Toil and trouble?&lt;/h2&gt;
&lt;p&gt;The dotcom bubble offers a potentially instructive parallel to our current era. In the late 1990s, investors poured money into Internet companies based on the promise of a transformed economy, seemingly ignoring whether individual businesses had viable paths to profitability. Between 1995 and March 2000, the Nasdaq index rose 600 percent. When sentiment shifted, the correction was severe: the Nasdaq fell 78 percent from its peak, reaching a low point in October 2002.&lt;/p&gt;
&lt;p&gt;Whether we'll see the same thing or worse if an AI bubble pops is mere speculation at this point. But similarly to the early 2000s, the question about today's market isn't necessarily about the utility of AI tools themselves (the Internet was useful, after all, despite the bubble), but whether the amount of money being poured into the companies that sell them is out of proportion with the potential profits those improvements might bring.&lt;/p&gt;
&lt;p&gt;We don't have a crystal ball to determine when such a bubble might pop, or even if it is guaranteed to do so, but we'll likely continue to see more warning signs ahead if AI-related deals continue to grow larger and larger over time.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Central bank says market concentration hasn't been this extreme in 50 years.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="The Bank of England building in London." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/bank_of_england-640x360.jpg" width="640" /&gt;
                  &lt;img alt="The Bank of England building in London." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/10/bank_of_england-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The Bank of England building in London.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Scott E Barbour via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;AI bubble talk is in the air, and among the chorus of voices warning of an AI-fueled market bubble (which includes OpenAI CEO Sam Altman and Amazon's Jeff Bezos) is the Bank of England, which warned on Wednesday that global financial markets could face a sharp correction if investor sentiment turns negative on AI.&lt;/p&gt;
&lt;p&gt;The UK central bank said US stock valuations resemble those seen near the peak of the dotcom bubble on some measures, with AI-focused companies making up an unprecedented portion of market value.&lt;/p&gt;
&lt;p&gt;In its quarterly report derived from a meeting of its Financial Policy Committee that took place last week, BoE wrote that "the risk of a sharp market correction has increased." Reuters notes that it's the BoE's strongest warning to date about potential AI-driven market declines. The committee, chaired by Governor Andrew Bailey, said spillover risks to Britain's financial system from such a shock were "material."&lt;/p&gt;
&lt;p&gt;The warning comes as the S&amp;amp;P 500 hit a record high on Tuesday, up 14 percent year to date. The BoE noted in its report that 30 percent of the S&amp;amp;P 500's valuation comes from just five companies at the top, which is the most concentrated the index has been in 50 years. These companies include chipmaker Nvidia, Microsoft, Apple, Amazon, and Facebook parent Meta, all of which have invested substantially in AI development.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Share valuations based on past earnings have also reached their highest levels since the dotcom bubble 25 years ago, though the BoE noted they appear less extreme when based on investors' expectations for future profits. "This, when combined with increasing concentration within market indices, leaves equity markets particularly exposed should expectations around the impact of AI become less optimistic," the central bank said.&lt;/p&gt;
&lt;h2&gt;Toil and trouble?&lt;/h2&gt;
&lt;p&gt;The dotcom bubble offers a potentially instructive parallel to our current era. In the late 1990s, investors poured money into Internet companies based on the promise of a transformed economy, seemingly ignoring whether individual businesses had viable paths to profitability. Between 1995 and March 2000, the Nasdaq index rose 600 percent. When sentiment shifted, the correction was severe: the Nasdaq fell 78 percent from its peak, reaching a low point in October 2002.&lt;/p&gt;
&lt;p&gt;Whether we'll see the same thing or worse if an AI bubble pops is mere speculation at this point. But similarly to the early 2000s, the question about today's market isn't necessarily about the utility of AI tools themselves (the Internet was useful, after all, despite the bubble), but whether the amount of money being poured into the companies that sell them is out of proportion with the potential profits those improvements might bring.&lt;/p&gt;
&lt;p&gt;We don't have a crystal ball to determine when such a bubble might pop, or even if it is guaranteed to do so, but we'll likely continue to see more warning signs ahead if AI-related deals continue to grow larger and larger over time.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/10/bank-of-england-warns-ai-stock-bubble-rivals-2000-dotcom-peak/</guid><pubDate>Wed, 08 Oct 2025 21:18:30 +0000</pubDate></item><item><title>[NEW] Even after Stargate, Oracle, Nvidia, and AMD, OpenAI has more big deals coming soon, Sam Altman says (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/08/even-after-stargate-oracle-nvidia-and-amd-openai-has-more-big-deals-coming-soon-sam-altman-says/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/Sam-Altman-OpenAI.jpg?resize=1200,680" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At nearly the same moment as Nvidia CEO Jensen Huang was expressing surprise over OpenAI’s multibillion-dollar deal with competitor AMD — shortly after his company agreed to invest up to $100 billion into the AI model maker — Sam Altman was saying that more such deals are in the works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Huang appeared on CNBC’s Squawk Box on Wednesday. When asked if he knew about the AMD deal before it was announced, he answered, “Not really.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As TechCrunch previously reported, OpenAI’s deal with AMD is unusual. AMD has agreed to grant OpenAI large tranches of AMD stock — up to 10% of the company over a period of years contingent on factors like increases in stock price. In exchange, OpenAI will use and help develop the chipmaker’s next-generation AI GPUs chips. This makes OpenAI a shareholder in AMD. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s deal is the reverse. Nvidia has invested in the AI model-making startup, making it a shareholder in OpenAI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI has been using Nvidia gear for years through cloud providers like Microsoft Azure, Oracle OCI, and CoreWeave, “This is the first time we’re going to sell directly to them,” Huang explained. He added that his company would still continue to supply gear to the cloud makers, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These direct sales, which include AI gear beyond GPUs like systems and networking, are intended to “prepare” OpenAI for the day when it is its own “self-hosted hyperscaler,” Huang said. In other words, when it’s using its own data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Huang admits that OpenAI doesn’t “have the money yet” to pay for all of this gear. He estimated that each gigawatt of AI data center will cost OpenAI “$50 to $60 billion,” to cover everything from the land and power to the servers and equipment.&amp;nbsp; &amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;So far, in 2025, OpenAI has commissioned 10 gigawatts’ worth of U.S. facilities through its $500 billion Stargate deal with partners Oracle and SoftBank. (Plus, it penned a $300 billion cloud deal with Oracle.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its partnership with Nvidia was for at least 10 gigawatts of AI data centers. Its partnership with AMD was for 6 gigawatts. Plus its “Stargate UK” partnership involves expanding data centers in the U.K., and it has other European commitments. By some estimates, OpenAI has this year inked $1 trillion worth of such deals. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Similar to the AMD deal, Nvidia’s deal has been criticized for being “circular,” Bloomberg reported. The critics say Nvidia is essentially underwriting OpenAI’s purchases, getting the AI startup’s stock for its efforts.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-altman-to-the-world-expect-more"&gt;Altman to the world: Expect more&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As Huang was dissecting OpenAI’s infrastructure needs on CNBC, OpenAI CEO Sam Altman’s interview with Andreessen Horowitz’s a16z Podcast dropped.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;During the podcast, a16z co-founder Ben Horowitz told Altman that he’s “very impressed by deal structure improvement,” referring to these most recent deals. Andreessen Horowitz is an OpenAI investor, so it would be shocking if he wasn’t impressed. OpenAI has found a way to potentially obtain billions of dollars of equipment on someone else’s dime. Repeatedly.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked about these recent deals, Altman said, “You should expect much more from us in the coming months.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman sees OpenAI’s future models and upcoming other products as so much more capable, thereby fueling so much more demand, that “we have decided that it is time to go make a very aggressive infrastructure bet,” he explained. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem is that OpenAI’s revenue today is currently nowhere near a $1 trillion, though it is, by all accounts, growing rapidly, reportedly hitting $4.5 billion in the first half of 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet Altman obviously believes that eventually all of this investment will pay for itself. “I’ve never been more confident in the research road map in front of us and also the economic value that will come from using those [future] models.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But, he said, OpenAI can’t get to all of that economic lushness on its own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To make the bet at this scale, we kind of need the whole industry, or big chunk of the industry, to support it. And this is from the level of electrons to model distribution and all the stuff in between, which is a lot. So we’re going to partner with a lot of people,” Altman said, with more deals expected in the coming months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So stand by, tech industry. OpenAI is still wheeling and dealing.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/Sam-Altman-OpenAI.jpg?resize=1200,680" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At nearly the same moment as Nvidia CEO Jensen Huang was expressing surprise over OpenAI’s multibillion-dollar deal with competitor AMD — shortly after his company agreed to invest up to $100 billion into the AI model maker — Sam Altman was saying that more such deals are in the works.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Huang appeared on CNBC’s Squawk Box on Wednesday. When asked if he knew about the AMD deal before it was announced, he answered, “Not really.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As TechCrunch previously reported, OpenAI’s deal with AMD is unusual. AMD has agreed to grant OpenAI large tranches of AMD stock — up to 10% of the company over a period of years contingent on factors like increases in stock price. In exchange, OpenAI will use and help develop the chipmaker’s next-generation AI GPUs chips. This makes OpenAI a shareholder in AMD. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nvidia’s deal is the reverse. Nvidia has invested in the AI model-making startup, making it a shareholder in OpenAI.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While OpenAI has been using Nvidia gear for years through cloud providers like Microsoft Azure, Oracle OCI, and CoreWeave, “This is the first time we’re going to sell directly to them,” Huang explained. He added that his company would still continue to supply gear to the cloud makers, too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These direct sales, which include AI gear beyond GPUs like systems and networking, are intended to “prepare” OpenAI for the day when it is its own “self-hosted hyperscaler,” Huang said. In other words, when it’s using its own data centers.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Huang admits that OpenAI doesn’t “have the money yet” to pay for all of this gear. He estimated that each gigawatt of AI data center will cost OpenAI “$50 to $60 billion,” to cover everything from the land and power to the servers and equipment.&amp;nbsp; &amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;So far, in 2025, OpenAI has commissioned 10 gigawatts’ worth of U.S. facilities through its $500 billion Stargate deal with partners Oracle and SoftBank. (Plus, it penned a $300 billion cloud deal with Oracle.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Its partnership with Nvidia was for at least 10 gigawatts of AI data centers. Its partnership with AMD was for 6 gigawatts. Plus its “Stargate UK” partnership involves expanding data centers in the U.K., and it has other European commitments. By some estimates, OpenAI has this year inked $1 trillion worth of such deals. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Similar to the AMD deal, Nvidia’s deal has been criticized for being “circular,” Bloomberg reported. The critics say Nvidia is essentially underwriting OpenAI’s purchases, getting the AI startup’s stock for its efforts.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-altman-to-the-world-expect-more"&gt;Altman to the world: Expect more&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;As Huang was dissecting OpenAI’s infrastructure needs on CNBC, OpenAI CEO Sam Altman’s interview with Andreessen Horowitz’s a16z Podcast dropped.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;During the podcast, a16z co-founder Ben Horowitz told Altman that he’s “very impressed by deal structure improvement,” referring to these most recent deals. Andreessen Horowitz is an OpenAI investor, so it would be shocking if he wasn’t impressed. OpenAI has found a way to potentially obtain billions of dollars of equipment on someone else’s dime. Repeatedly.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When asked about these recent deals, Altman said, “You should expect much more from us in the coming months.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman sees OpenAI’s future models and upcoming other products as so much more capable, thereby fueling so much more demand, that “we have decided that it is time to go make a very aggressive infrastructure bet,” he explained. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The problem is that OpenAI’s revenue today is currently nowhere near a $1 trillion, though it is, by all accounts, growing rapidly, reportedly hitting $4.5 billion in the first half of 2025.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Yet Altman obviously believes that eventually all of this investment will pay for itself. “I’ve never been more confident in the research road map in front of us and also the economic value that will come from using those [future] models.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But, he said, OpenAI can’t get to all of that economic lushness on its own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“To make the bet at this scale, we kind of need the whole industry, or big chunk of the industry, to support it. And this is from the level of electrons to model distribution and all the stuff in between, which is a lot. So we’re going to partner with a lot of people,” Altman said, with more deals expected in the coming months.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;So stand by, tech industry. OpenAI is still wheeling and dealing.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/08/even-after-stargate-oracle-nvidia-and-amd-openai-has-more-big-deals-coming-soon-sam-altman-says/</guid><pubDate>Wed, 08 Oct 2025 23:00:06 +0000</pubDate></item><item><title>[NEW] New memory framework builds AI agents that can handle the real world's unpredictability (AI | VentureBeat)</title><link>https://venturebeat.com/ai/new-memory-framework-builds-ai-agents-that-can-handle-the-real-worlds</link><description>[unable to retrieve full-text content]&lt;p&gt;Researchers at the &lt;a href="https://illinois.edu/"&gt;&lt;u&gt;University of Illinois Urbana-Champaign&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://research.google/teams/cloud-ai-research/"&gt;&lt;u&gt;Google Cloud AI Research&lt;/u&gt;&lt;/a&gt; have developed a framework that enables large language model (LLM) agents to organize their experiences into a memory bank, helping them get better at complex tasks over time.&lt;/p&gt;&lt;p&gt;The framework, called &lt;a href="https://arxiv.org/abs/2509.25140"&gt;&lt;u&gt;ReasoningBank&lt;/u&gt;&lt;/a&gt;, distills “generalizable reasoning strategies” from an agent’s successful and failed attempts to solve problems. The agent then uses this memory during inference to avoid repeating past mistakes and make better decisions as it faces new problems. The researchers show that when combined with &lt;a href="https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models-and-allows-them-to-outperform-llms"&gt;&lt;u&gt;test-time scaling techniques&lt;/u&gt;&lt;/a&gt;, where an agent makes multiple attempts at a problem, ReasoningBank significantly improves the performance and efficiency of LLM agents.&lt;/p&gt;&lt;p&gt;Their findings show that ReasoningBank consistently outperforms classic memory mechanisms across web browsing and software engineering benchmarks, offering a practical path toward building more adaptive and reliable AI agents for enterprise applications.&lt;/p&gt;&lt;h2&gt;The challenge of LLM agent memory&lt;/h2&gt;&lt;p&gt;As LLM agents are deployed in applications that run for long periods, they encounter a continuous stream of tasks. One of the key limitations of current LLM agents is their failure to learn from this accumulated experience. By approaching each task in isolation, they inevitably repeat past mistakes, discard valuable insights from related problems, and fail to develop skills that would make them more capable over time.&lt;/p&gt;&lt;p&gt;The solution to this limitation is to give agents some kind of memory. Previous efforts to give agents memory have focused on storing past interactions for reuse by organizing information in various forms from plain text to structured graphs. However, these approaches often fall short. Many use raw interaction logs or only store successful task examples. This means they can&amp;#x27;t distill higher-level, transferable reasoning patterns and, crucially, they don’t extract and use the valuable information from the agent’s failures. As the researchers note in their paper, “existing memory designs often remain limited to passive record-keeping rather than providing actionable, generalizable guidance for future decisions.”&lt;/p&gt;&lt;h2&gt;How ReasoningBank works&lt;/h2&gt;&lt;p&gt;ReasoningBank is a memory framework designed to overcome these limitations. Its central idea is to distill useful strategies and reasoning hints from past experiences into structured memory items that can be stored and reused.&lt;/p&gt;&lt;p&gt;According to Jun Yan, a Research Scientist at Google and co-author of the paper, this marks a fundamental shift in how agents operate. &amp;quot;Traditional agents operate statically—each task is processed in isolation,&amp;quot; Yan explained. &amp;quot;ReasoningBank changes this by turning every task experience (successful or failed) into structured, reusable reasoning memory. As a result, the agent doesn’t start from scratch with each customer; it recalls and adapts proven strategies from similar past cases.&amp;quot;&lt;/p&gt;&lt;p&gt;The framework processes both successful and failed experiences and turns them into a collection of useful strategies and preventive lessons. The agent judges success and failure through &lt;a href="https://venturebeat.com/ai/metas-self-taught-evaluator-enables-llms-to-create-their-own-training-data"&gt;&lt;u&gt;LLM-as-a-judge schemes&lt;/u&gt;&lt;/a&gt; to obviate the need for human labeling.&lt;/p&gt;&lt;p&gt;Yan provides a practical example of this process in action. An agent tasked with finding Sony headphones might fail because its broad search query returns over 4,000 irrelevant products. &amp;quot;ReasoningBank will first try to figure out why this approach failed,&amp;quot; Yan said. &amp;quot;It will then distill strategies such as ‘optimize search query’ and ‘confine products with category filtering.’ Those strategies will be extremely useful to get future similar tasks successfully done.&amp;quot;&lt;/p&gt;&lt;p&gt;The process operates in a closed loop. When an agent faces a new task, it uses an embedding-based search to retrieve relevant memories from ReasoningBank to guide its actions. These memories are inserted into the agent’s system prompt, providing context for its decision-making. Once the task is completed, the framework creates new memory items to extract insights from successes and failures. This new knowledge is then analyzed, distilled, and merged into the ReasoningBank, allowing the agent to continuously evolve and improve its capabilities.&lt;/p&gt;&lt;h2&gt;Supercharging memory with scaling&lt;/h2&gt;&lt;p&gt;The researchers found a powerful synergy between memory and &lt;a href="https://venturebeat.com/ai/deepmind-new-inference-time-scaling-technique-improves-planning-accuracy-in-llms"&gt;&lt;u&gt;test-time scaling&lt;/u&gt;&lt;/a&gt;. Classic test-time scaling involves generating multiple independent answers to the same question, but the researchers argue that this “vanilla form is suboptimal because it does not leverage inherent contrastive signal that arises from redundant exploration on the same problem.”&lt;/p&gt;&lt;p&gt;To address this, they propose Memory-aware Test-Time Scaling (MaTTS), which integrates scaling with ReasoningBank. MaTTS comes in two forms. In “parallel scaling,” the system generates multiple trajectories for the same query, then compares and contrasts them to identify consistent reasoning patterns. In sequential scaling, the agent iteratively refines its reasoning within a single attempt, with the intermediate notes and corrections also serving as valuable memory signals.&lt;/p&gt;&lt;p&gt;This creates a virtuous cycle: the existing memory in ReasoningBank steers the agent toward more promising solutions, while the diverse experiences generated through scaling enable the agent to create higher-quality memories to store in ReasoningBank. &lt;/p&gt;&lt;p&gt;“This positive feedback loop positions memory-driven experience scaling as a new scaling dimension for agents,” the researchers write.&lt;/p&gt;&lt;h2&gt;ReasoningBank in action&lt;/h2&gt;&lt;p&gt;The researchers tested their framework on &lt;a href="https://webarena.dev/"&gt;&lt;u&gt;WebArena &lt;/u&gt;&lt;/a&gt;(web browsing) and &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-Bench-Verified&lt;/u&gt;&lt;/a&gt; (software engineering) benchmarks, using models like Google’s Gemini 2.5 Pro and Anthropic’s Claude 3.7 Sonnet. They compared ReasoningBank against baselines including memory-free agents and agents using trajectory-based or workflow-based memory frameworks.&lt;/p&gt;&lt;p&gt;The results show that ReasoningBank consistently outperforms these baselines across all datasets and LLM backbones. On WebArena, it improved the overall success rate by up to 8.3 percentage points compared to a memory-free agent. It also generalized better on more difficult, cross-domain tasks, while reducing the number of interaction steps needed to complete tasks. When combined with MaTTS, both parallel and sequential scaling further boosted performance, consistently outperforming standard test-time scaling.&lt;/p&gt;&lt;p&gt;This efficiency gain has a direct impact on operational costs. Yan points to a case where a memory-free agent took eight trial-and-error steps just to find the right product filter on a website. &amp;quot;Those trial and error costs could be avoided by leveraging relevant insights from ReasoningBank,&amp;quot; he noted. &amp;quot;In this case, we save almost twice the operational costs,&amp;quot; which also improves the user experience by resolving issues faster.&lt;/p&gt;&lt;p&gt;For enterprises, ReasoningBank can help develop cost-effective agents that can learn from experience and adapt over time in complex workflows and areas like software development, customer support, and data analysis. As the paper concludes, “Our findings suggest a practical pathway toward building adaptive and lifelong-learning agents.”&lt;/p&gt;&lt;p&gt;Yan confirmed that their findings point toward a future of truly compositional intelligence. For example, a coding agent could learn discrete skills like API integration and database management from separate tasks. &amp;quot;Over time, these modular skills... become building blocks the agent can flexibly recombine to solve more complex tasks,&amp;quot; he said, suggesting a future where agents can autonomously assemble their knowledge to manage entire workflows with minimal human oversight.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Researchers at the &lt;a href="https://illinois.edu/"&gt;&lt;u&gt;University of Illinois Urbana-Champaign&lt;/u&gt;&lt;/a&gt; and &lt;a href="https://research.google/teams/cloud-ai-research/"&gt;&lt;u&gt;Google Cloud AI Research&lt;/u&gt;&lt;/a&gt; have developed a framework that enables large language model (LLM) agents to organize their experiences into a memory bank, helping them get better at complex tasks over time.&lt;/p&gt;&lt;p&gt;The framework, called &lt;a href="https://arxiv.org/abs/2509.25140"&gt;&lt;u&gt;ReasoningBank&lt;/u&gt;&lt;/a&gt;, distills “generalizable reasoning strategies” from an agent’s successful and failed attempts to solve problems. The agent then uses this memory during inference to avoid repeating past mistakes and make better decisions as it faces new problems. The researchers show that when combined with &lt;a href="https://venturebeat.com/ai/how-test-time-scaling-unlocks-hidden-reasoning-abilities-in-small-language-models-and-allows-them-to-outperform-llms"&gt;&lt;u&gt;test-time scaling techniques&lt;/u&gt;&lt;/a&gt;, where an agent makes multiple attempts at a problem, ReasoningBank significantly improves the performance and efficiency of LLM agents.&lt;/p&gt;&lt;p&gt;Their findings show that ReasoningBank consistently outperforms classic memory mechanisms across web browsing and software engineering benchmarks, offering a practical path toward building more adaptive and reliable AI agents for enterprise applications.&lt;/p&gt;&lt;h2&gt;The challenge of LLM agent memory&lt;/h2&gt;&lt;p&gt;As LLM agents are deployed in applications that run for long periods, they encounter a continuous stream of tasks. One of the key limitations of current LLM agents is their failure to learn from this accumulated experience. By approaching each task in isolation, they inevitably repeat past mistakes, discard valuable insights from related problems, and fail to develop skills that would make them more capable over time.&lt;/p&gt;&lt;p&gt;The solution to this limitation is to give agents some kind of memory. Previous efforts to give agents memory have focused on storing past interactions for reuse by organizing information in various forms from plain text to structured graphs. However, these approaches often fall short. Many use raw interaction logs or only store successful task examples. This means they can&amp;#x27;t distill higher-level, transferable reasoning patterns and, crucially, they don’t extract and use the valuable information from the agent’s failures. As the researchers note in their paper, “existing memory designs often remain limited to passive record-keeping rather than providing actionable, generalizable guidance for future decisions.”&lt;/p&gt;&lt;h2&gt;How ReasoningBank works&lt;/h2&gt;&lt;p&gt;ReasoningBank is a memory framework designed to overcome these limitations. Its central idea is to distill useful strategies and reasoning hints from past experiences into structured memory items that can be stored and reused.&lt;/p&gt;&lt;p&gt;According to Jun Yan, a Research Scientist at Google and co-author of the paper, this marks a fundamental shift in how agents operate. &amp;quot;Traditional agents operate statically—each task is processed in isolation,&amp;quot; Yan explained. &amp;quot;ReasoningBank changes this by turning every task experience (successful or failed) into structured, reusable reasoning memory. As a result, the agent doesn’t start from scratch with each customer; it recalls and adapts proven strategies from similar past cases.&amp;quot;&lt;/p&gt;&lt;p&gt;The framework processes both successful and failed experiences and turns them into a collection of useful strategies and preventive lessons. The agent judges success and failure through &lt;a href="https://venturebeat.com/ai/metas-self-taught-evaluator-enables-llms-to-create-their-own-training-data"&gt;&lt;u&gt;LLM-as-a-judge schemes&lt;/u&gt;&lt;/a&gt; to obviate the need for human labeling.&lt;/p&gt;&lt;p&gt;Yan provides a practical example of this process in action. An agent tasked with finding Sony headphones might fail because its broad search query returns over 4,000 irrelevant products. &amp;quot;ReasoningBank will first try to figure out why this approach failed,&amp;quot; Yan said. &amp;quot;It will then distill strategies such as ‘optimize search query’ and ‘confine products with category filtering.’ Those strategies will be extremely useful to get future similar tasks successfully done.&amp;quot;&lt;/p&gt;&lt;p&gt;The process operates in a closed loop. When an agent faces a new task, it uses an embedding-based search to retrieve relevant memories from ReasoningBank to guide its actions. These memories are inserted into the agent’s system prompt, providing context for its decision-making. Once the task is completed, the framework creates new memory items to extract insights from successes and failures. This new knowledge is then analyzed, distilled, and merged into the ReasoningBank, allowing the agent to continuously evolve and improve its capabilities.&lt;/p&gt;&lt;h2&gt;Supercharging memory with scaling&lt;/h2&gt;&lt;p&gt;The researchers found a powerful synergy between memory and &lt;a href="https://venturebeat.com/ai/deepmind-new-inference-time-scaling-technique-improves-planning-accuracy-in-llms"&gt;&lt;u&gt;test-time scaling&lt;/u&gt;&lt;/a&gt;. Classic test-time scaling involves generating multiple independent answers to the same question, but the researchers argue that this “vanilla form is suboptimal because it does not leverage inherent contrastive signal that arises from redundant exploration on the same problem.”&lt;/p&gt;&lt;p&gt;To address this, they propose Memory-aware Test-Time Scaling (MaTTS), which integrates scaling with ReasoningBank. MaTTS comes in two forms. In “parallel scaling,” the system generates multiple trajectories for the same query, then compares and contrasts them to identify consistent reasoning patterns. In sequential scaling, the agent iteratively refines its reasoning within a single attempt, with the intermediate notes and corrections also serving as valuable memory signals.&lt;/p&gt;&lt;p&gt;This creates a virtuous cycle: the existing memory in ReasoningBank steers the agent toward more promising solutions, while the diverse experiences generated through scaling enable the agent to create higher-quality memories to store in ReasoningBank. &lt;/p&gt;&lt;p&gt;“This positive feedback loop positions memory-driven experience scaling as a new scaling dimension for agents,” the researchers write.&lt;/p&gt;&lt;h2&gt;ReasoningBank in action&lt;/h2&gt;&lt;p&gt;The researchers tested their framework on &lt;a href="https://webarena.dev/"&gt;&lt;u&gt;WebArena &lt;/u&gt;&lt;/a&gt;(web browsing) and &lt;a href="https://www.swebench.com/"&gt;&lt;u&gt;SWE-Bench-Verified&lt;/u&gt;&lt;/a&gt; (software engineering) benchmarks, using models like Google’s Gemini 2.5 Pro and Anthropic’s Claude 3.7 Sonnet. They compared ReasoningBank against baselines including memory-free agents and agents using trajectory-based or workflow-based memory frameworks.&lt;/p&gt;&lt;p&gt;The results show that ReasoningBank consistently outperforms these baselines across all datasets and LLM backbones. On WebArena, it improved the overall success rate by up to 8.3 percentage points compared to a memory-free agent. It also generalized better on more difficult, cross-domain tasks, while reducing the number of interaction steps needed to complete tasks. When combined with MaTTS, both parallel and sequential scaling further boosted performance, consistently outperforming standard test-time scaling.&lt;/p&gt;&lt;p&gt;This efficiency gain has a direct impact on operational costs. Yan points to a case where a memory-free agent took eight trial-and-error steps just to find the right product filter on a website. &amp;quot;Those trial and error costs could be avoided by leveraging relevant insights from ReasoningBank,&amp;quot; he noted. &amp;quot;In this case, we save almost twice the operational costs,&amp;quot; which also improves the user experience by resolving issues faster.&lt;/p&gt;&lt;p&gt;For enterprises, ReasoningBank can help develop cost-effective agents that can learn from experience and adapt over time in complex workflows and areas like software development, customer support, and data analysis. As the paper concludes, “Our findings suggest a practical pathway toward building adaptive and lifelong-learning agents.”&lt;/p&gt;&lt;p&gt;Yan confirmed that their findings point toward a future of truly compositional intelligence. For example, a coding agent could learn discrete skills like API integration and database management from separate tasks. &amp;quot;Over time, these modular skills... become building blocks the agent can flexibly recombine to solve more complex tasks,&amp;quot; he said, suggesting a future where agents can autonomously assemble their knowledge to manage entire workflows with minimal human oversight.&lt;/p&gt;&lt;p&gt;
&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/new-memory-framework-builds-ai-agents-that-can-handle-the-real-worlds</guid><pubDate>Wed, 08 Oct 2025 23:35:00 +0000</pubDate></item></channel></rss>