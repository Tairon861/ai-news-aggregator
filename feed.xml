<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 10 Jun 2025 06:31:37 +0000</lastBuildDate><item><title>[NEW] Apple makes major AI advance with image generation technology rivaling DALL-E and Midjourney (AI News | VentureBeat)</title><link>https://venturebeat.com/ai/apple-makes-major-ai-advance-with-image-generation-technology-rivaling-dall-e-and-midjourney/</link><description>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Apple‘s machine learning research team has developed a breakthrough AI system for generating high-resolution images that could challenge the dominance of diffusion models, the technology powering popular image generators like DALL-E and Midjourney.&lt;/p&gt;



&lt;p&gt;The advancement, detailed in a research paper published last week, introduces “STARFlow,” a system developed by Apple researchers in collaboration with academic partners that combines normalizing flows with autoregressive transformers to achieve what the team calls “competitive performance” with state-of-the-art diffusion models.&lt;/p&gt;



&lt;p&gt;The breakthrough comes at a critical moment for Apple, which has faced mounting criticism over its struggles with artificial intelligence. At Monday’s Worldwide Developers Conference, the company unveiled only modest AI updates to its Apple Intelligence platform, highlighting the competitive pressure facing a company that many view as falling behind in the AI arms race.&lt;/p&gt;



&lt;p&gt;“To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution,” wrote the research team, which includes Apple machine learning researchers Jiatao Gu, Joshua M. Susskind, and Shuangfei Zhai, along with academic collaborators from institutions including The University of California, Berkeley and Georgia Tech.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-apple-is-fighting-back-against-openai-and-google-in-the-ai-wars"&gt;How Apple is fighting back against OpenAI and Google in the AI wars&lt;/h2&gt;



&lt;p&gt;The STARFlow research represents Apple’s broader effort to develop distinctive AI capabilities that could differentiate its products from competitors. While companies like Google and OpenAI have dominated headlines with their generative AI advances, Apple has been working on alternative approaches that could offer unique advantages.&lt;/p&gt;



&lt;p&gt;The research team tackled a fundamental challenge in AI image generation: scaling normalizing flows to work effectively with high-resolution images. Normalizing flows, a type of generative model that learns to transform simple distributions into complex ones, have traditionally been overshadowed by diffusion models and generative adversarial networks in image synthesis applications.&lt;/p&gt;



&lt;p&gt;“STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality,” the researchers wrote, demonstrating the system’s versatility across different types of image synthesis challenges.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-inside-the-mathematical-breakthrough-that-powers-apple-s-new-ai-system"&gt;Inside the mathematical breakthrough that powers Apple’s new AI system&lt;/h2&gt;



&lt;p&gt;Apple’s research team introduced several key innovations to overcome the limitations of existing normalizing flow approaches. The system employs what researchers call a “deep-shallow design,” using “a deep Transformer block [that] captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial.”&lt;/p&gt;



&lt;p&gt;The breakthrough also involves operating in the “latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling,” according to the paper. This approach allows the model to work with compressed representations of images rather than raw pixel data, significantly improving efficiency.&lt;/p&gt;



&lt;p&gt;Unlike diffusion models, which rely on iterative denoising processes, STARFlow maintains the mathematical properties of normalizing flows, enabling “exact maximum likelihood training in continuous spaces without discretization.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-starflow-means-for-apple-s-future-iphone-and-mac-products"&gt;What STARFlow means for Apple’s future iPhone and Mac products&lt;/h2&gt;



&lt;p&gt;The research arrives as Apple faces increasing pressure to demonstrate meaningful progress in artificial intelligence. A recent Bloomberg analysis highlighted how Apple Intelligence and Siri have struggled to compete with rivals. Apple’s modest announcements at WWDC this week underscored the company’s challenges in the AI space.&lt;/p&gt;



&lt;p&gt;For Apple, STARFlow’s exact likelihood training could offer advantages in applications requiring precise control over generated content or in scenarios where understanding model uncertainty is critical for decision-making — potentially valuable for enterprise applications and on-device AI capabilities that Apple has emphasized.&lt;/p&gt;



&lt;p&gt;The research demonstrates that alternative approaches to diffusion models can achieve comparable results, potentially opening new avenues for innovation that could play to Apple’s strengths in hardware-software integration and on-device processing.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-apple-is-betting-on-university-partnerships-to-solve-its-ai-problem"&gt;Why Apple is betting on university partnerships to solve its AI problem&lt;/h2&gt;



&lt;p&gt;The research exemplifies Apple’s strategy of collaborating with leading academic institutions to advance its AI capabilities. Co-author Tianrong Chen, a doctoral student at Georgia Tech who interned with Apple’s machine learning research team, brings expertise in stochastic optimal control and generative modeling.&lt;/p&gt;



&lt;p&gt;The collaboration also includes Ruixiang Zhang from U.C. Berkeley’s mathematics department and Laurent Dinh, a machine learning researcher known for pioneering work on flow-based models at Google Brain and DeepMind.&lt;/p&gt;



&lt;p&gt;“Crucially, our model remains an end-to-end normalizing flow,” the researchers emphasized, distinguishing their approach from hybrid methods that sacrifice mathematical tractability for improved performance.&lt;/p&gt;



&lt;p&gt;The full research paper is available on arXiv, providing technical details for researchers and engineers looking to build upon this work in the competitive field of generative AI. While STARFlow represents a significant technical achievement, the real test will be whether Apple can translate such research breakthroughs into the kind of consumer-facing AI features that have made competitors like ChatGPT household names. For a company that once revolutionized entire industries with products like the iPhone, the question isn’t whether Apple can innovate in AI — it’s whether they can do it fast enough.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</description><content:encoded>&lt;div class="post-boilerplate boilerplate-before" id="boilerplate_2682874"&gt;&lt;!-- wp:paragraph --&gt;
&lt;p&gt;&lt;em&gt;Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.&amp;nbsp;Learn more&lt;/em&gt;&lt;/p&gt;
&lt;!-- /wp:paragraph --&gt;

&lt;!-- wp:separator {"opacity":"css","className":"is-style-wide"} --&gt;
&lt;hr class="wp-block-separator has-css-opacity is-style-wide" /&gt;
&lt;!-- /wp:separator --&gt;&lt;/div&gt;&lt;p&gt;Apple‘s machine learning research team has developed a breakthrough AI system for generating high-resolution images that could challenge the dominance of diffusion models, the technology powering popular image generators like DALL-E and Midjourney.&lt;/p&gt;



&lt;p&gt;The advancement, detailed in a research paper published last week, introduces “STARFlow,” a system developed by Apple researchers in collaboration with academic partners that combines normalizing flows with autoregressive transformers to achieve what the team calls “competitive performance” with state-of-the-art diffusion models.&lt;/p&gt;



&lt;p&gt;The breakthrough comes at a critical moment for Apple, which has faced mounting criticism over its struggles with artificial intelligence. At Monday’s Worldwide Developers Conference, the company unveiled only modest AI updates to its Apple Intelligence platform, highlighting the competitive pressure facing a company that many view as falling behind in the AI arms race.&lt;/p&gt;



&lt;p&gt;“To our knowledge, this work is the first successful demonstration of normalizing flows operating effectively at this scale and resolution,” wrote the research team, which includes Apple machine learning researchers Jiatao Gu, Joshua M. Susskind, and Shuangfei Zhai, along with academic collaborators from institutions including The University of California, Berkeley and Georgia Tech.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-how-apple-is-fighting-back-against-openai-and-google-in-the-ai-wars"&gt;How Apple is fighting back against OpenAI and Google in the AI wars&lt;/h2&gt;



&lt;p&gt;The STARFlow research represents Apple’s broader effort to develop distinctive AI capabilities that could differentiate its products from competitors. While companies like Google and OpenAI have dominated headlines with their generative AI advances, Apple has been working on alternative approaches that could offer unique advantages.&lt;/p&gt;



&lt;p&gt;The research team tackled a fundamental challenge in AI image generation: scaling normalizing flows to work effectively with high-resolution images. Normalizing flows, a type of generative model that learns to transform simple distributions into complex ones, have traditionally been overshadowed by diffusion models and generative adversarial networks in image synthesis applications.&lt;/p&gt;



&lt;p&gt;“STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality,” the researchers wrote, demonstrating the system’s versatility across different types of image synthesis challenges.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-inside-the-mathematical-breakthrough-that-powers-apple-s-new-ai-system"&gt;Inside the mathematical breakthrough that powers Apple’s new AI system&lt;/h2&gt;



&lt;p&gt;Apple’s research team introduced several key innovations to overcome the limitations of existing normalizing flow approaches. The system employs what researchers call a “deep-shallow design,” using “a deep Transformer block [that] captures most of the model representational capacity, complemented by a few shallow Transformer blocks that are computationally efficient yet substantially beneficial.”&lt;/p&gt;



&lt;p&gt;The breakthrough also involves operating in the “latent space of pretrained autoencoders, which proves more effective than direct pixel-level modeling,” according to the paper. This approach allows the model to work with compressed representations of images rather than raw pixel data, significantly improving efficiency.&lt;/p&gt;



&lt;p&gt;Unlike diffusion models, which rely on iterative denoising processes, STARFlow maintains the mathematical properties of normalizing flows, enabling “exact maximum likelihood training in continuous spaces without discretization.”&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-what-starflow-means-for-apple-s-future-iphone-and-mac-products"&gt;What STARFlow means for Apple’s future iPhone and Mac products&lt;/h2&gt;



&lt;p&gt;The research arrives as Apple faces increasing pressure to demonstrate meaningful progress in artificial intelligence. A recent Bloomberg analysis highlighted how Apple Intelligence and Siri have struggled to compete with rivals. Apple’s modest announcements at WWDC this week underscored the company’s challenges in the AI space.&lt;/p&gt;



&lt;p&gt;For Apple, STARFlow’s exact likelihood training could offer advantages in applications requiring precise control over generated content or in scenarios where understanding model uncertainty is critical for decision-making — potentially valuable for enterprise applications and on-device AI capabilities that Apple has emphasized.&lt;/p&gt;



&lt;p&gt;The research demonstrates that alternative approaches to diffusion models can achieve comparable results, potentially opening new avenues for innovation that could play to Apple’s strengths in hardware-software integration and on-device processing.&lt;/p&gt;



&lt;h2 class="wp-block-heading" id="h-why-apple-is-betting-on-university-partnerships-to-solve-its-ai-problem"&gt;Why Apple is betting on university partnerships to solve its AI problem&lt;/h2&gt;



&lt;p&gt;The research exemplifies Apple’s strategy of collaborating with leading academic institutions to advance its AI capabilities. Co-author Tianrong Chen, a doctoral student at Georgia Tech who interned with Apple’s machine learning research team, brings expertise in stochastic optimal control and generative modeling.&lt;/p&gt;



&lt;p&gt;The collaboration also includes Ruixiang Zhang from U.C. Berkeley’s mathematics department and Laurent Dinh, a machine learning researcher known for pioneering work on flow-based models at Google Brain and DeepMind.&lt;/p&gt;



&lt;p&gt;“Crucially, our model remains an end-to-end normalizing flow,” the researchers emphasized, distinguishing their approach from hybrid methods that sacrifice mathematical tractability for improved performance.&lt;/p&gt;



&lt;p&gt;The full research paper is available on arXiv, providing technical details for researchers and engineers looking to build upon this work in the competitive field of generative AI. While STARFlow represents a significant technical achievement, the real test will be whether Apple can translate such research breakthroughs into the kind of consumer-facing AI features that have made competitors like ChatGPT household names. For a company that once revolutionized entire industries with products like the iPhone, the question isn’t whether Apple can innovate in AI — it’s whether they can do it fast enough.&lt;/p&gt;
&lt;div class="post-boilerplate boilerplate-after" id="boilerplate_2660155"&gt;&lt;!-- wp:shortcode --&gt;
		&lt;div class="Boilerplate__newsletter-container vb"&gt;
			&lt;div class="Boilerplate__newsletter-main"&gt;
				&lt;p&gt;&lt;strong&gt;Daily insights on business use cases with VB Daily&lt;/strong&gt;&lt;/p&gt;
				&lt;p class="copy"&gt;If you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.&lt;/p&gt;
				
				&lt;p class="Form__newsletter-legal"&gt;Read our Privacy Policy&lt;/p&gt;
				&lt;p class="Form__success" id="boilerplateNewsletterConfirmation"&gt;
					Thanks for subscribing. Check out more VB newsletters here.
				&lt;/p&gt;
				&lt;p class="Form__error"&gt;An error occured.&lt;/p&gt;
			&lt;/div&gt;

							&lt;div class="image-container"&gt;
					&lt;img alt="alt" src="https://venturebeat.com/wp-content/themes/vb-news/brand/img/vb-daily-phone.png" /&gt;
				&lt;/div&gt;
			
		&lt;/div&gt;
		
&lt;!-- /wp:shortcode --&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/apple-makes-major-ai-advance-with-image-generation-technology-rivaling-dall-e-and-midjourney/</guid><pubDate>Mon, 09 Jun 2025 18:50:06 +0000</pubDate></item><item><title>[NEW] From spatial widgets to realistic Personas: All the visionOS updates Apple announced at WWDC (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/09/from-spatial-widgets-to-realistic-personas-all-the-visionos-updates-apple-announced-at-wwdc/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple’s updates to visionOS 26, the operating system powering its mixed reality headset, build on last year’s Apple Vision Pro spatial computer that blends digital content with the physical world. At WWDC, Apple announced a range of updates for both consumer and enterprise customers, from new spatial widgets and content to more realistic Personas and more.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016950" height="742" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.57.51PM.png?w=680" width="1256" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;All widgets — including Calendar, shown here — are customizable, with a variety of options for frame width, color, and depth&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Apple’s widgets offer personalized and useful information at a glance. With visionOS 26, they become spatial, integrating into your space. You can customize the widgets to the size, color, and depth you like, and place them where you want.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;New widgets include a clock that you can decorate, weather that adapts to the weather outside near you, music for quick access to tunes, and photos that can transform into a panorama or a “window to another space.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-adding-depth-to-2d-images"&gt;&lt;strong&gt;Adding depth to 2D images&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016955" height="706" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.10.05PM.png?w=680" width="1270" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;An update to the visionOS Photos app uses a new AI algorithm that leverages computational depth to create multiple perspectives for your 2D photos, bringing images to life. Apple says it will feel like you can “lean right into them and look around.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spatial browsing on Safari can also make web browsing a more immersive experience. With certain supported articles, spatial browsing can hide distractions and reveal inline photos that “come alive as you scroll.” Developers can also add spatial browsing to their own apps.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-talking-heads-nbsp"&gt;&lt;strong&gt;Talking heads&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016957" height="692" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.10.47PM.png?w=680" width="1250" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;With VisionOS 26, Personas are transformed to feel more natural and familiar&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Apple released Personas, an AI avatar to represent you on video calls, on the Vision Pro as a beta feature last year. With visionOS 26, Apple says Personas “more realistically represent you.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new Personas take advantage of “volumetric rendering and machine learning technology” to enhance everything from how you look in full side profile view to delivering more accurate-looking hair, eyelashes, and complexion. Personas are all created on-device in a “matter of seconds,” Apple says.&lt;/p&gt;


&lt;h2 class="wp-block-heading" id="h-immerse-together"&gt;&lt;strong&gt;Immerse together&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016958" height="696" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.11.27PM.png?w=680" width="1260" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;visionOS 26 lets you and another headset-wearing friend watch a movie or play a spatial game together.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This capability is also being marketed for enterprise clients, allowing users to collaborate. For example, 3D design software company Dassault Systèmes is leveraging the ability with its 3DLive app to visualize 3D designs in person and with remote colleagues.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016967" height="746" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-3.01.12PM.png?w=680" width="1298" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Logitech Muse is a spatial accessory that will enable precise input and new ways to interact with collaboration apps like Spatial Analogue&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;visionOS 26 also lets organizations easily share a common pool of devices among team members, and even securely saves your eye and hand data, vision prescription, and accessibility settings to your iPhone so users can quickly use a shared team device or a friend’s Vision Pro as a guest user.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Apple said it would add more APIs so enterprises can create apps designed for visionOS. There’s a new “for your eyes only” mode that ensures only those who have been given access can see any confidential materials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, Apple announced Logitech Muse built for Vision Pro, a spatial accessory built for the headset that lets you draw and collaborate in 3D with precision.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-other-visionos-26-updates"&gt;&lt;strong&gt;Other visionOS 26 updates&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;More Apple Intelligence features are coming to the Apple Vision Pro. For instance, visionOS 26 supports new languages like French, German, Italian, Japanese, Korean, and Spanish, along with support for English in Australia, Canada, India, Singapore, and the U.K.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can also now “look to scroll” using just their eyes to explore apps and websites. They can also now unlock their iPhone while wearing the Apple Vision Pro, even when wearing the headset, and visionOS supports relaying calls from iPhone so you can accept a call from the Apple Vision Pro.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Apple’s updates to visionOS 26, the operating system powering its mixed reality headset, build on last year’s Apple Vision Pro spatial computer that blends digital content with the physical world. At WWDC, Apple announced a range of updates for both consumer and enterprise customers, from new spatial widgets and content to more realistic Personas and more.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016950" height="742" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.57.51PM.png?w=680" width="1256" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;All widgets — including Calendar, shown here — are customizable, with a variety of options for frame width, color, and depth&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Apple’s widgets offer personalized and useful information at a glance. With visionOS 26, they become spatial, integrating into your space. You can customize the widgets to the size, color, and depth you like, and place them where you want.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;New widgets include a clock that you can decorate, weather that adapts to the weather outside near you, music for quick access to tunes, and photos that can transform into a panorama or a “window to another space.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-adding-depth-to-2d-images"&gt;&lt;strong&gt;Adding depth to 2D images&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016955" height="706" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.10.05PM.png?w=680" width="1270" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;An update to the visionOS Photos app uses a new AI algorithm that leverages computational depth to create multiple perspectives for your 2D photos, bringing images to life. Apple says it will feel like you can “lean right into them and look around.”&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spatial browsing on Safari can also make web browsing a more immersive experience. With certain supported articles, spatial browsing can hide distractions and reveal inline photos that “come alive as you scroll.” Developers can also add spatial browsing to their own apps.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-talking-heads-nbsp"&gt;&lt;strong&gt;Talking heads&amp;nbsp;&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016957" height="692" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.10.47PM.png?w=680" width="1250" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;With VisionOS 26, Personas are transformed to feel more natural and familiar&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Apple released Personas, an AI avatar to represent you on video calls, on the Vision Pro as a beta feature last year. With visionOS 26, Apple says Personas “more realistically represent you.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new Personas take advantage of “volumetric rendering and machine learning technology” to enhance everything from how you look in full side profile view to delivering more accurate-looking hair, eyelashes, and complexion. Personas are all created on-device in a “matter of seconds,” Apple says.&lt;/p&gt;


&lt;h2 class="wp-block-heading" id="h-immerse-together"&gt;&lt;strong&gt;Immerse together&lt;/strong&gt;&lt;/h2&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016958" height="696" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-2.11.27PM.png?w=680" width="1260" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;visionOS 26 lets you and another headset-wearing friend watch a movie or play a spatial game together.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This capability is also being marketed for enterprise clients, allowing users to collaborate. For example, 3D design software company Dassault Systèmes is leveraging the ability with its 3DLive app to visualize 3D designs in person and with remote colleagues.&amp;nbsp;&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3016967" height="746" src="https://techcrunch.com/wp-content/uploads/2025/06/Screenshot-2025-06-09-at-3.01.12PM.png?w=680" width="1298" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Logitech Muse is a spatial accessory that will enable precise input and new ways to interact with collaboration apps like Spatial Analogue&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Apple&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;visionOS 26 also lets organizations easily share a common pool of devices among team members, and even securely saves your eye and hand data, vision prescription, and accessibility settings to your iPhone so users can quickly use a shared team device or a friend’s Vision Pro as a guest user.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Apple said it would add more APIs so enterprises can create apps designed for visionOS. There’s a new “for your eyes only” mode that ensures only those who have been given access can see any confidential materials.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Finally, Apple announced Logitech Muse built for Vision Pro, a spatial accessory built for the headset that lets you draw and collaborate in 3D with precision.&amp;nbsp;&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-other-visionos-26-updates"&gt;&lt;strong&gt;Other visionOS 26 updates&lt;/strong&gt;&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;More Apple Intelligence features are coming to the Apple Vision Pro. For instance, visionOS 26 supports new languages like French, German, Italian, Japanese, Korean, and Spanish, along with support for English in Australia, Canada, India, Singapore, and the U.K.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can also now “look to scroll” using just their eyes to explore apps and websites. They can also now unlock their iPhone while wearing the Apple Vision Pro, even when wearing the headset, and visionOS supports relaying calls from iPhone so you can accept a call from the Apple Vision Pro.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/09/from-spatial-widgets-to-realistic-personas-all-the-visionos-updates-apple-announced-at-wwdc/</guid><pubDate>Mon, 09 Jun 2025 19:23:08 +0000</pubDate></item><item><title>[NEW] OpenAI claims to have hit $10B in annual revenue (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/09/openai-claims-to-have-hit-10b-in-annual-revenue/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI says it recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That figure includes revenue from the company’s consumer products, ChatGPT business products, and its API, an OpenAI spokesperson told CNBC. Currently, OpenAI is serving more than 500 million weekly active users and 3 million paying business customers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The revenue milestone comes roughly two and a half years after OpenAI launched its popular chatbot platform, ChatGPT. The company is targeting $125 billion in revenue by 2029. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is under some pressure to increase revenue quickly. The company burns billions of dollars each year hiring and recruiting talent to work on its AI products, and securing the necessary infrastructure to train and run AI systems. OpenAI has not disclosed its operating expenses or whether it is close to profitability.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2197366846.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI says it recently hit $10 billion in annual recurring revenue, up from around $5.5 billion last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That figure includes revenue from the company’s consumer products, ChatGPT business products, and its API, an OpenAI spokesperson told CNBC. Currently, OpenAI is serving more than 500 million weekly active users and 3 million paying business customers.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The revenue milestone comes roughly two and a half years after OpenAI launched its popular chatbot platform, ChatGPT. The company is targeting $125 billion in revenue by 2029. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI is under some pressure to increase revenue quickly. The company burns billions of dollars each year hiring and recruiting talent to work on its AI products, and securing the necessary infrastructure to train and run AI systems. OpenAI has not disclosed its operating expenses or whether it is close to profitability.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/09/openai-claims-to-have-hit-10b-in-annual-revenue/</guid><pubDate>Mon, 09 Jun 2025 19:43:51 +0000</pubDate></item><item><title>[NEW] Helping machines understand visual content with AI (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/coactive-helps-machines-understand-visual-content-ai-0609</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-Coactive-AI-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Data should drive every decision a modern business makes. But most businesses have a massive blind spot: They don’t know what’s happening in their visual data.&lt;/p&gt;&lt;p&gt;Coactive is working to change that. The company, founded by Cody Coleman ’13, MEng ’15 and William Gaviria Rojas&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;’13, has created an artificial intelligence-powered platform that can make sense of data like images, audio, and video to unlock new insights.&lt;/p&gt;&lt;p&gt;Coactive’s platform can instantly search, organize, and analyze unstructured visual content to help businesses make faster, better decisions.&lt;/p&gt;&lt;p&gt;“In the first big data revolution, businesses got better at getting value out of their structured data,” Coleman says, referring to data from tables and spreadsheets. “But now, approximately 80 to 90 percent of the data in the world is unstructured. In the next chapter of big data, companies will have to process data like images, video, and audio at scale, and AI is a key piece of unlocking that capability.”&lt;/p&gt;&lt;p&gt;Coactive is already working with several large media and retail companies to help them understand their visual content without relying on manual sorting and tagging. That’s helping them get the right content to users faster, remove explicit content from their platforms, and uncover how specific content influences user behavior.&lt;/p&gt;&lt;p&gt;More broadly, the founders believe Coactive serves as an example of how AI can empower humans to work more efficiently and solve new problems.&lt;/p&gt;&lt;p&gt;“The word coactive means to work together concurrently, and that’s our grand vision: helping humans and machines work together,” Coleman says. “We believe that vision is more important now than ever because AI can either pull us apart or bring us together. We want Coactive to be an agent that pulls us together and gives human beings a new set of superpowers.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Giving computers vision&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Coleman met Gaviria Rojas in the summer before their first yearthrough the MIT Interphase Edge program. Both would go on to major in electrical engineering and computer science and work on bringing MIT OpenCourseWare content to Mexican universities, among other projects.&lt;/p&gt;&lt;p&gt;“That was a great example of entrepreneurship,” Coleman recalls of the OpenCourseWare project. “It was really empowering to be responsible for the business and the software development. It led me to start my own small web-development businesses afterward, and to take [the MIT course] Founder’s Journey.”&lt;/p&gt;&lt;p&gt;Coleman first explored the power of AI at MIT while working as a graduate researcher with the Office of Digital Learning (now MIT Open Learning), where he used machine learning to study how humans learn on MITx, which hosts massive, open online courses created by MIT faculty and instructors.&lt;/p&gt;&lt;p&gt;“It was really amazing to me that you could democratize this transformational journey that I went through at MIT with digital learning — and that you could apply AI and machine learning to create adaptive systems that not only help us understand how humans learn, but also deliver more personalized learning experiences to people around the world,” Coleman says of MITx. “That was also the first time I got to explore video content and apply AI to it.”&lt;/p&gt;&lt;p&gt;After MIT, Coleman went to Stanford University for his PhD, where he worked on lowering barriers to using AI. The research led him to work with companies like Pinterest and Meta on AI and machine-learning applications.&lt;/p&gt;&lt;p&gt;“That’s where I was able to see around the corner into the future of what people wanted to do with AI and their content,” Coleman recalls. “I was seeing how leading companies were using AI to drive business value, and that’s where the initial spark for Coactive came from. I thought, ‘What if we create an enterprise-grade operating system for content and multimodal AI to make that easy?’”&lt;/p&gt;&lt;p&gt;Meanwhile, Gaviria Rojas&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;moved to the Bay Area in 2020 and started working as&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;a data scientist at eBay. As part of the move, he needed help transporting his couch, and Coleman was the lucky friend he called.&lt;/p&gt;&lt;p&gt;“On the car ride, we realized we both saw an explosion happening around data and AI,” Gaviria Rojas says. “At MIT, we got a front row seat to the big data revolution, and we saw people inventing technologies to unlock value from that data at scale. Cody and I realized we had another powder keg about to explode with enterprises collecting tremendous amount of data, but this time it was multimodal data like images, video, audio, and text. There was a missing technology to unlock it at scale. That was AI.”&lt;/p&gt;&lt;p&gt;The platform the founders went on to build — what Coleman describes as an “AI operating system” — is model agnostic, meaning the company can swap out the AI systems under the hood as models continue to improve. Coactive’s platform includes prebuilt applications that business customers can use to do things like search through their content, generate metadata, and conduct analytics to extract insights.&lt;/p&gt;&lt;p&gt;“Before AI, computers would see the world through bytes, whereas humans would see the world through vision,” Coleman says. “Now with AI, machines can finally see the world like we do, and that’s going to cause the digital and physical worlds to blur.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Improving the human-computer interface&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Reuters’ database of images supplies the world’s journalists with millions of photos. Before Coactive, the company relied on reporters manually entering tags with each photo so that the right images would show up when journalists searched for certain subjects.&lt;/p&gt;&lt;p&gt;“It was incredible slow and expensive to go through all of these raw assets, so people just didn’t add tags,” Coleman says. “That meant when you searched for things, there were limited results even if relevant photos were in the database.”&lt;/p&gt;&lt;p&gt;Now, when journalists on Reuters’ website select ‘Enable AI Search,’ Coactive can pull up relevant content based on its AI system’s understanding of the details in each image and video.&lt;/p&gt;&lt;p&gt;“It’s vastly improving the quality of results for reporters, which enables them to tell better, more accurate stories than ever before,” Coleman says.&lt;/p&gt;&lt;p&gt;Reuters is not alone in struggling to manage all of its content. Digital asset management is a huge component of many media and retail companies, who today often rely on manually entered metadata for sorting and searching through that content.&lt;/p&gt;&lt;p&gt;Another Coactive customer is Fandom, which is one of the world’s largest platforms for information around TV shows, videogames, and movies with more than 300 million monthly active users. Fandom is using Coactive to understand visual data in their online communities and help remove excessive gore and sexualized content.&lt;/p&gt;&lt;p&gt;“It used to take 24 to 48 hours for Fandom to review each new piece of content,” Coleman says. “Now with Coactive, they’ve codified their community guidelines and can generate finer-grain information in an average of about 500 milliseconds.”&lt;/p&gt;&lt;p&gt;With every use case, the founders see Coactive as enabling a new paradigm in the ways humans work with machines.&lt;/p&gt;&lt;p&gt;“Throughout the history of human-computer interaction, we’ve had to bend over a keyboard and mouse to input information in a way that machines could understand,” Coleman says. “Now, for the first time, we can just speak naturally, we can share images and video with AI, and it can understand that content. That’s a fundamental change in the way we think about human-computer interactions. The core vision of Coactive is because of that change, we need a new operating system and a new way of working with content and AI.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT-Coactive-AI-01-press.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Data should drive every decision a modern business makes. But most businesses have a massive blind spot: They don’t know what’s happening in their visual data.&lt;/p&gt;&lt;p&gt;Coactive is working to change that. The company, founded by Cody Coleman ’13, MEng ’15 and William Gaviria Rojas&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;’13, has created an artificial intelligence-powered platform that can make sense of data like images, audio, and video to unlock new insights.&lt;/p&gt;&lt;p&gt;Coactive’s platform can instantly search, organize, and analyze unstructured visual content to help businesses make faster, better decisions.&lt;/p&gt;&lt;p&gt;“In the first big data revolution, businesses got better at getting value out of their structured data,” Coleman says, referring to data from tables and spreadsheets. “But now, approximately 80 to 90 percent of the data in the world is unstructured. In the next chapter of big data, companies will have to process data like images, video, and audio at scale, and AI is a key piece of unlocking that capability.”&lt;/p&gt;&lt;p&gt;Coactive is already working with several large media and retail companies to help them understand their visual content without relying on manual sorting and tagging. That’s helping them get the right content to users faster, remove explicit content from their platforms, and uncover how specific content influences user behavior.&lt;/p&gt;&lt;p&gt;More broadly, the founders believe Coactive serves as an example of how AI can empower humans to work more efficiently and solve new problems.&lt;/p&gt;&lt;p&gt;“The word coactive means to work together concurrently, and that’s our grand vision: helping humans and machines work together,” Coleman says. “We believe that vision is more important now than ever because AI can either pull us apart or bring us together. We want Coactive to be an agent that pulls us together and gives human beings a new set of superpowers.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Giving computers vision&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Coleman met Gaviria Rojas in the summer before their first yearthrough the MIT Interphase Edge program. Both would go on to major in electrical engineering and computer science and work on bringing MIT OpenCourseWare content to Mexican universities, among other projects.&lt;/p&gt;&lt;p&gt;“That was a great example of entrepreneurship,” Coleman recalls of the OpenCourseWare project. “It was really empowering to be responsible for the business and the software development. It led me to start my own small web-development businesses afterward, and to take [the MIT course] Founder’s Journey.”&lt;/p&gt;&lt;p&gt;Coleman first explored the power of AI at MIT while working as a graduate researcher with the Office of Digital Learning (now MIT Open Learning), where he used machine learning to study how humans learn on MITx, which hosts massive, open online courses created by MIT faculty and instructors.&lt;/p&gt;&lt;p&gt;“It was really amazing to me that you could democratize this transformational journey that I went through at MIT with digital learning — and that you could apply AI and machine learning to create adaptive systems that not only help us understand how humans learn, but also deliver more personalized learning experiences to people around the world,” Coleman says of MITx. “That was also the first time I got to explore video content and apply AI to it.”&lt;/p&gt;&lt;p&gt;After MIT, Coleman went to Stanford University for his PhD, where he worked on lowering barriers to using AI. The research led him to work with companies like Pinterest and Meta on AI and machine-learning applications.&lt;/p&gt;&lt;p&gt;“That’s where I was able to see around the corner into the future of what people wanted to do with AI and their content,” Coleman recalls. “I was seeing how leading companies were using AI to drive business value, and that’s where the initial spark for Coactive came from. I thought, ‘What if we create an enterprise-grade operating system for content and multimodal AI to make that easy?’”&lt;/p&gt;&lt;p&gt;Meanwhile, Gaviria Rojas&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;moved to the Bay Area in 2020 and started working as&lt;strong&gt;&amp;nbsp;&lt;/strong&gt;a data scientist at eBay. As part of the move, he needed help transporting his couch, and Coleman was the lucky friend he called.&lt;/p&gt;&lt;p&gt;“On the car ride, we realized we both saw an explosion happening around data and AI,” Gaviria Rojas says. “At MIT, we got a front row seat to the big data revolution, and we saw people inventing technologies to unlock value from that data at scale. Cody and I realized we had another powder keg about to explode with enterprises collecting tremendous amount of data, but this time it was multimodal data like images, video, audio, and text. There was a missing technology to unlock it at scale. That was AI.”&lt;/p&gt;&lt;p&gt;The platform the founders went on to build — what Coleman describes as an “AI operating system” — is model agnostic, meaning the company can swap out the AI systems under the hood as models continue to improve. Coactive’s platform includes prebuilt applications that business customers can use to do things like search through their content, generate metadata, and conduct analytics to extract insights.&lt;/p&gt;&lt;p&gt;“Before AI, computers would see the world through bytes, whereas humans would see the world through vision,” Coleman says. “Now with AI, machines can finally see the world like we do, and that’s going to cause the digital and physical worlds to blur.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Improving the human-computer interface&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Reuters’ database of images supplies the world’s journalists with millions of photos. Before Coactive, the company relied on reporters manually entering tags with each photo so that the right images would show up when journalists searched for certain subjects.&lt;/p&gt;&lt;p&gt;“It was incredible slow and expensive to go through all of these raw assets, so people just didn’t add tags,” Coleman says. “That meant when you searched for things, there were limited results even if relevant photos were in the database.”&lt;/p&gt;&lt;p&gt;Now, when journalists on Reuters’ website select ‘Enable AI Search,’ Coactive can pull up relevant content based on its AI system’s understanding of the details in each image and video.&lt;/p&gt;&lt;p&gt;“It’s vastly improving the quality of results for reporters, which enables them to tell better, more accurate stories than ever before,” Coleman says.&lt;/p&gt;&lt;p&gt;Reuters is not alone in struggling to manage all of its content. Digital asset management is a huge component of many media and retail companies, who today often rely on manually entered metadata for sorting and searching through that content.&lt;/p&gt;&lt;p&gt;Another Coactive customer is Fandom, which is one of the world’s largest platforms for information around TV shows, videogames, and movies with more than 300 million monthly active users. Fandom is using Coactive to understand visual data in their online communities and help remove excessive gore and sexualized content.&lt;/p&gt;&lt;p&gt;“It used to take 24 to 48 hours for Fandom to review each new piece of content,” Coleman says. “Now with Coactive, they’ve codified their community guidelines and can generate finer-grain information in an average of about 500 milliseconds.”&lt;/p&gt;&lt;p&gt;With every use case, the founders see Coactive as enabling a new paradigm in the ways humans work with machines.&lt;/p&gt;&lt;p&gt;“Throughout the history of human-computer interaction, we’ve had to bend over a keyboard and mouse to input information in a way that machines could understand,” Coleman says. “Now, for the first time, we can just speak naturally, we can share images and video with AI, and it can understand that content. That’s a fundamental change in the way we think about human-computer interactions. The core vision of Coactive is because of that change, we need a new operating system and a new way of working with content and AI.”&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/coactive-helps-machines-understand-visual-content-ai-0609</guid><pubDate>Mon, 09 Jun 2025 19:45:00 +0000</pubDate></item><item><title>[NEW] Envisioning a future where health care tech leaves some behind (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/envisioning-future-where-health-care-tech-leaves-some-behind-0609</link><description>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Will the perfect storm of potentially life-changing, artificial intelligence-driven health care and the desire to increase profits through subscription models alienate vulnerable patients?&lt;/p&gt;&lt;p&gt;For the third year in a row, MIT's&amp;nbsp;Envisioning the Future of Computing Prize asked students to describe, in&amp;nbsp;3,000 words or fewer, how advancements in computing could shape human society for the better or worse. All entries were eligible to win a number of cash prizes.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;Inspired by recent research on the greater effect microbiomes have on overall health, MIT-WHOI Joint Program in Oceanography and Applied Ocean Science and Engineering PhD candidate Annaliese Meyer created the concept of “B-Bots,” a synthetic bacterial mimic designed to regulate gut biomes and activated by Bluetooth.&amp;nbsp;&amp;nbsp;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;For the contest,&amp;nbsp;which challenges MIT students to articulate their musings for what a future driven by advances in computing holds, Meyer submitted a work of speculative fiction about how recipients of a revolutionary new health-care technology find their treatment in jeopardy with the introduction of a subscription-based pay model.&lt;/p&gt;&lt;p&gt;In her winning paper, titled “(Pre/Sub)scribe,” Meyer chronicles the usage of B-Bots from the perspective of both their creator and a B-Bots user named Briar. They celebrate the effects of the supplement, helping them manage vitamin deficiencies and chronic conditions like acid reflux and irritable bowel syndrome. Meyer says that the introduction of a B-Bots subscription model “seemed like a perfect opportunity to hopefully make clear that in a for-profit health-care system, even medical advances that would, in theory, be revolutionary for human health can end up causing more harm than good for the many people on the losing side of the massive wealth disparity in modern society.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/dt2bprtWVtk/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Envisioning the Future of Computing Prize 2025: Annaliese Meyer        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;As a Canadian, Meyer has experienced the differences between the health care systems in the United States and Canada. She recounts her mother’s recent cancer treatments, emphasizing the cost and coverage of treatments in British Columbia when compared to the U.S.&lt;/p&gt;&lt;p&gt;Aside from a cautionary tale of equity in the American health care system, Meyer hopes readers take away an additional scientific message on the complexity of gut microbiomes. Inspired by her thesis work in ocean metaproteomics, Meyer says, “I think a lot about when and why microbes produce different proteins to adapt to environmental changes, and how that depends on the rest of the microbial community and the exchange of metabolic products between organisms.”&lt;/p&gt;&lt;p&gt;Meyer had hoped to participate in the previous year’s contest, but the time constraints of her lab work put her submission on hold. Now in the midst of thesis work, she saw the contest as a way to add some variety to what she was writing while keeping engaged with her scientific interests. However, writing has always been a passion. “I wrote a lot as a kid (‘author’ actually often preceded ‘scientist’ as my dream job while I was in elementary school), and I still write fiction in my spare time,” she says.&lt;/p&gt;&lt;p&gt;Named the winner of the $10,000 grand prize, Meyer says the essay and presentation preparation were extremely rewarding.&lt;/p&gt;&lt;p&gt;“The chance to explore a new topic area which, though related to my field, was definitely out of my comfort zone, really pushed me as a writer and a scientist. It got me reading papers I’d never have found before, and digging into concepts that I’d barely ever encountered. (Did I have any real understanding of the patent process prior to this? Absolutely not.) The presentation dinner itself was a ton of fun; it was great to both be able to celebrate with my friends and colleagues as well as meet people from a bunch of different fields and departments around MIT.”&lt;br /&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Envisioning the future of the computing prize&lt;/strong&gt;&lt;br /&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Co-sponsored by the&amp;nbsp;Social and Ethical Responsibilities of Computing&amp;nbsp;(SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing and the School of Humanities, Arts, and Social Sciences (SHASS), with support from MAC3 Philanthropies, the contest this year attracted 65 submissions from undergraduate and graduate students across various majors, including brain and cognitive sciences, economics, electrical engineering and computer science, physics, anthropology, and others.&lt;/p&gt;&lt;p&gt;Caspar Hare, associate dean of SERC&amp;nbsp;and professor of philosophy, launched the prize in 2023. He says that the object of the prize was “to encourage MIT students to think about what they’re doing, not just in terms of advancing computing-related technologies, but also in terms of how the decisions they make may or may not work to our collective benefit.”&lt;/p&gt;&lt;p&gt;He emphasized that the Envisioning the Future of Computing prize will continue to remain “interesting and important” to the MIT community. There are plans in place to tweak next year’s contest, offering more opportunities for workshops and guidance for those interested in submitting essays.&lt;/p&gt;&lt;p&gt;“Everyone is excited to continue this for as long as it remains relevant, which could be forever,” he says, suggesting that in years to come the prize could give us a series of historical snapshots of what computing-related technologies MIT students found most compelling.&lt;/p&gt;&lt;p&gt;“Computing-related technology is going to be transforming and changing the world. MIT students will remain a big part of that.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Crowning a winner&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As part of a two-stage evaluation process, all the submitted essays were reviewed anonymously by a committee of faculty members from the college, SHASS, and the Department of Urban Studies and Planning. The judges moved forward three finalists based on the papers that were deemed to be the most articulate, thorough, grounded, imaginative, and inspiring.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;In early May,&amp;nbsp;a&amp;nbsp;live awards ceremony was held where the finalists were invited to give 20-minute presentations on their entries and took questions from the audience. Nearly 140 MIT community members, family members, and friends attended the ceremony in support of the finalists. The audience members and judging panel asked the presenters challenging and thoughtful questions on the societal impact of their fictional computing technologies.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;A final tally, which comprised 75 percent of their essay score and 25 percent of their presentation score, determined the winner.&lt;/p&gt;&lt;p&gt;This year’s judging panel included:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Marzyeh Ghassemi, associate professor in electrical engineering and computer science;&lt;/li&gt;&lt;li&gt;Caspar Hare, associate dean of SERC and professor of philosophy;&lt;/li&gt;&lt;li&gt;Jason Jackson, associate professor in political economy and urban planning;&lt;/li&gt;&lt;li&gt;Brad Skow, professor of philosophy;&lt;/li&gt;&lt;li&gt;Armando Solar-Lezama, associate director and chief operating officer of the MIT Computer Science and Artificial Intelligence Laboratory; and&lt;/li&gt;&lt;li&gt;Nikos Trichakis, interim associate dean of SERC and associate professor of operations management.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The judges also awarded $5,000 to the two runners-up:&amp;nbsp;Martin Staadecker, a graduate student in the Technology and Policy Program in the Institute for Data, Systems, and Society, for his essay on a fictional token-based system to track fossil fuels, and Juan Santoyo, a PhD candidate in the Department of Brain and Cognitive Sciences, for his short story of a field-deployed AI designed to help the mental health of soldiers in times of conflict. In addition,&amp;nbsp;eight honorable mentions were recognized, with each receiving a cash prize of $1,000.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;Will the perfect storm of potentially life-changing, artificial intelligence-driven health care and the desire to increase profits through subscription models alienate vulnerable patients?&lt;/p&gt;&lt;p&gt;For the third year in a row, MIT's&amp;nbsp;Envisioning the Future of Computing Prize asked students to describe, in&amp;nbsp;3,000 words or fewer, how advancements in computing could shape human society for the better or worse. All entries were eligible to win a number of cash prizes.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;Inspired by recent research on the greater effect microbiomes have on overall health, MIT-WHOI Joint Program in Oceanography and Applied Ocean Science and Engineering PhD candidate Annaliese Meyer created the concept of “B-Bots,” a synthetic bacterial mimic designed to regulate gut biomes and activated by Bluetooth.&amp;nbsp;&amp;nbsp;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;For the contest,&amp;nbsp;which challenges MIT students to articulate their musings for what a future driven by advances in computing holds, Meyer submitted a work of speculative fiction about how recipients of a revolutionary new health-care technology find their treatment in jeopardy with the introduction of a subscription-based pay model.&lt;/p&gt;&lt;p&gt;In her winning paper, titled “(Pre/Sub)scribe,” Meyer chronicles the usage of B-Bots from the perspective of both their creator and a B-Bots user named Briar. They celebrate the effects of the supplement, helping them manage vitamin deficiencies and chronic conditions like acid reflux and irritable bowel syndrome. Meyer says that the introduction of a B-Bots subscription model “seemed like a perfect opportunity to hopefully make clear that in a for-profit health-care system, even medical advances that would, in theory, be revolutionary for human health can end up causing more harm than good for the many people on the losing side of the massive wealth disparity in modern society.”&lt;/p&gt;        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-youtube-video paragraph--view-mode--default"&gt;
          

            
   

  &lt;div class="news-article--inline-video"&gt;
        &lt;div class="news-article--inline-video--container"&gt;

                             &lt;div class="news-article--inline-video--cover-image"&gt;
                      &lt;img alt="Video thumbnail" height="480" src="https://i1.ytimg.com/vi/dt2bprtWVtk/maxresdefault.jpg" width="640" /&gt;
                    
          Play video
        &lt;/div&gt;
                

                      &lt;/div&gt;
        &lt;div class="news-article--inline-video--caption"&gt;
      

            Envisioning the Future of Computing Prize 2025: Annaliese Meyer        

    &lt;/div&gt;
          &lt;/div&gt;  
        

      &lt;/div&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;As a Canadian, Meyer has experienced the differences between the health care systems in the United States and Canada. She recounts her mother’s recent cancer treatments, emphasizing the cost and coverage of treatments in British Columbia when compared to the U.S.&lt;/p&gt;&lt;p&gt;Aside from a cautionary tale of equity in the American health care system, Meyer hopes readers take away an additional scientific message on the complexity of gut microbiomes. Inspired by her thesis work in ocean metaproteomics, Meyer says, “I think a lot about when and why microbes produce different proteins to adapt to environmental changes, and how that depends on the rest of the microbial community and the exchange of metabolic products between organisms.”&lt;/p&gt;&lt;p&gt;Meyer had hoped to participate in the previous year’s contest, but the time constraints of her lab work put her submission on hold. Now in the midst of thesis work, she saw the contest as a way to add some variety to what she was writing while keeping engaged with her scientific interests. However, writing has always been a passion. “I wrote a lot as a kid (‘author’ actually often preceded ‘scientist’ as my dream job while I was in elementary school), and I still write fiction in my spare time,” she says.&lt;/p&gt;&lt;p&gt;Named the winner of the $10,000 grand prize, Meyer says the essay and presentation preparation were extremely rewarding.&lt;/p&gt;&lt;p&gt;“The chance to explore a new topic area which, though related to my field, was definitely out of my comfort zone, really pushed me as a writer and a scientist. It got me reading papers I’d never have found before, and digging into concepts that I’d barely ever encountered. (Did I have any real understanding of the patent process prior to this? Absolutely not.) The presentation dinner itself was a ton of fun; it was great to both be able to celebrate with my friends and colleagues as well as meet people from a bunch of different fields and departments around MIT.”&lt;br /&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Envisioning the future of the computing prize&lt;/strong&gt;&lt;br /&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Co-sponsored by the&amp;nbsp;Social and Ethical Responsibilities of Computing&amp;nbsp;(SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing and the School of Humanities, Arts, and Social Sciences (SHASS), with support from MAC3 Philanthropies, the contest this year attracted 65 submissions from undergraduate and graduate students across various majors, including brain and cognitive sciences, economics, electrical engineering and computer science, physics, anthropology, and others.&lt;/p&gt;&lt;p&gt;Caspar Hare, associate dean of SERC&amp;nbsp;and professor of philosophy, launched the prize in 2023. He says that the object of the prize was “to encourage MIT students to think about what they’re doing, not just in terms of advancing computing-related technologies, but also in terms of how the decisions they make may or may not work to our collective benefit.”&lt;/p&gt;&lt;p&gt;He emphasized that the Envisioning the Future of Computing prize will continue to remain “interesting and important” to the MIT community. There are plans in place to tweak next year’s contest, offering more opportunities for workshops and guidance for those interested in submitting essays.&lt;/p&gt;&lt;p&gt;“Everyone is excited to continue this for as long as it remains relevant, which could be forever,” he says, suggesting that in years to come the prize could give us a series of historical snapshots of what computing-related technologies MIT students found most compelling.&lt;/p&gt;&lt;p&gt;“Computing-related technology is going to be transforming and changing the world. MIT students will remain a big part of that.”&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Crowning a winner&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;As part of a two-stage evaluation process, all the submitted essays were reviewed anonymously by a committee of faculty members from the college, SHASS, and the Department of Urban Studies and Planning. The judges moved forward three finalists based on the papers that were deemed to be the most articulate, thorough, grounded, imaginative, and inspiring.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;In early May,&amp;nbsp;a&amp;nbsp;live awards ceremony was held where the finalists were invited to give 20-minute presentations on their entries and took questions from the audience. Nearly 140 MIT community members, family members, and friends attended the ceremony in support of the finalists. The audience members and judging panel asked the presenters challenging and thoughtful questions on the societal impact of their fictional computing technologies.&lt;br /&gt;&amp;nbsp;&lt;br /&gt;A final tally, which comprised 75 percent of their essay score and 25 percent of their presentation score, determined the winner.&lt;/p&gt;&lt;p&gt;This year’s judging panel included:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Marzyeh Ghassemi, associate professor in electrical engineering and computer science;&lt;/li&gt;&lt;li&gt;Caspar Hare, associate dean of SERC and professor of philosophy;&lt;/li&gt;&lt;li&gt;Jason Jackson, associate professor in political economy and urban planning;&lt;/li&gt;&lt;li&gt;Brad Skow, professor of philosophy;&lt;/li&gt;&lt;li&gt;Armando Solar-Lezama, associate director and chief operating officer of the MIT Computer Science and Artificial Intelligence Laboratory; and&lt;/li&gt;&lt;li&gt;Nikos Trichakis, interim associate dean of SERC and associate professor of operations management.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The judges also awarded $5,000 to the two runners-up:&amp;nbsp;Martin Staadecker, a graduate student in the Technology and Policy Program in the Institute for Data, Systems, and Society, for his essay on a fictional token-based system to track fossil fuels, and Juan Santoyo, a PhD candidate in the Department of Brain and Cognitive Sciences, for his short story of a field-deployed AI designed to help the mental health of soldiers in times of conflict. In addition,&amp;nbsp;eight honorable mentions were recognized, with each receiving a cash prize of $1,000.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/envisioning-future-where-health-care-tech-leaves-some-behind-0609</guid><pubDate>Mon, 09 Jun 2025 20:10:00 +0000</pubDate></item><item><title>[NEW] AI-enabled control system helps autonomous drones stay on target in uncertain environments (MIT News - Artificial intelligence)</title><link>https://news.mit.edu/2025/ai-enabled-control-system-helps-autonomous-drones-uncertain-environments-0609</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT_MetaLearning-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;An autonomous drone carrying water to help extinguish a wildfire in the Sierra Nevada might encounter swirling Santa Ana winds that threaten to push it off course. Rapidly adapting to these unknown disturbances inflight presents an enormous challenge for the drone’s&amp;nbsp;flight control system.&lt;/p&gt;&lt;p&gt;To help such a drone stay on target, MIT researchers developed a new, machine learning-based adaptive control&amp;nbsp;algorithm that could minimize its deviation from its intended trajectory in the face of unpredictable forces like gusty winds.&lt;/p&gt;&lt;p&gt;Unlike standard approaches, the new technique does not require the person programming the autonomous drone to know anything in advance about the structure of these uncertain disturbances. Instead, the control system’s artificial intelligence model learns all it needs to know from&amp;nbsp;a small amount of&amp;nbsp;observational data collected from 15 minutes of flight time.&lt;/p&gt;&lt;p&gt;Importantly, the technique automatically determines which optimization algorithm it should use to adapt to the disturbances, which improves tracking performance. It chooses the algorithm that best suits the geometry of specific disturbances this drone is facing.&lt;/p&gt;&lt;p&gt;The researchers train their control system to do both things simultaneously using a technique called meta-learning, which teaches the system how to adapt to different types of disturbances.&lt;/p&gt;&lt;p&gt;Taken together, these ingredients enable their adaptive control system to achieve 50 percent less trajectory tracking error than baseline methods in simulations and perform better with new wind speeds it didn’t see during training.&lt;/p&gt;&lt;p&gt;In the future, this adaptive control system could help autonomous drones more efficiently deliver heavy parcels despite strong winds or monitor fire-prone areas of a national park.&lt;/p&gt;&lt;p&gt;“The concurrent learning of these components is what gives our method its strength. By leveraging meta-learning, our controller can automatically make choices that will be best for quick adaptation,” says Navid Azizan, who is the Esther and Harold E. Edgerton Assistant Professor in the MIT Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), a principal investigator of the Laboratory for Information and Decision Systems (LIDS), and the senior author of a paper on this control system.&lt;/p&gt;&lt;p&gt;Azizan is joined on the paper by lead author Sunbochen Tang, a graduate student in the Department of Aeronautics and Astronautics, and Haoyuan Sun, a graduate student in the Department of Electrical Engineering and Computer Science. The research was recently presented at the Learning for Dynamics and Control Conference.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding the right algorithm&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Typically, a control system incorporates a function that models the drone and its environment, and includes some existing information on the structure of potential disturbances. But in a real world filled with uncertain conditions, it is often impossible to hand-design this structure in advance.&lt;/p&gt;&lt;p&gt;Many control systems use an adaptation method based on a popular optimization algorithm, known as gradient descent, to estimate the unknown parts of the problem and determine how to keep the drone as close as possible to its target trajectory during flight. However, gradient descent is only one algorithm in a larger family of algorithms available to choose, known as mirror descent.&lt;/p&gt;&lt;p&gt;“Mirror descent is a general family of algorithms, and for any given problem, one of these algorithms can be more suitable than others. The name of the game is how to choose the particular algorithm that is right for your problem. In our method, we automate this choice,” Azizan says.&lt;/p&gt;&lt;p&gt;In their control system, the researchers replaced the function that contains some structure of potential disturbances with a neural network model that learns to approximate them from data. In this way, they don’t need to have an a priori structure of the wind speeds this drone could encounter in advance.&lt;/p&gt;&lt;p&gt;Their method also uses an algorithm to automatically select the right mirror-descent function while learning the neural network model from data, rather than assuming a user has the ideal function picked out already. The researchers give this algorithm a range of functions to pick from, and it finds the one that best fits the problem at hand.&lt;/p&gt;&lt;p&gt;“Choosing a good distance-generating function to construct the right mirror-descent adaptation matters a lot in getting the right algorithm to reduce the tracking error,” Tang adds.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Learning to adapt&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While the wind speeds the drone may encounter could change every time it takes flight, the controller’s neural network and mirror function should stay the same so they don’t need to be recomputed each time.&lt;/p&gt;&lt;p&gt;To make their controller more flexible, the researchers use meta-learning, teaching it to adapt by showing it a range of wind speed families during training.&lt;/p&gt;&lt;p&gt;“Our method can cope with different objectives because, using meta-learning, we can learn a shared representation through different scenarios efficiently from data,” Tang explains.&lt;/p&gt;&lt;p&gt;In the end, the user feeds the control system a target trajectory and it continuously recalculates, in real-time, how the drone should produce thrust to keep it as close as possible to that trajectory while accommodating the uncertain disturbance it encounters.&lt;/p&gt;&lt;p&gt;In both simulations and real-world experiments, the researchers showed that their method led to significantly less trajectory tracking error than baseline approaches with every wind speed they tested.&lt;/p&gt;&lt;p&gt;“Even if the wind disturbances are much stronger than we had seen during training, our technique shows that it can still handle them successfully,” Azizan adds.&lt;/p&gt;&lt;p&gt;In addition, the margin by which their method outperformed the baselines grew as the wind speeds intensified, showing that it can adapt to challenging environments.&lt;/p&gt;&lt;p&gt;The team is now performing hardware experiments to test their control system on real drones with varying wind conditions and other disturbances.&lt;/p&gt;&lt;p&gt;They also want to extend their method so it can handle disturbances from multiple sources at once. For instance, changing wind speeds could cause the weight of a parcel the drone is carrying to shift in flight, especially when the drone is carrying sloshing payloads.&lt;/p&gt;&lt;p&gt;They also want to explore continual learning, so the drone could adapt to new disturbances without the need to also be retrained on the data it has seen so far.&lt;/p&gt;&lt;p&gt;“Navid and his collaborators have developed breakthrough work that combines meta-learning with conventional adaptive control to learn nonlinear features from data. Key to their approach is the use of mirror descent techniques that exploit the underlying geometry of the problem in ways prior art could not. Their work can contribute significantly to the design of autonomous systems that need to operate in complex and uncertain environments,” says Babak Hassibi, the Mose and Lillian S. Bohn Professor of Electrical Engineering and Computing and Mathematical Sciences at Caltech, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research was supported, in part, by MathWorks, the MIT-IBM Watson AI Lab, the MIT-Amazon Science Hub, and the MIT-Google Program for Computing Innovation.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://news.mit.edu/sites/default/files/images/202506/MIT_MetaLearning-01.jpg" /&gt;&lt;/div&gt;&lt;div class="news-article--content--body--inner"&gt;
            &lt;div class="paragraph paragraph--type--content-block-text paragraph--view-mode--default"&gt;
          

            &lt;p&gt;An autonomous drone carrying water to help extinguish a wildfire in the Sierra Nevada might encounter swirling Santa Ana winds that threaten to push it off course. Rapidly adapting to these unknown disturbances inflight presents an enormous challenge for the drone’s&amp;nbsp;flight control system.&lt;/p&gt;&lt;p&gt;To help such a drone stay on target, MIT researchers developed a new, machine learning-based adaptive control&amp;nbsp;algorithm that could minimize its deviation from its intended trajectory in the face of unpredictable forces like gusty winds.&lt;/p&gt;&lt;p&gt;Unlike standard approaches, the new technique does not require the person programming the autonomous drone to know anything in advance about the structure of these uncertain disturbances. Instead, the control system’s artificial intelligence model learns all it needs to know from&amp;nbsp;a small amount of&amp;nbsp;observational data collected from 15 minutes of flight time.&lt;/p&gt;&lt;p&gt;Importantly, the technique automatically determines which optimization algorithm it should use to adapt to the disturbances, which improves tracking performance. It chooses the algorithm that best suits the geometry of specific disturbances this drone is facing.&lt;/p&gt;&lt;p&gt;The researchers train their control system to do both things simultaneously using a technique called meta-learning, which teaches the system how to adapt to different types of disturbances.&lt;/p&gt;&lt;p&gt;Taken together, these ingredients enable their adaptive control system to achieve 50 percent less trajectory tracking error than baseline methods in simulations and perform better with new wind speeds it didn’t see during training.&lt;/p&gt;&lt;p&gt;In the future, this adaptive control system could help autonomous drones more efficiently deliver heavy parcels despite strong winds or monitor fire-prone areas of a national park.&lt;/p&gt;&lt;p&gt;“The concurrent learning of these components is what gives our method its strength. By leveraging meta-learning, our controller can automatically make choices that will be best for quick adaptation,” says Navid Azizan, who is the Esther and Harold E. Edgerton Assistant Professor in the MIT Department of Mechanical Engineering and the Institute for Data, Systems, and Society (IDSS), a principal investigator of the Laboratory for Information and Decision Systems (LIDS), and the senior author of a paper on this control system.&lt;/p&gt;&lt;p&gt;Azizan is joined on the paper by lead author Sunbochen Tang, a graduate student in the Department of Aeronautics and Astronautics, and Haoyuan Sun, a graduate student in the Department of Electrical Engineering and Computer Science. The research was recently presented at the Learning for Dynamics and Control Conference.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Finding the right algorithm&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Typically, a control system incorporates a function that models the drone and its environment, and includes some existing information on the structure of potential disturbances. But in a real world filled with uncertain conditions, it is often impossible to hand-design this structure in advance.&lt;/p&gt;&lt;p&gt;Many control systems use an adaptation method based on a popular optimization algorithm, known as gradient descent, to estimate the unknown parts of the problem and determine how to keep the drone as close as possible to its target trajectory during flight. However, gradient descent is only one algorithm in a larger family of algorithms available to choose, known as mirror descent.&lt;/p&gt;&lt;p&gt;“Mirror descent is a general family of algorithms, and for any given problem, one of these algorithms can be more suitable than others. The name of the game is how to choose the particular algorithm that is right for your problem. In our method, we automate this choice,” Azizan says.&lt;/p&gt;&lt;p&gt;In their control system, the researchers replaced the function that contains some structure of potential disturbances with a neural network model that learns to approximate them from data. In this way, they don’t need to have an a priori structure of the wind speeds this drone could encounter in advance.&lt;/p&gt;&lt;p&gt;Their method also uses an algorithm to automatically select the right mirror-descent function while learning the neural network model from data, rather than assuming a user has the ideal function picked out already. The researchers give this algorithm a range of functions to pick from, and it finds the one that best fits the problem at hand.&lt;/p&gt;&lt;p&gt;“Choosing a good distance-generating function to construct the right mirror-descent adaptation matters a lot in getting the right algorithm to reduce the tracking error,” Tang adds.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Learning to adapt&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;While the wind speeds the drone may encounter could change every time it takes flight, the controller’s neural network and mirror function should stay the same so they don’t need to be recomputed each time.&lt;/p&gt;&lt;p&gt;To make their controller more flexible, the researchers use meta-learning, teaching it to adapt by showing it a range of wind speed families during training.&lt;/p&gt;&lt;p&gt;“Our method can cope with different objectives because, using meta-learning, we can learn a shared representation through different scenarios efficiently from data,” Tang explains.&lt;/p&gt;&lt;p&gt;In the end, the user feeds the control system a target trajectory and it continuously recalculates, in real-time, how the drone should produce thrust to keep it as close as possible to that trajectory while accommodating the uncertain disturbance it encounters.&lt;/p&gt;&lt;p&gt;In both simulations and real-world experiments, the researchers showed that their method led to significantly less trajectory tracking error than baseline approaches with every wind speed they tested.&lt;/p&gt;&lt;p&gt;“Even if the wind disturbances are much stronger than we had seen during training, our technique shows that it can still handle them successfully,” Azizan adds.&lt;/p&gt;&lt;p&gt;In addition, the margin by which their method outperformed the baselines grew as the wind speeds intensified, showing that it can adapt to challenging environments.&lt;/p&gt;&lt;p&gt;The team is now performing hardware experiments to test their control system on real drones with varying wind conditions and other disturbances.&lt;/p&gt;&lt;p&gt;They also want to extend their method so it can handle disturbances from multiple sources at once. For instance, changing wind speeds could cause the weight of a parcel the drone is carrying to shift in flight, especially when the drone is carrying sloshing payloads.&lt;/p&gt;&lt;p&gt;They also want to explore continual learning, so the drone could adapt to new disturbances without the need to also be retrained on the data it has seen so far.&lt;/p&gt;&lt;p&gt;“Navid and his collaborators have developed breakthrough work that combines meta-learning with conventional adaptive control to learn nonlinear features from data. Key to their approach is the use of mirror descent techniques that exploit the underlying geometry of the problem in ways prior art could not. Their work can contribute significantly to the design of autonomous systems that need to operate in complex and uncertain environments,” says Babak Hassibi, the Mose and Lillian S. Bohn Professor of Electrical Engineering and Computing and Mathematical Sciences at Caltech, who was not involved with this work.&lt;/p&gt;&lt;p&gt;This research was supported, in part, by MathWorks, the MIT-IBM Watson AI Lab, the MIT-Amazon Science Hub, and the MIT-Google Program for Computing Innovation.&lt;/p&gt;        

      &lt;/div&gt;
        &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://news.mit.edu/2025/ai-enabled-control-system-helps-autonomous-drones-uncertain-environments-0609</guid><pubDate>Mon, 09 Jun 2025 20:40:00 +0000</pubDate></item><item><title>[NEW] Still no AI-powered, ‘more personalized’ Siri from Apple at WWDC 25 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/06/09/still-no-ai-powered-more-personalized-siri-from-apple-at-wwdc-25/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/CMC_7975.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At this year’s Worldwide Developers Conference (WWDC 25), Apple announced a slew of updates to its operating systems, services, and software, including a new look it dubbed “Liquid Glass” and a rebranded naming convention. Apple was notably quiet on one highly anticipated product: a more personalized, AI-powered Siri, which it first introduced at last year’s conference.&lt;/p&gt;&lt;p&gt;Apple’s SVP of Software Engineering, Craig Federighi, only gave the Siri update a brief mention during the keynote address, saying, “As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal. This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The time frame of “coming year” seems to indicate that Apple won’t have news before 2026. That’s a significant delay in the AI era, where new models, updates, and upgrades ship at a rapid pace.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;First announced at WWDC 24, the&amp;nbsp;more personalized Siri is expected to bring artificial intelligence updates to the beleaguered virtual assistant built into iPhone and other Apple devices. At the time, the company hyped it as the “next big step for Apple” and said Siri would be able to understand your “personal context,” like your relationships, communications, routine, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the assistant was going to be more useful by allowing you to take action within and across your apps. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Bloomberg reported that the in-development version of the more personalized Siri was functional, it was not consistently working properly. The report said its quality issues meant Siri only performed as it should two-thirds of the time, making it not viable to ship.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple officially announced in March it was&amp;nbsp;pushing back the launch, saying the Siri update would take longer to deliver than anticipated. The company also pulled SVP of Machine Learning and AI Strategy John Giannandrea off the Siri project and put Mike Rockwell, who had worked on the Vision Pro, in charge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The shake-up indicated the company was trying to get back on track after stumbling on a major release. It also suggested Apple’s AI technology was behind that of rivals, like OpenAI, Google, and Anthropic, worrying investors.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;In the meantime, Apple partnered with OpenAI to help close the gap; when users asked Siri questions the assistant couldn’t answer, those could be directed to ChatGPT instead. With the upcoming release, iOS 26, Apple has updated its AI image generation app, Image Playground, to use ChatGPT as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At this year’s WWDC, the company&amp;nbsp;continued&amp;nbsp;to make other AI promises, including developer access to the on-device foundation models, live translation, upgrades to Genmoji (in addition to aforementioned Image Playground), Visual Intelligence improvements, an AI “Workout Buddy” for Apple Watch, AI in Xcode, and the introduction of an updated, AI-powered version of its Shortcuts app for scripting and automation.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/CMC_7975.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At this year’s Worldwide Developers Conference (WWDC 25), Apple announced a slew of updates to its operating systems, services, and software, including a new look it dubbed “Liquid Glass” and a rebranded naming convention. Apple was notably quiet on one highly anticipated product: a more personalized, AI-powered Siri, which it first introduced at last year’s conference.&lt;/p&gt;&lt;p&gt;Apple’s SVP of Software Engineering, Craig Federighi, only gave the Siri update a brief mention during the keynote address, saying, “As we’ve shared, we’re continuing our work to deliver the features that make Siri even more personal. This work needed more time to reach our high-quality bar, and we look forward to sharing more about it in the coming year.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The time frame of “coming year” seems to indicate that Apple won’t have news before 2026. That’s a significant delay in the AI era, where new models, updates, and upgrades ship at a rapid pace.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;First announced at WWDC 24, the&amp;nbsp;more personalized Siri is expected to bring artificial intelligence updates to the beleaguered virtual assistant built into iPhone and other Apple devices. At the time, the company hyped it as the “next big step for Apple” and said Siri would be able to understand your “personal context,” like your relationships, communications, routine, and more.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Plus, the assistant was going to be more useful by allowing you to take action within and across your apps. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While Bloomberg reported that the in-development version of the more personalized Siri was functional, it was not consistently working properly. The report said its quality issues meant Siri only performed as it should two-thirds of the time, making it not viable to ship.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Apple officially announced in March it was&amp;nbsp;pushing back the launch, saying the Siri update would take longer to deliver than anticipated. The company also pulled SVP of Machine Learning and AI Strategy John Giannandrea off the Siri project and put Mike Rockwell, who had worked on the Vision Pro, in charge.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The shake-up indicated the company was trying to get back on track after stumbling on a major release. It also suggested Apple’s AI technology was behind that of rivals, like OpenAI, Google, and Anthropic, worrying investors.&lt;/p&gt;


&lt;p class="wp-block-paragraph"&gt;In the meantime, Apple partnered with OpenAI to help close the gap; when users asked Siri questions the assistant couldn’t answer, those could be directed to ChatGPT instead. With the upcoming release, iOS 26, Apple has updated its AI image generation app, Image Playground, to use ChatGPT as well.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At this year’s WWDC, the company&amp;nbsp;continued&amp;nbsp;to make other AI promises, including developer access to the on-device foundation models, live translation, upgrades to Genmoji (in addition to aforementioned Image Playground), Visual Intelligence improvements, an AI “Workout Buddy” for Apple Watch, AI in Xcode, and the introduction of an updated, AI-powered version of its Shortcuts app for scripting and automation.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/06/09/still-no-ai-powered-more-personalized-siri-from-apple-at-wwdc-25/</guid><pubDate>Mon, 09 Jun 2025 21:01:37 +0000</pubDate></item></channel></rss>