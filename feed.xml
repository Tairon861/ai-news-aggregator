<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 20 Sep 2025 01:27:08 +0000</lastBuildDate><item><title> ()</title><link>https://venturebeat.com/category/ai/feed/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://venturebeat.com/category/ai/feed/</guid></item><item><title>AI medical tools found to downplay symptoms of women, ethnic minorities (AI – Ars Technica)</title><link>https://arstechnica.com/health/2025/09/ai-medical-tools-found-to-downplay-symptoms-of-women-ethnic-minorities/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Bias-reflecting LLMs lead to inferior medical advice for female, Black, and Asian patients.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Montage of AI logs and medical professional holding smartphone" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/ai-medicalllm-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Montage of AI logs and medical professional holding smartphone" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/ai-medicalllm-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Many hospitals and doctors globally are using LLMs such as Gemini and ChatGPT.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          FT montage/Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Artificial intelligence tools used by doctors risk leading to worse health outcomes for women and ethnic minorities, as a growing body of research shows that many large language models downplay the symptoms of these patients.&lt;/p&gt;
&lt;p&gt;A series of recent studies have found that the uptake of AI models across the healthcare sector could lead to biased medical decisions, reinforcing patterns of under-treatment that already exist across different groups in Western societies.&lt;/p&gt;
&lt;p&gt;The findings by researchers at leading US and UK universities suggest that medical AI tools powered by LLMs have a tendency to not reflect the severity of symptoms among female patients, while also displaying less “empathy” toward Black and Asian ones.&lt;/p&gt;
&lt;p&gt;The warnings come as the world’s top AI groups such as Microsoft, Amazon, OpenAI, and Google rush to develop products that aim to reduce physicians’ workloads and speed up treatment, all in an effort to help overstretched health systems around the world.&lt;/p&gt;
&lt;p&gt;Many hospitals and doctors globally are using LLMs such as Gemini and ChatGPT as well as AI medical note-taking apps from start-ups including Nabla and Heidi to auto-generate transcripts of patient visits, highlight medically relevant details, and create clinical summaries.&lt;/p&gt;
&lt;p&gt;In June, Microsoft revealed it had built an AI-powered medical tool it claims is four times more successful than human doctors at diagnosing complex ailments.&lt;/p&gt;
&lt;p&gt;But research by the MIT’s Jameel Clinic in June found that AI models, such as OpenAI’s GPT-4, Meta’s Llama 3, and Palmyra-Med—a healthcare-focused LLM—recommended a much lower level of care for female patients, and suggested some patients self-treat at home instead of seeking help.&lt;/p&gt;
&lt;p&gt;A separate study by the MIT team showed that OpenAI’s GPT-4 and other models also displayed answers that had less compassion towards Black and Asian people seeking support for mental health problems.]&lt;/p&gt;
&lt;p&gt;That suggests “some patients could receive much less supportive guidance based purely on their perceived race by the model,” said Marzyeh Ghassemi, associate professor at MIT’s Jameel Clinic.&lt;/p&gt;
&lt;p&gt;Similarly, research by the London School of Economics found that Google’s Gemma model, which is used by more than half the local authorities in the UK to support social workers, downplayed women’s physical and mental issues in comparison with men’s when used to generate and summarize case notes.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ghassemi’s MIT team found that patients whose messages contained typos, informal language or uncertain phrasing were between 7-9 percent more likely to be advised against seeking medical care by AI models used in a medical setting, against those with perfectly formatted communications, even when the clinical content was the same.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;This could result in people who don’t speak English as a first language or are not comfortable in using technology being unfairly treated.&lt;/p&gt;
&lt;p&gt;The problem of harmful biases stems partly from the data used to train LLMs. General-purpose models, such as GPT-4, Llama, and Gemini, are trained using data from the internet, and the biases from those sources are therefore reflected in the responses. AI developers can also influence how this creeps into systems by adding safeguards after the model has been trained.&lt;/p&gt;
&lt;p&gt;“If you’re in any situation where there’s a chance that a Reddit subforum is advising your health decisions, I don’t think that that’s a safe place to be,” said Travis Zack, adjunct professor of University of California, San Francisco, and the chief medical officer of AI medical information start-up Open Evidence.&lt;/p&gt;
&lt;p&gt;In a study last year, Zack and his team found that GPT-4 did not take into account the demographic diversity of medical conditions, and tended to stereotype certain races, ethnicities, and genders.&lt;/p&gt;
&lt;p&gt;Researchers warned that AI tools can reinforce patterns of under-treatment that already exist in the healthcare sector, as data in health research is often heavily skewed towards men, and women’s health issues, for example, face chronic underfunding and research.&lt;/p&gt;
&lt;p&gt;OpenAI said many studies evaluated an older model of GPT-4, and the company had improved accuracy since its launch. It had teams working on reducing harmful or misleading outputs, with a particular focus on health. The company said it also worked with external clinicians and researchers to evaluate its models, stress test their behavior, and identify risks.&lt;/p&gt;
&lt;p&gt;The group has also developed a benchmark together with physicians to assess LLM capabilities in health, which takes into account user queries of varying styles, levels of relevance, and detail.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google said it took model bias “extremely seriously” and was developing privacy techniques that can sanitise sensitive datasets and develop safeguards against bias and discrimination.&lt;/p&gt;
&lt;p&gt;Researchers have suggested that one way to reduce medical bias in AI is to identify what data sets should not be used for training in the first place and then train on diverse and more representative health data sets.&lt;/p&gt;
&lt;p&gt;Zack said Open Evidence, which is used by 400,000 doctors in the US to summarize patient histories and retrieve information, trained its models on medical journals, the US Food and Drug Administration’s labels, health guidelines, and expert reviews. Every AI output is also backed up with a citation to a source.&lt;/p&gt;
&lt;p&gt;Earlier this year, researchers at University College London and King’s College London partnered with the UK’s NHS to build a generative AI model, called Foresight.&lt;/p&gt;
&lt;p&gt;The model was trained on anonymized patient data from 57 million people on medical events such as hospital admissions and COVID-19 vaccinations. Foresight was designed to predict probable health outcomes, such as hospitalization or heart attacks.&lt;/p&gt;
&lt;p&gt;“Working with national-scale data allows us to represent the full kind of kaleidoscopic state of England in terms of demographics and diseases,” said Chris Tomlinson, honorary senior research fellow at UCL, who is the lead researcher of the Foresight team. Although not perfect, Tomlinson said it offered a better start than more general datasets.&lt;/p&gt;
&lt;p&gt;European scientists have also trained an AI model called Delphi-2M that predicts susceptibility to diseases decades into the future, based on anonymized medical records from 400,000 participants in UK Biobank.&lt;/p&gt;
&lt;p&gt;But with real patient data of this scale, privacy often becomes an issue. The NHS Foresight project was paused in June to allow the UK’s Information Commissioner’s Office to consider a data protection complaint, filed by the British Medical Association and Royal College of General Practitioners, over its use of sensitive health data in the model’s training.&lt;/p&gt;
&lt;p&gt;In addition, experts have warned that AI systems often “hallucinate”—or make up answers—which could be particularly harmful in a medical context.&lt;/p&gt;
&lt;p&gt;But MIT’s Ghassemi said AI was bringing huge benefits to healthcare. “My hope is that we will start to refocus models in health on addressing crucial health gaps, not adding an extra percent to task performance that the doctors are honestly pretty good at anyway.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Bias-reflecting LLMs lead to inferior medical advice for female, Black, and Asian patients.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Montage of AI logs and medical professional holding smartphone" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/ai-medicalllm-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Montage of AI logs and medical professional holding smartphone" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/ai-medicalllm-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Many hospitals and doctors globally are using LLMs such as Gemini and ChatGPT.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          FT montage/Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Artificial intelligence tools used by doctors risk leading to worse health outcomes for women and ethnic minorities, as a growing body of research shows that many large language models downplay the symptoms of these patients.&lt;/p&gt;
&lt;p&gt;A series of recent studies have found that the uptake of AI models across the healthcare sector could lead to biased medical decisions, reinforcing patterns of under-treatment that already exist across different groups in Western societies.&lt;/p&gt;
&lt;p&gt;The findings by researchers at leading US and UK universities suggest that medical AI tools powered by LLMs have a tendency to not reflect the severity of symptoms among female patients, while also displaying less “empathy” toward Black and Asian ones.&lt;/p&gt;
&lt;p&gt;The warnings come as the world’s top AI groups such as Microsoft, Amazon, OpenAI, and Google rush to develop products that aim to reduce physicians’ workloads and speed up treatment, all in an effort to help overstretched health systems around the world.&lt;/p&gt;
&lt;p&gt;Many hospitals and doctors globally are using LLMs such as Gemini and ChatGPT as well as AI medical note-taking apps from start-ups including Nabla and Heidi to auto-generate transcripts of patient visits, highlight medically relevant details, and create clinical summaries.&lt;/p&gt;
&lt;p&gt;In June, Microsoft revealed it had built an AI-powered medical tool it claims is four times more successful than human doctors at diagnosing complex ailments.&lt;/p&gt;
&lt;p&gt;But research by the MIT’s Jameel Clinic in June found that AI models, such as OpenAI’s GPT-4, Meta’s Llama 3, and Palmyra-Med—a healthcare-focused LLM—recommended a much lower level of care for female patients, and suggested some patients self-treat at home instead of seeking help.&lt;/p&gt;
&lt;p&gt;A separate study by the MIT team showed that OpenAI’s GPT-4 and other models also displayed answers that had less compassion towards Black and Asian people seeking support for mental health problems.]&lt;/p&gt;
&lt;p&gt;That suggests “some patients could receive much less supportive guidance based purely on their perceived race by the model,” said Marzyeh Ghassemi, associate professor at MIT’s Jameel Clinic.&lt;/p&gt;
&lt;p&gt;Similarly, research by the London School of Economics found that Google’s Gemma model, which is used by more than half the local authorities in the UK to support social workers, downplayed women’s physical and mental issues in comparison with men’s when used to generate and summarize case notes.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ghassemi’s MIT team found that patients whose messages contained typos, informal language or uncertain phrasing were between 7-9 percent more likely to be advised against seeking medical care by AI models used in a medical setting, against those with perfectly formatted communications, even when the clinical content was the same.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;This could result in people who don’t speak English as a first language or are not comfortable in using technology being unfairly treated.&lt;/p&gt;
&lt;p&gt;The problem of harmful biases stems partly from the data used to train LLMs. General-purpose models, such as GPT-4, Llama, and Gemini, are trained using data from the internet, and the biases from those sources are therefore reflected in the responses. AI developers can also influence how this creeps into systems by adding safeguards after the model has been trained.&lt;/p&gt;
&lt;p&gt;“If you’re in any situation where there’s a chance that a Reddit subforum is advising your health decisions, I don’t think that that’s a safe place to be,” said Travis Zack, adjunct professor of University of California, San Francisco, and the chief medical officer of AI medical information start-up Open Evidence.&lt;/p&gt;
&lt;p&gt;In a study last year, Zack and his team found that GPT-4 did not take into account the demographic diversity of medical conditions, and tended to stereotype certain races, ethnicities, and genders.&lt;/p&gt;
&lt;p&gt;Researchers warned that AI tools can reinforce patterns of under-treatment that already exist in the healthcare sector, as data in health research is often heavily skewed towards men, and women’s health issues, for example, face chronic underfunding and research.&lt;/p&gt;
&lt;p&gt;OpenAI said many studies evaluated an older model of GPT-4, and the company had improved accuracy since its launch. It had teams working on reducing harmful or misleading outputs, with a particular focus on health. The company said it also worked with external clinicians and researchers to evaluate its models, stress test their behavior, and identify risks.&lt;/p&gt;
&lt;p&gt;The group has also developed a benchmark together with physicians to assess LLM capabilities in health, which takes into account user queries of varying styles, levels of relevance, and detail.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Google said it took model bias “extremely seriously” and was developing privacy techniques that can sanitise sensitive datasets and develop safeguards against bias and discrimination.&lt;/p&gt;
&lt;p&gt;Researchers have suggested that one way to reduce medical bias in AI is to identify what data sets should not be used for training in the first place and then train on diverse and more representative health data sets.&lt;/p&gt;
&lt;p&gt;Zack said Open Evidence, which is used by 400,000 doctors in the US to summarize patient histories and retrieve information, trained its models on medical journals, the US Food and Drug Administration’s labels, health guidelines, and expert reviews. Every AI output is also backed up with a citation to a source.&lt;/p&gt;
&lt;p&gt;Earlier this year, researchers at University College London and King’s College London partnered with the UK’s NHS to build a generative AI model, called Foresight.&lt;/p&gt;
&lt;p&gt;The model was trained on anonymized patient data from 57 million people on medical events such as hospital admissions and COVID-19 vaccinations. Foresight was designed to predict probable health outcomes, such as hospitalization or heart attacks.&lt;/p&gt;
&lt;p&gt;“Working with national-scale data allows us to represent the full kind of kaleidoscopic state of England in terms of demographics and diseases,” said Chris Tomlinson, honorary senior research fellow at UCL, who is the lead researcher of the Foresight team. Although not perfect, Tomlinson said it offered a better start than more general datasets.&lt;/p&gt;
&lt;p&gt;European scientists have also trained an AI model called Delphi-2M that predicts susceptibility to diseases decades into the future, based on anonymized medical records from 400,000 participants in UK Biobank.&lt;/p&gt;
&lt;p&gt;But with real patient data of this scale, privacy often becomes an issue. The NHS Foresight project was paused in June to allow the UK’s Information Commissioner’s Office to consider a data protection complaint, filed by the British Medical Association and Royal College of General Practitioners, over its use of sensitive health data in the model’s training.&lt;/p&gt;
&lt;p&gt;In addition, experts have warned that AI systems often “hallucinate”—or make up answers—which could be particularly harmful in a medical context.&lt;/p&gt;
&lt;p&gt;But MIT’s Ghassemi said AI was bringing huge benefits to healthcare. “My hope is that we will start to refocus models in health on addressing crucial health gaps, not adding an extra percent to task performance that the doctors are honestly pretty good at anyway.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2025 The Financial Times Ltd. All rights reserved Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/health/2025/09/ai-medical-tools-found-to-downplay-symptoms-of-women-ethnic-minorities/</guid><pubDate>Fri, 19 Sep 2025 13:30:27 +0000</pubDate></item><item><title>One week left: Lock in discounted pricing for TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/one-week-left-lock-in-discounted-pricing-for-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It’s official! We are in the final week to lock in your &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; pass and save up to $668. If you have been on the fence about joining one of the biggest tech gatherings of the year, where we will also celebrate 20 years of TechCrunch, now is the time to commit before prices rise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register here&lt;/strong&gt; to secure your discount before time runs out. Prices jump on September 26 at 11:59 p.m. PT.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 1 week" class="wp-image-3047653" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_1Week-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-top-voices-sharing-key-takeaways"&gt;Top voices sharing key takeaways&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;We’re bringing the biggest names in tech to San Francisco’s Moscone West this October 27–29. Who’s taking the stage? You’ll hear from today’s tech leaders:&lt;/p&gt;







&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Max Altschuler&lt;/strong&gt; — Founder, GTMfund&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Rajat Bhageria&lt;/strong&gt; — CEO, Chef Robotics&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Ann Bordetsky&lt;/strong&gt; — Partner, NEA&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Roelof Botha&lt;/strong&gt; — Partner, Sequoia Capital&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Chris Britt &lt;/strong&gt;— CEO and co-founder, Chime&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;David Cramer&lt;/strong&gt; — Co-founder and CPO, Sentry&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jai Das&lt;/strong&gt; — President and partner, Sapphire Ventures&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Elad Gil&lt;/strong&gt; — Early investor in Airbnb, Stripe, Notion, and Perplexity&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;David George&lt;/strong&gt; — General partner, a16z&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Aaron Levie&lt;/strong&gt; — CEO and Co-founder, Box&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Tekedra Mawakana&lt;/strong&gt; — Co-CEO, Waymo&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Anton Osika&lt;/strong&gt; — Co-founder and CEO, Lovable&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Even Rogers&lt;/strong&gt; — Co-founder and CEO, True Anomaly&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Katie Stanton&lt;/strong&gt; — General partner, Moxxie Ventures&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Astro Teller&lt;/strong&gt; — Captain of Moonshots, X, the Moonshot Factory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Thomas Wolf&lt;/strong&gt; — Co-founder and CSO, Hugging Face&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Sangeen Zeb&lt;/strong&gt; — General partner, GV&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Plus dozens &lt;strong&gt;more leaders&lt;/strong&gt; from Meta, Google Cloud, Pinterest, GitHub, GTMfund, Index Ventures, and beyond. Across &lt;strong&gt;250+ sessions&lt;/strong&gt; on industry stages, roundtables, and breakouts will be packed with startup energy, deal flow, and the industry’s top movers.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Anton Osika" class="wp-image-3041170" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Anton-Osika-e1756746220840.jpg?w=680" width="453" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechBBQ&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-connections-to-fuel-your-next-big-step"&gt;Connections to fuel your next big step&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt 2025 is where the next wave of ideas, products, and partnerships begins. It’s your chance to join a community of builders, operators, investors, and tech visionaries. Beyond big insights from the top voices in tech, you’ll leave with powerful connections to fuel your next stage of growth — whatever that looks like to you. From interactive roundtables and live Q&amp;amp;A sessions to curated 1:1 and small-group networking, Disrupt is where you turn your vision into reality.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Braindate networking" class="wp-image-2953563" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-braindate-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-register-by-september-26-to-save"&gt;Register by September 26 to save&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt; — your last chance to secure your discount ends in just one week. These low rates end on September 26 at 11:59 p.m. PT.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;It’s official! We are in the final week to lock in your &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; pass and save up to $668. If you have been on the fence about joining one of the biggest tech gatherings of the year, where we will also celebrate 20 years of TechCrunch, now is the time to commit before prices rise.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register here&lt;/strong&gt; to secure your discount before time runs out. Prices jump on September 26 at 11:59 p.m. PT.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 1 week" class="wp-image-3047653" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/TC25_1Week-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-top-voices-sharing-key-takeaways"&gt;Top voices sharing key takeaways&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;We’re bringing the biggest names in tech to San Francisco’s Moscone West this October 27–29. Who’s taking the stage? You’ll hear from today’s tech leaders:&lt;/p&gt;







&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Max Altschuler&lt;/strong&gt; — Founder, GTMfund&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Rajat Bhageria&lt;/strong&gt; — CEO, Chef Robotics&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Ann Bordetsky&lt;/strong&gt; — Partner, NEA&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Roelof Botha&lt;/strong&gt; — Partner, Sequoia Capital&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Chris Britt &lt;/strong&gt;— CEO and co-founder, Chime&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;David Cramer&lt;/strong&gt; — Co-founder and CPO, Sentry&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Jai Das&lt;/strong&gt; — President and partner, Sapphire Ventures&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Elad Gil&lt;/strong&gt; — Early investor in Airbnb, Stripe, Notion, and Perplexity&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;David George&lt;/strong&gt; — General partner, a16z&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Aaron Levie&lt;/strong&gt; — CEO and Co-founder, Box&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Tekedra Mawakana&lt;/strong&gt; — Co-CEO, Waymo&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Anton Osika&lt;/strong&gt; — Co-founder and CEO, Lovable&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Even Rogers&lt;/strong&gt; — Co-founder and CEO, True Anomaly&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Katie Stanton&lt;/strong&gt; — General partner, Moxxie Ventures&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Astro Teller&lt;/strong&gt; — Captain of Moonshots, X, the Moonshot Factory&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Thomas Wolf&lt;/strong&gt; — Co-founder and CSO, Hugging Face&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Sangeen Zeb&lt;/strong&gt; — General partner, GV&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Plus dozens &lt;strong&gt;more leaders&lt;/strong&gt; from Meta, Google Cloud, Pinterest, GitHub, GTMfund, Index Ventures, and beyond. Across &lt;strong&gt;250+ sessions&lt;/strong&gt; on industry stages, roundtables, and breakouts will be packed with startup energy, deal flow, and the industry’s top movers.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Anton Osika" class="wp-image-3041170" height="680" src="https://techcrunch.com/wp-content/uploads/2025/08/Anton-Osika-e1756746220840.jpg?w=680" width="453" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechBBQ&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-connections-to-fuel-your-next-big-step"&gt;Connections to fuel your next big step&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt 2025 is where the next wave of ideas, products, and partnerships begins. It’s your chance to join a community of builders, operators, investors, and tech visionaries. Beyond big insights from the top voices in tech, you’ll leave with powerful connections to fuel your next stage of growth — whatever that looks like to you. From interactive roundtables and live Q&amp;amp;A sessions to curated 1:1 and small-group networking, Disrupt is where you turn your vision into reality.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Braindate networking" class="wp-image-2953563" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-braindate-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-register-by-september-26-to-save"&gt;Register by September 26 to save&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Register now&lt;/strong&gt; — your last chance to secure your discount ends in just one week. These low rates end on September 26 at 11:59 p.m. PT.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/one-week-left-lock-in-discounted-pricing-for-techcrunch-disrupt-2025/</guid><pubDate>Fri, 19 Sep 2025 14:00:00 +0000</pubDate></item><item><title>TechEx Europe 2025: Practical learnings for AI leaders (AI News)</title><link>https://www.artificialintelligence-news.com/news/techex-europe-2025-practical-learnings-for-ai-leaders/</link><description>&lt;p&gt;On 24–25 September 2025, the RAI in Amsterdam will host TechEx Europe, an event that brings together 8,000+ participants and 250+ speakers in five co-located events: AI &amp;amp; Big Data Expo, Cyber Security &amp;amp; Cloud Expo, IoT Tech Expo, Digital Transformation Week, and Data Centre Expo.&lt;/p&gt;&lt;p&gt;The event offers opportunities to engage with peer executives and practitioners involved in AI operations and agentic AI systems. You’ll be able to benchmark individual approaches and explore the infrastructures and processes needed to deploy AI at scale.&lt;/p&gt;&lt;h2 class="wp-block-heading" id="orgca84761"&gt;AI operations on the agenda&lt;/h2&gt;&lt;p&gt;AI is moving rapidly from pilot projects to enterprise-wide deployment. Agentic AI is asking questions about governance, trust, and monitoring, and AI workloads reshape infrastructure requirements and business models. TechEx Europe places AI at the centre of wider discussions about cloud, data, IoT, and digital transformation, giving AI operations professionals a chance to situate their challenges amid the broader technology ecosystem.&lt;/p&gt;&lt;h2 class="wp-block-heading" id="orgbd55fab"&gt;Sessions and speakers to watch&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;AI &amp;amp; Big Data Expo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This track is the primary track for AI professionals, with dedicated sessions on operationalising AI, AI in the enterprise, and exploration of issues around governance and ethics.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Maxim Romanovsky (Deutsche Bank) will discuss how financial institutions approach AI operations, emphasising regulatory compliance and trust.&lt;/li&gt;&lt;li&gt;John Hearty (Mastercard) will cover real-world deployments and scaling lessons.&lt;/li&gt;&lt;li&gt;Alexander Gee (Reddit) and Altaf Patel (PepsiCo) will highlight sector-specific challenges, from content and moderation to supply chain optimisation.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Data Centre Expo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AI systems demand significant compute and low-latency access to sane datasets. Vladimir Prodanovic (NVIDIA) and Simon Goldthorpe (Equinix) will share their perspectives on infrastructure optimisation for AI workloads.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cyber Security &amp;amp; Cloud Expo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AI raises fresh security concerns. Sessions led by Andrew Byrd (NATO) and Amir Vashkover (Philips) will look at monitoring and securing environments where autonomous systems operate.&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-practical-takeaways-for-ai-operations"&gt;Practical takeaways for AI operations&lt;/h2&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Scaling responsibly: Evidence of how organisations move beyond prototypes into production environments.&lt;/li&gt;&lt;li&gt;Monitoring &amp;amp; governance: Frameworks for ensuring agentic AI aligns with ethical and legal standards.&lt;/li&gt;&lt;li&gt;Infrastructure readiness: Guidance from infrastructure providers on how to plan for AI’s compute, networking, and storage demands.&lt;/li&gt;&lt;li&gt;Cross-disciplinary learnings: Exposure to security, IoT, and transformation tracks will show AI’s integration with enterprise ecosystems, or exist in isolation.&lt;/li&gt;&lt;/ul&gt;&lt;h2 class="wp-block-heading" id="org2e33fd3"&gt;Approaching the event&lt;/h2&gt;&lt;p&gt;For two days, attendees will have multiple opportunities, including the ability to:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Focus&lt;/strong&gt; on operational themes in sessions on deployment and monitoring, plus purely conceptual talks.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Engage&lt;/strong&gt; with peers and experts in conversation with other AI professionals.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Explore&lt;/strong&gt; partnerships with vendor and data centre-focused discussions on cost and capacity planning for AI projects.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Frame questions&lt;/strong&gt; around autonomy and ethics at panel discussions. How are others managing agentic AI decision-making, transparency, and oversight?&lt;/p&gt;&lt;h2 class="wp-block-heading" id="org87848af"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;For those leading AI operations or exploring agentic AI, TechEx Europe 2025 offers exposure to the new in an environment where attendees can address the operational realities of deploying the technology. The event will provide a multi-sector view of how enterprises move from experimental AI, to trusted, scalable and autonomous systems. What infrastructures, safeguards, and leadership approaches are required along the way? You’ll find out!&lt;/p&gt;&lt;p&gt;Find out more and register your attendance here.&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;On 24–25 September 2025, the RAI in Amsterdam will host TechEx Europe, an event that brings together 8,000+ participants and 250+ speakers in five co-located events: AI &amp;amp; Big Data Expo, Cyber Security &amp;amp; Cloud Expo, IoT Tech Expo, Digital Transformation Week, and Data Centre Expo.&lt;/p&gt;&lt;p&gt;The event offers opportunities to engage with peer executives and practitioners involved in AI operations and agentic AI systems. You’ll be able to benchmark individual approaches and explore the infrastructures and processes needed to deploy AI at scale.&lt;/p&gt;&lt;h2 class="wp-block-heading" id="orgca84761"&gt;AI operations on the agenda&lt;/h2&gt;&lt;p&gt;AI is moving rapidly from pilot projects to enterprise-wide deployment. Agentic AI is asking questions about governance, trust, and monitoring, and AI workloads reshape infrastructure requirements and business models. TechEx Europe places AI at the centre of wider discussions about cloud, data, IoT, and digital transformation, giving AI operations professionals a chance to situate their challenges amid the broader technology ecosystem.&lt;/p&gt;&lt;h2 class="wp-block-heading" id="orgbd55fab"&gt;Sessions and speakers to watch&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;AI &amp;amp; Big Data Expo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;This track is the primary track for AI professionals, with dedicated sessions on operationalising AI, AI in the enterprise, and exploration of issues around governance and ethics.&lt;/p&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Maxim Romanovsky (Deutsche Bank) will discuss how financial institutions approach AI operations, emphasising regulatory compliance and trust.&lt;/li&gt;&lt;li&gt;John Hearty (Mastercard) will cover real-world deployments and scaling lessons.&lt;/li&gt;&lt;li&gt;Alexander Gee (Reddit) and Altaf Patel (PepsiCo) will highlight sector-specific challenges, from content and moderation to supply chain optimisation.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Data Centre Expo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AI systems demand significant compute and low-latency access to sane datasets. Vladimir Prodanovic (NVIDIA) and Simon Goldthorpe (Equinix) will share their perspectives on infrastructure optimisation for AI workloads.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cyber Security &amp;amp; Cloud Expo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;AI raises fresh security concerns. Sessions led by Andrew Byrd (NATO) and Amir Vashkover (Philips) will look at monitoring and securing environments where autonomous systems operate.&lt;/p&gt;&lt;h2 class="wp-block-heading" id="h-practical-takeaways-for-ai-operations"&gt;Practical takeaways for AI operations&lt;/h2&gt;&lt;ul class="wp-block-list"&gt;&lt;li&gt;Scaling responsibly: Evidence of how organisations move beyond prototypes into production environments.&lt;/li&gt;&lt;li&gt;Monitoring &amp;amp; governance: Frameworks for ensuring agentic AI aligns with ethical and legal standards.&lt;/li&gt;&lt;li&gt;Infrastructure readiness: Guidance from infrastructure providers on how to plan for AI’s compute, networking, and storage demands.&lt;/li&gt;&lt;li&gt;Cross-disciplinary learnings: Exposure to security, IoT, and transformation tracks will show AI’s integration with enterprise ecosystems, or exist in isolation.&lt;/li&gt;&lt;/ul&gt;&lt;h2 class="wp-block-heading" id="org2e33fd3"&gt;Approaching the event&lt;/h2&gt;&lt;p&gt;For two days, attendees will have multiple opportunities, including the ability to:&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Focus&lt;/strong&gt; on operational themes in sessions on deployment and monitoring, plus purely conceptual talks.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Engage&lt;/strong&gt; with peers and experts in conversation with other AI professionals.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Explore&lt;/strong&gt; partnerships with vendor and data centre-focused discussions on cost and capacity planning for AI projects.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Frame questions&lt;/strong&gt; around autonomy and ethics at panel discussions. How are others managing agentic AI decision-making, transparency, and oversight?&lt;/p&gt;&lt;h2 class="wp-block-heading" id="org87848af"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;For those leading AI operations or exploring agentic AI, TechEx Europe 2025 offers exposure to the new in an environment where attendees can address the operational realities of deploying the technology. The event will provide a multi-sector view of how enterprises move from experimental AI, to trusted, scalable and autonomous systems. What infrastructures, safeguards, and leadership approaches are required along the way? You’ll find out!&lt;/p&gt;&lt;p&gt;Find out more and register your attendance here.&lt;/p&gt;&lt;img src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/08/ai-expo-banner-2025.png" /&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and co-located with other leading technology events. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/techex-europe-2025-practical-learnings-for-ai-leaders/</guid><pubDate>Fri, 19 Sep 2025 14:15:24 +0000</pubDate></item><item><title>How developers are using Apple’s local AI models with iOS 26 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/how-developers-are-using-apples-local-ai-models-with-ios-26/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Earlier this year, Apple introduced its Foundation Models framework during WWDC 2025, which allows developers to use the company’s local AI models to power features in their applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company touted that with this framework, developers gain access to AI models without worrying about any inference cost. Plus, these local models have capabilities such as guided generation and tool calling built in.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As iOS 26 is rolling out to all users, developers have been updating their apps to include features powered by Apple’s local AI models. Apple’s models are small compared with leading models from OpenAI, Anthropic, Google, or Meta. That is why local-only features largely improve quality of life with these apps rather than introducing major changes to the app’s workflow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below are some of the first apps to tap into Apple’s AI framework.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lil-artist"&gt;Lil Artist&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Lil Artist app offers various interactive experiences to help kids learn different skills like creativity, math, and music. Developer Arima Jain shipped an AI story creator with the iOS 26 update. This allows users to select a character and a theme, with the app generating a story using AI. The developer said that the text generation in the story is powered by the local model.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047762" height="451" src="https://techcrunch.com/wp-content/uploads/2025/09/screenshot_3x_postspark_2025-09-01_01-19-08.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Lil Artist&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-daylish"&gt;Daylish&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The developer of the Daylish app is working on a prototype for automatically suggesting emojis for timeline events based on the title for the daily planner app.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-moneycoach"&gt;MoneyCoach&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Finance tracking app MoneyCoach has two neat features powered by local models. First, the app shows insights about your spending, such as whether you spent more than average on groceries for that particular week. The other feature automatically suggests categories and subcategories for a spending item for quick entries.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="A screenshot of financial tracking app MoneyCoach where the screen shows summary of accounts and shows a weekly insight on grocery spend for the week. " class="wp-image-3047763" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/01-Foundation-Models-Insights.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MoneyCoach&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-lookup"&gt;LookUp&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The word learning app LookUp has added two new modes using Apple’s AI models. There is a new learning mode, which leverages a local model to create examples corresponding to a word. Plus, the example asks users to explain the usage of the word in a sentence.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A screenshot of LookUp word learning app that shows a screen that displays an example to understand a new word " class="wp-image-3047764" height="400" src="https://techcrunch.com/wp-content/uploads/2025/09/Learn-Modes-2.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LookUp&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The developer is also using on-device models to generate a map view of a word’s origin. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="A screenshot of the Lookup app with a map view showing origin of a word." class="wp-image-3047765" height="400" src="https://techcrunch.com/wp-content/uploads/2025/09/Etymology-Maps-2.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Lookup&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-tasks"&gt;Tasks&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Just like a few other apps, the Tasks app implemented a feature to suggest tags for an entry using local models automatically. It’s also using these models to detect a recurring task and schedule it accordingly. And the app lets users speak a few things and use the local model to break them down into various tasks without using the internet.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="This is a lifestyle shot of Tasks app showing screen with a new feautre that uses local models to suggest tags when you enter a task." class="wp-image-3047766" height="407" src="https://techcrunch.com/wp-content/uploads/2025/09/Tasks-app.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tasks&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-day-one"&gt;Day One&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Automattic-owned journaling app Day One is using Apple’s models to get highlights and suggest titles for your entry. The team has also implemented a feature to generate prompts that nudge you to dive deeper and write more based on what you have already written.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A screenshot showing Day One journaling app's features implemented using Apple's local model. These features include a summary, title suggestion, and prompt generation to go deeper into writing. " class="wp-image-3047759" height="625" src="https://techcrunch.com/wp-content/uploads/2025/09/On-Device-AI-scaled-1.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Day One&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-crouton"&gt;Crouton&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Recipe app Crouton is using Apple Intelligence to suggest tags for a recipe and assign names to timers. It also uses AI to break down a block of text into easy-to-follow steps for cooking.&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-signeasy"&gt;SignEasy&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Digital signing app SignEasy is using Apple’s local models to extract key insights from a contract and give users a summary of the document they are signing.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We will continue updating this list as we discover more apps using Apple’s local models.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Earlier this year, Apple introduced its Foundation Models framework during WWDC 2025, which allows developers to use the company’s local AI models to power features in their applications.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company touted that with this framework, developers gain access to AI models without worrying about any inference cost. Plus, these local models have capabilities such as guided generation and tool calling built in.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As iOS 26 is rolling out to all users, developers have been updating their apps to include features powered by Apple’s local AI models. Apple’s models are small compared with leading models from OpenAI, Anthropic, Google, or Meta. That is why local-only features largely improve quality of life with these apps rather than introducing major changes to the app’s workflow.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Below are some of the first apps to tap into Apple’s AI framework.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-lil-artist"&gt;Lil Artist&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The Lil Artist app offers various interactive experiences to help kids learn different skills like creativity, math, and music. Developer Arima Jain shipped an AI story creator with the iOS 26 update. This allows users to select a character and a theme, with the app generating a story using AI. The developer said that the text generation in the story is powered by the local model.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047762" height="451" src="https://techcrunch.com/wp-content/uploads/2025/09/screenshot_3x_postspark_2025-09-01_01-19-08.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Lil Artist&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-daylish"&gt;Daylish&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The developer of the Daylish app is working on a prototype for automatically suggesting emojis for timeline events based on the title for the daily planner app.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-moneycoach"&gt;MoneyCoach&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Finance tracking app MoneyCoach has two neat features powered by local models. First, the app shows insights about your spending, such as whether you spent more than average on groceries for that particular week. The other feature automatically suggests categories and subcategories for a spending item for quick entries.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="A screenshot of financial tracking app MoneyCoach where the screen shows summary of accounts and shows a weekly insight on grocery spend for the week. " class="wp-image-3047763" height="680" src="https://techcrunch.com/wp-content/uploads/2025/09/01-Foundation-Models-Insights.jpeg?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;MoneyCoach&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-lookup"&gt;LookUp&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;The word learning app LookUp has added two new modes using Apple’s AI models. There is a new learning mode, which leverages a local model to create examples corresponding to a word. Plus, the example asks users to explain the usage of the word in a sentence.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A screenshot of LookUp word learning app that shows a screen that displays an example to understand a new word " class="wp-image-3047764" height="400" src="https://techcrunch.com/wp-content/uploads/2025/09/Learn-Modes-2.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;LookUp&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The developer is also using on-device models to generate a map view of a word’s origin. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large is-resized"&gt;&lt;img alt="A screenshot of the Lookup app with a map view showing origin of a word." class="wp-image-3047765" height="400" src="https://techcrunch.com/wp-content/uploads/2025/09/Etymology-Maps-2.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Lookup&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-tasks"&gt;Tasks&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Just like a few other apps, the Tasks app implemented a feature to suggest tags for an entry using local models automatically. It’s also using these models to detect a recurring task and schedule it accordingly. And the app lets users speak a few things and use the local model to break them down into various tasks without using the internet.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="This is a lifestyle shot of Tasks app showing screen with a new feautre that uses local models to suggest tags when you enter a task." class="wp-image-3047766" height="407" src="https://techcrunch.com/wp-content/uploads/2025/09/Tasks-app.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Tasks&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-day-one"&gt;Day One&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Automattic-owned journaling app Day One is using Apple’s models to get highlights and suggest titles for your entry. The team has also implemented a feature to generate prompts that nudge you to dive deeper and write more based on what you have already written.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A screenshot showing Day One journaling app's features implemented using Apple's local model. These features include a summary, title suggestion, and prompt generation to go deeper into writing. " class="wp-image-3047759" height="625" src="https://techcrunch.com/wp-content/uploads/2025/09/On-Device-AI-scaled-1.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Day One&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-crouton"&gt;Crouton&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Recipe app Crouton is using Apple Intelligence to suggest tags for a recipe and assign names to timers. It also uses AI to break down a block of text into easy-to-follow steps for cooking.&lt;/p&gt;

&lt;figure class="wp-block-embed aligncenter is-type-rich is-provider-twitter wp-block-embed-twitter"&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-signeasy"&gt;SignEasy&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Digital signing app SignEasy is using Apple’s local models to extract key insights from a contract and give users a summary of the document they are signing.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;We will continue updating this list as we discover more apps using Apple’s local models.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/how-developers-are-using-apples-local-ai-models-with-ios-26/</guid><pubDate>Fri, 19 Sep 2025 14:21:29 +0000</pubDate></item><item><title>Octopus Energy spins off its Kraken utility billing and AI platform (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/octopus-energy-spins-off-its-kraken-utility-billing-and-ai-platform/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/critical-infrastructure.jpeg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;British renewable energy provider Octopus Energy said this week that it’s spinning off Kraken, its tech platform for utilities, spurred in part by $500 million in committed annual revenue from other utilities and energy providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An eventual Kraken IPO could be valued at $15 billion and could occur within a year, according to The Wall Street Journal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Kraken was actually the company’s initial product, per Octopus CEO Greg Jackson. “We created Octopus as the ‘demo client,’” he told the Journal earlier this year. That demo client has since grown to provide power to more than 7.7 million households in the U.K. and another 2.8 million elsewhere.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The spinoff would help Kraken minimize conflicts of interest as it signs deals with utilities and power providers that aren’t Octopus, the company said. Octopus originally set the spinoff in motion last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Octopus was founded in 2015, and within a decade became the U.K.’s largest energy provider, overtaking British Gas, which was founded more than 200 years ago.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of that growth has stemmed from creative marketing and customer-acquisition strategies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One, called Zero Bills, allowed homeowners to eliminate their energy bills for a decade if they bought properties that were entirely electrified. Another, a tariff it calls Agile, encourages customers to shift their electricity use to times when there’s a surplus on the grid. In some cases, consumers can run loads of laundry for free.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As Octopus gathered data from those projects, it used AI models built into Kraken to sift through it to determine how increasing amounts of renewable energy can work on the grid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For utilities and power providers, Kraken allows them to call on power sources when needed, including renewables, solar, and so-called distributed energy resources, which include EV chargers, smart thermostats, and home batteries. It has also built a customer management system that covers everything from billing, meter management, and customer relations.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/critical-infrastructure.jpeg?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;British renewable energy provider Octopus Energy said this week that it’s spinning off Kraken, its tech platform for utilities, spurred in part by $500 million in committed annual revenue from other utilities and energy providers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;An eventual Kraken IPO could be valued at $15 billion and could occur within a year, according to The Wall Street Journal.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Kraken was actually the company’s initial product, per Octopus CEO Greg Jackson. “We created Octopus as the ‘demo client,’” he told the Journal earlier this year. That demo client has since grown to provide power to more than 7.7 million households in the U.K. and another 2.8 million elsewhere.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The spinoff would help Kraken minimize conflicts of interest as it signs deals with utilities and power providers that aren’t Octopus, the company said. Octopus originally set the spinoff in motion last year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Octopus was founded in 2015, and within a decade became the U.K.’s largest energy provider, overtaking British Gas, which was founded more than 200 years ago.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Part of that growth has stemmed from creative marketing and customer-acquisition strategies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One, called Zero Bills, allowed homeowners to eliminate their energy bills for a decade if they bought properties that were entirely electrified. Another, a tariff it calls Agile, encourages customers to shift their electricity use to times when there’s a surplus on the grid. In some cases, consumers can run loads of laundry for free.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;As Octopus gathered data from those projects, it used AI models built into Kraken to sift through it to determine how increasing amounts of renewable energy can work on the grid.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For utilities and power providers, Kraken allows them to call on power sources when needed, including renewables, solar, and so-called distributed energy resources, which include EV chargers, smart thermostats, and home batteries. It has also built a customer management system that covers everything from billing, meter management, and customer relations.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/octopus-energy-spins-off-its-kraken-utility-billing-and-ai-platform/</guid><pubDate>Fri, 19 Sep 2025 14:28:03 +0000</pubDate></item><item><title>Meta Ray-Ban Display and everything else unveiled at Meta Connect 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/meta-connect-2025-what-to-expect-and-how-to-watch/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At Meta Connect 2025, the company’s biggest event of the year, Mark Zuckerberg unveiled three new smart glasses: the second-generation Ray-Ban Meta, the Meta Ray-Ban Display and wristband controller, and the Oakley Meta Vanguard.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta says it has sold 2 million of the first-generation Ray-Ban Meta smart glasses, and earlier this year, Meta unveiled its latest AI-powered smart glasses with Oakley, which were designed for athletes. Silicon Valley is leaning heavily into AI wearables, and Meta seems to be one of the companies leading the charge.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;With Meta looking to regain its footing in the AI race and sell more hardware, the company had a lot at stake during Mark Zuckerberg’s Meta Connect 2025 keynote. Overall, Meta showcased some pretty impressive technology — the Meta Neural Band, the wristband controller that comes with the Meta Ray-Ban Display, is a particular highlight.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And yet, in a twist that felt reminiscent of HBO’s “Silicon Valley,” Zuckerberg’s demo of the AI capabilities on the Ray-Ban Metas failed. Whoops!&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While sharing a live video feed of the cooking content creator Jack Mancuso at Meta HQ, Zuckerberg asked the chef to demonstrate how his Ray-Ban Meta glasses could help him whip up a Korean-inspired steak sauce.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047688" height="374" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-18-at-9.15.32PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Jack Mancuso at Meta Connect.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“I love the setup you have here, with soy sauce and other ingredients. How can I help?” asked the chipper Meta AI voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mancuso asked for a recipe for a Korean-inspired steak sauce, and the AI voice began to list the ingredients that he would need — but Mancuso knows he needs to keep the demo succinct, so he interrupts and asks, “What do I do first?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;After a moment of silence that dragged a bit too long, Mancuso repeated, “What do I do first?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You’ve already combined the base ingredients, so now grate a pear to add to the sauce,” the AI said. But he had not yet combined the base ingredients, because he had not started making the recipe, hence the question of what to do first.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mancuso asked the same question again, and the AI gave the same response. The audience laughed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think the Wi-Fi might be messed up — back to you, Mark!” Mancuso said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You know what? It’s all good. The irony of the whole thing is that you spend years making the technology, and then the Wi-Fi of the day kind of&amp;nbsp;… catches you,” Zuckerberg said. “Anyway, we’ll go check out what he made later.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The whole interaction was a bit awkward, especially since the issue did not seem to be with the Wi-Fi. But even when things are going according to plan, these presentations can feel a bit hokey&amp;nbsp;… like the end of the keynote, when Zuckerberg and Diplo quite literally ran into the sunset together, wearing their Meta Oakley Vanguards. It had to be a busy day for Mark, so maybe he just needed an excuse to build some cardio into his schedule.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047719" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Lifestyle-Still-5.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph"&gt;Meta unveiled the second generation of its Ray-Ban Meta glasses, which first debuted in 2023. This spruced-up model features double the battery life of its predecessor, now lasting up to eight hours of mixed use on one charge. The second-generation glasses also support ultra HD 3K video recording, which the company says is twice as sharp as the last model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s smart glasses are also getting some new features with the release of the second-generation Ray-Ban Metas, like conversation focus, which will be available on the Ray-Ban Meta and Oakley Meta HSTN glasses.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047780" height="592" src="https://techcrunch.com/wp-content/uploads/2025/09/04_style-and-color_Carousel-01.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“If you’re eating at a hot new restaurant, commuting on the train, or catching your favorite DJ’s latest set, conversation focus uses your AI glasses’ open-ear speakers to amplify the voice of the person you’re talking to,” Meta said in its press release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Conversation focus isn’t out just yet, so we can’t say for sure if it’ll be any help for your next night out. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Live AI feature — which Meta failed to properly demo onstage — is also on its way. But it’s so energy-intensive that you can only use it for about an hour or two.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we make battery and energy efficiency optimizations, Meta AI will transition from something you prompt with a wake word to an always-available assistant,” the company said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The second-generation Ray-Ban Meta glasses are priced at $379.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047720" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Meta-Ray-Ban-Display-Outdoor-Lifestyle-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Meta Ray-Ban Display smart glasses are the most impressive glasses that Meta has unveiled to date, featuring a built-in display for apps, alerts, and directions on the right lens. But what sets this pair of smart glasses apart is its accompanying wristband controller, the Meta Neural Band.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This wristband lets Meta show off a bit of what it’s been spending so much time (and money) on in its Reality Labs division, which is notorious for losing billions of dollars a quarter.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047148" height="382" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-17-at-8.22.33PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Visually, the Meta Neural Band looks like a Fitbit without a screen. It’s powered by surface electromyography (sEMG), which can pick up on minute hand gestures and small movements. This is far more sophisticated than a wrist gesture on an Apple Watch. Users can write out text messages by holding their fingers together as if they were gripping a pen and “writing” out the text. This means that you can see a WhatsApp message come in on your right glasses lens, then answer it by “writing” your response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the glasses support Meta apps, but the company will have to support a wide variety of apps in the future to get the kind of adoption they’re looking for. Like&amp;nbsp;Apple&amp;nbsp;and&amp;nbsp;Google, Meta is betting that smart glasses could cut into the market share of the smartphone in the future — but it will be a big challenge to force such a massive cultural shift.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Meta Ray-Ban Display, which comes with the Meta Neural Band, will cost $799 and launches on September 30.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;For the bearish among us, it seems hard to imagine wearing smart glasses and sending text messages by handwriting in the air. But the Oakley Meta Vanguard smart glasses, which are designed for athletes, offer the most coherent use case yet for this kind of technology. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047722" height="450" src="https://techcrunch.com/wp-content/uploads/2025/09/Oakley-Meta-Vanguard-Lifestyle15.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Bikers, trail runners, and skiers can photograph their adventures without pulling out their phones; the glasses’ open-air speakers can play music during your workout, and even link with apps like Strava and Garmin to relay your stats. Like the other new glasses, the Vanguard model is also AI-enabled.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Unlike other models of Meta smart glasses, the Oakley Meta Vanguards have just one unified front lens with a camera in the middle, rather than two lenses with cameras on either side — it’s a design that makes more technical sense, and it’s a fashion statement that you can pull off in athletic eyewear, but not in eyeglasses (prove me wrong). The new glasses can capture video in up to 3K resolution and feature a 12-megapixel camera with a 122-degree wide-angle lens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The glasses have an IP67 dust and water resistance rating for use during intense workouts. Meta says the wraparound design of the glasses features Oakley PRIZM Lens technology, which is designed to block out sun, wind, and dust.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047782" height="597" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-19-at-10.38.59AM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Unless you’re an ultramarathon runner, these glasses will easily last throughout your workout. The glasses can stay on for nine hours, or six hours with continuous music playback. But the charging case that the glasses come with can provide an additional 36 hours of charge on the go. Meta claims that the charging case can quickly get the glasses to a 50% charge in 20 minutes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Oakley Meta Vanguard glasses retail for $499 and go on sale on October 21.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On the VR front, Meta did not release any new Quest headsets as part of this year’s Connect. Even though the conference and company are named after the metaverse, we learned about just a small number of updates to its VR, such as Hyperscape, which will allow developers and creators to build photorealistic spaces in virtual reality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta is reportedly developing an ultralight VR headset for launch by the end of 2026, so maybe we’ll see that come to fruition at the next Meta Connect event.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;At Meta Connect 2025, the company’s biggest event of the year, Mark Zuckerberg unveiled three new smart glasses: the second-generation Ray-Ban Meta, the Meta Ray-Ban Display and wristband controller, and the Oakley Meta Vanguard.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta says it has sold 2 million of the first-generation Ray-Ban Meta smart glasses, and earlier this year, Meta unveiled its latest AI-powered smart glasses with Oakley, which were designed for athletes. Silicon Valley is leaning heavily into AI wearables, and Meta seems to be one of the companies leading the charge.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;With Meta looking to regain its footing in the AI race and sell more hardware, the company had a lot at stake during Mark Zuckerberg’s Meta Connect 2025 keynote. Overall, Meta showcased some pretty impressive technology — the Meta Neural Band, the wristband controller that comes with the Meta Ray-Ban Display, is a particular highlight.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And yet, in a twist that felt reminiscent of HBO’s “Silicon Valley,” Zuckerberg’s demo of the AI capabilities on the Ray-Ban Metas failed. Whoops!&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While sharing a live video feed of the cooking content creator Jack Mancuso at Meta HQ, Zuckerberg asked the chef to demonstrate how his Ray-Ban Meta glasses could help him whip up a Korean-inspired steak sauce.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047688" height="374" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-18-at-9.15.32PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Jack Mancuso at Meta Connect.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“I love the setup you have here, with soy sauce and other ingredients. How can I help?” asked the chipper Meta AI voice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mancuso asked for a recipe for a Korean-inspired steak sauce, and the AI voice began to list the ingredients that he would need — but Mancuso knows he needs to keep the demo succinct, so he interrupts and asks, “What do I do first?”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;After a moment of silence that dragged a bit too long, Mancuso repeated, “What do I do first?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“You’ve already combined the base ingredients, so now grate a pear to add to the sauce,” the AI said. But he had not yet combined the base ingredients, because he had not started making the recipe, hence the question of what to do first.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Mancuso asked the same question again, and the AI gave the same response. The audience laughed.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I think the Wi-Fi might be messed up — back to you, Mark!” Mancuso said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“You know what? It’s all good. The irony of the whole thing is that you spend years making the technology, and then the Wi-Fi of the day kind of&amp;nbsp;… catches you,” Zuckerberg said. “Anyway, we’ll go check out what he made later.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The whole interaction was a bit awkward, especially since the issue did not seem to be with the Wi-Fi. But even when things are going according to plan, these presentations can feel a bit hokey&amp;nbsp;… like the end of the keynote, when Zuckerberg and Diplo quite literally ran into the sunset together, wearing their Meta Oakley Vanguards. It had to be a busy day for Mark, so maybe he just needed an excuse to build some cardio into his schedule.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047719" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Lifestyle-Still-5.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph"&gt;Meta unveiled the second generation of its Ray-Ban Meta glasses, which first debuted in 2023. This spruced-up model features double the battery life of its predecessor, now lasting up to eight hours of mixed use on one charge. The second-generation glasses also support ultra HD 3K video recording, which the company says is twice as sharp as the last model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta’s smart glasses are also getting some new features with the release of the second-generation Ray-Ban Metas, like conversation focus, which will be available on the Ray-Ban Meta and Oakley Meta HSTN glasses.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047780" height="592" src="https://techcrunch.com/wp-content/uploads/2025/09/04_style-and-color_Carousel-01.webp?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“If you’re eating at a hot new restaurant, commuting on the train, or catching your favorite DJ’s latest set, conversation focus uses your AI glasses’ open-ear speakers to amplify the voice of the person you’re talking to,” Meta said in its press release.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Conversation focus isn’t out just yet, so we can’t say for sure if it’ll be any help for your next night out. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Live AI feature — which Meta failed to properly demo onstage — is also on its way. But it’s so energy-intensive that you can only use it for about an hour or two.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we make battery and energy efficiency optimizations, Meta AI will transition from something you prompt with a wake word to an always-available assistant,” the company said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The second-generation Ray-Ban Meta glasses are priced at $379.&lt;/p&gt;



&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047720" height="383" src="https://techcrunch.com/wp-content/uploads/2025/09/Meta-Ray-Ban-Display-Outdoor-Lifestyle-1.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Meta Ray-Ban Display smart glasses are the most impressive glasses that Meta has unveiled to date, featuring a built-in display for apps, alerts, and directions on the right lens. But what sets this pair of smart glasses apart is its accompanying wristband controller, the Meta Neural Band.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This wristband lets Meta show off a bit of what it’s been spending so much time (and money) on in its Reality Labs division, which is notorious for losing billions of dollars a quarter.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047148" height="382" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-17-at-8.22.33PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Visually, the Meta Neural Band looks like a Fitbit without a screen. It’s powered by surface electromyography (sEMG), which can pick up on minute hand gestures and small movements. This is far more sophisticated than a wrist gesture on an Apple Watch. Users can write out text messages by holding their fingers together as if they were gripping a pen and “writing” out the text. This means that you can see a WhatsApp message come in on your right glasses lens, then answer it by “writing” your response.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For now, the glasses support Meta apps, but the company will have to support a wide variety of apps in the future to get the kind of adoption they’re looking for. Like&amp;nbsp;Apple&amp;nbsp;and&amp;nbsp;Google, Meta is betting that smart glasses could cut into the market share of the smartphone in the future — but it will be a big challenge to force such a massive cultural shift.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Meta Ray-Ban Display, which comes with the Meta Neural Band, will cost $799 and launches on September 30.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;For the bearish among us, it seems hard to imagine wearing smart glasses and sending text messages by handwriting in the air. But the Oakley Meta Vanguard smart glasses, which are designed for athletes, offer the most coherent use case yet for this kind of technology. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047722" height="450" src="https://techcrunch.com/wp-content/uploads/2025/09/Oakley-Meta-Vanguard-Lifestyle15.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Bikers, trail runners, and skiers can photograph their adventures without pulling out their phones; the glasses’ open-air speakers can play music during your workout, and even link with apps like Strava and Garmin to relay your stats. Like the other new glasses, the Vanguard model is also AI-enabled.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Unlike other models of Meta smart glasses, the Oakley Meta Vanguards have just one unified front lens with a camera in the middle, rather than two lenses with cameras on either side — it’s a design that makes more technical sense, and it’s a fashion statement that you can pull off in athletic eyewear, but not in eyeglasses (prove me wrong). The new glasses can capture video in up to 3K resolution and feature a 12-megapixel camera with a 122-degree wide-angle lens.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The glasses have an IP67 dust and water resistance rating for use during intense workouts. Meta says the wraparound design of the glasses features Oakley PRIZM Lens technology, which is designed to block out sun, wind, and dust.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3047782" height="597" src="https://techcrunch.com/wp-content/uploads/2025/09/Screenshot-2025-09-19-at-10.38.59AM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Meta&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Unless you’re an ultramarathon runner, these glasses will easily last throughout your workout. The glasses can stay on for nine hours, or six hours with continuous music playback. But the charging case that the glasses come with can provide an additional 36 hours of charge on the go. Meta claims that the charging case can quickly get the glasses to a 50% charge in 20 minutes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Oakley Meta Vanguard glasses retail for $499 and go on sale on October 21.&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;On the VR front, Meta did not release any new Quest headsets as part of this year’s Connect. Even though the conference and company are named after the metaverse, we learned about just a small number of updates to its VR, such as Hyperscape, which will allow developers and creators to build photorealistic spaces in virtual reality.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta is reportedly developing an ultralight VR headset for launch by the end of 2026, so maybe we’ll see that come to fruition at the next Meta Connect event.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/meta-connect-2025-what-to-expect-and-how-to-watch/</guid><pubDate>Fri, 19 Sep 2025 14:40:27 +0000</pubDate></item><item><title>Final hours to apply: Be the life of TechCrunch Disrupt 2025 by hosting your own Side Event (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/final-hours-be-the-life-of-techcrunch-disrupt-2025-by-hosting-your-own-side-event/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;This is it — today is your last chance to &lt;strong&gt;host a Side Event&lt;/strong&gt; at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; in San Francisco.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By tonight, applications close. No extensions. No late entries. &lt;strong&gt;Apply here&lt;/strong&gt;.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Disrupt Week, taking place October 25–31, is your chance to put your brand in front of 10,000+ founders and investors, plus the Bay Area tech ecosystem. To create the conversations that shape the conference. To stand out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We’ll handle the promotion. You own the event. All you need to do is submit your proposal now. Don’t miss this opportunity to shine the spotlight on your brand — apply before midnight.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Speaker and Soiree 2024" class="wp-image-2998969" height="473" src="https://techcrunch.com/wp-content/uploads/2025/04/Editors-Speakers-Soiree.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-this-rare-opportunity-to-boost-your-brand"&gt;Don’t miss this rare opportunity to boost your brand&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Still debating? Here’s what you’ll miss if you don’t host:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Influence where it matters:&lt;/strong&gt; Shape the conversations happening around Disrupt 2025 instead of just attending them.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Direct deal flow:&lt;/strong&gt; Get face time with founders and investors outside of the crowded expo floor.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Audience leverage:&lt;/strong&gt; Use your exclusive host discount codes to draw your own network into Disrupt and your event.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Competitive edge:&lt;/strong&gt; While others blend into the attendee crowd, you’ll stand out as a convener of ideas and people.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Amplification with credibility:&lt;/strong&gt; Having your event listed alongside Disrupt programming and the TechCrunch audience gives it weight — and we’ll help signal-boost while you fill the room with your targets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-hurry-before-applications-close-tonight"&gt;Hurry before applications close tonight&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Applying is easy. &lt;strong&gt;Click here to apply&lt;/strong&gt; for your compelling Side Event and make waves in the tech scene before, during, and after the main event.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;This is it — today is your last chance to &lt;strong&gt;host a Side Event&lt;/strong&gt; at &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; in San Francisco.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By tonight, applications close. No extensions. No late entries. &lt;strong&gt;Apply here&lt;/strong&gt;.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Disrupt Week, taking place October 25–31, is your chance to put your brand in front of 10,000+ founders and investors, plus the Bay Area tech ecosystem. To create the conversations that shape the conference. To stand out.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;We’ll handle the promotion. You own the event. All you need to do is submit your proposal now. Don’t miss this opportunity to shine the spotlight on your brand — apply before midnight.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Speaker and Soiree 2024" class="wp-image-2998969" height="473" src="https://techcrunch.com/wp-content/uploads/2025/04/Editors-Speakers-Soiree.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-this-rare-opportunity-to-boost-your-brand"&gt;Don’t miss this rare opportunity to boost your brand&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Still debating? Here’s what you’ll miss if you don’t host:&lt;/p&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Influence where it matters:&lt;/strong&gt; Shape the conversations happening around Disrupt 2025 instead of just attending them.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Direct deal flow:&lt;/strong&gt; Get face time with founders and investors outside of the crowded expo floor.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Audience leverage:&lt;/strong&gt; Use your exclusive host discount codes to draw your own network into Disrupt and your event.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Competitive edge:&lt;/strong&gt; While others blend into the attendee crowd, you’ll stand out as a convener of ideas and people.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Amplification with credibility:&lt;/strong&gt; Having your event listed alongside Disrupt programming and the TechCrunch audience gives it weight — and we’ll help signal-boost while you fill the room with your targets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 class="wp-block-heading" id="h-hurry-before-applications-close-tonight"&gt;Hurry before applications close tonight&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Applying is easy. &lt;strong&gt;Click here to apply&lt;/strong&gt; for your compelling Side Event and make waves in the tech scene before, during, and after the main event.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/final-hours-be-the-life-of-techcrunch-disrupt-2025-by-hosting-your-own-side-event/</guid><pubDate>Fri, 19 Sep 2025 15:00:00 +0000</pubDate></item><item><title>AI On: How Onboarding Teams of AI Agents Drives Productivity and Revenue for Businesses (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/onboarding-teams-ai-agents-productivity-revenue-businesses/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI is no longer solely a back-office tool. It’s a strategic partner that can augment decision-making across every line of business.&lt;/p&gt;
&lt;p&gt;Whether users aim to reduce operational overhead or personalize customer experiences at scale, custom AI agents are key.&lt;/p&gt;
&lt;p&gt;As AI agents are adopted across enterprises, managing their deployment will require a deliberate strategy. The first steps are architecting the enterprise AI infrastructure to optimize for fast, cost-efficient inference and creating a data pipeline that keeps agents continuously fed with timely, contextual information.&lt;/p&gt;
&lt;p&gt;Alongside human and hardware resourcing, onboarding AI agents will become a core strategic function for businesses as leaders orchestrate digital talent across the organization.&lt;/p&gt;
&lt;p&gt;Here’s how to onboard teams of AI agents:&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1. Choose the Right AI Agent for the Task&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Just as human employees are hired for specific roles, AI agents must be selected and trained based on the task they’re meant to perform. Enterprises now have access to a variety of AI models — including for language, vision, speech and reasoning — each with unique strengths.&lt;/p&gt;
&lt;p&gt;For that reason, proper model selection is critical to achieving business outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose a reasoning agent to solve complex problems that require puzzling through answers.&lt;/li&gt;
&lt;li&gt;Use a code-generation copilot to assist developers with writing, changing and merging code.&lt;/li&gt;
&lt;li&gt;Deploy a video analytics AI agent for analyzing site inspections or product defects.&lt;/li&gt;
&lt;li&gt;Onboard a customer service AI assistant that’s grounded in a specific knowledge base — rather than a generic foundation model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model selection affects agent performance, costs, security and business alignment. The right model enables the agent to accurately address business challenges, align with compliance requirements and safeguard sensitive data. Choosing an unsuitable model can lead to overconsumption of computing resources, higher operational costs and inaccurate predictions that negatively impact agent decision-making.&lt;/p&gt;
&lt;p&gt;With software like NVIDIA NIM and NeMo microservices, developers can swap in different models and connect tools based on their needs. The result: task-specific agents fine-tuned to meet a business’ goals, data strategy and compliance requirements.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;2. Upskill AI Agents by Connecting Them to Data&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Onboarding AI agents requires building a strong data strategy.&lt;/p&gt;
&lt;p&gt;AI agents work best with a consistent stream of data that’s specific to the task and the business they’re operating within.&lt;/p&gt;
&lt;p&gt;Institutional knowledge — the accumulated wisdom and experience within an organization — is a crucial asset that can often be lost when employees leave or retire. AI agents can play a pivotal role in capturing and preserving this knowledge for employees to use.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Connecting AI to data sources&lt;/b&gt;: To function at their best, AI agents must interpret a variety of data types, from structured databases to unstructured formats such as PDFs, images and videos. Such connection enables the agents to generate tailored, context-aware responses that go beyond the capabilities of a standalone foundation model, delivering more precise and valuable outcomes.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;AI as a knowledge repository&lt;/b&gt;: AI agents benefit from systems that capture, process and reuse data. A data flywheel continuously collects, processes and uses information to iteratively improve the underlying system. AI systems benefit from this flywheel, recording interactions, decisions and problem-solving approaches to self-optimize their model performance and efficiency. For example, integrating AI into customer service operations allows the system to learn from every conversation, capturing valuable feedback and questions. This data is then used to refine responses and maintain a comprehensive repository of institutional knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVIDIA NeMo supports the development of powerful data flywheels, providing the tools for continuously curating, refining and evaluating data and models. This enables AI agents to improve accuracy and optimize performance through ongoing adaptation and learning.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;3. Onboard AI Agents Into Lines of Business&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Once enterprises create the cloud-based, on-premises or hybrid AI infrastructure to support AI agents and refine the data strategy to feed those agents timely and contextual information, the next step is to systematically deploy AI agents across business units, moving from pilot to scale.&lt;/p&gt;
&lt;p&gt;According to a recent IDC survey of 125 chief information officers, the top three areas that enterprises are looking to integrate agentic AI are IT processes, business operations and customer service.&lt;/p&gt;
&lt;p&gt;In each area, AI agents help enhance the productivity of existing employees, such as by automating the ticketing process for IT engineers or giving employees easy access to data to help serve customers.&lt;/p&gt;
&lt;p&gt;AI agents in the enterprise could also be onboarded for:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic illustrating four ways AI agents can be used to improve business workflows. Collaboration: automatically provide data and information across groups of people. Content management: automate workflows, capture and analyze metrics, and create content. Customer resource management: analyze outcomes for workflows such as lead qualification, customer outreach or contact center management. Enterprise resource planning: automate financial transactions, or manage supply levels and ordering." class="alignnone wp-image-85103 size-full" height="512" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-onboarding-v4.jpg" width="1280" /&gt;&lt;/p&gt;
&lt;p&gt;For telecom operations, Amdocs builds verticalized AI agents using its amAIz platform to handle complex, multistep customer journeys — spanning sales, billing and care — and advance autonomous networks from optimized planning to efficient deployment. This helps ensure performance of the networks and the services they support.&lt;/p&gt;
&lt;p&gt;NVIDIA has partnered with various enterprises, such as enterprise software company ServiceNow, and global systems integrators, like Accenture and Deloitte, to build and deploy AI agents for maximum business impact across use cases and lines of business.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4. Provide Guardrails and Governance for AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Just like employees need clear guidelines to stay on track, AI models require well-defined guardrails to ensure they provide reliable, accurate outputs and operate within ethical boundaries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Topical guardrails&lt;/b&gt;: Topical guardrails prevent the AI from veering off into areas where they aren’t equipped to provide accurate answers. For instance, a customer service AI assistant should focus on resolving customer queries and not drift into unrelated topics such as upsells and offerings.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Content safety guardrails&lt;/b&gt;: Content safety guardrails moderate human-LLM interactions by classifying prompts and responses as safe or unsafe and tagging violations by category when unsafe. These guardrails filter out unwanted language and make sure references are made only to reliable sources, so the AI’s output is trustworthy.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Jailbreak &lt;/b&gt;&lt;b&gt;guardrails&lt;/b&gt;: With a growing number of agents having access to sensitive information, the agents could become vulnerable to data breaches over time. Jailbreak guardrails are designed to help with adversarial threats as well as detect and block jailbreak and prompt injection attempts targeting LLMs. These help ensure safer AI interactions by identifying malicious prompt manipulations in real time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVIDIA NeMo Guardrails empower enterprises to set and enforce domain-specific guidelines by providing a flexible, programmable framework that keeps AI agents aligned with organizational policies, helping ensure they consistently operate within approved topics, maintain safety standards and comply with security requirements with the least latency added at inference.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Started Onboarding AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The best AI agents are not one-size-fits-all. They’re custom-trained, purpose-built and continuously learning.&lt;/p&gt;
&lt;p&gt;Business leaders can start their AI agent onboarding process by asking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What business outcomes do we want AI to drive?&lt;/li&gt;
&lt;li&gt;What knowledge and tools does the AI need access to?&lt;/li&gt;
&lt;li&gt;Who are the human collaborators or overseers?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the near future, every line of business will have dedicated AI agents — trained on its data, tuned to its goals and aligned with its compliance needs. The organizations that invest in thoughtful onboarding, secure data strategies and continuous learning are poised to lead the next phase of enterprise transformation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch this&lt;/i&gt; &lt;i&gt;on-demand webinar&lt;/i&gt;&lt;i&gt; to learn how to create an automated data flywheel that continuously collects feedback to onboard, fine-tune and scale AI agents across enterprises.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, NVIDIA Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA AI news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;i&gt;Explore the self-paced &lt;/i&gt;&lt;i&gt;video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI is no longer solely a back-office tool. It’s a strategic partner that can augment decision-making across every line of business.&lt;/p&gt;
&lt;p&gt;Whether users aim to reduce operational overhead or personalize customer experiences at scale, custom AI agents are key.&lt;/p&gt;
&lt;p&gt;As AI agents are adopted across enterprises, managing their deployment will require a deliberate strategy. The first steps are architecting the enterprise AI infrastructure to optimize for fast, cost-efficient inference and creating a data pipeline that keeps agents continuously fed with timely, contextual information.&lt;/p&gt;
&lt;p&gt;Alongside human and hardware resourcing, onboarding AI agents will become a core strategic function for businesses as leaders orchestrate digital talent across the organization.&lt;/p&gt;
&lt;p&gt;Here’s how to onboard teams of AI agents:&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;1. Choose the Right AI Agent for the Task&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Just as human employees are hired for specific roles, AI agents must be selected and trained based on the task they’re meant to perform. Enterprises now have access to a variety of AI models — including for language, vision, speech and reasoning — each with unique strengths.&lt;/p&gt;
&lt;p&gt;For that reason, proper model selection is critical to achieving business outcomes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose a reasoning agent to solve complex problems that require puzzling through answers.&lt;/li&gt;
&lt;li&gt;Use a code-generation copilot to assist developers with writing, changing and merging code.&lt;/li&gt;
&lt;li&gt;Deploy a video analytics AI agent for analyzing site inspections or product defects.&lt;/li&gt;
&lt;li&gt;Onboard a customer service AI assistant that’s grounded in a specific knowledge base — rather than a generic foundation model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Model selection affects agent performance, costs, security and business alignment. The right model enables the agent to accurately address business challenges, align with compliance requirements and safeguard sensitive data. Choosing an unsuitable model can lead to overconsumption of computing resources, higher operational costs and inaccurate predictions that negatively impact agent decision-making.&lt;/p&gt;
&lt;p&gt;With software like NVIDIA NIM and NeMo microservices, developers can swap in different models and connect tools based on their needs. The result: task-specific agents fine-tuned to meet a business’ goals, data strategy and compliance requirements.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;2. Upskill AI Agents by Connecting Them to Data&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Onboarding AI agents requires building a strong data strategy.&lt;/p&gt;
&lt;p&gt;AI agents work best with a consistent stream of data that’s specific to the task and the business they’re operating within.&lt;/p&gt;
&lt;p&gt;Institutional knowledge — the accumulated wisdom and experience within an organization — is a crucial asset that can often be lost when employees leave or retire. AI agents can play a pivotal role in capturing and preserving this knowledge for employees to use.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Connecting AI to data sources&lt;/b&gt;: To function at their best, AI agents must interpret a variety of data types, from structured databases to unstructured formats such as PDFs, images and videos. Such connection enables the agents to generate tailored, context-aware responses that go beyond the capabilities of a standalone foundation model, delivering more precise and valuable outcomes.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;AI as a knowledge repository&lt;/b&gt;: AI agents benefit from systems that capture, process and reuse data. A data flywheel continuously collects, processes and uses information to iteratively improve the underlying system. AI systems benefit from this flywheel, recording interactions, decisions and problem-solving approaches to self-optimize their model performance and efficiency. For example, integrating AI into customer service operations allows the system to learn from every conversation, capturing valuable feedback and questions. This data is then used to refine responses and maintain a comprehensive repository of institutional knowledge.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVIDIA NeMo supports the development of powerful data flywheels, providing the tools for continuously curating, refining and evaluating data and models. This enables AI agents to improve accuracy and optimize performance through ongoing adaptation and learning.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;3. Onboard AI Agents Into Lines of Business&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Once enterprises create the cloud-based, on-premises or hybrid AI infrastructure to support AI agents and refine the data strategy to feed those agents timely and contextual information, the next step is to systematically deploy AI agents across business units, moving from pilot to scale.&lt;/p&gt;
&lt;p&gt;According to a recent IDC survey of 125 chief information officers, the top three areas that enterprises are looking to integrate agentic AI are IT processes, business operations and customer service.&lt;/p&gt;
&lt;p&gt;In each area, AI agents help enhance the productivity of existing employees, such as by automating the ticketing process for IT engineers or giving employees easy access to data to help serve customers.&lt;/p&gt;
&lt;p&gt;AI agents in the enterprise could also be onboarded for:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Infographic illustrating four ways AI agents can be used to improve business workflows. Collaboration: automatically provide data and information across groups of people. Content management: automate workflows, capture and analyze metrics, and create content. Customer resource management: analyze outcomes for workflows such as lead qualification, customer outreach or contact center management. Enterprise resource planning: automate financial transactions, or manage supply levels and ordering." class="alignnone wp-image-85103 size-full" height="512" src="https://blogs.nvidia.com/wp-content/uploads/2025/09/ai-on-onboarding-v4.jpg" width="1280" /&gt;&lt;/p&gt;
&lt;p&gt;For telecom operations, Amdocs builds verticalized AI agents using its amAIz platform to handle complex, multistep customer journeys — spanning sales, billing and care — and advance autonomous networks from optimized planning to efficient deployment. This helps ensure performance of the networks and the services they support.&lt;/p&gt;
&lt;p&gt;NVIDIA has partnered with various enterprises, such as enterprise software company ServiceNow, and global systems integrators, like Accenture and Deloitte, to build and deploy AI agents for maximum business impact across use cases and lines of business.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;4. Provide Guardrails and Governance for AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Just like employees need clear guidelines to stay on track, AI models require well-defined guardrails to ensure they provide reliable, accurate outputs and operate within ethical boundaries.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Topical guardrails&lt;/b&gt;: Topical guardrails prevent the AI from veering off into areas where they aren’t equipped to provide accurate answers. For instance, a customer service AI assistant should focus on resolving customer queries and not drift into unrelated topics such as upsells and offerings.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Content safety guardrails&lt;/b&gt;: Content safety guardrails moderate human-LLM interactions by classifying prompts and responses as safe or unsafe and tagging violations by category when unsafe. These guardrails filter out unwanted language and make sure references are made only to reliable sources, so the AI’s output is trustworthy.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Jailbreak &lt;/b&gt;&lt;b&gt;guardrails&lt;/b&gt;: With a growing number of agents having access to sensitive information, the agents could become vulnerable to data breaches over time. Jailbreak guardrails are designed to help with adversarial threats as well as detect and block jailbreak and prompt injection attempts targeting LLMs. These help ensure safer AI interactions by identifying malicious prompt manipulations in real time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;NVIDIA NeMo Guardrails empower enterprises to set and enforce domain-specific guidelines by providing a flexible, programmable framework that keeps AI agents aligned with organizational policies, helping ensure they consistently operate within approved topics, maintain safety standards and comply with security requirements with the least latency added at inference.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Get Started Onboarding AI Agents&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;The best AI agents are not one-size-fits-all. They’re custom-trained, purpose-built and continuously learning.&lt;/p&gt;
&lt;p&gt;Business leaders can start their AI agent onboarding process by asking:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What business outcomes do we want AI to drive?&lt;/li&gt;
&lt;li&gt;What knowledge and tools does the AI need access to?&lt;/li&gt;
&lt;li&gt;Who are the human collaborators or overseers?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the near future, every line of business will have dedicated AI agents — trained on its data, tuned to its goals and aligned with its compliance needs. The organizations that invest in thoughtful onboarding, secure data strategies and continuous learning are poised to lead the next phase of enterprise transformation.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Watch this&lt;/i&gt; &lt;i&gt;on-demand webinar&lt;/i&gt;&lt;i&gt; to learn how to create an automated data flywheel that continuously collects feedback to onboard, fine-tune and scale AI agents across enterprises.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Stay up to date on agentic AI, NVIDIA Nemotron and more by subscribing to &lt;/i&gt;&lt;i&gt;NVIDIA AI news&lt;/i&gt;&lt;i&gt;,&lt;/i&gt;&lt;i&gt; joining the community&lt;/i&gt;&lt;i&gt; and following NVIDIA AI on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;i&gt;Explore the self-paced &lt;/i&gt;&lt;i&gt;video tutorials and livestreams&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/onboarding-teams-ai-agents-productivity-revenue-businesses/</guid><pubDate>Fri, 19 Sep 2025 15:00:39 +0000</pubDate></item><item><title>Live demo fails, AI safety wins, and the golden age of robotics (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/live-demo-fails-ai-safety-wins-and-the-golden-age-of-robotics/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;This week on Equity, Anthony Ha, Kirsten Korosec, and Max Zeff unpack the biggest moves in AI, robotics, and regulation. Listen to the full episode to hear about:&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Equity will be back next week. Subscribe wherever you get your podcasts!&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&lt;/em&gt;&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;X and &lt;/em&gt;&lt;em&gt;Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/09/zuck-meta.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="has-text-align-left wp-block-paragraph" id="speakable-summary"&gt;This week on Equity, Anthony Ha, Kirsten Korosec, and Max Zeff unpack the biggest moves in AI, robotics, and regulation. Listen to the full episode to hear about:&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Equity will be back next week. Subscribe wherever you get your podcasts!&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&lt;/em&gt;&lt;/p&gt;



&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt; and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;X and &lt;/em&gt;&lt;em&gt;Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/live-demo-fails-ai-safety-wins-and-the-golden-age-of-robotics/</guid><pubDate>Fri, 19 Sep 2025 15:48:52 +0000</pubDate></item><item><title>Science journalists find ChatGPT is bad at summarizing scientific papers (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/09/science-journalists-find-chatgpt-is-bad-at-summarizing-scientific-papers/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        LLM "tended to sacrifice accuracy for simplicity" when writing news briefs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1418934535-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1418934535-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A live look at what's actually going on inside an LLM when it tries to summarize a scientific paper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Summarizing complex scientific findings for a non-expert audience is one of the most important things a science journalist does from day to day. Generating summaries of complex writing has also been frequently mentioned as one of the best use cases for large language models (despite some prominent counterexamples).&lt;/p&gt;
&lt;p&gt;With all that in mind, the team at the American Association for the Advancement of Science (AAAS) ran an informal year-long study to determine whether ChatGPT could produce the kind of "news brief" paper summaries that its "SciPak" team routinely writes for the journal Science and services like EurekAlert. These SciPak articles are designed to follow a specific and simplified format that conveys crucial information, such as the study's premise, methods, and context, to other journalists who might want to write about it.&lt;/p&gt;
&lt;p&gt;Now, in a new blog post and white paper discussing their findings, the AAAS journalists have concluded that ChatGPT can "passably emulate the structure of a SciPak-style brief," but with prose that "tended to sacrifice accuracy for simplicity" and which "required rigorous fact-checking by SciPak writers."&lt;/p&gt;
&lt;p&gt;"These technologies may have potential as helpful tools for science writers, but they are not ready for 'prime time,' at this point for the SciPak team," AAAS writer Abigail Eisenstadt said.&lt;/p&gt;
&lt;h2&gt;Where’s the human touch?&lt;/h2&gt;
&lt;p&gt;From December 2023 to December 2024, AAAS researchers selected up to two papers per week for ChatGPT to summarize using three different prompts of varying specificity. The team focused on papers with difficult elements like technical jargon, controversial insights, groundbreaking discoveries, human subjects, or non-traditional formats. The tests used the "Plus" version of the latest publicly available GPT models available through the study period, which generally spanned the eras of GPT-4 and GPT-4o.&lt;/p&gt;
&lt;p&gt;In total, 64 papers were summarized, and those summaries were evaluated both quantitatively and qualitatively by the same SciPak writers who had briefed those papers for the AAAS. The researchers note that this design "could not account for human biases," which we'd argue might be significant among journalists evaluating a tool that was threatening to take over one of their core job functions.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2118017 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="492" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/gptscience1.png" width="749" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      No, I don't think this machine summary can replace my human summary, now that you ask...

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          AAAS

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Still, the quantitative survey results among those journalists were pretty one-sided. On the question of whether the ChatGPT summaries "could feasibly blend into the rest of your summary lineups, the average summary rated a score of just 2.26 on a scale of 1 ("no, not at all") to 5 ("absolutely"). On the question of whether the summaries were "compelling," the LLM summaries averaged just 2.14 on the same scale. Across both questions, only a single summary earned a "5" from the human evaluator on either question, compared to 30 ratings of "1."&lt;/p&gt;
&lt;h2&gt;Not up to standards&lt;/h2&gt;
&lt;p&gt;Writers were also asked to write out more qualitative assessments of the individual summaries they evaluated. In these, the writers complained that ChatGPT often conflated correlation and causation, failed to provide context (e.g., that soft actuators tend to be very slow), and tended to overhype results by overusing words like "groundbreaking" and "novel" (though this last behavior went away when the prompts specifically addressed it).&lt;/p&gt;
&lt;p&gt;Overall, the researchers found that ChatGPT was usually good at "transcribing" what was written in a scientific paper, especially if that paper didn't have much nuance to it. But the LLM was weak at "translating" those findings by diving into methodologies, limitations, or big picture implications. Those weaknesses were especially true for papers that offered multiple differing results, or when the LLM was asked to summarize two related papers into one brief.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2118019 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="505" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/gptscience2.png" width="775" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      This AI summary just isn't compelling enough for me.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          AAAS

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While the tone and style of ChatGPT summaries were often a good match for human-authored content, "concerns about the factual accuracy in LLM-authored content" were prevalent, the journalists wrote. Even using ChatGPT summaries as a "starting point" for human editing "would require just as much, if not more, effort as drafting summaries themselves from scratch" due to the need for "extensive fact-checking," they added.&lt;/p&gt;
&lt;p&gt;These results might not be too surprising given previous studies that have shown AI search engines citing incorrect news sources a full 60 percent of the time. Still, the specific weaknesses are all the more glaring when discussing scientific papers, where accuracy and clarity of communication are paramount.&lt;/p&gt;
&lt;p&gt;In the end, the AAAS journalists concluded that ChatGPT "does not meet the style and standards for briefs in the SciPak press package." But the white paper did allow that it might be worth running the experiment again if ChatGPT "experiences a major update." For what it's worth, GPT-5 was introduced to the public in August.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        LLM "tended to sacrifice accuracy for simplicity" when writing news briefs.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1418934535-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/GettyImages-1418934535-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A live look at what's actually going on inside an LLM when it tries to summarize a scientific paper.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Summarizing complex scientific findings for a non-expert audience is one of the most important things a science journalist does from day to day. Generating summaries of complex writing has also been frequently mentioned as one of the best use cases for large language models (despite some prominent counterexamples).&lt;/p&gt;
&lt;p&gt;With all that in mind, the team at the American Association for the Advancement of Science (AAAS) ran an informal year-long study to determine whether ChatGPT could produce the kind of "news brief" paper summaries that its "SciPak" team routinely writes for the journal Science and services like EurekAlert. These SciPak articles are designed to follow a specific and simplified format that conveys crucial information, such as the study's premise, methods, and context, to other journalists who might want to write about it.&lt;/p&gt;
&lt;p&gt;Now, in a new blog post and white paper discussing their findings, the AAAS journalists have concluded that ChatGPT can "passably emulate the structure of a SciPak-style brief," but with prose that "tended to sacrifice accuracy for simplicity" and which "required rigorous fact-checking by SciPak writers."&lt;/p&gt;
&lt;p&gt;"These technologies may have potential as helpful tools for science writers, but they are not ready for 'prime time,' at this point for the SciPak team," AAAS writer Abigail Eisenstadt said.&lt;/p&gt;
&lt;h2&gt;Where’s the human touch?&lt;/h2&gt;
&lt;p&gt;From December 2023 to December 2024, AAAS researchers selected up to two papers per week for ChatGPT to summarize using three different prompts of varying specificity. The team focused on papers with difficult elements like technical jargon, controversial insights, groundbreaking discoveries, human subjects, or non-traditional formats. The tests used the "Plus" version of the latest publicly available GPT models available through the study period, which generally spanned the eras of GPT-4 and GPT-4o.&lt;/p&gt;
&lt;p&gt;In total, 64 papers were summarized, and those summaries were evaluated both quantitatively and qualitatively by the same SciPak writers who had briefed those papers for the AAAS. The researchers note that this design "could not account for human biases," which we'd argue might be significant among journalists evaluating a tool that was threatening to take over one of their core job functions.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;figure class="ars-wp-img-shortcode id-2118017 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="492" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/gptscience1.png" width="749" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      No, I don't think this machine summary can replace my human summary, now that you ask...

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          AAAS

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Still, the quantitative survey results among those journalists were pretty one-sided. On the question of whether the ChatGPT summaries "could feasibly blend into the rest of your summary lineups, the average summary rated a score of just 2.26 on a scale of 1 ("no, not at all") to 5 ("absolutely"). On the question of whether the summaries were "compelling," the LLM summaries averaged just 2.14 on the same scale. Across both questions, only a single summary earned a "5" from the human evaluator on either question, compared to 30 ratings of "1."&lt;/p&gt;
&lt;h2&gt;Not up to standards&lt;/h2&gt;
&lt;p&gt;Writers were also asked to write out more qualitative assessments of the individual summaries they evaluated. In these, the writers complained that ChatGPT often conflated correlation and causation, failed to provide context (e.g., that soft actuators tend to be very slow), and tended to overhype results by overusing words like "groundbreaking" and "novel" (though this last behavior went away when the prompts specifically addressed it).&lt;/p&gt;
&lt;p&gt;Overall, the researchers found that ChatGPT was usually good at "transcribing" what was written in a scientific paper, especially if that paper didn't have much nuance to it. But the LLM was weak at "translating" those findings by diving into methodologies, limitations, or big picture implications. Those weaknesses were especially true for papers that offered multiple differing results, or when the LLM was asked to summarize two related papers into one brief.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2118019 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="505" src="https://cdn.arstechnica.net/wp-content/uploads/2025/09/gptscience2.png" width="775" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      This AI summary just isn't compelling enough for me.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          AAAS

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;While the tone and style of ChatGPT summaries were often a good match for human-authored content, "concerns about the factual accuracy in LLM-authored content" were prevalent, the journalists wrote. Even using ChatGPT summaries as a "starting point" for human editing "would require just as much, if not more, effort as drafting summaries themselves from scratch" due to the need for "extensive fact-checking," they added.&lt;/p&gt;
&lt;p&gt;These results might not be too surprising given previous studies that have shown AI search engines citing incorrect news sources a full 60 percent of the time. Still, the specific weaknesses are all the more glaring when discussing scientific papers, where accuracy and clarity of communication are paramount.&lt;/p&gt;
&lt;p&gt;In the end, the AAAS journalists concluded that ChatGPT "does not meet the style and standards for briefs in the SciPak press package." But the white paper did allow that it might be worth running the experiment again if ChatGPT "experiences a major update." For what it's worth, GPT-5 was introduced to the public in August.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/09/science-journalists-find-chatgpt-is-bad-at-summarizing-scientific-papers/</guid><pubDate>Fri, 19 Sep 2025 17:10:09 +0000</pubDate></item><item><title>Meta’s AR ambitions meet reality, and California gets serious about AI safety … again (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/video/metas-ar-ambitions-meet-reality-and-california-gets-serious-about-ai-safetyagain/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Equity-hosts-9-19-25.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed alignwide is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;This week on Equity, Anthony Ha, Kirsten Korosec, and Max Zeff unpack the biggest moves in AI, robotics, and regulation. Listen to the full episode to hear about:&lt;br /&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&lt;/em&gt;&lt;br /&gt;&lt;em&gt;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt;, and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;X and &lt;/em&gt;&lt;em&gt;Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/09/Equity-hosts-9-19-25.png?resize=1200,670" /&gt;&lt;/div&gt;&lt;figure class="wp-block-embed alignwide is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;



&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;This week on Equity, Anthony Ha, Kirsten Korosec, and Max Zeff unpack the biggest moves in AI, robotics, and regulation. Listen to the full episode to hear about:&lt;br /&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&lt;/em&gt;&lt;br /&gt;&lt;em&gt;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt;, and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; &lt;/em&gt;&lt;em&gt;X and &lt;/em&gt;&lt;em&gt;Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/video/metas-ar-ambitions-meet-reality-and-california-gets-serious-about-ai-safetyagain/</guid><pubDate>Fri, 19 Sep 2025 17:20:38 +0000</pubDate></item><item><title>[NEW] Cracking product-market fit: Lessons from founders and investors at TechCrunch Disrupt 2025 (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/crack-the-code-to-startup-traction-with-insights-from-chef-robotics-nea-and-iconiq-at-techcrunch-disrupt-2025/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Finding product-market fit isn’t a milestone — it’s a messy, make-or-break journey. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; — taking place October 27–29 at Moscone West in San Francisco — Rajat Bhageria (Chef Robotics), Ann Bordetsky (NEA), and Murali Joshi (ICONIQ) break down how to navigate this critical phase. &lt;strong&gt;Register now.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Rajat Bhageria, Ann Bordetsky, Murali Joshi" class="wp-image-3034069" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Bhageria-Bordetsky-Joshi-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-no-more-guessing-just-growth"&gt;No more guessing — just growth&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Rajat Bhageria&lt;/strong&gt;: Founder and CEO of Chef Robotics, scaling AI-powered automation that’s transforming food production.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Ann Bordetsky&lt;/strong&gt;: Partner at NEA, previously at Uber and Twitter, spotting scrappy ingenuity that drives breakout success.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Murali Joshi&lt;/strong&gt;: Partner at ICONIQ, Forbes Midas Brink List honoree, with over $2.5 billion invested in companies like Drata, 1Password, and Fivetran.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;They’ll cover smart testing strategies, real-time iteration, and how to listen to users without getting lost in the noise — offering a rare inside look at what product-market fit really looks like.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-build-something-customers-can-t-live-without"&gt;Build something customers can’t live without&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re in prototype mode or scaling a growing product, this session will give founders actionable insight to cut the guesswork and focus on what actually moves the needle. Catch it on the &lt;strong&gt;Builders Stage&lt;/strong&gt; at TechCrunch Disrupt 2025. &lt;strong&gt;Grab your pass&lt;/strong&gt; before tomorrow ends to save up to $668.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Finding product-market fit isn’t a milestone — it’s a messy, make-or-break journey. At &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; — taking place October 27–29 at Moscone West in San Francisco — Rajat Bhageria (Chef Robotics), Ann Bordetsky (NEA), and Murali Joshi (ICONIQ) break down how to navigate this critical phase. &lt;strong&gt;Register now.&lt;/strong&gt;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Rajat Bhageria, Ann Bordetsky, Murali Joshi" class="wp-image-3034069" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Bhageria-Bordetsky-Joshi-Speaker-16x9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-no-more-guessing-just-growth"&gt;No more guessing — just growth&lt;/h2&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Rajat Bhageria&lt;/strong&gt;: Founder and CEO of Chef Robotics, scaling AI-powered automation that’s transforming food production.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Ann Bordetsky&lt;/strong&gt;: Partner at NEA, previously at Uber and Twitter, spotting scrappy ingenuity that drives breakout success.&lt;/li&gt;



&lt;li class="wp-block-list-item"&gt;&lt;strong&gt;Murali Joshi&lt;/strong&gt;: Partner at ICONIQ, Forbes Midas Brink List honoree, with over $2.5 billion invested in companies like Drata, 1Password, and Fivetran.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;They’ll cover smart testing strategies, real-time iteration, and how to listen to users without getting lost in the noise — offering a rare inside look at what product-market fit really looks like.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-build-something-customers-can-t-live-without"&gt;Build something customers can’t live without&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Whether you’re in prototype mode or scaling a growing product, this session will give founders actionable insight to cut the guesswork and focus on what actually moves the needle. Catch it on the &lt;strong&gt;Builders Stage&lt;/strong&gt; at TechCrunch Disrupt 2025. &lt;strong&gt;Grab your pass&lt;/strong&gt; before tomorrow ends to save up to $668.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/crack-the-code-to-startup-traction-with-insights-from-chef-robotics-nea-and-iconiq-at-techcrunch-disrupt-2025/</guid><pubDate>Fri, 19 Sep 2025 20:30:00 +0000</pubDate></item><item><title>[NEW] Deep researcher with test-time diffusion (The latest research from Google)</title><link>https://research.google/blog/deep-researcher-with-test-time-diffusion/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The recent advances in large language models (LLMs) have fueled the emergence of deep research (DR) agents. These agents demonstrate remarkable capabilities, including the generation of novel ideas, efficient information retrieval, experimental execution, and the subsequent drafting of comprehensive reports and academic papers.&lt;/p&gt;&lt;p&gt;Currently, most public DR agents use a variety of clever techniques to improve their results, like performing reasoning via chain-of-thought or generating multiple answers and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to find missing information or strengthen your arguments. This human pattern is surprisingly similar to the mechanism of &lt;i&gt;retrieval&lt;/i&gt;-augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts?&lt;/p&gt;&lt;p&gt;Today we introduce Test-Time Diffusion Deep Researcher (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via self-evolution enhances the quality of each step in the research workflow. Then, report-level refinement via denoising with retrieval applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;The recent advances in large language models (LLMs) have fueled the emergence of deep research (DR) agents. These agents demonstrate remarkable capabilities, including the generation of novel ideas, efficient information retrieval, experimental execution, and the subsequent drafting of comprehensive reports and academic papers.&lt;/p&gt;&lt;p&gt;Currently, most public DR agents use a variety of clever techniques to improve their results, like performing reasoning via chain-of-thought or generating multiple answers and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to find missing information or strengthen your arguments. This human pattern is surprisingly similar to the mechanism of &lt;i&gt;retrieval&lt;/i&gt;-augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts?&lt;/p&gt;&lt;p&gt;Today we introduce Test-Time Diffusion Deep Researcher (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via self-evolution enhances the quality of each step in the research workflow. Then, report-level refinement via denoising with retrieval applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/deep-researcher-with-test-time-diffusion/</guid><pubDate>Fri, 19 Sep 2025 20:43:00 +0000</pubDate></item><item><title>[NEW] Why California’s SB 53 might provide a meaningful check on big AI companies (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/09/19/why-californias-sb-53-might-provide-a-meaningful-check-on-big-ai-companies/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-950173010.jpg?resize=1200,861" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;California’s state senate recently gave final approval to a new AI safety bill, SB 53, sending it to Governor Gavin Newsom to either sign or veto.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If this all sounds familiar, that’s because Newsom vetoed another AI safety bill, also written by state senator Scott Wiener, last year. But SB 53 is narrower than Wiener’s previous SB 1047, with a focus on big AI companies making more than $500 million in annual revenue.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;I got the chance to discuss SB 53 with my colleagues Max Zeff and Kirsten Korosec on the latest episode of TechCrunch’s flagship podcast Equity. Max believes that Wiener’s new bill has a better shot of becoming law, partly because of that big company focus, and because it’s been endorsed by AI company Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Read a preview of our conversation about AI safety and state-level legislation below. (I’ve edited the transcript for length and clarity, and to make us sound slightly smarter.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Max:&lt;/strong&gt; Why should you care about AI safety legislation that’s passing a chamber in California? We’re entering this era where AI companies are becoming the most powerful companies in the world, and this is going to be potentially one of the few checks on their power.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is much narrower than SB 1047, which got a lot of pushback last year. But I think SB 53 still puts some meaningful regulations on the AI labs. It makes them publish safety reports for their models. If they have an incident, it basically forces them to report that to the government. And it also, for employees at these labs, if they have concerns, gives them a channel to report that to the government and not face pushback from the companies, even though a lot of them have signed NDAs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To me, this feels like a potentially meaningful check on tech companies’ power, something we haven’t really had for the last couple of decades.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Kirsten: &lt;/strong&gt;To your point about why it matters at the state level, it’s important to think about the fact that it’s California. Every major AI company is pretty much, if not based here, it has a major footprint in this state. Not that other states don’t matter — I do not want to be getting emails from the folks in Colorado or whatever —&amp;nbsp; but it does matter that it’s specifically California because it’s really a hub of AI activity.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;My question for you, though, Max, is it just seems like there’s a lot of exceptions and carve-outs. It’s narrower, but is it more complicated than the previous [bill]?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Max:&lt;/strong&gt; In some ways, yes. I would say the main carve-out of this bill is that it really tries to not apply to small startups. And basically, one of the main controversies around the last legislative effort from Senator Scott Weiner, who represents San Francisco, who authored this bill, a lot of people said it could harm the startup ecosystem, which a lot of people take issue with because that’s such a booming part of California’s economy right now.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This bill specifically applies to AI developers that are [generating] more than $500 million [from] their AI models. This really tries to target OpenAI, Google DeepMind, these big companies and not your run-of-the-mill startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Anthony:&lt;/strong&gt; As I understand it, if you’re a smaller startup, you do have to share some safety information, but not nearly as much.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s [also] worth talking about the broader landscape around AI regulation and the fact that one of the big changes between last year and this year is now we have a new president. The federal administration has taken much more of a stance of no regulation and companies should be able to do what they want, to the extent that they’ve actually included [language] in funding bills saying states cannot have their own AI regulation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I don’t think any of that has passed so far, but potentially they could try to get that through in the future. So this could be another front in which the Trump administration and blue states are fighting.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&lt;/em&gt;&lt;/p&gt;

&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt;, and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; X&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-950173010.jpg?resize=1200,861" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;California’s state senate recently gave final approval to a new AI safety bill, SB 53, sending it to Governor Gavin Newsom to either sign or veto.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If this all sounds familiar, that’s because Newsom vetoed another AI safety bill, also written by state senator Scott Wiener, last year. But SB 53 is narrower than Wiener’s previous SB 1047, with a focus on big AI companies making more than $500 million in annual revenue.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;I got the chance to discuss SB 53 with my colleagues Max Zeff and Kirsten Korosec on the latest episode of TechCrunch’s flagship podcast Equity. Max believes that Wiener’s new bill has a better shot of becoming law, partly because of that big company focus, and because it’s been endorsed by AI company Anthropic.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Read a preview of our conversation about AI safety and state-level legislation below. (I’ve edited the transcript for length and clarity, and to make us sound slightly smarter.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Max:&lt;/strong&gt; Why should you care about AI safety legislation that’s passing a chamber in California? We’re entering this era where AI companies are becoming the most powerful companies in the world, and this is going to be potentially one of the few checks on their power.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This is much narrower than SB 1047, which got a lot of pushback last year. But I think SB 53 still puts some meaningful regulations on the AI labs. It makes them publish safety reports for their models. If they have an incident, it basically forces them to report that to the government. And it also, for employees at these labs, if they have concerns, gives them a channel to report that to the government and not face pushback from the companies, even though a lot of them have signed NDAs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To me, this feels like a potentially meaningful check on tech companies’ power, something we haven’t really had for the last couple of decades.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Kirsten: &lt;/strong&gt;To your point about why it matters at the state level, it’s important to think about the fact that it’s California. Every major AI company is pretty much, if not based here, it has a major footprint in this state. Not that other states don’t matter — I do not want to be getting emails from the folks in Colorado or whatever —&amp;nbsp; but it does matter that it’s specifically California because it’s really a hub of AI activity.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;My question for you, though, Max, is it just seems like there’s a lot of exceptions and carve-outs. It’s narrower, but is it more complicated than the previous [bill]?&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Max:&lt;/strong&gt; In some ways, yes. I would say the main carve-out of this bill is that it really tries to not apply to small startups. And basically, one of the main controversies around the last legislative effort from Senator Scott Weiner, who represents San Francisco, who authored this bill, a lot of people said it could harm the startup ecosystem, which a lot of people take issue with because that’s such a booming part of California’s economy right now.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This bill specifically applies to AI developers that are [generating] more than $500 million [from] their AI models. This really tries to target OpenAI, Google DeepMind, these big companies and not your run-of-the-mill startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Anthony:&lt;/strong&gt; As I understand it, if you’re a smaller startup, you do have to share some safety information, but not nearly as much.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s [also] worth talking about the broader landscape around AI regulation and the fact that one of the big changes between last year and this year is now we have a new president. The federal administration has taken much more of a stance of no regulation and companies should be able to do what they want, to the extent that they’ve actually included [language] in funding bills saying states cannot have their own AI regulation.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I don’t think any of that has passed so far, but potentially they could try to get that through in the future. So this could be another front in which the Trump administration and blue states are fighting.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;Equity is TechCrunch’s flagship podcast, produced by Theresa Loconsolo, and posts every Wednesday and Friday.&lt;/em&gt;&lt;/p&gt;

&lt;p class="has-text-align-left wp-block-paragraph"&gt;&lt;em&gt;Subscribe to us on&lt;/em&gt;&lt;em&gt; Apple Podcasts&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Overcast&lt;/em&gt;&lt;em&gt;,&lt;/em&gt;&lt;em&gt; Spotify&lt;/em&gt;&lt;em&gt;, and all the casts. You also can follow Equity on&lt;/em&gt;&lt;em&gt; X&lt;/em&gt;&lt;em&gt; and &lt;/em&gt;&lt;em&gt;Threads&lt;/em&gt;&lt;em&gt;, at @EquityPod.&amp;nbsp;&lt;/em&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/09/19/why-californias-sb-53-might-provide-a-meaningful-check-on-big-ai-companies/</guid><pubDate>Fri, 19 Sep 2025 21:00:37 +0000</pubDate></item></channel></rss>