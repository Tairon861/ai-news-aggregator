<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 22 Jan 2026 18:37:27 +0000</lastBuildDate><item><title>Yann LeCun’s new venture is a contrarian bet against large language models (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AP23165452788339.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Yann LeCun is a Turing Award recipient and a top AI researcher, but he has long been a contrarian figure in the tech world. He believes that the industry’s current obsession with large language models is wrong-headed and will ultimately fail to solve many pressing problems.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Instead, he thinks we should be betting on world models—a different type of AI that accurately reflects the dynamics of the real world. He is also a staunch advocate for open-source AI and criticizes the closed approach of frontier labs like OpenAI and Anthropic.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Perhaps it’s no surprise, then, that he recently left Meta, where he had served as chief scientist for FAIR (Fundamental AI Research), the company's influential research lab that he founded. Meta has struggled to gain much traction with its open-source AI model Llama and has seen internal shake-ups, including the controversial acquisition of ScaleAI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;LeCun sat down with &lt;em&gt;MIT Technology Review&lt;/em&gt; in an exclusive online interview from his Paris apartment to discuss his new venture, life after Meta, the future of artificial intelligence, and why he thinks the industry is chasing the wrong ideas.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Both the questions and answers below have been edited for clarity and brevity.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You’ve just announced a new company, Advanced Machine Intelligence (AMI).&amp;nbsp; Tell me about the big ideas behind it.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;It is going to be a global company, but headquartered in Paris. You pronounce it “ami”—it means “friend” in French. I am excited. There is a very high concentration of talent in Europe, but it is not always given a proper environment to flourish. And there is certainly a huge demand from the industry and governments for a credible frontier AI company that is neither Chinese nor American. I think that is going to be to our advantage.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;So an ambitious alternative to the US-China binary we currently have. What made you want to pursue that third path?&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Well, there are sovereignty issues for a lot of countries, and they want some control over AI. What I’m advocating is that AI is going to become a platform, and most platforms tend to become open-source. Unfortunately, that’s not really the direction the American industry is taking. Right? As the competition increases, they feel like they have to be secretive. I think that is a strategic mistake.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;It’s certainly true for OpenAI, which went from very open to very closed, and Anthropic has always been closed. Google was sort of a little open. And then Meta, we’ll see. My sense is that it’s not going in a positive direction at this moment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Simultaneously, China has completely embraced this open approach. So all leading open-source AI platforms are Chinese, and the result is that academia and startups, outside of the US, have basically embraced Chinese models. There’s nothing wrong with that—you know, Chinese models are good. Chinese engineers and scientists are great. But you know, if there is a future in which all of our information diet is being mediated by AI assistance, and the choice is either English-speaking models produced by proprietary companies always close to the US or Chinese models which may be open-source but need to be fine-tuned so that they answer questions about Tiananmen Square in 1989—you know, it’s not a very pleasant and engaging future.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;They [the future models] should be able to be fine-tuned by anyone and produce a very high diversity of AI assistance, with different linguistic abilities and value systems and political biases and centers of interests. You need high diversity of assistance for the same reason that you need high diversity of press.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;That is certainly a compelling pitch. How are investors buying that idea so far?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;They really like it. A lot of venture capitalists are very much in favor of this idea of open-source, because they know for a lot of small startups, they really rely on open-source models. They don’t have the means to train their own model, and it’s kind of dangerous for them strategically to embrace a proprietary model.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;You recently left Meta. What’s your view on the company and Mark Zuckerberg’s leadership? There’s a perception that Meta has fumbled its AI advantage.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I think FAIR [LeCun’s lab at Meta] was extremely successful in the research part. Where Meta was less successful is in picking up on that research and pushing it into practical technology and products. Mark made some choices that he thought were the best for the company. I may not have agreed with all of them. For example, the robotics group at FAIR was let go, which I think was a strategic mistake. But I’m not the director of FAIR. People make decisions rationally, and there’s no reason to be upset.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;So, no bad blood? Could Meta be a future client for AMI?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Meta might be our first client! We’ll see. The work we are doing is not in direct competition. Our focus on world models for the physical world is very different from their focus on generative AI and LLMs.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;You were working on AI long before LLMs became a mainstream approach. But since ChatGPT broke out, LLMs have become almost synonymous with AI.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Yes, and we are going to change that. The public face of AI, perhaps, is mostly LLMs and chatbots of various types. But the latest ones of those are not pure LLMs. They are LLM plus a lot of things, like perception systems and code that solves particular problems. So we are going to see LLMs as kind of the orchestrator in systems, a little bit.&lt;/p&gt;  &lt;p&gt;Beyond LLMs, there is a lot of AI that is behind the scenes that runs a big chunk of our society. There are assistance driving programs in a car, quick-turn MRI images, algorithms that drive social media—that’s all AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You have been vocal in arguing that LLMs can only get us so far. Do you think LLMs are overhyped these days? Can you summarize to our readers why you believe that LLMs are not enough?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;There is a sense in which they have not been overhyped, which is that they are extremely useful to a lot of people, particularly if you write text, do research, or write code. LLMs manipulate language really well. But people have had this illusion, or delusion, that it is a matter of time until we can scale them up to having human-level intelligence, and that is simply false.&lt;/p&gt;  &lt;p&gt;The truly difficult part is understanding the real world. This is the Moravec Paradox (a phenomenon observed by the computer scientist Hans Moravec in 1988): What’s easy for us, like perception and navigation, is hard for computers, and vice versa. LLMs are limited to the discrete world of text. They can’t truly reason or plan, because they lack a model of the world. They can’t predict the consequences of their actions. This is why we don’t have a domestic robot that is as agile as a house cat, or a truly autonomous car.&lt;/p&gt; 
 &lt;p&gt;We are going to have AI systems that have humanlike and human-level intelligence, but they’re&amp;nbsp; not going to be built on LLMs, and it’s not going to happen next year or two years from now. It’s going to take a while. There are major conceptual breakthroughs that have to happen before we have AI systems that have human-level intelligence. And that is what I’ve been working on. And this company, AMI Labs, is focusing on the next generation.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;And your solution is world models and JEPA architecture (JEPA, or “joint embedding predictive architecture,” is a learning framework that trains AI models to understand the world, created by LeCun while he was at Meta). What’s the elevator pitch?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;The world is unpredictable. If you try to build a generative model that predicts every detail of the future, it will fail.&amp;nbsp; JEPA is not generative AI. It is a system that learns to represent videos really well. The key is to learn an abstract representation of the world and make predictions in that abstract space, ignoring the details you can’t predict. That’s what JEPA does. It learns the underlying rules of the world from observation, like a baby learning about gravity. This is the foundation for common sense, and it’s the key to building truly intelligent systems that can reason and plan in the real world. The most exciting work so far on this is coming from academia, not the big industrial labs stuck in the LLM world.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;The lack of non-text data has been a problem in taking AI systems further in understanding the physical world. JEPA is trained on videos. What other kinds of data will you be using?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Our systems will be trained on video, audio, and sensor data of all kinds—not just text. We are working with various modalities, from the position of a robot arm to lidar data to audio. I’m also involved in a project using JEPA to model complex physical and clinical phenomena.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What are some of the concrete, real-world applications you envision for world models?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The applications are vast. Think about complex industrial processes where you have thousands of sensors, like in a jet engine, a steel mill, or a chemical factory. There is no technique right now to build a complete, holistic model of these systems. A world model could learn this from the sensor data and predict how the system will behave. Or think of smart glasses that can watch what you’re doing, identify your actions, and then predict what you’re going to do next to assist you. This is what will finally make agentic systems reliable. An agentic system that is supposed to take actions in the world cannot work reliably unless it has a world model to predict the consequences of its actions. Without it, the system will inevitably make mistakes. This is the key to unlocking everything from truly useful domestic robots to Level 5 autonomous driving.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Humanoid robots are all the rage recently, especially ones built by companies from China. What’s your take?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There are all these brute-force ways to get around the limitations of learning systems, which require inordinate amounts of training data to do anything. So the secret of all the companies getting robots to do kung fu or dance is they are all planned in advance. But frankly, nobody—absolutely nobody—knows how to make those robots smart enough to be useful. Take my word for it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;You need an enormous amount of tele-operation training data for every single task, and when the environment changes a little bit, it doesn’t generalize very well. What this tells us is we are missing something very big. The reason why a 17-year-old can learn to drive in 20 hours is because they already know a lot about how the world behaves. If we want a generally useful domestic robot, we need systems to have a kind of good understanding of the physical world. That’s not going to happen until we have good world models and planning.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;strong&gt;There’s a growing sentiment that it’s becoming harder to do foundational AI research in academia because of the massive computing resources required. Do you think the most important innovations will now come from industry?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;No. LLMs are now technology development, not research. It’s true that it’s very difficult for academics to play an important role there because of the requirements for computation, data access, and engineering support. But it’s a product now. It’s not something academia should even be interested in. It’s like speech recognition in the early 2010s—it was a solved problem, and the progress was in the hands of industry.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;What academia should be working on is long-term objectives that go beyond the capabilities of current systems. That’s why I tell people in universities: Don’t work on LLMs. There is no point. You’re not going to be able to rival what’s going on in industry. Work on something else. Invent new techniques. The breakthroughs are not going to come from scaling up LLMs. The most exciting work on world models is coming from academia, not the big industrial labs. The whole idea of using attention circuits in neural nets came out of the University of Montreal. That research paper started the whole revolution. Now that the big companies are closing up, the breakthroughs are going to slow down. Academia needs access to computing resources, but they should be focused on the next big thing, not on refining the last one.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You wear many hats: professor, researcher, educator, public thinker … Now you just took on a new one. What is that going to look like for you?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I am going to be the executive chairman of the company, and Alex LeBrun [a former colleague from Meta AI] will be the CEO. It’s going to be LeCun and LeBrun—it’s nice if you pronounce it the French way.&lt;/p&gt;  &lt;p&gt;I am going to keep my position at NYU. I teach one class per year, I have PhD students and postdocs, so I am going to be kept based in New York. But I go to Paris pretty often because of my lab.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Does that mean that you won’t be very hands-on?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Well, there's two ways to be hands-on. One is to manage people day to day, and another is to actually get your hands dirty in research projects, right?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;I can do management, but I don't like doing it. This is not my mission in life. It’s really to make science and technology progress as far as we can, inspire other people to work on things that are interesting, and then contribute to those things. So that has been my role at Meta for the last seven years. I founded FAIR and led it for four to five years. I kind of hated being a director. I am not good at this career management thing. I'm much more visionary and a scientist.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What makes Alex LeBrun the right fit?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Alex is a serial entrepreneur; he’s built three successful AI companies. The first he sold to Microsoft; the second to Facebook, where he was head of the engineering division of FAIR in Paris. He then left to create Nabla, a very successful company in the health-care space. When I offered him the chance to join me in this effort, he accepted almost immediately. He has the experience to build the company, allowing me to focus on science and technology.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You’re headquartered in Paris. Where else do you plan to have offices?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We are a global company. There’s going to be an office in North America.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;New York, hopefully?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;New York is great. That’s where I am, right? And it’s not Silicon Valley. Silicon Valley is a bit of a monoculture.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What about Asia? I’m guessing Singapore, too?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Probably, yeah. I’ll let you guess.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;And how are you attracting talent?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We don’t have any issue recruiting. There are a lot of people in the AI research community who think the future of AI is in world models. Those people, regardless of pay package, will be motivated to come work for us because they believe in the technological future we are building. We’ve already recruited people from places like OpenAI, Google DeepMind, and xAI.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;I heard that Saining Xie, a prominent researcher from NYU and Google DeepMind, might be joining you as chief scientist. Any comments?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Saining is a brilliant researcher. I have a lot of admiration for him. I hired him twice already. I hired him at FAIR, and I convinced my colleagues at NYU that we should hire him there. Let’s just say I have a lot of respect for him.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;When will you be ready to share more details about AMI Labs, like financial backing or other core members?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Soon—in February, maybe. I’ll let you know.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AP23165452788339.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Yann LeCun is a Turing Award recipient and a top AI researcher, but he has long been a contrarian figure in the tech world. He believes that the industry’s current obsession with large language models is wrong-headed and will ultimately fail to solve many pressing problems.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Instead, he thinks we should be betting on world models—a different type of AI that accurately reflects the dynamics of the real world. He is also a staunch advocate for open-source AI and criticizes the closed approach of frontier labs like OpenAI and Anthropic.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Perhaps it’s no surprise, then, that he recently left Meta, where he had served as chief scientist for FAIR (Fundamental AI Research), the company's influential research lab that he founded. Meta has struggled to gain much traction with its open-source AI model Llama and has seen internal shake-ups, including the controversial acquisition of ScaleAI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;LeCun sat down with &lt;em&gt;MIT Technology Review&lt;/em&gt; in an exclusive online interview from his Paris apartment to discuss his new venture, life after Meta, the future of artificial intelligence, and why he thinks the industry is chasing the wrong ideas.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;&lt;em&gt;Both the questions and answers below have been edited for clarity and brevity.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You’ve just announced a new company, Advanced Machine Intelligence (AMI).&amp;nbsp; Tell me about the big ideas behind it.&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;It is going to be a global company, but headquartered in Paris. You pronounce it “ami”—it means “friend” in French. I am excited. There is a very high concentration of talent in Europe, but it is not always given a proper environment to flourish. And there is certainly a huge demand from the industry and governments for a credible frontier AI company that is neither Chinese nor American. I think that is going to be to our advantage.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;So an ambitious alternative to the US-China binary we currently have. What made you want to pursue that third path?&lt;/strong&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Well, there are sovereignty issues for a lot of countries, and they want some control over AI. What I’m advocating is that AI is going to become a platform, and most platforms tend to become open-source. Unfortunately, that’s not really the direction the American industry is taking. Right? As the competition increases, they feel like they have to be secretive. I think that is a strategic mistake.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;It’s certainly true for OpenAI, which went from very open to very closed, and Anthropic has always been closed. Google was sort of a little open. And then Meta, we’ll see. My sense is that it’s not going in a positive direction at this moment.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Simultaneously, China has completely embraced this open approach. So all leading open-source AI platforms are Chinese, and the result is that academia and startups, outside of the US, have basically embraced Chinese models. There’s nothing wrong with that—you know, Chinese models are good. Chinese engineers and scientists are great. But you know, if there is a future in which all of our information diet is being mediated by AI assistance, and the choice is either English-speaking models produced by proprietary companies always close to the US or Chinese models which may be open-source but need to be fine-tuned so that they answer questions about Tiananmen Square in 1989—you know, it’s not a very pleasant and engaging future.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;They [the future models] should be able to be fine-tuned by anyone and produce a very high diversity of AI assistance, with different linguistic abilities and value systems and political biases and centers of interests. You need high diversity of assistance for the same reason that you need high diversity of press.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;That is certainly a compelling pitch. How are investors buying that idea so far?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;They really like it. A lot of venture capitalists are very much in favor of this idea of open-source, because they know for a lot of small startups, they really rely on open-source models. They don’t have the means to train their own model, and it’s kind of dangerous for them strategically to embrace a proprietary model.&lt;/p&gt; 

 &lt;p&gt;&lt;strong&gt;You recently left Meta. What’s your view on the company and Mark Zuckerberg’s leadership? There’s a perception that Meta has fumbled its AI advantage.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I think FAIR [LeCun’s lab at Meta] was extremely successful in the research part. Where Meta was less successful is in picking up on that research and pushing it into practical technology and products. Mark made some choices that he thought were the best for the company. I may not have agreed with all of them. For example, the robotics group at FAIR was let go, which I think was a strategic mistake. But I’m not the director of FAIR. People make decisions rationally, and there’s no reason to be upset.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;So, no bad blood? Could Meta be a future client for AMI?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Meta might be our first client! We’ll see. The work we are doing is not in direct competition. Our focus on world models for the physical world is very different from their focus on generative AI and LLMs.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;You were working on AI long before LLMs became a mainstream approach. But since ChatGPT broke out, LLMs have become almost synonymous with AI.&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Yes, and we are going to change that. The public face of AI, perhaps, is mostly LLMs and chatbots of various types. But the latest ones of those are not pure LLMs. They are LLM plus a lot of things, like perception systems and code that solves particular problems. So we are going to see LLMs as kind of the orchestrator in systems, a little bit.&lt;/p&gt;  &lt;p&gt;Beyond LLMs, there is a lot of AI that is behind the scenes that runs a big chunk of our society. There are assistance driving programs in a car, quick-turn MRI images, algorithms that drive social media—that’s all AI.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You have been vocal in arguing that LLMs can only get us so far. Do you think LLMs are overhyped these days? Can you summarize to our readers why you believe that LLMs are not enough?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;There is a sense in which they have not been overhyped, which is that they are extremely useful to a lot of people, particularly if you write text, do research, or write code. LLMs manipulate language really well. But people have had this illusion, or delusion, that it is a matter of time until we can scale them up to having human-level intelligence, and that is simply false.&lt;/p&gt;  &lt;p&gt;The truly difficult part is understanding the real world. This is the Moravec Paradox (a phenomenon observed by the computer scientist Hans Moravec in 1988): What’s easy for us, like perception and navigation, is hard for computers, and vice versa. LLMs are limited to the discrete world of text. They can’t truly reason or plan, because they lack a model of the world. They can’t predict the consequences of their actions. This is why we don’t have a domestic robot that is as agile as a house cat, or a truly autonomous car.&lt;/p&gt; 
 &lt;p&gt;We are going to have AI systems that have humanlike and human-level intelligence, but they’re&amp;nbsp; not going to be built on LLMs, and it’s not going to happen next year or two years from now. It’s going to take a while. There are major conceptual breakthroughs that have to happen before we have AI systems that have human-level intelligence. And that is what I’ve been working on. And this company, AMI Labs, is focusing on the next generation.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;And your solution is world models and JEPA architecture (JEPA, or “joint embedding predictive architecture,” is a learning framework that trains AI models to understand the world, created by LeCun while he was at Meta). What’s the elevator pitch?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;The world is unpredictable. If you try to build a generative model that predicts every detail of the future, it will fail.&amp;nbsp; JEPA is not generative AI. It is a system that learns to represent videos really well. The key is to learn an abstract representation of the world and make predictions in that abstract space, ignoring the details you can’t predict. That’s what JEPA does. It learns the underlying rules of the world from observation, like a baby learning about gravity. This is the foundation for common sense, and it’s the key to building truly intelligent systems that can reason and plan in the real world. The most exciting work so far on this is coming from academia, not the big industrial labs stuck in the LLM world.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;The lack of non-text data has been a problem in taking AI systems further in understanding the physical world. JEPA is trained on videos. What other kinds of data will you be using?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Our systems will be trained on video, audio, and sensor data of all kinds—not just text. We are working with various modalities, from the position of a robot arm to lidar data to audio. I’m also involved in a project using JEPA to model complex physical and clinical phenomena.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What are some of the concrete, real-world applications you envision for world models?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;The applications are vast. Think about complex industrial processes where you have thousands of sensors, like in a jet engine, a steel mill, or a chemical factory. There is no technique right now to build a complete, holistic model of these systems. A world model could learn this from the sensor data and predict how the system will behave. Or think of smart glasses that can watch what you’re doing, identify your actions, and then predict what you’re going to do next to assist you. This is what will finally make agentic systems reliable. An agentic system that is supposed to take actions in the world cannot work reliably unless it has a world model to predict the consequences of its actions. Without it, the system will inevitably make mistakes. This is the key to unlocking everything from truly useful domestic robots to Level 5 autonomous driving.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Humanoid robots are all the rage recently, especially ones built by companies from China. What’s your take?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;There are all these brute-force ways to get around the limitations of learning systems, which require inordinate amounts of training data to do anything. So the secret of all the companies getting robots to do kung fu or dance is they are all planned in advance. But frankly, nobody—absolutely nobody—knows how to make those robots smart enough to be useful. Take my word for it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;You need an enormous amount of tele-operation training data for every single task, and when the environment changes a little bit, it doesn’t generalize very well. What this tells us is we are missing something very big. The reason why a 17-year-old can learn to drive in 20 hours is because they already know a lot about how the world behaves. If we want a generally useful domestic robot, we need systems to have a kind of good understanding of the physical world. That’s not going to happen until we have good world models and planning.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;&lt;strong&gt;There’s a growing sentiment that it’s becoming harder to do foundational AI research in academia because of the massive computing resources required. Do you think the most important innovations will now come from industry?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;No. LLMs are now technology development, not research. It’s true that it’s very difficult for academics to play an important role there because of the requirements for computation, data access, and engineering support. But it’s a product now. It’s not something academia should even be interested in. It’s like speech recognition in the early 2010s—it was a solved problem, and the progress was in the hands of industry.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt; &lt;p&gt;What academia should be working on is long-term objectives that go beyond the capabilities of current systems. That’s why I tell people in universities: Don’t work on LLMs. There is no point. You’re not going to be able to rival what’s going on in industry. Work on something else. Invent new techniques. The breakthroughs are not going to come from scaling up LLMs. The most exciting work on world models is coming from academia, not the big industrial labs. The whole idea of using attention circuits in neural nets came out of the University of Montreal. That research paper started the whole revolution. Now that the big companies are closing up, the breakthroughs are going to slow down. Academia needs access to computing resources, but they should be focused on the next big thing, not on refining the last one.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You wear many hats: professor, researcher, educator, public thinker … Now you just took on a new one. What is that going to look like for you?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;I am going to be the executive chairman of the company, and Alex LeBrun [a former colleague from Meta AI] will be the CEO. It’s going to be LeCun and LeBrun—it’s nice if you pronounce it the French way.&lt;/p&gt;  &lt;p&gt;I am going to keep my position at NYU. I teach one class per year, I have PhD students and postdocs, so I am going to be kept based in New York. But I go to Paris pretty often because of my lab.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Does that mean that you won’t be very hands-on?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Well, there's two ways to be hands-on. One is to manage people day to day, and another is to actually get your hands dirty in research projects, right?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_16"&gt; &lt;p&gt;I can do management, but I don't like doing it. This is not my mission in life. It’s really to make science and technology progress as far as we can, inspire other people to work on things that are interesting, and then contribute to those things. So that has been my role at Meta for the last seven years. I founded FAIR and led it for four to five years. I kind of hated being a director. I am not good at this career management thing. I'm much more visionary and a scientist.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What makes Alex LeBrun the right fit?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Alex is a serial entrepreneur; he’s built three successful AI companies. The first he sold to Microsoft; the second to Facebook, where he was head of the engineering division of FAIR in Paris. He then left to create Nabla, a very successful company in the health-care space. When I offered him the chance to join me in this effort, he accepted almost immediately. He has the experience to build the company, allowing me to focus on science and technology.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;You’re headquartered in Paris. Where else do you plan to have offices?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We are a global company. There’s going to be an office in North America.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;New York, hopefully?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;New York is great. That’s where I am, right? And it’s not Silicon Valley. Silicon Valley is a bit of a monoculture.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;What about Asia? I’m guessing Singapore, too?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_18"&gt;&lt;p&gt;Probably, yeah. I’ll let you guess.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;And how are you attracting talent?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;We don’t have any issue recruiting. There are a lot of people in the AI research community who think the future of AI is in world models. Those people, regardless of pay package, will be motivated to come work for us because they believe in the technological future we are building. We’ve already recruited people from places like OpenAI, Google DeepMind, and xAI.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;I heard that Saining Xie, a prominent researcher from NYU and Google DeepMind, might be joining you as chief scientist. Any comments?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Saining is a brilliant researcher. I have a lot of admiration for him. I hired him twice already. I hired him at FAIR, and I convinced my colleagues at NYU that we should hire him there. Let’s just say I have a lot of respect for him.&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;When will you be ready to share more details about AMI Labs, like financial backing or other core members?&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Soon—in February, maybe. I’ll let you know.&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/</guid><pubDate>Thu, 22 Jan 2026 10:00:00 +0000</pubDate></item><item><title>Gates Foundation and OpenAI test AI in African healthcare (AI News)</title><link>https://www.artificialintelligence-news.com/news/gates-foundation-and-openai-test-ai-in-african-healthcare/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Gates-Foundation-and-OpenAI-test-AI-in-African-healthcare-scaled-e1769053673751.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Primary healthcare systems across parts of Africa are under growing strain, caught between rising demand, chronic staff shortages, and shrinking international aid budgets. In that context, AI is being tested in healthcare less as a breakthrough technology and more as a way to keep basic services running.&lt;/p&gt;&lt;p&gt;According to reporting by &lt;em&gt;Reuters&lt;/em&gt;, the Gates Foundation and OpenAI are backing a new initiative, Horizon1000, that aims to introduce AI tools into primary healthcare clinics across several African countries. The project will begin in Rwanda and is intended to reach 1,000 clinics and surrounding communities by 2028, supported by a combined $50 million investment.&lt;/p&gt;&lt;p&gt;The timing is not accidental as global development assistance for health fell by just under 27% last year compared to 2024, the Gates Foundation estimates, following cuts that began in the United States and spread to other major donors such as Britain and Germany. Those reductions have coincided with the first rise in preventable child deaths this century, adding pressure to health systems already stretched thin.&lt;/p&gt;&lt;p&gt;Rather than focusing on advanced diagnostics or research, Horizon1000 is framed around everyday tasks that consume time in under-resourced clinics. AI tools under the programme are expected to assist with patient intake, triage, record keeping, appointment scheduling, and access to medical guidance, particularly in settings where one doctor may serve tens of thousands of people.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-gates-foundation-and-openai-focus-on-ai-support-in-healthcare"&gt;Gates Foundation and OpenAI focus on AI support in healthcare&lt;/h3&gt;&lt;p&gt;“In poorer countries with enormous health worker shortages and lack of health systems infrastructure, AI can be a gamechanger in expanding access to quality care,” Bill Gates wrote in a blog post announcing the initiative. Speaking to &lt;em&gt;Reuters&lt;/em&gt; at the World Economic Forum in Davos, Gates said the technology could help health systems recover after aid cuts slowed progress.&lt;/p&gt;&lt;p&gt;“Our commitment is that that revolution will at least happen in the poor countries as quickly as it happens in the rich countries,” he said.&lt;/p&gt;&lt;p&gt;The focus, according to both partners, is on supporting healthcare workers rather than replacing them. OpenAI is expected to provide technical expertise and AI systems, while the Gates Foundation will work with African governments and health authorities to oversee deployment and alignment with national guidelines.&lt;/p&gt;&lt;p&gt;Rwanda was chosen as the first pilot country in part because of its existing digital health efforts. The country established an AI health hub in Kigali last year and has positioned itself as a testbed for health technology projects. Paula Ingabire, Rwanda’s minister of information and communications technology and innovation, said the goal is to reduce administrative burdens while expanding access.&lt;/p&gt;&lt;p&gt;“It is about using AI responsibly to reduce the burden on healthcare workers, to improve the quality of care, and to reach more patients,” Ingabire said in a video statement released alongside the launch.&lt;/p&gt;&lt;p&gt;Under Horizon1000, AI tools may also be used before patients reach clinics. Gates told &lt;em&gt;Reuters&lt;/em&gt; the systems could support pregnant women and HIV patients with guidance ahead of visits, especially when language barriers exist between patients and providers.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-the-ai-tools-are-expected-to-handle"&gt;What the AI tools are expected to handle&lt;/h3&gt;&lt;p&gt;Once patients arrive, AI could help link records, reduce paperwork, and speed up routine processes.&lt;/p&gt;&lt;p&gt;“A typical visit, we think, can be about twice as fast and much better quality,” Gates said.&lt;/p&gt;&lt;p&gt;Those expectations highlight both the promise and the limits of the approach. While AI may help streamline workflows, its impact depends on reliable data, stable power and connectivity, trained staff, and clear oversight. Many previous digital health pilots in low-income settings have struggled to scale beyond initial trials once funding or external support tapered off.&lt;/p&gt;&lt;p&gt;Horizon1000’s designers say they are trying to avoid that pattern by working closely with local governments and health leaders rather than deploying one-size-fits-all systems. Tools are meant to be adapted to local clinical rules, languages, and care models. Even so, questions remain about long-term maintenance, data governance, and who bears responsibility if systems fail or produce errors.&lt;/p&gt;&lt;p&gt;The initiative also reflects a broader shift in how AI is being positioned in global health. Instead of headline-grabbing claims about medical breakthroughs, the emphasis here is on narrow, operational use cases that address staffing gaps and administrative overload. In that sense, AI is being treated less as a cure for weak health systems and more as a temporary support amid declining resources.&lt;/p&gt;&lt;p&gt;OpenAI’s involvement comes as the company expands its presence in healthcare, following earlier work on health-related applications. At the same time, it faces growing scrutiny over how its systems are trained, deployed, and governed, especially in sensitive sectors like medicine.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-test-of-ai-s-limits-in-healthcare-systems"&gt;A test of AI’s limits in healthcare systems&lt;/h3&gt;&lt;p&gt;For African health systems, the stakes are practical rather than symbolic. Sub-Saharan Africa faces an estimated shortage of nearly six million healthcare workers, a gap that training alone cannot close in the near term. If AI tools can help clinicians see more patients, reduce errors, or manage workloads more effectively, they may offer some relief. If they add complexity or require constant outside support, they risk becoming another layer of dependency.&lt;/p&gt;&lt;p&gt;Horizon1000 sits at that intersection. As aid budgets tighten and healthcare demands rise, the project offers a test of whether AI can play a useful, limited role in primary care without overstating its reach. The outcome will depend less on the technology itself than on how well it fits into the systems meant to use it.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: SAP and Fresenius to build sovereign AI backbone for healthcare&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Gates-Foundation-and-OpenAI-test-AI-in-African-healthcare-scaled-e1769053673751.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Primary healthcare systems across parts of Africa are under growing strain, caught between rising demand, chronic staff shortages, and shrinking international aid budgets. In that context, AI is being tested in healthcare less as a breakthrough technology and more as a way to keep basic services running.&lt;/p&gt;&lt;p&gt;According to reporting by &lt;em&gt;Reuters&lt;/em&gt;, the Gates Foundation and OpenAI are backing a new initiative, Horizon1000, that aims to introduce AI tools into primary healthcare clinics across several African countries. The project will begin in Rwanda and is intended to reach 1,000 clinics and surrounding communities by 2028, supported by a combined $50 million investment.&lt;/p&gt;&lt;p&gt;The timing is not accidental as global development assistance for health fell by just under 27% last year compared to 2024, the Gates Foundation estimates, following cuts that began in the United States and spread to other major donors such as Britain and Germany. Those reductions have coincided with the first rise in preventable child deaths this century, adding pressure to health systems already stretched thin.&lt;/p&gt;&lt;p&gt;Rather than focusing on advanced diagnostics or research, Horizon1000 is framed around everyday tasks that consume time in under-resourced clinics. AI tools under the programme are expected to assist with patient intake, triage, record keeping, appointment scheduling, and access to medical guidance, particularly in settings where one doctor may serve tens of thousands of people.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-gates-foundation-and-openai-focus-on-ai-support-in-healthcare"&gt;Gates Foundation and OpenAI focus on AI support in healthcare&lt;/h3&gt;&lt;p&gt;“In poorer countries with enormous health worker shortages and lack of health systems infrastructure, AI can be a gamechanger in expanding access to quality care,” Bill Gates wrote in a blog post announcing the initiative. Speaking to &lt;em&gt;Reuters&lt;/em&gt; at the World Economic Forum in Davos, Gates said the technology could help health systems recover after aid cuts slowed progress.&lt;/p&gt;&lt;p&gt;“Our commitment is that that revolution will at least happen in the poor countries as quickly as it happens in the rich countries,” he said.&lt;/p&gt;&lt;p&gt;The focus, according to both partners, is on supporting healthcare workers rather than replacing them. OpenAI is expected to provide technical expertise and AI systems, while the Gates Foundation will work with African governments and health authorities to oversee deployment and alignment with national guidelines.&lt;/p&gt;&lt;p&gt;Rwanda was chosen as the first pilot country in part because of its existing digital health efforts. The country established an AI health hub in Kigali last year and has positioned itself as a testbed for health technology projects. Paula Ingabire, Rwanda’s minister of information and communications technology and innovation, said the goal is to reduce administrative burdens while expanding access.&lt;/p&gt;&lt;p&gt;“It is about using AI responsibly to reduce the burden on healthcare workers, to improve the quality of care, and to reach more patients,” Ingabire said in a video statement released alongside the launch.&lt;/p&gt;&lt;p&gt;Under Horizon1000, AI tools may also be used before patients reach clinics. Gates told &lt;em&gt;Reuters&lt;/em&gt; the systems could support pregnant women and HIV patients with guidance ahead of visits, especially when language barriers exist between patients and providers.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-what-the-ai-tools-are-expected-to-handle"&gt;What the AI tools are expected to handle&lt;/h3&gt;&lt;p&gt;Once patients arrive, AI could help link records, reduce paperwork, and speed up routine processes.&lt;/p&gt;&lt;p&gt;“A typical visit, we think, can be about twice as fast and much better quality,” Gates said.&lt;/p&gt;&lt;p&gt;Those expectations highlight both the promise and the limits of the approach. While AI may help streamline workflows, its impact depends on reliable data, stable power and connectivity, trained staff, and clear oversight. Many previous digital health pilots in low-income settings have struggled to scale beyond initial trials once funding or external support tapered off.&lt;/p&gt;&lt;p&gt;Horizon1000’s designers say they are trying to avoid that pattern by working closely with local governments and health leaders rather than deploying one-size-fits-all systems. Tools are meant to be adapted to local clinical rules, languages, and care models. Even so, questions remain about long-term maintenance, data governance, and who bears responsibility if systems fail or produce errors.&lt;/p&gt;&lt;p&gt;The initiative also reflects a broader shift in how AI is being positioned in global health. Instead of headline-grabbing claims about medical breakthroughs, the emphasis here is on narrow, operational use cases that address staffing gaps and administrative overload. In that sense, AI is being treated less as a cure for weak health systems and more as a temporary support amid declining resources.&lt;/p&gt;&lt;p&gt;OpenAI’s involvement comes as the company expands its presence in healthcare, following earlier work on health-related applications. At the same time, it faces growing scrutiny over how its systems are trained, deployed, and governed, especially in sensitive sectors like medicine.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-a-test-of-ai-s-limits-in-healthcare-systems"&gt;A test of AI’s limits in healthcare systems&lt;/h3&gt;&lt;p&gt;For African health systems, the stakes are practical rather than symbolic. Sub-Saharan Africa faces an estimated shortage of nearly six million healthcare workers, a gap that training alone cannot close in the near term. If AI tools can help clinicians see more patients, reduce errors, or manage workloads more effectively, they may offer some relief. If they add complexity or require constant outside support, they risk becoming another layer of dependency.&lt;/p&gt;&lt;p&gt;Horizon1000 sits at that intersection. As aid budgets tighten and healthcare demands rise, the project offers a test of whether AI can play a useful, limited role in primary care without overstating its reach. The outcome will depend less on the technology itself than on how well it fits into the systems meant to use it.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: SAP and Fresenius to build sovereign AI backbone for healthcare&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/gates-foundation-and-openai-test-ai-in-african-healthcare/</guid><pubDate>Thu, 22 Jan 2026 10:00:00 +0000</pubDate></item><item><title>Why 2026 is a hot year for lithium (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/22/1131563/lithium-2026/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/GettyImages-872586396.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In 2026, I’m going to be closely watching the price of lithium.&lt;/p&gt;  &lt;p&gt;If you’re not in the habit of obsessively tracking commodity markets, I certainly don’t blame you. (Though the news lately definitely makes the case that minerals can have major implications for global politics and the economy.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But lithium is worthy of a close look right now.&lt;/p&gt;  &lt;p&gt;The metal is crucial for lithium-ion batteries used in phones and laptops, electric vehicles, and large-scale energy storage arrays on the grid. Prices have been on quite the roller coaster over the last few years, and they’re ticking up again after a low period. What happens next could have big implications for mining and battery technology.&lt;/p&gt; 
 &lt;p&gt;Before we look ahead, let’s take a quick trip down memory lane. In 2020, global EV sales started to really take off, driving up demand for the lithium used in their batteries. Because of that growing demand and a limited supply, prices shot up dramatically, with lithium carbonate going from under $10 per kilogram to a high of roughly $70 per kilogram in just two years.&lt;/p&gt;  &lt;p&gt;And the tech world took notice. During those high points, there was a ton of interest in developing alternative batteries that didn’t rely on lithium. I was writing about sodium-based batteries, iron-air batteries, and even experimental ones that were made with plastic.&lt;/p&gt; 
 &lt;p&gt;Researchers and startups were also hunting for alternative ways to get lithium, including battery recycling and processing methods like direct lithium extraction (more on this in a moment).&lt;/p&gt;  &lt;p&gt;But soon, prices crashed back down to earth. We saw lower-than-expected demand for EVs in the US, and developers ramped up mining and processing to meet demand. Through late 2024 and 2025, lithium carbonate was back around $10 a kilogram again. Avoiding lithium or finding new ways to get it suddenly looked a lot less crucial.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;That brings us to today: lithium prices are ticking up again. So far, it’s nowhere close to the dramatic rise we saw a few years ago, but analysts are watching closely. Strong EV growth in China is playing a major role—EVs still make up about 75% of battery demand today. But growth in stationary storage, batteries for the grid, is also contributing to rising demand for lithium in both China and the US.&lt;/p&gt;  &lt;p&gt;Higher prices could create new opportunities. The possibilities include alternative battery chemistries, specifically sodium-ion batteries, says Evelina Stoikou, head of battery technologies and supply chains at BloombergNEF. (I’ll note here that we recently named sodium-ion batteries to our 2026 list of 10 Breakthrough Technologies.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;It’s not just batteries, though. Another industry that could see big changes from a lithium price swing: extraction.&lt;/p&gt;  &lt;p&gt;Today, most lithium is mined from rocks, largely in Australia, before being shipped to China for processing. There’s a growing effort to process the mineral in other places, though, as countries try to create their own lithium supply chains. Tesla recently confirmed that it’s started production at its lithium refinery in Texas, which broke ground in 2023. We could see more investment in processing plants outside China if prices continue to climb.&lt;/p&gt;  &lt;p&gt;This could also be a key year for direct lithium extraction, as Katie Brigham wrote in a recent story for &lt;em&gt;Heatmap&lt;/em&gt;. That technology uses chemical or electrochemical processes to extract lithium from brine (salty water that’s usually sourced from salt lakes or underground reservoirs), quickly and cheaply. Companies including Lilac Solutions, Standard Lithium, and Rio Tinto are all making plans or starting construction on commercial facilities this year in the US and Argentina.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If there’s anything I’ve learned about following batteries and minerals over the past few years, it’s that predicting the future is impossible. But if you’re looking for tea leaves to read, lithium prices deserve a look.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/GettyImages-872586396.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;In 2026, I’m going to be closely watching the price of lithium.&lt;/p&gt;  &lt;p&gt;If you’re not in the habit of obsessively tracking commodity markets, I certainly don’t blame you. (Though the news lately definitely makes the case that minerals can have major implications for global politics and the economy.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;But lithium is worthy of a close look right now.&lt;/p&gt;  &lt;p&gt;The metal is crucial for lithium-ion batteries used in phones and laptops, electric vehicles, and large-scale energy storage arrays on the grid. Prices have been on quite the roller coaster over the last few years, and they’re ticking up again after a low period. What happens next could have big implications for mining and battery technology.&lt;/p&gt; 
 &lt;p&gt;Before we look ahead, let’s take a quick trip down memory lane. In 2020, global EV sales started to really take off, driving up demand for the lithium used in their batteries. Because of that growing demand and a limited supply, prices shot up dramatically, with lithium carbonate going from under $10 per kilogram to a high of roughly $70 per kilogram in just two years.&lt;/p&gt;  &lt;p&gt;And the tech world took notice. During those high points, there was a ton of interest in developing alternative batteries that didn’t rely on lithium. I was writing about sodium-based batteries, iron-air batteries, and even experimental ones that were made with plastic.&lt;/p&gt; 
 &lt;p&gt;Researchers and startups were also hunting for alternative ways to get lithium, including battery recycling and processing methods like direct lithium extraction (more on this in a moment).&lt;/p&gt;  &lt;p&gt;But soon, prices crashed back down to earth. We saw lower-than-expected demand for EVs in the US, and developers ramped up mining and processing to meet demand. Through late 2024 and 2025, lithium carbonate was back around $10 a kilogram again. Avoiding lithium or finding new ways to get it suddenly looked a lot less crucial.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;That brings us to today: lithium prices are ticking up again. So far, it’s nowhere close to the dramatic rise we saw a few years ago, but analysts are watching closely. Strong EV growth in China is playing a major role—EVs still make up about 75% of battery demand today. But growth in stationary storage, batteries for the grid, is also contributing to rising demand for lithium in both China and the US.&lt;/p&gt;  &lt;p&gt;Higher prices could create new opportunities. The possibilities include alternative battery chemistries, specifically sodium-ion batteries, says Evelina Stoikou, head of battery technologies and supply chains at BloombergNEF. (I’ll note here that we recently named sodium-ion batteries to our 2026 list of 10 Breakthrough Technologies.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;It’s not just batteries, though. Another industry that could see big changes from a lithium price swing: extraction.&lt;/p&gt;  &lt;p&gt;Today, most lithium is mined from rocks, largely in Australia, before being shipped to China for processing. There’s a growing effort to process the mineral in other places, though, as countries try to create their own lithium supply chains. Tesla recently confirmed that it’s started production at its lithium refinery in Texas, which broke ground in 2023. We could see more investment in processing plants outside China if prices continue to climb.&lt;/p&gt;  &lt;p&gt;This could also be a key year for direct lithium extraction, as Katie Brigham wrote in a recent story for &lt;em&gt;Heatmap&lt;/em&gt;. That technology uses chemical or electrochemical processes to extract lithium from brine (salty water that’s usually sourced from salt lakes or underground reservoirs), quickly and cheaply. Companies including Lilac Solutions, Standard Lithium, and Rio Tinto are all making plans or starting construction on commercial facilities this year in the US and Argentina.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;If there’s anything I’ve learned about following batteries and minerals over the past few years, it’s that predicting the future is impossible. But if you’re looking for tea leaves to read, lithium prices deserve a look.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;This article is from The Spark, &lt;/em&gt;MIT Technology Review&lt;em&gt;’s weekly climate newsletter. To receive it in your inbox every Wednesday, &lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/22/1131563/lithium-2026/</guid><pubDate>Thu, 22 Jan 2026 11:00:00 +0000</pubDate></item><item><title>Former Google trio is building an interactive AI-powered learning app for kids (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/former-google-trio-is-building-an-interactive-ai-powered-learning-app-for-kids/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Big Tech companies and upcoming startups want to use generative AI to build software and hardware for kids. A lot of those experiences are limited to text or voice, and kids might not find that captivating. Three former Google employees want to get over that hurdle with their generative AI-powered interactive app, Sparkli.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sparkli was founded last year by Lax Poojary, Lucie Marchand, and Myn Kang. As parents, Poojary and Kang were not able to satisfy their children’s curiosity or give engaging answers to their questions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Kids, by definition, are very curious, and my son would ask me questions about how cars work or how it rains. My approach was to use ChatGPT or Gemini to explain these concepts to a six-year-old, but that is still a wall of text. What kids want is an interactive experience. This was our core process behind founding Sparkli,” Poojary told TechCrunch over a call.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084423" height="486" src="https://techcrunch.com/wp-content/uploads/2026/01/Home-Screen.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Sparkli&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Prior to launching Sparkli, Poojary and Kang co-founded a travel aggregator called Touring Bird and a video-focused social commerce app, Shoploop, at Google’s Area 120, the company’s internal startup incubator. Poojary later went on to work at Google and YouTube on shopping. Marchand, who is the CTO of Sparkli, was also one of the co-founders of Shoploop and later worked at Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When a kid asked what Mars looks like fifty years ago, we might have shown them a picture,” said Poojary. “Ten years ago, we might have shown them a video. With Sparkli, we want kids to interact and experience what Mars is like.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup said that education systems often fall behind in teaching modern concepts. Sparkli wants to teach kids about topics like skills design, financial literacy, and entrepreneurship by creating an AI-powered learning “expedition.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app lets users explore some predefined topics in different categories or ask their own questions to create a learning path. The app also highlights one new topic every day to let kids learn something new. Kids can either listen to the generated voice or read the text. Chapters under one topic include a mix of audio, video, images, quizzes, and games. The app also creates choose-as-you-go adventures that don’t create the pressure of getting questions right or wrong.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084427" height="486" src="https://techcrunch.com/wp-content/uploads/2026/01/Simulator.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Sparkli&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Poojary mentioned that the startup uses generative AI to create all of its media assets on the fly. The company can create a learning experience within two minutes of a user asking a question, and it is trying to reduce this time further. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup mentioned that while AI assistants can help children learn certain topics, their focus is not on education. It said that to make its product effective, the first two hires were a PhD holder in educational science and AI and a teacher. This was a conscious decision to ensure its content better serves children, keeping principles of pedagogy in mind.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the key concerns around kids using AI is safety. Companies like OpenAI and Character.ai are facing lawsuits from parents who allege that these tools encouraged their children to self-harm. Sparkli said that while certain topics like sexual content are completely banned on the app, when a child asks about topics like self-harm, the app tries to teach them about emotional intelligence and encourages them to talk to their parents.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is piloting its app with an institute that has a network of schools with over 100,000 students. Currently, its target audience is children aged 5-12, and it tested its product in over 20 schools last year.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Sparkli has also built a teacher module that allows teachers to track progress and assign homework to kids. The company said that it was inspired by Duolingo to make the app engaging enough that kids can learn concepts and also feel like coming back to the app frequently. The app has streaks and rewards for kids for completing lessons regularly. It also gives kids quest cards, based on the initial avatar they have set up, for learning different topics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have seen a very positive response from our school pilots. Teachers often use Sparkli to create expeditions that kids can explore at the start of the class and lead them into a more discussion-based format. Some teachers also used it to create [homework] after they explain a topic to let kids explore further and get a measure of their understanding,” Poojary said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the startup wants to primarily work with schools globally for the next few months, it wants to open up consumer access and let parents download the app by mid-2026. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised $5 million in pre-seed funding led by Swiss venture firm Founderful. Sparkli is Founderful’s first pure-play edtech investment. The firm’s founding partner, Lukas Weder, said that the team’s technical skill and market opportunity nudged him to invest in the startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As a father of two kids who are in school now, I see them learning interesting stuff, but they don’t learn topics like financial literacy or innovation in technology. I thought from a product point of view, Sparkli gets them away from video games and lets them learn stuff in an immersive way,” Weder said.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Big Tech companies and upcoming startups want to use generative AI to build software and hardware for kids. A lot of those experiences are limited to text or voice, and kids might not find that captivating. Three former Google employees want to get over that hurdle with their generative AI-powered interactive app, Sparkli.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sparkli was founded last year by Lax Poojary, Lucie Marchand, and Myn Kang. As parents, Poojary and Kang were not able to satisfy their children’s curiosity or give engaging answers to their questions.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Kids, by definition, are very curious, and my son would ask me questions about how cars work or how it rains. My approach was to use ChatGPT or Gemini to explain these concepts to a six-year-old, but that is still a wall of text. What kids want is an interactive experience. This was our core process behind founding Sparkli,” Poojary told TechCrunch over a call.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084423" height="486" src="https://techcrunch.com/wp-content/uploads/2026/01/Home-Screen.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Sparkli&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Prior to launching Sparkli, Poojary and Kang co-founded a travel aggregator called Touring Bird and a video-focused social commerce app, Shoploop, at Google’s Area 120, the company’s internal startup incubator. Poojary later went on to work at Google and YouTube on shopping. Marchand, who is the CTO of Sparkli, was also one of the co-founders of Shoploop and later worked at Google.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“When a kid asked what Mars looks like fifty years ago, we might have shown them a picture,” said Poojary. “Ten years ago, we might have shown them a video. With Sparkli, we want kids to interact and experience what Mars is like.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup said that education systems often fall behind in teaching modern concepts. Sparkli wants to teach kids about topics like skills design, financial literacy, and entrepreneurship by creating an AI-powered learning “expedition.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The app lets users explore some predefined topics in different categories or ask their own questions to create a learning path. The app also highlights one new topic every day to let kids learn something new. Kids can either listen to the generated voice or read the text. Chapters under one topic include a mix of audio, video, images, quizzes, and games. The app also creates choose-as-you-go adventures that don’t create the pressure of getting questions right or wrong.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084427" height="486" src="https://techcrunch.com/wp-content/uploads/2026/01/Simulator.jpeg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Sparkli&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Poojary mentioned that the startup uses generative AI to create all of its media assets on the fly. The company can create a learning experience within two minutes of a user asking a question, and it is trying to reduce this time further. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup mentioned that while AI assistants can help children learn certain topics, their focus is not on education. It said that to make its product effective, the first two hires were a PhD holder in educational science and AI and a teacher. This was a conscious decision to ensure its content better serves children, keeping principles of pedagogy in mind.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of the key concerns around kids using AI is safety. Companies like OpenAI and Character.ai are facing lawsuits from parents who allege that these tools encouraged their children to self-harm. Sparkli said that while certain topics like sexual content are completely banned on the app, when a child asks about topics like self-harm, the app tries to teach them about emotional intelligence and encourages them to talk to their parents.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The company is piloting its app with an institute that has a network of schools with over 100,000 students. Currently, its target audience is children aged 5-12, and it tested its product in over 20 schools last year.&lt;/p&gt;

&lt;figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"&gt;&lt;p&gt;
[embedded content]
&lt;/p&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Sparkli has also built a teacher module that allows teachers to track progress and assign homework to kids. The company said that it was inspired by Duolingo to make the app engaging enough that kids can learn concepts and also feel like coming back to the app frequently. The app has streaks and rewards for kids for completing lessons regularly. It also gives kids quest cards, based on the initial avatar they have set up, for learning different topics.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have seen a very positive response from our school pilots. Teachers often use Sparkli to create expeditions that kids can explore at the start of the class and lead them into a more discussion-based format. Some teachers also used it to create [homework] after they explain a topic to let kids explore further and get a measure of their understanding,” Poojary said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the startup wants to primarily work with schools globally for the next few months, it wants to open up consumer access and let parents download the app by mid-2026. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company has raised $5 million in pre-seed funding led by Swiss venture firm Founderful. Sparkli is Founderful’s first pure-play edtech investment. The firm’s founding partner, Lukas Weder, said that the team’s technical skill and market opportunity nudged him to invest in the startup.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As a father of two kids who are in school now, I see them learning interesting stuff, but they don’t learn topics like financial literacy or innovation in technology. I thought from a product point of view, Sparkli gets them away from video games and lets them learn stuff in an immersive way,” Weder said.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/former-google-trio-is-building-an-interactive-ai-powered-learning-app-for-kids/</guid><pubDate>Thu, 22 Jan 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] Quadric rides the shift from cloud AI to on-device inference — and it’s paying off (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/quadric-rides-the-shift-from-cloud-ai-to-on-device-inference-and-its-paying-off/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Companies and governments are looking for tools to run AI locally in a bid to slash cloud infrastructure costs and build sovereign capability. Quadric, a chip-IP startup founded by veterans of early bitcoin mining firm 21E6, is trying to power that shift, scaling beyond automotive into laptops and industrial devices, with its on-device inference technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That expansion is already paying off. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Quadric posted $15 million to $20 million in licensing revenue in 2025, up from around $4 million in 2024, CEO Veerbhan Kheterpal (pictured above, center) told TechCrunch in an interview. The company, which is based in San Francisco and has an office in Pune, India, is targeting up to $35 million this year as it builds a royalty-driven on-device AI business. That growth has buoyed the company, which now has a post-money valuation of between $270 million and $300 million, up from around $100 million in its 2022 Series B, Kheterpal said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It has also helped attract investors to the company. Quadric announced last week a $30 million Series C round led by Accelerate Fund, managed by Beenext Capital Management, bringing its total funding to $72 million. The raise comes as investors and chipmakers look for ways to push more AI workloads from centralized cloud infrastructure onto devices and local servers, Kheterpal told TechCrunch.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-from-automotive-to-everything"&gt;From automotive to everything&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Quadric began in automotive, where on-device AI can power real-time functions like driver assistance. Kheterpal said the spread of transformer-based models in 2023 pushed inference into “everything,” creating a sharp business inflection over the past 18 months as more companies try to run AI locally rather than rely on the cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Nvidia is a strong platform for data-center AI,” Kheterpal said. “We were looking to build a similar CUDA-like or programmable infrastructure for on-device AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike Nvidia, Quadric does not make chips itself. Instead, it licenses programmable AI processor IP, which Kheterpal described as a “blueprint” that customers can embed into their own silicon, along with a software stack and toolchain to run models, including vision and voice, on-device.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3084789" height="933" src="https://techcrunch.com/wp-content/uploads/2026/01/quadric-chip-architecture.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Quadric’s tech is chip-agnostic and is driven by code.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Quadric&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s customers span printers, cars, and AI laptops, including Kyocera and Japan’s auto supplier Denso, which builds chips for Toyota vehicles. The first products based on Quadric’s technology are expected to ship this year, beginning with laptops, Kheterpal told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nonetheless, Quadric is now looking beyond traditional commercial deployments and into markets exploring “sovereign AI” strategies to reduce reliance on U.S.-based infrastructure, Kheterpal said. The startup is exploring customers in India and Malaysia, he added, and counts Moglix CEO Rahul Garg as a strategic investor helping shape its India “sovereign” approach. Quadric employs nearly 70 people worldwide, including about 40 in the U.S. and around 10 in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The push is being driven by the rising cost of centralized AI infrastructure and the difficulty many countries face in building hyperscale data centers, Kheterpal said, prompting more interest in “distributed AI” setups where inference runs on laptops or small on-premise servers inside offices rather than relying on cloud-based services for every query.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The World Economic Forum pointed to this shift in a recent article, as AI inference moves closer to users and away from purely centralized architectures. Similarly, EY said in a November report that the sovereign AI approach has gained traction as policymakers and industry groups push for domestic AI capabilities spanning compute, models, and data, rather than relying entirely on foreign infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For chipmakers, the challenge is that AI models are evolving faster than hardware design cycles, Kheterpal said. He argued that customers need programmable processor IP that can keep pace through software updates rather than requiring costly redesigns every time architectures shift from earlier vision-focused models to today’s transformer-based systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Quadric is pitching itself as an alternative to chip vendors such as Qualcomm, which typically uses its AI technology inside its own processors, as well as IP suppliers like Synopsys and Cadence, which sell neural processing engine blocks. Kheterpal said Qualcomm’s approach can lock customers into its own silicon, while traditional IP suppliers offer engine blocks that many customers find difficult to program.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The programmable approach by Quadric allows customers to support new AI models through software updates rather than redesigning hardware, giving an advantage in an industry where chip development can take years, while model architectures shift in a matter of months nowadays.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Quadric remains early in its buildout, with a handful of signed customers so far and much of its longer-term upside dependent on turning today’s licensing deals into high-volume shipments and recurring royalties.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Companies and governments are looking for tools to run AI locally in a bid to slash cloud infrastructure costs and build sovereign capability. Quadric, a chip-IP startup founded by veterans of early bitcoin mining firm 21E6, is trying to power that shift, scaling beyond automotive into laptops and industrial devices, with its on-device inference technology.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That expansion is already paying off. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Quadric posted $15 million to $20 million in licensing revenue in 2025, up from around $4 million in 2024, CEO Veerbhan Kheterpal (pictured above, center) told TechCrunch in an interview. The company, which is based in San Francisco and has an office in Pune, India, is targeting up to $35 million this year as it builds a royalty-driven on-device AI business. That growth has buoyed the company, which now has a post-money valuation of between $270 million and $300 million, up from around $100 million in its 2022 Series B, Kheterpal said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It has also helped attract investors to the company. Quadric announced last week a $30 million Series C round led by Accelerate Fund, managed by Beenext Capital Management, bringing its total funding to $72 million. The raise comes as investors and chipmakers look for ways to push more AI workloads from centralized cloud infrastructure onto devices and local servers, Kheterpal told TechCrunch.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-from-automotive-to-everything"&gt;From automotive to everything&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Quadric began in automotive, where on-device AI can power real-time functions like driver assistance. Kheterpal said the spread of transformer-based models in 2023 pushed inference into “everything,” creating a sharp business inflection over the past 18 months as more companies try to run AI locally rather than rely on the cloud.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Nvidia is a strong platform for data-center AI,” Kheterpal said. “We were looking to build a similar CUDA-like or programmable infrastructure for on-device AI.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike Nvidia, Quadric does not make chips itself. Instead, it licenses programmable AI processor IP, which Kheterpal described as a “blueprint” that customers can embed into their own silicon, along with a software stack and toolchain to run models, including vision and voice, on-device.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="alt" class="wp-image-3084789" height="933" src="https://techcrunch.com/wp-content/uploads/2026/01/quadric-chip-architecture.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Quadric’s tech is chip-agnostic and is driven by code.&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Quadric&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s customers span printers, cars, and AI laptops, including Kyocera and Japan’s auto supplier Denso, which builds chips for Toyota vehicles. The first products based on Quadric’s technology are expected to ship this year, beginning with laptops, Kheterpal told TechCrunch.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Nonetheless, Quadric is now looking beyond traditional commercial deployments and into markets exploring “sovereign AI” strategies to reduce reliance on U.S.-based infrastructure, Kheterpal said. The startup is exploring customers in India and Malaysia, he added, and counts Moglix CEO Rahul Garg as a strategic investor helping shape its India “sovereign” approach. Quadric employs nearly 70 people worldwide, including about 40 in the U.S. and around 10 in India.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The push is being driven by the rising cost of centralized AI infrastructure and the difficulty many countries face in building hyperscale data centers, Kheterpal said, prompting more interest in “distributed AI” setups where inference runs on laptops or small on-premise servers inside offices rather than relying on cloud-based services for every query.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The World Economic Forum pointed to this shift in a recent article, as AI inference moves closer to users and away from purely centralized architectures. Similarly, EY said in a November report that the sovereign AI approach has gained traction as policymakers and industry groups push for domestic AI capabilities spanning compute, models, and data, rather than relying entirely on foreign infrastructure.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For chipmakers, the challenge is that AI models are evolving faster than hardware design cycles, Kheterpal said. He argued that customers need programmable processor IP that can keep pace through software updates rather than requiring costly redesigns every time architectures shift from earlier vision-focused models to today’s transformer-based systems.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Quadric is pitching itself as an alternative to chip vendors such as Qualcomm, which typically uses its AI technology inside its own processors, as well as IP suppliers like Synopsys and Cadence, which sell neural processing engine blocks. Kheterpal said Qualcomm’s approach can lock customers into its own silicon, while traditional IP suppliers offer engine blocks that many customers find difficult to program.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The programmable approach by Quadric allows customers to support new AI models through software updates rather than redesigning hardware, giving an advantage in an industry where chip development can take years, while model architectures shift in a matter of months nowadays.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, Quadric remains early in its buildout, with a handful of signed customers so far and much of its longer-term upside dependent on turning today’s licensing deals into high-volume shipments and recurring royalties.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/quadric-rides-the-shift-from-cloud-ai-to-on-device-inference-and-its-paying-off/</guid><pubDate>Thu, 22 Jan 2026 12:00:00 +0000</pubDate></item><item><title>[NEW] The Download: Yann LeCun’s new venture, and lithium’s on the rise (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/22/1131680/the-download-yann-lecun-lithium-rise/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Yann LeCun’s new venture is a contrarian bet against large language models&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Yann LeCun is a Turing Award recipient and a top AI researcher, but he has long been a contrarian figure in the tech world. He believes that the industry’s current obsession with large language models is wrong-headed and will ultimately fail to solve many pressing problems.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Instead, he thinks we should be betting on world models—a different type of AI that accurately reflects the dynamics of the real world. Perhaps it’s no surprise, then, that he recently left Meta, where he had served as chief scientist for FAIR (Fundamental AI Research), the company's influential research lab that he founded.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;LeCun sat down with MIT Technology Review in an exclusive online interview from his Paris apartment to discuss his new venture, life after Meta, the future of artificial intelligence, and why he thinks the industry is chasing the wrong ideas.&amp;nbsp;Read the full interview.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt; 
   &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Why 2026 is a hot year for lithium&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In 2026, I’m going to be closely watching the price of lithium.&lt;/p&gt;  &lt;p&gt;If you’re not in the habit of obsessively tracking commodity markets, I certainly don’t blame you. (Though the news lately definitely makes the case that minerals can have major implications for global politics and the economy.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But lithium is worthy of a close look right now. The metal is crucial for lithium-ion batteries used in phones and laptops, electric vehicles, and large-scale energy storage arrays on the grid.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Prices have been on quite the roller coaster over the last few years, and they’re ticking up again. What happens next could have big implications for mining and battery technology.&amp;nbsp;Read the full story.&amp;nbsp;&lt;strong&gt;This story first appeared in The Spark, our newsletter all about the tech we can use to combat the climate crisis.&amp;nbsp;Sign up&amp;nbsp;to receive it in your inbox every Wednesday.&lt;/strong&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Trump has climbed down from his plan for the US to take Greenland&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;To the relief of many across Europe. (BBC)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Trump says he’s agreed a deal to access Greenland’s rare earths. Experts say that’s ‘bonkers.’&lt;/em&gt;&amp;nbsp;(CNN)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;European leaders are feeling flummoxed about what’s going on&lt;/em&gt;. (FT&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Apple is reportedly developing a wearable AI pin&lt;/strong&gt;&lt;br /&gt;It’s still in the very early stages—but this could be a huge deal if it makes it to launch.&amp;nbsp;(The Information&amp;nbsp;$)&lt;br /&gt;+ It’s also planning to revamp Siri and turn it into an AI chatbot. (Bloomberg&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Are we ready to trust AI with our bodies?&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 CEOs say AI saves people time. Their employees disagree.&lt;/strong&gt;&lt;br /&gt;Many even say that it’s currently dragging&amp;nbsp;&lt;em&gt;down&lt;/em&gt;&amp;nbsp;their productivity. (WSJ&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The AI boom will increase US carbon emissions—but it doesn’t have to.&lt;/em&gt;&amp;nbsp;(Wired&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;Let’s also not forget that large language models remain a security nightmare.&lt;/em&gt;&amp;nbsp;(IEEE Spectrum)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 This chart shows how measles cases are exploding in America&lt;/strong&gt;&lt;br /&gt;They’ve hit a 30-year high, with the US on track to lose its ‘elimination status.’ (Axios&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Things are poised to get even worse this year.&amp;nbsp;&lt;/em&gt;(Wired&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Your first humanoid robot coworker will almost definitely be Chinese&lt;/strong&gt;&lt;br /&gt;But will it be truly useful? That’s the even bigger question. (Wired&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;Nvidia CEO Jensen Huang says Europe could do more to compete in robotics and AI.&amp;nbsp;&lt;/em&gt;(CNBC)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Bezos’ Blue Origin is about to compete with Starlink&lt;/strong&gt;&lt;br /&gt;It plans to send the first ‘TeraWave’ satellites into space next year. (Reuters&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;On the ground in Ukraine’s largest Starlink repair shop.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 Trump’s family made $1.4 billion off crypto last year&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Move along, no conflicts of interest to see here. (Bloomberg&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 Comic-Con has banned AI art&lt;/strong&gt;&lt;br /&gt;After an artist-led backlash last week. (404 Media)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Hundreds of creatives are warning against an AI future built on ‘theft on a grand scale’.&lt;/em&gt;&amp;nbsp;(The Verge&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 What it’s like living without a smartphone for a month&lt;/strong&gt;&lt;br /&gt;Potentially blissful for you, but probably a bit annoying for everyone else. (The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Why teens with ADHD are particularly vulnerable to the perils of social media.&lt;/em&gt;&amp;nbsp;(Nature)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Elon Musk is feuding with a budget airline&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The airline is winning, in case you wondered. (WP&amp;nbsp;$)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I wouldn't edit anything about Donald Trump, because the man makes me insane.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Wikipedia founder Jimmy Wales tells&amp;nbsp;Wired&amp;nbsp;why he’s steering clear of the US President's page. &amp;nbsp;&amp;nbsp;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1085277" src="https://wp.technologyreview.com/wp-content/uploads/2023/12/231102_Subime-Systems_thumb.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;BOB O'CONNOR&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h4 class="wp-block-heading"&gt;How electricity could help tackle a surprising climate villain&lt;/h4&gt;  &lt;p&gt;Cement hides in plain sight—it’s used to build everything from roads and buildings to dams and basement floors. But it’s also a climate threat. Cement production accounts for more than 7% of global carbon dioxide emissions—more than sectors like aviation, shipping, or landfills.&lt;/p&gt; 
 &lt;p&gt;One solution to this climate catastrophe might be coursing through the pipes at Sublime Systems. The startup is developing an entirely new way to make cement. Instead of heating crushed-up rocks in lava-hot kilns, Sublime’s technology zaps them in water with electricity, kicking off chemical reactions that form the main ingredients in its cement.&lt;/p&gt;  &lt;p&gt;But it faces huge challenges: competing with established industry players, and persuading builders to use its materials in the first place.&amp;nbsp;Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Earth may be a garbage fire, but space is&amp;nbsp;beautiful.&amp;nbsp;&lt;br /&gt;+ Do you know how to tie your shoelaces up properly?&amp;nbsp;Are you sure?!&lt;br /&gt;+ I defy British readers not to feel a pang of nostalgia at these&amp;nbsp;crisp packets.&lt;br /&gt;+ Going to bed around the&amp;nbsp;same time every night&amp;nbsp;seems to be a habit worth adopting. ($)&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Yann LeCun’s new venture is a contrarian bet against large language models&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Yann LeCun is a Turing Award recipient and a top AI researcher, but he has long been a contrarian figure in the tech world. He believes that the industry’s current obsession with large language models is wrong-headed and will ultimately fail to solve many pressing problems.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Instead, he thinks we should be betting on world models—a different type of AI that accurately reflects the dynamics of the real world. Perhaps it’s no surprise, then, that he recently left Meta, where he had served as chief scientist for FAIR (Fundamental AI Research), the company's influential research lab that he founded.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;LeCun sat down with MIT Technology Review in an exclusive online interview from his Paris apartment to discuss his new venture, life after Meta, the future of artificial intelligence, and why he thinks the industry is chasing the wrong ideas.&amp;nbsp;Read the full interview.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Caiwei Chen&lt;/em&gt;&lt;/p&gt; 
   &lt;h2 class="wp-block-heading"&gt;&lt;strong&gt;Why 2026 is a hot year for lithium&lt;/strong&gt;&lt;/h2&gt;  &lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In 2026, I’m going to be closely watching the price of lithium.&lt;/p&gt;  &lt;p&gt;If you’re not in the habit of obsessively tracking commodity markets, I certainly don’t blame you. (Though the news lately definitely makes the case that minerals can have major implications for global politics and the economy.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;But lithium is worthy of a close look right now. The metal is crucial for lithium-ion batteries used in phones and laptops, electric vehicles, and large-scale energy storage arrays on the grid.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Prices have been on quite the roller coaster over the last few years, and they’re ticking up again. What happens next could have big implications for mining and battery technology.&amp;nbsp;Read the full story.&amp;nbsp;&lt;strong&gt;This story first appeared in The Spark, our newsletter all about the tech we can use to combat the climate crisis.&amp;nbsp;Sign up&amp;nbsp;to receive it in your inbox every Wednesday.&lt;/strong&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 Trump has climbed down from his plan for the US to take Greenland&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;To the relief of many across Europe. (BBC)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Trump says he’s agreed a deal to access Greenland’s rare earths. Experts say that’s ‘bonkers.’&lt;/em&gt;&amp;nbsp;(CNN)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;European leaders are feeling flummoxed about what’s going on&lt;/em&gt;. (FT&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 Apple is reportedly developing a wearable AI pin&lt;/strong&gt;&lt;br /&gt;It’s still in the very early stages—but this could be a huge deal if it makes it to launch.&amp;nbsp;(The Information&amp;nbsp;$)&lt;br /&gt;+ It’s also planning to revamp Siri and turn it into an AI chatbot. (Bloomberg&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Are we ready to trust AI with our bodies?&amp;nbsp;&lt;/em&gt;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3 CEOs say AI saves people time. Their employees disagree.&lt;/strong&gt;&lt;br /&gt;Many even say that it’s currently dragging&amp;nbsp;&lt;em&gt;down&lt;/em&gt;&amp;nbsp;their productivity. (WSJ&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;The AI boom will increase US carbon emissions—but it doesn’t have to.&lt;/em&gt;&amp;nbsp;(Wired&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;Let’s also not forget that large language models remain a security nightmare.&lt;/em&gt;&amp;nbsp;(IEEE Spectrum)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4 This chart shows how measles cases are exploding in America&lt;/strong&gt;&lt;br /&gt;They’ve hit a 30-year high, with the US on track to lose its ‘elimination status.’ (Axios&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Things are poised to get even worse this year.&amp;nbsp;&lt;/em&gt;(Wired&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;5 Your first humanoid robot coworker will almost definitely be Chinese&lt;/strong&gt;&lt;br /&gt;But will it be truly useful? That’s the even bigger question. (Wired&amp;nbsp;$)&lt;br /&gt;+&lt;em&gt;&amp;nbsp;Nvidia CEO Jensen Huang says Europe could do more to compete in robotics and AI.&amp;nbsp;&lt;/em&gt;(CNBC)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6 Bezos’ Blue Origin is about to compete with Starlink&lt;/strong&gt;&lt;br /&gt;It plans to send the first ‘TeraWave’ satellites into space next year. (Reuters&amp;nbsp;$)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;On the ground in Ukraine’s largest Starlink repair shop.&lt;/em&gt;&amp;nbsp;(MIT Technology Review)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;7 Trump’s family made $1.4 billion off crypto last year&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;Move along, no conflicts of interest to see here. (Bloomberg&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;8 Comic-Con has banned AI art&lt;/strong&gt;&lt;br /&gt;After an artist-led backlash last week. (404 Media)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Hundreds of creatives are warning against an AI future built on ‘theft on a grand scale’.&lt;/em&gt;&amp;nbsp;(The Verge&amp;nbsp;$)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;9 What it’s like living without a smartphone for a month&lt;/strong&gt;&lt;br /&gt;Potentially blissful for you, but probably a bit annoying for everyone else. (The Guardian)&lt;br /&gt;+&amp;nbsp;&lt;em&gt;Why teens with ADHD are particularly vulnerable to the perils of social media.&lt;/em&gt;&amp;nbsp;(Nature)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;10 Elon Musk is feuding with a budget airline&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;The airline is winning, in case you wondered. (WP&amp;nbsp;$)&lt;/p&gt;   &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“I wouldn't edit anything about Donald Trump, because the man makes me insane.”&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;—Wikipedia founder Jimmy Wales tells&amp;nbsp;Wired&amp;nbsp;why he’s steering clear of the US President's page. &amp;nbsp;&amp;nbsp;&lt;/p&gt;   
 &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1085277" src="https://wp.technologyreview.com/wp-content/uploads/2023/12/231102_Subime-Systems_thumb.jpg?w=3000" /&gt;&lt;div class="image-credit"&gt;BOB O'CONNOR&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;h4 class="wp-block-heading"&gt;How electricity could help tackle a surprising climate villain&lt;/h4&gt;  &lt;p&gt;Cement hides in plain sight—it’s used to build everything from roads and buildings to dams and basement floors. But it’s also a climate threat. Cement production accounts for more than 7% of global carbon dioxide emissions—more than sectors like aviation, shipping, or landfills.&lt;/p&gt; 
 &lt;p&gt;One solution to this climate catastrophe might be coursing through the pipes at Sublime Systems. The startup is developing an entirely new way to make cement. Instead of heating crushed-up rocks in lava-hot kilns, Sublime’s technology zaps them in water with electricity, kicking off chemical reactions that form the main ingredients in its cement.&lt;/p&gt;  &lt;p&gt;But it faces huge challenges: competing with established industry players, and persuading builders to use its materials in the first place.&amp;nbsp;Read the full story.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt;&lt;p&gt;&lt;em&gt;—Casey Crownhart&lt;/em&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;We can still have nice things&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;A place for comfort, fun and distraction to brighten up your day. (Got any ideas? &lt;/em&gt;&lt;em&gt;Drop me a line&lt;/em&gt;&lt;em&gt; or &lt;/em&gt;&lt;em&gt;skeet 'em at me&lt;/em&gt;&lt;em&gt;.)&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;+ Earth may be a garbage fire, but space is&amp;nbsp;beautiful.&amp;nbsp;&lt;br /&gt;+ Do you know how to tie your shoelaces up properly?&amp;nbsp;Are you sure?!&lt;br /&gt;+ I defy British readers not to feel a pang of nostalgia at these&amp;nbsp;crisp packets.&lt;br /&gt;+ Going to bed around the&amp;nbsp;same time every night&amp;nbsp;seems to be a habit worth adopting. ($)&lt;/p&gt;  &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/22/1131680/the-download-yann-lecun-lithium-rise/</guid><pubDate>Thu, 22 Jan 2026 13:10:00 +0000</pubDate></item><item><title>[NEW] Railway secures $100 million to challenge AWS with AI-native cloud infrastructure (AI | VentureBeat)</title><link>https://venturebeat.com/infrastructure/railway-secures-usd100-million-to-challenge-aws-with-ai-native-cloud</link><description>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt;, a San Francisco-based cloud platform that has quietly amassed two million developers without spending a dollar on marketing, announced Thursday that it raised $100 million in a Series B funding round, as surging demand for artificial intelligence applications exposes the limitations of legacy cloud infrastructure.&lt;/p&gt;&lt;p&gt;&lt;a href="https://tq.vc/"&gt;TQ Ventures&lt;/a&gt; led the round, with participation from &lt;a href="https://fpvventures.com/"&gt;FPV Ventures&lt;/a&gt;, &lt;a href="https://www.redpoint.com/"&gt;Redpoint&lt;/a&gt;, and &lt;a href="https://www.unusual.vc/"&gt;Unusual Ventures&lt;/a&gt;. The investment values Railway as one of the most significant infrastructure startups to emerge during the AI boom, capitalizing on developer frustration with the complexity and cost of traditional platforms like &lt;a href="https://aws.amazon.com/"&gt;Amazon Web Services&lt;/a&gt; and &lt;a href="https://cloud.google.com/"&gt;Google Cloud&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;quot;As AI models get better at writing code, more and more people are asking the age-old question: where, and how, do I run my applications?&amp;quot; said Jake Cooper, Railway&amp;#x27;s 28-year-old founder and chief executive, in an exclusive interview with VentureBeat. &amp;quot;The last generation of cloud primitives were slow and outdated, and now with AI moving everything faster, teams simply can&amp;#x27;t keep up.&amp;quot;&lt;/p&gt;&lt;p&gt;The funding is a dramatic acceleration for a company that has charted an unconventional path through the cloud computing industry. Railway raised just $24 million in total before this round, including a &lt;a href="https://techcrunch.com/2022/05/31/railway-snags-20m-to-streamline-the-process-of-deploying-apps-and-services/"&gt;$20 million Series A&lt;/a&gt; from Redpoint in 2022. The company now processes more than 10 million deployments monthly and handles over one trillion requests through its edge network — metrics that rival far larger and better-funded competitors.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why three-minute deploy times have become unacceptable in the age of AI coding assistants&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Railway&amp;#x27;s pitch rests on a simple observation: the tools developers use to deploy and manage software were designed for a slower era. A standard build-and-deploy cycle using &lt;a href="https://station.railway.com/feedback/terraform-provider-954567d7"&gt;Terraform&lt;/a&gt;, the industry-standard infrastructure tool, takes two to three minutes. That delay, once tolerable, has become a critical bottleneck as AI coding assistants like &lt;a href="https://claude.ai/login"&gt;Claude&lt;/a&gt;, &lt;a href="https://chatgpt.com/"&gt;ChatGPT&lt;/a&gt;, and &lt;a href="https://cursor.com/"&gt;Cursor&lt;/a&gt; can generate working code in seconds.&lt;/p&gt;&lt;p&gt;&amp;quot;When godly intelligence is on tap and can solve any problem in three seconds, those amalgamations of systems become bottlenecks,&amp;quot; Cooper told VentureBeat. &amp;quot;What was really cool for humans to deploy in 10 seconds or less is now table stakes for agents.&amp;quot;&lt;/p&gt;&lt;p&gt;The company claims its platform delivers deployments in under one second — fast enough to keep pace with AI-generated code. Customers report a tenfold increase in developer velocity and up to 65 percent cost savings compared to traditional cloud providers.&lt;/p&gt;&lt;p&gt;These numbers come directly from enterprise clients, not internal benchmarks. Daniel Lobaton, chief technology officer at G2X, a platform serving 100,000 federal contractors, measured deployment speed improvements of seven times faster and an 87 percent cost reduction after migrating to Railway. His infrastructure bill dropped from $15,000 per month to approximately $1,000.&lt;/p&gt;&lt;p&gt;&amp;quot;The work that used to take me a week on our previous infrastructure, I can do in Railway in like a day,&amp;quot; Lobaton said. &amp;quot;If I want to spin up a new service and test different architectures, it would take so long on our old setup. In Railway I can launch six services in two minutes.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the controversial decision to abandon Google Cloud and build data centers from scratch&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;What distinguishes &lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; from competitors like &lt;a href="https://render.com/"&gt;Render&lt;/a&gt; and &lt;a href="http://fly.io"&gt;Fly.io&lt;/a&gt; is the depth of its vertical integration. In 2024, the company made the unusual decision to abandon Google Cloud entirely and build its own data centers, a move that echoes the famous Alan Kay maxim: &amp;quot;People who are really serious about software should make their own hardware.&amp;quot;&lt;/p&gt;&lt;p&gt;&amp;quot;We wanted to design hardware in a way where we could build a differentiated experience,&amp;quot; Cooper said. &amp;quot;Having full control over the network, compute, and storage layers lets us do really fast build and deploy loops, the kind that allows us to move at &amp;#x27;agentic speed&amp;#x27; while staying 100 percent the smoothest ride in town.&amp;quot;&lt;/p&gt;&lt;p&gt;The approach paid dividends during recent &lt;a href="https://restofworld.org/2026/cloud-outages-2025-global-business-impact/"&gt;widespread outages&lt;/a&gt; that affected major cloud providers — Railway remained online throughout.&lt;/p&gt;&lt;p&gt;This soup-to-nuts control enables pricing that undercuts the hyperscalers by roughly 50 percent and newer cloud startups by three to four times. Railway charges by the second for actual compute usage: $0.00000386 per gigabyte-second of memory, $0.00000772 per vCPU-second, and $0.00000006 per gigabyte-second of storage. There are no charges for idle virtual machines — a stark contrast to the traditional cloud model where customers pay for provisioned capacity whether they use it or not.&lt;/p&gt;&lt;p&gt;&amp;quot;The conventional wisdom is that the big guys have economies of scale to offer better pricing,&amp;quot; Cooper noted. &amp;quot;But when they&amp;#x27;re charging for VMs that usually sit idle in the cloud, and we&amp;#x27;ve purpose-built everything to fit much more density on these machines, you have a big opportunity.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How 30 employees built a platform generating tens of millions in annual revenue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; has achieved its scale with a team of just 30 employees generating tens of millions in annual revenue — a ratio of revenue per employee that would be exceptional even for established software companies. The company grew revenue 3.5 times last year and continues to expand at 15 percent month-over-month.&lt;/p&gt;&lt;p&gt;Cooper emphasized that the fundraise was strategic rather than necessary. &amp;quot;We&amp;#x27;re default alive; there&amp;#x27;s no reason for us to raise money,&amp;quot; he said. &amp;quot;We raised because we see a massive opportunity to accelerate, not because we needed to survive.&amp;quot;&lt;/p&gt;&lt;p&gt;The company hired its first salesperson only last year and employs just two solutions engineers. Nearly all of Railway&amp;#x27;s two million users discovered the platform through word of mouth — developers telling other developers about a tool that actually works.&lt;/p&gt;&lt;p&gt;&amp;quot;We basically did the standard engineering thing: if you build it, they will come,&amp;quot; Cooper recalled. &amp;quot;And to some degree, they came.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From side projects to Fortune 500 deployments: Railway&amp;#x27;s unlikely corporate expansion&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite its grassroots developer community, Railway has made significant inroads into large organizations. The company claims that 31 percent of Fortune 500 companies now use its platform, though deployments range from company-wide infrastructure to individual team projects.&lt;/p&gt;&lt;p&gt;Notable customers include &lt;a href="https://www.biltrewards.com/"&gt;Bilt&lt;/a&gt;, the loyalty program company; Intuit&amp;#x27;s &lt;a href="https://www.goco.io/"&gt;GoCo&lt;/a&gt; subsidiary; TripAdvisor&amp;#x27;s &lt;a href="https://www.cruisecritic.com/"&gt;Cruise Critic&lt;/a&gt;; and &lt;a href="https://www.mgmresorts.com/en.html"&gt;MGM Resorts&lt;/a&gt;. &lt;a href="https://www.ycombinator.com/companies/kernel"&gt;Kernel&lt;/a&gt;, a Y Combinator-backed startup providing AI infrastructure to over 1,000 companies, runs its entire customer-facing system on Railway for $444 per month.&lt;/p&gt;&lt;p&gt;&amp;quot;At my previous company Clever, which sold for $500 million, I had six full-time engineers just managing AWS,&amp;quot; said Rafael Garcia, Kernel&amp;#x27;s chief technology officer. &amp;quot;Now I have six engineers total, and they all focus on product. Railway is exactly the tool I wish I had in 2012.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprise customers, &lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; offers security certifications including SOC 2 Type 2 compliance and HIPAA readiness, with business associate agreements available upon request. The platform provides single sign-on authentication, comprehensive audit logs, and the option to deploy within a customer&amp;#x27;s existing cloud environment through a &amp;quot;bring your own cloud&amp;quot; configuration.&lt;/p&gt;&lt;p&gt;Enterprise pricing starts at custom levels, with specific add-ons for extended log retention ($200 monthly), HIPAA BAAs ($1,000), enterprise support with SLOs ($2,000), and dedicated virtual machines ($10,000).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The startup&amp;#x27;s bold strategy to take on Amazon, Google, and a new generation of cloud rivals&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Railway enters a crowded market that includes not only the hyperscale cloud providers—Amazon Web Services, Microsoft Azure, and Google Cloud Platform—but also a growing cohort of developer-focused platforms like Vercel, Render, Fly.io, and Heroku.&lt;/p&gt;&lt;p&gt;Cooper argues that Railway&amp;#x27;s competitors fall into two camps, neither of which has fully committed to the new infrastructure model that AI demands.&lt;/p&gt;&lt;p&gt;&amp;quot;The hyperscalers have two competing systems, and they haven&amp;#x27;t gone all-in on the new model because their legacy revenue stream is still printing money,&amp;quot; he observed. &amp;quot;They have this mammoth pool of cash coming from people who provision a VM, use maybe 10 percent of it, and still pay for the whole thing. To what end are they actually interested in going all the way in on a new experience if they don&amp;#x27;t really need to?&amp;quot;&lt;/p&gt;&lt;p&gt;Against startup competitors, Railway differentiates by covering the full infrastructure stack. &amp;quot;We&amp;#x27;re not just containers; we&amp;#x27;ve got VM primitives, stateful storage, virtual private networking, automated load balancing,&amp;quot; Cooper said. &amp;quot;And we wrap all of this in an absurdly easy-to-use UI, with agentic primitives so agents can move 1,000 times faster.&amp;quot;&lt;/p&gt;&lt;p&gt;The platform supports databases including PostgreSQL, MySQL, MongoDB, and Redis; provides up to 256 terabytes of persistent storage with over 100,000 input/output operations per second; and enables deployment to four global regions spanning the United States, Europe, and Southeast Asia. Enterprise customers can scale to 112 vCPUs and 2 terabytes of RAM per service.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why investors are betting that AI will create a thousand times more software than exists today&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Railway&amp;#x27;s fundraise reflects broader investor enthusiasm for companies positioned to benefit from the AI coding revolution. As tools like &lt;a href="https://github.com/features/copilot"&gt;GitHub Copilot&lt;/a&gt;, &lt;a href="https://cursor.com/agents"&gt;Cursor&lt;/a&gt;, and &lt;a href="https://claude.ai/login"&gt;Claude&lt;/a&gt; become standard fixtures in developer workflows, the volume of code being written — and the infrastructure needed to run it — is expanding dramatically.&lt;/p&gt;&lt;p&gt;&amp;quot;The amount of software that&amp;#x27;s going to come online over the next five years is unfathomable compared to what existed before — we&amp;#x27;re talking a thousand times more software,&amp;quot; Cooper predicted. &amp;quot;All of that has to run somewhere.&amp;quot;&lt;/p&gt;&lt;p&gt;The company has already integrated directly with AI systems, building what Cooper calls &amp;quot;loops where Claude can hook in, call deployments, and analyze infrastructure automatically.&amp;quot; Railway released a Model Context Protocol server in August 2025 that allows AI coding agents to deploy applications and manage infrastructure directly from code editors.&lt;/p&gt;&lt;p&gt;&amp;quot;The notion of a developer is melting before our eyes,&amp;quot; Cooper said. &amp;quot;You don&amp;#x27;t have to be an engineer to engineer things anymore — you just need critical thinking and the ability to analyze things in a systems capacity.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What Railway plans to do with $100 million and zero marketing experience&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; plans to use the new capital to expand its global data center footprint, grow its team beyond 30 employees, and build what Cooper described as a proper go-to-market operation for the first time in the company&amp;#x27;s five-year history.&lt;/p&gt;&lt;p&gt;&amp;quot;One of my mentors said you raise money when you can change the trajectory of the business,&amp;quot; Cooper explained. &amp;quot;We&amp;#x27;ve built all the required substrate to scale indefinitely; what&amp;#x27;s been holding us back is simply talking about it. 2026 is the year we play on the world stage.&amp;quot;&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s investor roster reads like a who&amp;#x27;s who of developer infrastructure. Angel investors include &lt;a href="https://tom.preston-werner.com/"&gt;Tom Preston-Werner,&lt;/a&gt; co-founder of GitHub; &lt;a href="https://rauchg.com/about"&gt;Guillermo Rauch&lt;/a&gt;, chief executive of Vercel; &lt;a href="https://www.cockroachlabs.com/author/spencer-kimball/"&gt;Spencer Kimball&lt;/a&gt;, chief executive of Cockroach Labs; &lt;a href="https://www.datadoghq.com/about/leadership/"&gt;Olivier Pomel&lt;/a&gt;, chief executive of Datadog; and &lt;a href="https://sequoiacap.com/founder/jori-lallo/"&gt;Jori Lallo&lt;/a&gt;, co-founder of Linear.&lt;/p&gt;&lt;p&gt;The timing of Railway&amp;#x27;s expansion coincides with what many in Silicon Valley view as a fundamental shift in how software gets made. Coding assistants are no longer experimental curiosities — they have become essential tools that millions of developers rely on daily. Each line of AI-generated code needs somewhere to run, and the incumbents, by Cooper&amp;#x27;s telling, are too wedded to their existing business models to fully capitalize on the moment.&lt;/p&gt;&lt;p&gt;Whether &lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; can translate developer enthusiasm into sustained enterprise adoption remains an open question. The cloud infrastructure market is littered with promising startups that failed to break the grip of Amazon, Microsoft, and Google. But Cooper, who previously worked as a software engineer at &lt;a href="https://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;, &lt;a href="https://www.bloomberg.com/"&gt;Bloomberg&lt;/a&gt;, and &lt;a href="https://www.uber.com/"&gt;Uber&lt;/a&gt; before founding Railway in 2020, seems unfazed by the scale of his ambition.&lt;/p&gt;&lt;p&gt;&amp;quot;In five years, Railway [will be] the place where software gets created and evolved, period,&amp;quot; he said. &amp;quot;Deploy instantly, scale infinitely, with zero friction. That&amp;#x27;s the prize worth playing for, and there&amp;#x27;s no bigger one on offer.&amp;quot;&lt;/p&gt;&lt;p&gt;For a company that built a $100 million business by doing the opposite of what conventional startup wisdom dictates — no marketing, no sales team, no venture hype—the real test begins now. Railway spent five years proving that developers would find a better mousetrap on their own. The next five will determine whether the rest of the world is ready to get on board.&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;&lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt;, a San Francisco-based cloud platform that has quietly amassed two million developers without spending a dollar on marketing, announced Thursday that it raised $100 million in a Series B funding round, as surging demand for artificial intelligence applications exposes the limitations of legacy cloud infrastructure.&lt;/p&gt;&lt;p&gt;&lt;a href="https://tq.vc/"&gt;TQ Ventures&lt;/a&gt; led the round, with participation from &lt;a href="https://fpvventures.com/"&gt;FPV Ventures&lt;/a&gt;, &lt;a href="https://www.redpoint.com/"&gt;Redpoint&lt;/a&gt;, and &lt;a href="https://www.unusual.vc/"&gt;Unusual Ventures&lt;/a&gt;. The investment values Railway as one of the most significant infrastructure startups to emerge during the AI boom, capitalizing on developer frustration with the complexity and cost of traditional platforms like &lt;a href="https://aws.amazon.com/"&gt;Amazon Web Services&lt;/a&gt; and &lt;a href="https://cloud.google.com/"&gt;Google Cloud&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&amp;quot;As AI models get better at writing code, more and more people are asking the age-old question: where, and how, do I run my applications?&amp;quot; said Jake Cooper, Railway&amp;#x27;s 28-year-old founder and chief executive, in an exclusive interview with VentureBeat. &amp;quot;The last generation of cloud primitives were slow and outdated, and now with AI moving everything faster, teams simply can&amp;#x27;t keep up.&amp;quot;&lt;/p&gt;&lt;p&gt;The funding is a dramatic acceleration for a company that has charted an unconventional path through the cloud computing industry. Railway raised just $24 million in total before this round, including a &lt;a href="https://techcrunch.com/2022/05/31/railway-snags-20m-to-streamline-the-process-of-deploying-apps-and-services/"&gt;$20 million Series A&lt;/a&gt; from Redpoint in 2022. The company now processes more than 10 million deployments monthly and handles over one trillion requests through its edge network — metrics that rival far larger and better-funded competitors.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why three-minute deploy times have become unacceptable in the age of AI coding assistants&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Railway&amp;#x27;s pitch rests on a simple observation: the tools developers use to deploy and manage software were designed for a slower era. A standard build-and-deploy cycle using &lt;a href="https://station.railway.com/feedback/terraform-provider-954567d7"&gt;Terraform&lt;/a&gt;, the industry-standard infrastructure tool, takes two to three minutes. That delay, once tolerable, has become a critical bottleneck as AI coding assistants like &lt;a href="https://claude.ai/login"&gt;Claude&lt;/a&gt;, &lt;a href="https://chatgpt.com/"&gt;ChatGPT&lt;/a&gt;, and &lt;a href="https://cursor.com/"&gt;Cursor&lt;/a&gt; can generate working code in seconds.&lt;/p&gt;&lt;p&gt;&amp;quot;When godly intelligence is on tap and can solve any problem in three seconds, those amalgamations of systems become bottlenecks,&amp;quot; Cooper told VentureBeat. &amp;quot;What was really cool for humans to deploy in 10 seconds or less is now table stakes for agents.&amp;quot;&lt;/p&gt;&lt;p&gt;The company claims its platform delivers deployments in under one second — fast enough to keep pace with AI-generated code. Customers report a tenfold increase in developer velocity and up to 65 percent cost savings compared to traditional cloud providers.&lt;/p&gt;&lt;p&gt;These numbers come directly from enterprise clients, not internal benchmarks. Daniel Lobaton, chief technology officer at G2X, a platform serving 100,000 federal contractors, measured deployment speed improvements of seven times faster and an 87 percent cost reduction after migrating to Railway. His infrastructure bill dropped from $15,000 per month to approximately $1,000.&lt;/p&gt;&lt;p&gt;&amp;quot;The work that used to take me a week on our previous infrastructure, I can do in Railway in like a day,&amp;quot; Lobaton said. &amp;quot;If I want to spin up a new service and test different architectures, it would take so long on our old setup. In Railway I can launch six services in two minutes.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Inside the controversial decision to abandon Google Cloud and build data centers from scratch&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;What distinguishes &lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; from competitors like &lt;a href="https://render.com/"&gt;Render&lt;/a&gt; and &lt;a href="http://fly.io"&gt;Fly.io&lt;/a&gt; is the depth of its vertical integration. In 2024, the company made the unusual decision to abandon Google Cloud entirely and build its own data centers, a move that echoes the famous Alan Kay maxim: &amp;quot;People who are really serious about software should make their own hardware.&amp;quot;&lt;/p&gt;&lt;p&gt;&amp;quot;We wanted to design hardware in a way where we could build a differentiated experience,&amp;quot; Cooper said. &amp;quot;Having full control over the network, compute, and storage layers lets us do really fast build and deploy loops, the kind that allows us to move at &amp;#x27;agentic speed&amp;#x27; while staying 100 percent the smoothest ride in town.&amp;quot;&lt;/p&gt;&lt;p&gt;The approach paid dividends during recent &lt;a href="https://restofworld.org/2026/cloud-outages-2025-global-business-impact/"&gt;widespread outages&lt;/a&gt; that affected major cloud providers — Railway remained online throughout.&lt;/p&gt;&lt;p&gt;This soup-to-nuts control enables pricing that undercuts the hyperscalers by roughly 50 percent and newer cloud startups by three to four times. Railway charges by the second for actual compute usage: $0.00000386 per gigabyte-second of memory, $0.00000772 per vCPU-second, and $0.00000006 per gigabyte-second of storage. There are no charges for idle virtual machines — a stark contrast to the traditional cloud model where customers pay for provisioned capacity whether they use it or not.&lt;/p&gt;&lt;p&gt;&amp;quot;The conventional wisdom is that the big guys have economies of scale to offer better pricing,&amp;quot; Cooper noted. &amp;quot;But when they&amp;#x27;re charging for VMs that usually sit idle in the cloud, and we&amp;#x27;ve purpose-built everything to fit much more density on these machines, you have a big opportunity.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;How 30 employees built a platform generating tens of millions in annual revenue&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; has achieved its scale with a team of just 30 employees generating tens of millions in annual revenue — a ratio of revenue per employee that would be exceptional even for established software companies. The company grew revenue 3.5 times last year and continues to expand at 15 percent month-over-month.&lt;/p&gt;&lt;p&gt;Cooper emphasized that the fundraise was strategic rather than necessary. &amp;quot;We&amp;#x27;re default alive; there&amp;#x27;s no reason for us to raise money,&amp;quot; he said. &amp;quot;We raised because we see a massive opportunity to accelerate, not because we needed to survive.&amp;quot;&lt;/p&gt;&lt;p&gt;The company hired its first salesperson only last year and employs just two solutions engineers. Nearly all of Railway&amp;#x27;s two million users discovered the platform through word of mouth — developers telling other developers about a tool that actually works.&lt;/p&gt;&lt;p&gt;&amp;quot;We basically did the standard engineering thing: if you build it, they will come,&amp;quot; Cooper recalled. &amp;quot;And to some degree, they came.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;From side projects to Fortune 500 deployments: Railway&amp;#x27;s unlikely corporate expansion&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Despite its grassroots developer community, Railway has made significant inroads into large organizations. The company claims that 31 percent of Fortune 500 companies now use its platform, though deployments range from company-wide infrastructure to individual team projects.&lt;/p&gt;&lt;p&gt;Notable customers include &lt;a href="https://www.biltrewards.com/"&gt;Bilt&lt;/a&gt;, the loyalty program company; Intuit&amp;#x27;s &lt;a href="https://www.goco.io/"&gt;GoCo&lt;/a&gt; subsidiary; TripAdvisor&amp;#x27;s &lt;a href="https://www.cruisecritic.com/"&gt;Cruise Critic&lt;/a&gt;; and &lt;a href="https://www.mgmresorts.com/en.html"&gt;MGM Resorts&lt;/a&gt;. &lt;a href="https://www.ycombinator.com/companies/kernel"&gt;Kernel&lt;/a&gt;, a Y Combinator-backed startup providing AI infrastructure to over 1,000 companies, runs its entire customer-facing system on Railway for $444 per month.&lt;/p&gt;&lt;p&gt;&amp;quot;At my previous company Clever, which sold for $500 million, I had six full-time engineers just managing AWS,&amp;quot; said Rafael Garcia, Kernel&amp;#x27;s chief technology officer. &amp;quot;Now I have six engineers total, and they all focus on product. Railway is exactly the tool I wish I had in 2012.&amp;quot;&lt;/p&gt;&lt;p&gt;For enterprise customers, &lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; offers security certifications including SOC 2 Type 2 compliance and HIPAA readiness, with business associate agreements available upon request. The platform provides single sign-on authentication, comprehensive audit logs, and the option to deploy within a customer&amp;#x27;s existing cloud environment through a &amp;quot;bring your own cloud&amp;quot; configuration.&lt;/p&gt;&lt;p&gt;Enterprise pricing starts at custom levels, with specific add-ons for extended log retention ($200 monthly), HIPAA BAAs ($1,000), enterprise support with SLOs ($2,000), and dedicated virtual machines ($10,000).&lt;/p&gt;&lt;h2&gt;&lt;b&gt;The startup&amp;#x27;s bold strategy to take on Amazon, Google, and a new generation of cloud rivals&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Railway enters a crowded market that includes not only the hyperscale cloud providers—Amazon Web Services, Microsoft Azure, and Google Cloud Platform—but also a growing cohort of developer-focused platforms like Vercel, Render, Fly.io, and Heroku.&lt;/p&gt;&lt;p&gt;Cooper argues that Railway&amp;#x27;s competitors fall into two camps, neither of which has fully committed to the new infrastructure model that AI demands.&lt;/p&gt;&lt;p&gt;&amp;quot;The hyperscalers have two competing systems, and they haven&amp;#x27;t gone all-in on the new model because their legacy revenue stream is still printing money,&amp;quot; he observed. &amp;quot;They have this mammoth pool of cash coming from people who provision a VM, use maybe 10 percent of it, and still pay for the whole thing. To what end are they actually interested in going all the way in on a new experience if they don&amp;#x27;t really need to?&amp;quot;&lt;/p&gt;&lt;p&gt;Against startup competitors, Railway differentiates by covering the full infrastructure stack. &amp;quot;We&amp;#x27;re not just containers; we&amp;#x27;ve got VM primitives, stateful storage, virtual private networking, automated load balancing,&amp;quot; Cooper said. &amp;quot;And we wrap all of this in an absurdly easy-to-use UI, with agentic primitives so agents can move 1,000 times faster.&amp;quot;&lt;/p&gt;&lt;p&gt;The platform supports databases including PostgreSQL, MySQL, MongoDB, and Redis; provides up to 256 terabytes of persistent storage with over 100,000 input/output operations per second; and enables deployment to four global regions spanning the United States, Europe, and Southeast Asia. Enterprise customers can scale to 112 vCPUs and 2 terabytes of RAM per service.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Why investors are betting that AI will create a thousand times more software than exists today&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Railway&amp;#x27;s fundraise reflects broader investor enthusiasm for companies positioned to benefit from the AI coding revolution. As tools like &lt;a href="https://github.com/features/copilot"&gt;GitHub Copilot&lt;/a&gt;, &lt;a href="https://cursor.com/agents"&gt;Cursor&lt;/a&gt;, and &lt;a href="https://claude.ai/login"&gt;Claude&lt;/a&gt; become standard fixtures in developer workflows, the volume of code being written — and the infrastructure needed to run it — is expanding dramatically.&lt;/p&gt;&lt;p&gt;&amp;quot;The amount of software that&amp;#x27;s going to come online over the next five years is unfathomable compared to what existed before — we&amp;#x27;re talking a thousand times more software,&amp;quot; Cooper predicted. &amp;quot;All of that has to run somewhere.&amp;quot;&lt;/p&gt;&lt;p&gt;The company has already integrated directly with AI systems, building what Cooper calls &amp;quot;loops where Claude can hook in, call deployments, and analyze infrastructure automatically.&amp;quot; Railway released a Model Context Protocol server in August 2025 that allows AI coding agents to deploy applications and manage infrastructure directly from code editors.&lt;/p&gt;&lt;p&gt;&amp;quot;The notion of a developer is melting before our eyes,&amp;quot; Cooper said. &amp;quot;You don&amp;#x27;t have to be an engineer to engineer things anymore — you just need critical thinking and the ability to analyze things in a systems capacity.&amp;quot;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What Railway plans to do with $100 million and zero marketing experience&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;&lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; plans to use the new capital to expand its global data center footprint, grow its team beyond 30 employees, and build what Cooper described as a proper go-to-market operation for the first time in the company&amp;#x27;s five-year history.&lt;/p&gt;&lt;p&gt;&amp;quot;One of my mentors said you raise money when you can change the trajectory of the business,&amp;quot; Cooper explained. &amp;quot;We&amp;#x27;ve built all the required substrate to scale indefinitely; what&amp;#x27;s been holding us back is simply talking about it. 2026 is the year we play on the world stage.&amp;quot;&lt;/p&gt;&lt;p&gt;The company&amp;#x27;s investor roster reads like a who&amp;#x27;s who of developer infrastructure. Angel investors include &lt;a href="https://tom.preston-werner.com/"&gt;Tom Preston-Werner,&lt;/a&gt; co-founder of GitHub; &lt;a href="https://rauchg.com/about"&gt;Guillermo Rauch&lt;/a&gt;, chief executive of Vercel; &lt;a href="https://www.cockroachlabs.com/author/spencer-kimball/"&gt;Spencer Kimball&lt;/a&gt;, chief executive of Cockroach Labs; &lt;a href="https://www.datadoghq.com/about/leadership/"&gt;Olivier Pomel&lt;/a&gt;, chief executive of Datadog; and &lt;a href="https://sequoiacap.com/founder/jori-lallo/"&gt;Jori Lallo&lt;/a&gt;, co-founder of Linear.&lt;/p&gt;&lt;p&gt;The timing of Railway&amp;#x27;s expansion coincides with what many in Silicon Valley view as a fundamental shift in how software gets made. Coding assistants are no longer experimental curiosities — they have become essential tools that millions of developers rely on daily. Each line of AI-generated code needs somewhere to run, and the incumbents, by Cooper&amp;#x27;s telling, are too wedded to their existing business models to fully capitalize on the moment.&lt;/p&gt;&lt;p&gt;Whether &lt;a href="https://railway.com/"&gt;Railway&lt;/a&gt; can translate developer enthusiasm into sustained enterprise adoption remains an open question. The cloud infrastructure market is littered with promising startups that failed to break the grip of Amazon, Microsoft, and Google. But Cooper, who previously worked as a software engineer at &lt;a href="https://www.wolframalpha.com/"&gt;Wolfram Alpha&lt;/a&gt;, &lt;a href="https://www.bloomberg.com/"&gt;Bloomberg&lt;/a&gt;, and &lt;a href="https://www.uber.com/"&gt;Uber&lt;/a&gt; before founding Railway in 2020, seems unfazed by the scale of his ambition.&lt;/p&gt;&lt;p&gt;&amp;quot;In five years, Railway [will be] the place where software gets created and evolved, period,&amp;quot; he said. &amp;quot;Deploy instantly, scale infinitely, with zero friction. That&amp;#x27;s the prize worth playing for, and there&amp;#x27;s no bigger one on offer.&amp;quot;&lt;/p&gt;&lt;p&gt;For a company that built a $100 million business by doing the opposite of what conventional startup wisdom dictates — no marketing, no sales team, no venture hype—the real test begins now. Railway spent five years proving that developers would find a better mousetrap on their own. The next five will determine whether the rest of the world is ready to get on board.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/infrastructure/railway-secures-usd100-million-to-challenge-aws-with-ai-native-cloud</guid><pubDate>Thu, 22 Jan 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Spotify brings AI-powered Prompted Playlists to the US and Canada (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify is rolling out Prompted Playlists, a new AI playlist creation tool, to Premium subscribers in the U.S. and Canada. The feature, which was originally tested in New Zealand, allows users to make a playlist by describing what they want to hear, in their own words.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prompted Playlists builds on an earlier AI playlist product, launched in 2024, which allowed for simpler prompts like “get focused at work with instrumental electronica,” or “get pumped up with fun, upbeat, and positive songs.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Instead, the new Prompted Playlists feature lets users explain in more detail, and in a conversational mode, what they want to hear.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo to press, for example, Spotify showed off a playlist that was built on a long prompt, which read “Find me one artist I haven’t listened to yet, but would probably love, or an artist I’ve only heard one or two songs from, and introduce me to them. Build a playlist of songs that’ll give me an overview of their catalog so it feels like I’m getting to know them. Put the songs you think I’ll like the most in the top five spots.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea behind the new feature, explains J.J. Italiano, head of Global Music Curation and Discovery at Spotify, is to make it possible for anyone to build a playlist, even if they don’t know much about music curation or the correct words to use.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084170" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Sampled-Favorites.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“For most people,  isn’t a part of their job. You don’t always have the time or the energy to keep building the perfect playlist every time your mood changes, and that’s where prompted playlists will come in,” said Italiano, whose team makes popular Spotify playlists like Today’s Top Hits, New Music Friday, and Rap Caviar, among others. “This gives listeners access to that creative process without needing to know genres, or years, or industry language. You don’t need the right words. You just need your words.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you can describe a feeling, you can make a playlist,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI behind the feature analyzes the world of music in real time, including “trends, charts, culture, and history,” Spotify says, as well as the user’s entire listening history since joining the service.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the playlists are personalized to their creator by default, users can also use the tool to help break out of their usual listening habits and get different recommendations. That is, they can specifically tell the AI &lt;em&gt;not&lt;/em&gt; to use their own listening history as a reference point, or they can direct it to introduce them to songs they’ve never heard before, as in the example above.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084172" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Movie-Tearjerkers.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The prompts don’t have to include any musical terminology, either. For instance, users could ask for a playlist inspired by the weather or a favorite TV show.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Because the prompts are shareable, the feature could also lead to a new type of creator — one who makes AI prompts that others will want to try. While the prompt itself would be the same, each user’s resulting playlist would differ, as it’s personalized to their own tastes and listening history. They could then modify that playlist further, if they chose. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084173" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/All-Time-Favs.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify says Prompted Playlists are the “next evolution” of its earlier AI playlist feature. The new version is more tuned into real-time music trends and culture, understands the full arc of a user’s listening behavior — not just what they’ve listened to recently — and offers deeper control. However, the older AI playlist feature will not be shut down. Instead, the two products will live side-by-side, which could lead to consumer confusion, given the similarities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will have some usage limits in place because it’s still in beta, and those could change over time. It’s also only available in the English language for the time being.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company couldn’t say when Prompted Playlists would reach global subscribers, noting that it first wants to learn from these initial markets to inform future launches.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Spotify is rolling out Prompted Playlists, a new AI playlist creation tool, to Premium subscribers in the U.S. and Canada. The feature, which was originally tested in New Zealand, allows users to make a playlist by describing what they want to hear, in their own words.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Prompted Playlists builds on an earlier AI playlist product, launched in 2024, which allowed for simpler prompts like “get focused at work with instrumental electronica,” or “get pumped up with fun, upbeat, and positive songs.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Instead, the new Prompted Playlists feature lets users explain in more detail, and in a conversational mode, what they want to hear.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a demo to press, for example, Spotify showed off a playlist that was built on a long prompt, which read “Find me one artist I haven’t listened to yet, but would probably love, or an artist I’ve only heard one or two songs from, and introduce me to them. Build a playlist of songs that’ll give me an overview of their catalog so it feels like I’m getting to know them. Put the songs you think I’ll like the most in the top five spots.” &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The idea behind the new feature, explains J.J. Italiano, head of Global Music Curation and Discovery at Spotify, is to make it possible for anyone to build a playlist, even if they don’t know much about music curation or the correct words to use.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084170" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Sampled-Favorites.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“For most people,  isn’t a part of their job. You don’t always have the time or the energy to keep building the perfect playlist every time your mood changes, and that’s where prompted playlists will come in,” said Italiano, whose team makes popular Spotify playlists like Today’s Top Hits, New Music Friday, and Rap Caviar, among others. “This gives listeners access to that creative process without needing to know genres, or years, or industry language. You don’t need the right words. You just need your words.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you can describe a feeling, you can make a playlist,” he added.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The AI behind the feature analyzes the world of music in real time, including “trends, charts, culture, and history,” Spotify says, as well as the user’s entire listening history since joining the service.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the playlists are personalized to their creator by default, users can also use the tool to help break out of their usual listening habits and get different recommendations. That is, they can specifically tell the AI &lt;em&gt;not&lt;/em&gt; to use their own listening history as a reference point, or they can direct it to introduce them to songs they’ve never heard before, as in the example above.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084172" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/Movie-Tearjerkers.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The prompts don’t have to include any musical terminology, either. For instance, users could ask for a playlist inspired by the weather or a favorite TV show.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Because the prompts are shareable, the feature could also lead to a new type of creator — one who makes AI prompts that others will want to try. While the prompt itself would be the same, each user’s resulting playlist would differ, as it’s personalized to their own tastes and listening history. They could then modify that playlist further, if they chose. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3084173" height="383" src="https://techcrunch.com/wp-content/uploads/2026/01/All-Time-Favs.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify says Prompted Playlists are the “next evolution” of its earlier AI playlist feature. The new version is more tuned into real-time music trends and culture, understands the full arc of a user’s listening behavior — not just what they’ve listened to recently — and offers deeper control. However, the older AI playlist feature will not be shut down. Instead, the two products will live side-by-side, which could lead to consumer confusion, given the similarities.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The feature will have some usage limits in place because it’s still in beta, and those could change over time. It’s also only available in the English language for the time being.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company couldn’t say when Prompted Playlists would reach global subscribers, noting that it first wants to learn from these initial markets to inform future launches.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/spotify-brings-ai-powered-prompted-playlists-to-the-u-s-and-canada/</guid><pubDate>Thu, 22 Jan 2026 14:00:00 +0000</pubDate></item><item><title>[NEW] Flight Controls Are Cleared for Takeoff on GeForce NOW (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The wait is over, pilots. Flight control support — one of the most community-requested features for GeForce NOW — is live starting today, following its announcement at CES earlier this month.&lt;/p&gt;
&lt;p&gt;Virtual captains can now bring dedicated flight gear into the cloud and feel every roll, yaw and throttle change with even more precision, starting with the Thrustmaster T.Flight HOTAS One.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;Time is running out on our  T.Flight Hotas One MSFS Edition + 1 month of @NVIDIAGFN Ultimate giveaway! ⏰&lt;/p&gt;
&lt;p&gt;How to enter 👇&lt;br /&gt;1. Follow both @TMThrustmaster and @NVIDIAGFN accounts on social&lt;br /&gt;2. Repost this and share on your feed&lt;/p&gt;
&lt;p&gt;🎁 5 winners will be chosen – ends 1/24. pic.twitter.com/B89mGwJoNQ&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) January 19, 2026&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Remember to enter the giveaway for a chance to score a T.Flight Hotas One MSFS Edition and one month of a GeForce NOW Ultimate membership — follow Thrustmaster and GeForce NOW, and repost the giveaway post to enter before Saturday, Jan. 24.&lt;/p&gt;
&lt;p&gt;Also on the horizon, &lt;i&gt;Delta Force&lt;/i&gt; from Team Jade (TiMi Studio Group) is coming soon to GeForce NOW, adding another big-name title to the cloud lineup. Get ready for all the action by streaming the four new games in this cloud this week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Pilots Wanted&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Members can now jump into their favorite flight and space simulation games with a full stick-and-throttle setup streamed right from the cloud. It’s all about dialing in that authentic, hands-on flying experience while keeping latency low and gameplay responsive on GeForce NOW.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89331"&gt;&lt;img alt="flight stick row in the app on geforce now" class="wp-image-89331 size-large" height="678" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/image-1-1680x678.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89331"&gt;&lt;em&gt;Full throttle in the cloud.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To get right to flying, members can look for a dedicated row in the GeForce NOW app highlighting games that support flight controls, making it simple to spot great titles for putting their new setup through its paces.&lt;/p&gt;
&lt;p&gt;This initial flight control support rollout is just the beginning, with plans to keep tuning the experience and expand compatibility to more peripherals. Fire up the rig, plug in the Thrustmaster T.Flight HOTAS One and get ready to take off — the cloud cockpit is open and ready for flight.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Your Next Mission: ‘Delta Force’&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Team Jade’s &lt;i&gt;Delta Force&lt;/i&gt; is coming soon to the cloud, ready to drop players into high-stakes extraction with an all-out warfare mode where coordination and precision matter just as much as raw firepower. When it launches on GeForce NOW, members will be able to jump into the action instantly from almost any device, taking advantage of high‑performance streaming to stay locked at smooth frame rates even in the most chaotic firefights.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89315"&gt;&lt;img alt="Delta Force on GeForce NOW" class="size-large wp-image-89315" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/GFN_Thursday-Delta_Force-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89315"&gt;&lt;em&gt;Get ready to squad up, Delta Force is inbound soon.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;On GeForce NOW, every long‑range shot, helicopter insertion and tense objective push can feel crisp and responsive, whether playing on low‑powered laptops, Macs or mobile devices. The cloud makes it easy to squad up, drop in and get straight to the mission without worrying about downloads, patches or local hardware.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;New in the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89318"&gt;&lt;img alt="MIO on GeForce NOW" class="size-large wp-image-89318" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/GFN_Thursday-MIO_Memories_In_Orbit-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89318"&gt;&lt;em&gt;A graceful little robot having the worst day in the universe.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;MIO: Memories in Orbit&lt;/i&gt; is a neon-tinged metroidvania about a nimble little robot waking up on a massive, overgrown ark called the Vessel with nothing but fractured memories and a whole lot of trouble coming its way. Dart through low-gravity corridors, chain together elegant wall-runs, glides and grapples, and tangle with rogue machines as the Vessel itself quietly steals the spotlight — moody, decaying and just alive enough to keep a few secrets.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;MIO: Memories in Orbit &lt;/i&gt;(New release on Steam and Xbox, available on Game Pass, Jan. 20)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Bladesong &lt;/i&gt;(New release on Steam, Jan. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Rustler &lt;/i&gt;(New release on Epic Games Store, free starting Jan. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;The Gold River Project &lt;/i&gt;(New release on Steam, Jan. 23, GeForce RTX 5080-ready)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;If you could fly to any video game destination, where would it be? 🛫&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) January 21, 2026&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;The wait is over, pilots. Flight control support — one of the most community-requested features for GeForce NOW — is live starting today, following its announcement at CES earlier this month.&lt;/p&gt;
&lt;p&gt;Virtual captains can now bring dedicated flight gear into the cloud and feel every roll, yaw and throttle change with even more precision, starting with the Thrustmaster T.Flight HOTAS One.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;Time is running out on our  T.Flight Hotas One MSFS Edition + 1 month of @NVIDIAGFN Ultimate giveaway! ⏰&lt;/p&gt;
&lt;p&gt;How to enter 👇&lt;br /&gt;1. Follow both @TMThrustmaster and @NVIDIAGFN accounts on social&lt;br /&gt;2. Repost this and share on your feed&lt;/p&gt;
&lt;p&gt;🎁 5 winners will be chosen – ends 1/24. pic.twitter.com/B89mGwJoNQ&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) January 19, 2026&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Remember to enter the giveaway for a chance to score a T.Flight Hotas One MSFS Edition and one month of a GeForce NOW Ultimate membership — follow Thrustmaster and GeForce NOW, and repost the giveaway post to enter before Saturday, Jan. 24.&lt;/p&gt;
&lt;p&gt;Also on the horizon, &lt;i&gt;Delta Force&lt;/i&gt; from Team Jade (TiMi Studio Group) is coming soon to GeForce NOW, adding another big-name title to the cloud lineup. Get ready for all the action by streaming the four new games in this cloud this week.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Pilots Wanted&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Members can now jump into their favorite flight and space simulation games with a full stick-and-throttle setup streamed right from the cloud. It’s all about dialing in that authentic, hands-on flying experience while keeping latency low and gameplay responsive on GeForce NOW.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89331"&gt;&lt;img alt="flight stick row in the app on geforce now" class="wp-image-89331 size-large" height="678" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/image-1-1680x678.png" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89331"&gt;&lt;em&gt;Full throttle in the cloud.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;To get right to flying, members can look for a dedicated row in the GeForce NOW app highlighting games that support flight controls, making it simple to spot great titles for putting their new setup through its paces.&lt;/p&gt;
&lt;p&gt;This initial flight control support rollout is just the beginning, with plans to keep tuning the experience and expand compatibility to more peripherals. Fire up the rig, plug in the Thrustmaster T.Flight HOTAS One and get ready to take off — the cloud cockpit is open and ready for flight.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Your Next Mission: ‘Delta Force’&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Team Jade’s &lt;i&gt;Delta Force&lt;/i&gt; is coming soon to the cloud, ready to drop players into high-stakes extraction with an all-out warfare mode where coordination and precision matter just as much as raw firepower. When it launches on GeForce NOW, members will be able to jump into the action instantly from almost any device, taking advantage of high‑performance streaming to stay locked at smooth frame rates even in the most chaotic firefights.&lt;/p&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89315"&gt;&lt;img alt="Delta Force on GeForce NOW" class="size-large wp-image-89315" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/GFN_Thursday-Delta_Force-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89315"&gt;&lt;em&gt;Get ready to squad up, Delta Force is inbound soon.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;On GeForce NOW, every long‑range shot, helicopter insertion and tense objective push can feel crisp and responsive, whether playing on low‑powered laptops, Macs or mobile devices. The cloud makes it easy to squad up, drop in and get straight to the mission without worrying about downloads, patches or local hardware.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;New in the Cloud&lt;/b&gt;&lt;/h2&gt;
&lt;figure class="wp-caption aligncenter" id="attachment_89318"&gt;&lt;img alt="MIO on GeForce NOW" class="size-large wp-image-89318" height="840" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/GFN_Thursday-MIO_Memories_In_Orbit-1680x840.jpg" width="1680" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89318"&gt;&lt;em&gt;A graceful little robot having the worst day in the universe.&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;&lt;i&gt;MIO: Memories in Orbit&lt;/i&gt; is a neon-tinged metroidvania about a nimble little robot waking up on a massive, overgrown ark called the Vessel with nothing but fractured memories and a whole lot of trouble coming its way. Dart through low-gravity corridors, chain together elegant wall-runs, glides and grapples, and tangle with rogue machines as the Vessel itself quietly steals the spotlight — moody, decaying and just alive enough to keep a few secrets.&lt;/p&gt;
&lt;p&gt;In addition, members can look for the following:&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;i&gt;MIO: Memories in Orbit &lt;/i&gt;(New release on Steam and Xbox, available on Game Pass, Jan. 20)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Bladesong &lt;/i&gt;(New release on Steam, Jan. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;Rustler &lt;/i&gt;(New release on Epic Games Store, free starting Jan. 22)&lt;/li&gt;
&lt;li&gt;&lt;i&gt;The Gold River Project &lt;/i&gt;(New release on Steam, Jan. 23, GeForce RTX 5080-ready)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What are you planning to play this weekend? Let us know on X or in the comments below.&lt;/p&gt;
&lt;blockquote class="twitter-tweet"&gt;
&lt;p dir="ltr" lang="en"&gt;If you could fly to any video game destination, where would it be? 🛫&lt;/p&gt;
&lt;p&gt;— 🌩️ NVIDIA GeForce NOW (@NVIDIAGFN) January 21, 2026&lt;/p&gt;&lt;/blockquote&gt;


		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/</guid><pubDate>Thu, 22 Jan 2026 14:00:53 +0000</pubDate></item><item><title>[NEW] From Pilot to Profit: Survey Reveals the Financial Services Industry Is Doubling Down on AI Investment and Open Source (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI has taken center stage in financial services, automating the research and execution behind algorithmic trading and helping banks more accurately detect fraud and money laundering — all while improving risk management practices and expediting document processing.&lt;/p&gt;
&lt;p&gt;The sixth annual “NVIDIA State of AI in Financial Services” report, based on a survey of more than 800 industry professionals, found that AI usage in the industry has never been higher.&lt;/p&gt;
&lt;p&gt;Organizations are deploying and scaling AI use cases, such as fraud detection, risk management and customer service, to improve critical business functions that create meaningful return on investment. New types of AI — including AI agents — are streamlining processes ranging from back-office operations to investment research as financial institutions embrace the tools needed to build specialized AI, including open source foundation models and software.&lt;/p&gt;
&lt;p&gt;Highlights from this year’s report include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;89% said AI is helping increase annual revenue and decrease annual costs.&lt;/li&gt;
&lt;li&gt;73% of executives said AI is crucial to their future success, and nearly 100% said their AI budgets will increase or stay the same in the next year.&lt;/li&gt;
&lt;li&gt;65% of respondents said their company is actively using AI, up from 45% in last year’s report.&lt;/li&gt;
&lt;li&gt;61% are using or assessing generative AI, up 52% year over year.&lt;/li&gt;
&lt;li&gt;84% said open source models and software are important to their AI strategy.&lt;/li&gt;
&lt;li&gt;42% are using or assessing agentic AI, with 21% saying they’ve already deployed AI agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“Open source models are fundamentally changing the competitive dynamics in financial AI,” said Helen Yu, CEO of Tigon Advisory Corp. “The real value capture happens when institutions fine-tune these models on their proprietary transaction data, customer interaction histories and market intelligence, creating AI capabilities that competitors cannot replicate.”&lt;/p&gt;
&lt;p&gt;Read more below on some of the report’s key findings.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Building the Foundation of the Future With Open Source&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Open source models allow for flexibility and efficiency, enabling organizations to tailor development tools to their unique needs and make them more accurate by incorporating a financial institution’s proprietary data. As a result, 83% percent of respondents said open source is important to their organization’s AI strategy, with 43% saying it is very to extremely important.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-89244 size-medium" height="233" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/finance-question-21-importance-of-open-source-by-role-white-960x233.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;“Open source models can help banks close the gap with early movers, unlock cost efficiencies and safeguard against vendor lock-in, but they’re not without their limitations — proprietary approaches can unlock superior performance for domain-specific tasks,” said Alexandra Mousavizadeh, cofounder and co-CEO of Evident Insights. “Leading banks need to demonstrate proficiency in both approaches — applying the right kind of model to the right problem, in the right context.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;The Return on Investment of AI in Financial Services Is Clear&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Financial institutions have moved from piloting AI projects to deploying solutions that create business impact and scaling them across the organization. In turn, companies have begun to see significant return on investment from AI on the top and bottom lines.&lt;/p&gt;
&lt;p&gt;As stated above, 89% of survey respondents said AI has helped increase annual revenue and decrease annual costs. For many organizations, the impact has been significant, with 64% of respondents saying AI has helped increase annual revenue by more than 5% — including 29% who said revenue increased more than 10%.&lt;/p&gt;
&lt;p&gt;Similarly, 61% said AI had helped decrease annual costs by more than 5%, with 25% saying costs decreased more than 10%.&lt;/p&gt;
&lt;p&gt;Respondents cited a long list of AI use cases that have provided return on investment, including document processing and management, customer experience and engagement, algorithmic trading and risk management.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-89241 size-medium" height="337" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/finance-question-19-impact-on-business-operations-industry-top3-white-960x337.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Creating operational efficiencies is the largest improvement AI has made in financial services, according to 52% of respondents. And 48% said employee productivity was among the biggest improvements.&lt;/p&gt;
&lt;p&gt;“The most tangible ROI I’m seeing is in payment operations, specifically authorization optimization and intelligent routing,” said Dwayne Gefferie, payments strategist at Gefferie Group. “Agentic AI systems can now autonomously route transactions to the most optimized payment networks, dynamically adjust retry logic based on real-time issuer signals and make routing decisions under 200-millisecond routing that traditional rule-based systems simply can’t match. What makes this compelling is that every basis point improvement in authorization rates translates directly to revenue — there’s no ambiguity in measurement.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Success Leads to Increasing AI Budgets&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Given the shift from running proof of concepts to deploying AI-enabled applications into production, the financial services industry is looking to significantly expand AI budgets. Nearly 100% of respondents said their AI budgets would increase or stay the same in the coming year.&lt;/p&gt;
&lt;p&gt;About 41% of respondents said investment would go toward optimizing AI workflows and production, reinvesting in and improving the AI solutions that are already working.&lt;/p&gt;
&lt;p&gt;More than a third (34%) said they had an eye toward AI expansion in their organizations, with spending focused on identifying additional use cases. And 30% said that investment would focus on building or providing more access to AI infrastructure, such as on-premises installations or in the cloud.&lt;/p&gt;
&lt;p&gt;Investment will also flow to deployment and expansion of AI agents, which are advanced AI systems designed to autonomously reason, plan and execute complex tasks based on high-level goals. About 21% of respondents said AI agents have already been deployed, with another 22% saying AI agents will be deployed within the next year and beyond.&lt;/p&gt;
&lt;p&gt;“The institutions winning in AI are treating their proprietary data as a strategic asset for building differentiated AI products,” said Yu.&lt;/p&gt;
&lt;p&gt;Download the “State of AI in Financial Services: 2026 Trends” report for in-depth results and insights.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore &lt;/i&gt;&lt;i&gt;NVIDIA’s AI solutions and enterprise-level AI platforms for financial services&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI has taken center stage in financial services, automating the research and execution behind algorithmic trading and helping banks more accurately detect fraud and money laundering — all while improving risk management practices and expediting document processing.&lt;/p&gt;
&lt;p&gt;The sixth annual “NVIDIA State of AI in Financial Services” report, based on a survey of more than 800 industry professionals, found that AI usage in the industry has never been higher.&lt;/p&gt;
&lt;p&gt;Organizations are deploying and scaling AI use cases, such as fraud detection, risk management and customer service, to improve critical business functions that create meaningful return on investment. New types of AI — including AI agents — are streamlining processes ranging from back-office operations to investment research as financial institutions embrace the tools needed to build specialized AI, including open source foundation models and software.&lt;/p&gt;
&lt;p&gt;Highlights from this year’s report include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;89% said AI is helping increase annual revenue and decrease annual costs.&lt;/li&gt;
&lt;li&gt;73% of executives said AI is crucial to their future success, and nearly 100% said their AI budgets will increase or stay the same in the next year.&lt;/li&gt;
&lt;li&gt;65% of respondents said their company is actively using AI, up from 45% in last year’s report.&lt;/li&gt;
&lt;li&gt;61% are using or assessing generative AI, up 52% year over year.&lt;/li&gt;
&lt;li&gt;84% said open source models and software are important to their AI strategy.&lt;/li&gt;
&lt;li&gt;42% are using or assessing agentic AI, with 21% saying they’ve already deployed AI agents.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“Open source models are fundamentally changing the competitive dynamics in financial AI,” said Helen Yu, CEO of Tigon Advisory Corp. “The real value capture happens when institutions fine-tune these models on their proprietary transaction data, customer interaction histories and market intelligence, creating AI capabilities that competitors cannot replicate.”&lt;/p&gt;
&lt;p&gt;Read more below on some of the report’s key findings.&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Building the Foundation of the Future With Open Source&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Open source models allow for flexibility and efficiency, enabling organizations to tailor development tools to their unique needs and make them more accurate by incorporating a financial institution’s proprietary data. As a result, 83% percent of respondents said open source is important to their organization’s AI strategy, with 43% saying it is very to extremely important.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-89244 size-medium" height="233" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/finance-question-21-importance-of-open-source-by-role-white-960x233.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;“Open source models can help banks close the gap with early movers, unlock cost efficiencies and safeguard against vendor lock-in, but they’re not without their limitations — proprietary approaches can unlock superior performance for domain-specific tasks,” said Alexandra Mousavizadeh, cofounder and co-CEO of Evident Insights. “Leading banks need to demonstrate proficiency in both approaches — applying the right kind of model to the right problem, in the right context.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;The Return on Investment of AI in Financial Services Is Clear&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Financial institutions have moved from piloting AI projects to deploying solutions that create business impact and scaling them across the organization. In turn, companies have begun to see significant return on investment from AI on the top and bottom lines.&lt;/p&gt;
&lt;p&gt;As stated above, 89% of survey respondents said AI has helped increase annual revenue and decrease annual costs. For many organizations, the impact has been significant, with 64% of respondents saying AI has helped increase annual revenue by more than 5% — including 29% who said revenue increased more than 10%.&lt;/p&gt;
&lt;p&gt;Similarly, 61% said AI had helped decrease annual costs by more than 5%, with 25% saying costs decreased more than 10%.&lt;/p&gt;
&lt;p&gt;Respondents cited a long list of AI use cases that have provided return on investment, including document processing and management, customer experience and engagement, algorithmic trading and risk management.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-89241 size-medium" height="337" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/finance-question-19-impact-on-business-operations-industry-top3-white-960x337.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;Creating operational efficiencies is the largest improvement AI has made in financial services, according to 52% of respondents. And 48% said employee productivity was among the biggest improvements.&lt;/p&gt;
&lt;p&gt;“The most tangible ROI I’m seeing is in payment operations, specifically authorization optimization and intelligent routing,” said Dwayne Gefferie, payments strategist at Gefferie Group. “Agentic AI systems can now autonomously route transactions to the most optimized payment networks, dynamically adjust retry logic based on real-time issuer signals and make routing decisions under 200-millisecond routing that traditional rule-based systems simply can’t match. What makes this compelling is that every basis point improvement in authorization rates translates directly to revenue — there’s no ambiguity in measurement.”&lt;/p&gt;
&lt;h2&gt;&lt;strong&gt;Success Leads to Increasing AI Budgets&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Given the shift from running proof of concepts to deploying AI-enabled applications into production, the financial services industry is looking to significantly expand AI budgets. Nearly 100% of respondents said their AI budgets would increase or stay the same in the coming year.&lt;/p&gt;
&lt;p&gt;About 41% of respondents said investment would go toward optimizing AI workflows and production, reinvesting in and improving the AI solutions that are already working.&lt;/p&gt;
&lt;p&gt;More than a third (34%) said they had an eye toward AI expansion in their organizations, with spending focused on identifying additional use cases. And 30% said that investment would focus on building or providing more access to AI infrastructure, such as on-premises installations or in the cloud.&lt;/p&gt;
&lt;p&gt;Investment will also flow to deployment and expansion of AI agents, which are advanced AI systems designed to autonomously reason, plan and execute complex tasks based on high-level goals. About 21% of respondents said AI agents have already been deployed, with another 22% saying AI agents will be deployed within the next year and beyond.&lt;/p&gt;
&lt;p&gt;“The institutions winning in AI are treating their proprietary data as a strategic asset for building differentiated AI products,” said Yu.&lt;/p&gt;
&lt;p&gt;Download the “State of AI in Financial Services: 2026 Trends” report for in-depth results and insights.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Explore &lt;/i&gt;&lt;i&gt;NVIDIA’s AI solutions and enterprise-level AI platforms for financial services&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/</guid><pubDate>Thu, 22 Jan 2026 14:00:54 +0000</pubDate></item><item><title>[NEW] How to Get Started With Visual Generative AI on NVIDIA RTX PCs (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/</link><description>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI-powered content generation is now embedded in everyday tools like Adobe and Canva, with a slew of agencies and studios incorporating the technology into their workflows. Image models now deliver photorealistic results consistently, video models can generate long and coherent clips, and both can follow creative directions.&lt;/p&gt;
&lt;p&gt;Creators are increasingly running these workflows locally on PCs to keep assets under direct control, remove cloud service costs and eliminate the friction of iteration — making it easier to refine outputs at the pace real creative projects demand.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Since their inception, NVIDIA RTX PCs have been the system of choice for running creative AI due to their high performance — reducing iteration time — and the fact that users can run models on them for free, removing token anxiety.&lt;/p&gt;
&lt;p&gt;With recent RTX optimizations and new open-weight models introduced at CES earlier this month, creatives can work faster, more efficiently and with far greater creative control.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How to Get Started&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Getting started with visual generative AI can feel complex and limiting. Online AI generators are easy to use but offer limited control.&lt;/p&gt;
&lt;p&gt;Open source community tools like ComfyUI simplify setting up advanced creative workflows and are easy to install. They also provide an easy way to download the latest and greatest models, such as FLUX.2 and LTX-2, as well as top community workflows.&lt;/p&gt;
&lt;p&gt;Here’s how to get started with visual generative AI locally on RTX PCs using ComfyUI and popular models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Visit comfy.org to download and install ComfyUI for Windows.&lt;/li&gt;
&lt;li&gt;Launch ComfyUI.&lt;/li&gt;
&lt;li&gt;Create an initial image using the starter template:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-89353" height="777" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/comfyui-template-image-960x777.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Click on the “Templates” button, then on “Getting Started” and choose “1.1 Starter – Text to Image.”&lt;/li&gt;
&lt;li&gt;Connect the model “Node” to the “Save Image Node.” The nodes work in a pipeline to generate content using AI.&lt;/li&gt;
&lt;li&gt;Press the blue “Run” button and watch the green “Node” highlight as the RTX-powered PC generates its first image.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Change the prompt and run it again to enter more deeply into the creative world of visual generative AI.&lt;/p&gt;
&lt;p&gt;Read more below on how to dive into additional ComfyUI templates that use more advanced image and video models.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Model Sizes and GPUs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As users get more familiar with ComfyUI and the models that support it, they’ll need to consider GPU VRAM capacity and whether a model will fit within it. Here are some examples for getting started, depending on GPU VRAM:&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89347"&gt;&lt;img alt="alt" class="size-medium wp-image-89347" height="189" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/gpu-vram-image-gen-video-3d-chart-comparison-960x189.png" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89347"&gt;*Use FP4 models with NVIDIA GeForce RTX 50 Series GPUs, and FP8 models with RTX 40 Series GPUs for best results. This lets models use less VRAM while providing more performance.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Generating Images&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-89359 size-medium" height="768" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/2026-01-21-image-gen-tiger-960x768.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;To explore how to improve image generation quality using FLUX.2-Dev:&lt;/p&gt;
&lt;p&gt;From the ComfyUI “Templates” section, click on “All Templates” and search for “FLUX.2 Dev Text to Image.” Select it, and ComfyUI will load the collection of connected nodes, or “Workflow.”&lt;/p&gt;
&lt;p&gt;FLUX.2-Dev has model weights that will need to be downloaded.&lt;/p&gt;
&lt;p&gt;Model weights are the “knowledge” inside an AI model — think of them like the synapses in a brain. When an image generation model like FLUX.2 was trained, it learned patterns from millions of images. Those patterns are stored as billions of numerical values called “weights.”&lt;/p&gt;
&lt;p&gt;ComfyUI doesn’t come with these weights built in. Instead, it downloads them on demand from repositories like Hugging Face. These files are large (FLUX.2 can be &amp;gt;30GB depending on the version), which is why systems need enough storage and download time to grab them.&lt;/p&gt;
&lt;p&gt;A dialog will appear to guide users through downloading the model weights. The weight files (filename.safetensors) are automatically saved to the correct ComfyUI folder on a user’s PC.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-89350" height="777" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/2026-01-21-missing-models-screenshot-960x777.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Saving Workflows:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Now that the model weights are downloaded, the next step is to save this newly downloaded template as a “Workflow.”&lt;/p&gt;
&lt;p&gt;Users can click on the top-left hamburger menu (three lines) and choose “Save.” The workflow is now saved in the user’s list of “Workflows” (press W to show or hide the window). Close the tab to exit the workflow without losing any work.&lt;/p&gt;
&lt;p&gt;If the download dialog was accidentally closed before the model weights finished downloading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press W to quickly open the “Workflows” window.&lt;/li&gt;
&lt;li&gt;Select the Workflow and ComfyUI will load it. This will also prompt for any missing model weights to download.&lt;/li&gt;
&lt;li&gt;ComfyUI is now ready to generate an image using FLUX.2-Dev.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Prompt Tips for FLUX.2-Dev:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with clear, concrete descriptions of the subject, setting, style and mood — for example: “Cinematic closeup of a vintage race car in the rain, neon reflections on wet asphalt, high contrast, 35mm photography.” Short‑to‑medium length prompts&amp;nbsp; — a single, focused sentence or two — are usually easier to control than long, storylike prompts, especially when getting started.&lt;/li&gt;
&lt;li&gt;Add constraints to guide consistency and quality. Specify things like:
&lt;ul&gt;
&lt;li&gt;Framing (“wide shot” or “portrait”)&lt;/li&gt;
&lt;li&gt;Detail level (“high detail, sharp focus”)&lt;/li&gt;
&lt;li&gt;Realism (“photorealistic” or “stylized illustration”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If results are too busy, remove adjectives instead of adding more.&lt;/li&gt;
&lt;li&gt;Avoid negative prompting — stick to prompting what’s desired.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more about FLUX.2 prompting in this guide from Black Forest Labs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Save Locations on Disk:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Once done refining the image, right click on “Save Image Node” to open the image in a browser, or save it in a new location.&lt;/p&gt;
&lt;p&gt;ComfyUI’s default output folders are typically the following, based on the application type and OS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows (Standalone/Portable Version): The folder is usually found in C:\ComfyUI\output or a similar path within where the program was unzipped.&lt;/li&gt;
&lt;li&gt;Windows (Desktop Application): The path is usually located within the AppData directory, like: C:\Users\%username%\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\output&lt;/li&gt;
&lt;li&gt;Linux: The installation location defaults to ~/.config/ComfyUI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;Prompting Videos&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Explore how to improve video generation quality, using the new LTX-2 model as an example:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Lightrick’s LTX‑2 is an advanced audio-video model designed for controllable, storyboard-style video generation in ComfyUI. Once the LTX‑2 Image to Video Template and model weights are downloaded, start by treating the prompt like a short shot description, rather than a full movie script.&lt;/p&gt;
&lt;p&gt;Unlike the first two Templates, LTX‑2 Image to Video combines an image and a text prompt to generate video.&lt;/p&gt;
&lt;p&gt;Users can take one of the images generated in FLUX.2-Dev and add a text prompt to give it life.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Prompt Tips for LTX‑2:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For best results in ComfyUI, write a single flowing paragraph in the present tense or use a simple, script‑style format with scene headings (sluglines), action, character names and dialogue. Aim for four to six descriptive sentences that cover all the key aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Establish the shot and scene (wide/medium/closeup, lighting, color, textures, atmosphere).&lt;/li&gt;
&lt;li&gt;Describe the action as a clear sequence, define characters with visible traits and body language, and specify camera moves.&lt;/li&gt;
&lt;li&gt;Lastly, add audio, such as ambient sound, music and dialogue, using quotation marks.&lt;/li&gt;
&lt;li&gt;Match the level of detail to the shot scale. For example, closeups need more precise character and texture detail than wide shots. Be clear on how the camera relates to the subject, not just where it moves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional details to consider adding to prompts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Camera movement language:&lt;/b&gt; Specify directions like “slow dolly in,” “handheld tracking,” “over‑the‑shoulder shot,” “pans across,” “tilts upward,” “pushes in,” “pulls back” or “static frame.”&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Shot types:&lt;/b&gt; Specify wide, medium or close‑ups with thoughtful lighting, shallow depth of field and natural motion.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Pacing:&lt;/b&gt; Direct for slow motion, time‑lapses, lingering shots, continuous shots, freeze frames or seamless transitions that shape rhythm and tone.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Atmosphere:&lt;/b&gt; Add details like fog, mist, rain, golden hour light, reflections and rich surface textures that ground the scene.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Style:&lt;/b&gt; Early in the prompt, specify styles like painterly, film noir, analog film, stop‑motion, pixelated edges, fashion editorial or surreal.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Lighting&lt;/b&gt;: Direct backlighting, specific color palettes, soft rim light, lens flares or other lighting details using specific language.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Emotions&lt;/b&gt;: Focus on prompting for single‑subject performances with clear facial expressions and small gestures.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Voice and audio&lt;/b&gt;: Prompt characters to speak or sing in different languages, supported by clear ambient sound descriptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Optimizing VRAM Usage and Image Quality&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;As a frontier model, LTX-2 uses significant amounts of video memory (VRAM) to deliver quality results. Memory use goes up as resolution, frame rates, length or steps increase.&lt;/p&gt;
&lt;p&gt;ComfyUI and NVIDIA have collaborated to optimize a weight streaming feature that allows users to offload parts of the workflow to system memory if their GPU runs out of VRAM — but this comes at a cost in performance.&lt;/p&gt;
&lt;p&gt;Depending on the GPU and use case, users may want to constrain these factors to ensure reasonable generation times.&lt;/p&gt;
&lt;p&gt;LTX-2 is an incredibly advanced model — but as with any model, tweaking the settings has a big impact on quality.&lt;/p&gt;
&lt;p&gt;Learn more about optimizing LTX-2 usage with RTX GPUs in the Quick Start Guide for LTX-2 In ComfyUI.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Building a Custom Workflow With FLUX.2-Dev and LTX-2&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Users can simplify the process of hopping between ComfyUI Workflows with FLUX.2-Dev to generate an image, finding it on disk and adding it as an image prompt to the LTX-2 Image to Video Workflow by combining the models into a new workflow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open the saved FLUX.2-Dev Text to Image Workflow.&lt;/li&gt;
&lt;li&gt;Ctrl+left mouse click the FLUX.2-Dev Text to Image node.&lt;/li&gt;
&lt;li&gt;In the LTX-2 Image to Video Workflow, paste the node using Ctrl+V.&lt;/li&gt;
&lt;li&gt;Simply hover over the FLUX.2-Dev Text to Image node IMAGE dot, left click and drag to the Resize Image/Mask Input dot. A blue connector will appear.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Save with a new name, and text prompt for image and video in one workflow.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Advanced 3D Generation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond generating images with FLUX.2 and videos with LTX‑2, the next step is adding 3D guidance. The NVIDIA Blueprint for 3D-guided generative AI shows how to use 3D scenes and assets to drive more controllable, production-style image and video pipelines on RTX PCs — with ready-made workflows users can inspect, tweak and extend.&lt;/p&gt;
&lt;p&gt;Creators can show off their work, connect with other users and find help on the Stable Diffusion subreddit and ComfyUI Discord.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI — The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;💻&lt;b&gt;NVIDIA @ CES 2026&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;CES announcements included 4K AI video generation acceleration on PCs with LTX-2 and ComfyUI upgrades. Plus, major RTX accelerations across ComfyUI, LTX-2, Llama.cpp, Ollama, Hyperlink and more unlock video, image and text generation use cases on AI PCs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;📝 Black Forest Labs FLUX 2 Variants&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;FLUX.2 [klein] is a set of compact, ultrafast models that support both image generation and editing, delivering state-of-the-art image quality. The models are accelerated by NVFP4 and NVFP8, boosting speed by up to 2.5x and enabling them to run performantly across a wide range of RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;✨Project&lt;/b&gt; &lt;b&gt;G-Assist Update&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;With a new “Reasoning Mode” enabled by default, Project G-Assist gains an accuracy and intelligence boost, as well as the ability to action multiple commands at once. G-Assist can now control settings on G-SYNC monitors, CORSAIR peripherals and CORSAIR PC components through iCUE — covering lighting, profiles, performance and cooling.&lt;/p&gt;
&lt;p&gt;Support is also coming soon to Elgato Stream Decks, bringing G-Assist closer to a unified AI interface for tuning and controlling nearly any system. For G-Assist plug-in devs, a new Cursor-based plug-in builder accelerates development using Cursor’s agentic coding environment.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;AI-powered content generation is now embedded in everyday tools like Adobe and Canva, with a slew of agencies and studios incorporating the technology into their workflows. Image models now deliver photorealistic results consistently, video models can generate long and coherent clips, and both can follow creative directions.&lt;/p&gt;
&lt;p&gt;Creators are increasingly running these workflows locally on PCs to keep assets under direct control, remove cloud service costs and eliminate the friction of iteration — making it easier to refine outputs at the pace real creative projects demand.&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Since their inception, NVIDIA RTX PCs have been the system of choice for running creative AI due to their high performance — reducing iteration time — and the fact that users can run models on them for free, removing token anxiety.&lt;/p&gt;
&lt;p&gt;With recent RTX optimizations and new open-weight models introduced at CES earlier this month, creatives can work faster, more efficiently and with far greater creative control.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;How to Get Started&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Getting started with visual generative AI can feel complex and limiting. Online AI generators are easy to use but offer limited control.&lt;/p&gt;
&lt;p&gt;Open source community tools like ComfyUI simplify setting up advanced creative workflows and are easy to install. They also provide an easy way to download the latest and greatest models, such as FLUX.2 and LTX-2, as well as top community workflows.&lt;/p&gt;
&lt;p&gt;Here’s how to get started with visual generative AI locally on RTX PCs using ComfyUI and popular models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Visit comfy.org to download and install ComfyUI for Windows.&lt;/li&gt;
&lt;li&gt;Launch ComfyUI.&lt;/li&gt;
&lt;li&gt;Create an initial image using the starter template:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-89353" height="777" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/comfyui-template-image-960x777.png" width="960" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;Click on the “Templates” button, then on “Getting Started” and choose “1.1 Starter – Text to Image.”&lt;/li&gt;
&lt;li&gt;Connect the model “Node” to the “Save Image Node.” The nodes work in a pipeline to generate content using AI.&lt;/li&gt;
&lt;li&gt;Press the blue “Run” button and watch the green “Node” highlight as the RTX-powered PC generates its first image.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Change the prompt and run it again to enter more deeply into the creative world of visual generative AI.&lt;/p&gt;
&lt;p&gt;Read more below on how to dive into additional ComfyUI templates that use more advanced image and video models.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Model Sizes and GPUs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As users get more familiar with ComfyUI and the models that support it, they’ll need to consider GPU VRAM capacity and whether a model will fit within it. Here are some examples for getting started, depending on GPU VRAM:&lt;/p&gt;
&lt;figure class="wp-caption alignnone" id="attachment_89347"&gt;&lt;img alt="alt" class="size-medium wp-image-89347" height="189" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/gpu-vram-image-gen-video-3d-chart-comparison-960x189.png" width="960" /&gt;&lt;figcaption class="wp-caption-text" id="caption-attachment-89347"&gt;*Use FP4 models with NVIDIA GeForce RTX 50 Series GPUs, and FP8 models with RTX 40 Series GPUs for best results. This lets models use less VRAM while providing more performance.&lt;/figcaption&gt;&lt;/figure&gt;
&lt;h2&gt;&lt;b&gt;Generating Images&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone wp-image-89359 size-medium" height="768" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/2026-01-21-image-gen-tiger-960x768.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;To explore how to improve image generation quality using FLUX.2-Dev:&lt;/p&gt;
&lt;p&gt;From the ComfyUI “Templates” section, click on “All Templates” and search for “FLUX.2 Dev Text to Image.” Select it, and ComfyUI will load the collection of connected nodes, or “Workflow.”&lt;/p&gt;
&lt;p&gt;FLUX.2-Dev has model weights that will need to be downloaded.&lt;/p&gt;
&lt;p&gt;Model weights are the “knowledge” inside an AI model — think of them like the synapses in a brain. When an image generation model like FLUX.2 was trained, it learned patterns from millions of images. Those patterns are stored as billions of numerical values called “weights.”&lt;/p&gt;
&lt;p&gt;ComfyUI doesn’t come with these weights built in. Instead, it downloads them on demand from repositories like Hugging Face. These files are large (FLUX.2 can be &amp;gt;30GB depending on the version), which is why systems need enough storage and download time to grab them.&lt;/p&gt;
&lt;p&gt;A dialog will appear to guide users through downloading the model weights. The weight files (filename.safetensors) are automatically saved to the correct ComfyUI folder on a user’s PC.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt" class="alignnone size-medium wp-image-89350" height="777" src="https://blogs.nvidia.com/wp-content/uploads/2026/01/2026-01-21-missing-models-screenshot-960x777.png" width="960" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Saving Workflows:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Now that the model weights are downloaded, the next step is to save this newly downloaded template as a “Workflow.”&lt;/p&gt;
&lt;p&gt;Users can click on the top-left hamburger menu (three lines) and choose “Save.” The workflow is now saved in the user’s list of “Workflows” (press W to show or hide the window). Close the tab to exit the workflow without losing any work.&lt;/p&gt;
&lt;p&gt;If the download dialog was accidentally closed before the model weights finished downloading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press W to quickly open the “Workflows” window.&lt;/li&gt;
&lt;li&gt;Select the Workflow and ComfyUI will load it. This will also prompt for any missing model weights to download.&lt;/li&gt;
&lt;li&gt;ComfyUI is now ready to generate an image using FLUX.2-Dev.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Prompt Tips for FLUX.2-Dev:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with clear, concrete descriptions of the subject, setting, style and mood — for example: “Cinematic closeup of a vintage race car in the rain, neon reflections on wet asphalt, high contrast, 35mm photography.” Short‑to‑medium length prompts&amp;nbsp; — a single, focused sentence or two — are usually easier to control than long, storylike prompts, especially when getting started.&lt;/li&gt;
&lt;li&gt;Add constraints to guide consistency and quality. Specify things like:
&lt;ul&gt;
&lt;li&gt;Framing (“wide shot” or “portrait”)&lt;/li&gt;
&lt;li&gt;Detail level (“high detail, sharp focus”)&lt;/li&gt;
&lt;li&gt;Realism (“photorealistic” or “stylized illustration”)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;If results are too busy, remove adjectives instead of adding more.&lt;/li&gt;
&lt;li&gt;Avoid negative prompting — stick to prompting what’s desired.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Learn more about FLUX.2 prompting in this guide from Black Forest Labs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Save Locations on Disk:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Once done refining the image, right click on “Save Image Node” to open the image in a browser, or save it in a new location.&lt;/p&gt;
&lt;p&gt;ComfyUI’s default output folders are typically the following, based on the application type and OS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows (Standalone/Portable Version): The folder is usually found in C:\ComfyUI\output or a similar path within where the program was unzipped.&lt;/li&gt;
&lt;li&gt;Windows (Desktop Application): The path is usually located within the AppData directory, like: C:\Users\%username%\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\output&lt;/li&gt;
&lt;li&gt;Linux: The installation location defaults to ~/.config/ComfyUI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;Prompting Videos&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Explore how to improve video generation quality, using the new LTX-2 model as an example:&lt;/p&gt;
&lt;p&gt;[embedded content]&lt;/p&gt;
&lt;p&gt;Lightrick’s LTX‑2 is an advanced audio-video model designed for controllable, storyboard-style video generation in ComfyUI. Once the LTX‑2 Image to Video Template and model weights are downloaded, start by treating the prompt like a short shot description, rather than a full movie script.&lt;/p&gt;
&lt;p&gt;Unlike the first two Templates, LTX‑2 Image to Video combines an image and a text prompt to generate video.&lt;/p&gt;
&lt;p&gt;Users can take one of the images generated in FLUX.2-Dev and add a text prompt to give it life.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Prompt Tips for LTX‑2:&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For best results in ComfyUI, write a single flowing paragraph in the present tense or use a simple, script‑style format with scene headings (sluglines), action, character names and dialogue. Aim for four to six descriptive sentences that cover all the key aspects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Establish the shot and scene (wide/medium/closeup, lighting, color, textures, atmosphere).&lt;/li&gt;
&lt;li&gt;Describe the action as a clear sequence, define characters with visible traits and body language, and specify camera moves.&lt;/li&gt;
&lt;li&gt;Lastly, add audio, such as ambient sound, music and dialogue, using quotation marks.&lt;/li&gt;
&lt;li&gt;Match the level of detail to the shot scale. For example, closeups need more precise character and texture detail than wide shots. Be clear on how the camera relates to the subject, not just where it moves.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additional details to consider adding to prompts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Camera movement language:&lt;/b&gt; Specify directions like “slow dolly in,” “handheld tracking,” “over‑the‑shoulder shot,” “pans across,” “tilts upward,” “pushes in,” “pulls back” or “static frame.”&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Shot types:&lt;/b&gt; Specify wide, medium or close‑ups with thoughtful lighting, shallow depth of field and natural motion.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Pacing:&lt;/b&gt; Direct for slow motion, time‑lapses, lingering shots, continuous shots, freeze frames or seamless transitions that shape rhythm and tone.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Atmosphere:&lt;/b&gt; Add details like fog, mist, rain, golden hour light, reflections and rich surface textures that ground the scene.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Style:&lt;/b&gt; Early in the prompt, specify styles like painterly, film noir, analog film, stop‑motion, pixelated edges, fashion editorial or surreal.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Lighting&lt;/b&gt;: Direct backlighting, specific color palettes, soft rim light, lens flares or other lighting details using specific language.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Emotions&lt;/b&gt;: Focus on prompting for single‑subject performances with clear facial expressions and small gestures.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Voice and audio&lt;/b&gt;: Prompt characters to speak or sing in different languages, supported by clear ambient sound descriptions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Optimizing VRAM Usage and Image Quality&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;As a frontier model, LTX-2 uses significant amounts of video memory (VRAM) to deliver quality results. Memory use goes up as resolution, frame rates, length or steps increase.&lt;/p&gt;
&lt;p&gt;ComfyUI and NVIDIA have collaborated to optimize a weight streaming feature that allows users to offload parts of the workflow to system memory if their GPU runs out of VRAM — but this comes at a cost in performance.&lt;/p&gt;
&lt;p&gt;Depending on the GPU and use case, users may want to constrain these factors to ensure reasonable generation times.&lt;/p&gt;
&lt;p&gt;LTX-2 is an incredibly advanced model — but as with any model, tweaking the settings has a big impact on quality.&lt;/p&gt;
&lt;p&gt;Learn more about optimizing LTX-2 usage with RTX GPUs in the Quick Start Guide for LTX-2 In ComfyUI.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Building a Custom Workflow With FLUX.2-Dev and LTX-2&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Users can simplify the process of hopping between ComfyUI Workflows with FLUX.2-Dev to generate an image, finding it on disk and adding it as an image prompt to the LTX-2 Image to Video Workflow by combining the models into a new workflow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open the saved FLUX.2-Dev Text to Image Workflow.&lt;/li&gt;
&lt;li&gt;Ctrl+left mouse click the FLUX.2-Dev Text to Image node.&lt;/li&gt;
&lt;li&gt;In the LTX-2 Image to Video Workflow, paste the node using Ctrl+V.&lt;/li&gt;
&lt;li&gt;Simply hover over the FLUX.2-Dev Text to Image node IMAGE dot, left click and drag to the Resize Image/Mask Input dot. A blue connector will appear.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Save with a new name, and text prompt for image and video in one workflow.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Advanced 3D Generation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Beyond generating images with FLUX.2 and videos with LTX‑2, the next step is adding 3D guidance. The NVIDIA Blueprint for 3D-guided generative AI shows how to use 3D scenes and assets to drive more controllable, production-style image and video pipelines on RTX PCs — with ready-made workflows users can inspect, tweak and extend.&lt;/p&gt;
&lt;p&gt;Creators can show off their work, connect with other users and find help on the Stable Diffusion subreddit and ComfyUI Discord.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;#ICYMI — The Latest Advancements in NVIDIA RTX AI PCs&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;💻&lt;b&gt;NVIDIA @ CES 2026&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;CES announcements included 4K AI video generation acceleration on PCs with LTX-2 and ComfyUI upgrades. Plus, major RTX accelerations across ComfyUI, LTX-2, Llama.cpp, Ollama, Hyperlink and more unlock video, image and text generation use cases on AI PCs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;📝 Black Forest Labs FLUX 2 Variants&amp;nbsp;&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;FLUX.2 [klein] is a set of compact, ultrafast models that support both image generation and editing, delivering state-of-the-art image quality. The models are accelerated by NVFP4 and NVFP8, boosting speed by up to 2.5x and enabling them to run performantly across a wide range of RTX GPUs.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;✨Project&lt;/b&gt; &lt;b&gt;G-Assist Update&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;With a new “Reasoning Mode” enabled by default, Project G-Assist gains an accuracy and intelligence boost, as well as the ability to action multiple commands at once. G-Assist can now control settings on G-SYNC monitors, CORSAIR peripherals and CORSAIR PC components through iCUE — covering lighting, profiles, performance and cooling.&lt;/p&gt;
&lt;p&gt;Support is also coming soon to Elgato Stream Decks, bringing G-Assist closer to a unified AI interface for tuning and controlling nearly any system. For G-Assist plug-in devs, a new Cursor-based plug-in builder accelerates development using Cursor’s agentic coding environment.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Plug in to NVIDIA AI PC on &lt;/i&gt;&lt;i&gt;Facebook&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;Instagram&lt;/i&gt;&lt;i&gt;, &lt;/i&gt;&lt;i&gt;TikTok&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt; — and stay informed by subscribing to the &lt;/i&gt;&lt;i&gt;RTX AI PC newsletter&lt;/i&gt;&lt;i&gt;.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Follow NVIDIA Workstation on &lt;/i&gt;&lt;i&gt;LinkedIn&lt;/i&gt;&lt;i&gt; and &lt;/i&gt;&lt;i&gt;X&lt;/i&gt;&lt;i&gt;.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;See &lt;/i&gt;&lt;i&gt;notice&lt;/i&gt;&lt;i&gt; regarding software product information.&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/</guid><pubDate>Thu, 22 Jan 2026 14:00:57 +0000</pubDate></item><item><title>[NEW] Anthropic has to keep revising its technical interview test so you can’t cheat on it with Claude (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/anthropic-has-to-keep-revising-its-technical-interview-test-so-you-cant-cheat-on-it-with-claude/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/Claude2_Blog_V1-1.webp?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Since 2024, Anthropic’s performance optimization team has given job applicants a take-home test to make sure they know their stuff. But as AI coding tools have gotten better, the test has had to change a lot to stay ahead of AI-assisted cheating.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Team lead Tristan Hume described the history of the challenge in a blog post on Wednesday. “Each new Claude model has forced us to redesign the test,” Hume writes. “When given the same time limit, Claude Opus 4 outperformed most human applicants. That still allowed us to distinguish the strongest candidates — but then, Claude Opus 4.5 matched even those.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The result is a serious candidate-assessment problem. Without in-person proctoring, there’s no way to ensure someone isn’t using AI to cheat on the test — and if they do, they’ll quickly rise to the top. “Under the constraints of the take-home test, we no longer had a way to distinguish between the output of our top candidates and our most capable model,” Hume writes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The issue of AI cheating is already wreaking havoc at schools and universities around the world, so it’s ironic that AI labs are having to deal with it too. But Anthropic is also uniquely well-equipped to deal with the problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the end, Hume designed a new test that had less to do with optimizing hardware, making it sufficiently novel to stump contemporary AI tools. But as part of the post, he shared the original test to see if anyone reading could come up with a better solution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you can best Opus 4.5,” the post reads, “we’d love to hear from you.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/11/Claude2_Blog_V1-1.webp?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Since 2024, Anthropic’s performance optimization team has given job applicants a take-home test to make sure they know their stuff. But as AI coding tools have gotten better, the test has had to change a lot to stay ahead of AI-assisted cheating.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Team lead Tristan Hume described the history of the challenge in a blog post on Wednesday. “Each new Claude model has forced us to redesign the test,” Hume writes. “When given the same time limit, Claude Opus 4 outperformed most human applicants. That still allowed us to distinguish the strongest candidates — but then, Claude Opus 4.5 matched even those.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The result is a serious candidate-assessment problem. Without in-person proctoring, there’s no way to ensure someone isn’t using AI to cheat on the test — and if they do, they’ll quickly rise to the top. “Under the constraints of the take-home test, we no longer had a way to distinguish between the output of our top candidates and our most capable model,” Hume writes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The issue of AI cheating is already wreaking havoc at schools and universities around the world, so it’s ironic that AI labs are having to deal with it too. But Anthropic is also uniquely well-equipped to deal with the problem.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the end, Hume designed a new test that had less to do with optimizing hardware, making it sufficiently novel to stump contemporary AI tools. But as part of the post, he shared the original test to see if anyone reading could come up with a better solution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If you can best Opus 4.5,” the post reads, “we’d love to hear from you.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/anthropic-has-to-keep-revising-its-technical-interview-test-so-you-cant-cheat-on-it-with-claude/</guid><pubDate>Thu, 22 Jan 2026 14:54:23 +0000</pubDate></item><item><title>[NEW] From invisibility cloaks to AI chips: Neurophos raises $110M to build tiny optical processors for inferencing (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/from-invisibility-cloaks-to-ai-chips-neurophos-raises-110m-to-build-tiny-optical-processors-for-inferencing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/02/tc-backlight-e1689786273147.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Twenty years ago, a Duke University professor, David R. Smith, used artificial composite materials called “metamaterials” to make a real-life invisibility cloak. While this cloak didn’t really work like Harry Potter’s, exhibiting limited ability to conceal objects from the light of a single microwave length, those advances in material science did eventually trickle down to electromagnetism research.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, Austin-based Neurophos, a photonics startup spun out of Duke University and Metacept (an incubator run by Smith), is taking that research further to solve what may be the biggest problem facing AI labs and hyperscalers: how to scale computing power while keeping power consumption in check.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup has come up with a “metasurface modulator” with optical properties that enable it to serve as a tensor core processor for doing matrix vector multiplication — math that is at the heart of a lot of AI work (particularly inferencing), currently performed by specialized GPUs and TPUs that use traditional silicon gates and transistors. By fitting thousands of these modulators on a chip, Neurophos claims, its “optical processing unit” is significantly faster than the silicon GPUs currently used en masse at AI data centers, and far more efficient at inferencing (running trained models), which can be a fairly expensive task.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To fund the development of its chips, Neurophos has just raised $110 million in a Series A round led by Gates Frontier (Bill Gates’ venture firm), with participation from Microsoft’s M12, Carbon Direct, Aramco Ventures, Bosch Ventures, Tectonic Ventures, Space Capital, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, photonic chips are nothing new. In theory, photonic chips offer higher performance than traditional silicon because light produces less heat than electricity, it can travel faster, and is far less susceptible to changes in temperature and electromagnetic fields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But optical components tend to be much larger than their silicon counterparts, and can be difficult to mass-produce. And they also need converters to transform data from digital to analog and back, which can be large and take up a lot of power.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neurophos, however, posits that the metasurface it has developed can solve all of those problems in one swoop because it is about “10,000 times” smaller than traditional optical transistors. The small size, the startup claims, enables it to fit thousands of units on a chip, which results in far more efficiency than traditional silicon because the chip can do many more calculations at once.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“When you shrink the optical transistor, you can do way more math in the optics domain before you have to do that conversion back to the electronics domain,” Dr. Patrick Bowen, CEO and co-founder of Neurophos, told TechCrunch. “If you want to go fast, you have to solve the energy efficiency problem first. Because if you’re going to take a chip and make it 100 times faster, it burns 100 times more power. So you get the privilege of going fast after you solve the energy efficiency problem.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result, Neurophos claims, is an optical processing unit that can wildly outperform Nvidia’s B200 AI GPU. The startup says its chip can run at 56 GHz, yielding a peak 235 Peta Operations per Second (POPS) and consuming 675 watts, compared to the B200, which can deliver 9 POPS at 1,000 watts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bowen says Neurophos has already signed multiple customers (though he declined to name any), and companies including Microsoft are “looking very closely” at the startup’s products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the startup is entering a crowded market that’s dominated by Nvidia, the world’s most valuable public company, whose products have more or less underpinned the entire AI boom. There are also other companies working on photonics, though some, like Lightmatter, have pivoted to focusing on interconnects. And Neurophos is still a few years away from production, expecting its first chips to hit the market by mid-2028.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Bowen is confident the performance and efficiency advances provided by its metasurface modulators will prove a sufficient moat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What everyone else is doing is, and this includes Nvidia, in terms of the fundamental physics of the silicon, it’s really evolutionary rather than revolutionary, and it’s tied to the progress of TSMC. If you look at the improvement of TSMC nodes, on average, they improve in energy efficiency about 15%, and that takes a couple years,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even if we chart out Nvidia’s improvement in architecture over the years, by the time we come out in 2028, we still have massive advantages over everyone else in the market because we’re starting with a 50x over Blackwell in both energy efficiency and raw speed.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And to address the mass-manufacturing issues optical chips have traditionally faced, Neurophos says its chips can be made with standard silicon foundry materials, tools, and processes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh funding will be used for the development of the company’s first integrated photonic compute system, including data center-ready OPU modules, a full software stack, and early-access developer hardware. The company is also opening a San Francisco engineering site and expanding its HQ in Austin, Texas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Modern AI inference demands monumental amounts of power and compute,” Dr. Marc Tremblay, corporate vice president and technical fellow of core AI infrastructure at Microsoft, said in a statement. “We need a breakthrough in compute on par with the leaps we’ve seen in AI models themselves, which is what Neurophos’ technology and high-talent density team is developing.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2018/02/tc-backlight-e1689786273147.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Twenty years ago, a Duke University professor, David R. Smith, used artificial composite materials called “metamaterials” to make a real-life invisibility cloak. While this cloak didn’t really work like Harry Potter’s, exhibiting limited ability to conceal objects from the light of a single microwave length, those advances in material science did eventually trickle down to electromagnetism research.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Today, Austin-based Neurophos, a photonics startup spun out of Duke University and Metacept (an incubator run by Smith), is taking that research further to solve what may be the biggest problem facing AI labs and hyperscalers: how to scale computing power while keeping power consumption in check.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup has come up with a “metasurface modulator” with optical properties that enable it to serve as a tensor core processor for doing matrix vector multiplication — math that is at the heart of a lot of AI work (particularly inferencing), currently performed by specialized GPUs and TPUs that use traditional silicon gates and transistors. By fitting thousands of these modulators on a chip, Neurophos claims, its “optical processing unit” is significantly faster than the silicon GPUs currently used en masse at AI data centers, and far more efficient at inferencing (running trained models), which can be a fairly expensive task.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To fund the development of its chips, Neurophos has just raised $110 million in a Series A round led by Gates Frontier (Bill Gates’ venture firm), with participation from Microsoft’s M12, Carbon Direct, Aramco Ventures, Bosch Ventures, Tectonic Ventures, Space Capital, and others.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now, photonic chips are nothing new. In theory, photonic chips offer higher performance than traditional silicon because light produces less heat than electricity, it can travel faster, and is far less susceptible to changes in temperature and electromagnetic fields.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But optical components tend to be much larger than their silicon counterparts, and can be difficult to mass-produce. And they also need converters to transform data from digital to analog and back, which can be large and take up a lot of power.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Neurophos, however, posits that the metasurface it has developed can solve all of those problems in one swoop because it is about “10,000 times” smaller than traditional optical transistors. The small size, the startup claims, enables it to fit thousands of units on a chip, which results in far more efficiency than traditional silicon because the chip can do many more calculations at once.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“When you shrink the optical transistor, you can do way more math in the optics domain before you have to do that conversion back to the electronics domain,” Dr. Patrick Bowen, CEO and co-founder of Neurophos, told TechCrunch. “If you want to go fast, you have to solve the energy efficiency problem first. Because if you’re going to take a chip and make it 100 times faster, it burns 100 times more power. So you get the privilege of going fast after you solve the energy efficiency problem.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The result, Neurophos claims, is an optical processing unit that can wildly outperform Nvidia’s B200 AI GPU. The startup says its chip can run at 56 GHz, yielding a peak 235 Peta Operations per Second (POPS) and consuming 675 watts, compared to the B200, which can deliver 9 POPS at 1,000 watts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Bowen says Neurophos has already signed multiple customers (though he declined to name any), and companies including Microsoft are “looking very closely” at the startup’s products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, the startup is entering a crowded market that’s dominated by Nvidia, the world’s most valuable public company, whose products have more or less underpinned the entire AI boom. There are also other companies working on photonics, though some, like Lightmatter, have pivoted to focusing on interconnects. And Neurophos is still a few years away from production, expecting its first chips to hit the market by mid-2028.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Bowen is confident the performance and efficiency advances provided by its metasurface modulators will prove a sufficient moat.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“What everyone else is doing is, and this includes Nvidia, in terms of the fundamental physics of the silicon, it’s really evolutionary rather than revolutionary, and it’s tied to the progress of TSMC. If you look at the improvement of TSMC nodes, on average, they improve in energy efficiency about 15%, and that takes a couple years,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Even if we chart out Nvidia’s improvement in architecture over the years, by the time we come out in 2028, we still have massive advantages over everyone else in the market because we’re starting with a 50x over Blackwell in both energy efficiency and raw speed.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;And to address the mass-manufacturing issues optical chips have traditionally faced, Neurophos says its chips can be made with standard silicon foundry materials, tools, and processes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh funding will be used for the development of the company’s first integrated photonic compute system, including data center-ready OPU modules, a full software stack, and early-access developer hardware. The company is also opening a San Francisco engineering site and expanding its HQ in Austin, Texas.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Modern AI inference demands monumental amounts of power and compute,” Dr. Marc Tremblay, corporate vice president and technical fellow of core AI infrastructure at Microsoft, said in a statement. “We need a breakthrough in compute on par with the leaps we’ve seen in AI models themselves, which is what Neurophos’ technology and high-talent density team is developing.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/from-invisibility-cloaks-to-ai-chips-neurophos-raises-110m-to-build-tiny-optical-processors-for-inferencing/</guid><pubDate>Thu, 22 Jan 2026 15:00:52 +0000</pubDate></item><item><title>[NEW] Google reportedly snags team behind AI voice startup Hume AI (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/google-reportedly-snags-up-team-behind-ai-voice-startup-hume-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2166080568.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The top talent behind yet another promising AI startup has been gobbled up by an incumbent. As part of a new licensing agreement, Google DeepMind is bringing on the CEO and several of the top engineers at voice AI startup Hume AI, reports Wired.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s left of Hume AI will continue to supply its technology to other AI firms. No financial details of the deal were shared.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Google and Hume AI to confirm the news.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Alan Cowen and about seven other engineers will work with DeepMind to improve Gemini’s voice features, according to Wired.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Hume acqui-hire is the latest example of a leading AI firm scooping top talent off the market and skirting regulatory scrutiny by acquiring a startup’s team rather than the company outright. Last year, Google acquired viral AI coding startup Windsurf’s CEO and other top researchers, and OpenAI has acquired several startup teams in recent months, including Convogo and Roi. The Federal Trade Commission recently said that it would take a closer look at such deals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal also shows that voice is becoming the next frontier in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hume AI’s secret sauce is its model’s ability to understand a user’s emotions and mood based on their voice. In 2024, the startup launched its Empathetic Voice Interface, a conversational AI with emotional intelligence. Hume AI has raised close to $80 million to date according to PitchBook, and expects to bring in $100 million in revenue this year, per Wired.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Hume AI isn’t the only company working on voice-focused models. Google has been steadily improving its Gemini Live feature, which allows a user to have conversations with the chatbot. Last month, Google released a new native audio model for the Live API that improved the model’s ability to “handle complex workflows,” per the Gemini API release notes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others in the industry are investing big into voice capabilities as well. OpenAI is reportedly preparing to overhaul its audio models in preparation for its audio-first personal device, created with Jonny Ive’s io, to launch this year. Recent leaks suggest the device could be a form of earbuds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Meta also accelerated its AI audio push by acquiring startup Play AI. The Facebook-maker’s Ray-Ban smart glasses are increasingly relying on voice and audio capabilities for tasks like helping you hear conversations in noisy rooms and enabling hands-free control for calls, texts, music, and photos.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Voice is the only acceptable input mode for wearables,” investor Vanessa Larco told TechCrunch. “This acquisition will only accelerate the need for voice apps.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Demand continues to increase for voice capabilities. Earlier this month, ElevenLabs, the AI voice generation startup, said it crossed $330 million in annual recurring revenue.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/GettyImages-2166080568.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The top talent behind yet another promising AI startup has been gobbled up by an incumbent. As part of a new licensing agreement, Google DeepMind is bringing on the CEO and several of the top engineers at voice AI startup Hume AI, reports Wired.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What’s left of Hume AI will continue to supply its technology to other AI firms. No financial details of the deal were shared.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to Google and Hume AI to confirm the news.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CEO Alan Cowen and about seven other engineers will work with DeepMind to improve Gemini’s voice features, according to Wired.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Hume acqui-hire is the latest example of a leading AI firm scooping top talent off the market and skirting regulatory scrutiny by acquiring a startup’s team rather than the company outright. Last year, Google acquired viral AI coding startup Windsurf’s CEO and other top researchers, and OpenAI has acquired several startup teams in recent months, including Convogo and Roi. The Federal Trade Commission recently said that it would take a closer look at such deals.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The deal also shows that voice is becoming the next frontier in AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hume AI’s secret sauce is its model’s ability to understand a user’s emotions and mood based on their voice. In 2024, the startup launched its Empathetic Voice Interface, a conversational AI with emotional intelligence. Hume AI has raised close to $80 million to date according to PitchBook, and expects to bring in $100 million in revenue this year, per Wired.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Hume AI isn’t the only company working on voice-focused models. Google has been steadily improving its Gemini Live feature, which allows a user to have conversations with the chatbot. Last month, Google released a new native audio model for the Live API that improved the model’s ability to “handle complex workflows,” per the Gemini API release notes.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others in the industry are investing big into voice capabilities as well. OpenAI is reportedly preparing to overhaul its audio models in preparation for its audio-first personal device, created with Jonny Ive’s io, to launch this year. Recent leaks suggest the device could be a form of earbuds.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Meta also accelerated its AI audio push by acquiring startup Play AI. The Facebook-maker’s Ray-Ban smart glasses are increasingly relying on voice and audio capabilities for tasks like helping you hear conversations in noisy rooms and enabling hands-free control for calls, texts, music, and photos.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Voice is the only acceptable input mode for wearables,” investor Vanessa Larco told TechCrunch. “This acquisition will only accelerate the need for voice apps.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Demand continues to increase for voice capabilities. Earlier this month, ElevenLabs, the AI voice generation startup, said it crossed $330 million in annual recurring revenue.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/google-reportedly-snags-up-team-behind-ai-voice-startup-hume-ai/</guid><pubDate>Thu, 22 Jan 2026 15:12:51 +0000</pubDate></item><item><title>[NEW] eBay bans illicit automated shopping amid rapid rise of AI agents (AI - Ars Technica)</title><link>https://arstechnica.com/information-technology/2026/01/ebay-bans-illicit-automated-shopping-amid-rapid-rise-of-ai-agents/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New policy requires “buy for me” AI tools and chatbots to obtain permission before accessing the platform.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Studio shot of vintage robot toy standing by miniature shopping basket filled with electronic equipment" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_shopper_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Studio shot of vintage robot toy standing by miniature shopping basket filled with electronic equipment" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_shopper_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Westend61 via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, eBay updated its User Agreement to explicitly ban third-party “buy for me” agents and AI chatbots from interacting with its platform without permission, first spotted by Value Added Resource. On its face, a one-line terms of service update doesn’t seem like major news, but what it implies is more significant: The change reflects the rapid emergence of what some are calling “agentic commerce,” a new category of AI tools designed to browse, compare, and purchase products on behalf of users.&lt;/p&gt;
&lt;p&gt;eBay’s updated terms, which go into effect on February 20, 2026, specifically prohibit users from employing “buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review” to access eBay’s services without the site’s permission. The previous version of the agreement contained a general prohibition on robots, spiders, scrapers, and automated data gathering tools but did not mention AI agents or LLMs by name.&lt;/p&gt;
&lt;p&gt;At first glance, the phrase “agentic commerce” may sound like aspirational marketing jargon, but the tools are already here, and people are apparently using them. While fitting loosely under one label, these tools come in many forms.&lt;/p&gt;
&lt;p&gt;OpenAI first added shopping features to ChatGPT Search in April 2025, allowing users to browse product recommendations. By September, the company launched Instant Checkout, which lets users purchase items from Etsy and Shopify merchants directly within the chat interface. (In November, eBay CEO Jamie Iannone suggested the company might join OpenAI’s Instant Checkout program in the future.)&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Elsewhere, Perplexity offers “Buy with Pro,” a one-click checkout feature for its paying subscribers. Google recently announced its Universal Commerce Protocol, an open standard for AI agents to interact with retailers. And Amazon offers a “Buy For Me” feature, which uses AI to purchase items from external brand websites within the Amazon app.&lt;/p&gt;
&lt;h2&gt;Even with new restrictions, eBay leaves the door open&lt;/h2&gt;
&lt;p&gt;eBay’s policy update follows the company’s quiet changes to its robots.txt file in December, a special file on a web server that lists rules and prohibitions that sites hope web-crawling bots will follow. According to Modern Retail, eBay added a new “Robot &amp;amp; Agent Policy” to the file that prohibited automated scraping and buy-for-me agents. eBay later updated the file to add explicit blocks against bots from Perplexity, Anthropic, Amazon, and others, though it allowed Google’s shopping bot to access the site.&lt;/p&gt;
&lt;p&gt;However, restrictions in robots.txt files are basically honor-system suggestions. By adding the language to its User Agreement, eBay can now more easily take legal action against users or companies who violate the policy.&lt;/p&gt;
&lt;p&gt;Notably, even with this general mood against robotic commerce from outsiders, eBay’s new User Agreement policy does not prevent the company from developing its own AI shopping tools. CEO Jamie Iannone said on an October earnings call that eBay is “testing a variety of agentic experiences in search and shopping.” The rules also allow such bots “with the prior express permission of eBay,” which could open the door to official shopping partnerships with companies like OpenAI.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        New policy requires “buy for me” AI tools and chatbots to obtain permission before accessing the platform.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Studio shot of vintage robot toy standing by miniature shopping basket filled with electronic equipment" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_shopper_1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Studio shot of vintage robot toy standing by miniature shopping basket filled with electronic equipment" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/04/robot_shopper_1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Westend61 via Getty Images

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;On Tuesday, eBay updated its User Agreement to explicitly ban third-party “buy for me” agents and AI chatbots from interacting with its platform without permission, first spotted by Value Added Resource. On its face, a one-line terms of service update doesn’t seem like major news, but what it implies is more significant: The change reflects the rapid emergence of what some are calling “agentic commerce,” a new category of AI tools designed to browse, compare, and purchase products on behalf of users.&lt;/p&gt;
&lt;p&gt;eBay’s updated terms, which go into effect on February 20, 2026, specifically prohibit users from employing “buy-for-me agents, LLM-driven bots, or any end-to-end flow that attempts to place orders without human review” to access eBay’s services without the site’s permission. The previous version of the agreement contained a general prohibition on robots, spiders, scrapers, and automated data gathering tools but did not mention AI agents or LLMs by name.&lt;/p&gt;
&lt;p&gt;At first glance, the phrase “agentic commerce” may sound like aspirational marketing jargon, but the tools are already here, and people are apparently using them. While fitting loosely under one label, these tools come in many forms.&lt;/p&gt;
&lt;p&gt;OpenAI first added shopping features to ChatGPT Search in April 2025, allowing users to browse product recommendations. By September, the company launched Instant Checkout, which lets users purchase items from Etsy and Shopify merchants directly within the chat interface. (In November, eBay CEO Jamie Iannone suggested the company might join OpenAI’s Instant Checkout program in the future.)&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Elsewhere, Perplexity offers “Buy with Pro,” a one-click checkout feature for its paying subscribers. Google recently announced its Universal Commerce Protocol, an open standard for AI agents to interact with retailers. And Amazon offers a “Buy For Me” feature, which uses AI to purchase items from external brand websites within the Amazon app.&lt;/p&gt;
&lt;h2&gt;Even with new restrictions, eBay leaves the door open&lt;/h2&gt;
&lt;p&gt;eBay’s policy update follows the company’s quiet changes to its robots.txt file in December, a special file on a web server that lists rules and prohibitions that sites hope web-crawling bots will follow. According to Modern Retail, eBay added a new “Robot &amp;amp; Agent Policy” to the file that prohibited automated scraping and buy-for-me agents. eBay later updated the file to add explicit blocks against bots from Perplexity, Anthropic, Amazon, and others, though it allowed Google’s shopping bot to access the site.&lt;/p&gt;
&lt;p&gt;However, restrictions in robots.txt files are basically honor-system suggestions. By adding the language to its User Agreement, eBay can now more easily take legal action against users or companies who violate the policy.&lt;/p&gt;
&lt;p&gt;Notably, even with this general mood against robotic commerce from outsiders, eBay’s new User Agreement policy does not prevent the company from developing its own AI shopping tools. CEO Jamie Iannone said on an October earnings call that eBay is “testing a variety of agentic experiences in search and shopping.” The rules also allow such bots “with the prior express permission of eBay,” which could open the door to official shopping partnerships with companies like OpenAI.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/information-technology/2026/01/ebay-bans-illicit-automated-shopping-amid-rapid-rise-of-ai-agents/</guid><pubDate>Thu, 22 Jan 2026 15:56:33 +0000</pubDate></item><item><title>[NEW] Google’s AI Mode can now tap into your Gmail and Photos to provide tailored responses (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/googles-ai-mode-can-now-tap-into-your-gmail-and-photos-to-provide-tailored-responses/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI Mode, Google’s conversational Search feature for complex questions, is getting more personalized. The tech giant announced on Thursday that it’s bringing “Personal Intelligence” to AI Mode, enabling it to tap into your Gmail and Google Photos to provide more individualized responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company debuted Personal Intelligence last week in the Gemini app to allow the AI assistant to tailor its responses by connecting across your Google ecosystem, starting with Gmail, Photos, Search, and YouTube history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The opt-in feature is now starting to roll out to AI Mode to Google AI Pro and AI Ultra subscribers in English in the U.S. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By bringing Personal Intelligence to Gemini and AI Mode, Google is leveraging the wealth of user data already within its ecosystem. Since users already rely on services like Gmail and Photos, Google can deliver more personalized experiences that rivals can’t easily match. Of course, not everyone wants AI looking at their photos and emails, so you can turn Personal Intelligence on or off at any time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With Personal Intelligence, recommendations don’t just match your interests — they fit seamlessly into your life,” Robby Stein, VP of Product, Google Search, explained in a blog post. “You don’t have to constantly explain your preferences or existing plans, it selects recommendations just for you, right from the start.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3085026" height="403" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-22-at-10.13.11-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Say you’re planning a vacation and searching for things to do and places to eat that everyone in your family will enjoy. With Personal Intelligence, AI Mode can draw on your hotel booking in Gmail and past travel memories in Google Photos to suggest a tailored itinerary with something for everyone. For example, you might see recommendations like an old-timey ice cream parlor based on the many ice cream selfies stored in Google Photos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google notes that AI Mode won’t just give you a generic list of restaurants and activities; it instead provides a personalized starting point for planning.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Personal Intelligence can also be particularly helpful for shopping, because AI Mode considers the types of items you buy and where you shop,” Stein wrote. “If you need a new coat for your upcoming trip, AI Mode could automatically take into account the brands you prefer, as well as your flight confirmation in Gmail to identify the destination and timing (Chicago in March). You’ll get suggestions for windproof, versatile coats that fit the weather and your preferred look. It’s like a personal shopper who already knows your itinerary and the vibe you’re going for.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says other questions you could ask are: “Make a scavenger hunt for [partner’s name] to celebrate our anniversary. For each location, include a hint about us,” or “I’m decorating [child’s name ]’s bedroom, give me ideas for a theme and suggestions for decor.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company notes that AI Mode doesn’t train directly on your Gmail inbox or Google Photos library. Instead, it trains on specific prompts and the model’s responses.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI Mode, Google’s conversational Search feature for complex questions, is getting more personalized. The tech giant announced on Thursday that it’s bringing “Personal Intelligence” to AI Mode, enabling it to tap into your Gmail and Google Photos to provide more individualized responses.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company debuted Personal Intelligence last week in the Gemini app to allow the AI assistant to tailor its responses by connecting across your Google ecosystem, starting with Gmail, Photos, Search, and YouTube history.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The opt-in feature is now starting to roll out to AI Mode to Google AI Pro and AI Ultra subscribers in English in the U.S. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;By bringing Personal Intelligence to Gemini and AI Mode, Google is leveraging the wealth of user data already within its ecosystem. Since users already rely on services like Gmail and Photos, Google can deliver more personalized experiences that rivals can’t easily match. Of course, not everyone wants AI looking at their photos and emails, so you can turn Personal Intelligence on or off at any time.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“With Personal Intelligence, recommendations don’t just match your interests — they fit seamlessly into your life,” Robby Stein, VP of Product, Google Search, explained in a blog post. “You don’t have to constantly explain your preferences or existing plans, it selects recommendations just for you, right from the start.”&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3085026" height="403" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-22-at-10.13.11-AM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Say you’re planning a vacation and searching for things to do and places to eat that everyone in your family will enjoy. With Personal Intelligence, AI Mode can draw on your hotel booking in Gmail and past travel memories in Google Photos to suggest a tailored itinerary with something for everyone. For example, you might see recommendations like an old-timey ice cream parlor based on the many ice cream selfies stored in Google Photos.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google notes that AI Mode won’t just give you a generic list of restaurants and activities; it instead provides a personalized starting point for planning.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;“Personal Intelligence can also be particularly helpful for shopping, because AI Mode considers the types of items you buy and where you shop,” Stein wrote. “If you need a new coat for your upcoming trip, AI Mode could automatically take into account the brands you prefer, as well as your flight confirmation in Gmail to identify the destination and timing (Chicago in March). You’ll get suggestions for windproof, versatile coats that fit the weather and your preferred look. It’s like a personal shopper who already knows your itinerary and the vibe you’re going for.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google says other questions you could ask are: “Make a scavenger hunt for [partner’s name] to celebrate our anniversary. For each location, include a hint about us,” or “I’m decorating [child’s name ]’s bedroom, give me ideas for a theme and suggestions for decor.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company notes that AI Mode doesn’t train directly on your Gmail inbox or Google Photos library. Instead, it trains on specific prompts and the model’s responses.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/googles-ai-mode-can-now-tap-into-your-gmail-and-photos-to-provide-tailored-responses/</guid><pubDate>Thu, 22 Jan 2026 16:00:00 +0000</pubDate></item><item><title>[NEW] Google adds your Gmail and Photos to AI Mode to enable "Personal Intelligence" (AI - Ars Technica)</title><link>https://arstechnica.com/google/2026/01/google-ai-mode-can-now-customize-responses-with-your-email-and-photos/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Personal Intelligence is optional and rolling out first to AI Pro and AI Ultra subscribers.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Mode banner" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-640x360.png" width="640" /&gt;
                  &lt;img alt="AI Mode banner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google believes AI is the future of search, and it’s not shy about saying it. After adding account-level personalization to Gemini earlier this month, it’s now updating AI Mode with so-called “Personal Intelligence.” According to Google, this makes the bot’s answers more useful because they are tailored to your personal context.&lt;/p&gt;
&lt;p&gt;Starting today, the feature is rolling out to all users who subscribe to Google AI Pro or AI Ultra. However, it will be a Labs feature that needs to be explicitly enabled (subscribers will be prompted to do this). Google tends to expand access to new AI features to free accounts later on, so free users will most likely get access to Personal Intelligence in the future. Whenever this option does land on your account, it’s entirely optional and can be disabled at any time.&lt;/p&gt;
&lt;p&gt;If you decide to integrate your data with AI Mode, the search bot will be able to scan your Gmail and Google Photos. That’s less extensive than the Gemini app version, which supports Gmail, Photos, Search, and YouTube history. Gmail will probably be the biggest contributor to AI Mode—a great many life events involve confirmation emails. Traditional search results when you are logged in are adjusted based on your usage history, but this goes a step further.&lt;/p&gt;
&lt;p&gt;If you’re going to use AI Mode to find information, Personal Intelligence could actually be quite helpful. When you connect data from other Google apps, Google’s custom Gemini search model will instantly know about your preferences and background—that’s the kind of information you’d otherwise have to include in your search query to get the best output. With Personal Intelligence, AI Mode can just pull those details from your email or photos.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For example, as in the video below, you could ask about clothing options for an upcoming trip. Instead of telling the robot when and where you’re going in the prompt, it can get that information from your email confirmation. When AI Mode uses your personal context in a response, it will cite it in-line the same way it does for websites.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Personal Intelligence in AI Mode.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;h2&gt;Perfectly imperfect&lt;/h2&gt;
&lt;p&gt;Google says, as it often does, that AI is not perfect. AI Mode with Personal Intelligence can make mistakes, drawing the wrong conclusions from the data it mines from your account. In that case, Google suggests using a follow-up prompt to correct it and get more accurate information. It’s similar to the way you might refine a traditional Google search when the links aren’t to your liking.&lt;/p&gt;
&lt;p&gt;AI Mode and Google AI are generally supposed to improve over time to reduce such failures. The way you use the service contributes to that, but Google says the model is not being trained directly on your email or photos, even if you connect them to AI Mode. Instead, Google uses your prompts and the resulting output to train its AI models. Access to Gmail and Photos can be revoked at any time, but it sounds like there won’t be a simple way to toggle off Personal Intelligence for a single query, which is possible in Gemini.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Personal Intelligence is optional and rolling out first to AI Pro and AI Ultra subscribers.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="AI Mode banner" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-640x360.png" width="640" /&gt;
                  &lt;img alt="AI Mode banner" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/AI-Mode-1152x648.png" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Google

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Google believes AI is the future of search, and it’s not shy about saying it. After adding account-level personalization to Gemini earlier this month, it’s now updating AI Mode with so-called “Personal Intelligence.” According to Google, this makes the bot’s answers more useful because they are tailored to your personal context.&lt;/p&gt;
&lt;p&gt;Starting today, the feature is rolling out to all users who subscribe to Google AI Pro or AI Ultra. However, it will be a Labs feature that needs to be explicitly enabled (subscribers will be prompted to do this). Google tends to expand access to new AI features to free accounts later on, so free users will most likely get access to Personal Intelligence in the future. Whenever this option does land on your account, it’s entirely optional and can be disabled at any time.&lt;/p&gt;
&lt;p&gt;If you decide to integrate your data with AI Mode, the search bot will be able to scan your Gmail and Google Photos. That’s less extensive than the Gemini app version, which supports Gmail, Photos, Search, and YouTube history. Gmail will probably be the biggest contributor to AI Mode—a great many life events involve confirmation emails. Traditional search results when you are logged in are adjusted based on your usage history, but this goes a step further.&lt;/p&gt;
&lt;p&gt;If you’re going to use AI Mode to find information, Personal Intelligence could actually be quite helpful. When you connect data from other Google apps, Google’s custom Gemini search model will instantly know about your preferences and background—that’s the kind of information you’d otherwise have to include in your search query to get the best output. With Personal Intelligence, AI Mode can just pull those details from your email or photos.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;For example, as in the video below, you could ask about clothing options for an upcoming trip. Instead of telling the robot when and where you’re going in the prompt, it can get that information from your email confirmation. When AI Mode uses your personal context in a response, it will cite it in-line the same way it does for websites.&lt;/p&gt;
&lt;figure class="ars-video"&gt;&lt;div class="relative"&gt;&lt;/div&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Personal Intelligence in AI Mode.

          &lt;/div&gt;
  &lt;/div&gt;
&lt;/figure&gt;
&lt;h2&gt;Perfectly imperfect&lt;/h2&gt;
&lt;p&gt;Google says, as it often does, that AI is not perfect. AI Mode with Personal Intelligence can make mistakes, drawing the wrong conclusions from the data it mines from your account. In that case, Google suggests using a follow-up prompt to correct it and get more accurate information. It’s similar to the way you might refine a traditional Google search when the links aren’t to your liking.&lt;/p&gt;
&lt;p&gt;AI Mode and Google AI are generally supposed to improve over time to reduce such failures. The way you use the service contributes to that, but Google says the model is not being trained directly on your email or photos, even if you connect them to AI Mode. Instead, Google uses your prompts and the resulting output to train its AI models. Access to Gmail and Photos can be revoked at any time, but it sounds like there won’t be a simple way to toggle off Personal Intelligence for a single query, which is possible in Gemini.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/google/2026/01/google-ai-mode-can-now-customize-responses-with-your-email-and-photos/</guid><pubDate>Thu, 22 Jan 2026 16:35:41 +0000</pubDate></item><item><title>[NEW] Dispatch from Davos: hot air, big egos and cold flexes (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/22/1131687/hot-air-cold-flexes-at-davos/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260122_WEF.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This story first appeared in The Debrief, our subscriber-only newsletter about the biggest news in tech&amp;nbsp;by Mat Honan, Editor in Chief. Subscribe to read the next edition as soon as it lands. &lt;/em&gt;&lt;/p&gt;  &lt;p&gt;It’s supposed to be frigid in Davos this time of year. Part of the charm is seeing the world’s elite tromp through the streets in respectable suits and snow boots. But this year it’s positively balmy, with highs in the mid 30s, or a little over 1°C. The current conditions when I flew out of New York were colder, and definitely snowier. I’m told this is due to something called a föhn, a dry warm wind that’s been blowing across the Alps.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;I’m no meteorologist, but it’s true that there is a lot of hot air here.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On Wednesday, President Donald Trump arrived in Davos to address the assembly, and held forth for more than 90 minutes, weaving his way through remarks about the economy, Greenland, windmills, Switzerland, Rolexes, Venezuela, and drug prices. It was a talk lousy with gripes, grievances and outright falsehoods.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;One small example: Trump made a big deal of claiming that China, despite being the world leader in manufacturing windmill componentry, doesn’t actually use them for energy generation itself. In fact, it is the world leader in generation, as well.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I did not get to watch this spectacle from the room itself. Sad!&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;By the time I got to the Congress Hall where the address was taking place, there was already a massive scrum of people jostling to get in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I had just wrapped up moderating a panel on “the intelligent co-worker,” ie: AI agents in the workplace. I was really excited for this one as the speakers represented a diverse cross-section of the AI ecosystem. Christoph Schweizer, CEO of BCG had the macro strategic view; Enrique Lores, HP CEO, could speak to both hardware and large enterprises, Workera CEO Kian Katanforoosh has the inside view on workforce training and transformation, Manjul Shah CEO of Hippocratic AI addressed working in the high stakes field of healthcare, and Kate Kallot CEO of Amini AI gave perspective on the global south and Africa in particular.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Interestingly, most of the panel shied away from using the term co-worker, and some even rejected the term agent. But the view they painted was definitely one of humans working alongside AI and augmenting what’s possible. Shah, for example, talked about having agents call 16,000 people in Texas during a heat wave to perform a health and safety check. It was a great discussion. You can watch the whole thing here.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But by the time it let out, the push of people outside the Congress Hall was already too thick for me to get in. In fact I couldn’t even get into a nearby overflow room. I did make it into a third overflow room, but getting in meant navigating my way through a mass of people, so jammed in tight together that it reminded me of being at a Turnstile concert.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The speech blew way past its allotted time, and I had to step out early to get to yet another discussion. Walking through the halls while Trump spoke was a truly surreal experience. He had truly captured the attention of the gathered global elite. I don’t think I saw a single person not starting at a laptop, or phone or iPad, all watching the same video.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Trump is speaking again on Thursday in a previously unscheduled address to announce his Board of Peace. As is (I heard) Elon Musk. So it’s shaping up to be another big day for elite attention capture.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I should say, though, there are elites, and then there are &lt;em&gt;elites&lt;/em&gt;. And there are all sorts of ways of sorting out who is who. Your badge color is one of them. I have a white participant badge, because I was moderating panels. This gets you in pretty much anywhere and therefore is its own sort of status symbol. Where you are staying is another. I’m in Klosters, a neighboring town that’s a 40 minute train ride away from the Congress Centre. Not so elite.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are more subtle ways of status sorting, too. Yesterday I learned that when people ask if this is your first time at Davos, it’s sometimes meant as a way of trying to figure out how important you are. If you’re any kind of big deal, you’ve probably been coming for years.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But the best one I’ve yet encountered happened when I made small talk with the woman sitting next to me as I changed back into my snow boots. It turned out that, like me, she lived in California–at least part time. “But I don’t think I’ll stay there much longer,” she said, “due to the new tax law.” This was just an ice cold flex.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because California’s newly proposed tax legislation? It only targets billionaires.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome to Davos.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/260122_WEF.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This story first appeared in The Debrief, our subscriber-only newsletter about the biggest news in tech&amp;nbsp;by Mat Honan, Editor in Chief. Subscribe to read the next edition as soon as it lands. &lt;/em&gt;&lt;/p&gt;  &lt;p&gt;It’s supposed to be frigid in Davos this time of year. Part of the charm is seeing the world’s elite tromp through the streets in respectable suits and snow boots. But this year it’s positively balmy, with highs in the mid 30s, or a little over 1°C. The current conditions when I flew out of New York were colder, and definitely snowier. I’m told this is due to something called a föhn, a dry warm wind that’s been blowing across the Alps.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;I’m no meteorologist, but it’s true that there is a lot of hot air here.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;On Wednesday, President Donald Trump arrived in Davos to address the assembly, and held forth for more than 90 minutes, weaving his way through remarks about the economy, Greenland, windmills, Switzerland, Rolexes, Venezuela, and drug prices. It was a talk lousy with gripes, grievances and outright falsehoods.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;One small example: Trump made a big deal of claiming that China, despite being the world leader in manufacturing windmill componentry, doesn’t actually use them for energy generation itself. In fact, it is the world leader in generation, as well.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I did not get to watch this spectacle from the room itself. Sad!&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;By the time I got to the Congress Hall where the address was taking place, there was already a massive scrum of people jostling to get in.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I had just wrapped up moderating a panel on “the intelligent co-worker,” ie: AI agents in the workplace. I was really excited for this one as the speakers represented a diverse cross-section of the AI ecosystem. Christoph Schweizer, CEO of BCG had the macro strategic view; Enrique Lores, HP CEO, could speak to both hardware and large enterprises, Workera CEO Kian Katanforoosh has the inside view on workforce training and transformation, Manjul Shah CEO of Hippocratic AI addressed working in the high stakes field of healthcare, and Kate Kallot CEO of Amini AI gave perspective on the global south and Africa in particular.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Interestingly, most of the panel shied away from using the term co-worker, and some even rejected the term agent. But the view they painted was definitely one of humans working alongside AI and augmenting what’s possible. Shah, for example, talked about having agents call 16,000 people in Texas during a heat wave to perform a health and safety check. It was a great discussion. You can watch the whole thing here.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But by the time it let out, the push of people outside the Congress Hall was already too thick for me to get in. In fact I couldn’t even get into a nearby overflow room. I did make it into a third overflow room, but getting in meant navigating my way through a mass of people, so jammed in tight together that it reminded me of being at a Turnstile concert.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt;&lt;p&gt;The speech blew way past its allotted time, and I had to step out early to get to yet another discussion. Walking through the halls while Trump spoke was a truly surreal experience. He had truly captured the attention of the gathered global elite. I don’t think I saw a single person not starting at a laptop, or phone or iPad, all watching the same video.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Trump is speaking again on Thursday in a previously unscheduled address to announce his Board of Peace. As is (I heard) Elon Musk. So it’s shaping up to be another big day for elite attention capture.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I should say, though, there are elites, and then there are &lt;em&gt;elites&lt;/em&gt;. And there are all sorts of ways of sorting out who is who. Your badge color is one of them. I have a white participant badge, because I was moderating panels. This gets you in pretty much anywhere and therefore is its own sort of status symbol. Where you are staying is another. I’m in Klosters, a neighboring town that’s a 40 minute train ride away from the Congress Centre. Not so elite.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;There are more subtle ways of status sorting, too. Yesterday I learned that when people ask if this is your first time at Davos, it’s sometimes meant as a way of trying to figure out how important you are. If you’re any kind of big deal, you’ve probably been coming for years.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;But the best one I’ve yet encountered happened when I made small talk with the woman sitting next to me as I changed back into my snow boots. It turned out that, like me, she lived in California–at least part time. “But I don’t think I’ll stay there much longer,” she said, “due to the new tax law.” This was just an ice cold flex.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Because California’s newly proposed tax legislation? It only targets billionaires.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Welcome to Davos.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/22/1131687/hot-air-cold-flexes-at-davos/</guid><pubDate>Thu, 22 Jan 2026 16:39:55 +0000</pubDate></item><item><title>[NEW] Small models, big results: Achieving superior intent extraction through decomposition (The latest research from Google)</title><link>https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;As AI technologies advance, truly helpful agents will become capable of better anticipating user needs. For experiences on mobile devices to be truly helpful, the underlying models need to understand what the user is doing (or trying to do) when users interact with them. Once current and previous tasks are understood, the model has more context to predict potential next actions. For example, if a user previously searched for music festivals across Europe and is now looking for a flight to London, the agent could offer to find festivals in London on those specific dates.&lt;/p&gt;&lt;p&gt;Large multimodal LLMs are already quite good at understanding user intent from a user interface (UI) trajectory. But using LLMs for this task would typically require sending information to a server, which can be slow, costly, and carries the potential risk of exposing sensitive information.&lt;/p&gt;&lt;p&gt;Our recent paper “Small Models, Big Results: Achieving Superior Intent Extraction Through Decomposition”, presented at EMNLP 2025, addresses the question of how to use &lt;i&gt;small&lt;/i&gt; multimodal LLMs (MLLMs) to understand sequences of user interactions on the web and on mobile devices all on device. By separating user intent understanding into two stages, first summarizing each screen separately and then extracting an intent from the sequence of generated summaries, we make the task more tractable for small models. We also formalize metrics for evaluation of model performance and show that our approach yields results comparable to much larger models, illustrating its potential for on-device applications. This work builds on previous work from our team on user intent understanding.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;As AI technologies advance, truly helpful agents will become capable of better anticipating user needs. For experiences on mobile devices to be truly helpful, the underlying models need to understand what the user is doing (or trying to do) when users interact with them. Once current and previous tasks are understood, the model has more context to predict potential next actions. For example, if a user previously searched for music festivals across Europe and is now looking for a flight to London, the agent could offer to find festivals in London on those specific dates.&lt;/p&gt;&lt;p&gt;Large multimodal LLMs are already quite good at understanding user intent from a user interface (UI) trajectory. But using LLMs for this task would typically require sending information to a server, which can be slow, costly, and carries the potential risk of exposing sensitive information.&lt;/p&gt;&lt;p&gt;Our recent paper “Small Models, Big Results: Achieving Superior Intent Extraction Through Decomposition”, presented at EMNLP 2025, addresses the question of how to use &lt;i&gt;small&lt;/i&gt; multimodal LLMs (MLLMs) to understand sequences of user interactions on the web and on mobile devices all on device. By separating user intent understanding into two stages, first summarizing each screen separately and then extracting an intent from the sequence of generated summaries, we make the task more tractable for small models. We also formalize metrics for evaluation of model performance and show that our approach yields results comparable to much larger models, illustrating its potential for on-device applications. This work builds on previous work from our team on user intent understanding.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/small-models-big-results-achieving-superior-intent-extraction-through-decomposition/</guid><pubDate>Thu, 22 Jan 2026 16:56:44 +0000</pubDate></item><item><title>[NEW] Controlling AI agent sprawl: The CIO’s guide to governance (AI News)</title><link>https://www.artificialintelligence-news.com/news/controlling-ai-agent-sprawl-cio-guide-to-governance/</link><description>&lt;p&gt;Corporate networks are filling up with AI agents, creating a governance blind spot for leaders managing multi-cloud infrastructures.&lt;/p&gt;&lt;p&gt;As distinct business units race to adopt generative technologies, CIOs especially find their ecosystems populated by fragmented and unmonitored assets. This mirrors the shadow IT challenges of the cloud era, but involves autonomous actors capable of executing business logic and accessing sensitive data.&lt;/p&gt;&lt;p&gt;IDC projects the number of actively deployed AI agents will exceed one billion by 2029—a forty-fold increase from current levels. In the first half of 2025 alone, agent creation surged by 119 percent. For enterprise leadership, the immediate challenge shifts from building these agents to locating, auditing, and governing them across platforms.&lt;/p&gt;&lt;p&gt;Salesforce has responded to this fragmentation by expanding its MuleSoft Agent Fabric capabilities, introducing automated discovery tools designed to centralise the management of AI agents regardless of their origin.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-automating-discovery"&gt;Automating discovery&lt;/h3&gt;&lt;p&gt;Visibility remains the core issue for security and operations teams. When marketing teams deploy AI agents on one platform and logistics teams build on another, effective governance becomes difficult as central IT loses a consolidated view of the organisation’s digital workforce.&lt;/p&gt;&lt;p&gt;MuleSoft’s updated architecture addresses this via ‘Agent Scanners’. These tools continuously patrol major ecosystems – including Salesforce Agentforce, Amazon Bedrock, and Google Vertex AI – to identify running agents. Rather than relying on developers to manually register their deployments, the system automates detection.&lt;/p&gt;&lt;p&gt;Finding an agent is only the first step; compliance leaders need to understand the logic behind it. The scanners extract metadata detailing the agent’s capabilities, the LLMs driving it, and the specific data endpoints it is authorised to access. This information is then normalised into standard Agent-to-Agent (A2A) specifications, creating a uniform profile for assets regardless of the underlying vendor.&lt;/p&gt;&lt;p&gt;Andrew Comstock, SVP and GM of MuleSoft, said: “The most successful organisations of the next decade will be those that harness the full diversity of the multi-cloud AI landscape. The expanded capabilities of MuleSoft Agent Fabric give you the freedom to innovate across any platform while maintaining the unified visibility and control needed to scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-governance-and-cost-control-for-ai-agents"&gt;Governance and cost control for AI agents&lt;/h3&gt;&lt;p&gt;Unmanaged agents create financial inefficiency and risk exposure. Consider a CISO in the banking sector. Under standard operations, verifying a new loan-processing agent involves manually chasing documentation from development teams. Automated cataloguing allows security teams to immediately view which financial databases an agent accesses and verify its authorisation levels without manual intervention. This capability ensures security teams view real-time data rather than outdated snapshots.&lt;/p&gt;&lt;p&gt;From a financial perspective, visibility drives consolidation. Large enterprises frequently suffer from redundancy where regional teams independently procure or build similar tools. A multinational manufacturer, for instance, might have three separate teams paying for distinct summarisation agents on different platforms.&lt;/p&gt;&lt;p&gt;By using the MuleSoft Agent Visualizer to filter the estate by job type, operations leaders can identify these overlaps. Consolidating these into a single high-performing asset reduces redundant licensing costs and allows budget reallocation toward novel development.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-transitioning-successfully-to-an-agentic-enterprise"&gt;Transitioning successfully to an ‘Agentic Enterprise’&lt;/h3&gt;&lt;p&gt;Innovation often occurs at the edges, where data scientists build bespoke tools outside formal procurement channels.&lt;/p&gt;&lt;p&gt;The expanded Agent Fabric addresses this by allowing the registration of “homegrown” agents and Model Context Protocol (MCP) servers via URL. This is particularly relevant for sectors like logistics, where teams may build internal tools for proprietary database optimisation. Instead of remaining hidden, these assets can be registered and made discoverable for reuse across the company.&lt;/p&gt;&lt;p&gt;Jonathan Harvey, Head of AI Operations at Capita, said: “Agent Scanners will let us focus on innovation instead of inventory management. Knowing that every agent is automatically discovered and catalogued allows our teams to collaborate, reuse work, and build smarter multi-agent solutions.”&lt;/p&gt;&lt;p&gt;Similarly, AT&amp;amp;T is utilising the framework to orchestrate agents across customer support, chat, and voice interactions.&lt;/p&gt;&lt;p&gt;Brad Ringer, Enterprise &amp;amp; Integration Architect at AT&amp;amp;T, explained: “With AI moving so fast, MuleSoft Agent Fabric provides the framework we need to scale. It brings together and helps us orchestrate all of the agents and MCP servers we’re building in customer support, chat, and voice interactions. It isn’t just a tool; it’s a huge enabler for everything we’re doing next.”&lt;/p&gt;&lt;p&gt;The transition to an “Agentic Enterprise” requires a change in governance around how IT assets are tracked, rendering the days of managing integrations via stale spreadsheets incompatible with the speed of AI agent deployment.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Leaders must assume their inventory of AI agents is incomplete and deploy automated scanning tools to establish a baseline of truth. Once this baseline is established, governance policies should mandate that all agents – whether bought or built – expose their capabilities and data access privileges in a standardised format like A2A to facilitate monitoring.&lt;/p&gt;&lt;p&gt;Finally, executives can use the visibility provided by these tools to audit spend, identifying duplicate functionalities across cloud environments and merging them to control the Total Cost of Ownership (TCO).&amp;nbsp;&lt;/p&gt;&lt;p&gt;As organisations move from pilot programmes to mass deployment, the differentiator will not be the intelligence of individual agents, but the coherence of the network that connects them.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Balancing AI cost efficiency with data sovereignty&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111551" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Corporate networks are filling up with AI agents, creating a governance blind spot for leaders managing multi-cloud infrastructures.&lt;/p&gt;&lt;p&gt;As distinct business units race to adopt generative technologies, CIOs especially find their ecosystems populated by fragmented and unmonitored assets. This mirrors the shadow IT challenges of the cloud era, but involves autonomous actors capable of executing business logic and accessing sensitive data.&lt;/p&gt;&lt;p&gt;IDC projects the number of actively deployed AI agents will exceed one billion by 2029—a forty-fold increase from current levels. In the first half of 2025 alone, agent creation surged by 119 percent. For enterprise leadership, the immediate challenge shifts from building these agents to locating, auditing, and governing them across platforms.&lt;/p&gt;&lt;p&gt;Salesforce has responded to this fragmentation by expanding its MuleSoft Agent Fabric capabilities, introducing automated discovery tools designed to centralise the management of AI agents regardless of their origin.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-automating-discovery"&gt;Automating discovery&lt;/h3&gt;&lt;p&gt;Visibility remains the core issue for security and operations teams. When marketing teams deploy AI agents on one platform and logistics teams build on another, effective governance becomes difficult as central IT loses a consolidated view of the organisation’s digital workforce.&lt;/p&gt;&lt;p&gt;MuleSoft’s updated architecture addresses this via ‘Agent Scanners’. These tools continuously patrol major ecosystems – including Salesforce Agentforce, Amazon Bedrock, and Google Vertex AI – to identify running agents. Rather than relying on developers to manually register their deployments, the system automates detection.&lt;/p&gt;&lt;p&gt;Finding an agent is only the first step; compliance leaders need to understand the logic behind it. The scanners extract metadata detailing the agent’s capabilities, the LLMs driving it, and the specific data endpoints it is authorised to access. This information is then normalised into standard Agent-to-Agent (A2A) specifications, creating a uniform profile for assets regardless of the underlying vendor.&lt;/p&gt;&lt;p&gt;Andrew Comstock, SVP and GM of MuleSoft, said: “The most successful organisations of the next decade will be those that harness the full diversity of the multi-cloud AI landscape. The expanded capabilities of MuleSoft Agent Fabric give you the freedom to innovate across any platform while maintaining the unified visibility and control needed to scale.”&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-governance-and-cost-control-for-ai-agents"&gt;Governance and cost control for AI agents&lt;/h3&gt;&lt;p&gt;Unmanaged agents create financial inefficiency and risk exposure. Consider a CISO in the banking sector. Under standard operations, verifying a new loan-processing agent involves manually chasing documentation from development teams. Automated cataloguing allows security teams to immediately view which financial databases an agent accesses and verify its authorisation levels without manual intervention. This capability ensures security teams view real-time data rather than outdated snapshots.&lt;/p&gt;&lt;p&gt;From a financial perspective, visibility drives consolidation. Large enterprises frequently suffer from redundancy where regional teams independently procure or build similar tools. A multinational manufacturer, for instance, might have three separate teams paying for distinct summarisation agents on different platforms.&lt;/p&gt;&lt;p&gt;By using the MuleSoft Agent Visualizer to filter the estate by job type, operations leaders can identify these overlaps. Consolidating these into a single high-performing asset reduces redundant licensing costs and allows budget reallocation toward novel development.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-transitioning-successfully-to-an-agentic-enterprise"&gt;Transitioning successfully to an ‘Agentic Enterprise’&lt;/h3&gt;&lt;p&gt;Innovation often occurs at the edges, where data scientists build bespoke tools outside formal procurement channels.&lt;/p&gt;&lt;p&gt;The expanded Agent Fabric addresses this by allowing the registration of “homegrown” agents and Model Context Protocol (MCP) servers via URL. This is particularly relevant for sectors like logistics, where teams may build internal tools for proprietary database optimisation. Instead of remaining hidden, these assets can be registered and made discoverable for reuse across the company.&lt;/p&gt;&lt;p&gt;Jonathan Harvey, Head of AI Operations at Capita, said: “Agent Scanners will let us focus on innovation instead of inventory management. Knowing that every agent is automatically discovered and catalogued allows our teams to collaborate, reuse work, and build smarter multi-agent solutions.”&lt;/p&gt;&lt;p&gt;Similarly, AT&amp;amp;T is utilising the framework to orchestrate agents across customer support, chat, and voice interactions.&lt;/p&gt;&lt;p&gt;Brad Ringer, Enterprise &amp;amp; Integration Architect at AT&amp;amp;T, explained: “With AI moving so fast, MuleSoft Agent Fabric provides the framework we need to scale. It brings together and helps us orchestrate all of the agents and MCP servers we’re building in customer support, chat, and voice interactions. It isn’t just a tool; it’s a huge enabler for everything we’re doing next.”&lt;/p&gt;&lt;p&gt;The transition to an “Agentic Enterprise” requires a change in governance around how IT assets are tracked, rendering the days of managing integrations via stale spreadsheets incompatible with the speed of AI agent deployment.&amp;nbsp;&lt;/p&gt;&lt;p&gt;Leaders must assume their inventory of AI agents is incomplete and deploy automated scanning tools to establish a baseline of truth. Once this baseline is established, governance policies should mandate that all agents – whether bought or built – expose their capabilities and data access privileges in a standardised format like A2A to facilitate monitoring.&lt;/p&gt;&lt;p&gt;Finally, executives can use the visibility provided by these tools to audit spend, identifying duplicate functionalities across cloud environments and merging them to control the Total Cost of Ownership (TCO).&amp;nbsp;&lt;/p&gt;&lt;p&gt;As organisations move from pilot programmes to mass deployment, the differentiator will not be the intelligence of individual agents, but the coherence of the network that connects them.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;Balancing AI cost efficiency with data sovereignty&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-111551" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/image-2.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security &amp;amp; Cloud Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/controlling-ai-agent-sprawl-cio-guide-to-governance/</guid><pubDate>Thu, 22 Jan 2026 17:00:04 +0000</pubDate></item><item><title>[NEW] “Dr. Google” had its issues. Can ChatGPT Health do better? (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/22/1131692/dr-google-had-its-issues-can-chatgpt-health-do-better/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/chatgpt-health2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For the past two decades, there’s been a clear first step for anyone who starts experiencing new medical symptoms: Look them up online. The practice was so common that it gained the pejorative moniker “Dr. Google.” But times are changing, and many medical-information seekers are now using LLMs. According to OpenAI, 230 million people ask ChatGPT health-related queries each week.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s the context around the launch of OpenAI’s new ChatGPT Health product, which debuted earlier this month. It landed at an inauspicious time: Two days earlier, the news website SFGate had broken the story of Sam Nelson, a teenager who died of an overdose last year after extensive conversations with ChatGPT about how best to combine various drugs. In the wake of both pieces of news, multiple journalists questioned the wisdom of relying for medical advice on a tool that could cause such extreme harm.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Though ChatGPT Health lives in a separate sidebar tab from the rest of ChatGPT, it isn’t a new model. It’s more like a wrapper that provides one of OpenAI’s preexisting models with guidance and tools it can use to provide health advice—including some that allow it to access a user’s electronic medical records and fitness app data, if granted permission. There’s no doubt that ChatGPT and other large language models can make medical mistakes, and OpenAI emphasizes that ChatGPT Health is intended as an additional support, rather than a replacement for one’s doctor. But when doctors are unavailable or unable to help, people will turn to alternatives.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Some doctors see LLMs as a boon for medical literacy. The average patient might struggle to navigate the vast landscape of online medical information—and, in particular, to distinguish high-quality sources from polished but factually dubious websites—but LLMs can do that job for them, at least in theory. Treating patients who had searched for their symptoms on Google required “a lot of attacking patient anxiety [and] reducing misinformation,” says Marc Succi, an associate professor at Harvard Medical School and a practicing radiologist. But now, he says, “you see patients with a college education, a high school education, asking questions at the level of something an early med student might ask.”&lt;/p&gt; 
 &lt;p&gt;The release of ChatGPT Health, and Anthropic’s subsequent announcement of new health integrations for Claude, indicate that the AI giants are increasingly willing to acknowledge and encourage health-related uses of their models. Such uses certainly come with risks, given LLMs’ well-documented tendencies to agree with users and make up information rather than admit ignorance.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;But those risks also have to be weighed against potential benefits. There’s an analogy here to autonomous vehicles: When policymakers consider whether to allow Waymo in their city, the key metric is not whether its cars are ever involved in accidents but whether they cause less harm than the status quo of relying on human drivers. If Dr. ChatGPT is an improvement over Dr. Google—and early evidence suggests it may be—it could potentially lessen the enormous burden of medical misinformation and unnecessary health anxiety that the internet has created.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Pinning down the effectiveness of a chatbot such as ChatGPT or Claude for consumer health, however, is tricky. “It’s exceedingly difficult to evaluate an open-ended chatbot,” says Danielle Bitterman, the clinical lead for data science and AI at the Mass General Brigham health-care system. Large language models score well on medical licensing examinations, but those exams use multiple-choice questions that don’t reflect how people use chatbots to look up medical information.&lt;/p&gt; 
 &lt;p&gt;Sirisha Rambhatla, an assistant professor of management science and engineering at the University of Waterloo, attempted to close that gap by evaluating how GPT-4o responded to licensing exam questions when it did not have access to a list of possible answers. Medical experts who evaluated the responses scored only about half of them as entirely correct. But multiple-choice exam questions are designed to be tricky enough that the answer options don’t give them entirely away, and they’re still a pretty distant approximation for the sort of thing that a user would type into ChatGPT.&lt;/p&gt;  &lt;p&gt;A different study, which tested GPT-4o on more realistic prompts submitted by human volunteers, found that it answered medical questions correctly about 85% of the time. When I spoke with Amulya Yadav, an associate professor at Pennsylvania State University who runs the Responsible AI for Social Emancipation Lab and led the study, he made it clear that he wasn’t personally a fan of patient-facing medical LLMs. But he freely admits that, technically speaking, they seem up to the task—after all, he says, human doctors misdiagnose patients 10% to 15% of the time. “If I look at it dispassionately, it seems that the world is gonna change, whether I like it or not,” he says.&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;For people seeking medical information online, Yadav says, LLMs do seem to be a better choice than Google. Succi, the radiologist, also concluded that LLMs can be a better alternative to web search when he compared GPT-4’s responses to questions about common chronic medical conditions with the information presented in Google’s knowledge panel, the information box that sometimes appears on the right side of the search results.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Since Yadav’s and Succi’s studies appeared online, in the first half of 2025, OpenAI has released multiple new versions of GPT, and it’s reasonable to expect that GPT-5.2 would perform even better than its predecessors. But the studies do have important limitations: They focus on straightforward, factual questions, and they examine only brief interactions between users and chatbots or web search tools. Some of the weaknesses of LLMs—most notably their sycophancy and tendency to hallucinate—might be more likely to rear their heads in more extensive conversations and with people who are dealing with more complex problems. Reeva Lederman, a professor at the University of Melbourne who studies technology and health, notes that patients who don’t like the diagnosis or treatment recommendations that they receive from a doctor might seek out another opinion from an LLM—and the LLM, if it’s sycophantic, might encourage them to reject their doctor’s advice.&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;Some studies have found that LLMs will hallucinate and exhibit sycophancy in response to health-related prompts. For example, one study showed that GPT-4 and GPT-4o will happily accept and run with incorrect drug information included in a user’s question. In another, GPT-4o frequently concocted definitions for fake syndromes and lab tests mentioned in the user’s prompt. Given the abundance of medically dubious diagnoses and treatments floating around the internet, these patterns of LLM behavior could contribute to the spread of medical misinformation, particularly if people see LLMs as trustworthy.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;OpenAI has reported that the GPT-5 series of models is markedly less sycophantic and prone to hallucination than their predecessors, so the results of these studies might not apply to ChatGPT Health. The company also evaluated the model that powers ChatGPT Health on its responses to health-specific questions, using their publicly available HeathBench benchmark. HealthBench rewards models that express uncertainty when appropriate, recommend that users seek medical attention when necessary, and refrain from causing users unnecessary stress by telling them their condition is more serious that it truly is. It’s reasonable to assume that the model underlying ChatGPT Health exhibited those behaviors in testing, though Bitterman notes that some of the prompts in HealthBench were generated by LLMs, not users, which could limit how well the benchmark translates into the real world.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;An LLM that avoids alarmism seems like a clear improvement over systems that have people convincing themselves they have cancer after a few minutes of browsing. And as large language models, and the products built around them, continue to develop, whatever advantage Dr. ChatGPT has over Dr. Google will likely grow. The introduction of ChatGPT Health is certainly a move in that direction: By looking through your medical records, ChatGPT can potentially gain far more context about your specific health situation than could be included in any Google search, although numerous experts have cautioned against giving ChatGPT that access for privacy reasons.&lt;/p&gt;  &lt;p&gt;Even if ChatGPT Health and other new tools do represent a meaningful improvement over Google searches, they could still conceivably have a negative effect on health overall. Much as automated vehicles, even if they are safer than human-driven cars, might still prove a net negative if they encourage people to use public transit less, LLMs could undermine users’ health if they induce people to rely on the internet instead of human doctors, even if they do increase the quality of health information available online.&lt;/p&gt;  &lt;p&gt;Lederman says that this outcome is plausible. In her research, she has found that members of online communities centered on health tend to put their trust in users who express themselves well, regardless of the validity of the information they are sharing. Because ChatGPT communicates like an articulate person, some people might trust it too much, potentially to the exclusion of their doctor. But LLMs are certainly no replacement for a human doctor—at least not yet.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/01/chatgpt-health2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;For the past two decades, there’s been a clear first step for anyone who starts experiencing new medical symptoms: Look them up online. The practice was so common that it gained the pejorative moniker “Dr. Google.” But times are changing, and many medical-information seekers are now using LLMs. According to OpenAI, 230 million people ask ChatGPT health-related queries each week.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s the context around the launch of OpenAI’s new ChatGPT Health product, which debuted earlier this month. It landed at an inauspicious time: Two days earlier, the news website SFGate had broken the story of Sam Nelson, a teenager who died of an overdose last year after extensive conversations with ChatGPT about how best to combine various drugs. In the wake of both pieces of news, multiple journalists questioned the wisdom of relying for medical advice on a tool that could cause such extreme harm.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Though ChatGPT Health lives in a separate sidebar tab from the rest of ChatGPT, it isn’t a new model. It’s more like a wrapper that provides one of OpenAI’s preexisting models with guidance and tools it can use to provide health advice—including some that allow it to access a user’s electronic medical records and fitness app data, if granted permission. There’s no doubt that ChatGPT and other large language models can make medical mistakes, and OpenAI emphasizes that ChatGPT Health is intended as an additional support, rather than a replacement for one’s doctor. But when doctors are unavailable or unable to help, people will turn to alternatives.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Some doctors see LLMs as a boon for medical literacy. The average patient might struggle to navigate the vast landscape of online medical information—and, in particular, to distinguish high-quality sources from polished but factually dubious websites—but LLMs can do that job for them, at least in theory. Treating patients who had searched for their symptoms on Google required “a lot of attacking patient anxiety [and] reducing misinformation,” says Marc Succi, an associate professor at Harvard Medical School and a practicing radiologist. But now, he says, “you see patients with a college education, a high school education, asking questions at the level of something an early med student might ask.”&lt;/p&gt; 
 &lt;p&gt;The release of ChatGPT Health, and Anthropic’s subsequent announcement of new health integrations for Claude, indicate that the AI giants are increasingly willing to acknowledge and encourage health-related uses of their models. Such uses certainly come with risks, given LLMs’ well-documented tendencies to agree with users and make up information rather than admit ignorance.&amp;nbsp;&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;But those risks also have to be weighed against potential benefits. There’s an analogy here to autonomous vehicles: When policymakers consider whether to allow Waymo in their city, the key metric is not whether its cars are ever involved in accidents but whether they cause less harm than the status quo of relying on human drivers. If Dr. ChatGPT is an improvement over Dr. Google—and early evidence suggests it may be—it could potentially lessen the enormous burden of medical misinformation and unnecessary health anxiety that the internet has created.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Pinning down the effectiveness of a chatbot such as ChatGPT or Claude for consumer health, however, is tricky. “It’s exceedingly difficult to evaluate an open-ended chatbot,” says Danielle Bitterman, the clinical lead for data science and AI at the Mass General Brigham health-care system. Large language models score well on medical licensing examinations, but those exams use multiple-choice questions that don’t reflect how people use chatbots to look up medical information.&lt;/p&gt; 
 &lt;p&gt;Sirisha Rambhatla, an assistant professor of management science and engineering at the University of Waterloo, attempted to close that gap by evaluating how GPT-4o responded to licensing exam questions when it did not have access to a list of possible answers. Medical experts who evaluated the responses scored only about half of them as entirely correct. But multiple-choice exam questions are designed to be tricky enough that the answer options don’t give them entirely away, and they’re still a pretty distant approximation for the sort of thing that a user would type into ChatGPT.&lt;/p&gt;  &lt;p&gt;A different study, which tested GPT-4o on more realistic prompts submitted by human volunteers, found that it answered medical questions correctly about 85% of the time. When I spoke with Amulya Yadav, an associate professor at Pennsylvania State University who runs the Responsible AI for Social Emancipation Lab and led the study, he made it clear that he wasn’t personally a fan of patient-facing medical LLMs. But he freely admits that, technically speaking, they seem up to the task—after all, he says, human doctors misdiagnose patients 10% to 15% of the time. “If I look at it dispassionately, it seems that the world is gonna change, whether I like it or not,” he says.&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;For people seeking medical information online, Yadav says, LLMs do seem to be a better choice than Google. Succi, the radiologist, also concluded that LLMs can be a better alternative to web search when he compared GPT-4’s responses to questions about common chronic medical conditions with the information presented in Google’s knowledge panel, the information box that sometimes appears on the right side of the search results.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;Since Yadav’s and Succi’s studies appeared online, in the first half of 2025, OpenAI has released multiple new versions of GPT, and it’s reasonable to expect that GPT-5.2 would perform even better than its predecessors. But the studies do have important limitations: They focus on straightforward, factual questions, and they examine only brief interactions between users and chatbots or web search tools. Some of the weaknesses of LLMs—most notably their sycophancy and tendency to hallucinate—might be more likely to rear their heads in more extensive conversations and with people who are dealing with more complex problems. Reeva Lederman, a professor at the University of Melbourne who studies technology and health, notes that patients who don’t like the diagnosis or treatment recommendations that they receive from a doctor might seek out another opinion from an LLM—and the LLM, if it’s sycophantic, might encourage them to reject their doctor’s advice.&lt;/p&gt;  &lt;div class="wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex"&gt; &lt;p&gt;Some studies have found that LLMs will hallucinate and exhibit sycophancy in response to health-related prompts. For example, one study showed that GPT-4 and GPT-4o will happily accept and run with incorrect drug information included in a user’s question. In another, GPT-4o frequently concocted definitions for fake syndromes and lab tests mentioned in the user’s prompt. Given the abundance of medically dubious diagnoses and treatments floating around the internet, these patterns of LLM behavior could contribute to the spread of medical misinformation, particularly if people see LLMs as trustworthy.&lt;/p&gt; &lt;/div&gt;  &lt;p&gt;OpenAI has reported that the GPT-5 series of models is markedly less sycophantic and prone to hallucination than their predecessors, so the results of these studies might not apply to ChatGPT Health. The company also evaluated the model that powers ChatGPT Health on its responses to health-specific questions, using their publicly available HeathBench benchmark. HealthBench rewards models that express uncertainty when appropriate, recommend that users seek medical attention when necessary, and refrain from causing users unnecessary stress by telling them their condition is more serious that it truly is. It’s reasonable to assume that the model underlying ChatGPT Health exhibited those behaviors in testing, though Bitterman notes that some of the prompts in HealthBench were generated by LLMs, not users, which could limit how well the benchmark translates into the real world.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt;&lt;p&gt;An LLM that avoids alarmism seems like a clear improvement over systems that have people convincing themselves they have cancer after a few minutes of browsing. And as large language models, and the products built around them, continue to develop, whatever advantage Dr. ChatGPT has over Dr. Google will likely grow. The introduction of ChatGPT Health is certainly a move in that direction: By looking through your medical records, ChatGPT can potentially gain far more context about your specific health situation than could be included in any Google search, although numerous experts have cautioned against giving ChatGPT that access for privacy reasons.&lt;/p&gt;  &lt;p&gt;Even if ChatGPT Health and other new tools do represent a meaningful improvement over Google searches, they could still conceivably have a negative effect on health overall. Much as automated vehicles, even if they are safer than human-driven cars, might still prove a net negative if they encourage people to use public transit less, LLMs could undermine users’ health if they induce people to rely on the internet instead of human doctors, even if they do increase the quality of health information available online.&lt;/p&gt;  &lt;p&gt;Lederman says that this outcome is plausible. In her research, she has found that members of online communities centered on health tend to put their trust in users who express themselves well, regardless of the validity of the information they are sharing. Because ChatGPT communicates like an articulate person, some people might trust it too much, potentially to the exclusion of their doctor. But LLMs are certainly no replacement for a human doctor—at least not yet.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/22/1131692/dr-google-had-its-issues-can-chatgpt-health-do-better/</guid><pubDate>Thu, 22 Jan 2026 17:38:09 +0000</pubDate></item><item><title>[NEW] Google now offers free SAT practice exams, powered by Gemini (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/22/google-now-offers-free-sat-practice-exams-powered-by-gemini/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Prepping for the SAT is nobody’s idea of fun, but Google aims to make it less stressful with AI. The company announced that it’s now focusing its AI education efforts on standardized testing with free SAT practice exams powered by Gemini.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Students can prompt Gemini by typing “I want to take a practice SAT test,” and the AI will provide them with a free practice exam. Gemini then analyzes the results, highlighting strengths and identifying areas that need further review. It also offers detailed explanations for any incorrect answers.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3085165" height="371" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-22-at-1.27.15-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it partnered with education companies like The Princeton Review to ensure the content is vetted and that students are working with questions that closely mirror what they will encounter on the actual SAT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This recent move by Google is viewed as a game-changer for students who can’t readily access personalized SAT tutoring. By making SAT prep free, Google is trying to open the door for more students to compete on equal footing. However, it also sparks a broader conversation about the role of AI in education and just how much we want AI to shape how students learn.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The adoption of AI in education is not without controversy. Many teachers worry that students might end up leaning too heavily on tools like Gemini and ChatGPT to get their work done. If students let AI do all the thinking, it could chip away at their problem-solving skills. There are even studies out there that back this up, suggesting that relying too much on AI can actually weaken students’ ability to think critically and tackle challenges on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Let’s also not forget about human SAT tutors. Free AI-powered exam prep poses a significant threat to the traditional tutoring industry, which has long thrived on providing personalized coaching to college-bound students. With Google offering a free alternative, the job security of private SAT tutors may be at risk.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This news follows Google’s recent launch of a Gemini-powered feature that lets teachers create podcast-style audio lessons, which could help catch the attention of Gen Z students. Other available Gemini tools include features that help teachers brainstorm ideas, build lesson plans, and tailor learning materials for their classes.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Prepping for the SAT is nobody’s idea of fun, but Google aims to make it less stressful with AI. The company announced that it’s now focusing its AI education efforts on standardized testing with free SAT practice exams powered by Gemini.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Students can prompt Gemini by typing “I want to take a practice SAT test,” and the AI will provide them with a free practice exam. Gemini then analyzes the results, highlighting strengths and identifying areas that need further review. It also offers detailed explanations for any incorrect answers.&amp;nbsp;&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3085165" height="371" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-22-at-1.27.15-PM.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Google&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company said it partnered with education companies like The Princeton Review to ensure the content is vetted and that students are working with questions that closely mirror what they will encounter on the actual SAT.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This recent move by Google is viewed as a game-changer for students who can’t readily access personalized SAT tutoring. By making SAT prep free, Google is trying to open the door for more students to compete on equal footing. However, it also sparks a broader conversation about the role of AI in education and just how much we want AI to shape how students learn.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The adoption of AI in education is not without controversy. Many teachers worry that students might end up leaning too heavily on tools like Gemini and ChatGPT to get their work done. If students let AI do all the thinking, it could chip away at their problem-solving skills. There are even studies out there that back this up, suggesting that relying too much on AI can actually weaken students’ ability to think critically and tackle challenges on their own.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Let’s also not forget about human SAT tutors. Free AI-powered exam prep poses a significant threat to the traditional tutoring industry, which has long thrived on providing personalized coaching to college-bound students. With Google offering a free alternative, the job security of private SAT tutors may be at risk.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This news follows Google’s recent launch of a Gemini-powered feature that lets teachers create podcast-style audio lessons, which could help catch the attention of Gen Z students. Other available Gemini tools include features that help teachers brainstorm ideas, build lesson plans, and tailor learning materials for their classes.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/22/google-now-offers-free-sat-practice-exams-powered-by-gemini/</guid><pubDate>Thu, 22 Jan 2026 18:27:36 +0000</pubDate></item></channel></rss>