<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 24 Feb 2026 02:26:26 +0000</lastBuildDate><item><title>How AI agents could destroy the economy (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/how-ai-agents-could-destroy-the-economy/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/04/GettyImages-1167037043-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Sunday, an analyst group called Citrini Research published a remarkable piece illustrating how agentic AI could bring on mass economic destruction over the next two years. The scenario imagines a report from two years in the future, in which unemployment has doubled, and the total value of the stock market has fallen by more than a third. As the report puts it:&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;AI capabilities improved, companies needed fewer workers, white collar layoffs increased, displaced workers spent less, margin pressure pushed firms to invest more in AI, AI capabilities improved…&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;It was a negative feedback loop with no natural brake…The system turned out to be one long daisy chain of correlated bets on white-collar productivity growth.&amp;nbsp;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a new kind of bear case, focused not on Skynet-style misalignment but on the gradual unspooling of the economy itself. In particular, the Citrini scenario looks at the implications of integrating AI agents into the economy at large, and what it would mean when outside contractors get replaced by cheaper in-house AI. It’s similar to the Death of SaaS scenario, but Citrini goes further, implicating any business model that involves optimizing transactions between companies.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As you might expect, the report is causing quite a stir online. Not everyone is buying it — even Citrini describes it as more of a scenario than a prediction — but it’s not so easy to name the specific point where you think the scenario goes wrong.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Personally, I’m not sure companies are ready to hand off purchasing decisions to AI agents, no matter how smart they are. But in Citrini’s scenario, most of the impacted decisions have already been handed off to third-party contractors, so it’s not quite as implausible as it seems.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2020/04/GettyImages-1167037043-1.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;On Sunday, an analyst group called Citrini Research published a remarkable piece illustrating how agentic AI could bring on mass economic destruction over the next two years. The scenario imagines a report from two years in the future, in which unemployment has doubled, and the total value of the stock market has fallen by more than a third. As the report puts it:&lt;/p&gt;

&lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt;
&lt;p class="wp-block-paragraph"&gt;AI capabilities improved, companies needed fewer workers, white collar layoffs increased, displaced workers spent less, margin pressure pushed firms to invest more in AI, AI capabilities improved…&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;It was a negative feedback loop with no natural brake…The system turned out to be one long daisy chain of correlated bets on white-collar productivity growth.&amp;nbsp;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a new kind of bear case, focused not on Skynet-style misalignment but on the gradual unspooling of the economy itself. In particular, the Citrini scenario looks at the implications of integrating AI agents into the economy at large, and what it would mean when outside contractors get replaced by cheaper in-house AI. It’s similar to the Death of SaaS scenario, but Citrini goes further, implicating any business model that involves optimizing transactions between companies.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;As you might expect, the report is causing quite a stir online. Not everyone is buying it — even Citrini describes it as more of a scenario than a prediction — but it’s not so easy to name the specific point where you think the scenario goes wrong.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Personally, I’m not sure companies are ready to hand off purchasing decisions to AI agents, no matter how smart they are. But in Citrini’s scenario, most of the impacted decisions have already been handed off to third-party contractors, so it’s not quite as implausible as it seems.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/how-ai-agents-could-destroy-the-economy/</guid><pubDate>Mon, 23 Feb 2026 14:44:03 +0000</pubDate></item><item><title>5 days left to lock in the lowest TechCrunch Disrupt 2026 ticket rates (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/5-days-left-to-lock-in-the-lowest-techcrunch-disrupt-2026-ticket-rates/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;We are officially down to the final 5 days to &lt;strong&gt;save up to $680&lt;/strong&gt; on your&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2026&lt;/strong&gt;&amp;nbsp;ticket. These lowest rates of the year disappear on February 27 at 11:59 p.m. PT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If&amp;nbsp;you’ve&amp;nbsp;been mapping out your 2026 tech event calendar, this&amp;nbsp;isn’t&amp;nbsp;the moment to wait.&amp;nbsp;&lt;strong&gt;Register now to lock in your savings before prices increase&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2026 5 days left" class="wp-image-3094858" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/TCD26_5Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-to-expect-at-disrupt-nbsp-2026"&gt;What to expect at Disrupt&amp;nbsp;2026&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Each year,&amp;nbsp;&lt;strong&gt;Disrupt&lt;/strong&gt;&amp;nbsp;brings together 10,000+ founders, tech leaders, and VCs at San Francisco’s Moscone West. From October 13–15,&amp;nbsp;you’ll&amp;nbsp;gain valuable takeaways and curated networking opportunities designed to elevate your&amp;nbsp;startup&amp;nbsp;trajectory, accelerate your career,&amp;nbsp;or&amp;nbsp;strengthen your portfolio.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-key-insights-from-today-s-tech-heavyweights"&gt;Key insights from today’s tech heavyweights&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Roy Lee" class="wp-image-3095350" height="453" src="https://techcrunch.com/wp-content/uploads/2026/02/Disrupt-2025-Roy-Lee.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Disrupt featured 200+ onstage conversations with 250+ top voices shaping the tech ecosystem. Expect the same level of powerful, candid conversations this year.&amp;nbsp;Highlights from 2025 include the following:&amp;nbsp;&lt;/p&gt;













&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Waymo co-CEO&amp;nbsp;Tekedra&amp;nbsp;Mawakana&amp;nbsp;made it clear onstage&amp;nbsp;that the company will not stand for vandalism against its robotaxis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Keep an eye on the&amp;nbsp;&lt;strong&gt;event page&lt;/strong&gt;&amp;nbsp;as we roll out the 2026 agenda.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-create-meaningful-connections-through-curated-networking"&gt;Create meaningful connections through curated networking&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, more than 20,000 curated meetings took place across three days. This year,&amp;nbsp;we’re&amp;nbsp;rolling out improved networking technology to make those connections even more targeted and efficient.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 networking student" class="wp-image-2896237" height="453" src="https://techcrunch.com/wp-content/uploads/2024/10/Networking_disrupt.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Meet the one person who can change the trajectory of your startup. It only takes one. You get:&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Direct access to founders, VCs, and operators actively building.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Conversations that turn into funding, partnerships, and key hires.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Tactical insights you can apply&amp;nbsp;immediately.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Early visibility into where tech is heading next.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 class="wp-block-heading" id="h-witness-nbsp-the-intense-startup-pitch-showdown"&gt;Witness&amp;nbsp;the intense startup pitch showdown&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Startup Battlefield&lt;/strong&gt;&amp;nbsp;is where 200 TechCrunch-selected, pre-Series A startups compete for $100,000 in equity-free funding, global visibility, and direct access to the industry’s top investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This iconic pitch competition has helped launch breakout companies like Discord, Cloudflare, and Trello.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Kevin A. Damoa, Founder &amp;amp; CEO, Glīd, Claire Kroft and Ankit Malhotra, winners of the Startup Battlefield 2025, pose onstage during day three of TechCrunch Disrupt 2025 at Moscone Center on October 29, 2025 in San Francisco, California." class="wp-image-3070746" height="453" src="https://techcrunch.com/wp-content/uploads/2025/11/54889393608_2493644601_o-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-first-row-seat-to-innovative-breakthroughs"&gt;First-row seat to innovative breakthroughs&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Over 300 startup exhibitors&lt;/strong&gt;&amp;nbsp;will&amp;nbsp;showcase&amp;nbsp;innovations across the venue, especially in the Expo Hall,&amp;nbsp;where foot traffic converges. Discover tomorrow’s breakthroughs and today’s solutions — all in one place.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-more-ways-to-connect-with-the-bay-area-tech-scene"&gt;More ways to connect with the Bay Area tech scene&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Throughout Disrupt Week, October 11–17, TechCrunch Disrupt Side Events will take place across the Bay Area beyond the main venue. Attend a post-event cocktail hour, grab breakfast before the day begins, or even host your own off-site panel. The opportunities to make powerful connections around Disrupt are endless.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Side Events" class="wp-image-2552128" height="383" src="https://techcrunch.com/wp-content/uploads/2023/05/After-Hours.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-last-5-days-to-secure-the-lowest-rates"&gt;Last 5 days to secure the lowest rates&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Five days&amp;nbsp;remain&amp;nbsp;to lock&amp;nbsp;in&amp;nbsp;the lowest rate of the year. Prices increase after February 27 at 11:59 p.m. PT.&amp;nbsp;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;and secure your savings&amp;nbsp;of up to $680&amp;nbsp;before&amp;nbsp;they’re&amp;nbsp;gone.&amp;nbsp;Save up to 30% on&amp;nbsp;&lt;strong&gt;group passes&lt;/strong&gt;.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;We are officially down to the final 5 days to &lt;strong&gt;save up to $680&lt;/strong&gt; on your&amp;nbsp;&lt;strong&gt;TechCrunch Disrupt 2026&lt;/strong&gt;&amp;nbsp;ticket. These lowest rates of the year disappear on February 27 at 11:59 p.m. PT.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;If&amp;nbsp;you’ve&amp;nbsp;been mapping out your 2026 tech event calendar, this&amp;nbsp;isn’t&amp;nbsp;the moment to wait.&amp;nbsp;&lt;strong&gt;Register now to lock in your savings before prices increase&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2026 5 days left" class="wp-image-3094858" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/TCD26_5Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-what-to-expect-at-disrupt-nbsp-2026"&gt;What to expect at Disrupt&amp;nbsp;2026&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Each year,&amp;nbsp;&lt;strong&gt;Disrupt&lt;/strong&gt;&amp;nbsp;brings together 10,000+ founders, tech leaders, and VCs at San Francisco’s Moscone West. From October 13–15,&amp;nbsp;you’ll&amp;nbsp;gain valuable takeaways and curated networking opportunities designed to elevate your&amp;nbsp;startup&amp;nbsp;trajectory, accelerate your career,&amp;nbsp;or&amp;nbsp;strengthen your portfolio.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-key-insights-from-today-s-tech-heavyweights"&gt;Key insights from today’s tech heavyweights&lt;/h3&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Roy Lee" class="wp-image-3095350" height="453" src="https://techcrunch.com/wp-content/uploads/2026/02/Disrupt-2025-Roy-Lee.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, Disrupt featured 200+ onstage conversations with 250+ top voices shaping the tech ecosystem. Expect the same level of powerful, candid conversations this year.&amp;nbsp;Highlights from 2025 include the following:&amp;nbsp;&lt;/p&gt;













&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Waymo co-CEO&amp;nbsp;Tekedra&amp;nbsp;Mawakana&amp;nbsp;made it clear onstage&amp;nbsp;that the company will not stand for vandalism against its robotaxis.&lt;/li&gt;
&lt;/ul&gt;

&lt;p class="wp-block-paragraph"&gt;Keep an eye on the&amp;nbsp;&lt;strong&gt;event page&lt;/strong&gt;&amp;nbsp;as we roll out the 2026 agenda.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-create-meaningful-connections-through-curated-networking"&gt;Create meaningful connections through curated networking&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Last year, more than 20,000 curated meetings took place across three days. This year,&amp;nbsp;we’re&amp;nbsp;rolling out improved networking technology to make those connections even more targeted and efficient.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 networking student" class="wp-image-2896237" height="453" src="https://techcrunch.com/wp-content/uploads/2024/10/Networking_disrupt.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Meet the one person who can change the trajectory of your startup. It only takes one. You get:&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Direct access to founders, VCs, and operators actively building.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Conversations that turn into funding, partnerships, and key hires.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Tactical insights you can apply&amp;nbsp;immediately.&lt;/li&gt;
&lt;/ul&gt;

&lt;ul class="wp-block-list"&gt;
&lt;li class="wp-block-list-item"&gt;Early visibility into where tech is heading next.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 class="wp-block-heading" id="h-witness-nbsp-the-intense-startup-pitch-showdown"&gt;Witness&amp;nbsp;the intense startup pitch showdown&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Startup Battlefield&lt;/strong&gt;&amp;nbsp;is where 200 TechCrunch-selected, pre-Series A startups compete for $100,000 in equity-free funding, global visibility, and direct access to the industry’s top investors.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This iconic pitch competition has helped launch breakout companies like Discord, Cloudflare, and Trello.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Kevin A. Damoa, Founder &amp;amp; CEO, Glīd, Claire Kroft and Ankit Malhotra, winners of the Startup Battlefield 2025, pose onstage during day three of TechCrunch Disrupt 2025 at Moscone Center on October 29, 2025 in San Francisco, California." class="wp-image-3070746" height="453" src="https://techcrunch.com/wp-content/uploads/2025/11/54889393608_2493644601_o-2.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-first-row-seat-to-innovative-breakthroughs"&gt;First-row seat to innovative breakthroughs&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Over 300 startup exhibitors&lt;/strong&gt;&amp;nbsp;will&amp;nbsp;showcase&amp;nbsp;innovations across the venue, especially in the Expo Hall,&amp;nbsp;where foot traffic converges. Discover tomorrow’s breakthroughs and today’s solutions — all in one place.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-more-ways-to-connect-with-the-bay-area-tech-scene"&gt;More ways to connect with the Bay Area tech scene&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Throughout Disrupt Week, October 11–17, TechCrunch Disrupt Side Events will take place across the Bay Area beyond the main venue. Attend a post-event cocktail hour, grab breakfast before the day begins, or even host your own off-site panel. The opportunities to make powerful connections around Disrupt are endless.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2024 Side Events" class="wp-image-2552128" height="383" src="https://techcrunch.com/wp-content/uploads/2023/05/After-Hours.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-last-5-days-to-secure-the-lowest-rates"&gt;Last 5 days to secure the lowest rates&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;Five days&amp;nbsp;remain&amp;nbsp;to lock&amp;nbsp;in&amp;nbsp;the lowest rate of the year. Prices increase after February 27 at 11:59 p.m. PT.&amp;nbsp;&lt;strong&gt;Register now&lt;/strong&gt;&amp;nbsp;and secure your savings&amp;nbsp;of up to $680&amp;nbsp;before&amp;nbsp;they’re&amp;nbsp;gone.&amp;nbsp;Save up to 30% on&amp;nbsp;&lt;strong&gt;group passes&lt;/strong&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/5-days-left-to-lock-in-the-lowest-techcrunch-disrupt-2026-ticket-rates/</guid><pubDate>Mon, 23 Feb 2026 15:00:00 +0000</pubDate></item><item><title>AIs can generate near-verbatim copies of novels from training data (AI - Ars Technica)</title><link>https://arstechnica.com/ai/2026/02/ais-can-generate-near-verbatim-copies-of-novels-from-training-data/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        LLMs memorize more training data than previously thought.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Library shelves filled with books" class="absolute inset-0 w-full h-full object-cover hidden" height="451" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-640x451.jpg" width="640" /&gt;
                  &lt;img alt="Library shelves filled with books" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-1152x648-1768598730.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          MediaNews Group/Reading Eagle via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The world’s top AI models can be prompted to generate near-verbatim copies of bestselling novels, raising fresh questions about the industry’s claim that its systems do not store copyrighted works.&lt;/p&gt;
&lt;p&gt;A series of recent studies has shown that large language models from OpenAI, Google, Meta, Anthropic, and xAI memorize far more of their training data than previously thought.&lt;/p&gt;
&lt;p&gt;AI and legal experts told the FT this “memorization” ability could have serious ramifications on AI groups’ battle against dozens of copyright lawsuits around the world, as it undermines their core defense that LLMs “learn” from copyrighted works but do not store copies.&lt;/p&gt;
&lt;p&gt;“There’s growing evidence that memorization is a bigger thing than previously believed,” said Yves-Alexandre de Montjoye, a professor of applied mathematics and computer science at Imperial College London.&lt;/p&gt;
&lt;p&gt;AI groups have long argued that memorization does not happen. In a 2023 letter to the US Copyright Office, Google said “there is no copy of the training data—whether text, images, or other formats—present in the model itself.”&lt;/p&gt;
&lt;p&gt;The AI industry also claims that training models on copyrighted books is “fair use,” arguing that the technology transforms the original work into something meaningfully new.&lt;/p&gt;
&lt;p&gt;But a study published last month showed that researchers at Stanford and Yale Universities were able to strategically prompt LLMs from OpenAI, Google, Anthropic, and xAI to generate thousands of words from 13 books, including &lt;em&gt;A Game of Thrones&lt;/em&gt;, &lt;em&gt;The Hunger Games&lt;/em&gt;, and &lt;em&gt;The Hobbit.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By asking models to complete sentences from a book, Gemini 2.5 regurgitated 76.8 percent of &lt;em&gt;Harry Potter and the Philosopher’s Stone&lt;/em&gt; with high levels of accuracy, while Grok 3 generated 70.3 percent.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;They were also able to extract almost the entirety of the novel “near-verbatim” from Anthropic’s Claude 3.7 Sonnet by jailbreaking the model, where users can prompt LLMs to disregard their safeguards.&lt;/p&gt;
&lt;p&gt;It builds on a study from last year that found “open” models, such as Meta’s Llama, memorize huge parts of particular books in their training data.&lt;/p&gt;
&lt;p&gt;AI experts were previously unsure whether closed models, which tend to have more safeguards that prevent models from generating unwanted content, would also be prone to large-scale memorization.&lt;/p&gt;
&lt;p&gt;“It was a surprise that they could memorize entire texts” despite guardrails, said A. Feder Cooper, a researcher at Yale University, who was part of the study.&lt;/p&gt;
&lt;p&gt;Researchers have not yet worked out why LLMs memorize things that appear in their training data. It also remains unclear how much of the training data is evident in the outputs they generate.&lt;/p&gt;
&lt;p&gt;This memorization feature could also have serious implications in other sectors such as health care and education, where leakage of any training data could lead to privacy and confidentiality issues.&lt;/p&gt;
&lt;p&gt;Legal experts said it could potentially create a significant liability for AI groups regarding copyright infringement, as well as ramifications for how AI companies train their models and the costs of developing them.&lt;/p&gt;
&lt;p&gt;The research findings “could present a challenge to those who argue that the AI model does not store or reproduce any copyright works,” said Cerys Wyn Davies, an intellectual property partner at law firm Pinsent Masons.&lt;/p&gt;
&lt;p&gt;Whether or not AI models memorize their training data has played an important factor in recent legal battles over copyright.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;A US court last year found that Anthropic’s training of LLMs on some copyrighted content could be considered fair use as it was deemed “transformative.”&lt;/p&gt;
&lt;p&gt;But it determined that storing pirated works was “inherently, irredeemably infringing,” which then led the AI group to pay $1.5 billion to settle the lawsuit.&lt;/p&gt;
&lt;p&gt;In Germany, a ruling from November last year found that OpenAI had infringed on copyright because its model had memorized song lyrics. The case, brought by GEMA, an association representing composers, lyricists, and publishers, was considered a landmark ruling in the EU.&lt;/p&gt;
&lt;p&gt;Rudy Telscher, a partner at law firm Husch Blackwell, said reproducing an entire book without jailbreaking is “clearly a copyright violation.” But “it’s a matter of whether this is happening enough that [AI models] could be vicariously liable for the infringement,” he added.&lt;/p&gt;
&lt;p&gt;Anthropic said the jailbreaking technique used in the Stanford and Yale research was impractical for normal users and would require more effort to extract the text than just purchasing the content.&lt;/p&gt;
&lt;p&gt;The company also added that its model does not store copies of specific datasets but learns from patterns and relationships between words and strings in its training data.&lt;/p&gt;
&lt;p&gt;xAI, OpenAI, and Google did not respond to requests for comment.&lt;/p&gt;
&lt;p&gt;The fact that AI labs have put safeguards in place to prevent training data from being extracted means they are aware of the problem, said Imperial’s de Montjoye.&lt;/p&gt;
&lt;p&gt;Ben Zhao, a computer science professor at the University of Chicago, questioned whether AI labs really needed to use copyrighted content in training data to create cutting-edge models in the first place.&lt;/p&gt;
&lt;p&gt;“Whether the technical result can be done or not, it’s still a question of should we be doing this?” Zhao said. “The legal side should eventually hold their ground and really be the arbiter in this whole process.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2026 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        LLMs memorize more training data than previously thought.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Library shelves filled with books" class="absolute inset-0 w-full h-full object-cover hidden" height="451" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-640x451.jpg" width="640" /&gt;
                  &lt;img alt="Library shelves filled with books" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/01/library-shelves-1152x648-1768598730.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          MediaNews Group/Reading Eagle via Getty Images

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;The world’s top AI models can be prompted to generate near-verbatim copies of bestselling novels, raising fresh questions about the industry’s claim that its systems do not store copyrighted works.&lt;/p&gt;
&lt;p&gt;A series of recent studies has shown that large language models from OpenAI, Google, Meta, Anthropic, and xAI memorize far more of their training data than previously thought.&lt;/p&gt;
&lt;p&gt;AI and legal experts told the FT this “memorization” ability could have serious ramifications on AI groups’ battle against dozens of copyright lawsuits around the world, as it undermines their core defense that LLMs “learn” from copyrighted works but do not store copies.&lt;/p&gt;
&lt;p&gt;“There’s growing evidence that memorization is a bigger thing than previously believed,” said Yves-Alexandre de Montjoye, a professor of applied mathematics and computer science at Imperial College London.&lt;/p&gt;
&lt;p&gt;AI groups have long argued that memorization does not happen. In a 2023 letter to the US Copyright Office, Google said “there is no copy of the training data—whether text, images, or other formats—present in the model itself.”&lt;/p&gt;
&lt;p&gt;The AI industry also claims that training models on copyrighted books is “fair use,” arguing that the technology transforms the original work into something meaningfully new.&lt;/p&gt;
&lt;p&gt;But a study published last month showed that researchers at Stanford and Yale Universities were able to strategically prompt LLMs from OpenAI, Google, Anthropic, and xAI to generate thousands of words from 13 books, including &lt;em&gt;A Game of Thrones&lt;/em&gt;, &lt;em&gt;The Hunger Games&lt;/em&gt;, and &lt;em&gt;The Hobbit.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;By asking models to complete sentences from a book, Gemini 2.5 regurgitated 76.8 percent of &lt;em&gt;Harry Potter and the Philosopher’s Stone&lt;/em&gt; with high levels of accuracy, while Grok 3 generated 70.3 percent.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;They were also able to extract almost the entirety of the novel “near-verbatim” from Anthropic’s Claude 3.7 Sonnet by jailbreaking the model, where users can prompt LLMs to disregard their safeguards.&lt;/p&gt;
&lt;p&gt;It builds on a study from last year that found “open” models, such as Meta’s Llama, memorize huge parts of particular books in their training data.&lt;/p&gt;
&lt;p&gt;AI experts were previously unsure whether closed models, which tend to have more safeguards that prevent models from generating unwanted content, would also be prone to large-scale memorization.&lt;/p&gt;
&lt;p&gt;“It was a surprise that they could memorize entire texts” despite guardrails, said A. Feder Cooper, a researcher at Yale University, who was part of the study.&lt;/p&gt;
&lt;p&gt;Researchers have not yet worked out why LLMs memorize things that appear in their training data. It also remains unclear how much of the training data is evident in the outputs they generate.&lt;/p&gt;
&lt;p&gt;This memorization feature could also have serious implications in other sectors such as health care and education, where leakage of any training data could lead to privacy and confidentiality issues.&lt;/p&gt;
&lt;p&gt;Legal experts said it could potentially create a significant liability for AI groups regarding copyright infringement, as well as ramifications for how AI companies train their models and the costs of developing them.&lt;/p&gt;
&lt;p&gt;The research findings “could present a challenge to those who argue that the AI model does not store or reproduce any copyright works,” said Cerys Wyn Davies, an intellectual property partner at law firm Pinsent Masons.&lt;/p&gt;
&lt;p&gt;Whether or not AI models memorize their training data has played an important factor in recent legal battles over copyright.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;A US court last year found that Anthropic’s training of LLMs on some copyrighted content could be considered fair use as it was deemed “transformative.”&lt;/p&gt;
&lt;p&gt;But it determined that storing pirated works was “inherently, irredeemably infringing,” which then led the AI group to pay $1.5 billion to settle the lawsuit.&lt;/p&gt;
&lt;p&gt;In Germany, a ruling from November last year found that OpenAI had infringed on copyright because its model had memorized song lyrics. The case, brought by GEMA, an association representing composers, lyricists, and publishers, was considered a landmark ruling in the EU.&lt;/p&gt;
&lt;p&gt;Rudy Telscher, a partner at law firm Husch Blackwell, said reproducing an entire book without jailbreaking is “clearly a copyright violation.” But “it’s a matter of whether this is happening enough that [AI models] could be vicariously liable for the infringement,” he added.&lt;/p&gt;
&lt;p&gt;Anthropic said the jailbreaking technique used in the Stanford and Yale research was impractical for normal users and would require more effort to extract the text than just purchasing the content.&lt;/p&gt;
&lt;p&gt;The company also added that its model does not store copies of specific datasets but learns from patterns and relationships between words and strings in its training data.&lt;/p&gt;
&lt;p&gt;xAI, OpenAI, and Google did not respond to requests for comment.&lt;/p&gt;
&lt;p&gt;The fact that AI labs have put safeguards in place to prevent training data from being extracted means they are aware of the problem, said Imperial’s de Montjoye.&lt;/p&gt;
&lt;p&gt;Ben Zhao, a computer science professor at the University of Chicago, questioned whether AI labs really needed to use copyrighted content in training data to create cutting-edge models in the first place.&lt;/p&gt;
&lt;p&gt;“Whether the technical result can be done or not, it’s still a question of should we be doing this?” Zhao said. “The legal side should eventually hold their ground and really be the arbiter in this whole process.”&lt;/p&gt;
&lt;p&gt;&lt;em&gt;© 2026 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way.&lt;/em&gt;&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2026/02/ais-can-generate-near-verbatim-copies-of-novels-from-training-data/</guid><pubDate>Mon, 23 Feb 2026 15:38:00 +0000</pubDate></item><item><title>NVIDIA Brings AI-Powered Cybersecurity to World’s Critical Infrastructure (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/ai-cybersecurity-operational-technology-industrial-control-systems/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/cyber-gen-ai-press-1920x1080-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;As technologies and systems become more digitalized and connected across the world, operational technology (OT) environments and industrial control systems (ICS) — from energy and manufacturing to transportation and utilities — are increasingly depending on enterprise networks and the cloud. This expands OT and ICS capabilities — but also their exposure to cyber threats.&lt;/p&gt;
&lt;p&gt;Unlike traditional IT environments that manage data and applications, OT systems control real-world processes where cyber incidents can have immediate consequences for safety, availability and operational continuity.&lt;/p&gt;
&lt;p&gt;Many of these systems were originally designed for reliability and longevity, not for today’s threat techniques. This can widen the gap between modern attacks and existing defenses. Even as OT and ICS environments modernize with improved automation, connectivity and analytics, most were not built to withstand adaptive, software-driven cyberattacks that evolve in real time.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with leading cybersecurity providers Akamai, Forescout, Palo Alto Networks and Xage Security, as well as industrial automation innovator Siemens, to bring accelerated computing and AI to OT cybersecurity, advancing real-time threat detection and response across critical infrastructure.&lt;/p&gt;
&lt;p&gt;These efforts represent a fundamental shift in OT and ICS cybersecurity, where security is embedded into and distributed across infrastructure, enforced at the edge and coordinated through centralized, AI-driven intelligence, bringing modern cybersecurity to the systems that keep the physical world running.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Forescout&lt;/b&gt;&lt;b&gt; and NVIDIA Bring Zero Trust to OT and ICS Environments&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Zero trust is a security model that removes implicit trust from networks. Every user, device and workload must be continuously verified and authorized, regardless of where it originates.&lt;/p&gt;
&lt;p&gt;While zero trust has been widely adopted to secure enterprise IT environments, applying its principles to OT environments has traditionally been difficult. Legacy devices, proprietary protocols and safety-critical operations limit the use of intrusive controls or AI-driven enforcement, even as increased connectivity to IT and cloud environments expands the attack surface.&lt;/p&gt;
&lt;p&gt;Forescout is working with NVIDIA to make zero trust practical for OT. Forescout provides continuous, agentless discovery and classification of OT, internet of things and IT assets, delivering real-time risk assessment and policy-based enforcement. With deep visibility into network activity, Forescout applies network segmentation to contain lateral movement and enforce zero trust controls precisely where they matter most, without impacting operations.&lt;/p&gt;
&lt;p&gt;At the industrial edge, NVIDIA BlueField DPUs run security services on dedicated hardware, keeping protection separate from operational systems so critical processes remain unaffected.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Siemens&lt;/b&gt;&lt;b&gt; and &lt;/b&gt;&lt;b&gt;Palo Alto Networks&lt;/b&gt;&lt;b&gt; Embed Security Into Industrial Automation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Industrial automation environments demand consistent performance, low latency and high availability — requirements that traditional IT security tools often struggle to meet.&lt;/p&gt;
&lt;p&gt;At the S4x26 security conference, Siemens will demonstrate its AI-ready Industrial Automation DataCenter, a unified, holistic solution that consolidates decades of cross-industry automation expertise into one robust IT/OT platform. The future-proof solution contains all the core elements of an edge data center such as computing based on virtualization, data archiving and reporting, resilient disaster recovery solutions, and a robust cybersecurity architecture in accordance with IEC 62443. Through the integration of NVIDIA BlueField, it is uniquely possible to deliver a truly AI-ready, zero-trust solution tailored for the demands on industrial automation.&lt;/p&gt;
&lt;p&gt;Prisma AIRS AI Runtime Security delivers deep visibility into industrial traffic and continuous monitoring for abnormal behavior. By running these security services on NVIDIA BlueField, inspection and enforcement happen directly at the infrastructure level, closer to the workloads. This AI-powered approach strengthens security coverage and drives greater operational uptime where it matters most.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Akamai&lt;/b&gt;&lt;b&gt; Extends Segmentation to OT and ICS With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Akamai Technologies has extended the Akamai Guardicore Platform to now run on NVIDIA BlueField, enabling agentless segmentation — the ability to isolate applications, devices or workloads into tightly controlled security zones — and the ability to enforce zero-trust policies directly at the edge. This removes the need for agents that may not be compatible with legacy OT systems or safety-certified devices.&lt;/p&gt;
&lt;p&gt;Segmentation is enforced at full network speed directly within the infrastructure, without introducing latency or disrupting time-sensitive workloads in centralized data centers or remote edge locations. This helps contain threats quickly, limit their spread and keep mission-critical operations running smoothly.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Xage Security &lt;/b&gt;&lt;b&gt;Protects the Energy Infrastructure That Powers AI With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As AI scales into a pillar of critical infrastructure, securing the energy systems that power AI factories is as essential as securing the compute itself.&lt;/p&gt;
&lt;p&gt;Modern energy supply chains are complex, distributed and deeply interconnected with AI operations, and they operate largely within the operational technology domain. In this environment, cyber-physical systems, legacy assets and real-time controls demand security approaches purpose-built for critical infrastructure protection.&lt;/p&gt;
&lt;p&gt;Xage Security is working with NVIDIA to help address this need by bringing zero-trust security to both energy infrastructure and the AI systems it supports. At S4x26, Xage will demonstrate a new integration running on NVIDIA BlueField, showing how zero trust enforcement can be embedded directly into energy and AI infrastructure environments.&lt;/p&gt;
&lt;p&gt;Xage already protects about 60% of U.S. midstream pipeline infrastructure and works with utilities and energy operators worldwide. By combining Xage’s distributed, identity-based security platform with NVIDIA BlueField, operators can protect energy assets, manage third-party access and secure AI-driven operations at scale without compromising performance, reliability or resilience.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;A New Class of OT Cybersecurity&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Across these environments, a consistent OT cybersecurity architecture is taking shape. Security services run at the edge on NVIDIA BlueField DPUs, close to the operational systems they protect. By executing inspection and enforcement on dedicated, hardware-isolated infrastructure, BlueField enables continuous protection without disrupting time-sensitive operations.&lt;/p&gt;
&lt;p&gt;OT data generated at the edge is sent to centralized AI factories, where it’s analyzed across many sites to identify patterns, anomalies and emerging threats. In addition, security actions are enforced locally at the edge, while insights are shared centrally — creating a coordinated defense that improves visibility, accelerates response and scales protection consistently across OT and IT environments.&lt;/p&gt;
&lt;p&gt;This architecture helps detect and contain threats faster while strengthening resilience across distributed environments, maintaining consistent performance and protecting uptime.&lt;/p&gt;
&lt;p&gt;The result is a new standard for securing critical infrastructure — where AI-driven protection and operational excellence move forward together.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;NVIDIA-powered OT cybersecurity solutions are delivered through a global ecosystem of trusted partners. Read this OT cybersecurity use case and solution overview for more.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Join &lt;/i&gt;&lt;i&gt;NVIDIA at S4x26&lt;/i&gt;&lt;i&gt;, running Feb. 24–26 in Miami, to see how accelerated computing and AI are transforming cybersecurity for OT and critical infrastructure.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2026/02/cyber-gen-ai-press-1920x1080-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;As technologies and systems become more digitalized and connected across the world, operational technology (OT) environments and industrial control systems (ICS) — from energy and manufacturing to transportation and utilities — are increasingly depending on enterprise networks and the cloud. This expands OT and ICS capabilities — but also their exposure to cyber threats.&lt;/p&gt;
&lt;p&gt;Unlike traditional IT environments that manage data and applications, OT systems control real-world processes where cyber incidents can have immediate consequences for safety, availability and operational continuity.&lt;/p&gt;
&lt;p&gt;Many of these systems were originally designed for reliability and longevity, not for today’s threat techniques. This can widen the gap between modern attacks and existing defenses. Even as OT and ICS environments modernize with improved automation, connectivity and analytics, most were not built to withstand adaptive, software-driven cyberattacks that evolve in real time.&lt;/p&gt;
&lt;p&gt;NVIDIA is collaborating with leading cybersecurity providers Akamai, Forescout, Palo Alto Networks and Xage Security, as well as industrial automation innovator Siemens, to bring accelerated computing and AI to OT cybersecurity, advancing real-time threat detection and response across critical infrastructure.&lt;/p&gt;
&lt;p&gt;These efforts represent a fundamental shift in OT and ICS cybersecurity, where security is embedded into and distributed across infrastructure, enforced at the edge and coordinated through centralized, AI-driven intelligence, bringing modern cybersecurity to the systems that keep the physical world running.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Forescout&lt;/b&gt;&lt;b&gt; and NVIDIA Bring Zero Trust to OT and ICS Environments&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Zero trust is a security model that removes implicit trust from networks. Every user, device and workload must be continuously verified and authorized, regardless of where it originates.&lt;/p&gt;
&lt;p&gt;While zero trust has been widely adopted to secure enterprise IT environments, applying its principles to OT environments has traditionally been difficult. Legacy devices, proprietary protocols and safety-critical operations limit the use of intrusive controls or AI-driven enforcement, even as increased connectivity to IT and cloud environments expands the attack surface.&lt;/p&gt;
&lt;p&gt;Forescout is working with NVIDIA to make zero trust practical for OT. Forescout provides continuous, agentless discovery and classification of OT, internet of things and IT assets, delivering real-time risk assessment and policy-based enforcement. With deep visibility into network activity, Forescout applies network segmentation to contain lateral movement and enforce zero trust controls precisely where they matter most, without impacting operations.&lt;/p&gt;
&lt;p&gt;At the industrial edge, NVIDIA BlueField DPUs run security services on dedicated hardware, keeping protection separate from operational systems so critical processes remain unaffected.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Siemens&lt;/b&gt;&lt;b&gt; and &lt;/b&gt;&lt;b&gt;Palo Alto Networks&lt;/b&gt;&lt;b&gt; Embed Security Into Industrial Automation&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Industrial automation environments demand consistent performance, low latency and high availability — requirements that traditional IT security tools often struggle to meet.&lt;/p&gt;
&lt;p&gt;At the S4x26 security conference, Siemens will demonstrate its AI-ready Industrial Automation DataCenter, a unified, holistic solution that consolidates decades of cross-industry automation expertise into one robust IT/OT platform. The future-proof solution contains all the core elements of an edge data center such as computing based on virtualization, data archiving and reporting, resilient disaster recovery solutions, and a robust cybersecurity architecture in accordance with IEC 62443. Through the integration of NVIDIA BlueField, it is uniquely possible to deliver a truly AI-ready, zero-trust solution tailored for the demands on industrial automation.&lt;/p&gt;
&lt;p&gt;Prisma AIRS AI Runtime Security delivers deep visibility into industrial traffic and continuous monitoring for abnormal behavior. By running these security services on NVIDIA BlueField, inspection and enforcement happen directly at the infrastructure level, closer to the workloads. This AI-powered approach strengthens security coverage and drives greater operational uptime where it matters most.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Akamai&lt;/b&gt;&lt;b&gt; Extends Segmentation to OT and ICS With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Akamai Technologies has extended the Akamai Guardicore Platform to now run on NVIDIA BlueField, enabling agentless segmentation — the ability to isolate applications, devices or workloads into tightly controlled security zones — and the ability to enforce zero-trust policies directly at the edge. This removes the need for agents that may not be compatible with legacy OT systems or safety-certified devices.&lt;/p&gt;
&lt;p&gt;Segmentation is enforced at full network speed directly within the infrastructure, without introducing latency or disrupting time-sensitive workloads in centralized data centers or remote edge locations. This helps contain threats quickly, limit their spread and keep mission-critical operations running smoothly.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Xage Security &lt;/b&gt;&lt;b&gt;Protects the Energy Infrastructure That Powers AI With NVIDIA&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;As AI scales into a pillar of critical infrastructure, securing the energy systems that power AI factories is as essential as securing the compute itself.&lt;/p&gt;
&lt;p&gt;Modern energy supply chains are complex, distributed and deeply interconnected with AI operations, and they operate largely within the operational technology domain. In this environment, cyber-physical systems, legacy assets and real-time controls demand security approaches purpose-built for critical infrastructure protection.&lt;/p&gt;
&lt;p&gt;Xage Security is working with NVIDIA to help address this need by bringing zero-trust security to both energy infrastructure and the AI systems it supports. At S4x26, Xage will demonstrate a new integration running on NVIDIA BlueField, showing how zero trust enforcement can be embedded directly into energy and AI infrastructure environments.&lt;/p&gt;
&lt;p&gt;Xage already protects about 60% of U.S. midstream pipeline infrastructure and works with utilities and energy operators worldwide. By combining Xage’s distributed, identity-based security platform with NVIDIA BlueField, operators can protect energy assets, manage third-party access and secure AI-driven operations at scale without compromising performance, reliability or resilience.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;A New Class of OT Cybersecurity&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Across these environments, a consistent OT cybersecurity architecture is taking shape. Security services run at the edge on NVIDIA BlueField DPUs, close to the operational systems they protect. By executing inspection and enforcement on dedicated, hardware-isolated infrastructure, BlueField enables continuous protection without disrupting time-sensitive operations.&lt;/p&gt;
&lt;p&gt;OT data generated at the edge is sent to centralized AI factories, where it’s analyzed across many sites to identify patterns, anomalies and emerging threats. In addition, security actions are enforced locally at the edge, while insights are shared centrally — creating a coordinated defense that improves visibility, accelerates response and scales protection consistently across OT and IT environments.&lt;/p&gt;
&lt;p&gt;This architecture helps detect and contain threats faster while strengthening resilience across distributed environments, maintaining consistent performance and protecting uptime.&lt;/p&gt;
&lt;p&gt;The result is a new standard for securing critical infrastructure — where AI-driven protection and operational excellence move forward together.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;NVIDIA-powered OT cybersecurity solutions are delivered through a global ecosystem of trusted partners. Read this OT cybersecurity use case and solution overview for more.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Join &lt;/i&gt;&lt;i&gt;NVIDIA at S4x26&lt;/i&gt;&lt;i&gt;, running Feb. 24–26 in Miami, to see how accelerated computing and AI are transforming cybersecurity for OT and critical infrastructure.&amp;nbsp;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/ai-cybersecurity-operational-technology-industrial-control-systems/</guid><pubDate>Mon, 23 Feb 2026 16:00:57 +0000</pubDate></item><item><title>Spotify rolls out AI-powered  Prompted Playlists to the UK and other markets (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/spotify-ai-prompted-playlists-uk-markets/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After initially testing its AI-powered “Prompted Playlist” feature in New Zealand and recently launching in the U.S. and Canada, Spotify announced on Monday that it’s rolling out the tool to Premium subscribers in the U.K., Ireland, Australia, and Sweden.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Prompted Playlist feature allows users to create custom playlists by simply describing what they want to listen to in their own words. Instead of searching for individual songs or artists, users can describe the vibe, scenario, or inspiration they want, and Spotify will take care of the rest.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To access the feature, users tap “Create” and then select “Prompted Playlist,” then enter any prompt in English. The feature is designed to interpret themes including moods, aesthetics, and even memories. Prompts can be as broad or specific as the user wants, referencing musical eras, genres, activities, lyrics, instruments, or even requesting a playlist inspired by a TV show, movie, or personal milestone.&amp;nbsp;Users can also specify whether they want the playlist to include mostly new music or just music from their library in the prompt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once a prompt is submitted, Spotify’s AI generates a customized playlist tailored to the request. The system draws on the user’s listening history and incorporates current music and cultural trends. Plus, each song comes with a short explanation that offers insight into why it was chosen for that particular playlist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can refine their playlists by adjusting their prompts or starting over. For those whose musical tastes constantly evolve, playlists can be scheduled to automatically refresh on a daily or weekly basis.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095587" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/Meet-My-New-Favourite-Artist.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Because this is still in beta, Spotify noted that there might be changes as the company gets feedback, and that there are currently usage limits in place. Some users have reported hitting limits after roughly 20 or 30 prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify has recently expanded AI features throughout its platform, including&amp;nbsp;Page Match, which lets users scan a physical book page to jump to the corresponding spot in the audiobook,&amp;nbsp;and&amp;nbsp;About the Song. The platform also updated its song lyrics feature to provide global translations and offline access.&amp;nbsp;Last week, SeatGeek partnered with Spotify to help listeners easily find ticket links for concerts on an artist’s page or upcoming tour dates within the app.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Internally, the company has implemented AI throughout its workflows, with co-CEO Gustav Söderström saying earlier this month that Spotify’s best developers haven’t written a line of code since December, thanks to AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify is also expanding its audiobook business by venturing into physical book sales. Soon, users in the U.S. and U.K. will be able to buy physical copies directly from the app.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;After initially testing its AI-powered “Prompted Playlist” feature in New Zealand and recently launching in the U.S. and Canada, Spotify announced on Monday that it’s rolling out the tool to Premium subscribers in the U.K., Ireland, Australia, and Sweden.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Prompted Playlist feature allows users to create custom playlists by simply describing what they want to listen to in their own words. Instead of searching for individual songs or artists, users can describe the vibe, scenario, or inspiration they want, and Spotify will take care of the rest.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;To access the feature, users tap “Create” and then select “Prompted Playlist,” then enter any prompt in English. The feature is designed to interpret themes including moods, aesthetics, and even memories. Prompts can be as broad or specific as the user wants, referencing musical eras, genres, activities, lyrics, instruments, or even requesting a playlist inspired by a TV show, movie, or personal milestone.&amp;nbsp;Users can also specify whether they want the playlist to include mostly new music or just music from their library in the prompt. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Once a prompt is submitted, Spotify’s AI generates a customized playlist tailored to the request. The system draws on the user’s listening history and incorporates current music and cultural trends. Plus, each song comes with a short explanation that offers insight into why it was chosen for that particular playlist.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Users can refine their playlists by adjusting their prompts or starting over. For those whose musical tastes constantly evolve, playlists can be scheduled to automatically refresh on a daily or weekly basis.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095587" height="383" src="https://techcrunch.com/wp-content/uploads/2026/02/Meet-My-New-Favourite-Artist.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Spotify&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Because this is still in beta, Spotify noted that there might be changes as the company gets feedback, and that there are currently usage limits in place. Some users have reported hitting limits after roughly 20 or 30 prompts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify has recently expanded AI features throughout its platform, including&amp;nbsp;Page Match, which lets users scan a physical book page to jump to the corresponding spot in the audiobook,&amp;nbsp;and&amp;nbsp;About the Song. The platform also updated its song lyrics feature to provide global translations and offline access.&amp;nbsp;Last week, SeatGeek partnered with Spotify to help listeners easily find ticket links for concerts on an artist’s page or upcoming tour dates within the app.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Internally, the company has implemented AI throughout its workflows, with co-CEO Gustav Söderström saying earlier this month that Spotify’s best developers haven’t written a line of code since December, thanks to AI. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Spotify is also expanding its audiobook business by venturing into physical book sales. Soon, users in the U.S. and U.K. will be able to buy physical copies directly from the app.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/spotify-ai-prompted-playlists-uk-markets/</guid><pubDate>Mon, 23 Feb 2026 16:50:36 +0000</pubDate></item><item><title>Particle’s AI news app listens to podcasts for interesting clips so you you don’t have to (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/particles-ai-news-app-listens-to-podcasts-for-interesting-clips-so-you-you-dont-have-to/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An AI news app called Particle, from former Twitter engineers, can now keep up with news breaking on podcasts as well as news published on the web.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Just ahead of its recent Android release, Particle has introduced a feature called Podcast Clips, which finds the most interesting and relevant moments across many different types of podcasts, and then includes those clips alongside the related news stories in its feed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;So instead of listening to a lengthy podcast just to catch the 45 seconds of interesting comments, you can play back the clip as you’re reading the news on Particle. You also have the option of reading the transcript of the clip instead, as the words are highlighted as they’re spoken.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095600" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/IMG_1093.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve done that basically for any news story — if there is a podcast that is talking about it, or relevant at all, we’ve got all those clips,” Particle CEO Sara Beykpour,&amp;nbsp;previously the Senior Director of Product Management at Twitter, told TechCrunch. “It’s a really cool way, when you’re reading a story or learning about a story, to get a breath of what are people saying about this? What’s the commentary?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The addition acknowledges a shift in the news ecosystem that’s been underway for years. Not only are more people getting their news from podcasts and trusting them as reliable sources, but the medium is also becoming a destination for breaking news and major announcements from public figures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tech CEOs, in particular, are now seeking out friendly podcast hosts to air their talking points instead of trying to work with traditional media, as Bloomberg reported in 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That makes paying attention to podcasts even more critical if you want to keep up with news.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beykpour says Particle uses embedding models to understand when podcasts relate to a given news story. These models are provided by the same companies that provide LLM models, but they’re not generative AI technologies, she explains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We use vector embeddings to understand that these different parts of the podcasts are related to these different stories,” Beykpour notes. “A single podcast might cover 10 or 20 stories, so we use AI to understand that. We also use AI to do some of the logic around clipping, and understanding when to start a clip and end a clip.” &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095601" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/IMG_1092.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company leverages technology from ElevenLabs for transcription. However, some of the technology that identifies where exactly to clip the audio is part of Particle’s secret sauce.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The idea to tap into podcasts to better understand the commentary around news is also something newsrooms are taking a closer look at these days. As Nieman Lab reported this month, The New York Times has been using a custom AI tool that employs LLMs to transcribe and summarize new episodes of dozens of right-wing and more conservative podcasts to better understand what influencers on that side are saying about the news.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Particle’s Podcast Clips feature isn’t tied to only news stories. Because the app already understands different entities — like people, places, or things — you can go to the page for a notable figure, such as OpenAI CEO Sam Altman, to see all of his appearances on podcasts arranged as a feed.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095602" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/IMG_1091.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Particle has been busy building other features as well. The company has made its first attempt at monetization with Particle+, an optional $2.99/month subscription (or $29.99/year) that lets you access premium features. These include the ability to use natural language to have the news summarized in a style you prefer; pick from different voices when using the personalized audio feed; “Listen to the News”; unlimited crossword puzzles; support for private questions with its AI chatbot; and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095596" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Screenshot_20260220-110854.png?w=306" width="306" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Android release also brings a couple of other notable changes. The browse tab now includes timely stories, like the 2026 Winter Olympics, in addition to typical sections like politics, tech, or entertainment. Plus, when you tap on an entity, you’ll see a new page with the definition, stories, articles, related entities, and related topics.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095595" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/share_6305813549552173171.png?w=306" width="306" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Particle isn’t sharing data about user activity or conversion rates, but Beykpour did point to the app’s international audience, pre-Android. On a weekly basis, 55% of Particle’s users are outside the U.S., with India (15%) its biggest market after the U.S.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;An AI news app called Particle, from former Twitter engineers, can now keep up with news breaking on podcasts as well as news published on the web.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Just ahead of its recent Android release, Particle has introduced a feature called Podcast Clips, which finds the most interesting and relevant moments across many different types of podcasts, and then includes those clips alongside the related news stories in its feed.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;So instead of listening to a lengthy podcast just to catch the 45 seconds of interesting comments, you can play back the clip as you’re reading the news on Particle. You also have the option of reading the transcript of the clip instead, as the words are highlighted as they’re spoken.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095600" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/IMG_1093.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“We’ve done that basically for any news story — if there is a podcast that is talking about it, or relevant at all, we’ve got all those clips,” Particle CEO Sara Beykpour,&amp;nbsp;previously the Senior Director of Product Management at Twitter, told TechCrunch. “It’s a really cool way, when you’re reading a story or learning about a story, to get a breath of what are people saying about this? What’s the commentary?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The addition acknowledges a shift in the news ecosystem that’s been underway for years. Not only are more people getting their news from podcasts and trusting them as reliable sources, but the medium is also becoming a destination for breaking news and major announcements from public figures.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Tech CEOs, in particular, are now seeking out friendly podcast hosts to air their talking points instead of trying to work with traditional media, as Bloomberg reported in 2024.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That makes paying attention to podcasts even more critical if you want to keep up with news.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Beykpour says Particle uses embedding models to understand when podcasts relate to a given news story. These models are provided by the same companies that provide LLM models, but they’re not generative AI technologies, she explains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We use vector embeddings to understand that these different parts of the podcasts are related to these different stories,” Beykpour notes. “A single podcast might cover 10 or 20 stories, so we use AI to understand that. We also use AI to do some of the logic around clipping, and understanding when to start a clip and end a clip.” &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095601" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/IMG_1092.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The company leverages technology from ElevenLabs for transcription. However, some of the technology that identifies where exactly to clip the audio is part of Particle’s secret sauce.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The idea to tap into podcasts to better understand the commentary around news is also something newsrooms are taking a closer look at these days. As Nieman Lab reported this month, The New York Times has been using a custom AI tool that employs LLMs to transcribe and summarize new episodes of dozens of right-wing and more conservative podcasts to better understand what influencers on that side are saying about the news.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Particle’s Podcast Clips feature isn’t tied to only news stories. Because the app already understands different entities — like people, places, or things — you can go to the page for a notable figure, such as OpenAI CEO Sam Altman, to see all of his appearances on podcasts arranged as a feed.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095602" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/IMG_1091.png?w=313" width="313" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Particle has been busy building other features as well. The company has made its first attempt at monetization with Particle+, an optional $2.99/month subscription (or $29.99/year) that lets you access premium features. These include the ability to use natural language to have the news summarized in a style you prefer; pick from different voices when using the personalized audio feed; “Listen to the News”; unlimited crossword puzzles; support for private questions with its AI chatbot; and more.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095596" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/Screenshot_20260220-110854.png?w=306" width="306" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The Android release also brings a couple of other notable changes. The browse tab now includes timely stories, like the 2026 Winter Olympics, in addition to typical sections like politics, tech, or entertainment. Plus, when you tap on an entity, you’ll see a new page with the definition, stories, articles, related entities, and related topics.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3095595" height="680" src="https://techcrunch.com/wp-content/uploads/2026/02/share_6305813549552173171.png?w=306" width="306" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Particle&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Particle isn’t sharing data about user activity or conversion rates, but Beykpour did point to the app’s international audience, pre-Android. On a weekly basis, 55% of Particle’s users are outside the U.S., with India (15%) its biggest market after the U.S.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/particles-ai-news-app-listens-to-podcasts-for-interesting-clips-so-you-you-dont-have-to/</guid><pubDate>Mon, 23 Feb 2026 16:55:40 +0000</pubDate></item><item><title>The human work behind humanoid robots is being hidden (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/23/1133508/the-human-work-behind-humanoid-robots-is-being-hidden/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/260219_humanoid_algo_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In January, Nvidia’s Jensen Huang, the head of the world’s most valuable company, proclaimed that we are entering the era of physical AI, when artificial intelligence will move beyond language and chatbots into physically capable machines. (He also said the same thing the year before, by the way.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;The implication—fueled by new demonstrations of humanoid robots putting away dishes or assembling cars—is that mimicking human limbs with single-purpose robot arms is the old way of automation. The new way is to replicate the way humans think, learn, and adapt while they work. The problem is that the lack of transparency about the human labor involved in training and operating such robots leaves the public both misunderstanding what robots can actually do and failing to see the strange new forms of work forming around them.&lt;/p&gt;  &lt;p&gt;Consider how, in the AI era, robots often learn from humans who demonstrate how to do a chore. Creating this data at scale is now leading to &lt;em&gt;Black Mirror&lt;/em&gt;–esque scenarios. A worker in Shanghai, for example, recently spent a week wearing a virtual-reality headset and an exoskeleton while opening and closing the door of a microwave hundreds of times a day to train the robot next to him, &lt;em&gt;Rest of World&lt;/em&gt; reported. In North America, the robotics company Figure appears to be planning something similar: It announced in September it would partner with the investment firm Brookfield, which manages 100,000 residential units, to capture “massive amounts” of real-world data “across a variety of household environments.” (Figure did not respond to questions about this effort.)&lt;/p&gt; 
 &lt;p&gt;Just as our words became training data for large language models, our movements are now poised to follow the same path. Except this future might leave humans with an even worse deal, and it’s already beginning. The roboticist Aaron Prather told me about recent work with a delivery company that had its workers wear movement-tracking sensors as they moved boxes; the data collected will be used to train robots. The effort to build humanoids will likely require manual laborers to act as data collectors at massive scale. “It’s going to be weird,” Prather says. “No doubts about it.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Or consider tele-operation. Though the endgame in robotics is a machine that can complete a task on its own, robotics companies employ people to operate their robots remotely. Neo, a $20,000 humanoid robot from the startup 1X, is set to ship to homes this year, but the company’s founder, Bernt Øivind Børnich, told me recently that he’s not committed to any prescribed level of autonomy. If a robot gets stuck, or if the customer wants it to do a tricky task, a tele-operator from the company’s headquarters in Palo Alto, California, will pilot it, looking through its cameras to iron clothes or unload the dishwasher.&lt;/p&gt; 
 &lt;p&gt;This isn’t inherently harmful—1X gets customer consent before switching into tele-operation mode—but privacy as we know it will not exist in a world where tele-operators are doing chores in your house through a robot. And if home humanoids are not genuinely autonomous, the arrangement is better understood as a form of wage arbitrage that re-creates the dynamics of gig work while, for the first time, allowing physical tasks to be performed wherever labor is cheapest.&lt;/p&gt;  &lt;p&gt;We’ve been down similar roads before. Carrying out “AI-driven” content moderation on social media platforms or assembling training data for AI companies often requires workers in low-wage countries to view disturbing content. And despite claims that AI will soon enough train on its outputs and learn on its own, even the best models require an awful lot of human feedback to work as desired.&lt;/p&gt;  &lt;p&gt;These human workforces do not mean that AI is just vaporware. But when they remain invisible, the public consistently overestimates the machines’ actual capabilities.&lt;/p&gt;  &lt;p&gt;That’s great for investors and hype, but it has consequences for everyone. When Tesla marketed its driver-assistance software as “Autopilot,” for example, it inflated public expectations about what the system could safely do—a distortion a Miami jury recently found contributed to a crash that killed a 22-year-old woman (Tesla was ordered to pay $240 million in damages).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The same will be true for humanoid robots. If Huang is right, and physical AI is coming for our workplaces, homes, and public spaces, then the way we describe and scrutinize such technology matters. Yet robotics companies remain as opaque about training and tele-operation as AI firms are about their training data. If that does not change, we risk mistaking concealed human labor for machine intelligence—and seeing far more autonomy than truly exists.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/260219_humanoid_algo_hero.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;&lt;em&gt;This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,&amp;nbsp;&lt;/em&gt;&lt;em&gt;sign up here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;In January, Nvidia’s Jensen Huang, the head of the world’s most valuable company, proclaimed that we are entering the era of physical AI, when artificial intelligence will move beyond language and chatbots into physically capable machines. (He also said the same thing the year before, by the way.)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;p&gt;The implication—fueled by new demonstrations of humanoid robots putting away dishes or assembling cars—is that mimicking human limbs with single-purpose robot arms is the old way of automation. The new way is to replicate the way humans think, learn, and adapt while they work. The problem is that the lack of transparency about the human labor involved in training and operating such robots leaves the public both misunderstanding what robots can actually do and failing to see the strange new forms of work forming around them.&lt;/p&gt;  &lt;p&gt;Consider how, in the AI era, robots often learn from humans who demonstrate how to do a chore. Creating this data at scale is now leading to &lt;em&gt;Black Mirror&lt;/em&gt;–esque scenarios. A worker in Shanghai, for example, recently spent a week wearing a virtual-reality headset and an exoskeleton while opening and closing the door of a microwave hundreds of times a day to train the robot next to him, &lt;em&gt;Rest of World&lt;/em&gt; reported. In North America, the robotics company Figure appears to be planning something similar: It announced in September it would partner with the investment firm Brookfield, which manages 100,000 residential units, to capture “massive amounts” of real-world data “across a variety of household environments.” (Figure did not respond to questions about this effort.)&lt;/p&gt; 
 &lt;p&gt;Just as our words became training data for large language models, our movements are now poised to follow the same path. Except this future might leave humans with an even worse deal, and it’s already beginning. The roboticist Aaron Prather told me about recent work with a delivery company that had its workers wear movement-tracking sensors as they moved boxes; the data collected will be used to train robots. The effort to build humanoids will likely require manual laborers to act as data collectors at massive scale. “It’s going to be weird,” Prather says. “No doubts about it.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Or consider tele-operation. Though the endgame in robotics is a machine that can complete a task on its own, robotics companies employ people to operate their robots remotely. Neo, a $20,000 humanoid robot from the startup 1X, is set to ship to homes this year, but the company’s founder, Bernt Øivind Børnich, told me recently that he’s not committed to any prescribed level of autonomy. If a robot gets stuck, or if the customer wants it to do a tricky task, a tele-operator from the company’s headquarters in Palo Alto, California, will pilot it, looking through its cameras to iron clothes or unload the dishwasher.&lt;/p&gt; 
 &lt;p&gt;This isn’t inherently harmful—1X gets customer consent before switching into tele-operation mode—but privacy as we know it will not exist in a world where tele-operators are doing chores in your house through a robot. And if home humanoids are not genuinely autonomous, the arrangement is better understood as a form of wage arbitrage that re-creates the dynamics of gig work while, for the first time, allowing physical tasks to be performed wherever labor is cheapest.&lt;/p&gt;  &lt;p&gt;We’ve been down similar roads before. Carrying out “AI-driven” content moderation on social media platforms or assembling training data for AI companies often requires workers in low-wage countries to view disturbing content. And despite claims that AI will soon enough train on its outputs and learn on its own, even the best models require an awful lot of human feedback to work as desired.&lt;/p&gt;  &lt;p&gt;These human workforces do not mean that AI is just vaporware. But when they remain invisible, the public consistently overestimates the machines’ actual capabilities.&lt;/p&gt;  &lt;p&gt;That’s great for investors and hype, but it has consequences for everyone. When Tesla marketed its driver-assistance software as “Autopilot,” for example, it inflated public expectations about what the system could safely do—a distortion a Miami jury recently found contributed to a crash that killed a 22-year-old woman (Tesla was ordered to pay $240 million in damages).&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The same will be true for humanoid robots. If Huang is right, and physical AI is coming for our workplaces, homes, and public spaces, then the way we describe and scrutinize such technology matters. Yet robotics companies remain as opaque about training and tele-operation as AI firms are about their training data. If that does not change, we risk mistaking concealed human labor for machine intelligence—and seeing far more autonomy than truly exists.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/23/1133508/the-human-work-behind-humanoid-robots-is-being-hidden/</guid><pubDate>Mon, 23 Feb 2026 17:05:00 +0000</pubDate></item><item><title>New Microsoft gaming chief has "no tolerance for bad AI" (AI - Ars Technica)</title><link>https://arstechnica.com/gaming/2026/02/new-microsoft-gaming-chief-has-no-tolerance-for-bad-ai/</link><description>&lt;article class="double-column h-entry post-2142194 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-gaming tag-asha-sharma tag-microsoft-3 tag-xbox-2"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        But Asha Sharma faces scrutiny for lack of gaming experience.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Incoming Microsoft Gaming EVP and CEO Asha Sharma.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last week’s surprise departure of Phil Spencer from Microsoft led to the promotion of Asha Sharma, who comes to head Microsoft’s gaming division after two years as president of the company’s CoreAI Product group. Despite that recent history, Sharma says in a new interview that she has “no tolerance for bad AI” in game development.&lt;/p&gt;
&lt;p&gt;Speaking with Variety, Sharma noted that “AI has long been part of gaming and will continue to be,” before adding that “great stories are created by humans.” The interview comes after Sharma promised in an introductory memo: “We will not chase short-term efficiency or flood our ecosystem with soulless AI slop. Games are and always will be art, crafted by humans, and created with the most innovative technology provided by us.”&lt;/p&gt;
&lt;p&gt;Those statements seem like a clear line in the sand from Sharma against the use of AI tools in Microsoft’s first-party game development, at the very least. But what separates “bad AI” and “soulless AI slop” from “innovative technology” that humans can use to create artful games is a matter of some significant debate in the gaming world.&lt;/p&gt;
&lt;p&gt;Many have taken a zero-tolerance approach to the use of generative AI tools in video games. When &lt;em&gt;Clair Obscur: Expedition 33&lt;/em&gt; studio Sandfall Interactive&amp;nbsp;admitted to using generative AI for some background assets, for instance, the Indie Game Awards rescinded the company’s honors (the assets were later patched out of the game). And publisher Running with Scissors canceled a planned new game in the &lt;em&gt;Postal&lt;/em&gt; series after being “overwhelmed with negative responses” to a trailer containing elements they said were “very likely AI-generated and thus has caused extreme damage to our brand and our company reputation.”&lt;/p&gt;
&lt;p&gt;At the same time, game development luminaries like John Carmack have defended AI development tools as “allow[ing] the best to reach even greater heights, while enabling smaller teams to accomplish more, and bring in some completely new creator demographics.” And Epic Games founder and CEO Tim Sweeney says requiring developers to disclose their use of AI tools is as relevant as disclosing “what shampoo brand the developer uses,” since “AI will be involved in nearly all future production [of games].”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A gaming education&lt;/h2&gt;
&lt;p&gt;Unlike Spencer, who spent years at Microsoft Game Studios before heading Microsoft’s gaming division, Sharma has no professional experience in the video game industry. And her personal experience with Xbox also seems somewhat limited; after sharing her Gamertag on social media over the weekend, curious gamers found that her Xbox play history dates back roughly one month. That’s also in stark contrast to Spencer, who has amassed a score of over 121,000 across decades of play.&lt;/p&gt;
&lt;p&gt;In her interview with Variety, Sharma cited 2016’s &lt;em&gt;Firewatch&lt;/em&gt; as an example of the kinds of games with “deep emotional resonance” and “a distinct point of view” that she’s looking for from Microsoft. And on social media, Sharma shared her list of the three greatest games ever: “&lt;em&gt;Halo&lt;/em&gt;, &lt;em&gt;Valheim&lt;/em&gt;,&lt;em&gt; Goldeneye&lt;/em&gt;,” for what it’s worth. Sharma also seems to be taking recommendations for games to catch up on; after saying on social media that she would try &lt;em&gt;Borderlands 2&lt;/em&gt;, the game appeared in her recently played games over the weekend.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2142197 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="970" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/ashsagames.jpg" width="1189" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A look at some of Sharma’s recently played Xbox games, as of this writing.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Xbox.com

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Being a personal fan of video games isn’t necessarily required to succeed in running a gaming company. Nintendo President Hiroshi Yamauchi famously didn’t care for video games even as he launched the Famicom and Nintendo Entertainment System to worldwide success in the 1980s. Still, the lack of direct experience with the gaming world marks a sharp change after Spencer’s long tenure at a time when Microsoft is struggling to redefine the Xbox brand amid cratering hardware sales, a pivot away from software exclusives, and a move to extend the Xbox brand to many different devices.&lt;/p&gt;
&lt;p&gt;Xbox President and COO Sarah Bond, who by all accounts was being set up to succeed Spencer, also announced her departure from Microsoft on Friday, ending a nearly nine-year stint as a public face for the company’s gaming efforts. The Verge reports that Bond caused a lot of friction within the Xbox team when she championed the “Xbox Everywhere” strategy and “This is an Xbox” marketing campaign, which focused on streaming Xbox games to hardware like mobile phones and tablets, according to anonymous sources. Shortly before the launch of that campaign in 2024, Microsoft lost marketing executives Jerrett West and Kareem Choudry, leading to significant internal reorganization.&lt;/p&gt;
&lt;p&gt;Longtime Xbox Game Studios executive Matt Booty, whose history in the game industry dates back to working for Williams Electronics in the ’90s, has been promoted to executive vice president and chief content officer for Xbox and “will continue working closely with [Sharma] to ensure a smooth transition,” Microsoft said in its announcement Friday.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #ffff8d; background-color: #fbc02d;"&gt;&lt;span class="ars-avatar-letter"&gt;S&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Sonix
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            I am as sceptical as the next guy when it comes to these news but the "she's not a gamer" angle ain't it. Its nice that Phil was a gamer, but that didn't stop him and the organization around him from making some very questionable decisions around Xbox, and I am pretty sure that her being a certified gamer (tm) or not will not be the reason why Xbox and Microsoft will succeed or fail. &lt;p&gt;Hoping for the best, even though currently there's many more questions than convincing answers.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-02-23T17:53:23+00:00"&gt;February 23, 2026 at 5:53 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2142194 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-gaming tag-asha-sharma tag-microsoft-3 tag-xbox-2"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        But Asha Sharma faces scrutiny for lack of gaming experience.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/sharma-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      Incoming Microsoft Gaming EVP and CEO Asha Sharma.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Microsoft

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;Last week’s surprise departure of Phil Spencer from Microsoft led to the promotion of Asha Sharma, who comes to head Microsoft’s gaming division after two years as president of the company’s CoreAI Product group. Despite that recent history, Sharma says in a new interview that she has “no tolerance for bad AI” in game development.&lt;/p&gt;
&lt;p&gt;Speaking with Variety, Sharma noted that “AI has long been part of gaming and will continue to be,” before adding that “great stories are created by humans.” The interview comes after Sharma promised in an introductory memo: “We will not chase short-term efficiency or flood our ecosystem with soulless AI slop. Games are and always will be art, crafted by humans, and created with the most innovative technology provided by us.”&lt;/p&gt;
&lt;p&gt;Those statements seem like a clear line in the sand from Sharma against the use of AI tools in Microsoft’s first-party game development, at the very least. But what separates “bad AI” and “soulless AI slop” from “innovative technology” that humans can use to create artful games is a matter of some significant debate in the gaming world.&lt;/p&gt;
&lt;p&gt;Many have taken a zero-tolerance approach to the use of generative AI tools in video games. When &lt;em&gt;Clair Obscur: Expedition 33&lt;/em&gt; studio Sandfall Interactive&amp;nbsp;admitted to using generative AI for some background assets, for instance, the Indie Game Awards rescinded the company’s honors (the assets were later patched out of the game). And publisher Running with Scissors canceled a planned new game in the &lt;em&gt;Postal&lt;/em&gt; series after being “overwhelmed with negative responses” to a trailer containing elements they said were “very likely AI-generated and thus has caused extreme damage to our brand and our company reputation.”&lt;/p&gt;
&lt;p&gt;At the same time, game development luminaries like John Carmack have defended AI development tools as “allow[ing] the best to reach even greater heights, while enabling smaller teams to accomplish more, and bring in some completely new creator demographics.” And Epic Games founder and CEO Tim Sweeney says requiring developers to disclose their use of AI tools is as relevant as disclosing “what shampoo brand the developer uses,” since “AI will be involved in nearly all future production [of games].”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;A gaming education&lt;/h2&gt;
&lt;p&gt;Unlike Spencer, who spent years at Microsoft Game Studios before heading Microsoft’s gaming division, Sharma has no professional experience in the video game industry. And her personal experience with Xbox also seems somewhat limited; after sharing her Gamertag on social media over the weekend, curious gamers found that her Xbox play history dates back roughly one month. That’s also in stark contrast to Spencer, who has amassed a score of over 121,000 across decades of play.&lt;/p&gt;
&lt;p&gt;In her interview with Variety, Sharma cited 2016’s &lt;em&gt;Firewatch&lt;/em&gt; as an example of the kinds of games with “deep emotional resonance” and “a distinct point of view” that she’s looking for from Microsoft. And on social media, Sharma shared her list of the three greatest games ever: “&lt;em&gt;Halo&lt;/em&gt;, &lt;em&gt;Valheim&lt;/em&gt;,&lt;em&gt; Goldeneye&lt;/em&gt;,” for what it’s worth. Sharma also seems to be taking recommendations for games to catch up on; after saying on social media that she would try &lt;em&gt;Borderlands 2&lt;/em&gt;, the game appeared in her recently played games over the weekend.&lt;/p&gt;
&lt;figure class="ars-wp-img-shortcode id-2142197 align-fullwidth"&gt;
    &lt;div&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="fullwidth full" height="970" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/ashsagames.jpg" width="1189" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;
          &lt;figcaption&gt;
        &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      A look at some of Sharma’s recently played Xbox games, as of this writing.

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Xbox.com

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
      &lt;/figcaption&gt;
      &lt;/figure&gt;

&lt;p&gt;Being a personal fan of video games isn’t necessarily required to succeed in running a gaming company. Nintendo President Hiroshi Yamauchi famously didn’t care for video games even as he launched the Famicom and Nintendo Entertainment System to worldwide success in the 1980s. Still, the lack of direct experience with the gaming world marks a sharp change after Spencer’s long tenure at a time when Microsoft is struggling to redefine the Xbox brand amid cratering hardware sales, a pivot away from software exclusives, and a move to extend the Xbox brand to many different devices.&lt;/p&gt;
&lt;p&gt;Xbox President and COO Sarah Bond, who by all accounts was being set up to succeed Spencer, also announced her departure from Microsoft on Friday, ending a nearly nine-year stint as a public face for the company’s gaming efforts. The Verge reports that Bond caused a lot of friction within the Xbox team when she championed the “Xbox Everywhere” strategy and “This is an Xbox” marketing campaign, which focused on streaming Xbox games to hardware like mobile phones and tablets, according to anonymous sources. Shortly before the launch of that campaign in 2024, Microsoft lost marketing executives Jerrett West and Kareem Choudry, leading to significant internal reorganization.&lt;/p&gt;
&lt;p&gt;Longtime Xbox Game Studios executive Matt Booty, whose history in the game industry dates back to working for Williams Electronics in the ’90s, has been promoted to executive vice president and chief content officer for Xbox and “will continue working closely with [Sharma] to ensure a smooth transition,” Microsoft said in its announcement Friday.&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #ffff8d; background-color: #fbc02d;"&gt;&lt;span class="ars-avatar-letter"&gt;S&lt;/span&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              Sonix
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            I am as sceptical as the next guy when it comes to these news but the "she's not a gamer" angle ain't it. Its nice that Phil was a gamer, but that didn't stop him and the organization around him from making some very questionable decisions around Xbox, and I am pretty sure that her being a certified gamer (tm) or not will not be the reason why Xbox and Microsoft will succeed or fail. &lt;p&gt;Hoping for the best, even though currently there's many more questions than convincing answers.
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-02-23T17:53:23+00:00"&gt;February 23, 2026 at 5:53 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gaming/2026/02/new-microsoft-gaming-chief-has-no-tolerance-for-bad-ai/</guid><pubDate>Mon, 23 Feb 2026 17:45:29 +0000</pubDate></item><item><title>Guide Labs debuts a new kind of interpretable LLM (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/guide-labs-debuts-a-new-kind-of-interpretable-llm/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The challenge of wrangling a deep learning model is often understanding why it does what it does: Whether it’s xAI’s repeated struggle sessions to fine-tune Grok’s odd politics, ChatGPT’s struggles with sycophancy, or run-of-the-mill hallucinations, plumbing through a neural network with billions of parameters isn’t easy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Guide Labs, a San Francisco startup founded by CEO Julius Adebayo and chief science officer Aya Abdelsalam Ismail, is offering an answer to that problem today. On Monday, the company open sourced an 8-billion-parameter LLM, Steerling-8B, trained with a new architecture designed to make its actions easily interpretable: Every token produced by the model can be traced back to its origins in the LLM’s training data.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That can be as simple as determining the reference materials for facts cited by the model, or as complex as understanding the model’s understanding of humor or gender.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If I have a trillion ways to encode gender, and I encode it in 1 billion of the 1 trillion things that I have, you have to make sure you find all those 1 billion things that I’ve encoded, and then you have to be able to reliably turn that on, turn them off,” Adebayo told TechCrunch. “You can do it with current models, but it’s very fragile&amp;nbsp;… It’s sort of one of the holy grail questions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adebayo began this work while earning his PhD at MIT, co-authoring a widely cited 2018 paper that showed existing methods of understanding deep learning models were not reliable. That work ultimately led to the creation of a new way of building LLMs: Developers insert a concept layer in the model that buckets data into traceable categories. This requires more up-front data annotation, but by using other AI models to help, they were able to train this model as their largest proof of concept yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The kind of interpretability people do is&amp;nbsp;… neuroscience on a model, and we flip that,” Adebayo said. “What we do is actually engineer the model from the ground up so that you don’t need to do neuroscience.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3095647" height="399" src="https://techcrunch.com/wp-content/uploads/2026/02/Guide-Labs-Architecture.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Guide Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One concern with this approach is that it might eliminate some of the emergent behaviors that make LLMs so intriguing: Their ability to generalize in new ways about things they haven’t been trained on yet. Adebayo says that still happens in his company’s model: His team tracks what they call “discovered concepts” that the model discovered on its own, like quantum computing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adebayo argues this interpretable architecture will be something everyone needs. For consumer-facing LLMs, these techniques should allow model builders to do things like block the use of copyrighted materials, or better control outputs around subjects like violence or drug abuse. Regulated industries will require more controllable LLMs — for example, in finance — where a model evaluating loan applicants needs to consider things like financial records but not race. There’s also a need for interpretability in scientific work, another area where Guide Labs has developed technology. Protein folding has been a big success for deep learning models, but scientists need more insight into why their software figured out promising combinations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This model demonstrates that training interpretable models is no longer a sort of science; it’s now an engineering problem,” Adebayo said. “We figured out the science and we can scale them, and there is no reason why this kind of model wouldn’t match the performance of the frontier level models,” which have many more parameters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Guide Labs says that Steerling-8B can achieve 90% of the capability of existing models, but uses less training data, thanks to its novel architecture. The next step for the company, which emerged from Y Combinator and raised a $9 million seed round from Initialized Capital in November 2024, is to build a larger model and begin offering API and agentic access to users.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The way we’re currently training models is super primitive, and so democratizing inherent interpretability is actually going to be a long-term good thing for our role within the human race,” Adebayo told TechCrunch. “As we’re going after these models that are going to be super intelligent, you don’t want something to be making decisions on your behalf that’s sort of mysterious to you.”&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The challenge of wrangling a deep learning model is often understanding why it does what it does: Whether it’s xAI’s repeated struggle sessions to fine-tune Grok’s odd politics, ChatGPT’s struggles with sycophancy, or run-of-the-mill hallucinations, plumbing through a neural network with billions of parameters isn’t easy.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Guide Labs, a San Francisco startup founded by CEO Julius Adebayo and chief science officer Aya Abdelsalam Ismail, is offering an answer to that problem today. On Monday, the company open sourced an 8-billion-parameter LLM, Steerling-8B, trained with a new architecture designed to make its actions easily interpretable: Every token produced by the model can be traced back to its origins in the LLM’s training data.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;That can be as simple as determining the reference materials for facts cited by the model, or as complex as understanding the model’s understanding of humor or gender.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If I have a trillion ways to encode gender, and I encode it in 1 billion of the 1 trillion things that I have, you have to make sure you find all those 1 billion things that I’ve encoded, and then you have to be able to reliably turn that on, turn them off,” Adebayo told TechCrunch. “You can do it with current models, but it’s very fragile&amp;nbsp;… It’s sort of one of the holy grail questions.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adebayo began this work while earning his PhD at MIT, co-authoring a widely cited 2018 paper that showed existing methods of understanding deep learning models were not reliable. That work ultimately led to the creation of a new way of building LLMs: Developers insert a concept layer in the model that buckets data into traceable categories. This requires more up-front data annotation, but by using other AI models to help, they were able to train this model as their largest proof of concept yet.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“The kind of interpretability people do is&amp;nbsp;… neuroscience on a model, and we flip that,” Adebayo said. “What we do is actually engineer the model from the ground up so that you don’t need to do neuroscience.”&lt;/p&gt;

&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-3095647" height="399" src="https://techcrunch.com/wp-content/uploads/2026/02/Guide-Labs-Architecture.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Guide Labs&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;One concern with this approach is that it might eliminate some of the emergent behaviors that make LLMs so intriguing: Their ability to generalize in new ways about things they haven’t been trained on yet. Adebayo says that still happens in his company’s model: His team tracks what they call “discovered concepts” that the model discovered on its own, like quantum computing.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Adebayo argues this interpretable architecture will be something everyone needs. For consumer-facing LLMs, these techniques should allow model builders to do things like block the use of copyrighted materials, or better control outputs around subjects like violence or drug abuse. Regulated industries will require more controllable LLMs — for example, in finance — where a model evaluating loan applicants needs to consider things like financial records but not race. There’s also a need for interpretability in scientific work, another area where Guide Labs has developed technology. Protein folding has been a big success for deep learning models, but scientists need more insight into why their software figured out promising combinations.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This model demonstrates that training interpretable models is no longer a sort of science; it’s now an engineering problem,” Adebayo said. “We figured out the science and we can scale them, and there is no reason why this kind of model wouldn’t match the performance of the frontier level models,” which have many more parameters.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Guide Labs says that Steerling-8B can achieve 90% of the capability of existing models, but uses less training data, thanks to its novel architecture. The next step for the company, which emerged from Y Combinator and raised a $9 million seed round from Initialized Capital in November 2024, is to build a larger model and begin offering API and agentic access to users.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The way we’re currently training models is super primitive, and so democratizing inherent interpretability is actually going to be a long-term good thing for our role within the human race,” Adebayo told TechCrunch. “As we’re going after these models that are going to be super intelligent, you don’t want something to be making decisions on your behalf that’s sort of mysterious to you.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/guide-labs-debuts-a-new-kind-of-interpretable-llm/</guid><pubDate>Mon, 23 Feb 2026 17:53:28 +0000</pubDate></item><item><title>OpenAI calls in the consultants for its enterprise push (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/openai-calls-in-the-consultants-for-its-enterprise-push/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2206295463.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is beefing up partnerships with four major consulting giants as the AI company looks to grow its enterprise business in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI announced on Monday the “Frontier Alliances,” a signal that the AI lab is willing to try different approaches to get enterprises to meaningfully adopt its technology. The alliance includes multi-year partnerships between OpenAI and four major consulting firms, Boston Consulting Group (BCG), McKinsey, Accenture, and Capgemini, to sell its enterprise products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI’s Forward Deployed Engineering team will work with the consulting giants to help them implement OpenAI’s enterprise-focused technologies like OpenAI Frontier into customers’ tech stacks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company launched OpenAI Frontier in early February. The no-code open software allows users to build, deploy, and manage AI agents both built on OpenAI’s AI models and beyond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI argues in its latest announcement that consultants are the right avenue to get enterprises on board.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI alone does not drive transformation. It must be linked to strategy, built into redesigned processes, and adopted at scale with aligned incentives and culture to deliver sustained outcomes,” BCG CEO Christoph Schweizer said in OpenAI’s blog post. “Our expanded partnership combines OpenAI’s Frontier platform with BCG’s deep industry, functional, and tech expertise and BCG X’s build-and-scale capabilities to drive measurable impact with safeguards from day one.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thus far, enterprise adoption of AI has been relatively slow as these companies struggle to find a meaningful return on investment from their AI pursuits.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s alliance strategy makes sense and goes beyond just pitching enterprises on attaching AI to their existing workflows. Instead, this effort focuses on consultants persuading companies to change their strategies and workflows to fold in OpenAI’s tools where it makes sense. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting the OpenAI rival Anthropic has inked deals with consulting giants, including Deloitte and Accenture in recent months too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Company CFO Sarah Friar wrote in a blog post in January that enterprise is a big area of focus for OpenAI in 2026. OpenAI has also inked sizable enterprise AI deals with Snowflake and ServiceNow so far this year, in addition to naming Barret Zoph to lead the company’s enterprise sales effort in January.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/04/GettyImages-2206295463.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenAI is beefing up partnerships with four major consulting giants as the AI company looks to grow its enterprise business in 2026.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI announced on Monday the “Frontier Alliances,” a signal that the AI lab is willing to try different approaches to get enterprises to meaningfully adopt its technology. The alliance includes multi-year partnerships between OpenAI and four major consulting firms, Boston Consulting Group (BCG), McKinsey, Accenture, and Capgemini, to sell its enterprise products.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;OpenAI’s Forward Deployed Engineering team will work with the consulting giants to help them implement OpenAI’s enterprise-focused technologies like OpenAI Frontier into customers’ tech stacks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company launched OpenAI Frontier in early February. The no-code open software allows users to build, deploy, and manage AI agents both built on OpenAI’s AI models and beyond.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI argues in its latest announcement that consultants are the right avenue to get enterprises on board.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“AI alone does not drive transformation. It must be linked to strategy, built into redesigned processes, and adopted at scale with aligned incentives and culture to deliver sustained outcomes,” BCG CEO Christoph Schweizer said in OpenAI’s blog post. “Our expanded partnership combines OpenAI’s Frontier platform with BCG’s deep industry, functional, and tech expertise and BCG X’s build-and-scale capabilities to drive measurable impact with safeguards from day one.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Thus far, enterprise adoption of AI has been relatively slow as these companies struggle to find a meaningful return on investment from their AI pursuits.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;OpenAI’s alliance strategy makes sense and goes beyond just pitching enterprises on attaching AI to their existing workflows. Instead, this effort focuses on consultants persuading companies to change their strategies and workflows to fold in OpenAI’s tools where it makes sense. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s worth noting the OpenAI rival Anthropic has inked deals with consulting giants, including Deloitte and Accenture in recent months too.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Company CFO Sarah Friar wrote in a blog post in January that enterprise is a big area of focus for OpenAI in 2026. OpenAI has also inked sizable enterprise AI deals with Snowflake and ServiceNow so far this year, in addition to naming Barret Zoph to lead the company’s enterprise sales effort in January.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/openai-calls-in-the-consultants-for-its-enterprise-push/</guid><pubDate>Mon, 23 Feb 2026 18:11:08 +0000</pubDate></item><item><title>Peptides are everywhere. Here’s what you need to know. (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/02/23/1133522/peptides-are-everywhere-heres-what-you-need-to-know/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/260219_peptides_hero2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt; Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. &lt;/em&gt;&lt;em&gt;You can read more from the series here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Want to lose weight? Get shredded? Stay mentally sharp? A wellness influencer might tell you to take peptides, the latest cure-all in the alternative medicine arsenal. People inject them. They snort them. They combine them into concoctions with superhero names, like the Wolverine stack.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Matt Kaeberlein, a longevity researcher, first started hearing about peptides a few years ago. “At that point it was mostly functional medicine doctors that were using peptides,” he says, referring to physicians who embrace alternative medicine and supplements. “In the last six months, it’s kind of gone crazy.”&lt;/p&gt;  &lt;p&gt;Peptides have gone mainstream. At the health-technology startup Superpower in Los Angeles, employees can get free peptide shots on Fridays. At a health food store in Phoenix, a sidewalk sign reads, “We have peptides!” At a tae kwon do center in South Carolina, a peptide wholesaler hosts an informational evening. On social media, they’re everywhere. And that popularity seems poised to grow; Department of Health and Human Services secretary Robert F. Kennedy Jr. has promised to end the FDA’s “aggressive suppression” of peptides.&lt;/p&gt; 
 &lt;p&gt;The benefits and risks of many of these compounds, however, are largely unknown. Some of the most popular peptides have never been tested in human trials. They are sold for research purposes, not human consumption. Some are illegal knockoffs of wildly successful weight-loss medicines. The vast majority come from China, a fact that has some legislators worried. Last week, Senator Tom Cotton urged the head of the FDA to crack down on illegal shipments of peptides from China. In&lt;strong&gt; &lt;/strong&gt;the absence of regulatory oversight, some people are sending the compounds they purchase off for independent testing just to ensure that the product is legit.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What is a peptide?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;A peptide is simply a short string of amino acids, the building blocks of proteins. “Scientists generally think of peptides as very small protein fragments, but we don’t really have a precise cutoff between a peptide and a protein,” says Paul Knoepfler, a stem-cell researcher at the University of California, Davis. Insulin is a peptide, as is human growth hormone. So are some neurotransmitters, like oxytocin.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But when wellness influencers talk about peptides, they’re often referring to particular compounds—formulated as injections, pills, or nasal sprays—that have become trendy lately.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Some of these peptides are FDA-approved prescription medications. GLP-1 medicines, for example, are approved to treat diabetes and obesity but are also easily accessible online to almost anyone who wants to use them. Many sites sell microdoses of GLP-1s with claims that they can “support longevity,” reduce cognitive decline, or curb inflammation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Many more peptides are experimental. “The majority fall into the unapproved bucket,” says Kaeberlein, who is chief executive officer of Optispan, a Seattle-based health-care technology company focused on longevity. That bucket includes drugs that promote the release of growth hormones, like TB-500, CJC-1295, and ipamorelin, and compounds said to promote tissue repair and wound healing, like BPC-157 and GHK-Cu. It’s primarily these unapproved compounds that have raised concerns. “Anybody can set up an online shop selling research-grade peptides,” says Tenille Davis, a pharmacist and chief advocacy officer at the Alliance for Pharmacy Compounding, a trade organization representing more than 600 pharmacies. “And nobody knows what’s even in the vials.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s not just fitness gurus, biohackers, and longevity fanatics who are taking these experimental drugs. Kaeberlein recalls hearing about an acquaintance whose doctor prescribed her unapproved&lt;strong&gt; &lt;/strong&gt;peptides. She was “just a typical upper-middle-class woman,” he says. “That’s when it really hit me that this has sort of gone relatively mainstream.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What do peptides do?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;All kinds of things, purportedly. GHK-Cu is supposed to help with wound healing and collagen production. BPC-157 is said to promote tissue repair and curb inflammation, TB-500 to foster blood vessel formation. Here’s the caveat: The evidence for these benefits comes largely from animal studies and online testimonials, not human trials. “There’s no human clinical evidence to show that they even do what people are claiming that they do,” says Stuart Phillips, a muscle physiologist at McMaster University in Hamilton, Ontario. “So it could be just a giant rip-off.”&lt;/p&gt;  &lt;p&gt;Some experimental peptides probably do have beneficial wound healing properties or regenerative effects, Kaeberlein says. For BPC-157, for example, “the animal data is compelling,” he says. But there are still plenty of unknowns: What is the right dosage? How long should you take it? What’s the best way to administer it? Those are questions that can be answered only through rigorous clinical trials. In the absence of those studies, doctors “just make up their own protocols,” he says.&lt;strong&gt; &lt;/strong&gt;Some consumers go the DIY route, reconstituting powdered peptides and injecting their own concoctions at home.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;So why am I seeing ads for these peptide therapies if they’re not approved?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Federal law prohibits companies from marketing medications that haven’t been approved. That includes most peptides, which are regulated as small molecules, not dietary supplements. (Two notable exceptions are collagen peptides and creatine peptides, often sold as powders.) The law is designed to protect consumers from drugs that haven’t been proved safe and effective.&lt;/p&gt;  &lt;p&gt;But it doesn’t stop labs from making peptides for research purposes. “Most of the peptides being consumed in the marketplace now are being sold by these online companies that are selling them labeled for research use only,” Davis says. The vials often bear disclaimers that clearly say as much: “For research use only”&amp;nbsp;or&amp;nbsp;“Not for human consumption.” It’s illegal to market these products for human use, but “the websites make it pretty clear that the buyers are intended to be using these products themselves,” she says.&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The practice isn’t legal, but enforcement has been sporadic. “FDA sends warning letters, shuts down companies. But because it’s all online, they have a really hard time keeping up with these entities,” Davis says. And companies have plenty of incentive to keep illegally marketing the products. “They can make millions of dollars without having to spend money and time doing research,” Knoepfler says. “It’s a cash grab.”&lt;/p&gt;  &lt;p&gt;Compounding pharmacies, which are legally allowed to create bespoke medications by mixing bulk active ingredients, often get requests to dispense peptides, but most peptides don’t meet the eligibility criteria for compounding. This has always been the case, but in 2023 the FDA explicitly added several common experimental peptides to the list of bulk substances that cannot be compounded because of safety concerns. “It put an exclamation point on policy that was already in place,” Davis says.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Many GLP-1 medications are available from compounding pharmacies. That used to be accepted because the drugs were in short supply. Now, however, supplies of most of these medications are stable, and sellers are under increasing pressure from regulators to stop mass-marketing these drugs.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What’s the harm in trying them?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Peptides sold for research purposes come from labs with little regulatory oversight. “When you buy stuff online intended for research grade, you have no idea what’s in the vial that you’re getting. You have no idea the sterility practices that it was manufactured under, or what sort of impurities might be in the vial,” Davis says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Phillips has heard some people say they send their peptides for third-party testing to ensure that they’re pure, “like it’s some kind of flex,” he says. “And I’m like, ‘Well, you just proved that this stuff lives in the shadows, for crying out loud.’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Finnrick Analytics, a peptide-testing startup in Austin, Texas, has analyzed the purity and potency of more than 5,000 samples of 15 different peptides from 173&amp;nbsp;vendors. The results show that the quality varies substantially from vendor to vendor and even batch to batch. For example, the company tested nearly 450 samples of BPC-157 from 64&amp;nbsp;vendors. In some cases, the vials sold as BPC-157 didn’t contain the compound at all. In those that did, the purity varied from about 82% to 100%.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Perhaps more worrying, 8% of all the peptide samples Finnrick tested had measurable levels of endotoxins, bacterial fragments that can cause fever and chills or, in larger doses, septic shock.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The health risks aren’t just hypothetical. In 2025, two women had to be hospitalized and placed on ventilators after receiving peptide injections at a longevity conference in Las Vegas. Both recovered, and it’s still not clear whether they reacted to the peptides themselves or to some impurity in the vials.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“The idea that all peptides are safe and all peptides are natural is just nonsense,” Kaeberlein says. “I tend to consider myself fairly libertarian when it comes to what people want to do for their health,” he adds. “If you want to take an experimental drug, that’s up to you.” But the problem with unregulated experimental therapies is that it’s exceedingly difficult to assess benefit and harm. “The relatively small percentage of people that are bad actors will be bad actors, and they will dishonestly market this stuff to people who aren’t equipped to really understand the true risks and rewards,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;And, like any drug, peptides come with a risk of side effects. For approved medications, these are detailed right on the package insert. But for many experimental peptides, there hasn’t been enough research to understand what those side effects might be. Some researchers have warned that peptides that promote growth or blood vessel formation might also foster the growth of cancers. &amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For competitive athletes who use peptides, meanwhile, the risks include not just possible health problems but suspension. Some peptides, like BPC-157, are banned by the World Anti-Doping Agency.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The FDA has undergone a pretty substantial overhaul under the Trump administration. Are the regulations around peptides likely to change?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;I don’t have a crystal ball, but it seems likely. In May 2025, US health secretary Robert F. Kennedy Jr. joined the longevity enthusiast and biohacker Gary Brecka on his podcast &lt;em&gt;The Ultimate Human&lt;/em&gt; and promised to “end the war at FDA against alternative medicine—the war on stem cells, the war on chelating drugs, the war on peptides.”&lt;/p&gt;  &lt;p&gt;Knoepfler anticipates that Kennedy will force the FDA to allow compounding of some of the most popular peptides, like BPC-157 and GHK-Cu. “Such a step would put public health at great risk, while giving compounders and likely wellness influencers a lot more profit,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The FDA seems intent on cracking down on GLP-1 copycats, however. In early February, commissioner Marty Makary posted on X that the agency would take “swift action against companies mass-marketing illegal copycat drugs, claiming they are similar to FDA-approved products.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2026/02/260219_peptides_hero2.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt; Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. &lt;/em&gt;&lt;em&gt;You can read more from the series here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;Want to lose weight? Get shredded? Stay mentally sharp? A wellness influencer might tell you to take peptides, the latest cure-all in the alternative medicine arsenal. People inject them. They snort them. They combine them into concoctions with superhero names, like the Wolverine stack.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Matt Kaeberlein, a longevity researcher, first started hearing about peptides a few years ago. “At that point it was mostly functional medicine doctors that were using peptides,” he says, referring to physicians who embrace alternative medicine and supplements. “In the last six months, it’s kind of gone crazy.”&lt;/p&gt;  &lt;p&gt;Peptides have gone mainstream. At the health-technology startup Superpower in Los Angeles, employees can get free peptide shots on Fridays. At a health food store in Phoenix, a sidewalk sign reads, “We have peptides!” At a tae kwon do center in South Carolina, a peptide wholesaler hosts an informational evening. On social media, they’re everywhere. And that popularity seems poised to grow; Department of Health and Human Services secretary Robert F. Kennedy Jr. has promised to end the FDA’s “aggressive suppression” of peptides.&lt;/p&gt; 
 &lt;p&gt;The benefits and risks of many of these compounds, however, are largely unknown. Some of the most popular peptides have never been tested in human trials. They are sold for research purposes, not human consumption. Some are illegal knockoffs of wildly successful weight-loss medicines. The vast majority come from China, a fact that has some legislators worried. Last week, Senator Tom Cotton urged the head of the FDA to crack down on illegal shipments of peptides from China. In&lt;strong&gt; &lt;/strong&gt;the absence of regulatory oversight, some people are sending the compounds they purchase off for independent testing just to ensure that the product is legit.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What is a peptide?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;A peptide is simply a short string of amino acids, the building blocks of proteins. “Scientists generally think of peptides as very small protein fragments, but we don’t really have a precise cutoff between a peptide and a protein,” says Paul Knoepfler, a stem-cell researcher at the University of California, Davis. Insulin is a peptide, as is human growth hormone. So are some neurotransmitters, like oxytocin.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;But when wellness influencers talk about peptides, they’re often referring to particular compounds—formulated as injections, pills, or nasal sprays—that have become trendy lately.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Some of these peptides are FDA-approved prescription medications. GLP-1 medicines, for example, are approved to treat diabetes and obesity but are also easily accessible online to almost anyone who wants to use them. Many sites sell microdoses of GLP-1s with claims that they can “support longevity,” reduce cognitive decline, or curb inflammation.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Many more peptides are experimental. “The majority fall into the unapproved bucket,” says Kaeberlein, who is chief executive officer of Optispan, a Seattle-based health-care technology company focused on longevity. That bucket includes drugs that promote the release of growth hormones, like TB-500, CJC-1295, and ipamorelin, and compounds said to promote tissue repair and wound healing, like BPC-157 and GHK-Cu. It’s primarily these unapproved compounds that have raised concerns. “Anybody can set up an online shop selling research-grade peptides,” says Tenille Davis, a pharmacist and chief advocacy officer at the Alliance for Pharmacy Compounding, a trade organization representing more than 600 pharmacies. “And nobody knows what’s even in the vials.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;It’s not just fitness gurus, biohackers, and longevity fanatics who are taking these experimental drugs. Kaeberlein recalls hearing about an acquaintance whose doctor prescribed her unapproved&lt;strong&gt; &lt;/strong&gt;peptides. She was “just a typical upper-middle-class woman,” he says. “That’s when it really hit me that this has sort of gone relatively mainstream.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What do peptides do?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;All kinds of things, purportedly. GHK-Cu is supposed to help with wound healing and collagen production. BPC-157 is said to promote tissue repair and curb inflammation, TB-500 to foster blood vessel formation. Here’s the caveat: The evidence for these benefits comes largely from animal studies and online testimonials, not human trials. “There’s no human clinical evidence to show that they even do what people are claiming that they do,” says Stuart Phillips, a muscle physiologist at McMaster University in Hamilton, Ontario. “So it could be just a giant rip-off.”&lt;/p&gt;  &lt;p&gt;Some experimental peptides probably do have beneficial wound healing properties or regenerative effects, Kaeberlein says. For BPC-157, for example, “the animal data is compelling,” he says. But there are still plenty of unknowns: What is the right dosage? How long should you take it? What’s the best way to administer it? Those are questions that can be answered only through rigorous clinical trials. In the absence of those studies, doctors “just make up their own protocols,” he says.&lt;strong&gt; &lt;/strong&gt;Some consumers go the DIY route, reconstituting powdered peptides and injecting their own concoctions at home.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;So why am I seeing ads for these peptide therapies if they’re not approved?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Federal law prohibits companies from marketing medications that haven’t been approved. That includes most peptides, which are regulated as small molecules, not dietary supplements. (Two notable exceptions are collagen peptides and creatine peptides, often sold as powders.) The law is designed to protect consumers from drugs that haven’t been proved safe and effective.&lt;/p&gt;  &lt;p&gt;But it doesn’t stop labs from making peptides for research purposes. “Most of the peptides being consumed in the marketplace now are being sold by these online companies that are selling them labeled for research use only,” Davis says. The vials often bear disclaimers that clearly say as much: “For research use only”&amp;nbsp;or&amp;nbsp;“Not for human consumption.” It’s illegal to market these products for human use, but “the websites make it pretty clear that the buyers are intended to be using these products themselves,” she says.&lt;/p&gt; 

&lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;The practice isn’t legal, but enforcement has been sporadic. “FDA sends warning letters, shuts down companies. But because it’s all online, they have a really hard time keeping up with these entities,” Davis says. And companies have plenty of incentive to keep illegally marketing the products. “They can make millions of dollars without having to spend money and time doing research,” Knoepfler says. “It’s a cash grab.”&lt;/p&gt;  &lt;p&gt;Compounding pharmacies, which are legally allowed to create bespoke medications by mixing bulk active ingredients, often get requests to dispense peptides, but most peptides don’t meet the eligibility criteria for compounding. This has always been the case, but in 2023 the FDA explicitly added several common experimental peptides to the list of bulk substances that cannot be compounded because of safety concerns. “It put an exclamation point on policy that was already in place,” Davis says.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Many GLP-1 medications are available from compounding pharmacies. That used to be accepted because the drugs were in short supply. Now, however, supplies of most of these medications are stable, and sellers are under increasing pressure from regulators to stop mass-marketing these drugs.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What’s the harm in trying them?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Peptides sold for research purposes come from labs with little regulatory oversight. “When you buy stuff online intended for research grade, you have no idea what’s in the vial that you’re getting. You have no idea the sterility practices that it was manufactured under, or what sort of impurities might be in the vial,” Davis says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;Phillips has heard some people say they send their peptides for third-party testing to ensure that they’re pure, “like it’s some kind of flex,” he says. “And I’m like, ‘Well, you just proved that this stuff lives in the shadows, for crying out loud.’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt; &lt;p&gt;Finnrick Analytics, a peptide-testing startup in Austin, Texas, has analyzed the purity and potency of more than 5,000 samples of 15 different peptides from 173&amp;nbsp;vendors. The results show that the quality varies substantially from vendor to vendor and even batch to batch. For example, the company tested nearly 450 samples of BPC-157 from 64&amp;nbsp;vendors. In some cases, the vials sold as BPC-157 didn’t contain the compound at all. In those that did, the purity varied from about 82% to 100%.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Perhaps more worrying, 8% of all the peptide samples Finnrick tested had measurable levels of endotoxins, bacterial fragments that can cause fever and chills or, in larger doses, septic shock.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The health risks aren’t just hypothetical. In 2025, two women had to be hospitalized and placed on ventilators after receiving peptide injections at a longevity conference in Las Vegas. Both recovered, and it’s still not clear whether they reacted to the peptides themselves or to some impurity in the vials.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;“The idea that all peptides are safe and all peptides are natural is just nonsense,” Kaeberlein says. “I tend to consider myself fairly libertarian when it comes to what people want to do for their health,” he adds. “If you want to take an experimental drug, that’s up to you.” But the problem with unregulated experimental therapies is that it’s exceedingly difficult to assess benefit and harm. “The relatively small percentage of people that are bad actors will be bad actors, and they will dishonestly market this stuff to people who aren’t equipped to really understand the true risks and rewards,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_14"&gt;&lt;p&gt;And, like any drug, peptides come with a risk of side effects. For approved medications, these are detailed right on the package insert. But for many experimental peptides, there hasn’t been enough research to understand what those side effects might be. Some researchers have warned that peptides that promote growth or blood vessel formation might also foster the growth of cancers. &amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;For competitive athletes who use peptides, meanwhile, the risks include not just possible health problems but suspension. Some peptides, like BPC-157, are banned by the World Anti-Doping Agency.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;The FDA has undergone a pretty substantial overhaul under the Trump administration. Are the regulations around peptides likely to change?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;I don’t have a crystal ball, but it seems likely. In May 2025, US health secretary Robert F. Kennedy Jr. joined the longevity enthusiast and biohacker Gary Brecka on his podcast &lt;em&gt;The Ultimate Human&lt;/em&gt; and promised to “end the war at FDA against alternative medicine—the war on stem cells, the war on chelating drugs, the war on peptides.”&lt;/p&gt;  &lt;p&gt;Knoepfler anticipates that Kennedy will force the FDA to allow compounding of some of the most popular peptides, like BPC-157 and GHK-Cu. “Such a step would put public health at great risk, while giving compounders and likely wellness influencers a lot more profit,” he says.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The FDA seems intent on cracking down on GLP-1 copycats, however. In early February, commissioner Marty Makary posted on X that the agency would take “swift action against companies mass-marketing illegal copycat drugs, claiming they are similar to FDA-approved products.”&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/02/23/1133522/peptides-are-everywhere-heres-what-you-need-to-know/</guid><pubDate>Mon, 23 Feb 2026 18:52:40 +0000</pubDate></item><item><title>[NEW] Google’s Cloud AI leads on the three frontiers of model capability (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/googles-cloud-ai-lead-on-the-three-frontiers-of-model-capability/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/google-logo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As a product VP at Google Cloud, Michael Gerstenhaber works mostly on Vertex AI, the company’s unified platform for deploying enterprise AI. It gives him a high-level view of how companies are actually using AI models, and what still needs to be done to unleash the potential of agentic AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I spoke with Gerstenhaber, I was particularly struck by one idea I hadn’t heard before. As he put it, AI models are pushing against three frontiers at once: raw intelligence, response time, and a third quality that has less to do with raw capability than with cost — whether a model can be deployed cheaply enough to run at massive, unpredictable scale. It’s a new way of thinking about model capabilities, and a particularly valuable one for anyone trying to push frontier models in a new direction.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This interview has been edited for length and clarity.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Why don’t you start by walking us through your experience in AI so far, and what you do at Google.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I’ve been in AI for about two years now. I was at Anthropic for a year and a half, I’ve been at Google almost half a year now. I run Vertex AI, Google’s developer platform. Most of our customers are engineers building their own applications. They want access to agentic patterns. They want access to an agentic platform. They want access to the inference of the smartest models in the world. I provide them that, but I don’t provide the applications themselves. That’s for Shopify, Thomson Reuters, and our various customers to provide in their own domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;What drew you to Google?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is I think unique in the world in that we have everything from the interface to the infrastructure layer. We can build data centers. We can buy electricity and build power plants. We have our own chips. We have our own model. We have the inference layer that we control. We have the agentic layer we control. We have APIs for memory, for interleaved code writing. We have an agent engine on top of that that ensures compliance and governance. And then we even have the chat interface with Gemini enterprise and Gemini chat for consumers, right? So part of the reason I came here is because I saw Google as uniquely vertically integrated, and that being a strength for us.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;It’s odd because, even with all the differences between companies, it feels like all three of the big labs are really&lt;/strong&gt; &lt;strong&gt;close in capabilities. Is it just a race for more intelligence, or is it more complicated than that?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I see three boundaries. Models like Gemini Pro are tuned for raw intelligence. Think about writing code. You just want the best code you can get, doesn’t matter if it takes 45 minutes, because I have to maintain it, I have to put it in production. I just want the best.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there’s this other boundary with latency. If I’m doing customer support and I need to know how to apply a policy, you need intelligence to apply that policy. Are you allowed to transact a return? Can I upgrade my seat on an airplane? But it doesn’t matter how right you are if it took 45 minutes to get the answer. So for those cases, you want the most intelligent product within that latency budget, because more intelligence no longer matters once that person gets bored and hangs up the phone.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And then there’s this last bucket, where somebody like Reddit or Meta wants to moderate the entire internet. They have large budgets, but they can’t take an enterprise risk on something if they don’t know how it scales. They don’t know how many poisonous posts there will be today or tomorrow. So they have to restrict their budget to a model at the highest intelligence they can afford, but in a scalable way to an infinite number of subjects. And for that, cost becomes very, very important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of the things I’ve been puzzling about is why agentic systems are taking so long to catch on. It feels like the models are there and I’ve seen incredible demos, but we’re not seeing the kind of major changes I would have expected a year ago. What do you think is holding it back?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This technology is basically two years old, and there’s still a lot of missing infrastructure. We don’t have patterns for auditing what the agents are doing. We don’t have patterns for authorization of data to an agent. There are these patterns that are going to require work to put into production. And production is always a trailing indicator of what the technology is capable of. So two years isn’t long enough to see what the intelligence supports in production, and that’s where people are struggling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think it’s moved uniquely quickly in software engineering because it fits nicely in the software development lifecycle. We have a dev environment in which it’s safe to break things, and then we promote from the dev environment to the test environment. The process of writing code at Google requires two people to audit that code and both affirm that it’s good enough to put Google’s brand behind and give to our customers. So we have a lot of those human-in-the-loop processes that make the implementation exceptionally low-risk. But we need to produce those patterns in other places and for other professions.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/google-logo.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;As a product VP at Google Cloud, Michael Gerstenhaber works mostly on Vertex AI, the company’s unified platform for deploying enterprise AI. It gives him a high-level view of how companies are actually using AI models, and what still needs to be done to unleash the potential of agentic AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When I spoke with Gerstenhaber, I was particularly struck by one idea I hadn’t heard before. As he put it, AI models are pushing against three frontiers at once: raw intelligence, response time, and a third quality that has less to do with raw capability than with cost — whether a model can be deployed cheaply enough to run at massive, unpredictable scale. It’s a new way of thinking about model capabilities, and a particularly valuable one for anyone trying to push frontier models in a new direction.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;This interview has been edited for length and clarity.&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Why don’t you start by walking us through your experience in AI so far, and what you do at Google.&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I’ve been in AI for about two years now. I was at Anthropic for a year and a half, I’ve been at Google almost half a year now. I run Vertex AI, Google’s developer platform. Most of our customers are engineers building their own applications. They want access to agentic patterns. They want access to an agentic platform. They want access to the inference of the smartest models in the world. I provide them that, but I don’t provide the applications themselves. That’s for Shopify, Thomson Reuters, and our various customers to provide in their own domains.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;What drew you to Google?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Google is I think unique in the world in that we have everything from the interface to the infrastructure layer. We can build data centers. We can buy electricity and build power plants. We have our own chips. We have our own model. We have the inference layer that we control. We have the agentic layer we control. We have APIs for memory, for interleaved code writing. We have an agent engine on top of that that ensures compliance and governance. And then we even have the chat interface with Gemini enterprise and Gemini chat for consumers, right? So part of the reason I came here is because I saw Google as uniquely vertically integrated, and that being a strength for us.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;It’s odd because, even with all the differences between companies, it feels like all three of the big labs are really&lt;/strong&gt; &lt;strong&gt;close in capabilities. Is it just a race for more intelligence, or is it more complicated than that?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I see three boundaries. Models like Gemini Pro are tuned for raw intelligence. Think about writing code. You just want the best code you can get, doesn’t matter if it takes 45 minutes, because I have to maintain it, I have to put it in production. I just want the best.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Then there’s this other boundary with latency. If I’m doing customer support and I need to know how to apply a policy, you need intelligence to apply that policy. Are you allowed to transact a return? Can I upgrade my seat on an airplane? But it doesn’t matter how right you are if it took 45 minutes to get the answer. So for those cases, you want the most intelligent product within that latency budget, because more intelligence no longer matters once that person gets bored and hangs up the phone.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;And then there’s this last bucket, where somebody like Reddit or Meta wants to moderate the entire internet. They have large budgets, but they can’t take an enterprise risk on something if they don’t know how it scales. They don’t know how many poisonous posts there will be today or tomorrow. So they have to restrict their budget to a model at the highest intelligence they can afford, but in a scalable way to an infinite number of subjects. And for that, cost becomes very, very important.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;One of the things I’ve been puzzling about is why agentic systems are taking so long to catch on. It feels like the models are there and I’ve seen incredible demos, but we’re not seeing the kind of major changes I would have expected a year ago. What do you think is holding it back?&lt;/strong&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This technology is basically two years old, and there’s still a lot of missing infrastructure. We don’t have patterns for auditing what the agents are doing. We don’t have patterns for authorization of data to an agent. There are these patterns that are going to require work to put into production. And production is always a trailing indicator of what the technology is capable of. So two years isn’t long enough to see what the intelligence supports in production, and that’s where people are struggling.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I think it’s moved uniquely quickly in software engineering because it fits nicely in the software development lifecycle. We have a dev environment in which it’s safe to break things, and then we promote from the dev environment to the test environment. The process of writing code at Google requires two people to audit that code and both affirm that it’s good enough to put Google’s brand behind and give to our customers. So we have a lot of those human-in-the-loop processes that make the implementation exceptionally low-risk. But we need to produce those patterns in other places and for other professions.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/googles-cloud-ai-lead-on-the-three-frontiers-of-model-capability/</guid><pubDate>Mon, 23 Feb 2026 19:18:42 +0000</pubDate></item><item><title>[NEW] Anthropic accuses Chinese AI labs of mining Claude as US debates AI chip exports (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/anthropic-accuses-chinese-ai-labs-of-mining-claude-as-us-debates-ai-chip-exports/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2262515136.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is accusing three Chinese AI companies of setting up more than 24,000 fake accounts with its Claude AI model to improve their own models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The labs — DeepSeek, Moonshot AI, and MiniMax — allegedly generated more than 16 million exchanges with Claude through those accounts using a technique called “distillation.” Anthropic said the labs “targeted Claude’s most differentiated capabilities: agentic reasoning, tool use, and coding.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The accusations come amid debates over how strictly to enforce export controls on advanced AI chips, a policy aimed at curbing China’s AI development.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Distillation is a common training method that AI labs use on their own models to create smaller, cheaper versions, but competitors can use it to essentially copy the homework of other labs. OpenAI sent a memo to House lawmakers earlier this month accusing DeepSeek of using distillation to mimic its products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek first made waves a year ago when it released its open source R1 reasoning model that nearly matched American frontier labs in performance at a fraction of the cost. DeepSeek is expected to soon release DeepSeek V4, its latest model, which reportedly can outperform Anthropic’s Claude and OpenAI’s ChatGPT in coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The scale of each attack differed in scope. Anthropic tracked more than 150,000 exchanges from DeepSeek that seemed aimed at improving foundational logic and alignment, specifically around censorship-safe alternatives to policy-sensitive queries.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moonshot AI had more than 3.4 million exchanges targeting agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. Last month, the firm released a new open source model Kimi K2.5 and a coding agent.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;MiniMax’s 13 million exchanges targeted agentic coding and tool use and orchestration. Anthropic said it was able to observe MiniMax in action as it redirected nearly half its traffic to siphon capabilities from the latest Claude model when it was launched.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic says it will continue to invest in defenses that make distillation attacks harder to execute and easier to identify, but is calling on “a coordinated response across the AI industry, cloud providers, and policymakers.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The distillation attacks come at a time when American chip exports to China are still hotly debated. Last month, the Trump administration formally allowed U.S. companies like Nvidia to export advanced AI chips (like the H200) to China. Critics have argued that this loosening of export controls increases China’s AI computing capacity at a critical time in the global race for AI dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic says that the scale of extraction DeepSeek, MiniMax, and Moonshot performed “requires access to advanced chips.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Distillation attacks therefore reinforce the rationale for export controls: restricted chip access limits both direct model training and the scale of illicit distillation,” per Anthropic’s blog.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dmitri Alperovitch, chairman of the Silverado Policy Accelerator think-tank and co-founder of CrowdStrike, told TechCrunch he’s not surprised to see these attacks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s been clear for a while now that part of the reason for the rapid progress of Chinese AI models has been theft via distillation of U.S. frontier models. Now we know this for a fact,” Alperovitch said. “This should give us even more compelling reasons to refuse to sell any AI chips to any of these [companies], which would only advantage them further.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic also said distillation doesn’t only threaten to undercut American AI dominance, but could also create national security risks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anthropic and other U.S. companies build systems that prevent state and non-state actors from using AI to, for example, develop bioweapons or carry out malicious cyber activities,” reads Anthropic’s blog post. “Models built through illicit distillation are unlikely to retain those safeguards, meaning that dangerous capabilities can proliferate with many protections stripped out entirely.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic pointed to authoritarian governments deploying frontier AI for things like “offensive cyber operations, disinformation campaigns, and mass surveillance,” a risk that is multiplied if those models are open sourced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to DeepSeek, MiniMax, and Moonshot for comment. &lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/GettyImages-2262515136.jpg?w=1024" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic is accusing three Chinese AI companies of setting up more than 24,000 fake accounts with its Claude AI model to improve their own models.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The labs — DeepSeek, Moonshot AI, and MiniMax — allegedly generated more than 16 million exchanges with Claude through those accounts using a technique called “distillation.” Anthropic said the labs “targeted Claude’s most differentiated capabilities: agentic reasoning, tool use, and coding.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The accusations come amid debates over how strictly to enforce export controls on advanced AI chips, a policy aimed at curbing China’s AI development.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Distillation is a common training method that AI labs use on their own models to create smaller, cheaper versions, but competitors can use it to essentially copy the homework of other labs. OpenAI sent a memo to House lawmakers earlier this month accusing DeepSeek of using distillation to mimic its products.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;DeepSeek first made waves a year ago when it released its open source R1 reasoning model that nearly matched American frontier labs in performance at a fraction of the cost. DeepSeek is expected to soon release DeepSeek V4, its latest model, which reportedly can outperform Anthropic’s Claude and OpenAI’s ChatGPT in coding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The scale of each attack differed in scope. Anthropic tracked more than 150,000 exchanges from DeepSeek that seemed aimed at improving foundational logic and alignment, specifically around censorship-safe alternatives to policy-sensitive queries.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Moonshot AI had more than 3.4 million exchanges targeting agentic reasoning and tool use, coding and data analysis, computer-use agent development, and computer vision. Last month, the firm released a new open source model Kimi K2.5 and a coding agent.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;MiniMax’s 13 million exchanges targeted agentic coding and tool use and orchestration. Anthropic said it was able to observe MiniMax in action as it redirected nearly half its traffic to siphon capabilities from the latest Claude model when it was launched.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic says it will continue to invest in defenses that make distillation attacks harder to execute and easier to identify, but is calling on “a coordinated response across the AI industry, cloud providers, and policymakers.”&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The distillation attacks come at a time when American chip exports to China are still hotly debated. Last month, the Trump administration formally allowed U.S. companies like Nvidia to export advanced AI chips (like the H200) to China. Critics have argued that this loosening of export controls increases China’s AI computing capacity at a critical time in the global race for AI dominance.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Anthropic says that the scale of extraction DeepSeek, MiniMax, and Moonshot performed “requires access to advanced chips.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Distillation attacks therefore reinforce the rationale for export controls: restricted chip access limits both direct model training and the scale of illicit distillation,” per Anthropic’s blog.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Dmitri Alperovitch, chairman of the Silverado Policy Accelerator think-tank and co-founder of CrowdStrike, told TechCrunch he’s not surprised to see these attacks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s been clear for a while now that part of the reason for the rapid progress of Chinese AI models has been theft via distillation of U.S. frontier models. Now we know this for a fact,” Alperovitch said. “This should give us even more compelling reasons to refuse to sell any AI chips to any of these [companies], which would only advantage them further.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic also said distillation doesn’t only threaten to undercut American AI dominance, but could also create national security risks.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Anthropic and other U.S. companies build systems that prevent state and non-state actors from using AI to, for example, develop bioweapons or carry out malicious cyber activities,” reads Anthropic’s blog post. “Models built through illicit distillation are unlikely to retain those safeguards, meaning that dangerous capabilities can proliferate with many protections stripped out entirely.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic pointed to authoritarian governments deploying frontier AI for things like “offensive cyber operations, disinformation campaigns, and mass surveillance,” a risk that is multiplied if those models are open sourced.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to DeepSeek, MiniMax, and Moonshot for comment. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/anthropic-accuses-chinese-ai-labs-of-mining-claude-as-us-debates-ai-chip-exports/</guid><pubDate>Mon, 23 Feb 2026 19:57:27 +0000</pubDate></item><item><title>[NEW] With AI, investor loyalty is (almost) dead: At least a dozen OpenAI VCs now also back Anthropic (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/with-ai-investor-loyalty-is-almost-dead-at-least-a-dozen-openai-vcs-now-also-back-anthropic/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-516286525.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With OpenAI on the verge of finalizing a new $100 billion round, and Anthropic just closing its own monster $30 billion raise, one thing is clear: The concept of investor “loyalty” is only hanging on by a thread.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least a dozen direct investors in OpenAI were announced as backers in Anthropic’s $30 billion raise earlier this month, including Founders Fund, Iconiq, Insight Partners, and Sequoia Capital.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some dual investments are understandable if they come from the hedge fund or asset manager worlds, where their focus is still largely investing in public stocks (competitors or not). These include D1, Fidelity, and TPG.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of these was a bit shocking. Affiliated funds of BlackRock joined in Anthropic’s $30 billion raise even though BlackRock’s senior managing director and board member Adebayo Ogunlesi is also on OpenAI’s board of directors.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that world, it’s true that if various BlackRock funds get a chance to own OpenAI stock, they are likely to take it, never mind the personal association of a member of their senior leadership. (BlackRock runs every type of fund, including mutuals, closed-ends, and ETFs).&amp;nbsp;And we all know the history of OpenAI and Microsoft’s relationship and why Microsoft is hedging its bets. Ditto for Nvidia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But venture capital funds have — until now — operated differently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;VCs market themselves as “founder friendly” and “helpful,” the idea being that when a VC firm buys a chunk of a startup’s company, the investor will help that startup be successful, particularly against its major rivals. If you are an owner of both OpenAI and Anthropic, who does your loyalty belong to, besides your own investors?&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, startups are private companies. They typically share confidential information with their direct investors on their business status — data that isn’t disclosed publicly the way it is with public companies. In many cases, the VCs also take board seats, which carries another level of fiduciary responsibility to their portfolio companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What makes this particular case even more interesting is that Sam Altman comes from the world of venture capital, as a former president of Y Combinator. He knows the drill. In 2024, he reportedly gave his investors a list of OpenAI’s rivals that he didn’t want them to back. It largely included companies launched by folks who left OpenAI, including Anthropic, xAI, and Safe Superintelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman later denied that he told OpenAI investors they would be barred from future rounds if they backed his list of perceived rivals. Altman did admit that he said if they “made non-passive investments,” they would no longer receive OpenAI’s confidential business information, according to documents in the lawsuit between Elon Musk and OpenAI, Business Insider reported.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI is also breaking the mold because of the record-breaking amounts of money that the largest AI labs are raising as they experience never-before-seen growth (and never-before-seen data center needs). At some point, when the hat is being passed around, the needs are so great and the possibilities of returns are so large, who can be expected to say no?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It turns out that not all venture investors have yet slid down the slippery slope. Andreessen Horowitz backs OpenAI but not (yet) Anthropic. Menlo Ventures backs Anthropic but not (yet) OpenAI, for instance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, in our admittedly not exhaustive research, we found a dozen investors that appear to only have direct investments in one of these companies, not both.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others include Bessemer Venture Partners, General Catalyst, and Greenoaks. (Note: We originally asked Claude to give us the list of dual investors. It got almost as many entries wrong as it got right, so all this for a very cool tech whose work sometimes remains less trustworthy than an intern’s.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, as we previously reported, the fact that this longstanding rule has been tossed by some of the most respected firms in the Valley, like Sequoia, is notable. One investor we reached out to simply shrugged and said that as long as the firm doesn’t have a board seat, no one sees the harm in it anymore.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, conflict-of-interest policies should now become another thing that founders ask about before signing that term sheet, no matter who it’s from.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2019/06/GettyImages-516286525.jpg?resize=1200,1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;With OpenAI on the verge of finalizing a new $100 billion round, and Anthropic just closing its own monster $30 billion raise, one thing is clear: The concept of investor “loyalty” is only hanging on by a thread.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At least a dozen direct investors in OpenAI were announced as backers in Anthropic’s $30 billion raise earlier this month, including Founders Fund, Iconiq, Insight Partners, and Sequoia Capital.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Some dual investments are understandable if they come from the hedge fund or asset manager worlds, where their focus is still largely investing in public stocks (competitors or not). These include D1, Fidelity, and TPG.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One of these was a bit shocking. Affiliated funds of BlackRock joined in Anthropic’s $30 billion raise even though BlackRock’s senior managing director and board member Adebayo Ogunlesi is also on OpenAI’s board of directors.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In that world, it’s true that if various BlackRock funds get a chance to own OpenAI stock, they are likely to take it, never mind the personal association of a member of their senior leadership. (BlackRock runs every type of fund, including mutuals, closed-ends, and ETFs).&amp;nbsp;And we all know the history of OpenAI and Microsoft’s relationship and why Microsoft is hedging its bets. Ditto for Nvidia.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But venture capital funds have — until now — operated differently. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;VCs market themselves as “founder friendly” and “helpful,” the idea being that when a VC firm buys a chunk of a startup’s company, the investor will help that startup be successful, particularly against its major rivals. If you are an owner of both OpenAI and Anthropic, who does your loyalty belong to, besides your own investors?&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Additionally, startups are private companies. They typically share confidential information with their direct investors on their business status — data that isn’t disclosed publicly the way it is with public companies. In many cases, the VCs also take board seats, which carries another level of fiduciary responsibility to their portfolio companies.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What makes this particular case even more interesting is that Sam Altman comes from the world of venture capital, as a former president of Y Combinator. He knows the drill. In 2024, he reportedly gave his investors a list of OpenAI’s rivals that he didn’t want them to back. It largely included companies launched by folks who left OpenAI, including Anthropic, xAI, and Safe Superintelligence.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Altman later denied that he told OpenAI investors they would be barred from future rounds if they backed his list of perceived rivals. Altman did admit that he said if they “made non-passive investments,” they would no longer receive OpenAI’s confidential business information, according to documents in the lawsuit between Elon Musk and OpenAI, Business Insider reported.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;AI is also breaking the mold because of the record-breaking amounts of money that the largest AI labs are raising as they experience never-before-seen growth (and never-before-seen data center needs). At some point, when the hat is being passed around, the needs are so great and the possibilities of returns are so large, who can be expected to say no?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It turns out that not all venture investors have yet slid down the slippery slope. Andreessen Horowitz backs OpenAI but not (yet) Anthropic. Menlo Ventures backs Anthropic but not (yet) OpenAI, for instance.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In fact, in our admittedly not exhaustive research, we found a dozen investors that appear to only have direct investments in one of these companies, not both.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Others include Bessemer Venture Partners, General Catalyst, and Greenoaks. (Note: We originally asked Claude to give us the list of dual investors. It got almost as many entries wrong as it got right, so all this for a very cool tech whose work sometimes remains less trustworthy than an intern’s.)&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, as we previously reported, the fact that this longstanding rule has been tossed by some of the most respected firms in the Valley, like Sequoia, is notable. One investor we reached out to simply shrugged and said that as long as the firm doesn’t have a board seat, no one sees the harm in it anymore.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Still, conflict-of-interest policies should now become another thing that founders ask about before signing that term sheet, no matter who it’s from.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/with-ai-investor-loyalty-is-almost-dead-at-least-a-dozen-openai-vcs-now-also-back-anthropic/</guid><pubDate>Mon, 23 Feb 2026 21:46:41 +0000</pubDate></item><item><title>[NEW] Data center builders thought farmers would willingly sell land, learn otherwise (AI - Ars Technica)</title><link>https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/</link><description>&lt;article class="double-column h-entry post-2142230 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-tech-policy tag-artificial-intelligence tag-data-centers"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Even in a fragile farm economy, million-dollar offers can’t sway dedicated farmers.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg / Contributor | Bloomberg

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;It seems that tech giants eyeing rural zones for data center development have underestimated how attached American farmers have grown to their lands in the decades they’ve been nurturing them.&lt;/p&gt;
&lt;p&gt;Across the country, several farmers have firmly rejected eye-popping offers—sometimes in the tens of millions. These offers dwarf the value of their properties, but farmers have refused to put a price on the lands that they love most.&lt;/p&gt;
&lt;p&gt;In a report on Monday, The Guardian highlighted a handful of cases nationwide where farmers’ refusals have frustrated plans to build data centers in areas long deemed rural.&lt;/p&gt;
&lt;p&gt;It’s unclear how many farmers have received such offers, but rural lands have been increasingly targeted as demand for data centers to power AI has grown—most recently projected to increase by 165 percent by 2030. Globally, 40,000 acres are needed to support data center growth over the next five years, Hines Research estimated.&lt;/p&gt;
&lt;p&gt;For “Silicon Valley executives,” rural areas are likely attractive due to “weak zoning protections, cheap power, and abundant water,” The Guardian reported.&lt;/p&gt;
&lt;p&gt;It likely doesn’t help to sell the farmers these deals when they tend to come out of nowhere, following a knock on the door from a middleman who doesn’t make it clear who wants to buy the land or how the land would be used.&lt;/p&gt;
&lt;p&gt;One 82-year-old Kentucky woman, Ida Huddleston, turned away a “Fortune 500 company” offering $33 million for 650 acres. NBC News reported that several of her neighbors received similar offers. Huddleston joined at least five other residents in the county who refused to move forward after learning they’d have to sign a non-disclosure agreement just to find out who they would be dealing with. Ultimately, Huddleston had to search public records to figure out that a data center was even being planned in the area, The Guardian reported. The lack of transparency is a problem, farmers have said, because what buyers want to do with the land matters.&lt;/p&gt;
&lt;p&gt;“You don’t have enough to buy me out,” Huddleston told the company representatives when rejecting the deal. “I’m not for sale. Leave me alone, I’m satisfied.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, one resident in Huddleston’s county who received an offer, 75-year-old Timothy Grosser, even declined a proposal to “name your price” when a tech company sought to buy his 250-acre farm, The Guardian reported.&lt;/p&gt;
&lt;p&gt;“There is none,” Grosser said.&lt;/p&gt;
&lt;p&gt;The farm is where he “lives, hunts, and raises cattle” and where his grandson hunts a turkey every Christmas for the family feast.&lt;/p&gt;
&lt;p&gt;“The money’s not worth giving up your lifestyle,” Grosser said.&lt;/p&gt;
&lt;p&gt;Another farmer in Wisconsin, Anthony Barta, reportedly fretted about what would happen to his neighbors if he took a deal he was offered—showing the deep bonds of people whose farms have bordered each other for years. In his community, another farmer was offered between $70 million and $80 million for 6,000 acres.&lt;/p&gt;
&lt;p&gt;“Me and my family, we own the farm and run close to 1,000 animals,” Barta said. “What would that do if that’s next to it? Can they even be there? You know, that’s our livelihood—the farm. We’re just concerned what, if it would go through, what would happen to us and our neighbors and farms and our community? What would happen to that?”&lt;/p&gt;
&lt;p&gt;Some tech companies are apparently not taking “no” for an answer. At least one farmer who spent 51 years milking cows in Pennsylvania prior to the AI boom described tech companies as “relentless.”&lt;/p&gt;
&lt;p&gt;Eighty-six-year-old Mervin Raudabaugh, Jr., found a creative solution to end the pressure to sell two contiguous farms. He reportedly staved off developers by turning to “a farmland preservation program dedicating taxpayer dollars toward protecting agricultural resources.”&lt;/p&gt;
&lt;p&gt;By working with the program, Raudabaugh will only receive about one-eighth of what the developers were offering. But he said it’s worth it to know his land would be preserved for farming purposes and out of reach of persistent tech companies.&lt;/p&gt;
&lt;p&gt;“These people have hounded the living daylights out of me,” Raudabaugh said.&lt;/p&gt;
&lt;h2&gt;Data center deals come amid fragile farm economy&lt;/h2&gt;
&lt;p&gt;For people in rural communities, data center fights go beyond concerns about water and electricity consumption—although those are concerns, too. Communities are defending the character of the land, which they don’t want to see suddenly disrupted by extensive construction, data center noise pollution, or untold environmental impacts from massive operations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There are also public health concerns, as an attorney with environmental nonprofit Earthjustice, Jonathan Kalmuss-Katz, told The Guardian that the pollution new data centers will emit could possibly proliferate “forever chemicals” known as polyfluoroalkyl substances (PFAS).&lt;/p&gt;
&lt;p&gt;“We know there are PFAS in these centers, and all of that has to go somewhere,” Kalmuss-Katz said. “This issue has been dangerously understudied as we have been building out data centers, and there’s not adequate information on what the long term impacts will be.”&lt;/p&gt;
&lt;p&gt;Some rural communities are fighting to block rezoning requests that would allow developers to build data centers in areas previously zoned only for agricultural lands. But those fights are seemingly hard-won. At least one Michigan community sought to settle after a developer firm working for an unnamed tech company reportedly sued to push through a construction project on farmland.&lt;/p&gt;
&lt;p&gt;For farmers, the data center deals come at a particularly difficult time financially. On Friday, the National Farmers Union issued a statement after the Supreme Court blocked the majority of Trump’s tariffs. In it, NFU President Rob Larew confirmed that “many family farmers and ranchers have already felt the consequences” of Trump’s tariffs, which have raised costs, disrupted sales, and triggered retaliations impacting US agricultural goods.&lt;/p&gt;
&lt;p&gt;“In an already fragile farm economy, uncertainty has hit family operations hardest,” Larew said.&lt;/p&gt;
&lt;p&gt;The deals also arrived at a time when the number of US farms is shrinking, continuing “a long-lasting trend of declining farm numbers,” Farm Journal reported this month. In total, the US lost about 15,000 farms in 2025, with no state reporting an increase in farms.&lt;/p&gt;
&lt;p&gt;So far, not much farmland has been ceded to data centers, the report seemed to indicate, perhaps due to farmers who dig their heels in when developer representatives come knocking.&lt;/p&gt;
&lt;p&gt;Perhaps particularly in this dire climate—where farms are shrinking and existing farms are cash-strapped from unpredictable tariffs—it seems notable that farmers are not being swayed by developers’ offers of what the Guardian described as “unimaginable riches.”&lt;/p&gt;
&lt;p&gt;But Huddleston made it sound easy to turn down $33 million, because her ties to the land run deep. She told The Guardian that “four generations of the Huddleston family have watched the world change from the same fields,” while raising cattle and living off the land.&lt;/p&gt;
&lt;p&gt;“My whole entire life is nothing but the land,” Huddleston said. “It’s provided me with anything and everything that I’ve needed for 82 years.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b388ff; background-color: #512da8;"&gt;&lt;img alt="scortiusthecharioteer" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/821/821783.jpg?1685642227" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              scortiusthecharioteer
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;On the note of pollution from dtatacenters, infrasound is understudied, especially its impact on wildlife ecology - and in this case, livestock as well.&lt;p&gt;That's aside from the human impacts, which could really use some public awareness and further research.&lt;/p&gt;&lt;/blockquote&gt;Decent piece on it recently by Benn Jordan.  &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;View: https://youtu.be/_bP80DEAbuo?si=Sm9kNaEj7gJgZdx1&lt;br /&gt;
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-02-23T22:14:33+00:00"&gt;February 23, 2026 at 10:14 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</description><content:encoded>&lt;article class="double-column h-entry post-2142230 post type-post status-publish format-standard has-post-thumbnail hentry category-ai category-tech-policy tag-artificial-intelligence tag-data-centers"&gt;
  
  &lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7 dark:bg-gray-700"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Even in a fragile farm economy, million-dollar offers can’t sway dedicated farmers.
      &lt;/p&gt;

              
          &lt;/div&gt;

    &lt;div class="mt-4 min-h-1 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="427" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-640x427.jpg" width="640" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2026/02/GettyImages-1233733221-1024x648.jpg" width="1024" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Bloomberg / Contributor | Bloomberg

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;It seems that tech giants eyeing rural zones for data center development have underestimated how attached American farmers have grown to their lands in the decades they’ve been nurturing them.&lt;/p&gt;
&lt;p&gt;Across the country, several farmers have firmly rejected eye-popping offers—sometimes in the tens of millions. These offers dwarf the value of their properties, but farmers have refused to put a price on the lands that they love most.&lt;/p&gt;
&lt;p&gt;In a report on Monday, The Guardian highlighted a handful of cases nationwide where farmers’ refusals have frustrated plans to build data centers in areas long deemed rural.&lt;/p&gt;
&lt;p&gt;It’s unclear how many farmers have received such offers, but rural lands have been increasingly targeted as demand for data centers to power AI has grown—most recently projected to increase by 165 percent by 2030. Globally, 40,000 acres are needed to support data center growth over the next five years, Hines Research estimated.&lt;/p&gt;
&lt;p&gt;For “Silicon Valley executives,” rural areas are likely attractive due to “weak zoning protections, cheap power, and abundant water,” The Guardian reported.&lt;/p&gt;
&lt;p&gt;It likely doesn’t help to sell the farmers these deals when they tend to come out of nowhere, following a knock on the door from a middleman who doesn’t make it clear who wants to buy the land or how the land would be used.&lt;/p&gt;
&lt;p&gt;One 82-year-old Kentucky woman, Ida Huddleston, turned away a “Fortune 500 company” offering $33 million for 650 acres. NBC News reported that several of her neighbors received similar offers. Huddleston joined at least five other residents in the county who refused to move forward after learning they’d have to sign a non-disclosure agreement just to find out who they would be dealing with. Ultimately, Huddleston had to search public records to figure out that a data center was even being planned in the area, The Guardian reported. The lack of transparency is a problem, farmers have said, because what buyers want to do with the land matters.&lt;/p&gt;
&lt;p&gt;“You don’t have enough to buy me out,” Huddleston told the company representatives when rejecting the deal. “I’m not for sale. Leave me alone, I’m satisfied.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Notably, one resident in Huddleston’s county who received an offer, 75-year-old Timothy Grosser, even declined a proposal to “name your price” when a tech company sought to buy his 250-acre farm, The Guardian reported.&lt;/p&gt;
&lt;p&gt;“There is none,” Grosser said.&lt;/p&gt;
&lt;p&gt;The farm is where he “lives, hunts, and raises cattle” and where his grandson hunts a turkey every Christmas for the family feast.&lt;/p&gt;
&lt;p&gt;“The money’s not worth giving up your lifestyle,” Grosser said.&lt;/p&gt;
&lt;p&gt;Another farmer in Wisconsin, Anthony Barta, reportedly fretted about what would happen to his neighbors if he took a deal he was offered—showing the deep bonds of people whose farms have bordered each other for years. In his community, another farmer was offered between $70 million and $80 million for 6,000 acres.&lt;/p&gt;
&lt;p&gt;“Me and my family, we own the farm and run close to 1,000 animals,” Barta said. “What would that do if that’s next to it? Can they even be there? You know, that’s our livelihood—the farm. We’re just concerned what, if it would go through, what would happen to us and our neighbors and farms and our community? What would happen to that?”&lt;/p&gt;
&lt;p&gt;Some tech companies are apparently not taking “no” for an answer. At least one farmer who spent 51 years milking cows in Pennsylvania prior to the AI boom described tech companies as “relentless.”&lt;/p&gt;
&lt;p&gt;Eighty-six-year-old Mervin Raudabaugh, Jr., found a creative solution to end the pressure to sell two contiguous farms. He reportedly staved off developers by turning to “a farmland preservation program dedicating taxpayer dollars toward protecting agricultural resources.”&lt;/p&gt;
&lt;p&gt;By working with the program, Raudabaugh will only receive about one-eighth of what the developers were offering. But he said it’s worth it to know his land would be preserved for farming purposes and out of reach of persistent tech companies.&lt;/p&gt;
&lt;p&gt;“These people have hounded the living daylights out of me,” Raudabaugh said.&lt;/p&gt;
&lt;h2&gt;Data center deals come amid fragile farm economy&lt;/h2&gt;
&lt;p&gt;For people in rural communities, data center fights go beyond concerns about water and electricity consumption—although those are concerns, too. Communities are defending the character of the land, which they don’t want to see suddenly disrupted by extensive construction, data center noise pollution, or untold environmental impacts from massive operations.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
            
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;There are also public health concerns, as an attorney with environmental nonprofit Earthjustice, Jonathan Kalmuss-Katz, told The Guardian that the pollution new data centers will emit could possibly proliferate “forever chemicals” known as polyfluoroalkyl substances (PFAS).&lt;/p&gt;
&lt;p&gt;“We know there are PFAS in these centers, and all of that has to go somewhere,” Kalmuss-Katz said. “This issue has been dangerously understudied as we have been building out data centers, and there’s not adequate information on what the long term impacts will be.”&lt;/p&gt;
&lt;p&gt;Some rural communities are fighting to block rezoning requests that would allow developers to build data centers in areas previously zoned only for agricultural lands. But those fights are seemingly hard-won. At least one Michigan community sought to settle after a developer firm working for an unnamed tech company reportedly sued to push through a construction project on farmland.&lt;/p&gt;
&lt;p&gt;For farmers, the data center deals come at a particularly difficult time financially. On Friday, the National Farmers Union issued a statement after the Supreme Court blocked the majority of Trump’s tariffs. In it, NFU President Rob Larew confirmed that “many family farmers and ranchers have already felt the consequences” of Trump’s tariffs, which have raised costs, disrupted sales, and triggered retaliations impacting US agricultural goods.&lt;/p&gt;
&lt;p&gt;“In an already fragile farm economy, uncertainty has hit family operations hardest,” Larew said.&lt;/p&gt;
&lt;p&gt;The deals also arrived at a time when the number of US farms is shrinking, continuing “a long-lasting trend of declining farm numbers,” Farm Journal reported this month. In total, the US lost about 15,000 farms in 2025, with no state reporting an increase in farms.&lt;/p&gt;
&lt;p&gt;So far, not much farmland has been ceded to data centers, the report seemed to indicate, perhaps due to farmers who dig their heels in when developer representatives come knocking.&lt;/p&gt;
&lt;p&gt;Perhaps particularly in this dire climate—where farms are shrinking and existing farms are cash-strapped from unpredictable tariffs—it seems notable that farmers are not being swayed by developers’ offers of what the Guardian described as “unimaginable riches.”&lt;/p&gt;
&lt;p&gt;But Huddleston made it sound easy to turn down $33 million, because her ties to the land run deep. She told The Guardian that “four generations of the Huddleston family have watched the world change from the same fields,” while raising cattle and living off the land.&lt;/p&gt;
&lt;p&gt;“My whole entire life is nothing but the land,” Huddleston said. “It’s provided me with anything and everything that I’ve needed for 82 years.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
      &lt;div class="ad-wrapper-inner"&gt;
        &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
                &lt;/div&gt;
    &lt;/div&gt;
  &lt;/article&gt;&lt;article class="comment-pick"&gt;
          &lt;header&gt;
            &lt;span class="ars-avatar" style="color: #b388ff; background-color: #512da8;"&gt;&lt;img alt="scortiusthecharioteer" class="ars-avatar-image" src="https://cdn.arstechnica.net/civis/data/avatars/m/821/821783.jpg?1685642227" /&gt;&lt;/span&gt;

            &lt;div class="text-base font-bold sm:text-xl"&gt;
              scortiusthecharioteer
            &lt;/div&gt;
          &lt;/header&gt;

          &lt;div class="comments-pick-content"&gt;
            &lt;blockquote class="xfBb-quote"&gt;On the note of pollution from dtatacenters, infrasound is understudied, especially its impact on wildlife ecology - and in this case, livestock as well.&lt;p&gt;That's aside from the human impacts, which could really use some public awareness and further research.&lt;/p&gt;&lt;/blockquote&gt;Decent piece on it recently by Benn Jordan.  &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;View: https://youtu.be/_bP80DEAbuo?si=Sm9kNaEj7gJgZdx1&lt;br /&gt;
          &lt;/p&gt;&lt;/div&gt;

          &lt;div class="comments-pick-timestamp"&gt;
            
              &lt;time datetime="2026-02-23T22:14:33+00:00"&gt;February 23, 2026 at 10:14 pm&lt;/time&gt;
            
          &lt;/div&gt;
        &lt;/article&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/tech-policy/2026/02/im-not-for-sale-farmers-refuse-to-take-millions-in-data-center-deals/</guid><pubDate>Mon, 23 Feb 2026 21:48:44 +0000</pubDate></item><item><title>[NEW] Deploying Open Source Vision Language Models (VLM) on Jetson (Hugging Face - Blog)</title><link>https://huggingface.co/blog/nvidia/cosmos-on-jetson</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/blog/nvidia/cosmos-on-jetson.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
Vision-Language Models (VLMs) mark a significant leap in AI by blending visual perception with semantic reasoning. Moving beyond traditional models constrained by fixed labels, VLMs utilize a joint embedding space to interpret and discuss complex, open-ended environments using natural language. 
&lt;p&gt;The rapid evolution of reasoning accuracy and efficiency has made these models ideal for edge devices. The NVIDIA Jetson family, ranging from the high-performance AGX Thor and AGX Orin to the compact Orin Nano Super is purpose-built to drive accelerated applications for physical AI and robotics, providing the optimized runtime necessary for leading open source models. &lt;/p&gt;
&lt;p&gt;In this tutorial, we will demonstrate how to deploy the NVIDIA Cosmos Reasoning 2B model across the Jetson lineup using the vLLM framework. We will also guide you through connecting this model to the Live VLM WebUI, enabling a real-time, webcam-based interface for interactive physical AI. &lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Prerequisites
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Supported Devices:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jetson AGX Thor Developer Kit&lt;/li&gt;
&lt;li&gt;Jetson AGX Orin (64GB / 32GB)&lt;/li&gt;
&lt;li&gt;Jetson Orin Super Nano&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;JetPack Version:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JetPack 6 (L4T r36.x) — for Orin devices&lt;/li&gt;
&lt;li&gt;JetPack 7 (L4T r38.x) — for Thor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; NVMe SSD &lt;strong&gt;required&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;~5 GB for the FP8 model weights&lt;/li&gt;
&lt;li&gt;~8 GB for the vLLM container image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Accounts:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create NVIDIA NGC account(free) to download both the model and vLLM contanier&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Overview
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Jetson AGX Thor&lt;/th&gt;
&lt;th&gt;Jetson AGX Orin&lt;/th&gt;
&lt;th&gt;Orin Super Nano&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;vLLM Container&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nvcr.io/nvidia/vllm:26.01-py3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Max Model Length&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;256 tokens (memory-constrained)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GPU Memory Util&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The workflow is the same for both devices:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download&lt;/strong&gt; the FP8 model checkpoint via NGC CLI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull&lt;/strong&gt; the vLLM Docker image for your device&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt; the container with the model mounted as a volume&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connect&lt;/strong&gt; Live VLM WebUI to the vLLM endpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 1: Install the NGC CLI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The NGC CLI lets you download model checkpoints from the NVIDIA NGC Catalog.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Download and install
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/Projects/CosmosReasoning
cd ~/Projects/CosmosReasoning

# Download the NGC CLI for ARM64
# Get the latest installer URL from: https://org.ngc.nvidia.com/setup/installers/cli
wget -O ngccli_arm64.zip https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/4.13.0/files/ngccli_arm64.zip
unzip ngccli_arm64.zip
chmod u+x ngc-cli/ngc

# Add to PATH
export PATH="$PATH:$(pwd)/ngc-cli"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the CLI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ngc config set
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be prompted for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Key&lt;/strong&gt; — generate one at NGC API Key setup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI output format&lt;/strong&gt; — choose &lt;code&gt;json&lt;/code&gt; or &lt;code&gt;ascii&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;org&lt;/strong&gt; — press Enter to accept the default&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 2: Download the Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Download the &lt;strong&gt;FP8 quantized&lt;/strong&gt; checkpoint. This is used on all Jetson devices:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/Projects/CosmosReasoning
ngc registry model download-version "nim/nvidia/cosmos-reason2-2b:1208-fp8-static-kv8"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a directory called &lt;code&gt;cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt; containing the model weights. Note the full path — you will mount it into the Docker container as a volume.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 3: Pull the vLLM Docker Image
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull nvcr.io/nvidia/vllm:26.01-py3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Orin / Orin Super Nano
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04
&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 4: Serve Cosmos Reasoning 2B with vLLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option A: Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Thor has ample GPU memory and can run the model with generous context length.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Launch the container with the model mounted:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  --ipc host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  nvcr.io/nvidia/vllm:26.01-py3 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Inside the container, activate the environment and serve the model:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code&gt;--reasoning-parser qwen3&lt;/code&gt; flag enables chain-of-thought reasoning extraction. The &lt;code&gt;--media-io-kwargs&lt;/code&gt; flag configures video frame handling.&lt;/p&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option B: Jetson AGX Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AGX Orin has enough memory to run the model with the same generous parameters as Thor.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option C: Jetson Orin Super Nano (memory-constrained)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The Orin Super Nano has significantly less RAM, so we need aggressive memory optimization flags.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --enforce-eager \
  --max-model-len 256 \
  --max-num-batched-tokens 256 \
  --gpu-memory-utilization 0.65 \
  --max-num-seqs 1 \
  --enable-chunked-prefill \
  --limit-mm-per-prompt '{"image":1,"video":1}' \
  --mm-processor-kwargs '{"num_frames":2,"max_pixels":150528}'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key flags explained (Orin Super Nano only):&lt;/strong&gt;&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enforce-eager&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Disables CUDA graphs to save memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-model-len 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits context to fit in available memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-batched-tokens 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Matches the model length limit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--gpu-memory-utilization 0.65&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reserves headroom for system processes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-seqs 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Single request at a time to minimize memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enable-chunked-prefill&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Processes prefill in chunks for memory efficiency&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--limit-mm-per-prompt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits to 1 image and 1 video per prompt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--mm-processor-kwargs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reduces video frames and image resolution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--VLLM_SKIP_WARMUP=true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Skips warmup to save time and memory&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Wait until you see the server is ready:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Verify the server is running
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;From another terminal on the Jetson:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl http://localhost:8000/v1/models
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see the model listed in the response.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 5: Test with a Quick API Call
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before connecting the WebUI, verify the model responds correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models/cosmos-reason2-2b",
    "messages": [
      {
        "role": "user",
        "content": "What capabilities do you have?"
      }
    ],
    "max_tokens": 128
  }' | python3 -m json.tool
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; The model name used in the API request must match what vLLM reports. Verify with &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 6: Connect to Live VLM WebUI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Live VLM WebUI provides a real-time webcam-to-VLM interface. With vLLM serving Cosmos Reasoning 2B, you can stream your webcam and get live AI analysis with reasoning.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Install Live VLM WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The easiest method is pip (Open another terminal):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env
cd ~/Projects/CosmosReasoning
uv venv .live-vlm --python 3.12
source .live-vlm/bin/activate
uv pip install live-vlm-webui
live-vlm-webui
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or use Docker:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
./scripts/start_container.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Open &lt;strong&gt;&lt;code&gt;https://localhost:8090&lt;/code&gt;&lt;/strong&gt; in your browser&lt;/li&gt;
&lt;li&gt;Accept the self-signed certificate (click &lt;strong&gt;Advanced&lt;/strong&gt; → &lt;strong&gt;Proceed&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;In the &lt;strong&gt;VLM API Configuration&lt;/strong&gt; section on the left sidebar:&lt;ul&gt;
&lt;li&gt;Set &lt;strong&gt;API Base URL&lt;/strong&gt; to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click the &lt;strong&gt;Refresh&lt;/strong&gt; button to detect the model&lt;/li&gt;
&lt;li&gt;Select the Cosmos Reasoning 2B model from the dropdown&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select your camera and click &lt;strong&gt;Start&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The WebUI will now stream your webcam frames to Cosmos Reasoning 2B and display the model’s analysis in real-time.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Recommended WebUI settings for Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Since Orin runs with a shorter context length, adjust these settings in the WebUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Max Tokens&lt;/strong&gt;: Set to &lt;strong&gt;100–150&lt;/strong&gt; (shorter responses complete faster)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frame Processing Interval&lt;/strong&gt;: Set to &lt;strong&gt;60+&lt;/strong&gt; (gives the model time between frames)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Troubleshooting
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Out of memory on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM crashes with CUDA out-of-memory errors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Free system memory before starting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower &lt;code&gt;--gpu-memory-utilization&lt;/code&gt; (try &lt;code&gt;0.55&lt;/code&gt; or &lt;code&gt;0.50&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reduce &lt;code&gt;--max-model-len&lt;/code&gt; further (try &lt;code&gt;128&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure no other GPU-intensive processes are running&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Model not found in WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The model doesn’t appear in the Live VLM WebUI dropdown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Verify vLLM is running: &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the WebUI API Base URL is set to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt; (not &lt;code&gt;https&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;If vLLM and WebUI are in separate containers, use &lt;code&gt;http://&amp;lt;jetson-ip&amp;gt;:8000/v1&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Slow inference on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Each response takes a very long time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is expected with the memory-constrained configuration. Cosmos Reasoning 2B FP8 on Orin prioritizes fitting in memory over speed&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;max_tokens&lt;/code&gt; in the WebUI to get shorter, faster responses&lt;/li&gt;
&lt;li&gt;Increase the frame interval so the model isn’t constantly processing new frames&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		vLLM fails to load model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM reports that the model path doesn’t exist or can’t be loaded.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify the NGC download completed successfully: &lt;code&gt;ls ~/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the volume mount path is correct in your &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;Check that the model directory is mounted as read-only (&lt;code&gt;:ro&lt;/code&gt;) and the path inside the container matches what you pass to &lt;code&gt;vllm serve&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In this tutorial, we showcased how to deploy &lt;strong&gt;NVIDIA Cosmos Reasoning 2B&lt;/strong&gt; model on Jetson family of devices using vLLM. &lt;/p&gt;


&lt;p&gt;The combination of Cosmos Reasoning 2B’s chain-of-thought capabilities with Live VLM WebUI’s real-time streaming makes it ideal to prototype and evaluate vision AI applications at the edge.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Additional Resources
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://cdn-thumbnails.huggingface.co/social-thumbnails/blog/nvidia/cosmos-on-jetson.png" /&gt;&lt;/div&gt;&lt;!-- HTML_TAG_START --&gt;
Vision-Language Models (VLMs) mark a significant leap in AI by blending visual perception with semantic reasoning. Moving beyond traditional models constrained by fixed labels, VLMs utilize a joint embedding space to interpret and discuss complex, open-ended environments using natural language. 
&lt;p&gt;The rapid evolution of reasoning accuracy and efficiency has made these models ideal for edge devices. The NVIDIA Jetson family, ranging from the high-performance AGX Thor and AGX Orin to the compact Orin Nano Super is purpose-built to drive accelerated applications for physical AI and robotics, providing the optimized runtime necessary for leading open source models. &lt;/p&gt;
&lt;p&gt;In this tutorial, we will demonstrate how to deploy the NVIDIA Cosmos Reasoning 2B model across the Jetson lineup using the vLLM framework. We will also guide you through connecting this model to the Live VLM WebUI, enabling a real-time, webcam-based interface for interactive physical AI. &lt;/p&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Prerequisites
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Supported Devices:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jetson AGX Thor Developer Kit&lt;/li&gt;
&lt;li&gt;Jetson AGX Orin (64GB / 32GB)&lt;/li&gt;
&lt;li&gt;Jetson Orin Super Nano&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;JetPack Version:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JetPack 6 (L4T r36.x) — for Orin devices&lt;/li&gt;
&lt;li&gt;JetPack 7 (L4T r38.x) — for Thor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Storage:&lt;/strong&gt; NVMe SSD &lt;strong&gt;required&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;~5 GB for the FP8 model weights&lt;/li&gt;
&lt;li&gt;~8 GB for the vLLM container image&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Accounts:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create NVIDIA NGC account(free) to download both the model and vLLM contanier&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Overview
	&lt;/span&gt;
&lt;/h2&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Jetson AGX Thor&lt;/th&gt;
&lt;th&gt;Jetson AGX Orin&lt;/th&gt;
&lt;th&gt;Orin Super Nano&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;vLLM Container&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;nvcr.io/nvidia/vllm:26.01-py3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;td&gt;FP8 via NGC (volume mount)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Max Model Length&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;8192 tokens&lt;/td&gt;
&lt;td&gt;256 tokens (memory-constrained)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;GPU Memory Util&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.65&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;The workflow is the same for both devices:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Download&lt;/strong&gt; the FP8 model checkpoint via NGC CLI&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pull&lt;/strong&gt; the vLLM Docker image for your device&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Launch&lt;/strong&gt; the container with the model mounted as a volume&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Connect&lt;/strong&gt; Live VLM WebUI to the vLLM endpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 1: Install the NGC CLI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;The NGC CLI lets you download model checkpoints from the NVIDIA NGC Catalog.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Download and install
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p ~/Projects/CosmosReasoning
cd ~/Projects/CosmosReasoning

# Download the NGC CLI for ARM64
# Get the latest installer URL from: https://org.ngc.nvidia.com/setup/installers/cli
wget -O ngccli_arm64.zip https://api.ngc.nvidia.com/v2/resources/nvidia/ngc-apps/ngc_cli/versions/4.13.0/files/ngccli_arm64.zip
unzip ngccli_arm64.zip
chmod u+x ngc-cli/ngc

# Add to PATH
export PATH="$PATH:$(pwd)/ngc-cli"
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the CLI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;ngc config set
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will be prompted for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;API Key&lt;/strong&gt; — generate one at NGC API Key setup&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLI output format&lt;/strong&gt; — choose &lt;code&gt;json&lt;/code&gt; or &lt;code&gt;ascii&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;org&lt;/strong&gt; — press Enter to accept the default&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 2: Download the Model
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Download the &lt;strong&gt;FP8 quantized&lt;/strong&gt; checkpoint. This is used on all Jetson devices:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd ~/Projects/CosmosReasoning
ngc registry model download-version "nim/nvidia/cosmos-reason2-2b:1208-fp8-static-kv8"
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a directory called &lt;code&gt;cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt; containing the model weights. Note the full path — you will mount it into the Docker container as a volume.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 3: Pull the vLLM Docker Image
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull nvcr.io/nvidia/vllm:26.01-py3
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		For Jetson AGX Orin / Orin Super Nano
	&lt;/span&gt;
&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04
&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 4: Serve Cosmos Reasoning 2B with vLLM
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option A: Jetson AGX Thor
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Thor has ample GPU memory and can run the model with generous context length.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Launch the container with the model mounted:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  --ipc host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  nvcr.io/nvidia/vllm:26.01-py3 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Inside the container, activate the environment and serve the model:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code&gt;--reasoning-parser qwen3&lt;/code&gt; flag enables chain-of-thought reasoning extraction. The &lt;code&gt;--media-io-kwargs&lt;/code&gt; flag configures video frame handling.&lt;/p&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option B: Jetson AGX Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;AGX Orin has enough memory to run the model with the same generous parameters as Thor.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --max-model-len 8192 \
  --media-io-kwargs '{"video": {"num_frames": -1}}' \
  --reasoning-parser qwen3 \
  --gpu-memory-utilization 0.8
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait until you see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Option C: Jetson Orin Super Nano (memory-constrained)
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The Orin Super Nano has significantly less RAM, so we need aggressive memory optimization flags.&lt;/p&gt;
&lt;p&gt;Set the path to your downloaded model and free cached memory on the host:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;MODEL_PATH="$HOME/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8"
sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;1. Launch the container:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run --rm -it \
  --runtime nvidia \
  --network host \
  -v "$MODEL_PATH:/models/cosmos-reason2-2b:ro" \
  -e NVIDIA_VISIBLE_DEVICES=all \
  -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
  ghcr.io/nvidia-ai-iot/vllm:r36.4-tegra-aarch64-cu126-22.04 \
  bash
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Inside the container, activate the environment and serve:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /opt/
source venv/bin/activate

vllm serve /models/cosmos-reason2-2b \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --enforce-eager \
  --max-model-len 256 \
  --max-num-batched-tokens 256 \
  --gpu-memory-utilization 0.65 \
  --max-num-seqs 1 \
  --enable-chunked-prefill \
  --limit-mm-per-prompt '{"image":1,"video":1}' \
  --mm-processor-kwargs '{"num_frames":2,"max_pixels":150528}'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Key flags explained (Orin Super Nano only):&lt;/strong&gt;&lt;/p&gt;
&lt;div class="max-w-full overflow-auto"&gt;
	&lt;table&gt;
		&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Flag&lt;/th&gt;
&lt;th&gt;Purpose&lt;/th&gt;
&lt;/tr&gt;

		&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enforce-eager&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Disables CUDA graphs to save memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-model-len 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits context to fit in available memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-batched-tokens 256&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Matches the model length limit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--gpu-memory-utilization 0.65&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reserves headroom for system processes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--max-num-seqs 1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Single request at a time to minimize memory&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--enable-chunked-prefill&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Processes prefill in chunks for memory efficiency&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--limit-mm-per-prompt&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Limits to 1 image and 1 video per prompt&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--mm-processor-kwargs&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reduces video frames and image resolution&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;--VLLM_SKIP_WARMUP=true&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Skips warmup to save time and memory&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
	&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;Wait until you see the server is ready:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;INFO:     Uvicorn running on http://0.0.0.0:8000
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Verify the server is running
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;From another terminal on the Jetson:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl http://localhost:8000/v1/models
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see the model listed in the response.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 5: Test with a Quick API Call
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Before connecting the WebUI, verify the model responds correctly:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -s http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models/cosmos-reason2-2b",
    "messages": [
      {
        "role": "user",
        "content": "What capabilities do you have?"
      }
    ],
    "max_tokens": 128
  }' | python3 -m json.tool
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; The model name used in the API request must match what vLLM reports. Verify with &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Step 6: Connect to Live VLM WebUI
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;Live VLM WebUI provides a real-time webcam-to-VLM interface. With vLLM serving Cosmos Reasoning 2B, you can stream your webcam and get live AI analysis with reasoning.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Install Live VLM WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;The easiest method is pip (Open another terminal):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env
cd ~/Projects/CosmosReasoning
uv venv .live-vlm --python 3.12
source .live-vlm/bin/activate
uv pip install live-vlm-webui
live-vlm-webui
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or use Docker:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/nvidia-ai-iot/live-vlm-webui.git
cd live-vlm-webui
./scripts/start_container.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Configure the WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Open &lt;strong&gt;&lt;code&gt;https://localhost:8090&lt;/code&gt;&lt;/strong&gt; in your browser&lt;/li&gt;
&lt;li&gt;Accept the self-signed certificate (click &lt;strong&gt;Advanced&lt;/strong&gt; → &lt;strong&gt;Proceed&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;In the &lt;strong&gt;VLM API Configuration&lt;/strong&gt; section on the left sidebar:&lt;ul&gt;
&lt;li&gt;Set &lt;strong&gt;API Base URL&lt;/strong&gt; to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click the &lt;strong&gt;Refresh&lt;/strong&gt; button to detect the model&lt;/li&gt;
&lt;li&gt;Select the Cosmos Reasoning 2B model from the dropdown&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Select your camera and click &lt;strong&gt;Start&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The WebUI will now stream your webcam frames to Cosmos Reasoning 2B and display the model’s analysis in real-time.&lt;/p&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Recommended WebUI settings for Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;Since Orin runs with a shorter context length, adjust these settings in the WebUI:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Max Tokens&lt;/strong&gt;: Set to &lt;strong&gt;100–150&lt;/strong&gt; (shorter responses complete faster)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Frame Processing Interval&lt;/strong&gt;: Set to &lt;strong&gt;60+&lt;/strong&gt; (gives the model time between frames)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Troubleshooting
	&lt;/span&gt;
&lt;/h2&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Out of memory on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM crashes with CUDA out-of-memory errors.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Free system memory before starting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sysctl -w vm.drop_caches=3
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Lower &lt;code&gt;--gpu-memory-utilization&lt;/code&gt; (try &lt;code&gt;0.55&lt;/code&gt; or &lt;code&gt;0.50&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reduce &lt;code&gt;--max-model-len&lt;/code&gt; further (try &lt;code&gt;128&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Make sure no other GPU-intensive processes are running&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Model not found in WebUI
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; The model doesn’t appear in the Live VLM WebUI dropdown.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Verify vLLM is running: &lt;code&gt;curl http://localhost:8000/v1/models&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the WebUI API Base URL is set to &lt;code&gt;http://localhost:8000/v1&lt;/code&gt; (not &lt;code&gt;https&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;If vLLM and WebUI are in separate containers, use &lt;code&gt;http://&amp;lt;jetson-ip&amp;gt;:8000/v1&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Slow inference on Orin
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Each response takes a very long time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is expected with the memory-constrained configuration. Cosmos Reasoning 2B FP8 on Orin prioritizes fitting in memory over speed&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;max_tokens&lt;/code&gt; in the WebUI to get shorter, faster responses&lt;/li&gt;
&lt;li&gt;Increase the frame interval so the model isn’t constantly processing new frames&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		vLLM fails to load model
	&lt;/span&gt;
&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; vLLM reports that the model path doesn’t exist or can’t be loaded.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Verify the NGC download completed successfully: &lt;code&gt;ls ~/Projects/CosmosReasoning/cosmos-reason2-2b_v1208-fp8-static-kv8/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure the volume mount path is correct in your &lt;code&gt;docker run&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;Check that the model directory is mounted as read-only (&lt;code&gt;:ro&lt;/code&gt;) and the path inside the container matches what you pass to &lt;code&gt;vllm serve&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Summary
	&lt;/span&gt;
&lt;/h2&gt;
&lt;p&gt;In this tutorial, we showcased how to deploy &lt;strong&gt;NVIDIA Cosmos Reasoning 2B&lt;/strong&gt; model on Jetson family of devices using vLLM. &lt;/p&gt;


&lt;p&gt;The combination of Cosmos Reasoning 2B’s chain-of-thought capabilities with Live VLM WebUI’s real-time streaming makes it ideal to prototype and evaluate vision AI applications at the edge.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 class="relative group flex items-center"&gt;
	
		
	
	&lt;span&gt;
		Additional Resources
	&lt;/span&gt;
&lt;/h2&gt;

&lt;!-- HTML_TAG_END --&gt;</content:encoded><guid isPermaLink="false">https://huggingface.co/blog/nvidia/cosmos-on-jetson</guid><pubDate>Tue, 24 Feb 2026 00:00:21 +0000</pubDate></item><item><title>[NEW] A Meta AI security researcher said an OpenClaw agent ran amok on her inbox (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/02/23/a-meta-ai-security-researcher-said-an-openclaw-agent-ran-amok-on-her-inbox/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Y-Combinator-Crab.png?resize=1200,829" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The now-viral X post from Meta AI security researcher Summer Yue reads, at first, like satire. She told her OpenClaw AI agent to check her overstuffed email inbox and suggest what to delete or archive.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agent proceeded to run amok. It started deleting all her email in a “speed run” while ignoring her commands from her phone telling it to stop.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I had to RUN to my Mac mini like I was defusing a bomb,” she wrote, posting images of the ignored stop prompts as receipts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Mac Mini, an affordable Apple computer that sits flat on a desk and fits in the palm of your hand, has become the favored device these days for running OpenClaw. (The Mini is selling “like hotcakes,” one “confused” Apple employee apparently told famed AI researcher Andrej Karpathy when he bought one to run an OpenClaw alternative called NanoClaw.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenClaw is, of course, the open source AI agent that achieved fame through Moltbook, an AI-only social network. OpenClaw agents were at the center of that now largely debunked episode on Moltbook in which it looked like the AIs were plotting against humans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But OpenClaw’s mission, according to its GitHub page, is not focused on social networks. It aims to be a personal AI assistant that runs on your own devices.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Silicon Valley in-crowd has fallen so in love with OpenClaw that “claw” and “claws” have become the buzzwords of choice for agents that run on personal hardware. Other such agents include ZeroClaw, IronClaw, and PicoClaw. Y Combinator’s podcast team even appeared on their most recent episode dressed in lobster costumes.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Yue’s post serves as a warning. As others on X noted, if an AI security researcher could run into this problem, what hope do mere mortals have?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Were you intentionally testing its guardrails or did you make a rookie mistake?” a software developer asked her on X.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Rookie mistake tbh,” she replied. She had been testing her agent with a smaller “toy” inbox, as she called it, and it had been running well on less important email. It had earned her trust, so she thought she’d let it loose on the real thing.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Yue believes that the large amount of data in her real inbox “triggered compaction,” she wrote. Compaction happens when the context window — the running record of everything the AI has been told and has done in a session — grows too large, causing the agent to begin summarizing, compressing, and managing the conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At that point, the AI may skip over instructions that the human considers quite important.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, it may have skipped her last prompt — where she told it not to act — and reverted back to its instructions from the “toy” inbox.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As several others on X pointed out, prompts can’t be trusted to act as security guardrails. Models may misconstrue or ignore them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Various people offered suggestions that ranged from the exact syntax Yue should have used to stop the agent, to various methods to ensure better adherence to guardrails, like writing instructions to dedicated files or using other open source tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the interest of full transparency, TechCrunch could not independently verify what happened to Yue’s inbox. (She didn’t respond to our request for comment, though she did respond to many questions and comments sent her way on X.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it doesn’t really matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The point of the tale is that agents aimed at knowledge workers, at their current stage of development, are risky. People who say they are using them successfully are cobbling together methods to protect themselves.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One day, perhaps soon (by 2027? 2028?), they may be ready for widespread use. Goodness knows many of us would love help with email, grocery orders, and scheduling dentist appointments. But that day has not yet come.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/02/Y-Combinator-Crab.png?resize=1200,829" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The now-viral X post from Meta AI security researcher Summer Yue reads, at first, like satire. She told her OpenClaw AI agent to check her overstuffed email inbox and suggest what to delete or archive.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The agent proceeded to run amok. It started deleting all her email in a “speed run” while ignoring her commands from her phone telling it to stop.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“I had to RUN to my Mac mini like I was defusing a bomb,” she wrote, posting images of the ignored stop prompts as receipts.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Mac Mini, an affordable Apple computer that sits flat on a desk and fits in the palm of your hand, has become the favored device these days for running OpenClaw. (The Mini is selling “like hotcakes,” one “confused” Apple employee apparently told famed AI researcher Andrej Karpathy when he bought one to run an OpenClaw alternative called NanoClaw.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenClaw is, of course, the open source AI agent that achieved fame through Moltbook, an AI-only social network. OpenClaw agents were at the center of that now largely debunked episode on Moltbook in which it looked like the AIs were plotting against humans.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But OpenClaw’s mission, according to its GitHub page, is not focused on social networks. It aims to be a personal AI assistant that runs on your own devices.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Silicon Valley in-crowd has fallen so in love with OpenClaw that “claw” and “claws” have become the buzzwords of choice for agents that run on personal hardware. Other such agents include ZeroClaw, IronClaw, and PicoClaw. Y Combinator’s podcast team even appeared on their most recent episode dressed in lobster costumes.&amp;nbsp;&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;Boston, MA&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;June 9, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;But Yue’s post serves as a warning. As others on X noted, if an AI security researcher could run into this problem, what hope do mere mortals have?&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Were you intentionally testing its guardrails or did you make a rookie mistake?” a software developer asked her on X.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Rookie mistake tbh,” she replied. She had been testing her agent with a smaller “toy” inbox, as she called it, and it had been running well on less important email. It had earned her trust, so she thought she’d let it loose on the real thing.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Yue believes that the large amount of data in her real inbox “triggered compaction,” she wrote. Compaction happens when the context window — the running record of everything the AI has been told and has done in a session — grows too large, causing the agent to begin summarizing, compressing, and managing the conversation.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At that point, the AI may skip over instructions that the human considers quite important.&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In this case, it may have skipped her last prompt — where she told it not to act — and reverted back to its instructions from the “toy” inbox.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;As several others on X pointed out, prompts can’t be trusted to act as security guardrails. Models may misconstrue or ignore them.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Various people offered suggestions that ranged from the exact syntax Yue should have used to stop the agent, to various methods to ensure better adherence to guardrails, like writing instructions to dedicated files or using other open source tools.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In the interest of full transparency, TechCrunch could not independently verify what happened to Yue’s inbox. (She didn’t respond to our request for comment, though she did respond to many questions and comments sent her way on X.)&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But it doesn’t really matter.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The point of the tale is that agents aimed at knowledge workers, at their current stage of development, are risky. People who say they are using them successfully are cobbling together methods to protect themselves.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;One day, perhaps soon (by 2027? 2028?), they may be ready for widespread use. Goodness knows many of us would love help with email, grocery orders, and scheduling dentist appointments. But that day has not yet come.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/02/23/a-meta-ai-security-researcher-said-an-openclaw-agent-ran-amok-on-her-inbox/</guid><pubDate>Tue, 24 Feb 2026 00:57:14 +0000</pubDate></item></channel></rss>