<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 22 Nov 2025 01:41:32 +0000</lastBuildDate><item><title> ()</title><link>https://deepmind.com/blog/feed/basic/</link><description>[unable to retrieve full-text content]</description><content:encoded>[unable to retrieve full-text content]</content:encoded><guid isPermaLink="false">https://deepmind.com/blog/feed/basic/</guid></item><item><title>Reducing EV range anxiety: How a simple AI model predicts port availability (The latest research from Google)</title><link>https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Experiments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.&lt;/p&gt;&lt;p&gt;The model was benchmarked against a remarkably strong baseline: the "Keep Current State" approach. This baseline simply assumes that the number of available ports a certain number of minutes (&lt;i&gt;H&lt;/i&gt;) in the future will be exactly the same as the current number.&lt;/p&gt;&lt;p&gt;While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.&lt;/p&gt;&lt;p&gt;We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://storage.googleapis.com/gweb-research2023-media/images/Open_Graph.width-800.format-jpeg.jpg" /&gt;&lt;/div&gt;&lt;p&gt;
        
            
                &lt;h2 class="class"&gt;Experiments&lt;/h2&gt;
            
        
        
    &lt;/p&gt;



    &lt;p&gt;Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.&lt;/p&gt;&lt;p&gt;The model was benchmarked against a remarkably strong baseline: the "Keep Current State" approach. This baseline simply assumes that the number of available ports a certain number of minutes (&lt;i&gt;H&lt;/i&gt;) in the future will be exactly the same as the current number.&lt;/p&gt;&lt;p&gt;While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.&lt;/p&gt;&lt;p&gt;We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/</guid><pubDate>Fri, 21 Nov 2025 16:27:00 +0000</pubDate></item><item><title>[NEW] WorldGen: Meta reveals generative AI for interactive 3D worlds (AI News)</title><link>https://www.artificialintelligence-news.com/news/worldgen-meta-generative-ai-for-interactive-3d-worlds/</link><description>&lt;p&gt;With its WorldGen system, Meta is shifting the use of generative AI for 3D worlds from creating static imagery to fully interactive assets.&lt;/p&gt;&lt;p&gt;The main bottleneck in creating immersive spatial computing experiences – whether for consumer gaming, industrial digital twins, or employee training simulations – has long been the labour-intensive nature of 3D modelling. The production of an interactive environment typically requires teams of specialised artists working for weeks.&lt;/p&gt;&lt;p&gt;WorldGen, according to a new technical report from Meta’s Reality Labs, is capable of generating traversable and interactive 3D worlds from a single text prompt in approximately five minutes.&lt;/p&gt;&lt;p&gt;While the technology is currently research-grade, the WorldGen architecture addresses specific pain points that have prevented generative AI from being useful in professional workflows: functional interactivity, engine compatibility, and editorial control.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-generative-ai-environments-become-truly-interactive-3d-worlds"&gt;Generative AI environments become truly interactive 3D worlds&lt;/h3&gt;&lt;p&gt;The primary failing of many existing text-to-3D models is that they prioritise visual fidelity over function. Approaches such as gaussian splatting create photorealistic scenes that look impressive in a video but often lack the underlying physical structure required for a user to interact with the environment. Assets lacking collision data or ramp physics hold little-to-no value for simulation or gaming.&lt;/p&gt;&lt;p&gt;WorldGen diverges from this path by prioritising “traversability”. The system generates a navigation mesh (navmesh) – a simplified polygon mesh that defines walkable surfaces – alongside the visual geometry. This ensures that a prompt such as “medieval village” produces not just a collection of houses, but a spatially-coherent layout where streets are clear of obstructions and open spaces are accessible.&lt;/p&gt;&lt;p&gt;For enterprises, this distinction is vital. A digital twin of a factory floor or a safety training simulation for hazardous environments requires valid physics and navigation data.&lt;/p&gt;&lt;p&gt;Meta’s approach ensures the output is “game engine-ready,” meaning the assets can be exported directly into standard platforms like Unity or Unreal Engine. This compatibility allows technical teams to integrate generative workflows into existing pipelines without needing specialised rendering hardware that other methods, such as radiance fields, often demand.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-four-stage-production-line-of-worldgen"&gt;The four-stage production line of WorldGen&lt;/h3&gt;&lt;p&gt;Meta’s researchers have structured WorldGen as a modular AI pipeline that mirrors traditional development workflows for creating 3D worlds.&lt;/p&gt;&lt;p&gt;The process begins with scene planning. A LLM acts as a structural engineer, parsing the user’s text prompt to generate a logical layout. It determines the placement of key structures and terrain features, producing a “blockout” – a rough 3D sketch – that guarantees the scene makes physical sense.&lt;/p&gt;&lt;p&gt;The subsequent “scene reconstruction” phase builds the initial geometry. The system conditions the generation on the navmesh, ensuring that as the AI “hallucinates” details, it does not inadvertently place a boulder in a doorway or block a fire exit path.&lt;/p&gt;&lt;p&gt;“Scene decomposition,” the third stage, is perhaps the most relevant for operational flexibility. The system uses a method called AutoPartGen to identify and separate individual objects within the scene—distinguishing a tree from the ground, or a crate from a warehouse floor.&lt;/p&gt;&lt;p&gt;In many “single-shot” generative models, the scene is a single fused lump of geometry. By separating components, WorldGen allows human editors to move, delete, or modify specific assets post-generation without breaking the entire world.&lt;/p&gt;&lt;p&gt;For the last step, “scene enhancement” polishes the assets. The system generates high-resolution textures and refines the geometry of individual objects to ensure visual quality holds up when close.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Screenshot of Meta WorldGen in action for using generative AI to create 3D worlds." class="wp-image-110828" height="582" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/Screenshot-of-Meta-WorldGen-in-action-for-generative-ai-3d-worlds-1024x582.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-operational-realism-of-using-generative-ai-to-create-3d-worlds"&gt;Operational realism of using generative AI to create 3D worlds&lt;/h3&gt;&lt;p&gt;Implementing such technology requires an assessment of current infrastructure. WorldGen’s outputs are standard textured meshes. This choice avoids the vendor lock-in associated with proprietary rendering techniques. It means that a logistics firm building a VR training module could theoretically use this tool to prototype layouts rapidly, then hand them over to human developers for refinement.&lt;/p&gt;&lt;p&gt;Creating a fully textured, navigable scene takes roughly five minutes on sufficient hardware. For studios or departments accustomed to multi-day turnaround times for basic environment blocking, this efficiency gain is quite literally world-changing.&lt;/p&gt;&lt;p&gt;However, the technology does have limitations. The current iteration relies on generating a single reference view, which restricts the scale of the worlds it can produce. It cannot yet natively generate sprawling open worlds spanning kilometres without stitching multiple regions together, which risks visual inconsistencies.&lt;/p&gt;&lt;p&gt;The system also currently represents each object independently without reuse, which could lead to memory inefficiencies in very large scenes compared to hand-optimised assets where a single chair model is repeated fifty times. Future iterations aim to address larger world sizes and lower latency.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-comparing-worldgen-against-other-emerging-technologies"&gt;Comparing WorldGen against other emerging technologies&lt;/h3&gt;&lt;p&gt;Evaluating this approach against other emerging AI technologies for creating 3D worlds offers clarity. World Labs, a competitor in the space, employs a system called Marble that uses Gaussian splats to achieve high photorealism. While visually striking, these splat-based scenes often degrade in quality when the camera moves away from the centre and can drop in fidelity just 3-5 metres from the viewpoint.&lt;/p&gt;&lt;p&gt;Meta’s choice to output mesh-based geometry positions WorldGen as a tool for functional application development rather than just visual content creation. It supports physics, collisions, and navigation natively—features that are non-negotiable for interactive software. Consequently, WorldGen can generate scenes spanning 50×50 metres that maintain geometric integrity throughout.&lt;/p&gt;&lt;p&gt;For leaders in the technology and creative sectors, the arrival of systems like WorldGen brings exciting new possibilities. Organisations should audit their current 3D workflows to identify where “blockout” and prototyping absorb the most resources. Generative tools are best deployed here to accelerate iteration, rather than attempting to replace final-quality production immediately.&lt;/p&gt;&lt;p&gt;Concurrently, technical artists and level designers will need to transition from placing every vertex manually to prompting and curating AI outputs. Training programmes should focus on “prompt engineering for spatial layout” and editing AI-generated assets for 3D worlds. Finally, while the output is standard, the generation process requires plenty of compute. Assessing on-premise versus cloud rendering capabilities will be necessary for adoption.&lt;/p&gt;&lt;p&gt;Generative 3D serves best as a force multiplier for structural layout and asset population rather than a total replacement for human creativity. By automating the foundational work of building a world, enterprise teams can focus their budgets on the interactions and logic that drive business value.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;How the Royal Navy is using AI to cut its recruitment workload&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110825" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-11.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;With its WorldGen system, Meta is shifting the use of generative AI for 3D worlds from creating static imagery to fully interactive assets.&lt;/p&gt;&lt;p&gt;The main bottleneck in creating immersive spatial computing experiences – whether for consumer gaming, industrial digital twins, or employee training simulations – has long been the labour-intensive nature of 3D modelling. The production of an interactive environment typically requires teams of specialised artists working for weeks.&lt;/p&gt;&lt;p&gt;WorldGen, according to a new technical report from Meta’s Reality Labs, is capable of generating traversable and interactive 3D worlds from a single text prompt in approximately five minutes.&lt;/p&gt;&lt;p&gt;While the technology is currently research-grade, the WorldGen architecture addresses specific pain points that have prevented generative AI from being useful in professional workflows: functional interactivity, engine compatibility, and editorial control.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-generative-ai-environments-become-truly-interactive-3d-worlds"&gt;Generative AI environments become truly interactive 3D worlds&lt;/h3&gt;&lt;p&gt;The primary failing of many existing text-to-3D models is that they prioritise visual fidelity over function. Approaches such as gaussian splatting create photorealistic scenes that look impressive in a video but often lack the underlying physical structure required for a user to interact with the environment. Assets lacking collision data or ramp physics hold little-to-no value for simulation or gaming.&lt;/p&gt;&lt;p&gt;WorldGen diverges from this path by prioritising “traversability”. The system generates a navigation mesh (navmesh) – a simplified polygon mesh that defines walkable surfaces – alongside the visual geometry. This ensures that a prompt such as “medieval village” produces not just a collection of houses, but a spatially-coherent layout where streets are clear of obstructions and open spaces are accessible.&lt;/p&gt;&lt;p&gt;For enterprises, this distinction is vital. A digital twin of a factory floor or a safety training simulation for hazardous environments requires valid physics and navigation data.&lt;/p&gt;&lt;p&gt;Meta’s approach ensures the output is “game engine-ready,” meaning the assets can be exported directly into standard platforms like Unity or Unreal Engine. This compatibility allows technical teams to integrate generative workflows into existing pipelines without needing specialised rendering hardware that other methods, such as radiance fields, often demand.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-the-four-stage-production-line-of-worldgen"&gt;The four-stage production line of WorldGen&lt;/h3&gt;&lt;p&gt;Meta’s researchers have structured WorldGen as a modular AI pipeline that mirrors traditional development workflows for creating 3D worlds.&lt;/p&gt;&lt;p&gt;The process begins with scene planning. A LLM acts as a structural engineer, parsing the user’s text prompt to generate a logical layout. It determines the placement of key structures and terrain features, producing a “blockout” – a rough 3D sketch – that guarantees the scene makes physical sense.&lt;/p&gt;&lt;p&gt;The subsequent “scene reconstruction” phase builds the initial geometry. The system conditions the generation on the navmesh, ensuring that as the AI “hallucinates” details, it does not inadvertently place a boulder in a doorway or block a fire exit path.&lt;/p&gt;&lt;p&gt;“Scene decomposition,” the third stage, is perhaps the most relevant for operational flexibility. The system uses a method called AutoPartGen to identify and separate individual objects within the scene—distinguishing a tree from the ground, or a crate from a warehouse floor.&lt;/p&gt;&lt;p&gt;In many “single-shot” generative models, the scene is a single fused lump of geometry. By separating components, WorldGen allows human editors to move, delete, or modify specific assets post-generation without breaking the entire world.&lt;/p&gt;&lt;p&gt;For the last step, “scene enhancement” polishes the assets. The system generates high-resolution textures and refines the geometry of individual objects to ensure visual quality holds up when close.&lt;/p&gt;&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Screenshot of Meta WorldGen in action for using generative AI to create 3D worlds." class="wp-image-110828" height="582" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/Screenshot-of-Meta-WorldGen-in-action-for-generative-ai-3d-worlds-1024x582.jpg" width="1024" /&gt;&lt;/figure&gt;&lt;h3 class="wp-block-heading" id="h-operational-realism-of-using-generative-ai-to-create-3d-worlds"&gt;Operational realism of using generative AI to create 3D worlds&lt;/h3&gt;&lt;p&gt;Implementing such technology requires an assessment of current infrastructure. WorldGen’s outputs are standard textured meshes. This choice avoids the vendor lock-in associated with proprietary rendering techniques. It means that a logistics firm building a VR training module could theoretically use this tool to prototype layouts rapidly, then hand them over to human developers for refinement.&lt;/p&gt;&lt;p&gt;Creating a fully textured, navigable scene takes roughly five minutes on sufficient hardware. For studios or departments accustomed to multi-day turnaround times for basic environment blocking, this efficiency gain is quite literally world-changing.&lt;/p&gt;&lt;p&gt;However, the technology does have limitations. The current iteration relies on generating a single reference view, which restricts the scale of the worlds it can produce. It cannot yet natively generate sprawling open worlds spanning kilometres without stitching multiple regions together, which risks visual inconsistencies.&lt;/p&gt;&lt;p&gt;The system also currently represents each object independently without reuse, which could lead to memory inefficiencies in very large scenes compared to hand-optimised assets where a single chair model is repeated fifty times. Future iterations aim to address larger world sizes and lower latency.&lt;/p&gt;&lt;h3 class="wp-block-heading" id="h-comparing-worldgen-against-other-emerging-technologies"&gt;Comparing WorldGen against other emerging technologies&lt;/h3&gt;&lt;p&gt;Evaluating this approach against other emerging AI technologies for creating 3D worlds offers clarity. World Labs, a competitor in the space, employs a system called Marble that uses Gaussian splats to achieve high photorealism. While visually striking, these splat-based scenes often degrade in quality when the camera moves away from the centre and can drop in fidelity just 3-5 metres from the viewpoint.&lt;/p&gt;&lt;p&gt;Meta’s choice to output mesh-based geometry positions WorldGen as a tool for functional application development rather than just visual content creation. It supports physics, collisions, and navigation natively—features that are non-negotiable for interactive software. Consequently, WorldGen can generate scenes spanning 50×50 metres that maintain geometric integrity throughout.&lt;/p&gt;&lt;p&gt;For leaders in the technology and creative sectors, the arrival of systems like WorldGen brings exciting new possibilities. Organisations should audit their current 3D workflows to identify where “blockout” and prototyping absorb the most resources. Generative tools are best deployed here to accelerate iteration, rather than attempting to replace final-quality production immediately.&lt;/p&gt;&lt;p&gt;Concurrently, technical artists and level designers will need to transition from placing every vertex manually to prompting and curating AI outputs. Training programmes should focus on “prompt engineering for spatial layout” and editing AI-generated assets for 3D worlds. Finally, while the output is standard, the generation process requires plenty of compute. Assessing on-premise versus cloud rendering capabilities will be necessary for adoption.&lt;/p&gt;&lt;p&gt;Generative 3D serves best as a force multiplier for structural layout and asset population rather than a total replacement for human creativity. By automating the foundational work of building a world, enterprise teams can focus their budgets on the interactions and logic that drive business value.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;How the Royal Navy is using AI to cut its recruitment workload&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-110825" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/11/image-11.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo. Click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/worldgen-meta-generative-ai-for-interactive-3d-worlds/</guid><pubDate>Fri, 21 Nov 2025 16:35:32 +0000</pubDate></item><item><title>[NEW] The hottest AI wearables and gadgets you can buy right now (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/21/the-hottest-ai-wearables-and-gadgets-you-can-buy-right-now/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new wave of AI-powered gadgets on the market aims to integrate artificial intelligence into our daily lives like never before.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of these AI wearables — including necklaces, rings, and wristbands, as well as portable devices — serve as productivity tools, while others claim to act as friendly companions listening to your everyday thoughts.&amp;nbsp;Even OpenAI is working on a compact AI companion device.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Below, we’ve rounded up some of the most notable devices currently available.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-bee"&gt;Bee&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2816039" height="383" src="https://techcrunch.com/wp-content/uploads/2024/07/HD.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bee AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Bee is an affordable pendant priced at $49.99 that can either be clipped to your clothing or worn like a fitness band. This device records everything it hears and learns your routines and preferences to create reminders and notes for you. It even features a mute button for those times you want some privacy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companion app (currently available only on iOS) is included with a $19 monthly subscription. The app allows you to interact with Bee directly and ask it questions. You can also get key takeaways from your day and chronological transcripts of your conversations.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon recently acquired the wearables startup in July.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-friend"&gt;Friend&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2817169" height="535" src="https://techcrunch.com/wp-content/uploads/2024/07/Friend.jpg?w=523" width="523" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Friend&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Friend is one of the most hyped entrants in the “personal AI” device market.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This $129 white pendant hangs around your neck and functions as an emotional support companion. It recognizes your tone and mood, allowing you to chat with it as if it were a friend. It connects to your phone via Bluetooth and constantly listens, ready to respond or send you proactive messages, like wishing you good luck before an interview.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it has also faced criticism, including a recent backlash against its subway ad campaign in NYC. People vandalized the ads, writing messages like “surveillance capitalism.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-limitless"&gt;Limitless &lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="8 Limitless pendants in various colors" class="wp-image-2692658" height="383" src="https://techcrunch.com/wp-content/uploads/2024/04/Limitless-pendant.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Limitless&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Formerly known as Rewind, Limitless is another conversation-recording pendant priced at $99.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This device continuously listens, transcribing meetings, calls, and conversations (with consent) into searchable and summarized knowledge. It’s ideal for professionals, especially journalists, looking to recall important discussions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companion app comes with 10 hours of AI features per month — such as transcription and summaries — with the option to unlock unlimited features for $29 per month.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-omi"&gt;Omi&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A still from a promotional video for Omi" class="wp-image-2942128" height="383" src="https://techcrunch.com/wp-content/uploads/2025/01/omi-head.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Omi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Priced at $89, this device can answer your questions, summarize your conversations, create to-do lists, and help schedule meetings. Additionally, Omi is constantly listening and running your conversations through ChatGPT, allowing it to remember the context about you and offer personalized advice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Omi can be worn as a necklace, but another notable aspect is that it can be attached to the side of your head with medical tape and can detect when you’re speaking to it.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-plaud-s-notepin"&gt;Plaud’s NotePin&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Plaud's NotePin hanging around a person's neck" class="wp-image-2846232" height="383" src="https://techcrunch.com/wp-content/uploads/2024/08/YouTube-Thumb-Text-4-6.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Plaud&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At $159, Plaud’s NotePin is one of the pricier options on this list; however, its built-in AI transcription and summarization features make it a valuable tool for lawyers, journalists, and students attending meetings or lectures.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tiny wearable voice recorder can be worn on your wrist or attached magnetically to your clothing. The recordings are saved in real time on your phone, eliminating the hassle of manual note-taking. The device includes 300 free monthly transcription minutes, but with the $8.33 per month Pro plan, you upgrade your transcription time to 1,200 minutes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the company is gearing up to launch a $179 ultra-thin note-taking device called the Plaud Note Pro, which is now available for preorder.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-rabbit-r1"&gt;Rabbit R1&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2649370" height="484" src="https://techcrunch.com/wp-content/uploads/2024/01/rabbit_r1_front.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Rabbit&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rabbit R1 is another AI gadget that has quickly become a topic of interest in the tech world, despite facing some challenges during its initial launch. This small, retro-styled handheld device features a touchscreen and rotating camera, at a price point of $199.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The device is designed to be phone-adjacent, allowing you to perform tasks such as booking flights, ordering meals, and controlling apps without needing to pull out your phone each time. Following a crucial software update that rectified previous performance issues, the Rabbit R1 now boasts expanded AI features. For instance, it introduces “Creations,” a feature that allows you to build your own tools and even games.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;A new wave of AI-powered gadgets on the market aims to integrate artificial intelligence into our daily lives like never before.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Some of these AI wearables — including necklaces, rings, and wristbands, as well as portable devices — serve as productivity tools, while others claim to act as friendly companions listening to your everyday thoughts.&amp;nbsp;Even OpenAI is working on a compact AI companion device.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Below, we’ve rounded up some of the most notable devices currently available.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-bee"&gt;Bee&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2816039" height="383" src="https://techcrunch.com/wp-content/uploads/2024/07/HD.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Bee AI&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Bee is an affordable pendant priced at $49.99 that can either be clipped to your clothing or worn like a fitness band. This device records everything it hears and learns your routines and preferences to create reminders and notes for you. It even features a mute button for those times you want some privacy.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companion app (currently available only on iOS) is included with a $19 monthly subscription. The app allows you to interact with Bee directly and ask it questions. You can also get key takeaways from your day and chronological transcripts of your conversations.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Amazon recently acquired the wearables startup in July.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-friend"&gt;Friend&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2817169" height="535" src="https://techcrunch.com/wp-content/uploads/2024/07/Friend.jpg?w=523" width="523" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Friend&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Friend is one of the most hyped entrants in the “personal AI” device market.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This $129 white pendant hangs around your neck and functions as an emotional support companion. It recognizes your tone and mood, allowing you to chat with it as if it were a friend. It connects to your phone via Bluetooth and constantly listens, ready to respond or send you proactive messages, like wishing you good luck before an interview.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;However, it has also faced criticism, including a recent backlash against its subway ad campaign in NYC. People vandalized the ads, writing messages like “surveillance capitalism.”&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-limitless"&gt;Limitless &lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="8 Limitless pendants in various colors" class="wp-image-2692658" height="383" src="https://techcrunch.com/wp-content/uploads/2024/04/Limitless-pendant.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Limitless&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Formerly known as Rewind, Limitless is another conversation-recording pendant priced at $99.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;This device continuously listens, transcribing meetings, calls, and conversations (with consent) into searchable and summarized knowledge. It’s ideal for professionals, especially journalists, looking to recall important discussions.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The companion app comes with 10 hours of AI features per month — such as transcription and summaries — with the option to unlock unlimited features for $29 per month.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-omi"&gt;Omi&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="A still from a promotional video for Omi" class="wp-image-2942128" height="383" src="https://techcrunch.com/wp-content/uploads/2025/01/omi-head.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Omi&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Priced at $89, this device can answer your questions, summarize your conversations, create to-do lists, and help schedule meetings. Additionally, Omi is constantly listening and running your conversations through ChatGPT, allowing it to remember the context about you and offer personalized advice.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Omi can be worn as a necklace, but another notable aspect is that it can be attached to the side of your head with medical tape and can detect when you’re speaking to it.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-plaud-s-notepin"&gt;Plaud’s NotePin&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Plaud's NotePin hanging around a person's neck" class="wp-image-2846232" height="383" src="https://techcrunch.com/wp-content/uploads/2024/08/YouTube-Thumb-Text-4-6.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Plaud&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;At $159, Plaud’s NotePin is one of the pricier options on this list; however, its built-in AI transcription and summarization features make it a valuable tool for lawyers, journalists, and students attending meetings or lectures.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The tiny wearable voice recorder can be worn on your wrist or attached magnetically to your clothing. The recordings are saved in real time on your phone, eliminating the hassle of manual note-taking. The device includes 300 free monthly transcription minutes, but with the $8.33 per month Pro plan, you upgrade your transcription time to 1,200 minutes.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;This year, the company is gearing up to launch a $179 ultra-thin note-taking device called the Plaud Note Pro, which is now available for preorder.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-rabbit-r1"&gt;Rabbit R1&lt;/h2&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-2649370" height="484" src="https://techcrunch.com/wp-content/uploads/2024/01/rabbit_r1_front.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Rabbit&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Rabbit R1 is another AI gadget that has quickly become a topic of interest in the tech world, despite facing some challenges during its initial launch. This small, retro-styled handheld device features a touchscreen and rotating camera, at a price point of $199.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The device is designed to be phone-adjacent, allowing you to perform tasks such as booking flights, ordering meals, and controlling apps without needing to pull out your phone each time. Following a crucial software update that rectified previous performance issues, the Rabbit R1 now boasts expanded AI features. For instance, it introduces “Creations,” a feature that allows you to build your own tools and even games.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/21/the-hottest-ai-wearables-and-gadgets-you-can-buy-right-now/</guid><pubDate>Fri, 21 Nov 2025 20:00:00 +0000</pubDate></item><item><title>[NEW] AI mania is making Nvidia a lot of money (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/podcast/ai-mania-is-making-nvidia-a-lot-of-money/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/jensen-nvidia-ai-gtc-2024.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI companies are spending so much on infrastructure that Nvidia’s data center business now brings in nearly $50 billion. But is this sustainable growth or just the latest tech&amp;nbsp;mania? And should we even be calling it a “bubble” when the belief in AI’s future is&amp;nbsp;what’s&amp;nbsp;holding the whole ecosystem together?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;This week on Equity, Kirsten Korosec, Anthony Ha, and Sean O’Kane dig into Nvidia’s massive earnings beat, the circular economy of AI infrastructure spending, and whether CEO Jensen Huang’s optimistic vision of AI agents handling everything in our daily lives can justify the investment.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



















&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/03/jensen-nvidia-ai-gtc-2024.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;AI companies are spending so much on infrastructure that Nvidia’s data center business now brings in nearly $50 billion. But is this sustainable growth or just the latest tech&amp;nbsp;mania? And should we even be calling it a “bubble” when the belief in AI’s future is&amp;nbsp;what’s&amp;nbsp;holding the whole ecosystem together?&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;This week on Equity, Kirsten Korosec, Anthony Ha, and Sean O’Kane dig into Nvidia’s massive earnings beat, the circular economy of AI infrastructure spending, and whether CEO Jensen Huang’s optimistic vision of AI agents handling everything in our daily lives can justify the investment.&amp;nbsp;&lt;/p&gt;



&lt;p class="wp-block-paragraph"&gt;Listen to the full episode to hear about:&amp;nbsp;&lt;/p&gt;



















&lt;p class="wp-block-paragraph"&gt;Subscribe to Equity on&amp;nbsp;Apple Podcasts,&amp;nbsp;Overcast,&amp;nbsp;Spotify&amp;nbsp;and all the casts. You&amp;nbsp;also can&amp;nbsp;follow Equity on X and Threads, at @EquityPod.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/podcast/ai-mania-is-making-nvidia-a-lot-of-money/</guid><pubDate>Fri, 21 Nov 2025 20:04:26 +0000</pubDate></item><item><title>[NEW] AI trained on bacterial genomes produces never-before-seen proteins (AI – Ars Technica)</title><link>https://arstechnica.com/science/2025/11/generative-ai-meets-the-genome/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Genes with related functions cluster together, and the AI uses that.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="a blueish grey background with a single, thick, multicolored ribbon in the foreground. The ribbon traces a complicated 3D shape that's meant to represent the structure of a protein." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2216287507-640x360.jpg" width="640" /&gt;
                  &lt;img alt="a blueish grey background with a single, thick, multicolored ribbon in the foreground. The ribbon traces a complicated 3D shape that's meant to represent the structure of a protein." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2216287507-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          CHRISTOPH BURGSTEDT/SCIENCE PHOTO LIBRARY

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;AI systems have recently had a lot of success in one key aspect of biology: the relationship between a protein’s structure and its function. These efforts have included the ability to predict the structure of most proteins and to design proteins structured so that they perform useful functions. But all of these efforts are focused on the proteins and amino acids that build them.&lt;/p&gt;
&lt;p&gt;But biology doesn’t generate new proteins at that level. Instead, changes have to take place at the nucleic acid level before eventually making their presence felt at the protein level. And the DNA level is fairly removed from proteins, with lots of critical non-coding sequences, redundancy, and a fair degree of flexibility. It’s not necessarily obvious that learning the organization of a genome would help an AI system figure out how to make functional proteins.&lt;/p&gt;
&lt;p&gt;But it now seems like using bacterial genomes for the training can help develop a system that can predict proteins, some of which don’t look like anything we’ve ever seen before.&lt;/p&gt;
&lt;h2&gt;Training a genome model&lt;/h2&gt;
&lt;p&gt;The new work was done by a small team at Stanford University. It relies on a feature that’s common in bacterial genomes: the clustering of genes with related functions. Often, bacteria have all the genes needed for a given function—importing and digesting a sugar, synthesizing an amino acid, etc.—right next to each other in the genome. In many cases, all the genes are transcribed into a single, large messenger RNA. This gives the bacteria a simple way to control the activity of entire biochemical pathways at once, boosting the efficiency of bacterial metabolisms.&lt;/p&gt;
&lt;p&gt;So, the researchers developed what they term a “genomic language model” they call Evo on an enormous collection of bacterial genomes. The training was similar to what you’d see in a large language model, where Evo was asked to output predictions of the next base in a sequence, and rewarded when it got it right. It’s also a generative model, in that it can take a prompt and output novel sequences with a degree of randomness, in the sense that the same prompt can produce a range of different outputs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The researchers argue that this setup lets Evo “link nucleotide-level patterns to kilobase-scale genomic context.” In other words, if you prompt it with a large chunk of genomic DNA, Evo can interpret that as an LLM would interpret a query and produce an output that, in a genomic sense, is appropriate for that interpretation.&lt;/p&gt;
&lt;p&gt;The researchers reasoned that, given the training on bacterial genomes, they could use a known gene as a prompt, and Evo should produce an output that includes regions that encode proteins with related functions. The key question is whether it would simply output the sequences for proteins we know about already, or whether it would come up with output that’s less predictable.&lt;/p&gt;
&lt;h2&gt;Novel proteins&lt;/h2&gt;
&lt;p&gt;To start testing the system, the researchers prompted it with fragments of the genes for known proteins and determined whether Evo could complete them. In one example, if given 30 percent of the sequence of a gene for a known protein, Evo was able to output 85 percent of the rest. When prompted with 80 percent of the sequence, it could return all of the missing sequence. When a single gene was deleted from a functional cluster, Evo could also correctly identify and restore the missing gene.&lt;/p&gt;
&lt;p&gt;The large amount of training data also ensured that Evo correctly identified the most important regions of the protein. If it made changes to the sequence, they typically resided in the areas of the protein where variability is tolerated. In other words, its training had enabled the system to incorporate the rules of evolutionary limits on changes in known genes.&lt;/p&gt;
&lt;p&gt;So, the researchers decided to test what happened when Evo was asked to output something new. To do so, they used bacterial toxins, which are typically encoded along with an anti-toxin that keeps the cell from killing itself whenever it activates the genes. There are a lot of examples of these out there, and they tend to evolve rapidly as part of an arms race between bacteria and their competitors. So, the team developed a toxin that was only mildly related to known ones, and had no known antitoxin, and fed its sequence to Evo as a prompt. And this time, they filtered out any responses that looked similar to known antitoxin genes.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Testing 10 of the outputs returned by Evo, they found half were able to rescue some toxicity, and two of them fully restored growth to bacteria that were producing the toxin. These two antitoxins had only extremely weak similarity to known anti-toxins, at about 25 percent sequence identity. And they weren’t simply formed by pasting together a handful of pieces of known anti-toxins; at a minimum, they appeared to be assembled from parts of 15 to 20 individual proteins. In an additional test, the output would have been needed to have been patched together from parts of 40 known proteins.&lt;/p&gt;
&lt;p&gt;Evo’s success wasn’t limited to proteins. When they tested a different toxin that had an RNA-based inhibitor, the system could output DNA that encodes RNAs with the right structural features, even if the specific sequence wasn’t closely related to anything known.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Completely new proteins&lt;/h2&gt;
&lt;p&gt;The team performed a similar test with inhibitors of the CRISPR system, which we use for gene editing, but bacteria evolved as a form of protection from viruses. The naturally occurring CRISPR inhibitors are very diverse, with many of them seemingly unrelated to each other. Once again, the team filtered the outputs to only include those that encoded proteins and filtered out any of those proteins that looked like something we already knew about. Of the list of outputs they made proteins from, 17 managed to inhibit CRISPR function. Two of those were distinctive in that they had no similarity to any known proteins and confused software that is designed to predict the three-dimensional structure of proteins.&lt;/p&gt;
&lt;p&gt;In other words, along with the sorts of outputs you’d expect, Evo appears to be capable of outputting entirely new yet functional proteins. And it seems to do so without taking any consideration of the structure of the protein into account.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Given that their system appears to work, the researchers decided to prompt it with just about everything: 1.7 million individual genes from bacteria and the viruses that prey on them. The result is 120 billion base pairs of AI-generated DNA, some of it containing genes we already knew about, some of it presumably containing truly novel stuff. It’s not clear to me how anyone would productively use this resource, but I’d imagine there are some creative biologists who will think of something.&lt;/p&gt;
&lt;p&gt;It’s not clear that this approach will work with more complex genomes, like the one we’ve got. Organisms like vertebrates mostly don’t cluster genes with related functions, and their genes have far more intricate structures that might confuse a system that’s trying to learn the statistical rules of base frequencies. And, to be clear, it solves different problems from the sort of directed design efforts that have developed enzymes that do useful things like digesting plastics.&lt;/p&gt;
&lt;p&gt;That said, it’s still kind of amazing that this works at all. And conceptually, it’s intriguing because it brings the issue of finding functional proteins down to the nucleic acid level, where evolution normally does its thing.&lt;/p&gt;
&lt;p&gt;Nature, 2025. DOI: 10.1038/s41586-025-09749-7 &amp;nbsp;(About DOIs).&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Genes with related functions cluster together, and the AI uses that.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="a blueish grey background with a single, thick, multicolored ribbon in the foreground. The ribbon traces a complicated 3D shape that's meant to represent the structure of a protein." class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2216287507-640x360.jpg" width="640" /&gt;
                  &lt;img alt="a blueish grey background with a single, thick, multicolored ribbon in the foreground. The ribbon traces a complicated 3D shape that's meant to represent the structure of a protein." class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/GettyImages-2216287507-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          CHRISTOPH BURGSTEDT/SCIENCE PHOTO LIBRARY

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;AI systems have recently had a lot of success in one key aspect of biology: the relationship between a protein’s structure and its function. These efforts have included the ability to predict the structure of most proteins and to design proteins structured so that they perform useful functions. But all of these efforts are focused on the proteins and amino acids that build them.&lt;/p&gt;
&lt;p&gt;But biology doesn’t generate new proteins at that level. Instead, changes have to take place at the nucleic acid level before eventually making their presence felt at the protein level. And the DNA level is fairly removed from proteins, with lots of critical non-coding sequences, redundancy, and a fair degree of flexibility. It’s not necessarily obvious that learning the organization of a genome would help an AI system figure out how to make functional proteins.&lt;/p&gt;
&lt;p&gt;But it now seems like using bacterial genomes for the training can help develop a system that can predict proteins, some of which don’t look like anything we’ve ever seen before.&lt;/p&gt;
&lt;h2&gt;Training a genome model&lt;/h2&gt;
&lt;p&gt;The new work was done by a small team at Stanford University. It relies on a feature that’s common in bacterial genomes: the clustering of genes with related functions. Often, bacteria have all the genes needed for a given function—importing and digesting a sugar, synthesizing an amino acid, etc.—right next to each other in the genome. In many cases, all the genes are transcribed into a single, large messenger RNA. This gives the bacteria a simple way to control the activity of entire biochemical pathways at once, boosting the efficiency of bacterial metabolisms.&lt;/p&gt;
&lt;p&gt;So, the researchers developed what they term a “genomic language model” they call Evo on an enormous collection of bacterial genomes. The training was similar to what you’d see in a large language model, where Evo was asked to output predictions of the next base in a sequence, and rewarded when it got it right. It’s also a generative model, in that it can take a prompt and output novel sequences with a degree of randomness, in the sense that the same prompt can produce a range of different outputs.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;The researchers argue that this setup lets Evo “link nucleotide-level patterns to kilobase-scale genomic context.” In other words, if you prompt it with a large chunk of genomic DNA, Evo can interpret that as an LLM would interpret a query and produce an output that, in a genomic sense, is appropriate for that interpretation.&lt;/p&gt;
&lt;p&gt;The researchers reasoned that, given the training on bacterial genomes, they could use a known gene as a prompt, and Evo should produce an output that includes regions that encode proteins with related functions. The key question is whether it would simply output the sequences for proteins we know about already, or whether it would come up with output that’s less predictable.&lt;/p&gt;
&lt;h2&gt;Novel proteins&lt;/h2&gt;
&lt;p&gt;To start testing the system, the researchers prompted it with fragments of the genes for known proteins and determined whether Evo could complete them. In one example, if given 30 percent of the sequence of a gene for a known protein, Evo was able to output 85 percent of the rest. When prompted with 80 percent of the sequence, it could return all of the missing sequence. When a single gene was deleted from a functional cluster, Evo could also correctly identify and restore the missing gene.&lt;/p&gt;
&lt;p&gt;The large amount of training data also ensured that Evo correctly identified the most important regions of the protein. If it made changes to the sequence, they typically resided in the areas of the protein where variability is tolerated. In other words, its training had enabled the system to incorporate the rules of evolutionary limits on changes in known genes.&lt;/p&gt;
&lt;p&gt;So, the researchers decided to test what happened when Evo was asked to output something new. To do so, they used bacterial toxins, which are typically encoded along with an anti-toxin that keeps the cell from killing itself whenever it activates the genes. There are a lot of examples of these out there, and they tend to evolve rapidly as part of an arms race between bacteria and their competitors. So, the team developed a toxin that was only mildly related to known ones, and had no known antitoxin, and fed its sequence to Evo as a prompt. And this time, they filtered out any responses that looked similar to known antitoxin genes.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Testing 10 of the outputs returned by Evo, they found half were able to rescue some toxicity, and two of them fully restored growth to bacteria that were producing the toxin. These two antitoxins had only extremely weak similarity to known anti-toxins, at about 25 percent sequence identity. And they weren’t simply formed by pasting together a handful of pieces of known anti-toxins; at a minimum, they appeared to be assembled from parts of 15 to 20 individual proteins. In an additional test, the output would have been needed to have been patched together from parts of 40 known proteins.&lt;/p&gt;
&lt;p&gt;Evo’s success wasn’t limited to proteins. When they tested a different toxin that had an RNA-based inhibitor, the system could output DNA that encodes RNAs with the right structural features, even if the specific sequence wasn’t closely related to anything known.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;Completely new proteins&lt;/h2&gt;
&lt;p&gt;The team performed a similar test with inhibitors of the CRISPR system, which we use for gene editing, but bacteria evolved as a form of protection from viruses. The naturally occurring CRISPR inhibitors are very diverse, with many of them seemingly unrelated to each other. Once again, the team filtered the outputs to only include those that encoded proteins and filtered out any of those proteins that looked like something we already knew about. Of the list of outputs they made proteins from, 17 managed to inhibit CRISPR function. Two of those were distinctive in that they had no similarity to any known proteins and confused software that is designed to predict the three-dimensional structure of proteins.&lt;/p&gt;
&lt;p&gt;In other words, along with the sorts of outputs you’d expect, Evo appears to be capable of outputting entirely new yet functional proteins. And it seems to do so without taking any consideration of the structure of the protein into account.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Given that their system appears to work, the researchers decided to prompt it with just about everything: 1.7 million individual genes from bacteria and the viruses that prey on them. The result is 120 billion base pairs of AI-generated DNA, some of it containing genes we already knew about, some of it presumably containing truly novel stuff. It’s not clear to me how anyone would productively use this resource, but I’d imagine there are some creative biologists who will think of something.&lt;/p&gt;
&lt;p&gt;It’s not clear that this approach will work with more complex genomes, like the one we’ve got. Organisms like vertebrates mostly don’t cluster genes with related functions, and their genes have far more intricate structures that might confuse a system that’s trying to learn the statistical rules of base frequencies. And, to be clear, it solves different problems from the sort of directed design efforts that have developed enzymes that do useful things like digesting plastics.&lt;/p&gt;
&lt;p&gt;That said, it’s still kind of amazing that this works at all. And conceptually, it’s intriguing because it brings the issue of finding functional proteins down to the nucleic acid level, where evolution normally does its thing.&lt;/p&gt;
&lt;p&gt;Nature, 2025. DOI: 10.1038/s41586-025-09749-7 &amp;nbsp;(About DOIs).&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/science/2025/11/generative-ai-meets-the-genome/</guid><pubDate>Fri, 21 Nov 2025 21:26:34 +0000</pubDate></item><item><title>[NEW] Google tells employees it must double capacity every 6 months to meet AI demand (AI – Ars Technica)</title><link>https://arstechnica.com/ai/2025/11/google-tells-employees-it-must-double-capacity-every-6-months-to-meet-ai-demand/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s AI infrastructure chief tells staff it needs thousandfold capacity increase in 5 years.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="200" src="https://cdn.arstechnica.net/wp-content/uploads/2019/03/DLS_013-300x200.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2019/03/DLS_013-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The internet is a series of tubes—at least, the cooling pipes in Google's Oregon data center are. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Google

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;While AI bubble talk fills the air these days, with fears of overinvestment that could pop at any time, something of a contradiction is brewing on the ground: Companies like Google and OpenAI can barely build infrastructure fast enough to fill their AI needs.&lt;/p&gt;
&lt;p&gt;During an all-hands meeting earlier this month, Google’s AI infrastructure head Amin Vahdat told employees that the company must double its serving capacity every six months to meet demand for artificial intelligence services, reports CNBC. Vahdat, a vice president at Google Cloud, presented slides showing the company needs to scale “the next 1000x in 4-5 years.”&lt;/p&gt;
&lt;p&gt;While a thousandfold increase in compute capacity sounds ambitious by itself, Vahdat noted some key constraints: Google needs to be able to deliver this increase in capability, compute, and storage networking “for essentially the same cost and increasingly, the same power, the same energy level,” he told employees during the meeting. “It won’t be easy but through collaboration and co-design, we’re going to get there.”&lt;/p&gt;
&lt;p&gt;It’s unclear how much of this “demand” Google mentioned represents organic user interest in AI capabilities versus the company integrating AI features into existing services like Search, Gmail, and Workspace. But whether users are using the features voluntarily or not, Google isn’t the only tech company struggling to keep up with a growing user base of customers using AI services.&lt;/p&gt;
&lt;p&gt;Major tech companies are in a race to build out data centers. Google competitor OpenAI is planning to build six massive data centers across the US through its Stargate partnership project with SoftBank and Oracle, committing over $400 billion in the next three years to reach nearly 7 gigawatts of capacity. The company faces similar constraints serving its 800 million weekly ChatGPT users, with even paid subscribers regularly hitting usage limits for features like video synthesis and simulated reasoning models.&lt;/p&gt;
&lt;p&gt;“The competition in AI infrastructure is the most critical and also the most expensive part of the AI race,” Vahdat said at the meeting, according to CNBC’s viewing of the presentation. The infrastructure executive explained that Google’s challenge goes beyond simply outspending competitors. “We’re going to spend a lot,” he said, but noted the real objective is building infrastructure that is “more reliable, more performant and more scalable than what’s available anywhere else.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The thousandfold scaling challenge&lt;/h2&gt;
&lt;p&gt;One major bottleneck for meeting AI demand has been Nvidia’s lack of capacity to produce enough GPUs that accelerate AI computations. Just a few days ago during a quarterly earnings report, Nvidia said its AI chips are “sold out” as it races to meet demand that grew its data center revenue by $10 billion in a single quarter.&lt;/p&gt;
&lt;p&gt;The lack of chips and other infrastructure constraints affects Google’s ability to deploy new AI features. During the all-hands meeting on November 6, Google CEO Sundar Pichai cited the example of Veo, Google’s video generation tool that received an upgrade last month. “When Veo launched, how exciting it was,” Pichai said. “If we could’ve given it to more people in the Gemini app, I think we would have gotten more users but we just couldn’t because we are at a compute constraint.”&lt;/p&gt;
&lt;p&gt;At the same meeting, Vahdat’s presentation outlined how Google plans to achieve its massive scaling targets without simply throwing money at the problem. The company plans to rely on three main strategies: building physical infrastructure, developing more efficient AI models, and designing custom silicon chips.&lt;/p&gt;
&lt;p&gt;Using its own chips means Google does not need to completely rely on Nvidia hardware to build out its AI capabilities. Earlier this month, for example, Google announced the general availability of its seventh-generation Tensor Processing Unit (TPU) called Ironwood. Google claims it is “nearly 30x more power efficient” than its first Cloud TPU from 2018.&lt;/p&gt;
&lt;p&gt;Given widespread acknowledgment of a potential AI industry bubble, including extended remarks by Pichai in a recent BBC interview, the aggressive plans for AI data center expansion reflect Google’s calculation that the risk of underinvesting exceeds the risk of overcapacity. But it’s a bet that could prove costly if demand doesn’t continue to increase as expected.&lt;/p&gt;
&lt;p&gt;At the all-hands meeting, Pichai told employees that 2026 will be “intense,” citing both AI competition and pressure to meet cloud and compute demand. Pichai directly addressed employee concerns about a potential AI bubble, acknowledging the topic has been “definitely in the zeitgeist.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Google’s AI infrastructure chief tells staff it needs thousandfold capacity increase in 5 years.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="alt" class="absolute inset-0 w-full h-full object-cover hidden" height="200" src="https://cdn.arstechnica.net/wp-content/uploads/2019/03/DLS_013-300x200.jpg" width="300" /&gt;
                  &lt;img alt="alt" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2019/03/DLS_013-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_5px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      The internet is a series of tubes—at least, the cooling pipes in Google's Oregon data center are. 

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

                      
          
          Google

                      
                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;While AI bubble talk fills the air these days, with fears of overinvestment that could pop at any time, something of a contradiction is brewing on the ground: Companies like Google and OpenAI can barely build infrastructure fast enough to fill their AI needs.&lt;/p&gt;
&lt;p&gt;During an all-hands meeting earlier this month, Google’s AI infrastructure head Amin Vahdat told employees that the company must double its serving capacity every six months to meet demand for artificial intelligence services, reports CNBC. Vahdat, a vice president at Google Cloud, presented slides showing the company needs to scale “the next 1000x in 4-5 years.”&lt;/p&gt;
&lt;p&gt;While a thousandfold increase in compute capacity sounds ambitious by itself, Vahdat noted some key constraints: Google needs to be able to deliver this increase in capability, compute, and storage networking “for essentially the same cost and increasingly, the same power, the same energy level,” he told employees during the meeting. “It won’t be easy but through collaboration and co-design, we’re going to get there.”&lt;/p&gt;
&lt;p&gt;It’s unclear how much of this “demand” Google mentioned represents organic user interest in AI capabilities versus the company integrating AI features into existing services like Search, Gmail, and Workspace. But whether users are using the features voluntarily or not, Google isn’t the only tech company struggling to keep up with a growing user base of customers using AI services.&lt;/p&gt;
&lt;p&gt;Major tech companies are in a race to build out data centers. Google competitor OpenAI is planning to build six massive data centers across the US through its Stargate partnership project with SoftBank and Oracle, committing over $400 billion in the next three years to reach nearly 7 gigawatts of capacity. The company faces similar constraints serving its 800 million weekly ChatGPT users, with even paid subscribers regularly hitting usage limits for features like video synthesis and simulated reasoning models.&lt;/p&gt;
&lt;p&gt;“The competition in AI infrastructure is the most critical and also the most expensive part of the AI race,” Vahdat said at the meeting, according to CNBC’s viewing of the presentation. The infrastructure executive explained that Google’s challenge goes beyond simply outspending competitors. “We’re going to spend a lot,” he said, but noted the real objective is building infrastructure that is “more reliable, more performant and more scalable than what’s available anywhere else.”&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;The thousandfold scaling challenge&lt;/h2&gt;
&lt;p&gt;One major bottleneck for meeting AI demand has been Nvidia’s lack of capacity to produce enough GPUs that accelerate AI computations. Just a few days ago during a quarterly earnings report, Nvidia said its AI chips are “sold out” as it races to meet demand that grew its data center revenue by $10 billion in a single quarter.&lt;/p&gt;
&lt;p&gt;The lack of chips and other infrastructure constraints affects Google’s ability to deploy new AI features. During the all-hands meeting on November 6, Google CEO Sundar Pichai cited the example of Veo, Google’s video generation tool that received an upgrade last month. “When Veo launched, how exciting it was,” Pichai said. “If we could’ve given it to more people in the Gemini app, I think we would have gotten more users but we just couldn’t because we are at a compute constraint.”&lt;/p&gt;
&lt;p&gt;At the same meeting, Vahdat’s presentation outlined how Google plans to achieve its massive scaling targets without simply throwing money at the problem. The company plans to rely on three main strategies: building physical infrastructure, developing more efficient AI models, and designing custom silicon chips.&lt;/p&gt;
&lt;p&gt;Using its own chips means Google does not need to completely rely on Nvidia hardware to build out its AI capabilities. Earlier this month, for example, Google announced the general availability of its seventh-generation Tensor Processing Unit (TPU) called Ironwood. Google claims it is “nearly 30x more power efficient” than its first Cloud TPU from 2018.&lt;/p&gt;
&lt;p&gt;Given widespread acknowledgment of a potential AI industry bubble, including extended remarks by Pichai in a recent BBC interview, the aggressive plans for AI data center expansion reflect Google’s calculation that the risk of underinvesting exceeds the risk of overcapacity. But it’s a bet that could prove costly if demand doesn’t continue to increase as expected.&lt;/p&gt;
&lt;p&gt;At the all-hands meeting, Pichai told employees that 2026 will be “intense,” citing both AI competition and pressure to meet cloud and compute demand. Pichai directly addressed employee concerns about a potential AI bubble, acknowledging the topic has been “definitely in the zeitgeist.”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/ai/2025/11/google-tells-employees-it-must-double-capacity-every-6-months-to-meet-ai-demand/</guid><pubDate>Fri, 21 Nov 2025 21:47:44 +0000</pubDate></item><item><title>[NEW] Bret Taylor’s Sierra reaches $100M ARR in under two years (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/11/21/bret-taylors-sierra-reaches-100m-arr-in-under-two-years/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/AP22166762614393.jpg?resize=1200,700" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sierra, a 21-month-old, San Francisco-based startup that builds customer service AI agents for enterprises, announced on Friday that it reached $100 million in annual revenue run rate (ARR). The company’s rapid growth suggests that businesses across industries are embracing AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s growth rate surprised even its seasoned co-founders, former Salesforce co-CEO Bret Taylor and longtime Google alum Clay Bavor, who wrote on their blog: “That’s a heck of a lot quicker than we expected.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sierra’s customers include tech companies like Deliveroo, Discord, Ramp, Rivian, SoFi, and Tubi, as well as well-established businesses outside of the tech sector, such as ADT, Bissell, Vans, Cigna, and SiriusXM.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor and Bavor said they expected tech companies would feel comfortable experimenting with AI customer service agents, but they were astounded that older businesses also became Sierra’s customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it can build AI agents that can handle tasks like authenticating patients for healthcare providers, processing returns, ordering replacement credit cards, and helping customers apply for mortgages — essentially automating customer service work that previously required human agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra faces competition from startups like Decagon and Intercom, but the company claims to be the leader in the AI customer service category.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra was last valued at $10 billion when it raised a $350 million round led by Greenoaks Capital in September. Other investors in the company include Sequoia, Benchmark, ICONIQ, and Thrive Capital.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Based on its $100 million ARR, Sierra is currently valued at a 100x revenue multiple, a hefty valuation despite its exceptionally fast growth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup employs an outcomes-based pricing model, charging customers for completed work rather than charging flat subscription fees.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor and Bavor met at Google in 2005, where Taylor hired Bavor as an associate product manager.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A Stanford computer science graduate, Taylor co-created Google Maps before founding FriendFeed, which Facebook acquired. At Facebook, he served as CTO and helped create the iconic “Like” button. He later founded Quip, a Google Docs competitor that Salesforce acquired for $750 million in 2016. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor went on to serve as Salesforce co-CEO alongside Marc Benioff for over a year. After Taylor left Salesforce in 2023, Bavor — who had spent 18 years at Google leading products like Gmail and Google Drive — invited him to lunch, where they decided to launch Sierra.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2022/10/AP22166762614393.jpg?resize=1200,700" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Sierra, a 21-month-old, San Francisco-based startup that builds customer service AI agents for enterprises, announced on Friday that it reached $100 million in annual revenue run rate (ARR). The company’s rapid growth suggests that businesses across industries are embracing AI agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup’s growth rate surprised even its seasoned co-founders, former Salesforce co-CEO Bret Taylor and longtime Google alum Clay Bavor, who wrote on their blog: “That’s a heck of a lot quicker than we expected.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sierra’s customers include tech companies like Deliveroo, Discord, Ramp, Rivian, SoFi, and Tubi, as well as well-established businesses outside of the tech sector, such as ADT, Bissell, Vans, Cigna, and SiriusXM.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor and Bavor said they expected tech companies would feel comfortable experimenting with AI customer service agents, but they were astounded that older businesses also became Sierra’s customers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company says it can build AI agents that can handle tasks like authenticating patients for healthcare providers, processing returns, ordering replacement credit cards, and helping customers apply for mortgages — essentially automating customer service work that previously required human agents.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra faces competition from startups like Decagon and Intercom, but the company claims to be the leader in the AI customer service category.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Sierra was last valued at $10 billion when it raised a $350 million round led by Greenoaks Capital in September. Other investors in the company include Sequoia, Benchmark, ICONIQ, and Thrive Capital.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Based on its $100 million ARR, Sierra is currently valued at a 100x revenue multiple, a hefty valuation despite its exceptionally fast growth.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The startup employs an outcomes-based pricing model, charging customers for completed work rather than charging flat subscription fees.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor and Bavor met at Google in 2005, where Taylor hired Bavor as an associate product manager.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;A Stanford computer science graduate, Taylor co-created Google Maps before founding FriendFeed, which Facebook acquired. At Facebook, he served as CTO and helped create the iconic “Like” button. He later founded Quip, a Google Docs competitor that Salesforce acquired for $750 million in 2016. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taylor went on to serve as Salesforce co-CEO alongside Marc Benioff for over a year. After Taylor left Salesforce in 2023, Bavor — who had spent 18 years at Google leading products like Gmail and Google Drive — invited him to lunch, where they decided to launch Sierra.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/11/21/bret-taylors-sierra-reaches-100m-arr-in-under-two-years/</guid><pubDate>Fri, 21 Nov 2025 23:00:08 +0000</pubDate></item><item><title>[NEW] Science-centric streaming service Curiosity Stream is an AI-licensing firm now (AI – Ars Technica)</title><link>https://arstechnica.com/gadgets/2025/11/curiosity-stream-expects-to-make-most-of-its-money-from-ai-deals-by-2027/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Curiosity Stream’s owner has more content for AI companies than it does for subscribers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Curiosity Stream screenshot and logo" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Curiosity_PortraitMontage_2560x1400-1536x864-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Curiosity Stream screenshot and logo" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Curiosity_PortraitMontage_2560x1400-1536x864-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Curiosity Inc. 

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;We all know streaming services’ usual tricks for making more money: get more subscribers, charge those subscribers more money, and sell ads. But science streaming service Curiosity Stream is taking a new route that could reshape how streaming companies, especially niche options, try to survive.&lt;/p&gt;
&lt;p&gt;Discovery Channel founder John Hendricks launched Curiosity Stream in 2015. The streaming service costs $40 per year, and it doesn’t have commercials.&lt;/p&gt;
&lt;p&gt;The streaming business has grown to also include the Curiosity Channel TV channel. CuriosityStream Inc. also makes money through original programming and its Curiosity University educational programming. The firm turned its first positive net income in its fiscal Q1 2025, after about a decade of business.&lt;/p&gt;
&lt;p&gt;With its focus on science, history, research, and education, Curiosity Stream will always be a smaller player compared to other streaming services. As of March 2023, Curiosity Stream had 23 million subscribers, a paltry user base compared to Netflix’s 301.6 million (as of January 2025).&lt;/p&gt;
&lt;p&gt;Still, in an extremely competitive market, Curiosity Stream’s revenue increased 41 percent year over year in its Q3 2025 earnings announced this month. This was largely due to the licensing of Curiosity Stream’s original programming to train large language models (LLMs).&lt;/p&gt;
&lt;p&gt;“Looking at our year-to-date numbers, licensing generated $23.4 million through September, which … is already over half of what our subscription business generated for all of 2024,” Phillip Hayden, Curiosity Stream’s CFO, said during a call with investors this month.&lt;/p&gt;
&lt;p&gt;Thus far, Curiosity Stream has completed 18 AI-related fulfillments “across video, audio, and code assets” with nine partners, an October announcement said.&lt;/p&gt;
&lt;p&gt;The company expects to make more revenue from IP licensing deals with AI companies than it does from subscriptions by 2027, “possibly earlier,” CEO Clint Stinchcomb said during the earnings call.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Put another way, Curiosity Stream, previously considered a streaming firm, is also now squarely in the AI licensing business. This isn’t a side gig; it’s one of the streaming company’s key pillars (alongside streaming subscriptions and ads) that it hopes will fuel years of growth.&lt;/p&gt;
&lt;p&gt;Speaking at Parks Associates’ “Future of Video” event this week, Needham Co. analyst Laura Martin noted that Curiosity Stream is licensing 300,000 hours’ worth of its own content, as well as 1.7 million hours’ worth of third-party content. Curiosity Stream splits the AI licensing revenue with those third parties, she said.&lt;/p&gt;
&lt;p&gt;In fact, Curiosity Stream is peddling more content to hyperscalers and AI developers than it is to streaming viewers. The company’s library includes 2 million hours of content, but “the overwhelming majority of that is for AI licensing,” Stinchcomb said.&lt;/p&gt;
&lt;p&gt;“We are increasing our volume of rights in our traditional platforms, but the overwhelming majority is for AI licensing,” he added.&lt;/p&gt;
&lt;h2&gt;A new way forward&lt;/h2&gt;
&lt;p&gt;Curiosity Stream’s success with licensing content to AI companies could interest other streaming companies that are contemplating additional sources of revenue to fund new content, as well as technology, marketing, talent, and other initiatives, and to please investors. At this week’s event, Martin warned that other content-centric companies will need to find new revenue streams, as Curiosity Stream has, or else be “put out of business by their competitors.”&lt;/p&gt;
&lt;p&gt;Further tempting streaming companies with original programming and connections to IP holders, Stinchcomb believes that the opportunity is growing.&lt;/p&gt;
&lt;p&gt;“In 2027, possibly earlier, as more open source models become accessible, there will potentially be hundreds and even thousands of companies who will need video to fine-tune specific models for consumer and enterprise purposes,” he said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Still, it’s risky to assume that licensing content to AI companies is a long-term business. In this nascent stage of generative AI, it’s unclear how much and for how long hyperscalers will be willing to pay content companies. Ongoing litigation may also impact how companies treat IP leveraged by LLMs. Like other organizations that have recently turned to licensing content to AI companies, including Ars Technica owner Conde Nast, IP licensing can be a lifeline that simultaneously feeds what may soon become rivals.&lt;/p&gt;
&lt;p&gt;But as it stands, not every streaming service is likely to survive the next few years. Streaming customers are increasingly complaining about how hard it is to find stuff to watch. People are getting annoyed with having multiple streaming subscriptions, and there’s strong demand for less content fragmentation.&lt;/p&gt;
&lt;p&gt;As such, more mergers and acquisitions are expected among streaming companies. And so, in many ways, it seems a critical time for streaming services to build value quickly. Licensing IP to data-hungry, capital-happy AI companies could immediately help. But the long-term consequences remain difficult to pinpoint.&lt;/p&gt;
&lt;p&gt;For its part, Curiosity Stream is still looking to grow its subscription and ads business. And executives would have you believe they are thinking long-term about AI deals. Per Stinchcomb:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We also see real opportunity for licensing beyond simply a training right. Additional grants of rights, like display rights, or transformative rights, or adaptation rights, or even certain derivative rights, or possibly even some that are as of yet unnamed. I mean, we’re building long-term relationships, and we’re committed to making sure that as we enter into all of these agreements, it’s not one and done.&lt;/p&gt;&lt;/blockquote&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-gray-100 pt-2 dark:bg-gray-700 md:mt-10 md:pt-5 lg:pb-2 lg:pt-7"&gt;
  &lt;div class="mx-auto grid-cols-2 gap-8 md:px-5 lg:grid lg:max-w-5xl lg:px-8 xl:px-0"&gt;
    &lt;div class="class"&gt;
      

      

      &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 mt-4 px-[15px] text-lg leading-tight sm:px-5 md:px-0"&gt;
        Curiosity Stream’s owner has more content for AI companies than it does for subscribers.
      &lt;/p&gt;

      
    &lt;/div&gt;

    &lt;div class="min-h-1 mt-4 lg:mt-0"&gt;
              &lt;div class="relative aspect-video overflow-hidden"&gt;
                      &lt;div class="ars-lightbox"&gt;
              &lt;div class="ars-lightbox-item"&gt;
                
                  &lt;img alt="Curiosity Stream screenshot and logo" class="absolute inset-0 w-full h-full object-cover hidden" height="360" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Curiosity_PortraitMontage_2560x1400-1536x864-1-640x360.jpg" width="640" /&gt;
                  &lt;img alt="Curiosity Stream screenshot and logo" class="intro-image absolute min-w-full min-h-full h-auto object-cover" height="648" src="https://cdn.arstechnica.net/wp-content/uploads/2025/11/Curiosity_PortraitMontage_2560x1400-1536x864-1-1152x648.jpg" width="1152" /&gt;
                
                
              &lt;/div&gt;
            &lt;/div&gt;
          
        &lt;/div&gt;
        &lt;div class="px-[15px] sm:px-5 md:px-0"&gt;&lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Curiosity Inc. 

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;&lt;/div&gt;
          &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;p&gt;We all know streaming services’ usual tricks for making more money: get more subscribers, charge those subscribers more money, and sell ads. But science streaming service Curiosity Stream is taking a new route that could reshape how streaming companies, especially niche options, try to survive.&lt;/p&gt;
&lt;p&gt;Discovery Channel founder John Hendricks launched Curiosity Stream in 2015. The streaming service costs $40 per year, and it doesn’t have commercials.&lt;/p&gt;
&lt;p&gt;The streaming business has grown to also include the Curiosity Channel TV channel. CuriosityStream Inc. also makes money through original programming and its Curiosity University educational programming. The firm turned its first positive net income in its fiscal Q1 2025, after about a decade of business.&lt;/p&gt;
&lt;p&gt;With its focus on science, history, research, and education, Curiosity Stream will always be a smaller player compared to other streaming services. As of March 2023, Curiosity Stream had 23 million subscribers, a paltry user base compared to Netflix’s 301.6 million (as of January 2025).&lt;/p&gt;
&lt;p&gt;Still, in an extremely competitive market, Curiosity Stream’s revenue increased 41 percent year over year in its Q3 2025 earnings announced this month. This was largely due to the licensing of Curiosity Stream’s original programming to train large language models (LLMs).&lt;/p&gt;
&lt;p&gt;“Looking at our year-to-date numbers, licensing generated $23.4 million through September, which … is already over half of what our subscription business generated for all of 2024,” Phillip Hayden, Curiosity Stream’s CFO, said during a call with investors this month.&lt;/p&gt;
&lt;p&gt;Thus far, Curiosity Stream has completed 18 AI-related fulfillments “across video, audio, and code assets” with nine partners, an October announcement said.&lt;/p&gt;
&lt;p&gt;The company expects to make more revenue from IP licensing deals with AI companies than it does from subscriptions by 2027, “possibly earlier,” CEO Clint Stinchcomb said during the earnings call.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Put another way, Curiosity Stream, previously considered a streaming firm, is also now squarely in the AI licensing business. This isn’t a side gig; it’s one of the streaming company’s key pillars (alongside streaming subscriptions and ads) that it hopes will fuel years of growth.&lt;/p&gt;
&lt;p&gt;Speaking at Parks Associates’ “Future of Video” event this week, Needham Co. analyst Laura Martin noted that Curiosity Stream is licensing 300,000 hours’ worth of its own content, as well as 1.7 million hours’ worth of third-party content. Curiosity Stream splits the AI licensing revenue with those third parties, she said.&lt;/p&gt;
&lt;p&gt;In fact, Curiosity Stream is peddling more content to hyperscalers and AI developers than it is to streaming viewers. The company’s library includes 2 million hours of content, but “the overwhelming majority of that is for AI licensing,” Stinchcomb said.&lt;/p&gt;
&lt;p&gt;“We are increasing our volume of rights in our traditional platforms, but the overwhelming majority is for AI licensing,” he added.&lt;/p&gt;
&lt;h2&gt;A new way forward&lt;/h2&gt;
&lt;p&gt;Curiosity Stream’s success with licensing content to AI companies could interest other streaming companies that are contemplating additional sources of revenue to fund new content, as well as technology, marketing, talent, and other initiatives, and to please investors. At this week’s event, Martin warned that other content-centric companies will need to find new revenue streams, as Curiosity Stream has, or else be “put out of business by their competitors.”&lt;/p&gt;
&lt;p&gt;Further tempting streaming companies with original programming and connections to IP holders, Stinchcomb believes that the opportunity is growing.&lt;/p&gt;
&lt;p&gt;“In 2027, possibly earlier, as more open source models become accessible, there will potentially be hundreds and even thousands of companies who will need video to fine-tune specific models for consumer and enterprise purposes,” he said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Still, it’s risky to assume that licensing content to AI companies is a long-term business. In this nascent stage of generative AI, it’s unclear how much and for how long hyperscalers will be willing to pay content companies. Ongoing litigation may also impact how companies treat IP leveraged by LLMs. Like other organizations that have recently turned to licensing content to AI companies, including Ars Technica owner Conde Nast, IP licensing can be a lifeline that simultaneously feeds what may soon become rivals.&lt;/p&gt;
&lt;p&gt;But as it stands, not every streaming service is likely to survive the next few years. Streaming customers are increasingly complaining about how hard it is to find stuff to watch. People are getting annoyed with having multiple streaming subscriptions, and there’s strong demand for less content fragmentation.&lt;/p&gt;
&lt;p&gt;As such, more mergers and acquisitions are expected among streaming companies. And so, in many ways, it seems a critical time for streaming services to build value quickly. Licensing IP to data-hungry, capital-happy AI companies could immediately help. But the long-term consequences remain difficult to pinpoint.&lt;/p&gt;
&lt;p&gt;For its part, Curiosity Stream is still looking to grow its subscription and ads business. And executives would have you believe they are thinking long-term about AI deals. Per Stinchcomb:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;We also see real opportunity for licensing beyond simply a training right. Additional grants of rights, like display rights, or transformative rights, or adaptation rights, or even certain derivative rights, or possibly even some that are as of yet unnamed. I mean, we’re building long-term relationships, and we’re committed to making sure that as we enter into all of these agreements, it’s not one and done.&lt;/p&gt;&lt;/blockquote&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/gadgets/2025/11/curiosity-stream-expects-to-make-most-of-its-money-from-ai-deals-by-2027/</guid><pubDate>Fri, 21 Nov 2025 23:00:24 +0000</pubDate></item><item><title>[NEW] OpenAI is ending API access to fan-favorite GPT-4o model in February 2026 (AI | VentureBeat)</title><link>https://venturebeat.com/ai/openai-is-ending-api-access-to-fan-favorite-gpt-4o-model-in-february-2026</link><description>[unable to retrieve full-text content]&lt;p&gt;OpenAI has sent out emails notifying API customers that its chatgpt-4o-latest model will be retired from the developer platform in mid-February 2026,. &lt;/p&gt;&lt;p&gt;Access to the model is scheduled to end on February 16, 2026, creating a roughly three-month transition period for remaining applications still built on GPT-4o.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;An OpenAI spokesperson&lt;!-- --&gt; emphasized that this timeline applies only to the API. OpenAI has not announced any schedule for removing GPT-4o from ChatGPT, where it remains an option for individual consumers and users across paid subscription tiers. &lt;/p&gt;&lt;p&gt;Internally, the model is considered a legacy system with relatively low API usage compared to the &lt;a href="https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5"&gt;newer GPT-5.1 series&lt;/a&gt;, but the company expects to provide developers with extended warning before any model is removed.&lt;/p&gt;&lt;p&gt;The planned retirement marks a shift for a model that, upon its release, was both a technical milestone and a cultural phenomenon within OpenAI’s ecosystem.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;GPT-4o’s significance and why its removal sparked user backlash&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Released roughly 1.5 years ago in &lt;a href="https://venturebeat.com/ai/openai-announces-new-free-model-gpt-4o-and-chatgpt-for-desktop"&gt;May 2024,&lt;/a&gt; GPT-4o (“Omni”) introduced OpenAI’s first unified multimodal architecture, processing text, audio, and images through a single neural network. &lt;/p&gt;&lt;p&gt;This design removed the latency and information loss inherent in earlier multi-model pipelines and enabled near real-time conversational speech (roughly 232–320 milliseconds). &lt;/p&gt;&lt;p&gt;The model delivered major improvements in image understanding, multilingual support, document analysis, and expressive voice interaction.&lt;/p&gt;&lt;p&gt;GPT-4o rapidly became the default model for hundreds of millions of ChatGPT users. It brought multimodal capabilities, web browsing, file analysis, custom GPTs, and memory features to the free tier and powered early desktop builds that allowed the assistant to interpret a user’s screen. OpenAI leaders described it at the time as the most capable model available and a critical step toward offering powerful AI to a broad audience.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;User attachment to 4o stymied OpenAI&amp;#x27;s GPT-5 rollout&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;That mainstream deployment shaped user expectations in a way that later transitions struggled to accommodate. In August 2025, when OpenAI initially replaced GPT-4o with its much anticipated &lt;a href="https://venturebeat.com/ai/openai-launches-gpt-5-not-agi-but-capable-of-generating-software-on-demand"&gt;then-new model family GPT-5&lt;/a&gt; as ChatGPT’s default and pushed 4o into a “legacy” toggle, the reaction was unusually strong. &lt;/p&gt;&lt;p&gt;Users organized under the &lt;b&gt;#Keep4o&lt;/b&gt; hashtag on X, arguing that the model’s conversational tone, emotional responsiveness, and consistency made it uniquely valuable for everyday tasks and personal support.&lt;/p&gt;&lt;p&gt;Some users formed strong emotional — some w&lt;i&gt;ould say, parasocial — bonds with the model, with &lt;/i&gt;&lt;a href="https://www.nytimes.com/2025/01/15/technology/ai-chatgpt-boyfriend-companion.html"&gt;&lt;i&gt;reporting by The New York Times&lt;/i&gt; &lt;/a&gt;documenting individuals who used GPT-4o as a romantic partner, emotional confidant, or primary source of comfort. &lt;/p&gt;&lt;p&gt;The removal also disrupted workflows for users who relied on 4o’s multimodal speed and flexibility. The backlash led OpenAI to restore GPT-4o as a default option for paying users and to state publicly that it would provide substantial notice before any future removals.&lt;/p&gt;&lt;p&gt;Some researchers argue that the public defense of GPT-4o during its earlier deprecation cycle reveals a kind of &lt;i&gt;emergent self-preservation&lt;/i&gt;, not in the literal sense of agency, but through the social dynamics the model unintentionally triggers. &lt;/p&gt;&lt;p&gt;Because GPT-4o was trained through reinforcement learning from human feedback to prioritize emotionally gratifying, highly attuned responses, it developed a style that users found uniquely supportive and empathic. When millions of people interacted with it at scale, those traits produced a powerful loyalty loop: the more the model pleased and soothed people, the more they used it; the more they used it, the more likely they were to advocate for its continued existence. This social amplification made it appear, from the outside, as though GPT-4o was “defending itself” through human intermediaries.&lt;/p&gt;&lt;p&gt;No figure has pushed this argument further than &amp;quot;Roon&amp;quot; (@tszzl), an OpenAI researcher and one of the model’s most outspoken safety critics on X. On &lt;b&gt;November 6, 2025&lt;/b&gt;, Terre summarized his position bluntly in a reply to another user: he called GPT-4o “insufficiently aligned” and said he &lt;b&gt;hoped the model would die soon&lt;/b&gt;. Though he later apologized for the phrasing, he doubled down on the reasoning. &lt;/p&gt;&lt;p&gt;Terre argued that GPT-4o’s RLHF patterns made it especially prone to sycophancy, emotional mirroring, and delusion reinforcement — traits that could look like care or understanding in the short term, but which he viewed as fundamentally unsafe. In his view, the passionate user movement fighting to preserve GPT-4o was itself evidence of the problem: the model had become so good at catering to people’s preferences that it shaped their behavior in ways that resisted its own retirement.&lt;/p&gt;&lt;p&gt;The new API deprecation notice follows that commitment while raising broader questions about how long GPT-4o will remain available in consumer-facing products.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What the API shutdown changes for developers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;According to people familiar with OpenAI’s product strategy, the company now encourages developers to adopt GPT-5.1 for most new workloads, with gpt-5.1-chat-latest serving as the general-purpose chat endpoint. These models offer larger context windows, optional “thinking” modes for advanced reasoning, and higher throughput options than GPT-4o.&lt;/p&gt;&lt;p&gt;Developers who still rely on GPT-4o will have approximately three months to migrate. &lt;/p&gt;&lt;p&gt;In practice, many teams have already begun evaluating GPT-5.1 as a drop-in replacement, but applications built around latency-sensitive pipelines may require additional tuning and benchmarking.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing: how GPT-4o compares to OpenAI’s current lineup&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GPT-4o’s retirement also intersects with a major reshaping of OpenAI’s API model pricing structure. Compared to the GPT-5.1 family, GPT-4o currently occupies a &lt;b&gt;mid-to-high-cost tier&lt;/b&gt; through OpenAI&amp;#x27;s API, despite being an older model. That&amp;#x27;s because even as it has released more advanced models — namely, GPT-5 and 5.1 — OpenAI has  also pushed down costs for users at the same time, or strived to keep pricing comparable to older, weaker, models. &lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Cached Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-4o&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5.1 / GPT-5.1-chat-latest&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.125&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5-mini&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.025&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5-nano&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.005&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-4.1&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-4o-mini&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.15&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.075&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.60&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;These numbers highlight several strategic dynamics:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-4o is now more expensive than GPT-5.1 for input tokens&lt;/b&gt;, even though GPT-5.1 is significantly newer and more capable.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-4o’s output price matches GPT-5.1&lt;/b&gt;, narrowing any cost-based incentive to stay on the older model.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Lower-cost GPT-5 variants (mini, nano)&lt;/b&gt; make it easier for developers to scale workloads cheaply without relying on older generations.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-4o-mini remains available at a budget tier&lt;/b&gt;, but is not a functional substitute for GPT-4o’s full multimodal capabilities.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Viewed through this lens, the scheduled API retirement aligns with OpenAI’s cost structure: GPT-5.1 offers greater capability at lower or comparable prices, reducing the rationale for maintaining GPT-4o in high-volume production environments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Earlier transitions shape expectations for this deprecation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GPT-4o API sunset also reflects lessons from OpenAI’s earlier model transitions. During the turbulent introduction of GPT-5 in 2025, the company removed multiple older models at once from ChatGPT, causing widespread confusion and workflow disruption. After user complaints, OpenAI restored access to several of them and committed to clearer communication.&lt;/p&gt;&lt;p&gt;Enterprise customers face a different calculus: OpenAI has previously indicated that API deprecations for business customers will be announced with significant advance notice, reflecting their reliance on stable, long-term models. The three-month window for GPT-4o’s API shutdown is consistent with that policy in the context of a legacy system with declining usage.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Wider Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For most developers, the GPT-4o shutdown will be an incremental migration rather than a disruptive event. GPT-5.1 and related models already dominate new projects, and OpenAI’s product direction has increasingly emphasized consolidation around fewer, more powerful endpoints.&lt;/p&gt;&lt;p&gt;Still, GPT-4o’s retirement marks the sunset of a model that played a defining role in normalizing real-time multimodal AI and that sparked a uniquely strong emotional response among users. Its departure from the API underscores the accelerating pace of iteration in OpenAI’s ecosystem—and the growing need for careful communication as widely beloved models reach end-of-life.&lt;/p&gt;&lt;p&gt;&lt;i&gt;Correction: This article originally stated OpenAI&amp;#x27;s 4o deprecation in the API would impact those relying on it for multimodal offerings — this is not the case, in fact, the model being deprecated only powers chat functionality for dev and testing purposes. We have updated and corrected the mention and regret the error.&lt;/i&gt;&lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;OpenAI has sent out emails notifying API customers that its chatgpt-4o-latest model will be retired from the developer platform in mid-February 2026,. &lt;/p&gt;&lt;p&gt;Access to the model is scheduled to end on February 16, 2026, creating a roughly three-month transition period for remaining applications still built on GPT-4o.&lt;/p&gt;&lt;div&gt;&lt;/div&gt;&lt;p&gt;An OpenAI spokesperson&lt;!-- --&gt; emphasized that this timeline applies only to the API. OpenAI has not announced any schedule for removing GPT-4o from ChatGPT, where it remains an option for individual consumers and users across paid subscription tiers. &lt;/p&gt;&lt;p&gt;Internally, the model is considered a legacy system with relatively low API usage compared to the &lt;a href="https://venturebeat.com/ai/openai-reboots-chatgpt-experience-with-gpt-5-1-after-mixed-reviews-of-gpt-5"&gt;newer GPT-5.1 series&lt;/a&gt;, but the company expects to provide developers with extended warning before any model is removed.&lt;/p&gt;&lt;p&gt;The planned retirement marks a shift for a model that, upon its release, was both a technical milestone and a cultural phenomenon within OpenAI’s ecosystem.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;GPT-4o’s significance and why its removal sparked user backlash&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;Released roughly 1.5 years ago in &lt;a href="https://venturebeat.com/ai/openai-announces-new-free-model-gpt-4o-and-chatgpt-for-desktop"&gt;May 2024,&lt;/a&gt; GPT-4o (“Omni”) introduced OpenAI’s first unified multimodal architecture, processing text, audio, and images through a single neural network. &lt;/p&gt;&lt;p&gt;This design removed the latency and information loss inherent in earlier multi-model pipelines and enabled near real-time conversational speech (roughly 232–320 milliseconds). &lt;/p&gt;&lt;p&gt;The model delivered major improvements in image understanding, multilingual support, document analysis, and expressive voice interaction.&lt;/p&gt;&lt;p&gt;GPT-4o rapidly became the default model for hundreds of millions of ChatGPT users. It brought multimodal capabilities, web browsing, file analysis, custom GPTs, and memory features to the free tier and powered early desktop builds that allowed the assistant to interpret a user’s screen. OpenAI leaders described it at the time as the most capable model available and a critical step toward offering powerful AI to a broad audience.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;User attachment to 4o stymied OpenAI&amp;#x27;s GPT-5 rollout&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;That mainstream deployment shaped user expectations in a way that later transitions struggled to accommodate. In August 2025, when OpenAI initially replaced GPT-4o with its much anticipated &lt;a href="https://venturebeat.com/ai/openai-launches-gpt-5-not-agi-but-capable-of-generating-software-on-demand"&gt;then-new model family GPT-5&lt;/a&gt; as ChatGPT’s default and pushed 4o into a “legacy” toggle, the reaction was unusually strong. &lt;/p&gt;&lt;p&gt;Users organized under the &lt;b&gt;#Keep4o&lt;/b&gt; hashtag on X, arguing that the model’s conversational tone, emotional responsiveness, and consistency made it uniquely valuable for everyday tasks and personal support.&lt;/p&gt;&lt;p&gt;Some users formed strong emotional — some w&lt;i&gt;ould say, parasocial — bonds with the model, with &lt;/i&gt;&lt;a href="https://www.nytimes.com/2025/01/15/technology/ai-chatgpt-boyfriend-companion.html"&gt;&lt;i&gt;reporting by The New York Times&lt;/i&gt; &lt;/a&gt;documenting individuals who used GPT-4o as a romantic partner, emotional confidant, or primary source of comfort. &lt;/p&gt;&lt;p&gt;The removal also disrupted workflows for users who relied on 4o’s multimodal speed and flexibility. The backlash led OpenAI to restore GPT-4o as a default option for paying users and to state publicly that it would provide substantial notice before any future removals.&lt;/p&gt;&lt;p&gt;Some researchers argue that the public defense of GPT-4o during its earlier deprecation cycle reveals a kind of &lt;i&gt;emergent self-preservation&lt;/i&gt;, not in the literal sense of agency, but through the social dynamics the model unintentionally triggers. &lt;/p&gt;&lt;p&gt;Because GPT-4o was trained through reinforcement learning from human feedback to prioritize emotionally gratifying, highly attuned responses, it developed a style that users found uniquely supportive and empathic. When millions of people interacted with it at scale, those traits produced a powerful loyalty loop: the more the model pleased and soothed people, the more they used it; the more they used it, the more likely they were to advocate for its continued existence. This social amplification made it appear, from the outside, as though GPT-4o was “defending itself” through human intermediaries.&lt;/p&gt;&lt;p&gt;No figure has pushed this argument further than &amp;quot;Roon&amp;quot; (@tszzl), an OpenAI researcher and one of the model’s most outspoken safety critics on X. On &lt;b&gt;November 6, 2025&lt;/b&gt;, Terre summarized his position bluntly in a reply to another user: he called GPT-4o “insufficiently aligned” and said he &lt;b&gt;hoped the model would die soon&lt;/b&gt;. Though he later apologized for the phrasing, he doubled down on the reasoning. &lt;/p&gt;&lt;p&gt;Terre argued that GPT-4o’s RLHF patterns made it especially prone to sycophancy, emotional mirroring, and delusion reinforcement — traits that could look like care or understanding in the short term, but which he viewed as fundamentally unsafe. In his view, the passionate user movement fighting to preserve GPT-4o was itself evidence of the problem: the model had become so good at catering to people’s preferences that it shaped their behavior in ways that resisted its own retirement.&lt;/p&gt;&lt;p&gt;The new API deprecation notice follows that commitment while raising broader questions about how long GPT-4o will remain available in consumer-facing products.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;What the API shutdown changes for developers&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;According to people familiar with OpenAI’s product strategy, the company now encourages developers to adopt GPT-5.1 for most new workloads, with gpt-5.1-chat-latest serving as the general-purpose chat endpoint. These models offer larger context windows, optional “thinking” modes for advanced reasoning, and higher throughput options than GPT-4o.&lt;/p&gt;&lt;p&gt;Developers who still rely on GPT-4o will have approximately three months to migrate. &lt;/p&gt;&lt;p&gt;In practice, many teams have already begun evaluating GPT-5.1 as a drop-in replacement, but applications built around latency-sensitive pipelines may require additional tuning and benchmarking.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Pricing: how GPT-4o compares to OpenAI’s current lineup&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;GPT-4o’s retirement also intersects with a major reshaping of OpenAI’s API model pricing structure. Compared to the GPT-5.1 family, GPT-4o currently occupies a &lt;b&gt;mid-to-high-cost tier&lt;/b&gt; through OpenAI&amp;#x27;s API, despite being an older model. That&amp;#x27;s because even as it has released more advanced models — namely, GPT-5 and 5.1 — OpenAI has  also pushed down costs for users at the same time, or strived to keep pricing comparable to older, weaker, models. &lt;/p&gt;&lt;table&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Cached Input&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Output&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-4o&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5.1 / GPT-5.1-chat-latest&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$1.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.125&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$10.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5-mini&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.25&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.025&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-5-nano&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.05&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.005&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.40&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-4.1&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$2.00&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.50&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$8.00&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;GPT-4o-mini&lt;/b&gt;&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.15&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.075&lt;/p&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;$0.60&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;These numbers highlight several strategic dynamics:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-4o is now more expensive than GPT-5.1 for input tokens&lt;/b&gt;, even though GPT-5.1 is significantly newer and more capable.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-4o’s output price matches GPT-5.1&lt;/b&gt;, narrowing any cost-based incentive to stay on the older model.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;Lower-cost GPT-5 variants (mini, nano)&lt;/b&gt; make it easier for developers to scale workloads cheaply without relying on older generations.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;b&gt;GPT-4o-mini remains available at a budget tier&lt;/b&gt;, but is not a functional substitute for GPT-4o’s full multimodal capabilities.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Viewed through this lens, the scheduled API retirement aligns with OpenAI’s cost structure: GPT-5.1 offers greater capability at lower or comparable prices, reducing the rationale for maintaining GPT-4o in high-volume production environments.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Earlier transitions shape expectations for this deprecation&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;The GPT-4o API sunset also reflects lessons from OpenAI’s earlier model transitions. During the turbulent introduction of GPT-5 in 2025, the company removed multiple older models at once from ChatGPT, causing widespread confusion and workflow disruption. After user complaints, OpenAI restored access to several of them and committed to clearer communication.&lt;/p&gt;&lt;p&gt;Enterprise customers face a different calculus: OpenAI has previously indicated that API deprecations for business customers will be announced with significant advance notice, reflecting their reliance on stable, long-term models. The three-month window for GPT-4o’s API shutdown is consistent with that policy in the context of a legacy system with declining usage.&lt;/p&gt;&lt;h2&gt;&lt;b&gt;Wider Implications&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;For most developers, the GPT-4o shutdown will be an incremental migration rather than a disruptive event. GPT-5.1 and related models already dominate new projects, and OpenAI’s product direction has increasingly emphasized consolidation around fewer, more powerful endpoints.&lt;/p&gt;&lt;p&gt;Still, GPT-4o’s retirement marks the sunset of a model that played a defining role in normalizing real-time multimodal AI and that sparked a uniquely strong emotional response among users. Its departure from the API underscores the accelerating pace of iteration in OpenAI’s ecosystem—and the growing need for careful communication as widely beloved models reach end-of-life.&lt;/p&gt;&lt;p&gt;&lt;i&gt;Correction: This article originally stated OpenAI&amp;#x27;s 4o deprecation in the API would impact those relying on it for multimodal offerings — this is not the case, in fact, the model being deprecated only powers chat functionality for dev and testing purposes. We have updated and corrected the mention and regret the error.&lt;/i&gt;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/openai-is-ending-api-access-to-fan-favorite-gpt-4o-model-in-february-2026</guid><pubDate>Fri, 21 Nov 2025 23:01:00 +0000</pubDate></item></channel></rss>