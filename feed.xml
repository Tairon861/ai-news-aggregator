<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 20 Oct 2025 18:32:24 +0000</lastBuildDate><item><title>The man betting everything on AI and Bill Belichick (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/the-man-betting-everything-on-ai-and-bill-belichick/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.13.38AM.png?resize=1200,807" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Lee Roberts meets me at the University Club of San Francisco on a Friday morning, hours before his football team will lose to Cal in heartbreaking fashion – a fumble at the goal line, because little about the University of North Carolina at Chapel Hill’s expensive experiment with Bill Belichick has gone according to script.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Roberts, the Chancellor of UNC, doesn’t know this yet. Right now, he’s in California to talk about artificial intelligence, which is both forward thinking and also – I’d hazard a guess – a welcome distraction from a lot else happening at the 235-year-old school.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“No one’s going to say to [students after they graduate from college], ‘Do the best job you can, but if you use AI, you’ll be in trouble,’” Roberts tells me, leaning into his central thesis about preparing students for the real world. “Yet we have some faculty members who are effectively saying that to students right now.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts has joined me in between other meetings in the city, including with AI companies, because UNC has decided to make AI its north star. It’s a business bet, really. Roberts spent three decades in finance, most recently as managing partner of a private investment firm, and served as state budget director under a Republican governor. He taught budgeting as an adjunct at Duke but never worked in academic administration before becoming UNC’s interim chancellor in January of last year, a post made permanent eight months later.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Never mind that the university just lost 118 federal grants totaling $38 million as part of a sweeping effort by the federal government to terminate more than 4,000 grants across 600 institutions. Never mind that more than 900 people last year signed a statement saying they wouldn’t recognize Roberts as chancellor when he was appointed, calling the process a political “coronation” rather than a search. Never mind that Belichick’s much-touted return to football is currently a 2-4 trainwreck, with write-ups about the team’s dysfunction becoming routine fodder for sports writers. Roberts is focused on the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At UNC, Roberts explains, there’s a spectrum between faculty who are “leaning forward” with AI and those who have “their heads in the sand.” It’s diplomatic phrasing for what is clearly a culture war playing out in faculty lounges across UNC and – it’s probably safe to assume – at other schools across the world. While one UNC professor is assigning more research than students could complete without AI (“much closer to a real world scenario,” says Roberts), others are treating chatbots like anabolic steroids. If you use them, you’re cheating.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have 4,000 faculty members,” Roberts says, as a cable car clatters past the open window beside our table. “And they pride themselves, as they should, on their independence and autonomy in how they teach their classes.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It sounds a little like code for: tenured professors can’t be forced to do anything. So Roberts is creating “incentive-based programs” to move the ball forward, like promoting one of the school’s deans into the role of Vice Provost for AI at the university. That individual, Jeffrey Bardzell, has been a professor for more than 20 years and has “experience both in technology and as a humanist,” says Roberts, adding that Bardzell is “exceptionally well-placed to help the faculty as a whole come further up to speed.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;UNC is barreling ahead on other fronts in the meantime. In its biggest development to date, the university announced this month that it is merging two schools – the School of Data Science and Society and the School of Information and Library Science – into one yet-to-be-named entity with AI studies at the center of the Venn diagram.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;UNC isn’t alone in betting big on AI. At least 14 colleges now offer bachelor’s degrees in AI, and universities like Arizona State University have made headlines for integrating AI tools across all disciplines.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, creating this new school has worried some of the school’s library science students, who wonder what will happen to their degrees, according to a report in The Daily Tar Heel, the school’s independent student newspaper. At least one faculty member also complained anonymously in a statement to the paper, saying Roberts pushed for the school without a “cogent idea” of what it will entail, adding that the “careers of faculty, staff and students at both of these schools are being sacrificed to Roberts’ ego.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts tells me the implementation will be collaborative, not top-down. He’s also clear that the move is proactive, not reactive. “This is not about shutting anything down,” he says. “It’s not predominantly a cost-savings move,” he continues, a possible nod to those lost federal research dollars, which amount to 3.5% of UNC’s overall research funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts doesn’t minimize the devastation of losing grants – “in many cases, [people] lose their life’s work,” he acknowledges – but he’s also quick to note that 3.5% is “well within our average annual variance.” He adds that he has been spending “a lot of time talking with policymakers and legislators in Washington about the tremendous good that federal research funding represents. We need to be especially vigilant right now, when there’s so much uncertainty around [these dollars] that it’s really changing the basic structure of how large research universities have been funded.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, it raises questions about resources in the aggregate. Though UNC’s AI push is the topic du jour, I ask about the $10 million the school is paying Bill Belichick annually as part of a five-year deal signed back in January. I’m from Cleveland, I tell Roberts. I remember when Belichick cut Browns quarterback Bernie Kosar, a hometown hero. The city never forgave him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts is ready for this. College sports are changing rapidly, he says. Every peer institution spends at least as much on football; many spend more. Football drives revenue for 28 other sports. UNC just won its fourth national championship in women’s lacrosse, its 23rd in women’s soccer. None of that happens without football money.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If we had hired somebody else and we were [down some games], everybody would be saying, ‘Hey, man, you could have had Bill Belichick,’” Roberts offers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In reality, the prevailing narrative about Belichick isn’t just about wins and losses. Even if, ultimately, that’s exactly what it’s about, numerous outlets have published stories describing chaos inside the program, with players, parents, coaches, and administrators all painting a picture of a legendary NFL coach whose style doesn’t translate to college kids.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Roberts isn’t making decisions based on “a couple of news stories,” he says. “Coach Belichick, in my view, has done a really good job integrating with our campus,” Roberts says. He shows up at other teams’ games. He sends pizzas to fraternities on Saturday nights. He grew up on a college campus – his father was the coach at Navy.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hours after our conversation, UNC will lose to Cal when wide receiver Nathan Leacock loses control of the ball just as he’s crossing into the end zone for what would have been the game-winning touchdown. I can only imagine what the immediate reaction is like back in Chapel Hill.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;My sense is that Roberts will brush it off. He may never be forgiven for not having a traditional academic background, but he also can’t afford to care that this bothers some people. I note that the 900-person petition took issue with the fact that, among the top 50 universities, Roberts is the only leader without higher education administration experience. The petition ran in The Daily Tar Heel, which has been critical of Roberts’ chancellorship throughout.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I don’t think it was 900 students,” Roberts corrects me. “I think it was 900 people, regardless of whether they were students, faculty, staff, or just people in the world who signed an online petition.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I ask how he felt about the whole episode. “No matter what your background was before you came into a job like this, you would have a lot to learn,” Roberts says. If you were a provost, you’d know nothing about “the business or finance or budgetary or political or operational or real estate sides of the university.” If you came from business, you’d need to learn the academic side.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a reasonable point. The modern university chancellor is part CEO, part diplomat, part fundraiser, part sports executive. Presumably, no one arrives with all the skills required. “I think almost no matter what you did previously before coming into a job like this, there would be a learning curve,” Roberts says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What strikes me about Roberts is that he seems relatively unbothered. The federal funding cuts are within the normal range. The Belichick hire is a wait-and-see situation. As for some of the faculty’s resistance to AI, it’s a puzzle to be solved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s also making big bets just as higher education is being squeezed every which way. Federal funding is uncertain. Birth rate declines threaten future enrollment. The value of a college degree is in question, with more students graduating to find the only jobs available to them are low-wage gigs they could have landed without spending staggering amounts on college. Now AI threatens to upend the whole model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Roberts sees opportunity where others might see a crisis. He also thinks the window of opportunity is shorter than some might imagine. “The challenge of AI is that we have to work relatively quickly, and we also have to cooperate across academic disciplines,” he says. “And those are two things that universities, historically, are not especially good at.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Whether Roberts’ game plan works remains to be seen. What’s clear is that he’s betting moving fast and shaking things up is better than moving slowly and preserving tradition at highly ranked UNC. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re going to try to make Carolina the number one public university in America,” he tells me.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s an ambitious vision, and as he delivers it, for better or worse, he sounds very much like a Silicon Valley CEO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;To hear this interview with Roberts, listen to TechCrunch’s StrictlyVC Download podcast; new episodes drop every Tuesday&lt;/em&gt;.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.13.38AM.png?resize=1200,807" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Lee Roberts meets me at the University Club of San Francisco on a Friday morning, hours before his football team will lose to Cal in heartbreaking fashion – a fumble at the goal line, because little about the University of North Carolina at Chapel Hill’s expensive experiment with Bill Belichick has gone according to script.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Roberts, the Chancellor of UNC, doesn’t know this yet. Right now, he’s in California to talk about artificial intelligence, which is both forward thinking and also – I’d hazard a guess – a welcome distraction from a lot else happening at the 235-year-old school.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“No one’s going to say to [students after they graduate from college], ‘Do the best job you can, but if you use AI, you’ll be in trouble,’” Roberts tells me, leaning into his central thesis about preparing students for the real world. “Yet we have some faculty members who are effectively saying that to students right now.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts has joined me in between other meetings in the city, including with AI companies, because UNC has decided to make AI its north star. It’s a business bet, really. Roberts spent three decades in finance, most recently as managing partner of a private investment firm, and served as state budget director under a Republican governor. He taught budgeting as an adjunct at Duke but never worked in academic administration before becoming UNC’s interim chancellor in January of last year, a post made permanent eight months later.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Never mind that the university just lost 118 federal grants totaling $38 million as part of a sweeping effort by the federal government to terminate more than 4,000 grants across 600 institutions. Never mind that more than 900 people last year signed a statement saying they wouldn’t recognize Roberts as chancellor when he was appointed, calling the process a political “coronation” rather than a search. Never mind that Belichick’s much-touted return to football is currently a 2-4 trainwreck, with write-ups about the team’s dysfunction becoming routine fodder for sports writers. Roberts is focused on the future.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;At UNC, Roberts explains, there’s a spectrum between faculty who are “leaning forward” with AI and those who have “their heads in the sand.” It’s diplomatic phrasing for what is clearly a culture war playing out in faculty lounges across UNC and – it’s probably safe to assume – at other schools across the world. While one UNC professor is assigning more research than students could complete without AI (“much closer to a real world scenario,” says Roberts), others are treating chatbots like anabolic steroids. If you use them, you’re cheating.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We have 4,000 faculty members,” Roberts says, as a cable car clatters past the open window beside our table. “And they pride themselves, as they should, on their independence and autonomy in how they teach their classes.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It sounds a little like code for: tenured professors can’t be forced to do anything. So Roberts is creating “incentive-based programs” to move the ball forward, like promoting one of the school’s deans into the role of Vice Provost for AI at the university. That individual, Jeffrey Bardzell, has been a professor for more than 20 years and has “experience both in technology and as a humanist,” says Roberts, adding that Bardzell is “exceptionally well-placed to help the faculty as a whole come further up to speed.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;UNC is barreling ahead on other fronts in the meantime. In its biggest development to date, the university announced this month that it is merging two schools – the School of Data Science and Society and the School of Information and Library Science – into one yet-to-be-named entity with AI studies at the center of the Venn diagram.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;UNC isn’t alone in betting big on AI. At least 14 colleges now offer bachelor’s degrees in AI, and universities like Arizona State University have made headlines for integrating AI tools across all disciplines.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Still, creating this new school has worried some of the school’s library science students, who wonder what will happen to their degrees, according to a report in The Daily Tar Heel, the school’s independent student newspaper. At least one faculty member also complained anonymously in a statement to the paper, saying Roberts pushed for the school without a “cogent idea” of what it will entail, adding that the “careers of faculty, staff and students at both of these schools are being sacrificed to Roberts’ ego.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts tells me the implementation will be collaborative, not top-down. He’s also clear that the move is proactive, not reactive. “This is not about shutting anything down,” he says. “It’s not predominantly a cost-savings move,” he continues, a possible nod to those lost federal research dollars, which amount to 3.5% of UNC’s overall research funding.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts doesn’t minimize the devastation of losing grants – “in many cases, [people] lose their life’s work,” he acknowledges – but he’s also quick to note that 3.5% is “well within our average annual variance.” He adds that he has been spending “a lot of time talking with policymakers and legislators in Washington about the tremendous good that federal research funding represents. We need to be especially vigilant right now, when there’s so much uncertainty around [these dollars] that it’s really changing the basic structure of how large research universities have been funded.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Of course, it raises questions about resources in the aggregate. Though UNC’s AI push is the topic du jour, I ask about the $10 million the school is paying Bill Belichick annually as part of a five-year deal signed back in January. I’m from Cleveland, I tell Roberts. I remember when Belichick cut Browns quarterback Bernie Kosar, a hometown hero. The city never forgave him.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Roberts is ready for this. College sports are changing rapidly, he says. Every peer institution spends at least as much on football; many spend more. Football drives revenue for 28 other sports. UNC just won its fourth national championship in women’s lacrosse, its 23rd in women’s soccer. None of that happens without football money.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“If we had hired somebody else and we were [down some games], everybody would be saying, ‘Hey, man, you could have had Bill Belichick,’” Roberts offers.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In reality, the prevailing narrative about Belichick isn’t just about wins and losses. Even if, ultimately, that’s exactly what it’s about, numerous outlets have published stories describing chaos inside the program, with players, parents, coaches, and administrators all painting a picture of a legendary NFL coach whose style doesn’t translate to college kids.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Roberts isn’t making decisions based on “a couple of news stories,” he says. “Coach Belichick, in my view, has done a really good job integrating with our campus,” Roberts says. He shows up at other teams’ games. He sends pizzas to fraternities on Saturday nights. He grew up on a college campus – his father was the coach at Navy.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Hours after our conversation, UNC will lose to Cal when wide receiver Nathan Leacock loses control of the ball just as he’s crossing into the end zone for what would have been the game-winning touchdown. I can only imagine what the immediate reaction is like back in Chapel Hill.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;My sense is that Roberts will brush it off. He may never be forgiven for not having a traditional academic background, but he also can’t afford to care that this bothers some people. I note that the 900-person petition took issue with the fact that, among the top 50 universities, Roberts is the only leader without higher education administration experience. The petition ran in The Daily Tar Heel, which has been critical of Roberts’ chancellorship throughout.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“I don’t think it was 900 students,” Roberts corrects me. “I think it was 900 people, regardless of whether they were students, faculty, staff, or just people in the world who signed an online petition.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;I ask how he felt about the whole episode. “No matter what your background was before you came into a job like this, you would have a lot to learn,” Roberts says. If you were a provost, you’d know nothing about “the business or finance or budgetary or political or operational or real estate sides of the university.” If you came from business, you’d need to learn the academic side.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s a reasonable point. The modern university chancellor is part CEO, part diplomat, part fundraiser, part sports executive. Presumably, no one arrives with all the skills required. “I think almost no matter what you did previously before coming into a job like this, there would be a learning curve,” Roberts says.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;What strikes me about Roberts is that he seems relatively unbothered. The federal funding cuts are within the normal range. The Belichick hire is a wait-and-see situation. As for some of the faculty’s resistance to AI, it’s a puzzle to be solved.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He’s also making big bets just as higher education is being squeezed every which way. Federal funding is uncertain. Birth rate declines threaten future enrollment. The value of a college degree is in question, with more students graduating to find the only jobs available to them are low-wage gigs they could have landed without spending staggering amounts on college. Now AI threatens to upend the whole model.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;But Roberts sees opportunity where others might see a crisis. He also thinks the window of opportunity is shorter than some might imagine. “The challenge of AI is that we have to work relatively quickly, and we also have to cooperate across academic disciplines,” he says. “And those are two things that universities, historically, are not especially good at.”&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Whether Roberts’ game plan works remains to be seen. What’s clear is that he’s betting moving fast and shaking things up is better than moving slowly and preserving tradition at highly ranked UNC. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re going to try to make Carolina the number one public university in America,” he tells me.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s an ambitious vision, and as he delivers it, for better or worse, he sounds very much like a Silicon Valley CEO.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;To hear this interview with Roberts, listen to TechCrunch’s StrictlyVC Download podcast; new episodes drop every Tuesday&lt;/em&gt;.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/the-man-betting-everything-on-ai-and-bill-belichick/</guid><pubDate>Mon, 20 Oct 2025 08:10:52 +0000</pubDate></item><item><title>Scale AI alum raises $9M for AI serving critical industries in MENA (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/scale-ai-alum-raises-9m-for-ai-serving-critical-industries-in-mena/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/IMG_2363.jpeg?resize=1200,896" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Bilal Abu-Ghazaleh had just moved to London few days before our call, splitting his time between there and Dubai.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After nearly a decade in the U.S., including a stint at Scale AI, he’s bringing that experience to his next venture: 1001 AI , a company creating AI infrastructure for critical industries across the Middle East and North Africa (MENA).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup recently raised a $9 million seed round led by CIV, General Catalyst, and Lux Capital. Other backers include global and regional angels such as Chris Ré, Amjad Masad (Replit), Amira Sajwani (DAMAC), Khalid Bin Bader Al Saud (RAED Ventures), and Hisham Alfalih (Lean Technologies).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abu-Ghazaleh said his two-month-old company promises to cut inefficiencies in high-stakes sectors like aviation, logistics, and oil and gas through an AI-native operating system for decision-making.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Just looking at the top three or four industries like airports, ports, construction, and oil and gas, we see more than $10 billion in inefficiencies across the Gulf alone,” the founder and CEO said in an interview with TechCrunch. “That’s just in markets like the UAE, Saudi Arabia, and Qatar. Even without counting other sectors, these industries represent a massive opportunity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, any efficiencies found in airport operations can compound the savings, impacting both the airport and its airlines. Meanwhile, he said nine out of ten of the regions mega-projects fall behind schedule or go over budget, meaning even small increases in efficiencies can save these projects serious money. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;1001 AI hopes to sell its decision-making AI to new projects after it launches its first product, which is scheduled by year’s end. The startup is in talks with some of the Gulf’s largest construction firms and airports, said Abu-Ghazaleh.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Born and raised in Jordan, Abu-Ghazaleh moved to the U.S. for college and later joined the Bay Area’s startup scene. After an early product role at computer vision startup Hive AI, he joined Scale AI in 2020 during its rapid expansion. There, he rose through the ranks from operations associate to director of the company’s GenAI operations, scaling its contributor network responsible for annotating and labeling training data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He was later set to join Scale’s international public sector unit, which builds AI solutions for foreign governments. But when Meta invested in Scale, the company shifted direction, and Abu-Ghazaleh left to found 1001 AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Gulf, particularly the UAE and Saudi Arabia, has become one of the world’s most aggressive adopters of AI. From sovereign-backed ventures like G42 in Abu Dhabi to Saudi Arabia’s National Center for AI, governments are investing billions to build local AI infrastructure and attract global talent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For Abu-Ghazaleh, that mix of appetite, budget, and urgency makes the region a perfect testing ground. But unlike most AI startups focused on software or enterprise tools, 1001 targets real-world physical operations, an area where the company’s investors believe the potential is even greater in the Middle East.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re extremely bullish on AI that solves physical-world problems at scale i.e, optimizing how airports turn around flights, how ports move cargo, how construction sites operate,” said Deena Shakir, partner at&amp;nbsp;Lux&amp;nbsp;Capital. “The MENA region offers significant potential in this space with mission-critical infrastructure that’s under-digitized and ripe for transformation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the product is still under development, Abu-Ghazaleh offered a glimpse into how it works. The system pulls in data from a client’s existing software, models operational workflows, and issues real-time directives to improve efficiency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today, an operations manager might manually call someone to reroute a fuel truck or send a cleaning crew to another gate,” said Abu-Ghazaleh. “With our system, that orchestration happens automatically. The AI orchestrator uses real-time data to reroute vehicles, reassign crews, and adjust operations without human intervention.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike most early-stage AI startups that target specific industries, Abu-Ghazaleh says 1001 can be accessible by many because operational flows across industries often look the same.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That model borrows from the rigor of consulting and contract work. The team spends weeks embedded with clients, running co-development sprints to tailor its systems to each operation’s realities, the CEO said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Bilal&amp;nbsp;is building the decision engine to automate that complexity with Scale-proven execution and the regional gravity to make 1001 the platform this market builds on,” commented Neeraj Arora, managing director at General Catalyst.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new funding will accelerate early deployments across aviation, logistics, and infrastructure, while fueling recruitment in engineering, operations, and go-to-market role as it grows its team across Dubai and London. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;1001 AI plans to launch its first customer deployment by the end of the year, starting with construction. Over the next five years, Abu-Ghazaleh wants the company to become the Gulf’s go-to orchestration layer for these industries before expanding globally.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/IMG_2363.jpeg?resize=1200,896" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Bilal Abu-Ghazaleh had just moved to London few days before our call, splitting his time between there and Dubai.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After nearly a decade in the U.S., including a stint at Scale AI, he’s bringing that experience to his next venture: 1001 AI , a company creating AI infrastructure for critical industries across the Middle East and North Africa (MENA).&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The startup recently raised a $9 million seed round led by CIV, General Catalyst, and Lux Capital. Other backers include global and regional angels such as Chris Ré, Amjad Masad (Replit), Amira Sajwani (DAMAC), Khalid Bin Bader Al Saud (RAED Ventures), and Hisham Alfalih (Lean Technologies).&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Abu-Ghazaleh said his two-month-old company promises to cut inefficiencies in high-stakes sectors like aviation, logistics, and oil and gas through an AI-native operating system for decision-making.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Just looking at the top three or four industries like airports, ports, construction, and oil and gas, we see more than $10 billion in inefficiencies across the Gulf alone,” the founder and CEO said in an interview with TechCrunch. “That’s just in markets like the UAE, Saudi Arabia, and Qatar. Even without counting other sectors, these industries represent a massive opportunity.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For example, any efficiencies found in airport operations can compound the savings, impacting both the airport and its airlines. Meanwhile, he said nine out of ten of the regions mega-projects fall behind schedule or go over budget, meaning even small increases in efficiencies can save these projects serious money. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;1001 AI hopes to sell its decision-making AI to new projects after it launches its first product, which is scheduled by year’s end. The startup is in talks with some of the Gulf’s largest construction firms and airports, said Abu-Ghazaleh.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Born and raised in Jordan, Abu-Ghazaleh moved to the U.S. for college and later joined the Bay Area’s startup scene. After an early product role at computer vision startup Hive AI, he joined Scale AI in 2020 during its rapid expansion. There, he rose through the ranks from operations associate to director of the company’s GenAI operations, scaling its contributor network responsible for annotating and labeling training data.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He was later set to join Scale’s international public sector unit, which builds AI solutions for foreign governments. But when Meta invested in Scale, the company shifted direction, and Abu-Ghazaleh left to found 1001 AI.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Gulf, particularly the UAE and Saudi Arabia, has become one of the world’s most aggressive adopters of AI. From sovereign-backed ventures like G42 in Abu Dhabi to Saudi Arabia’s National Center for AI, governments are investing billions to build local AI infrastructure and attract global talent.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;For Abu-Ghazaleh, that mix of appetite, budget, and urgency makes the region a perfect testing ground. But unlike most AI startups focused on software or enterprise tools, 1001 targets real-world physical operations, an area where the company’s investors believe the potential is even greater in the Middle East.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We’re extremely bullish on AI that solves physical-world problems at scale i.e, optimizing how airports turn around flights, how ports move cargo, how construction sites operate,” said Deena Shakir, partner at&amp;nbsp;Lux&amp;nbsp;Capital. “The MENA region offers significant potential in this space with mission-critical infrastructure that’s under-digitized and ripe for transformation.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While the product is still under development, Abu-Ghazaleh offered a glimpse into how it works. The system pulls in data from a client’s existing software, models operational workflows, and issues real-time directives to improve efficiency.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Today, an operations manager might manually call someone to reroute a fuel truck or send a cleaning crew to another gate,” said Abu-Ghazaleh. “With our system, that orchestration happens automatically. The AI orchestrator uses real-time data to reroute vehicles, reassign crews, and adjust operations without human intervention.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Unlike most early-stage AI startups that target specific industries, Abu-Ghazaleh says 1001 can be accessible by many because operational flows across industries often look the same.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;That model borrows from the rigor of consulting and contract work. The team spends weeks embedded with clients, running co-development sprints to tailor its systems to each operation’s realities, the CEO said.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“Bilal&amp;nbsp;is building the decision engine to automate that complexity with Scale-proven execution and the regional gravity to make 1001 the platform this market builds on,” commented Neeraj Arora, managing director at General Catalyst.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The new funding will accelerate early deployments across aviation, logistics, and infrastructure, while fueling recruitment in engineering, operations, and go-to-market role as it grows its team across Dubai and London. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;1001 AI plans to launch its first customer deployment by the end of the year, starting with construction. Over the next five years, Abu-Ghazaleh wants the company to become the Gulf’s go-to orchestration layer for these industries before expanding globally.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/scale-ai-alum-raises-9m-for-ai-serving-critical-industries-in-mena/</guid><pubDate>Mon, 20 Oct 2025 08:15:50 +0000</pubDate></item><item><title>Flowers of the future (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/20/1125345/plant-future-climate-change-research-project/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Flowers play a key role in most landscapes, from urban to rural areas. There might be dandelions poking through the cracks in the pavement, wildflowers on the highway median, or poppies covering a hillside. We might notice the time of year they bloom and connect that to our changing climate. Perhaps we are familiar with their cycles: bud, bloom, wilt, seed. Yet flowers have much more to tell in their bright blooms: The very shape they take is formed by local and global climate conditions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The form of a flower is a visual display of its climate, if you know what to look for. In a dry year, its petals’ pigmentation may change. In a warm year, the flower might grow bigger. The flower’s ultraviolet-absorbing pigment increases with higher ozone levels. As the climate changes in the future, how might flowers change?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="white flower and a purple flower" class="wp-image-1125929" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/antho.jpg?w=275" /&gt;&lt;figcaption class="wp-element-caption"&gt;Anthocyanins are red or indigo pigments that supply antioxidants and photoprotectants, which help a plant tolerate climate-related stresses such as droughts.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;© 2021 SULLIVAN CN, KOSKI MH&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;An artistic research project called Plant Futures imagines how a single species of flower might evolve in response to climate change between 2023 and 2100—and invites us to reflect on the complex, long-term impacts of our warming world. The project has created one flower for every year from 2023 to 2100. The form of each one is data-driven, based on climate projections and research into how climate influences flowers’ visual attributes.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="two rows of flowers that are both yellow and purple" class="wp-image-1125930" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-3-d.png?w=1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;More ultraviolet pigment protects flowers’ pollen against increasing ozone levels.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;MARCO TODESCO&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="a white flower with a yellow center" class="wp-image-1125931" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-3-e-Rosa__Kent__d.j.b_01.jpg?w=640" /&gt;&lt;figcaption class="wp-element-caption"&gt;Under unpredictable weather conditions, the speculative flowers grow a second layer of petals. In botany, a second layer is called a “double bloom” and arises from random mutations.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF ANNELIE BERNER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Plant Futures began during an artist residency in Helsinki, where I worked closely with the biologist Aku Korhonen to understand how climate change affected the local ecosystem. While exploring the primeval Haltiala forest, I learned of the &lt;em&gt;Circaea alpina&lt;/em&gt;, a tiny flower that was once rare in that area but has become more common as temperatures have risen in recent years. Yet its habitat is delicate: The plant requires shade and a moist environment, and the spruce population that provides those conditions is declining in the face of new forest pathogens. I wondered: What if the &lt;em&gt;Circaea alpina &lt;/em&gt;could survive in spite of climate uncertainty? If the dark, shaded bogs turn into bright meadows and the wet ground dries out, how might the flower adapt in order to survive? This flower’s potential became the project’s grounding point.&amp;nbsp;&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1125926" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-2_herbarium.jpg?w=2221" width="2221" /&gt;&lt;figcaption class="wp-element-caption"&gt;The author studying historical &lt;em&gt;Circaea&lt;/em&gt; samples in the Luomus Botanical Collections.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF ANNELIE BERNER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Outside the forest, I worked with botanical experts in the Luomus Botanical Collections. I studied samples of &lt;em&gt;Circaea &lt;/em&gt;flowers from as far back as 1906, and I researched historical climate conditions in an attempt to understand how flower size and color related to a year’s temperature and precipitation patterns.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I researched how other flowering plants respond to changes to their climate conditions and wondered how the &lt;em&gt;Circaea&lt;/em&gt; would need to adapt to thrive in a future world. If such changes happened, what would the &lt;em&gt;Circaea&lt;/em&gt; look like in 2100?&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1125939" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-4_technical-interface.jpg" /&gt;&lt;figcaption class="wp-element-caption"&gt;We designed the future flowers through a combination of data-driven algorithmic mapping and artistic control. I worked with the data artist Marcin Ignac from Variable Studio to create 3D flowers whose appearance was connected to climate data. Using Nodes.io, we made a 3D model of the &lt;em&gt;Circaea alpina&lt;/em&gt; based on its current morphology and then mapped how those physical parameters might shift as the climate changes. For example, as the temperature rises and precipitation decreases in the data set, the petal color shifts toward red, reflecting how flowers protect themselves with an increase in anthocyanins. Changes in temperature, carbon dioxide levels, and precipitation rates combine to affect the flowers’ size, density of veins, UV pigments, color, and tendency toward double bloom.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125943" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-6_2025.png?w=2914" /&gt;&lt;figcaption class="wp-element-caption"&gt;2025: &lt;em&gt;Circaea alpina&lt;/em&gt; is ever so slightly larger than usual owing to a warmer summer, but it is otherwise close to the typical &lt;em&gt;Circaea&lt;/em&gt; flower in size, color, and other attributes.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125944" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-7_2064.png?w=2915" /&gt;&lt;figcaption class="wp-element-caption"&gt;2064: We see a bigger flower with more petals, given an increase in carbon dioxide levels and temperature. The bull’s-eye pattern, composed of UV pigment, is bigger and messier because of an increase in ozone and solar radiation. A second tier of petals reflects uncertainty in the climate model.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125945" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-8_2074.png?w=2915" /&gt;&lt;figcaption class="wp-element-caption"&gt;2074: The flower becomes pinker, an antioxidative response to the stress of consecutive dry days and higher temperatures. Its size increases, primarily because of higher levels of carbon dioxide. The double bloom of petals persists as the climate model’s projections increase in uncertainty.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125946" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-9_2100.png?w=2915" /&gt;&lt;figcaption class="wp-element-caption"&gt;2100: The flower’s veins are densely packed, which could signal appropriation of a technique leaves use to improve water transport during droughts. It could also be part of a strategy to attract pollinators in the face of worsening air quality that degrades the transmission of scents.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125949" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-10_variable-flowers.png?w=1639" width="1639" /&gt;&lt;figcaption class="wp-element-caption"&gt;2023—2100: Each year, the speculative flower changes. Size, color, and form shift in accordance with the increased temperature and carbon dioxide levels and the changes in precipitation patterns.&lt;/figcaption&gt;&lt;/figure&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1125952" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-11_alternative-sculpture.jpg?w=2116" width="2116" /&gt;&lt;figcaption class="wp-element-caption"&gt;In this 10-centimeter cube of plexiglass, the future flowers are “preserved,” allowing the viewer to see them in a comparative, layered view.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF ANNELIE BERNER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;Based in Copenhagen, Annelie Berner is a designer, researcher, teacher, and artist specializing in data visualization.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Flowers play a key role in most landscapes, from urban to rural areas. There might be dandelions poking through the cracks in the pavement, wildflowers on the highway median, or poppies covering a hillside. We might notice the time of year they bloom and connect that to our changing climate. Perhaps we are familiar with their cycles: bud, bloom, wilt, seed. Yet flowers have much more to tell in their bright blooms: The very shape they take is formed by local and global climate conditions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The form of a flower is a visual display of its climate, if you know what to look for. In a dry year, its petals’ pigmentation may change. In a warm year, the flower might grow bigger. The flower’s ultraviolet-absorbing pigment increases with higher ozone levels. As the climate changes in the future, how might flowers change?&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image alignright size-large"&gt;&lt;img alt="white flower and a purple flower" class="wp-image-1125929" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/antho.jpg?w=275" /&gt;&lt;figcaption class="wp-element-caption"&gt;Anthocyanins are red or indigo pigments that supply antioxidants and photoprotectants, which help a plant tolerate climate-related stresses such as droughts.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;© 2021 SULLIVAN CN, KOSKI MH&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;An artistic research project called Plant Futures imagines how a single species of flower might evolve in response to climate change between 2023 and 2100—and invites us to reflect on the complex, long-term impacts of our warming world. The project has created one flower for every year from 2023 to 2100. The form of each one is data-driven, based on climate projections and research into how climate influences flowers’ visual attributes.&amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="two rows of flowers that are both yellow and purple" class="wp-image-1125930" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-3-d.png?w=1280" /&gt;&lt;figcaption class="wp-element-caption"&gt;More ultraviolet pigment protects flowers’ pollen against increasing ozone levels.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;MARCO TODESCO&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt;&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="a white flower with a yellow center" class="wp-image-1125931" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-3-e-Rosa__Kent__d.j.b_01.jpg?w=640" /&gt;&lt;figcaption class="wp-element-caption"&gt;Under unpredictable weather conditions, the speculative flowers grow a second layer of petals. In botany, a second layer is called a “double bloom” and arises from random mutations.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF ANNELIE BERNER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Plant Futures began during an artist residency in Helsinki, where I worked closely with the biologist Aku Korhonen to understand how climate change affected the local ecosystem. While exploring the primeval Haltiala forest, I learned of the &lt;em&gt;Circaea alpina&lt;/em&gt;, a tiny flower that was once rare in that area but has become more common as temperatures have risen in recent years. Yet its habitat is delicate: The plant requires shade and a moist environment, and the spruce population that provides those conditions is declining in the face of new forest pathogens. I wondered: What if the &lt;em&gt;Circaea alpina &lt;/em&gt;could survive in spite of climate uncertainty? If the dark, shaded bogs turn into bright meadows and the wet ground dries out, how might the flower adapt in order to survive? This flower’s potential became the project’s grounding point.&amp;nbsp;&lt;/p&gt; 
&lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1125926" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-2_herbarium.jpg?w=2221" width="2221" /&gt;&lt;figcaption class="wp-element-caption"&gt;The author studying historical &lt;em&gt;Circaea&lt;/em&gt; samples in the Luomus Botanical Collections.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF ANNELIE BERNER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Outside the forest, I worked with botanical experts in the Luomus Botanical Collections. I studied samples of &lt;em&gt;Circaea &lt;/em&gt;flowers from as far back as 1906, and I researched historical climate conditions in an attempt to understand how flower size and color related to a year’s temperature and precipitation patterns.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;I researched how other flowering plants respond to changes to their climate conditions and wondered how the &lt;em&gt;Circaea&lt;/em&gt; would need to adapt to thrive in a future world. If such changes happened, what would the &lt;em&gt;Circaea&lt;/em&gt; look like in 2100?&amp;nbsp;&lt;/p&gt;  &lt;figure class="wp-block-image size-full"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1125939" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-4_technical-interface.jpg" /&gt;&lt;figcaption class="wp-element-caption"&gt;We designed the future flowers through a combination of data-driven algorithmic mapping and artistic control. I worked with the data artist Marcin Ignac from Variable Studio to create 3D flowers whose appearance was connected to climate data. Using Nodes.io, we made a 3D model of the &lt;em&gt;Circaea alpina&lt;/em&gt; based on its current morphology and then mapped how those physical parameters might shift as the climate changes. For example, as the temperature rises and precipitation decreases in the data set, the petal color shifts toward red, reflecting how flowers protect themselves with an increase in anthocyanins. Changes in temperature, carbon dioxide levels, and precipitation rates combine to affect the flowers’ size, density of veins, UV pigments, color, and tendency toward double bloom.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125943" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-6_2025.png?w=2914" /&gt;&lt;figcaption class="wp-element-caption"&gt;2025: &lt;em&gt;Circaea alpina&lt;/em&gt; is ever so slightly larger than usual owing to a warmer summer, but it is otherwise close to the typical &lt;em&gt;Circaea&lt;/em&gt; flower in size, color, and other attributes.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125944" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-7_2064.png?w=2915" /&gt;&lt;figcaption class="wp-element-caption"&gt;2064: We see a bigger flower with more petals, given an increase in carbon dioxide levels and temperature. The bull’s-eye pattern, composed of UV pigment, is bigger and messier because of an increase in ozone and solar radiation. A second tier of petals reflects uncertainty in the climate model.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125945" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-8_2074.png?w=2915" /&gt;&lt;figcaption class="wp-element-caption"&gt;2074: The flower becomes pinker, an antioxidative response to the stress of consecutive dry days and higher temperatures. Its size increases, primarily because of higher levels of carbon dioxide. The double bloom of petals persists as the climate model’s projections increase in uncertainty.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125946" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-9_2100.png?w=2915" /&gt;&lt;figcaption class="wp-element-caption"&gt;2100: The flower’s veins are densely packed, which could signal appropriation of a technique leaves use to improve water transport during droughts. It could also be part of a strategy to attract pollinators in the face of worsening air quality that degrades the transmission of scents.&lt;/figcaption&gt;&lt;/figure&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1125949" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-10_variable-flowers.png?w=1639" width="1639" /&gt;&lt;figcaption class="wp-element-caption"&gt;2023—2100: Each year, the speculative flower changes. Size, color, and form shift in accordance with the increased temperature and carbon dioxide levels and the changes in precipitation patterns.&lt;/figcaption&gt;&lt;/figure&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1125952" height="2000" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/image-11_alternative-sculpture.jpg?w=2116" width="2116" /&gt;&lt;figcaption class="wp-element-caption"&gt;In this 10-centimeter cube of plexiglass, the future flowers are “preserved,” allowing the viewer to see them in a comparative, layered view.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY OF ANNELIE BERNER&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;Based in Copenhagen, Annelie Berner is a designer, researcher, teacher, and artist specializing in data visualization.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/20/1125345/plant-future-climate-change-research-project/</guid><pubDate>Mon, 20 Oct 2025 10:00:00 +0000</pubDate></item><item><title>AI could predict who will have a heart attack (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/20/1125336/ai-heart-attack-prediction/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AdobeStock_632825639.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;For all the modern marvels of cardiology, we struggle to predict who will have a heart attack. Many people never get screened at all. Now, startups like Bunkerhill Health, Nanox.AI, and HeartLung Technologies are applying AI algorithms to screen millions of CT scans for early signs of heart disease. This technology could be a breakthrough for public health, applying an old tool to uncover patients whose high risk for a heart attack is hiding in plain sight. But it remains unproven at scale while raising thorny questions about implementation and even how we define disease.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last year, an estimated 20 million Americans had chest CT scans done, after an event like a car accident or to screen for lung cancer. Frequently, they show evidence of coronary artery calcium (CAC), a marker for heart attack risk, that is buried or not mentioned in a radiology report focusing on ruling out bony injuries, life-threatening internal trauma, or cancer.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Dedicated testing for CAC remains an underutilized method of predicting heart attack risk. Over decades, plaque in heart arteries moves through its own life cycle, hardening from lipid-rich residue into calcium. Heart attacks themselves typically occur when younger, lipid-rich plaque unpredictably ruptures, kicking off a clotting cascade of inflammation that ultimately blocks the heart’s blood supply. Calcified plaque is generally stable, but finding CAC suggests that younger, more rupture-prone plaque is likely present too.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Coronary artery calcium can often be spotted on chest CTs, and its concentration can be subjectively described. Normally, quantifying a person’s CAC score involves obtaining a heart-specific CT scan. Algorithms that calculate CAC scores from routine chest CTs, however, could massively expand access to this metric. In practice, these algorithms could then be deployed to alert patients and their doctors about abnormally high scores, encouraging them to seek further care. Today, the footprint of the startups offering AI-derived CAC scores is not large, but it is growing quickly. As their use grows, these algorithms may identify high-risk patients who are traditionally missed or who are on the margins of care.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Historically, CAC scans were believed to have marginal benefit and were marketed to the worried well. Even today, most insurers won’t cover them. Attitudes, though, may be shifting. More expert groups are endorsing CAC scores as a way to refine cardiovascular risk estimates and persuade skeptical patients to start taking statins.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The promise of AI-derived CAC scores is part of a broader trend toward mining troves of medical data to spot otherwise undetected disease. But while it seems promising, the practice raises plenty of questions. For example, CAC scores ­haven’t proved useful as a blunt instrument for universal screening. A 2022 Danish study evaluating a population-based program, for example, showed no benefit in mortality rates for patients who had undergone CAC screening tests. If AI delivered this information automatically, would the calculus really shift?&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;And with widespread adoption, abnormal CAC scores will become common. Who follows up on these findings? “Many health systems aren’t yet set up to act on incidental calcium findings at scale,” says Nishith Khandwala, the cofounder of Bunkerhill Health. Without a standard procedure for doing so, he says, “you risk creating more work than value.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;There’s also the question of whether these AI-generated scores would actually improve patient care. For a symptomatic patient, a CAC score of zero may offer false reassurance. For the asymptomatic patient with a high CAC score, the next steps remain uncertain. Beyond statins, it isn’t clear if these patients would benefit from starting costly cholesterol-lowering drugs such as Repatha or other PCSK9-inhibitors. It may encourage some to pursue unnecessary but costly downstream procedures that could even end up doing harm. Currently, AI-derived CAC scoring is not reimbursed as a separate service by Medicare or most insurers. The business case for this technology today, effectively, lies in these potentially perverse incentives.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At a fundamental level, this approach could actually change how we define disease. Adam Rodman, a hospitalist and AI expert at Beth Israel Deaconess Medical Center in Boston, has observed that AI-derived CAC scores share similarities with the “incidentaloma,” a term coined in the 1980s to describe unexpected findings on CT scans. In both cases, the normal pattern of diagnosis—in which doctors and patients deliberately embark on testing to figure out what’s causing a specific problem—were fundamentally disrupted. But, as Rodman notes, incidentalomas were still found by humans reviewing the scans.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now, he says, we are entering an era of “machine-based nosology,” where algorithms define diseases on their own terms. As machines make more diagnoses, they may catch things we miss. But Rodman and I began to wonder if a two-tiered diagnostic future may emerge, where “haves” pay for brand-name algorithms while “have-nots” settle for lesser alternatives.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For patients who have no risk factors or are detached from regular medical care, an AI-derived CAC score could potentially catch problems earlier and rewrite the script. But how these scores reach people, what is done about them, and whether they can ultimately improve patient outcomes at scale remain open questions. For now—holding the pen as they toggle between patients and algorithmic outputs—clinicians still matter.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Vishal Khetpal is a fellow in cardiovascular disease. The views expressed in this article do not represent those of his employers.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/AdobeStock_632825639.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;For all the modern marvels of cardiology, we struggle to predict who will have a heart attack. Many people never get screened at all. Now, startups like Bunkerhill Health, Nanox.AI, and HeartLung Technologies are applying AI algorithms to screen millions of CT scans for early signs of heart disease. This technology could be a breakthrough for public health, applying an old tool to uncover patients whose high risk for a heart attack is hiding in plain sight. But it remains unproven at scale while raising thorny questions about implementation and even how we define disease.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Last year, an estimated 20 million Americans had chest CT scans done, after an event like a car accident or to screen for lung cancer. Frequently, they show evidence of coronary artery calcium (CAC), a marker for heart attack risk, that is buried or not mentioned in a radiology report focusing on ruling out bony injuries, life-threatening internal trauma, or cancer.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_3"&gt; &lt;p&gt;Dedicated testing for CAC remains an underutilized method of predicting heart attack risk. Over decades, plaque in heart arteries moves through its own life cycle, hardening from lipid-rich residue into calcium. Heart attacks themselves typically occur when younger, lipid-rich plaque unpredictably ruptures, kicking off a clotting cascade of inflammation that ultimately blocks the heart’s blood supply. Calcified plaque is generally stable, but finding CAC suggests that younger, more rupture-prone plaque is likely present too.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Coronary artery calcium can often be spotted on chest CTs, and its concentration can be subjectively described. Normally, quantifying a person’s CAC score involves obtaining a heart-specific CT scan. Algorithms that calculate CAC scores from routine chest CTs, however, could massively expand access to this metric. In practice, these algorithms could then be deployed to alert patients and their doctors about abnormally high scores, encouraging them to seek further care. Today, the footprint of the startups offering AI-derived CAC scores is not large, but it is growing quickly. As their use grows, these algorithms may identify high-risk patients who are traditionally missed or who are on the margins of care.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Historically, CAC scans were believed to have marginal benefit and were marketed to the worried well. Even today, most insurers won’t cover them. Attitudes, though, may be shifting. More expert groups are endorsing CAC scores as a way to refine cardiovascular risk estimates and persuade skeptical patients to start taking statins.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The promise of AI-derived CAC scores is part of a broader trend toward mining troves of medical data to spot otherwise undetected disease. But while it seems promising, the practice raises plenty of questions. For example, CAC scores ­haven’t proved useful as a blunt instrument for universal screening. A 2022 Danish study evaluating a population-based program, for example, showed no benefit in mortality rates for patients who had undergone CAC screening tests. If AI delivered this information automatically, would the calculus really shift?&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;And with widespread adoption, abnormal CAC scores will become common. Who follows up on these findings? “Many health systems aren’t yet set up to act on incidental calcium findings at scale,” says Nishith Khandwala, the cofounder of Bunkerhill Health. Without a standard procedure for doing so, he says, “you risk creating more work than value.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_5"&gt;&lt;p&gt;There’s also the question of whether these AI-generated scores would actually improve patient care. For a symptomatic patient, a CAC score of zero may offer false reassurance. For the asymptomatic patient with a high CAC score, the next steps remain uncertain. Beyond statins, it isn’t clear if these patients would benefit from starting costly cholesterol-lowering drugs such as Repatha or other PCSK9-inhibitors. It may encourage some to pursue unnecessary but costly downstream procedures that could even end up doing harm. Currently, AI-derived CAC scoring is not reimbursed as a separate service by Medicare or most insurers. The business case for this technology today, effectively, lies in these potentially perverse incentives.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;At a fundamental level, this approach could actually change how we define disease. Adam Rodman, a hospitalist and AI expert at Beth Israel Deaconess Medical Center in Boston, has observed that AI-derived CAC scores share similarities with the “incidentaloma,” a term coined in the 1980s to describe unexpected findings on CT scans. In both cases, the normal pattern of diagnosis—in which doctors and patients deliberately embark on testing to figure out what’s causing a specific problem—were fundamentally disrupted. But, as Rodman notes, incidentalomas were still found by humans reviewing the scans.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Now, he says, we are entering an era of “machine-based nosology,” where algorithms define diseases on their own terms. As machines make more diagnoses, they may catch things we miss. But Rodman and I began to wonder if a two-tiered diagnostic future may emerge, where “haves” pay for brand-name algorithms while “have-nots” settle for lesser alternatives.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;For patients who have no risk factors or are detached from regular medical care, an AI-derived CAC score could potentially catch problems earlier and rewrite the script. But how these scores reach people, what is done about them, and whether they can ultimately improve patient outcomes at scale remain open questions. For now—holding the pen as they toggle between patients and algorithmic outputs—clinicians still matter.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Vishal Khetpal is a fellow in cardiovascular disease. The views expressed in this article do not represent those of his employers.&amp;nbsp;&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/20/1125336/ai-heart-attack-prediction/</guid><pubDate>Mon, 20 Oct 2025 10:00:00 +0000</pubDate></item><item><title>Should an AI copy of you help decide if you live or die? (AI – Ars Technica)</title><link>https://arstechnica.com/features/2025/10/should-an-ai-copy-of-you-help-decide-if-you-live-or-die/</link><description>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Doctors share top concerns of AI surrogates aiding life-or-death decisions.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/pull-the-plug.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For more than a decade, researchers have wondered whether artificial intelligence could help predict what incapacitated patients might want when doctors must make life-or-death decisions on their behalf.&lt;/p&gt;
&lt;p&gt;It remains one of the most high-stakes questions in health care AI today. But as AI improves, some experts increasingly see it as inevitable that digital “clones” of patients could one day aid family members, doctors, and ethics boards in making end-of-life decisions that are aligned with a patient’s values and goals.&lt;/p&gt;
&lt;p&gt;Ars spoke with experts conducting or closely monitoring this research who confirmed that no hospital has yet deployed so-called “AI surrogates.” But AI researcher Muhammad Aurangzeb Ahmad is aiming to change that, taking the first steps toward piloting AI surrogates at a US medical facility.&lt;/p&gt;
&lt;p&gt;“This is very brand new, so very few people are working on it,” Ahmad told Ars.&lt;/p&gt;
&lt;p&gt;Ahmad is a resident fellow working with trauma department faculty at the University of Washington’s UW Medicine. His research is based at Harborview Medical Center in Seattle, a public hospital in the UW Medicine health system. UW Medicine is integrated with “one of the world’s largest medical research programs” to pursue its mission of improving public health outcomes, UW’s website says.&lt;/p&gt;
&lt;p&gt;UW wasn’t specifically seeking a fellow to experiment with AI surrogates, Ahmad told Ars. But since his project proposal was accepted, he has spent most of this year “in the conceptual phase,” working toward testing the accuracy of AI models based on Harborview patient data.&lt;/p&gt;
&lt;p&gt;The main limitation of this testing, Ahmad said, is that he can only verify the accuracy of his models if patients survive and can later confirm that the model made the right choice. But this is just the first step, he said. The accuracy testing could then expand to other facilities in the network, with the aim of developing AI surrogates that can accurately predict patient preferences about “two-thirds” of the time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Currently, Ahmad’s models are focused on analyzing data that Harborview already collects, such as injury severity, medical history, prior medical choices, and demographic information.&lt;/p&gt;
&lt;p&gt;“We use that information, feed it to a machine learning predictive model, and then in the retrospective data, we observe how well the model is doing,” Ahmad said.&lt;/p&gt;
&lt;p&gt;No patient has yet interacted with Ahmad’s models, he confirmed. UW Medicine spokesperson Susan Gregg told Ars there’s “considerable work to complete prior to launch,” and the system “would be approved only after a multiple-stage review process.”&lt;/p&gt;
&lt;p&gt;“We have not enrolled any patients at Harborview,” Ahmad said. “We are still at the phase of defining the scope and what theoretical considerations to take into account. It will be some time before it gets off the ground, given the challenges involved.”&lt;/p&gt;
&lt;p&gt;In the future, though, Ahmad envisions models that would also analyze textual data, perhaps from patient-approved recorded conversations with their doctors, to inform their AI copy’s predictions. In that world, trusted human surrogates, such as family members, could provide other textual data from chats or texts with the patient. In the technology’s most “ideal” form, Ahmad sees patients interacting with AI systems throughout their lives, providing feedback to refine models as the patients age through the health system.&lt;/p&gt;
&lt;p&gt;“It takes time to get the relevant data,” Ahmad said.&lt;/p&gt;
&lt;p&gt;Before patients could begin interacting with AI surrogates, any human subject testing would need to be approved by an institutional review board (IRB), Ahmad said.&lt;/p&gt;
&lt;p&gt;Ultimately, he expects that AI surrogates won’t be a perfect model but rather a set of rigorously tested systems that doctors and loved ones can consult when assessing all the known information about what a patient would want in critical moments.&lt;/p&gt;
&lt;p&gt;Whether hospitals would ever adopt such a system is unclear. “Within this space, practitioners are more conservative, and I would even argue that’s rightfully so,” Ahmad said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Gregg told Ars that UW Medicine supports the “thoughtful exploration of innovative ideas, such as the potential responsible and transparent use of AI surrogates in end-of-life care,” as they reflect “our commitment to advancing both science and compassion in medicine.”&lt;/p&gt;
&lt;p&gt;“While end-of-life decision-making represents a particularly complex area, we view these decisions as essential to addressing important questions, such as how to best honor patient wishes when they may be unable to communicate them directly or have no next of kin to do so on their behalf,” Gregg said.&lt;/p&gt;
&lt;h2&gt;Is AI a bad fit for patients with no human surrogates?&lt;/h2&gt;
&lt;p&gt;It has always been hard for doctors to determine what patients want when they can’t speak for themselves. A patient may refuse to be put on a ventilator or receive dialysis or cardiopulmonary resuscitation (CPR) if, for example, they’ve expressed that they want to avoid discomfort at the end of their life. Others may fear complications like infections or have no desire to rely on a machine for life support. Some patients, like young people involved in accidents, may have never expressed preferences.&lt;/p&gt;
&lt;p&gt;Emily Moin, a physician in an intensive care unit in Pennsylvania, told Ars that time is a factor in these decisions, but it’s imperative that a human surrogate who may better understand the patient’s wishes be involved.&lt;/p&gt;
&lt;p&gt;“When we’re in one of these fast-paced situations where we don’t know, but we have a patient in front of us who has died, we will err on the side of providing [CPR] until we are able to arrive at the clinical judgment that that effort is no longer indicated or until we’re able to engage with a surrogate decision maker,” Moin explained.&lt;/p&gt;
&lt;p&gt;Reaching the surrogate, she said, is “an important part of taking care of someone.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ahmad hopes that AI could help alleviate stress in uncertain moments. For doctors and surrogates, these decisions can be “very emotionally taxing,” Ahmad told Ars, leading many people to second-guess what the patient would choose. Some studies have shown that surrogates often get it wrong, he said, and he believes AI could help improve the odds of success.&lt;/p&gt;
&lt;p&gt;Seeking to nip this problem in the bud, health systems have historically pushed patients to complete “advanced directives” to log their preferences. Over time, though, it has become clear that patients’ preferences tend to be unstable, sometimes changing within days.&lt;/p&gt;
&lt;p&gt;Doctors must also consider that some patients have no stated preferences. Others, Moin said, have reported that their preferences changed after receiving lifesaving treatments because they now know what to expect. There are likely other limitations of Ahmad’s planned testing, which would determine accuracy by checking whether the AI’s decision matches what a patient says they would have wanted after recovery, Moin said.&lt;/p&gt;
&lt;p&gt;“These decisions are dynamically constructed and context-dependent,” Moin said. “And if you’re assessing the performance of the model based on asking someone after they’ve recovered what they would have said before they recovered, that’s not going to provide you with an accurate representation.”&lt;/p&gt;
&lt;p&gt;Moin said one of the big problems with medical AI is that people expect it to “provide better predictions than what we’re currently able to generate.” But the models are being trained on “convenient ground truths,” she said, that don’t “provide meaningful examples for models to learn about the situations” where the models would be employed.&lt;/p&gt;
&lt;p&gt;“I imagine that they would actually want to deploy this model to help to make decisions for unrepresented patients, patients who can’t communicate, patients who don’t have a surrogate,” Moin said, “but those are exactly the patients where you’ll never be able to know what the so-called ground truth is, and then you’ll never be able to assess your bias, and you’ll never be able to assess your model’s performance.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Family members may default to agreeing with AI&lt;/h2&gt;
&lt;p&gt;Culturally, the US has shifted from being “very focused on patient autonomy” to “more of a shared decision-making and, at times, family- and community-focused lens” as the standard for making these difficult decisions, Moin said.&lt;/p&gt;
&lt;p&gt;The longer a doctor knows a patient, and the more conversations a patient’s health team has with family members, the more likely it is for health systems to be able to adapt to respect the patient’s wishes over time, Moin suggested.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;That idea echoes Ahmad’s “ideal” AI surrogate model. But Moin said that if patients talk to an AI, it could actually discourage them from having important conversations with family members. Studies have found that if a patient fills out advanced directives, it can become harder to determine their preferences, Moin said, because patients may be less likely to discuss their preferences with loved ones.&lt;/p&gt;
&lt;p&gt;Earlier this year, Moin urged human surrogates to remain closely involved in do-not-resuscitate orders, writing that doctors who unilaterally make these decisions have an ethical obligation to “ensure that patients and surrogate decision-makers are aware that the decision has been made” and face “the lowest of barriers” to expressing disagreement.&lt;/p&gt;
&lt;p&gt;“Forgoing CPR is one of the most consequential treatment decisions a patient or surrogate can make because, if invoked, it will necessarily lead to death,” Moin wrote.&lt;/p&gt;
&lt;p&gt;Moin told Ars she hopes an AI surrogate’s outputs would never be weighted more than a human surrogate’s opinion, which is based on lived experience with a patient. “But I do worry that there could be culture shifts and other pressures that would encourage clinicians and family members, for that matter, to lean on products like these more heavily,” she said.&lt;/p&gt;
&lt;p&gt;“I can imagine a scenario where, say, a doctor is expected to round on 24 critically ill patients in one day, and the family member is resistant to sitting down for a conversation,” Moin said. “So yeah, maybe all parties involved would default to the shortcut of incorporating the information from this model.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Moin called for more public awareness and debate on AI surrogates, noting that “people really hate” the use of algorithms to determine who gets care.&lt;/p&gt;
&lt;p&gt;“I don’t think that it would be good for patients or clinicians or society for that matter,” Moin said.&lt;/p&gt;
&lt;p&gt;She’s particularly worried that “patients who can’t speak for themselves and who don’t have a clear loved one” would be “the ones who would be most vulnerable to suffering harms” of AI surrogates making wrong calls. Too many such mistakes could further erode trust in health systems, Moin said.&lt;/p&gt;
&lt;h2&gt;AI surrogates may be redundant&lt;/h2&gt;
&lt;p&gt;These decisions are “psychosocially fraught” for everyone involved, Teva Brender, a hospitalist at a medical center for veterans in San Francisco, told Ars. That’s why testing like Ahmad’s is important, he said.&lt;/p&gt;
&lt;p&gt;Last year, Brender co-authored an opinion piece&amp;nbsp;noting “how difficult it can be for families to make decisions for incapacitated patients,” particularly in geriatrics, palliative, and critical care settings.&lt;/p&gt;
&lt;p&gt;“For many, the notion of incorporating AI into goals-of-care conversations will conjure nightmarish visions of a dystopian future wherein we entrust deeply human decisions to algorithms,” Brender’s team wrote. “We share these apprehensions.”&lt;/p&gt;
&lt;p&gt;But with doctors’ and surrogates’ predictions facing significant limitations, “it behooves us to consider how AI could be safely, ethically, and equitably deployed to help surrogates for individuals who are seriously ill,” Brender’s team concluded.&lt;/p&gt;
&lt;p&gt;And it’s “equally important,” Brender told Ars, to help patients choose surrogates and prepare them to substitute their judgments.&lt;/p&gt;
&lt;p&gt;Brender believes Ahmad’s research is worthwhile since there are “lots of questions” requiring scientific research. But he’s “glad to hear” that AI surrogates are “not actually being used among patients” at Harborview yet. “I can’t imagine that an IRB would approve such a thing this early,” he told Ars.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And AI surrogates may end up playing a redundant role, leading this potential use for AI to fall out of favor, Brender said.&lt;/p&gt;
&lt;p&gt;“The devil’s advocate perspective,” Brender said, is that AI surrogates would just be doing “what a good clinician does anyway,” which is to ask surrogates, “Hey, who was this person? What did they enjoy doing? What brought meaning to their life?”&lt;/p&gt;
&lt;p&gt;“Do you need an AI to do that?” Brender asked. “I’m not so sure.”&lt;/p&gt;
&lt;h2&gt;AI can’t replace human surrogates, doctors warn&lt;/h2&gt;
&lt;p&gt;Last month, bioethics expert Robert Truog joined R. Sean Morrison, a doctor dedicated to advancing palliative care aimed at improving the quality of life for people suffering life-threatening illnesses, in emphasizing that AI should never replace human surrogates in resuscitation decisions.&lt;/p&gt;
&lt;p&gt;“Decisions about hypothetical scenarios do not correlate with decisions that need to be made in real time,” Morrison, who is chair of the Brookdale Department of Geriatrics and Palliative Medicine at Mount Sinai, told Ars. “AI cannot fix this fundamental issue—it is not a matter of better prediction. Patients’ preferences often represent a snapshot in time that are simply not predictive of the future.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The warning came after Georg Starke, a doctor and senior research associate at the Chair for Ethics of AI and Neuroscience at the Technical University of Munich, co-authored a proof-of-concept showing that three AI models, on average, performed better than human surrogates in predicting patient preferences.&lt;/p&gt;
&lt;p&gt;Starke’s study relied on existing data from Swiss respondents of a European survey that tracked population health trends of individuals over 50 years old. The dataset offered “comprehensive information on participants’ end-of-life preferences, including questions concerning” CPR. That allowed the team to build three models: a simple model, a model based on commonly available electronic health records, and a more “personalized” model. Each model successfully predicted whether a patient experiencing cardiac arrest would want CPR, with an accuracy of up to 70 percent.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;His team’s research was intended to “ground a long-standing ethical debate in empirical data,” Starke told Ars.&lt;/p&gt;
&lt;p&gt;“For over a decade, people have speculated about using algorithms to improve clinical decision-making for incapacitated patients, but no one had shown whether such a program could actually be designed,” Starke said. “Our study was meant to test if it’s feasible, explore how well it performs, identify which factors influence the models’ decisions, and spark a broader debate about the technology.”&lt;/p&gt;
&lt;p&gt;A key limitation of AI models depending on “‘accuracy’ alone”—especially if that “accuracy” is “achieved by chance or by pattern-matching purely demographic data outside an individual’s control”—is that the outputs don’t “necessarily reflect an autonomous choice,” Starke said.&lt;/p&gt;
&lt;p&gt;Like Truog and Morrison, Starke’s team emphasized that “human surrogates will remain essential sources for the contextual aspects of specific situations,” particularly with patients with dementia, and agreed that AI models “should not replace surrogate decision-making.”&lt;/p&gt;
&lt;h2&gt;Chatbot surrogates could be bad&lt;/h2&gt;
&lt;p&gt;Human surrogates may grow to trust AI systems in the future, but “it’s all about how the information is presented,” Brender, the hospitalist, told Ars.&lt;/p&gt;
&lt;p&gt;He thinks that AI systems could best serve as a “launchpad” for discussions, giving surrogates a way to consider what data may be significant to the patient.&lt;/p&gt;
&lt;p&gt;But he agreed with Moin that without transparency about how AI surrogates arrive at decisions, AI could sow distrust.&lt;/p&gt;
&lt;p&gt;Imagine, for example, if an AI system didn’t know about a new treatment for cancer that could completely change a patient’s prognosis. Patients might be better served, Brender suggested, if hospitals invested in AI to improve prognosis instead of “literally predicting what a patient would want.” Truog and Morrison also suggested that AI research like Ahmad’s could help hospitals determine what kinds of patients tend to have more stable preferences over time.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Brender suggested that a nightmare scenario could arise if an AI surrogate, presented in a chatbot interface, leads doctors and family members to put “too much trust” in an algorithm. That’s why transparency and rigorous testing will be critical if this technology is ever deployed, he said.&lt;/p&gt;
&lt;p&gt;“If a black-box algorithm says that grandmother would not want resuscitation, I don’t know that that’s helpful,” Brender said. “You need it to be explainable.”&lt;/p&gt;
&lt;h2&gt;Research on bias of AI surrogates doesn’t exist&lt;/h2&gt;
&lt;p&gt;Ahmad agreed that a human should always be in the loop. He emphasized that he’s not rushing to deploy his AI models, which remain in the conceptual phase. Complicating his work, there’s currently little research exploring bias and fairness in the use of AI surrogates.&lt;/p&gt;
&lt;p&gt;Ahmad aims to begin to fill in that gap with a pre-print paper set for release this week that maps out various notions of fairness and then examines fairness across moral traditions. Ultimately, Ahmad suggests, fairness in using AI surrogates “extends beyond parity of outcomes to encompass moral representation, fidelity to the patient’s values, relationships, and worldview.”&lt;/p&gt;
&lt;p class="p1"&gt;“The central question becomes not only, ‘Is the model unbiased?’ but ‘&lt;span class="s1"&gt;Whose moral universe does the model inhabit?'” Ahmad wrote, providing an example:&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p class="p1"&gt;Consider the following: Two patients of &lt;span class="s1"&gt;similar clinical profiles may differ in moral reasoning, one guided by autonomy, another by family or religious duty. &lt;/span&gt;Treating them “similarly” in algorithmic terms would constitute moral erasure. Individual fairness requires incorporating value-sensitive features, such as recorded spiritual preferences or statements about comfort, without violating privacy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It could be more than a decade before the technology is deployed to patients, if it ever happens, Ahmad suggested, because of how challenging it is for AI models to be trained to calculate something as complex as a person’s values and beliefs.&lt;/p&gt;
&lt;p&gt;“That’s where things become really complicated,” Ahmad told Ars, noting “there’s societal norms, and then there’s norms within a particular religious group.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Consider an “extreme example,” Ahmad said. Imagine the puzzle doctors might face if they’re trying to decide if a pregnant woman involved in an accident should be taken off a ventilator because outdated records show she once marked that as her preference. A human surrogate, like her partner or a family member, might be able to advocate on her behalf to stay on the ventilator, particularly if the woman holds pro-life views, he said.&lt;/p&gt;
&lt;p&gt;Without a human surrogate, doctors could turn to AI to help them make a decision, but only if the AI system is able to capture the patient’s values and beliefs based on “patterns learned from data, clinical variables, demographic information, linguistic markers in clinical notes, and possibly the patient’s digital footprint,” Ahmad’s paper explains.&lt;/p&gt;
&lt;p&gt;Then there’s the issue of AI models being “somewhat brittle,” Ahmad said, perhaps giving “a very different answer” if a question is worded slightly differently or in a “clever” way the model doesn’t understand.&lt;/p&gt;
&lt;p&gt;Ahmad is not shying away from what he calls “the problem of engineering values.” To better understand how other researchers are approaching the issue and what expectations patients may have for AI surrogates, Ahmad recently attended an evangelical Christian conference on AI in Dallas, Texas. There, it seemed clear that in a future where AI surrogates are integrated into hospitals, some patients may have high expectations about how well large language models (LLMs) can replicate their inner truths.&lt;/p&gt;
&lt;p&gt;“One thing that really stood out was that people—especially when it comes to LLMs—there was a lot of discussions around having versions of LLMs which reflected their values,” Ahmad said.&lt;/p&gt;
&lt;p&gt;Starke told Ars he thinks it would be ideal to build models based on the most accessible electronic health records, at least from a clinical perspective. To best serve patients, though, he agreed with Ahmad and thinks that “an ideal dataset would be large, diverse, longitudinal, and purpose-built.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“It would combine demographic and clinical variables, documented advance-care-planning data, patient-recorded values and goals, and contextual information about specific decisions,” he said.&lt;/p&gt;
&lt;p&gt;“Including textual and conversational data could further increase a model’s ability to learn &lt;em&gt;why&lt;/em&gt; preferences arise and change, not just &lt;em&gt;what&lt;/em&gt; a patient’s preference was at a single point in time,” Starke said.&lt;/p&gt;
&lt;p&gt;Ahmad suggested that future research could focus on validating fairness frameworks in clinical trials, evaluating moral trade-offs through simulations, and exploring how cross-cultural bioethics can be combined with AI designs.&lt;/p&gt;
&lt;p&gt;Only then might AI surrogates be ready to be deployed, but only as “decision aids,” Ahmad wrote. Any “contested outputs” should automatically “trigger [an] ethics review,” Ahmad wrote, concluding that “the fairest AI surrogate is one that invites conversation, admits doubt, and leaves room for care.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;“AI will not absolve us”&lt;/h2&gt;
&lt;p&gt;Ahmad is hoping to test his conceptual models at various UW sites over the next five years, which would offer “some way to quantify how good this technology is,” he said.&lt;/p&gt;
&lt;p&gt;“After that, I think there’s a collective decision regarding how as a society we decide to integrate or not integrate something like this,” Ahmad said.&lt;/p&gt;
&lt;p&gt;In his paper, he warned against chatbot AI surrogates that could be interpreted as a simulation of the patient, predicting that future models may even speak in patients’ voices and suggesting that the “comfort and familiarity” of such tools might blur “the boundary between assistance and emotional manipulation.”&lt;/p&gt;
&lt;p&gt;Starke agreed that more research and “richer conversations” between patients and doctors are needed.&lt;/p&gt;
&lt;p&gt;“We should be cautious not to apply AI indiscriminately as a solution in search of a problem,” Starke said. “AI will not absolve us from making difficult ethical decisions, especially decisions concerning life and death.”&lt;/p&gt;
&lt;p&gt;Truog, the bioethics expert, told Ars he “could imagine that AI could” one day “provide a surrogate decision maker with some interesting information, and it would be helpful.”&lt;/p&gt;
&lt;p&gt;But a “problem with all of these pathways… is that they frame the decision of whether to perform CPR as a binary choice, regardless of context or the circumstances of the cardiac arrest,” Truog’s editorial said. “In the real world, the answer to the question of whether the patient would want to have CPR” when they’ve lost consciousness, “in almost all cases,” is “it depends.”&lt;/p&gt;
&lt;p&gt;When Truog thinks about the kinds of situations he could end up in, he knows he wouldn’t just be considering his own values, health, and quality of life. His choice “might depend on what my children thought” or “what the financial consequences would be on the details of what my prognosis would be,” he told Ars.&lt;/p&gt;
&lt;p&gt;“I would want my wife or another person that knew me well to be making those decisions,” Truog said. “I wouldn’t want somebody to say, ‘Well, here’s what AI told us about it.'”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</description><content:encoded>&lt;header&gt;
  &lt;div class="dusk:bg-gray-700 my-4 bg-white py-4 dark:bg-gray-700 md:my-10 md:py-8"&gt;
  &lt;div class="mx-auto max-w-2xl px-4 md:px-8 lg:grid lg:max-w-6xl"&gt;
    

    

    &lt;p class="text-gray-550 dark:text-gray-250 dusk:text-gray-250 my-3 text-2xl leading-[1.1] md:leading-[1.2]"&gt;
      Doctors share top concerns of AI surrogates aiding life-or-death decisions.
    &lt;/p&gt;

    

    &lt;div class="relative"&gt;
              &lt;div class="ars-lightbox"&gt;
          &lt;div class="ars-lightbox-item"&gt;
            
              &lt;img alt="alt" class="intro-image" height="1440" src="https://cdn.arstechnica.net/wp-content/uploads/2025/07/pull-the-plug.jpg" width="2560" /&gt;
            
            
          &lt;/div&gt;
        &lt;/div&gt;
          &lt;/div&gt;

    &lt;div&gt;
      &lt;div class="caption font-impact dusk:text-gray-300 mb-4 mt-2 inline-flex flex-row items-stretch gap-1 text-base leading-tight text-gray-400 dark:text-gray-300"&gt;
    &lt;div class="caption-icon bg-[left_top_7px] w-[10px] shrink-0"&gt;&lt;/div&gt;
    &lt;div class="caption-content"&gt;
      

              &lt;span class="caption-credit mt-2 text-xs"&gt;
          Credit:

          
          Aurich Lawson

                  &lt;/span&gt;
          &lt;/div&gt;
  &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;/header&gt;


  

  
      
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
                      
                      
          &lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;For more than a decade, researchers have wondered whether artificial intelligence could help predict what incapacitated patients might want when doctors must make life-or-death decisions on their behalf.&lt;/p&gt;
&lt;p&gt;It remains one of the most high-stakes questions in health care AI today. But as AI improves, some experts increasingly see it as inevitable that digital “clones” of patients could one day aid family members, doctors, and ethics boards in making end-of-life decisions that are aligned with a patient’s values and goals.&lt;/p&gt;
&lt;p&gt;Ars spoke with experts conducting or closely monitoring this research who confirmed that no hospital has yet deployed so-called “AI surrogates.” But AI researcher Muhammad Aurangzeb Ahmad is aiming to change that, taking the first steps toward piloting AI surrogates at a US medical facility.&lt;/p&gt;
&lt;p&gt;“This is very brand new, so very few people are working on it,” Ahmad told Ars.&lt;/p&gt;
&lt;p&gt;Ahmad is a resident fellow working with trauma department faculty at the University of Washington’s UW Medicine. His research is based at Harborview Medical Center in Seattle, a public hospital in the UW Medicine health system. UW Medicine is integrated with “one of the world’s largest medical research programs” to pursue its mission of improving public health outcomes, UW’s website says.&lt;/p&gt;
&lt;p&gt;UW wasn’t specifically seeking a fellow to experiment with AI surrogates, Ahmad told Ars. But since his project proposal was accepted, he has spent most of this year “in the conceptual phase,” working toward testing the accuracy of AI models based on Harborview patient data.&lt;/p&gt;
&lt;p&gt;The main limitation of this testing, Ahmad said, is that he can only verify the accuracy of his models if patients survive and can later confirm that the model made the right choice. But this is just the first step, he said. The accuracy testing could then expand to other facilities in the network, with the aim of developing AI surrogates that can accurately predict patient preferences about “two-thirds” of the time.&lt;/p&gt;

          
                      &lt;div class="ars-interlude-container in-content-interlude mx-auto max-w-xl"&gt;
            &lt;/div&gt;
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Currently, Ahmad’s models are focused on analyzing data that Harborview already collects, such as injury severity, medical history, prior medical choices, and demographic information.&lt;/p&gt;
&lt;p&gt;“We use that information, feed it to a machine learning predictive model, and then in the retrospective data, we observe how well the model is doing,” Ahmad said.&lt;/p&gt;
&lt;p&gt;No patient has yet interacted with Ahmad’s models, he confirmed. UW Medicine spokesperson Susan Gregg told Ars there’s “considerable work to complete prior to launch,” and the system “would be approved only after a multiple-stage review process.”&lt;/p&gt;
&lt;p&gt;“We have not enrolled any patients at Harborview,” Ahmad said. “We are still at the phase of defining the scope and what theoretical considerations to take into account. It will be some time before it gets off the ground, given the challenges involved.”&lt;/p&gt;
&lt;p&gt;In the future, though, Ahmad envisions models that would also analyze textual data, perhaps from patient-approved recorded conversations with their doctors, to inform their AI copy’s predictions. In that world, trusted human surrogates, such as family members, could provide other textual data from chats or texts with the patient. In the technology’s most “ideal” form, Ahmad sees patients interacting with AI systems throughout their lives, providing feedback to refine models as the patients age through the health system.&lt;/p&gt;
&lt;p&gt;“It takes time to get the relevant data,” Ahmad said.&lt;/p&gt;
&lt;p&gt;Before patients could begin interacting with AI surrogates, any human subject testing would need to be approved by an institutional review board (IRB), Ahmad said.&lt;/p&gt;
&lt;p&gt;Ultimately, he expects that AI surrogates won’t be a perfect model but rather a set of rigorously tested systems that doctors and loved ones can consult when assessing all the known information about what a patient would want in critical moments.&lt;/p&gt;
&lt;p&gt;Whether hospitals would ever adopt such a system is unclear. “Within this space, practitioners are more conservative, and I would even argue that’s rightfully so,” Ahmad said.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Gregg told Ars that UW Medicine supports the “thoughtful exploration of innovative ideas, such as the potential responsible and transparent use of AI surrogates in end-of-life care,” as they reflect “our commitment to advancing both science and compassion in medicine.”&lt;/p&gt;
&lt;p&gt;“While end-of-life decision-making represents a particularly complex area, we view these decisions as essential to addressing important questions, such as how to best honor patient wishes when they may be unable to communicate them directly or have no next of kin to do so on their behalf,” Gregg said.&lt;/p&gt;
&lt;h2&gt;Is AI a bad fit for patients with no human surrogates?&lt;/h2&gt;
&lt;p&gt;It has always been hard for doctors to determine what patients want when they can’t speak for themselves. A patient may refuse to be put on a ventilator or receive dialysis or cardiopulmonary resuscitation (CPR) if, for example, they’ve expressed that they want to avoid discomfort at the end of their life. Others may fear complications like infections or have no desire to rely on a machine for life support. Some patients, like young people involved in accidents, may have never expressed preferences.&lt;/p&gt;
&lt;p&gt;Emily Moin, a physician in an intensive care unit in Pennsylvania, told Ars that time is a factor in these decisions, but it’s imperative that a human surrogate who may better understand the patient’s wishes be involved.&lt;/p&gt;
&lt;p&gt;“When we’re in one of these fast-paced situations where we don’t know, but we have a patient in front of us who has died, we will err on the side of providing [CPR] until we are able to arrive at the clinical judgment that that effort is no longer indicated or until we’re able to engage with a surrogate decision maker,” Moin explained.&lt;/p&gt;
&lt;p&gt;Reaching the surrogate, she said, is “an important part of taking care of someone.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Ahmad hopes that AI could help alleviate stress in uncertain moments. For doctors and surrogates, these decisions can be “very emotionally taxing,” Ahmad told Ars, leading many people to second-guess what the patient would choose. Some studies have shown that surrogates often get it wrong, he said, and he believes AI could help improve the odds of success.&lt;/p&gt;
&lt;p&gt;Seeking to nip this problem in the bud, health systems have historically pushed patients to complete “advanced directives” to log their preferences. Over time, though, it has become clear that patients’ preferences tend to be unstable, sometimes changing within days.&lt;/p&gt;
&lt;p&gt;Doctors must also consider that some patients have no stated preferences. Others, Moin said, have reported that their preferences changed after receiving lifesaving treatments because they now know what to expect. There are likely other limitations of Ahmad’s planned testing, which would determine accuracy by checking whether the AI’s decision matches what a patient says they would have wanted after recovery, Moin said.&lt;/p&gt;
&lt;p&gt;“These decisions are dynamically constructed and context-dependent,” Moin said. “And if you’re assessing the performance of the model based on asking someone after they’ve recovered what they would have said before they recovered, that’s not going to provide you with an accurate representation.”&lt;/p&gt;
&lt;p&gt;Moin said one of the big problems with medical AI is that people expect it to “provide better predictions than what we’re currently able to generate.” But the models are being trained on “convenient ground truths,” she said, that don’t “provide meaningful examples for models to learn about the situations” where the models would be employed.&lt;/p&gt;
&lt;p&gt;“I imagine that they would actually want to deploy this model to help to make decisions for unrepresented patients, patients who can’t communicate, patients who don’t have a surrogate,” Moin said, “but those are exactly the patients where you’ll never be able to know what the so-called ground truth is, and then you’ll never be able to assess your bias, and you’ll never be able to assess your model’s performance.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;h2&gt;Family members may default to agreeing with AI&lt;/h2&gt;
&lt;p&gt;Culturally, the US has shifted from being “very focused on patient autonomy” to “more of a shared decision-making and, at times, family- and community-focused lens” as the standard for making these difficult decisions, Moin said.&lt;/p&gt;
&lt;p&gt;The longer a doctor knows a patient, and the more conversations a patient’s health team has with family members, the more likely it is for health systems to be able to adapt to respect the patient’s wishes over time, Moin suggested.&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;That idea echoes Ahmad’s “ideal” AI surrogate model. But Moin said that if patients talk to an AI, it could actually discourage them from having important conversations with family members. Studies have found that if a patient fills out advanced directives, it can become harder to determine their preferences, Moin said, because patients may be less likely to discuss their preferences with loved ones.&lt;/p&gt;
&lt;p&gt;Earlier this year, Moin urged human surrogates to remain closely involved in do-not-resuscitate orders, writing that doctors who unilaterally make these decisions have an ethical obligation to “ensure that patients and surrogate decision-makers are aware that the decision has been made” and face “the lowest of barriers” to expressing disagreement.&lt;/p&gt;
&lt;p&gt;“Forgoing CPR is one of the most consequential treatment decisions a patient or surrogate can make because, if invoked, it will necessarily lead to death,” Moin wrote.&lt;/p&gt;
&lt;p&gt;Moin told Ars she hopes an AI surrogate’s outputs would never be weighted more than a human surrogate’s opinion, which is based on lived experience with a patient. “But I do worry that there could be culture shifts and other pressures that would encourage clinicians and family members, for that matter, to lean on products like these more heavily,” she said.&lt;/p&gt;
&lt;p&gt;“I can imagine a scenario where, say, a doctor is expected to round on 24 critically ill patients in one day, and the family member is resistant to sitting down for a conversation,” Moin said. “So yeah, maybe all parties involved would default to the shortcut of incorporating the information from this model.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Moin called for more public awareness and debate on AI surrogates, noting that “people really hate” the use of algorithms to determine who gets care.&lt;/p&gt;
&lt;p&gt;“I don’t think that it would be good for patients or clinicians or society for that matter,” Moin said.&lt;/p&gt;
&lt;p&gt;She’s particularly worried that “patients who can’t speak for themselves and who don’t have a clear loved one” would be “the ones who would be most vulnerable to suffering harms” of AI surrogates making wrong calls. Too many such mistakes could further erode trust in health systems, Moin said.&lt;/p&gt;
&lt;h2&gt;AI surrogates may be redundant&lt;/h2&gt;
&lt;p&gt;These decisions are “psychosocially fraught” for everyone involved, Teva Brender, a hospitalist at a medical center for veterans in San Francisco, told Ars. That’s why testing like Ahmad’s is important, he said.&lt;/p&gt;
&lt;p&gt;Last year, Brender co-authored an opinion piece&amp;nbsp;noting “how difficult it can be for families to make decisions for incapacitated patients,” particularly in geriatrics, palliative, and critical care settings.&lt;/p&gt;
&lt;p&gt;“For many, the notion of incorporating AI into goals-of-care conversations will conjure nightmarish visions of a dystopian future wherein we entrust deeply human decisions to algorithms,” Brender’s team wrote. “We share these apprehensions.”&lt;/p&gt;
&lt;p&gt;But with doctors’ and surrogates’ predictions facing significant limitations, “it behooves us to consider how AI could be safely, ethically, and equitably deployed to help surrogates for individuals who are seriously ill,” Brender’s team concluded.&lt;/p&gt;
&lt;p&gt;And it’s “equally important,” Brender told Ars, to help patients choose surrogates and prepare them to substitute their judgments.&lt;/p&gt;
&lt;p&gt;Brender believes Ahmad’s research is worthwhile since there are “lots of questions” requiring scientific research. But he’s “glad to hear” that AI surrogates are “not actually being used among patients” at Harborview yet. “I can’t imagine that an IRB would approve such a thing this early,” he told Ars.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;And AI surrogates may end up playing a redundant role, leading this potential use for AI to fall out of favor, Brender said.&lt;/p&gt;
&lt;p&gt;“The devil’s advocate perspective,” Brender said, is that AI surrogates would just be doing “what a good clinician does anyway,” which is to ask surrogates, “Hey, who was this person? What did they enjoy doing? What brought meaning to their life?”&lt;/p&gt;
&lt;p&gt;“Do you need an AI to do that?” Brender asked. “I’m not so sure.”&lt;/p&gt;
&lt;h2&gt;AI can’t replace human surrogates, doctors warn&lt;/h2&gt;
&lt;p&gt;Last month, bioethics expert Robert Truog joined R. Sean Morrison, a doctor dedicated to advancing palliative care aimed at improving the quality of life for people suffering life-threatening illnesses, in emphasizing that AI should never replace human surrogates in resuscitation decisions.&lt;/p&gt;
&lt;p&gt;“Decisions about hypothetical scenarios do not correlate with decisions that need to be made in real time,” Morrison, who is chair of the Brookdale Department of Geriatrics and Palliative Medicine at Mount Sinai, told Ars. “AI cannot fix this fundamental issue—it is not a matter of better prediction. Patients’ preferences often represent a snapshot in time that are simply not predictive of the future.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;The warning came after Georg Starke, a doctor and senior research associate at the Chair for Ethics of AI and Neuroscience at the Technical University of Munich, co-authored a proof-of-concept showing that three AI models, on average, performed better than human surrogates in predicting patient preferences.&lt;/p&gt;
&lt;p&gt;Starke’s study relied on existing data from Swiss respondents of a European survey that tracked population health trends of individuals over 50 years old. The dataset offered “comprehensive information on participants’ end-of-life preferences, including questions concerning” CPR. That allowed the team to build three models: a simple model, a model based on commonly available electronic health records, and a more “personalized” model. Each model successfully predicted whether a patient experiencing cardiac arrest would want CPR, with an accuracy of up to 70 percent.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;His team’s research was intended to “ground a long-standing ethical debate in empirical data,” Starke told Ars.&lt;/p&gt;
&lt;p&gt;“For over a decade, people have speculated about using algorithms to improve clinical decision-making for incapacitated patients, but no one had shown whether such a program could actually be designed,” Starke said. “Our study was meant to test if it’s feasible, explore how well it performs, identify which factors influence the models’ decisions, and spark a broader debate about the technology.”&lt;/p&gt;
&lt;p&gt;A key limitation of AI models depending on “‘accuracy’ alone”—especially if that “accuracy” is “achieved by chance or by pattern-matching purely demographic data outside an individual’s control”—is that the outputs don’t “necessarily reflect an autonomous choice,” Starke said.&lt;/p&gt;
&lt;p&gt;Like Truog and Morrison, Starke’s team emphasized that “human surrogates will remain essential sources for the contextual aspects of specific situations,” particularly with patients with dementia, and agreed that AI models “should not replace surrogate decision-making.”&lt;/p&gt;
&lt;h2&gt;Chatbot surrogates could be bad&lt;/h2&gt;
&lt;p&gt;Human surrogates may grow to trust AI systems in the future, but “it’s all about how the information is presented,” Brender, the hospitalist, told Ars.&lt;/p&gt;
&lt;p&gt;He thinks that AI systems could best serve as a “launchpad” for discussions, giving surrogates a way to consider what data may be significant to the patient.&lt;/p&gt;
&lt;p&gt;But he agreed with Moin that without transparency about how AI surrogates arrive at decisions, AI could sow distrust.&lt;/p&gt;
&lt;p&gt;Imagine, for example, if an AI system didn’t know about a new treatment for cancer that could completely change a patient’s prognosis. Patients might be better served, Brender suggested, if hospitals invested in AI to improve prognosis instead of “literally predicting what a patient would want.” Truog and Morrison also suggested that AI research like Ahmad’s could help hospitals determine what kinds of patients tend to have more stable preferences over time.&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;Brender suggested that a nightmare scenario could arise if an AI surrogate, presented in a chatbot interface, leads doctors and family members to put “too much trust” in an algorithm. That’s why transparency and rigorous testing will be critical if this technology is ever deployed, he said.&lt;/p&gt;
&lt;p&gt;“If a black-box algorithm says that grandmother would not want resuscitation, I don’t know that that’s helpful,” Brender said. “You need it to be explainable.”&lt;/p&gt;
&lt;h2&gt;Research on bias of AI surrogates doesn’t exist&lt;/h2&gt;
&lt;p&gt;Ahmad agreed that a human should always be in the loop. He emphasized that he’s not rushing to deploy his AI models, which remain in the conceptual phase. Complicating his work, there’s currently little research exploring bias and fairness in the use of AI surrogates.&lt;/p&gt;
&lt;p&gt;Ahmad aims to begin to fill in that gap with a pre-print paper set for release this week that maps out various notions of fairness and then examines fairness across moral traditions. Ultimately, Ahmad suggests, fairness in using AI surrogates “extends beyond parity of outcomes to encompass moral representation, fidelity to the patient’s values, relationships, and worldview.”&lt;/p&gt;
&lt;p class="p1"&gt;“The central question becomes not only, ‘Is the model unbiased?’ but ‘&lt;span class="s1"&gt;Whose moral universe does the model inhabit?'” Ahmad wrote, providing an example:&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p class="p1"&gt;Consider the following: Two patients of &lt;span class="s1"&gt;similar clinical profiles may differ in moral reasoning, one guided by autonomy, another by family or religious duty. &lt;/span&gt;Treating them “similarly” in algorithmic terms would constitute moral erasure. Individual fairness requires incorporating value-sensitive features, such as recorded spiritual preferences or statements about comfort, without violating privacy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It could be more than a decade before the technology is deployed to patients, if it ever happens, Ahmad suggested, because of how challenging it is for AI models to be trained to calculate something as complex as a person’s values and beliefs.&lt;/p&gt;
&lt;p&gt;“That’s where things become really complicated,” Ahmad told Ars, noting “there’s societal norms, and then there’s norms within a particular religious group.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="my-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;p&gt;Consider an “extreme example,” Ahmad said. Imagine the puzzle doctors might face if they’re trying to decide if a pregnant woman involved in an accident should be taken off a ventilator because outdated records show she once marked that as her preference. A human surrogate, like her partner or a family member, might be able to advocate on her behalf to stay on the ventilator, particularly if the woman holds pro-life views, he said.&lt;/p&gt;
&lt;p&gt;Without a human surrogate, doctors could turn to AI to help them make a decision, but only if the AI system is able to capture the patient’s values and beliefs based on “patterns learned from data, clinical variables, demographic information, linguistic markers in clinical notes, and possibly the patient’s digital footprint,” Ahmad’s paper explains.&lt;/p&gt;
&lt;p&gt;Then there’s the issue of AI models being “somewhat brittle,” Ahmad said, perhaps giving “a very different answer” if a question is worded slightly differently or in a “clever” way the model doesn’t understand.&lt;/p&gt;
&lt;p&gt;Ahmad is not shying away from what he calls “the problem of engineering values.” To better understand how other researchers are approaching the issue and what expectations patients may have for AI surrogates, Ahmad recently attended an evangelical Christian conference on AI in Dallas, Texas. There, it seemed clear that in a future where AI surrogates are integrated into hospitals, some patients may have high expectations about how well large language models (LLMs) can replicate their inner truths.&lt;/p&gt;
&lt;p&gt;“One thing that really stood out was that people—especially when it comes to LLMs—there was a lot of discussions around having versions of LLMs which reflected their values,” Ahmad said.&lt;/p&gt;
&lt;p&gt;Starke told Ars he thinks it would be ideal to build models based on the most accessible electronic health records, at least from a clinical perspective. To best serve patients, though, he agreed with Ahmad and thinks that “an ideal dataset would be large, diverse, longitudinal, and purpose-built.”&lt;/p&gt;

          
                  &lt;/div&gt;

              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;
                    
        &lt;div class="ad-wrapper with-label is-fullwidth"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--mid-content"&gt;
          &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
          
    
    &lt;div class="mt-2.5 mx-auto px-[15px] sm:px-5 lg:grid lg:max-w-5xl lg:grid-cols-3 lg:gap-6 lg:px-8 xl:px-0"&gt;
      &lt;div class="relative lg:col-span-2"&gt;

        
        &lt;div class="post-content post-content-double"&gt;
          
          
&lt;p&gt;“It would combine demographic and clinical variables, documented advance-care-planning data, patient-recorded values and goals, and contextual information about specific decisions,” he said.&lt;/p&gt;
&lt;p&gt;“Including textual and conversational data could further increase a model’s ability to learn &lt;em&gt;why&lt;/em&gt; preferences arise and change, not just &lt;em&gt;what&lt;/em&gt; a patient’s preference was at a single point in time,” Starke said.&lt;/p&gt;
&lt;p&gt;Ahmad suggested that future research could focus on validating fairness frameworks in clinical trials, evaluating moral trade-offs through simulations, and exploring how cross-cultural bioethics can be combined with AI designs.&lt;/p&gt;
&lt;p&gt;Only then might AI surrogates be ready to be deployed, but only as “decision aids,” Ahmad wrote. Any “contested outputs” should automatically “trigger [an] ethics review,” Ahmad wrote, concluding that “the fairest AI surrogate is one that invites conversation, admits doubt, and leaves room for care.”&lt;/p&gt;
&lt;div class="page-anchor-wrapper"&gt;&lt;/div&gt;
&lt;h2&gt;“AI will not absolve us”&lt;/h2&gt;
&lt;p&gt;Ahmad is hoping to test his conceptual models at various UW sites over the next five years, which would offer “some way to quantify how good this technology is,” he said.&lt;/p&gt;
&lt;p&gt;“After that, I think there’s a collective decision regarding how as a society we decide to integrate or not integrate something like this,” Ahmad said.&lt;/p&gt;
&lt;p&gt;In his paper, he warned against chatbot AI surrogates that could be interpreted as a simulation of the patient, predicting that future models may even speak in patients’ voices and suggesting that the “comfort and familiarity” of such tools might blur “the boundary between assistance and emotional manipulation.”&lt;/p&gt;
&lt;p&gt;Starke agreed that more research and “richer conversations” between patients and doctors are needed.&lt;/p&gt;
&lt;p&gt;“We should be cautious not to apply AI indiscriminately as a solution in search of a problem,” Starke said. “AI will not absolve us from making difficult ethical decisions, especially decisions concerning life and death.”&lt;/p&gt;
&lt;p&gt;Truog, the bioethics expert, told Ars he “could imagine that AI could” one day “provide a surrogate decision maker with some interesting information, and it would be helpful.”&lt;/p&gt;
&lt;p&gt;But a “problem with all of these pathways… is that they frame the decision of whether to perform CPR as a binary choice, regardless of context or the circumstances of the cardiac arrest,” Truog’s editorial said. “In the real world, the answer to the question of whether the patient would want to have CPR” when they’ve lost consciousness, “in almost all cases,” is “it depends.”&lt;/p&gt;
&lt;p&gt;When Truog thinks about the kinds of situations he could end up in, he knows he wouldn’t just be considering his own values, health, and quality of life. His choice “might depend on what my children thought” or “what the financial consequences would be on the details of what my prognosis would be,” he told Ars.&lt;/p&gt;
&lt;p&gt;“I would want my wife or another person that knew me well to be making those decisions,” Truog said. “I wouldn’t want somebody to say, ‘Well, here’s what AI told us about it.'”&lt;/p&gt;


          
                  &lt;/div&gt;

                  
          &lt;div class="-mx-2.5 sm:mx-0"&gt;
  &lt;/div&gt;






  


  
              &lt;/div&gt;

      
      &lt;div class="dusk:bg-gray-100 hidden min-w-[300px] justify-self-end lg:block dark:bg-gray-50"&gt;
        
                  &lt;div class="ad-wrapper is-sticky is-rail"&gt;
        &lt;div class="ad-wrapper-inner"&gt;
            &lt;div class="ad ad--rail"&gt;&lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
              &lt;/div&gt;
    &lt;/div&gt;</content:encoded><guid isPermaLink="false">https://arstechnica.com/features/2025/10/should-an-ai-copy-of-you-help-decide-if-you-live-or-die/</guid><pubDate>Mon, 20 Oct 2025 11:00:09 +0000</pubDate></item><item><title>This retina implant lets people with vision loss do a crossword puzzle (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/20/1126065/this-retina-implant-lets-people-with-vision-loss-do-a-crossword-puzzle/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Science Corporation—a competitor to Neuralink founded by the former president of Elon Musk’s brain-interface venture—has leapfrogged its rival after acquiring, at a fire-sale price, a vision implant that’s in advanced testing,.&lt;/p&gt;  &lt;p&gt;The implant produces a form of “artificial vision” that lets some patients read text and do crosswords, according to a report published in the &lt;em&gt;New England Journal of Medicine&lt;/em&gt; today.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The implant is a microelectronic chip placed under the retina. Using signals from a camera mounted on a pair of glasses, the chip emits bursts of electricity in order to bypass photoreceptor cells damaged by macular degeneration, the leading cause of vision loss in elderly people.&lt;/p&gt;  &lt;p&gt;“The magnitude of the effect is what’s notable,” says José-Alain Sahel, a University of Pittsburgh vision scientist who led testing of the system, which is called PRIMA. “There’s a patient in the UK and she is reading the pages of a regular book, which is unprecedented.”&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Until last year, the device was being developed by Pixium Vision, a French startup cofounded by Sahel, which faced bankruptcy after it couldn’t raise more cash.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s when Science Corporation swept in to purchase the company’s assets for about €4 million ($4.7 million), according to court filings.&lt;/p&gt; 
 &lt;p&gt;“Science was able to buy it for very cheap just when the study was coming out, so it was good timing for them,” says Sahel. “They could quickly access very advanced technology that’s closer to the market, which is good for a company to have.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Science was founded in 2021 by Max Hodak, the first president of Neuralink, after his sudden departure from that company. Since its founding, Science has raised around $290 million, according to the venture capital database Pitchbook, and used the money to launch broad-ranging exploratory research on brain interfaces and new types of vision treatments.&lt;/p&gt;  &lt;p&gt;“The ambition here is to build a big, standalone medical technology company that would fit in with an Apple, Samsung, or an Alphabet,” Hodak said in an interview at Science’s labs in Alameda, California in September. “The goal is to change the world in important ways … but we need to make money in order to invest in these programs.”&lt;/p&gt;  &lt;p&gt;By acquiring the PRIMA implant program, Science effectively vaulted past years of development and testing. The company has requested approval to sell the eye chip in Europe and is in discussions with regulators in the US.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Unlike Neuralink’s implant, which records brain signals so paralyzed recipients can use their thoughts to move a computer mouse, the retina chip sends information into the brain to produce vision. Because the retina is an outgrowth of the brain, the chip qualifies as a type of brain-computer interface.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Artificial vision systems have been studied for years and one, called the Argus II, even reached the market and was installed in the eyes of about 400 people. But that product was later withdrawn after it proved to be a money-loser, according to Cortigent, the company that now owns that technology.&lt;/p&gt;  &lt;p&gt;Thirty-eight patients in Europe received a PRIMA implant in one eye. On average, the study found, they were able to read five additional lines on a vision chart—the kind with rows of letters, each smaller than the last. Some of that improvement was due to what Sahel calls “various tricks” like using a zoom function, which allows patients to zero in on text they want to read.&lt;/p&gt;  &lt;p&gt;The type of vision loss being treated with the new implant is called geographic atrophy, in which patients have peripheral vision but can’t make out objects directly in front of them, like words or faces. According to Prevent Blindness, an advocacy organization, this type of central vision loss affects around one in 10 people over 80. &amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The implant was originally designed starting 20 years ago by Daniel Palanker, a laser expert and now a professor at Stanford University, who says his breakthrough was realizing that light beams could supply both energy and information to a chip placed under the retina.&amp;nbsp;Other implants, like Argus II, use a wire, which adds complexity.&lt;/p&gt;  &lt;p&gt;“The chip has no brains at all. It just turns light into electrical current that flows into the tissue,” says Palanker. “Patients describe the color they see as yellowish blue or sun color.”&lt;/p&gt;  &lt;p&gt;The system works using a wearable camera that records a scene and then blasts bright infrared light into the eye, using a wavelength humans can’t see. That light hits the chip, which is covered by “what are basically tiny solar panels,” says Palanker. “We just try to replace the photoreceptors with a photo-array.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1126085" height="1688" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Cooking-and-reading-recipe-with-PRIMA-5.png?w=3000" width="3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;A diagram of how a visual scene could be represented by a retinal implant.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY SCIENCE CORPORATION&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The current system produces about 400 spots of vision, which lets users make out the outlines of words and objects. Palanaker says a next-generation device will have five times as many “pixels” and should let people see more: “What we discovered in the trial is that even though you stimulate individual pixels, patients perceive it as continuous. The patient says ‘I see a line,’ “I see a letter.’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;Palanker says it will be important to keep improving the system because “the market size depends on the quality of the vision produced.”&lt;/p&gt;  &lt;p&gt;When Pixium teetered on insolvency, Palanker says, he helped search for a buyer, meeting with Hodak. “It was a fire sale, not a celebration,” he says. “But for me it’s a very lucky outcome, because it means the product is going forward. And the purchase price doesn’t really matter, because there’s a big investment needed to bring it to market. It’s going to cost money.” &amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Photo of the PRIMA Glasses and Pocket Processor." class="wp-image-1126012" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/251017_prima_embed1.jpg?w=3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;The PRIMA artificial vision system has a battery pack/controller and an eye-mounted camera.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY SCIENCE CORPORATION&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;During a visit to Science’s headquarters, Hodak described the company’s effort to redesign the system into something sleeker and more user-friendly. In the original design, in addition to the wearable camera, the patient has to carry around a bulky controller containing a battery and laser, as well as buttons to zoom in and out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But Science has already prototyped a version in which those electronics are squeezed into what look like an extra-large pair of sunglasses.&lt;/p&gt;  &lt;p&gt;“The implant is great, but we’ll have new glasses on patients fairly shortly,” Hodak says. “This will substantially improve their ability to have it with them all day.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Other companies also want to treat blindness with brain-computer interfaces, but some think it might be better to send signals directly into the brain. This year, Neuralink has been touting plans for “Blindsight,” a project to send electrical signals directly into the brain’s visual cortex, bypassing the retina entirely. It has yet to test the approach in a person.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;Science Corporation—a competitor to Neuralink founded by the former president of Elon Musk’s brain-interface venture—has leapfrogged its rival after acquiring, at a fire-sale price, a vision implant that’s in advanced testing,.&lt;/p&gt;  &lt;p&gt;The implant produces a form of “artificial vision” that lets some patients read text and do crosswords, according to a report published in the &lt;em&gt;New England Journal of Medicine&lt;/em&gt; today.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;The implant is a microelectronic chip placed under the retina. Using signals from a camera mounted on a pair of glasses, the chip emits bursts of electricity in order to bypass photoreceptor cells damaged by macular degeneration, the leading cause of vision loss in elderly people.&lt;/p&gt;  &lt;p&gt;“The magnitude of the effect is what’s notable,” says José-Alain Sahel, a University of Pittsburgh vision scientist who led testing of the system, which is called PRIMA. “There’s a patient in the UK and she is reading the pages of a regular book, which is unprecedented.”&amp;nbsp;&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;Until last year, the device was being developed by Pixium Vision, a French startup cofounded by Sahel, which faced bankruptcy after it couldn’t raise more cash.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That’s when Science Corporation swept in to purchase the company’s assets for about €4 million ($4.7 million), according to court filings.&lt;/p&gt; 
 &lt;p&gt;“Science was able to buy it for very cheap just when the study was coming out, so it was good timing for them,” says Sahel. “They could quickly access very advanced technology that’s closer to the market, which is good for a company to have.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;Science was founded in 2021 by Max Hodak, the first president of Neuralink, after his sudden departure from that company. Since its founding, Science has raised around $290 million, according to the venture capital database Pitchbook, and used the money to launch broad-ranging exploratory research on brain interfaces and new types of vision treatments.&lt;/p&gt;  &lt;p&gt;“The ambition here is to build a big, standalone medical technology company that would fit in with an Apple, Samsung, or an Alphabet,” Hodak said in an interview at Science’s labs in Alameda, California in September. “The goal is to change the world in important ways … but we need to make money in order to invest in these programs.”&lt;/p&gt;  &lt;p&gt;By acquiring the PRIMA implant program, Science effectively vaulted past years of development and testing. The company has requested approval to sell the eye chip in Europe and is in discussions with regulators in the US.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;Unlike Neuralink’s implant, which records brain signals so paralyzed recipients can use their thoughts to move a computer mouse, the retina chip sends information into the brain to produce vision. Because the retina is an outgrowth of the brain, the chip qualifies as a type of brain-computer interface.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Artificial vision systems have been studied for years and one, called the Argus II, even reached the market and was installed in the eyes of about 400 people. But that product was later withdrawn after it proved to be a money-loser, according to Cortigent, the company that now owns that technology.&lt;/p&gt;  &lt;p&gt;Thirty-eight patients in Europe received a PRIMA implant in one eye. On average, the study found, they were able to read five additional lines on a vision chart—the kind with rows of letters, each smaller than the last. Some of that improvement was due to what Sahel calls “various tricks” like using a zoom function, which allows patients to zero in on text they want to read.&lt;/p&gt;  &lt;p&gt;The type of vision loss being treated with the new implant is called geographic atrophy, in which patients have peripheral vision but can’t make out objects directly in front of them, like words or faces. According to Prevent Blindness, an advocacy organization, this type of central vision loss affects around one in 10 people over 80. &amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;The implant was originally designed starting 20 years ago by Daniel Palanker, a laser expert and now a professor at Stanford University, who says his breakthrough was realizing that light beams could supply both energy and information to a chip placed under the retina.&amp;nbsp;Other implants, like Argus II, use a wire, which adds complexity.&lt;/p&gt;  &lt;p&gt;“The chip has no brains at all. It just turns light into electrical current that flows into the tissue,” says Palanker. “Patients describe the color they see as yellowish blue or sun color.”&lt;/p&gt;  &lt;p&gt;The system works using a wearable camera that records a scene and then blasts bright infrared light into the eye, using a wavelength humans can’t see. That light hits the chip, which is covered by “what are basically tiny solar panels,” says Palanker. “We just try to replace the photoreceptors with a photo-array.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" class="wp-image-1126085" height="1688" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/Cooking-and-reading-recipe-with-PRIMA-5.png?w=3000" width="3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;A diagram of how a visual scene could be represented by a retinal implant.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY SCIENCE CORPORATION&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;The current system produces about 400 spots of vision, which lets users make out the outlines of words and objects. Palanaker says a next-generation device will have five times as many “pixels” and should let people see more: “What we discovered in the trial is that even though you stimulate individual pixels, patients perceive it as continuous. The patient says ‘I see a line,’ “I see a letter.’”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;p&gt;Palanker says it will be important to keep improving the system because “the market size depends on the quality of the vision produced.”&lt;/p&gt;  &lt;p&gt;When Pixium teetered on insolvency, Palanker says, he helped search for a buyer, meeting with Hodak. “It was a fire sale, not a celebration,” he says. “But for me it’s a very lucky outcome, because it means the product is going forward. And the purchase price doesn’t really matter, because there’s a big investment needed to bring it to market. It’s going to cost money.” &amp;nbsp;&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Photo of the PRIMA Glasses and Pocket Processor." class="wp-image-1126012" src="https://wp.technologyreview.com/wp-content/uploads/2025/10/251017_prima_embed1.jpg?w=3000" /&gt;&lt;figcaption class="wp-element-caption"&gt;The PRIMA artificial vision system has a battery pack/controller and an eye-mounted camera.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;COURTESY SCIENCE CORPORATION&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;During a visit to Science’s headquarters, Hodak described the company’s effort to redesign the system into something sleeker and more user-friendly. In the original design, in addition to the wearable camera, the patient has to carry around a bulky controller containing a battery and laser, as well as buttons to zoom in and out.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;But Science has already prototyped a version in which those electronics are squeezed into what look like an extra-large pair of sunglasses.&lt;/p&gt;  &lt;p&gt;“The implant is great, but we’ll have new glasses on patients fairly shortly,” Hodak says. “This will substantially improve their ability to have it with them all day.”&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Other companies also want to treat blindness with brain-computer interfaces, but some think it might be better to send signals directly into the brain. This year, Neuralink has been touting plans for “Blindsight,” a project to send electrical signals directly into the brain’s visual cortex, bypassing the retina entirely. It has yet to test the approach in a person.&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/20/1126065/this-retina-implant-lets-people-with-vision-loss-do-a-crossword-puzzle/</guid><pubDate>Mon, 20 Oct 2025 12:00:00 +0000</pubDate></item><item><title>The Download: a promising retina implant, and how climate change affects flowers (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/20/1126099/the-download-a-promising-retina-implant-and-how-climate-change-affects-flowers/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;This retina implant lets people with vision loss do a crossword puzzle&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;The news: &lt;/strong&gt;Science Corporation—a competitor to Neuralink founded by the former president of Elon Musk's brain-interface venture—has leapfrogged its rival after acquiring a vision implant in advanced testing for a fire-sale price. The implant produces a form of “artificial vision” that lets some patients read text and do crosswords, according to a report published in &lt;em&gt;The&lt;/em&gt; &lt;em&gt;New England Journal of Medicine &lt;/em&gt;today.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How it works: &lt;/strong&gt;The implant is a microelectronic chip placed under the retina. Using signals from a camera mounted on a pair of glasses, the chip emits bursts of electricity in order to bypass photoreceptor cells damaged by macular degeneration, the leading cause of vision loss in the elderly. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How will flowers respond to climate change?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Flowers play a key role in most landscapes, from urban to rural areas. Yet flowers have much more to tell in their bright blooms: The very shape they take is formed by local and global climate conditions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The form of a flower is a visual display of its climate, if you know what to look for. In a dry year, its petals’ pigmentation may change. In a warm year, the flower might grow bigger. The flower’s ultraviolet-absorbing pigment increases with higher ozone levels.&lt;/p&gt;  &lt;p&gt;Now, a new artistic project sets out to answer the question: As the climate changes in the future, how might flowers change? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Annelie Berner&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;This story is from our forthcoming print issue, which is all about the body. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;2025 climate tech companies to watch: Redwood Materials and its new AI microgrids&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Over the past few years, Redwood Materials has become one of the top US battery recyclers, joining forces with the likes of Volkswagen, BMW, and Toyota to process old electric-vehicle batteries and recover materials that can be used to make new ones.&lt;/p&gt;&lt;p&gt;Now it's moving into reuse as well. Redwood Energy, a new branch of the company, incorporates used EV batteries into microgrids to power energy-hungry AI data centers. Read the full story.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;—Peter Hall&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Redwood Materials is one of our 10 climate tech companies to watch—our annual list of some of the most promising climate tech firms on the planet. &lt;/strong&gt;&lt;strong&gt;Check out the rest of the list here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 AWS is recovering from a major outage&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s racing to get hundreds of apps and services back online. (The Verge)&lt;br /&gt;+ &lt;em&gt;Snapchat, Roblox and banking services are among those affected. &lt;/em&gt;(The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 OpenAI made—then retracted—a claim it had made a major math breakthrough&lt;/strong&gt;&lt;br /&gt;After math experts and rival AI firms ridiculed its poorly-worded declaration. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;What’s next for AI and math. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 The grave costs of Trump’s war on climate science&lt;/strong&gt;&lt;br /&gt;It’s affecting the accuracy of forecasting systems globally, not just in the US. (FT $)&lt;br /&gt;+ &lt;em&gt;Trump himself led an effort to derail plans to tax shipping pollution. &lt;/em&gt;(Politico $)&lt;br /&gt;+ &lt;em&gt;How to make clean energy progress under Trump in the states. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 China claims the US is behind a cyberattack on its national time center&lt;br /&gt;&lt;/strong&gt;It says it has years’ worth of irrefutable evidence of data stealing. (Reuters)&lt;br /&gt;+ &lt;em&gt;US experts allegedly exploited vulnerabilities in mobile phones belonging to National Time Service Center workers. &lt;/em&gt;(Bloomberg $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Is AI-generated art real art?&lt;br /&gt;It’s a question gallery and museum curators across the world are debating. (NYT $)&lt;br /&gt;+ &lt;em&gt;Artisan craftmakers are happy to resist the pull of AI. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;This tool claims to trace how much of an AI image has been drawn from existing material.&lt;/em&gt; (The Guardian)&lt;br /&gt;+ &lt;em&gt;From slop to Sotheby’s? AI art enters a new phase. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 Chipmaker Nexperia has accused its ousted CEO of spreading falsehoods&lt;/strong&gt;&lt;br /&gt;Zhang Xuezheng reportedly claimed it was operating independently in China. (Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 This whistleblower raised concerns about the safety of US data under DOGE&lt;/strong&gt;&lt;br /&gt;And says the hostile reception to his complaint led to him leaving his dream job. (WP $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Aid agencies have been criticized for using AI “poverty porn”&lt;br /&gt;&lt;/strong&gt;But the NGOs say its use protects the identities of real people in social media campaigns. (The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9&lt;/strong&gt; &lt;strong&gt;EVs lose their value much faster than gas-powered cars&lt;br /&gt;&lt;/strong&gt;Which isn’t exactly an incentive for prospective first-time buyers. (Rest of World)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 What happens to our brains when we dream 🧠&lt;/strong&gt;&lt;br /&gt;We’re learning more about the many liminal states they can slip through. (Quanta Magazine)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Hoisted by their own GPTards.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Meta’s chief AI scientist Yann LeCun pokes fun at OpenAI after the company walked back its claim it had made a major math breakthrough in a post on X.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2025/02/250214_evbatteryfire.gif?fit=1456,818" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;One option for electric vehicle fires? Let them burn.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Although there isn’t solid data on the frequency of EV battery fires, it’s no secret that these fires are happening.&lt;/p&gt;&lt;p&gt;Despite that, manufacturers offer no standardized steps on how to fight them or avoid them in the first place. What’s more, with EVs, it’s never entirely clear whether the fire is truly out.&lt;/p&gt;&lt;p&gt;Patrick Durham, the owner of one of a growing number of private companies helping first responders learn how to deal with lithium-ion battery safety, has a solution. He believes that the best way to manage EV fires right now is to let them burn. But such an approach not only goes against firefighters’ instincts—it’d require a significant cultural shift. Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;em&gt;This is today's edition of&amp;nbsp;The Download&lt;/em&gt;,&lt;em&gt;&amp;nbsp;our weekday newsletter that provides a daily dose of what's going on in the world of technology.&lt;/em&gt;&lt;/p&gt;  &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;This retina implant lets people with vision loss do a crossword puzzle&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;&lt;strong&gt;The news: &lt;/strong&gt;Science Corporation—a competitor to Neuralink founded by the former president of Elon Musk's brain-interface venture—has leapfrogged its rival after acquiring a vision implant in advanced testing for a fire-sale price. The implant produces a form of “artificial vision” that lets some patients read text and do crosswords, according to a report published in &lt;em&gt;The&lt;/em&gt; &lt;em&gt;New England Journal of Medicine &lt;/em&gt;today.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;How it works: &lt;/strong&gt;The implant is a microelectronic chip placed under the retina. Using signals from a camera mounted on a pair of glasses, the chip emits bursts of electricity in order to bypass photoreceptor cells damaged by macular degeneration, the leading cause of vision loss in the elderly. Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Antonio Regalado&lt;/em&gt;&lt;/p&gt; 
   &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;How will flowers respond to climate change?&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Flowers play a key role in most landscapes, from urban to rural areas. Yet flowers have much more to tell in their bright blooms: The very shape they take is formed by local and global climate conditions.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;The form of a flower is a visual display of its climate, if you know what to look for. In a dry year, its petals’ pigmentation may change. In a warm year, the flower might grow bigger. The flower’s ultraviolet-absorbing pigment increases with higher ozone levels.&lt;/p&gt;  &lt;p&gt;Now, a new artistic project sets out to answer the question: As the climate changes in the future, how might flowers change? Read the full story.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;—Annelie Berner&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;&lt;strong&gt;This story is from our forthcoming print issue, which is all about the body. If you haven’t already, &lt;/strong&gt;&lt;strong&gt;subscribe now&lt;/strong&gt;&lt;strong&gt; to receive future issues once they land.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;2025 climate tech companies to watch: Redwood Materials and its new AI microgrids&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;Over the past few years, Redwood Materials has become one of the top US battery recyclers, joining forces with the likes of Volkswagen, BMW, and Toyota to process old electric-vehicle batteries and recover materials that can be used to make new ones.&lt;/p&gt;&lt;p&gt;Now it's moving into reuse as well. Redwood Energy, a new branch of the company, incorporates used EV batteries into microgrids to power energy-hungry AI data centers. Read the full story.&lt;/p&gt; 

 &lt;p&gt;&lt;em&gt;—Peter Hall&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Redwood Materials is one of our 10 climate tech companies to watch—our annual list of some of the most promising climate tech firms on the planet. &lt;/strong&gt;&lt;strong&gt;Check out the rest of the list here&lt;/strong&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;The must-reads&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;&lt;em&gt;I’ve combed the internet to find you today’s most fun/important/scary/fascinating stories about technology.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;1 AWS is recovering from a major outage&amp;nbsp;&lt;/strong&gt;&lt;br /&gt;It’s racing to get hundreds of apps and services back online. (The Verge)&lt;br /&gt;+ &lt;em&gt;Snapchat, Roblox and banking services are among those affected. &lt;/em&gt;(The Guardian)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2 OpenAI made—then retracted—a claim it had made a major math breakthrough&lt;/strong&gt;&lt;br /&gt;After math experts and rival AI firms ridiculed its poorly-worded declaration. (TechCrunch)&lt;br /&gt;+ &lt;em&gt;What’s next for AI and math. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;3 The grave costs of Trump’s war on climate science&lt;/strong&gt;&lt;br /&gt;It’s affecting the accuracy of forecasting systems globally, not just in the US. (FT $)&lt;br /&gt;+ &lt;em&gt;Trump himself led an effort to derail plans to tax shipping pollution. &lt;/em&gt;(Politico $)&lt;br /&gt;+ &lt;em&gt;How to make clean energy progress under Trump in the states. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;4 China claims the US is behind a cyberattack on its national time center&lt;br /&gt;&lt;/strong&gt;It says it has years’ worth of irrefutable evidence of data stealing. (Reuters)&lt;br /&gt;+ &lt;em&gt;US experts allegedly exploited vulnerabilities in mobile phones belonging to National Time Service Center workers. &lt;/em&gt;(Bloomberg $)&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;5 Is AI-generated art real art?&lt;br /&gt;It’s a question gallery and museum curators across the world are debating. (NYT $)&lt;br /&gt;+ &lt;em&gt;Artisan craftmakers are happy to resist the pull of AI. &lt;/em&gt;(FT $)&lt;br /&gt;+ &lt;em&gt;This tool claims to trace how much of an AI image has been drawn from existing material.&lt;/em&gt; (The Guardian)&lt;br /&gt;+ &lt;em&gt;From slop to Sotheby’s? AI art enters a new phase. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;6 Chipmaker Nexperia has accused its ousted CEO of spreading falsehoods&lt;/strong&gt;&lt;br /&gt;Zhang Xuezheng reportedly claimed it was operating independently in China. (Bloomberg $)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;7 This whistleblower raised concerns about the safety of US data under DOGE&lt;/strong&gt;&lt;br /&gt;And says the hostile reception to his complaint led to him leaving his dream job. (WP $)&lt;br /&gt;+ &lt;em&gt;DOGE’s tech takeover threatens the safety and stability of our critical data. &lt;/em&gt;(MIT Technology Review)&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;8 Aid agencies have been criticized for using AI “poverty porn”&lt;br /&gt;&lt;/strong&gt;But the NGOs say its use protects the identities of real people in social media campaigns. (The Guardian)&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;9&lt;/strong&gt; &lt;strong&gt;EVs lose their value much faster than gas-powered cars&lt;br /&gt;&lt;/strong&gt;Which isn’t exactly an incentive for prospective first-time buyers. (Rest of World)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;&lt;strong&gt;10 What happens to our brains when we dream 🧠&lt;/strong&gt;&lt;br /&gt;We’re learning more about the many liminal states they can slip through. (Quanta Magazine)&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;Quote of the day&lt;/strong&gt;&lt;/p&gt;  &lt;p class="has-large-font-size"&gt;&lt;strong&gt;“Hoisted by their own GPTards.”&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;—Meta’s chief AI scientist Yann LeCun pokes fun at OpenAI after the company walked back its claim it had made a major math breakthrough in a post on X.&lt;/p&gt;    &lt;p class="has-medium-font-size"&gt;&lt;strong&gt;One more thing&lt;/strong&gt;&lt;/p&gt;  &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="alt" src="https://wp.technologyreview.com/wp-content/uploads/2025/02/250214_evbatteryfire.gif?fit=1456,818" /&gt;&lt;/figure&gt;  &lt;p&gt;&lt;strong&gt;One option for electric vehicle fires? Let them burn.&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Although there isn’t solid data on the frequency of EV battery fires, it’s no secret that these fires are happening.&lt;/p&gt;&lt;p&gt;Despite that, manufacturers offer no standardized steps on how to fight them or avoid them in the first place. What’s more, with EVs, it’s never entirely clear whether the fire is truly out.&lt;/p&gt;&lt;p&gt;Patrick Durham, the owner of one of a growing number of private companies helping first responders learn how to deal with lithium-ion battery safety, has a solution. He believes that the best way to manage EV fires right now is to let them burn. But such an approach not only goes against firefighters’ instincts—it’d require a significant cultural shift. Read the full story.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/20/1126099/the-download-a-promising-retina-implant-and-how-climate-change-affects-flowers/</guid><pubDate>Mon, 20 Oct 2025 12:10:00 +0000</pubDate></item><item><title>[NEW] Adobe Foundry wants to rebuild Firefly for your brand — not just tweak it (AI | VentureBeat)</title><link>https://venturebeat.com/ai/adobe-foundry-wants-to-rebuild-firefly-for-your-brand-not-just-tweak-it</link><description>[unable to retrieve full-text content]&lt;p&gt;Hoping to attract more enterprise teams to its ecosystem, &lt;a href="https://www.adobe.com/"&gt;&lt;u&gt;Adobe&lt;/u&gt;&lt;/a&gt; launched a new model customization service called Adobe AI Foundry, which would create bespoke versions of its flagship AI model, Firefly.&lt;/p&gt;&lt;p&gt;Adobe AI Foundry will work with enterprise customers to rearchitect and retrain &lt;a href="https://venturebeat.com/ai/adobe-firefly-ai-video-generator-debuts-the-most-ip-safe-ai-tool-yet"&gt;&lt;u&gt;Firefly models&lt;/u&gt;&lt;/a&gt; specific to the client. AI Foundry version models are different from custom Firefly models in that Foundry models understand multiple concepts compared to custom models with only a single concept. These models will &lt;a href="https://venturebeat.com/ai/adobe-previews-firefly-video-ai-model-offering-high-quality-generations"&gt;&lt;u&gt;also be multimodal&lt;/u&gt;&lt;/a&gt;, offering a wider use case than custom Firefly models, which can only ingest and respond with images. &lt;/p&gt;&lt;p&gt;Adobe AI Foundry models, with Firefly at its base, will know a company’s brand tone, image and video style, products and services and all its IP. The models will generate content based on this information for any use case the company wants. &lt;/p&gt;&lt;p&gt;Hannah Elsakr, vice president, GenAI New Business Ventures at Adobe, told VentureBeat that the idea to set up AI Foundry came because enterprise customers wanted more sophisticated custom versions of Firefly. But with how complex the needs of enterprises are, Adobe will be doing the rearchitecting rather than handing the reins over to customers. &lt;/p&gt;&lt;p&gt;“We will retrain our own Firefly commercially safe models with the enterprise IP. We keep that IP separate. We never take that back into the base model, and the enterprise itself owns that output,” Elsakr said. &lt;/p&gt;&lt;p&gt;Adobe will deploy the Foundry version of Firefly through its API solution, Firefly Services. &lt;/p&gt;&lt;p&gt;Elsakr likened AI Foundry to an advisory service, since Adobe will have teams working directly with enterprise customers to retrain the model. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deep tuning&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Elsakr refers to Foundry as a deep tuning method because it goes further than simply fine-tuning a model.&lt;/p&gt;&lt;p&gt;“The way we think about it, maybe more layman&amp;#x27;s terms, is that we&amp;#x27;re surgically reopening the Firefly-based models,” Elsakr said. “So you get the benefit of all the world&amp;#x27;s knowledge from our image model or a video model. We&amp;#x27;re going back in time and are bringing in the IP from the enterprise, like a brand. It could be footage from a shot style, whatever they have a license to contribute. We then retrain. We call this continuous pre-training, where we overweigh the model to dial some things differently. So we&amp;#x27;re literally retraining our base model, and that&amp;#x27;s why we call it deep tuning instead of fine-tuning.”&lt;/p&gt;&lt;p&gt;Part of the training pipeline involves Adobe’s embedded teams working with the company to identify the data they would need. Then the data is securely transferred and ingested before being tagged. It is fed to the base model, and then Adobe begins a pre-training model run. &lt;/p&gt;&lt;p&gt;Elsakr maintains the Foundry versions of Firefly will not be small or distilled models. Often, the additional data from companies expands the parameters of Firefly.&lt;/p&gt;&lt;p&gt;Two early customers of Adobe AI Foundry are Home Depot and Walt Disney Imagineering, the research and development arm of Disney for its theme parks. &lt;/p&gt;&lt;p&gt;“We are always exploring innovative ways to enhance our customer experience and streamline our creative workflows. Adobe’s AI Foundry represents an exciting step forward in embracing cutting-edge technologies to deepen customer engagement and deliver impactful content across our digital channels,” said Molly Battin, senior vice president and chief marketing officer at The Home Depot.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;More customization&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprises often turn to &lt;a href="https://venturebeat.com/ai/fine-tuning-vs-in-context-learning-new-research-guides-better-llm-customization-for-real-world-tasks"&gt;&lt;u&gt;fine-tuning and model customization&lt;/u&gt;&lt;/a&gt; to bring large language models with their vast external knowledge closer to their company’s needs. Fine-tuning also enables enterprise users to utilize models only in the context of their organization’s data, so the model doesn’t respond with text wholly unrelated to the business.&lt;/p&gt;&lt;p&gt;Most organizations, however, do the fine-tuning themselves. They connect to the model’s API and begin retraining it to answer based on their ground truth or their preferences. Several methods for fine-tuning exist, including some that can be done &lt;a href="https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models"&gt;&lt;u&gt;with just a prompt&lt;/u&gt;&lt;/a&gt;. Other model providers also try to make it easier for their customers to fine-tune models, such as &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; with its &lt;a href="https://venturebeat.com/ai/you-can-now-fine-tune-your-enterprises-own-version-of-openais-o4-mini-reasoning-model-with-reinforcement-learning"&gt;&lt;u&gt;o4-mini reasoning model&lt;/u&gt;&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Elsakr said she expects some companies will have three versions of Firefly: the Foundry version for most projects, a custom Firefly for specific single-concept use cases, and the base Firefly because some teams want a model less encumbered by corporate knowledge. &lt;/p&gt;</description><content:encoded>[unable to retrieve full-text content]&lt;p&gt;Hoping to attract more enterprise teams to its ecosystem, &lt;a href="https://www.adobe.com/"&gt;&lt;u&gt;Adobe&lt;/u&gt;&lt;/a&gt; launched a new model customization service called Adobe AI Foundry, which would create bespoke versions of its flagship AI model, Firefly.&lt;/p&gt;&lt;p&gt;Adobe AI Foundry will work with enterprise customers to rearchitect and retrain &lt;a href="https://venturebeat.com/ai/adobe-firefly-ai-video-generator-debuts-the-most-ip-safe-ai-tool-yet"&gt;&lt;u&gt;Firefly models&lt;/u&gt;&lt;/a&gt; specific to the client. AI Foundry version models are different from custom Firefly models in that Foundry models understand multiple concepts compared to custom models with only a single concept. These models will &lt;a href="https://venturebeat.com/ai/adobe-previews-firefly-video-ai-model-offering-high-quality-generations"&gt;&lt;u&gt;also be multimodal&lt;/u&gt;&lt;/a&gt;, offering a wider use case than custom Firefly models, which can only ingest and respond with images. &lt;/p&gt;&lt;p&gt;Adobe AI Foundry models, with Firefly at its base, will know a company’s brand tone, image and video style, products and services and all its IP. The models will generate content based on this information for any use case the company wants. &lt;/p&gt;&lt;p&gt;Hannah Elsakr, vice president, GenAI New Business Ventures at Adobe, told VentureBeat that the idea to set up AI Foundry came because enterprise customers wanted more sophisticated custom versions of Firefly. But with how complex the needs of enterprises are, Adobe will be doing the rearchitecting rather than handing the reins over to customers. &lt;/p&gt;&lt;p&gt;“We will retrain our own Firefly commercially safe models with the enterprise IP. We keep that IP separate. We never take that back into the base model, and the enterprise itself owns that output,” Elsakr said. &lt;/p&gt;&lt;p&gt;Adobe will deploy the Foundry version of Firefly through its API solution, Firefly Services. &lt;/p&gt;&lt;p&gt;Elsakr likened AI Foundry to an advisory service, since Adobe will have teams working directly with enterprise customers to retrain the model. &lt;/p&gt;&lt;h3&gt;&lt;b&gt;Deep tuning&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Elsakr refers to Foundry as a deep tuning method because it goes further than simply fine-tuning a model.&lt;/p&gt;&lt;p&gt;“The way we think about it, maybe more layman&amp;#x27;s terms, is that we&amp;#x27;re surgically reopening the Firefly-based models,” Elsakr said. “So you get the benefit of all the world&amp;#x27;s knowledge from our image model or a video model. We&amp;#x27;re going back in time and are bringing in the IP from the enterprise, like a brand. It could be footage from a shot style, whatever they have a license to contribute. We then retrain. We call this continuous pre-training, where we overweigh the model to dial some things differently. So we&amp;#x27;re literally retraining our base model, and that&amp;#x27;s why we call it deep tuning instead of fine-tuning.”&lt;/p&gt;&lt;p&gt;Part of the training pipeline involves Adobe’s embedded teams working with the company to identify the data they would need. Then the data is securely transferred and ingested before being tagged. It is fed to the base model, and then Adobe begins a pre-training model run. &lt;/p&gt;&lt;p&gt;Elsakr maintains the Foundry versions of Firefly will not be small or distilled models. Often, the additional data from companies expands the parameters of Firefly.&lt;/p&gt;&lt;p&gt;Two early customers of Adobe AI Foundry are Home Depot and Walt Disney Imagineering, the research and development arm of Disney for its theme parks. &lt;/p&gt;&lt;p&gt;“We are always exploring innovative ways to enhance our customer experience and streamline our creative workflows. Adobe’s AI Foundry represents an exciting step forward in embracing cutting-edge technologies to deepen customer engagement and deliver impactful content across our digital channels,” said Molly Battin, senior vice president and chief marketing officer at The Home Depot.&lt;/p&gt;&lt;h3&gt;&lt;b&gt;More customization&lt;/b&gt;&lt;/h3&gt;&lt;p&gt;Enterprises often turn to &lt;a href="https://venturebeat.com/ai/fine-tuning-vs-in-context-learning-new-research-guides-better-llm-customization-for-real-world-tasks"&gt;&lt;u&gt;fine-tuning and model customization&lt;/u&gt;&lt;/a&gt; to bring large language models with their vast external knowledge closer to their company’s needs. Fine-tuning also enables enterprise users to utilize models only in the context of their organization’s data, so the model doesn’t respond with text wholly unrelated to the business.&lt;/p&gt;&lt;p&gt;Most organizations, however, do the fine-tuning themselves. They connect to the model’s API and begin retraining it to answer based on their ground truth or their preferences. Several methods for fine-tuning exist, including some that can be done &lt;a href="https://venturebeat.com/ai/researchers-find-adding-this-one-simple-sentence-to-prompts-makes-ai-models"&gt;&lt;u&gt;with just a prompt&lt;/u&gt;&lt;/a&gt;. Other model providers also try to make it easier for their customers to fine-tune models, such as &lt;a href="https://openai.com/"&gt;&lt;u&gt;OpenAI&lt;/u&gt;&lt;/a&gt; with its &lt;a href="https://venturebeat.com/ai/you-can-now-fine-tune-your-enterprises-own-version-of-openais-o4-mini-reasoning-model-with-reinforcement-learning"&gt;&lt;u&gt;o4-mini reasoning model&lt;/u&gt;&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;Elsakr said she expects some companies will have three versions of Firefly: the Foundry version for most projects, a custom Firefly for specific single-concept use cases, and the base Firefly because some teams want a model less encumbered by corporate knowledge. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://venturebeat.com/ai/adobe-foundry-wants-to-rebuild-firefly-for-your-brand-not-just-tweak-it</guid><pubDate>Mon, 20 Oct 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Adobe launches a foundry service that builds custom generative AI models for enterprises (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/adobe-launches-a-foundry-service-that-builds-custom-generative-ai-models-for-enterprises/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/Adobe.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Creative design giant Adobe is beefing up the products it offers businesses to include custom generative AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe launched Adobe AI Foundry on Monday, a new offering that allows enterprises to work with the company to build custom generative AI models trained on their branding and intellectual property.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Foundry’s custom models, which can produce text, images, video, and other mediums like 3D scenes, are built off Adobe’s Firefly family of AI models. These Firefly models were launched in 2023 and have been trained entirely on licensed data. Adobe’s Foundry service then fine-tunes these models for each customer using their intellectual property.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Foundry service pricing is based on usage, as opposed to by seat like many of Adobe’s other products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hannah Elsakr, a vice president of generative AI new business ventures at Adobe, told TechCrunch the Foundry service was a natural expansion of the company’s enterprise AI products — and customers were asking for more customization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is elevating a lot of the capabilities we already had,” Elsakr said. “The enterprise has asked us to come in and advise us, help us, partner with us, be our premier creative marketing AI partner on this.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Adobe released its Firefly models in 2023, enterprises have used them to create more than 25 billion assets.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Elsakr said these custom models will help brands be able to better keep up with their advertising campaigns. A customer could create an ad campaign for a product once and use a custom Adobe model to help generate the same ad for different seasons, languages, or formats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s highly personalized,” Elsakr said. “We’ve been talking about personalized commerce for so long, but generative AI and Firefly make it possible to put the brand in the hand of the consumer in an on-brand way.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the capabilities of the new tools, Esakr said Adobe is not trying to replace human creatives by any means and rather just give them better versions of the tools they were already using to create their content.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Our stance is humanity is at the center of creativity and that can’t be replaced,” Elsakr said. “We have been for decades in the business of providing creative tooling that helps uplift narrative, tell storytelling, your ability to envision and execute your creative vision. Firefly and Foundry are just the next evolution of giving you tools in the toolkit that elevate your ability to tell a story.”&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2023/08/Adobe.jpg?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Creative design giant Adobe is beefing up the products it offers businesses to include custom generative AI models. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Adobe launched Adobe AI Foundry on Monday, a new offering that allows enterprises to work with the company to build custom generative AI models trained on their branding and intellectual property.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The Foundry’s custom models, which can produce text, images, video, and other mediums like 3D scenes, are built off Adobe’s Firefly family of AI models. These Firefly models were launched in 2023 and have been trained entirely on licensed data. Adobe’s Foundry service then fine-tunes these models for each customer using their intellectual property.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Foundry service pricing is based on usage, as opposed to by seat like many of Adobe’s other products.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Hannah Elsakr, a vice president of generative AI new business ventures at Adobe, told TechCrunch the Foundry service was a natural expansion of the company’s enterprise AI products — and customers were asking for more customization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“This is elevating a lot of the capabilities we already had,” Elsakr said. “The enterprise has asked us to come in and advise us, help us, partner with us, be our premier creative marketing AI partner on this.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Since Adobe released its Firefly models in 2023, enterprises have used them to create more than 25 billion assets.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Elsakr said these custom models will help brands be able to better keep up with their advertising campaigns. A customer could create an ad campaign for a product once and use a custom Adobe model to help generate the same ad for different seasons, languages, or formats.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“It’s highly personalized,” Elsakr said. “We’ve been talking about personalized commerce for so long, but generative AI and Firefly make it possible to put the brand in the hand of the consumer in an on-brand way.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Despite the capabilities of the new tools, Esakr said Adobe is not trying to replace human creatives by any means and rather just give them better versions of the tools they were already using to create their content.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“Our stance is humanity is at the center of creativity and that can’t be replaced,” Elsakr said. “We have been for decades in the business of providing creative tooling that helps uplift narrative, tell storytelling, your ability to envision and execute your creative vision. Firefly and Foundry are just the next evolution of giving you tools in the toolkit that elevate your ability to tell a story.”&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/adobe-launches-a-foundry-service-that-builds-custom-generative-ai-models-for-enterprises/</guid><pubDate>Mon, 20 Oct 2025 13:00:00 +0000</pubDate></item><item><title>[NEW] Arm provides edge AI platform to startups via flexible access (AI News)</title><link>https://www.artificialintelligence-news.com/news/arm-provides-edge-ai-platform-startups-flexible-access/</link><description>&lt;p&gt;Arm has announced that it’s providing its most powerful edge AI platform, Armv9, to startups via its Flexible Access programme.&lt;/p&gt;&lt;p&gt;The “Flexible Access” model is essentially a ‘try before you buy’ for chip designers. It gives companies upfront, low-cost, or no-cost (for qualifying startups) access to a wide range of Arm technology, tools, and resources. They can experiment and iterate designs freely while only paying license fees for the technology they use in final designs.&lt;/p&gt;&lt;p&gt;This approach has already been a “catalyst for innovation,” according to Arm. The model helped to create around 400 successful chip designs (or “tape-outs”) over the last five years. You’ve likely heard of some of the companies already using it, like Raspberry Pi, Hailo, and SiMa.ai.&lt;/p&gt;&lt;p&gt;The Armv9 edge AI platform pairs the super-efficient Arm Cortex-A320 processor with the Arm Ethos-U85 NPU, which is the bit that handles the heavy AI lifting. This duo is capable of running AI models with over one billion parameters right on the device itself, no cloud connection needed.&lt;/p&gt;&lt;p&gt;This is the tech that will power the next-gen edge AI applications such as smart cameras that don’t just record but understand what they’re seeing, smart home gadgets that learn your habits, and robots you can interact with using vision, voice, and gesture.&lt;/p&gt;&lt;p&gt;Paul Williamson, who runs the IoT business at Arm, believes the next wave of AI innovation will happen “at the edge – in the devices, interfaces, and systems that bring intelligence closer to where data is created.”&lt;/p&gt;&lt;p&gt;A huge benefit here is privacy and security. By processing everything locally, machines can “perceive and respond like humans, while keeping inference and data processing securely on-device”. Your personal data doesn’t have to be sent off to a server just to figure out what you said. The Armv9 platform also bakes in security features like Pointer Authentication Code (PAC) and Memory Tagging Extension (MTE) to keep that on-device data safe.&lt;/p&gt;&lt;p&gt;Research from VDC predicts that, by 2028, AI will be the “dominant technology used across IoT projects”. Arm’s technology is “already at the center of this transformation,” and this move just solidifies its position.&lt;/p&gt;&lt;p&gt;For all the developers keen to get started with edge AI, the Arm Cortex-A320 will be available through the programme in November 2025, with the Ethos-U85 AI processor following in early 2026.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;NVIDIA GPUs to power Oracle’s next-gen enterprise AI services&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109805" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-1.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;p&gt;Arm has announced that it’s providing its most powerful edge AI platform, Armv9, to startups via its Flexible Access programme.&lt;/p&gt;&lt;p&gt;The “Flexible Access” model is essentially a ‘try before you buy’ for chip designers. It gives companies upfront, low-cost, or no-cost (for qualifying startups) access to a wide range of Arm technology, tools, and resources. They can experiment and iterate designs freely while only paying license fees for the technology they use in final designs.&lt;/p&gt;&lt;p&gt;This approach has already been a “catalyst for innovation,” according to Arm. The model helped to create around 400 successful chip designs (or “tape-outs”) over the last five years. You’ve likely heard of some of the companies already using it, like Raspberry Pi, Hailo, and SiMa.ai.&lt;/p&gt;&lt;p&gt;The Armv9 edge AI platform pairs the super-efficient Arm Cortex-A320 processor with the Arm Ethos-U85 NPU, which is the bit that handles the heavy AI lifting. This duo is capable of running AI models with over one billion parameters right on the device itself, no cloud connection needed.&lt;/p&gt;&lt;p&gt;This is the tech that will power the next-gen edge AI applications such as smart cameras that don’t just record but understand what they’re seeing, smart home gadgets that learn your habits, and robots you can interact with using vision, voice, and gesture.&lt;/p&gt;&lt;p&gt;Paul Williamson, who runs the IoT business at Arm, believes the next wave of AI innovation will happen “at the edge – in the devices, interfaces, and systems that bring intelligence closer to where data is created.”&lt;/p&gt;&lt;p&gt;A huge benefit here is privacy and security. By processing everything locally, machines can “perceive and respond like humans, while keeping inference and data processing securely on-device”. Your personal data doesn’t have to be sent off to a server just to figure out what you said. The Armv9 platform also bakes in security features like Pointer Authentication Code (PAC) and Memory Tagging Extension (MTE) to keep that on-device data safe.&lt;/p&gt;&lt;p&gt;Research from VDC predicts that, by 2028, AI will be the “dominant technology used across IoT projects”. Arm’s technology is “already at the center of this transformation,” and this move just solidifies its position.&lt;/p&gt;&lt;p&gt;For all the developers keen to get started with edge AI, the Arm Cortex-A320 will be available through the programme in November 2025, with the Ethos-U85 AI processor following in early 2026.&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: &lt;/strong&gt;&lt;strong&gt;NVIDIA GPUs to power Oracle’s next-gen enterprise AI services&lt;/strong&gt;&lt;/p&gt;&lt;figure class="wp-block-image aligncenter size-full is-resized"&gt;&lt;img alt="Banner for AI &amp;amp; Big Data Expo by TechEx events." class="wp-image-109805" height="90" src="https://www.artificialintelligence-news.com/wp-content/uploads/2025/10/image-1.png" width="728" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events including the Cyber Security Expo, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/arm-provides-edge-ai-platform-startups-flexible-access/</guid><pubDate>Mon, 20 Oct 2025 13:02:53 +0000</pubDate></item><item><title>[NEW] Final countdown: Only 7 days until TechCrunch Disrupt 2025 and ticket prices increase (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/final-countdown-only-7-days-until-techcrunch-disrupt-2025-and-ticket-prices-increase/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;T-minus 7 days until &lt;strong&gt;TechCrunch&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;Disrupt&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;2025&lt;/strong&gt; officially kicks off at San Francisco’s Moscone West! One of the year’s biggest tech events is ready to dominate the Bay Area’s thriving tech landscape for almost the entire week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Get ready for one of the biggest tech gatherings on October 27-29. Tech experts from every corner of the globe will come together to engage with the latest innovations, learn trends, and connect through unparalleled networking. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on Disrupt ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; on your pass. &lt;strong&gt;Bring a plus-one&lt;/strong&gt; and get 60% off their ticket, or &lt;strong&gt;bring your community&lt;/strong&gt; and save 30%.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 7 days" class="wp-image-3059244" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_7Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-you-nbsp-can-t-nbsp-miss-disrupt-2025"&gt;Why you&amp;nbsp;can’t&amp;nbsp;miss Disrupt 2025&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-10-000-startup-and-vc-leaders"&gt;10,000+ startup and VC leaders&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Forge powerful connections with the 10,000 founders, VCs, operators, and innovators that gather at Disrupt.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-300-nbsp-innovations"&gt;300+&amp;nbsp;innovations&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Discover groundbreaking innovations from 300+ startups from around the world&amp;nbsp;showcased&amp;nbsp;in the Expo Hall&amp;nbsp;and spread throughout the venue. See their latest breakthroughs in action.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-250-nbsp-industry-experts"&gt;250+&amp;nbsp;industry experts&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Gain invaluable insights from some of the top industry heavyweights across&amp;nbsp;&lt;strong&gt;5&amp;nbsp;of the industry stages&lt;/strong&gt;&amp;nbsp;and sessions.&amp;nbsp;Featured speakers include&amp;nbsp;Vinod Khosla,&amp;nbsp;Elizabeth Stone&amp;nbsp;(Netflix), and&amp;nbsp;Kevin Scott&amp;nbsp;(Microsoft).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Box CEO Aaron Levie on stage at TechCrunch Disrupt in San Francisco in 2019." class="wp-image-2833981" height="454" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1178603809.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-200-nbsp-hands-on-sessions"&gt;200+&amp;nbsp;hands-on sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Engage in &lt;strong&gt;Q&amp;amp;A breakouts and dynamic roundtable sessions&lt;/strong&gt; led by industry experts, tackling today’s key challenges and tomorrow’s breakthroughs. Topics include the scaling playbook for 2026, exiting strategies, the future of AI, and so much more. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-startup-battlefield-200"&gt;Startup Battlefield 200&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Witness the high-stakes &lt;strong&gt;startup showdown&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;that is a highlight of Disrupt. Twenty handpicked startups, chosen from thousands, will pitch to a panel of leading VC judges, competing for a $100,000 equity-free prize, VC attention, and the iconic Disrupt Cup.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Salva Health Co-Founder &amp;amp; CEO Valentina Agudelo Vargas, winner of the Startup Battlefield 2024, poses onstage during TechCrunch Disrupt 2024 Day 3 at Moscone Center on October 30, 2024 in San Francisco." class="wp-image-2913234" height="453" src="https://techcrunch.com/wp-content/uploads/2024/11/54105085427_2cae9d0502_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-meaningful-connections-maximum-impact"&gt;Meaningful connections, maximum impact&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to impromptu networking across the venue, use Braindate to post and explore topics, then dive into them in 1:1 or small-group sessions in the Networking Lounge. This is where meaningful connections start.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-50-side-events"&gt;50+ Side Events&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Extend the excitement of Disrupt beyond the main event by joining company-hosted &lt;strong&gt;Side Events&lt;/strong&gt; across San Francisco. Immerse yourself in the vibrant tech community in a relaxed setting, with options ranging from workshops and happy hours to cocktail parties, morning runs, and job fairs — there’s something for everyone.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-10-000-tech-leaders-will-be-at-disrupt-will-you-be-nbsp-there"&gt;10,000+ tech leaders will be at Disrupt. Will you be&amp;nbsp;there?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;There are endless reasons to join this epic tech conference, but the best way to understand its value is to experience it yourself. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on the last 7 days of ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; and take advantage of &lt;strong&gt;60% savings on a second pass&lt;/strong&gt;. Prices for all tickets will increase when the doors open at Moscone West at Disrupt on October 27.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;T-minus 7 days until &lt;strong&gt;TechCrunch&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;Disrupt&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;&lt;strong&gt;2025&lt;/strong&gt; officially kicks off at San Francisco’s Moscone West! One of the year’s biggest tech events is ready to dominate the Bay Area’s thriving tech landscape for almost the entire week.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Get ready for one of the biggest tech gatherings on October 27-29. Tech experts from every corner of the globe will come together to engage with the latest innovations, learn trends, and connect through unparalleled networking. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on Disrupt ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; on your pass. &lt;strong&gt;Bring a plus-one&lt;/strong&gt; and get 60% off their ticket, or &lt;strong&gt;bring your community&lt;/strong&gt; and save 30%.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 7 days" class="wp-image-3059244" height="383" src="https://techcrunch.com/wp-content/uploads/2025/10/TC25_7Days-16X9-Dark.png?w=680" width="680" /&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-why-you-nbsp-can-t-nbsp-miss-disrupt-2025"&gt;Why you&amp;nbsp;can’t&amp;nbsp;miss Disrupt 2025&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-10-000-startup-and-vc-leaders"&gt;10,000+ startup and VC leaders&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Forge powerful connections with the 10,000 founders, VCs, operators, and innovators that gather at Disrupt.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-300-nbsp-innovations"&gt;300+&amp;nbsp;innovations&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Discover groundbreaking innovations from 300+ startups from around the world&amp;nbsp;showcased&amp;nbsp;in the Expo Hall&amp;nbsp;and spread throughout the venue. See their latest breakthroughs in action.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-250-nbsp-industry-experts"&gt;250+&amp;nbsp;industry experts&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Gain invaluable insights from some of the top industry heavyweights across&amp;nbsp;&lt;strong&gt;5&amp;nbsp;of the industry stages&lt;/strong&gt;&amp;nbsp;and sessions.&amp;nbsp;Featured speakers include&amp;nbsp;Vinod Khosla,&amp;nbsp;Elizabeth Stone&amp;nbsp;(Netflix), and&amp;nbsp;Kevin Scott&amp;nbsp;(Microsoft).&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Box CEO Aaron Levie on stage at TechCrunch Disrupt in San Francisco in 2019." class="wp-image-2833981" height="454" src="https://techcrunch.com/wp-content/uploads/2024/08/GettyImages-1178603809.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Steve Jennings / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-200-nbsp-hands-on-sessions"&gt;200+&amp;nbsp;hands-on sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Engage in &lt;strong&gt;Q&amp;amp;A breakouts and dynamic roundtable sessions&lt;/strong&gt; led by industry experts, tackling today’s key challenges and tomorrow’s breakthroughs. Topics include the scaling playbook for 2026, exiting strategies, the future of AI, and so much more. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h3 class="wp-block-heading" id="h-startup-battlefield-200"&gt;Startup Battlefield 200&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Witness the high-stakes &lt;strong&gt;startup showdown&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;that is a highlight of Disrupt. Twenty handpicked startups, chosen from thousands, will pitch to a panel of leading VC judges, competing for a $100,000 equity-free prize, VC attention, and the iconic Disrupt Cup.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Salva Health Co-Founder &amp;amp; CEO Valentina Agudelo Vargas, winner of the Startup Battlefield 2024, poses onstage during TechCrunch Disrupt 2024 Day 3 at Moscone Center on October 30, 2024 in San Francisco." class="wp-image-2913234" height="453" src="https://techcrunch.com/wp-content/uploads/2024/11/54105085427_2cae9d0502_o.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-meaningful-connections-maximum-impact"&gt;Meaningful connections, maximum impact&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;In addition to impromptu networking across the venue, use Braindate to post and explore topics, then dive into them in 1:1 or small-group sessions in the Networking Lounge. This is where meaningful connections start.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-50-side-events"&gt;50+ Side Events&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;Extend the excitement of Disrupt beyond the main event by joining company-hosted &lt;strong&gt;Side Events&lt;/strong&gt; across San Francisco. Immerse yourself in the vibrant tech community in a relaxed setting, with options ranging from workshops and happy hours to cocktail parties, morning runs, and job fairs — there’s something for everyone.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-10-000-tech-leaders-will-be-at-disrupt-will-you-be-nbsp-there"&gt;10,000+ tech leaders will be at Disrupt. Will you be&amp;nbsp;there?&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;There are endless reasons to join this epic tech conference, but the best way to understand its value is to experience it yourself. &lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Don’t miss out on the last 7 days of ticket savings! &lt;strong&gt;Save up to $444&lt;/strong&gt; and take advantage of &lt;strong&gt;60% savings on a second pass&lt;/strong&gt;. Prices for all tickets will increase when the doors open at Moscone West at Disrupt on October 27.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/final-countdown-only-7-days-until-techcrunch-disrupt-2025-and-ticket-prices-increase/</guid><pubDate>Mon, 20 Oct 2025 14:00:00 +0000</pubDate></item><item><title>[NEW] Last-minute ticket deal for TechCrunch Disrupt 2025: Save 60% on your plus-one (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/last-minute-ticket-deal-for-techcrunch-disrupt-2025-save-60-on-your-plus-one/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today begins the 7-day countdown to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, the flagship tech conference bringing together 10,000+ leaders, VCs, operators, and innovators, taking place at Moscone West in San Francisco, October 27–29.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To celebrate the countdown, we’re offering a special 60% discount for your plus-one. Buy your pass and save up to $444, &lt;strong&gt;plus get 60% off a second ticket&lt;/strong&gt; for your co-founder, partner, friend, or team member.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your two discounted passes here&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-ticket-type-nbsp-perks"&gt;Disrupt 2025 ticket type&amp;nbsp;perks&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-industry-stage-programming-5-stages"&gt;Industry&amp;nbsp;Stage&amp;nbsp;programming: 5&amp;nbsp;stages&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For&amp;nbsp;Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Five massive stages that are designed to bring insights from the top leaders across the tech ecosystem, such as Vinod Khosla, Aaron Levie (Box), Elizabeth Stone (Netflix), Kevin Scott (Microsoft), and more. &lt;strong&gt;Explore the agenda and meet the speakers&lt;/strong&gt; taking the stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;AI Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Join AI experts as they unpack the latest advancements and possibilities in artificial intelligence. Explore the science driving deep tech innovations, the products it’s shaping, and the ethical, social, and legal questions it raises.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Builders Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— One of the classic founder-focused stages at Disrupt, the Builders Stage delivers the key strategies and lessons from veteran founders and builders to help you achieve your entrepreneurial dreams.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Sit front and center on the main stage to witness the iconic Startup Battlefield 200, where the top 20 handpicked startups pitch to a panel of leading VCs. Learn from these leaders about what it takes to build a viable startup, and gain insights from exclusive discussion sessions with industry heavyweights.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt startup Battlefield presentation" class="wp-image-2964435" height="453" src="https://techcrunch.com/wp-content/uploads/2024/12/Startup-battlefield-disrupt-2024.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Going Public Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— No matter if you’re at day one or planning your IPO, the Going Public Stage delivers lessons, frameworks, and founder stories that resonate across every stage of the scaling journey.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Space Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Emerging companies are disrupting the space industry at a pivotal time when technology and markets are aligning to drive significant breakthroughs in space exploration.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-interactive-roundtable-sessions"&gt;Interactive roundtable sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These &lt;strong&gt;small group dynamic sessions&lt;/strong&gt; let everyone dive deep into a topic, ask questions, share experiences, and truly connect, led by industry leaders from across different sectors in tech.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-q-amp-a-breakout-nbsp-sessions"&gt;Q&amp;amp;A breakout&amp;nbsp;sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Located right next to the Expo Hall, &lt;strong&gt;these sessions&lt;/strong&gt; are led by a panel of industry experts who will cover a topic and then end the session by answering questions from the audience. Bring your pen and notebook; this is where the learning happens.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Breakout session" class="wp-image-2758365" height="340" src="https://techcrunch.com/wp-content/uploads/2024/05/breakout_1200x600.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-expo-hall"&gt;Expo Hall&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meet 300+ startups showcasing their innovations at all stages. The Expo Hall is the heart of the venue, where 10,000+ tech leaders and VCs come to discover what’s next and connect with startups ready to pitch and demo their solutions.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-powerful-networking"&gt;Powerful networking&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for startup and individual growth. Schedule 1:1 or small group sessions via Braindate, then meet in the Networking Lounge to dive deep into topics, spark insightful conversations, and form impactful connections.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Braindate networking" class="wp-image-2953563" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-braindate-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-disrupt-and-the-plus-one-ticket-discount"&gt;Don’t miss Disrupt — and the plus-one ticket discount&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner. Don’t miss the final chance to pocket real savings for plus-one and group passes. Bring your crew and experience it together. &lt;strong&gt;Register for you and your plus-one&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;or &lt;strong&gt;grab community passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Today begins the 7-day countdown to &lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt;, the flagship tech conference bringing together 10,000+ leaders, VCs, operators, and innovators, taking place at Moscone West in San Francisco, October 27–29.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;To celebrate the countdown, we’re offering a special 60% discount for your plus-one. Buy your pass and save up to $444, &lt;strong&gt;plus get 60% off a second ticket&lt;/strong&gt; for your co-founder, partner, friend, or team member.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Get your two discounted passes here&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 class="wp-block-heading" id="h-disrupt-2025-ticket-type-nbsp-perks"&gt;Disrupt 2025 ticket type&amp;nbsp;perks&lt;/h2&gt;

&lt;h3 class="wp-block-heading" id="h-industry-stage-programming-5-stages"&gt;Industry&amp;nbsp;Stage&amp;nbsp;programming: 5&amp;nbsp;stages&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For&amp;nbsp;Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Five massive stages that are designed to bring insights from the top leaders across the tech ecosystem, such as Vinod Khosla, Aaron Levie (Box), Elizabeth Stone (Netflix), Kevin Scott (Microsoft), and more. &lt;strong&gt;Explore the agenda and meet the speakers&lt;/strong&gt; taking the stage.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;AI Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Join AI experts as they unpack the latest advancements and possibilities in artificial intelligence. Explore the science driving deep tech innovations, the products it’s shaping, and the ethical, social, and legal questions it raises.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt AI Stage" class="wp-image-3048038" height="454" src="https://techcrunch.com/wp-content/uploads/2025/09/Disrupt-2025-AI-Stage.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Builders Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— One of the classic founder-focused stages at Disrupt, the Builders Stage delivers the key strategies and lessons from veteran founders and builders to help you achieve your entrepreneurial dreams.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Disrupt Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Sit front and center on the main stage to witness the iconic Startup Battlefield 200, where the top 20 handpicked startups pitch to a panel of leading VCs. Learn from these leaders about what it takes to build a viable startup, and gain insights from exclusive discussion sessions with industry heavyweights.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt startup Battlefield presentation" class="wp-image-2964435" height="453" src="https://techcrunch.com/wp-content/uploads/2024/12/Startup-battlefield-disrupt-2024.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Kimberly White / Getty Images&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Going Public Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— No matter if you’re at day one or planning your IPO, the Going Public Stage delivers lessons, frameworks, and founder stories that resonate across every stage of the scaling journey.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;Space Stage&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;— Emerging companies are disrupting the space industry at a pivotal time when technology and markets are aligning to drive significant breakthroughs in space exploration.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-interactive-roundtable-sessions"&gt;Interactive roundtable sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For Attendee, Founder, Investor, Student, and&amp;nbsp;Non-Profit&amp;nbsp;Passes&lt;/em&gt;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;These &lt;strong&gt;small group dynamic sessions&lt;/strong&gt; let everyone dive deep into a topic, ask questions, share experiences, and truly connect, led by industry leaders from across different sectors in tech.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-q-amp-a-breakout-nbsp-sessions"&gt;Q&amp;amp;A breakout&amp;nbsp;sessions&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Located right next to the Expo Hall, &lt;strong&gt;these sessions&lt;/strong&gt; are led by a panel of industry experts who will cover a topic and then end the session by answering questions from the audience. Bring your pen and notebook; this is where the learning happens.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Breakout session" class="wp-image-2758365" height="340" src="https://techcrunch.com/wp-content/uploads/2024/05/breakout_1200x600.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;TechCrunch&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 class="wp-block-heading" id="h-expo-hall"&gt;Expo Hall&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meet 300+ startups showcasing their innovations at all stages. The Expo Hall is the heart of the venue, where 10,000+ tech leaders and VCs come to discover what’s next and connect with startups ready to pitch and demo their solutions.&lt;/p&gt;

&lt;h3 class="wp-block-heading" id="h-powerful-networking"&gt;Powerful networking&lt;/h3&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;em&gt;For all pass types&lt;/em&gt;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Disrupt is &lt;em&gt;the&lt;/em&gt; launchpad for startup and individual growth. Schedule 1:1 or small group sessions via Braindate, then meet in the Networking Lounge to dive deep into topics, spark insightful conversations, and form impactful connections.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 Braindate networking" class="wp-image-2953563" height="453" src="https://techcrunch.com/wp-content/uploads/2025/01/disrupt-2024-braindate-networking.jpg?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Slava Blazer Photography&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 class="wp-block-heading" id="h-don-t-miss-disrupt-and-the-plus-one-ticket-discount"&gt;Don’t miss Disrupt — and the plus-one ticket discount&lt;/h2&gt;

&lt;p class="wp-block-paragraph"&gt;&lt;strong&gt;TechCrunch Disrupt 2025&lt;/strong&gt; is just around the corner. Don’t miss the final chance to pocket real savings for plus-one and group passes. Bring your crew and experience it together. &lt;strong&gt;Register for you and your plus-one&lt;/strong&gt;&lt;strong&gt; &lt;/strong&gt;or &lt;strong&gt;grab community passes&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="TechCrunch Disrupt 2025 no anniversary" class="wp-image-3040972" height="383" src="https://techcrunch.com/wp-content/uploads/2025/08/TC25_Disrupt_General_Article_No-Anniversary-at-all_Headers_1920x1080.png?w=680" width="680" /&gt;&lt;/figure&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/last-minute-ticket-deal-for-techcrunch-disrupt-2025-save-60-on-your-plus-one/</guid><pubDate>Mon, 20 Oct 2025 14:30:00 +0000</pubDate></item><item><title>[NEW] OpenEvidence, the ChatGPT for doctors, raises $200M at $6B valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/openevidence-the-chatgpt-for-doctors-raises-200m-at-6b-valuation/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/openevidence.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenEvidence, a tool that doctors and nurses have likened to ChatGPT for medicine, plans to announce a $200 million raise at a $6 billion valuation, The New York Times reports.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh funds come three months after the startup raised a $210 million round at a $3.5 billion valuation, a testament to the intense investor interest in industry-specific AI applications.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Trained on medical journals like JAMA and the New England Journal of Medicine, the OpenEvidence platform helps users quickly get answers to existing medical knowledge to help treat patients. Verified medical professionals can access OpenEvidence’s tool, which is supported by advertising, for free. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenEvidence has grown quickly since its founding in 2022. Its number of clinical consultations per month has nearly doubled to 15 million since July, per the Times. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Google Ventures, with participation from Sequoia Capital, Kleiner Perkins, Blackstone, Thrive Capital, Coatue Management, Bond, and Craft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenEvidence for comment.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2025/10/openevidence.png?w=1200" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;OpenEvidence, a tool that doctors and nurses have likened to ChatGPT for medicine, plans to announce a $200 million raise at a $6 billion valuation, The New York Times reports.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The fresh funds come three months after the startup raised a $210 million round at a $3.5 billion valuation, a testament to the intense investor interest in industry-specific AI applications.&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Trained on medical journals like JAMA and the New England Journal of Medicine, the OpenEvidence platform helps users quickly get answers to existing medical knowledge to help treat patients. Verified medical professionals can access OpenEvidence’s tool, which is supported by advertising, for free. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;OpenEvidence has grown quickly since its founding in 2022. Its number of clinical consultations per month has nearly doubled to 15 million since July, per the Times. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The round was led by Google Ventures, with participation from Sequoia Capital, Kleiner Perkins, Blackstone, Thrive Capital, Coatue Management, Bond, and Craft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to OpenEvidence for comment.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/openevidence-the-chatgpt-for-doctors-raises-200m-at-6b-valuation/</guid><pubDate>Mon, 20 Oct 2025 14:46:58 +0000</pubDate></item><item><title>[NEW] How AI Is Unlocking Level 4 Autonomous Driving (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2018/01/automotive-key-visual-corp-blog-level4-av-og-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;span&gt;When the Society of Automotive Engineers established its &lt;/span&gt;&lt;span&gt;framework for vehicle autonomy&lt;/span&gt;&lt;span&gt; in 2014, it created the industry-standard roadmap for self-driving technology.&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The levels of automation progress from level 1 (driver assistance) to level 2 (partial automation), level 3 (conditional automation), level 4 (high automation) and level 5 (full automation).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Predicting when each level would arrive proved more challenging than defining them. This uncertainty created industry-wide anticipation, as breakthroughs seemed perpetually just around the corner.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;That dynamic has shifted dramatically in recent years, with more progress in autonomous driving in the past three to four years than in the previous decade combined. Below, learn about recent advancements that have made such rapid progress possible.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Is Level 4 Autonomous Driving?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;Level 4 autonomous driving enables vehicles to handle all driving tasks within specific operating zones — such as certain cities or routes — without the need for human intervention. This high automation level uses AI breakthroughs including foundation models, end-to-end architectures and reasoning models to navigate complex scenarios.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Today, level 4 “high automation” is bringing the vision of &lt;/span&gt;&lt;span&gt;autonomous driving&lt;/span&gt;&lt;span&gt; closer to a scalable, commercially viable reality.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Six AI Breakthroughs Advancing Autonomous Vehicles&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;Six major AI breakthroughs are converging to accelerate level 4 autonomy:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;1. Foundation Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Foundation models&lt;/span&gt;&lt;span&gt; can tap internet-scale knowledge, not just proprietary driving fleet data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;When humans learn to drive at, say, 18 years old, they’re bringing 18 years of world experience to the endeavor. Similarly, foundation models bring a breadth of knowledge — understanding unusual scenarios and predicting outcomes based on general world knowledge.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With foundation models, a vehicle encountering a mattress in the road or a ball rolling into the street can now reason its way through scenarios it has never seen before, drawing on information learned from vast training datasets.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;2. End-to-End Architectures&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Traditional autonomous driving systems used separate modules for perception, planning and control — losing information at each handoff.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;End-to-end autonomy&lt;/span&gt;&lt;span&gt; architectures have the potential to change that. With end-to-end architectures, a single network processes sensor inputs directly into driving decisions, maintaining context throughout. While the concept of end-to-end architectures is not new, architectural advancements and improved training methodologies are finally making this paradigm viable, resulting in better autonomous decision-making with less engineering complexity.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;3. Reasoning Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Reasoning vision language action (VLA)&lt;/span&gt;&lt;span&gt; models integrate diverse perceptual inputs, language understanding and action generation with step-by-step reasoning. This enables them to break down complex situations, evaluate multiple possible outcomes and decide on the best course of action — much like humans do.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Systems powered by reasoning models deliver far greater reliability and performance, with explainable, step-by-step decision-making. For autonomous vehicles, this means the ability to flag unusual decision patterns for real-time safety monitoring, as well as post-incident debugging to reveal why a vehicle took a particular action. This improves the performance of autonomous vehicles while building user trust. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;4. Simulation&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With physical testing alone, it would take decades to test a driving policy in every possible driving scenario, if ever achievable at all. Enter &lt;/span&gt;&lt;span&gt;simulation&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Technologies like &lt;/span&gt;&lt;span&gt;neural reconstruction&lt;/span&gt;&lt;span&gt; can be used to create interactive simulations from real-world sensor data, while world models like &lt;/span&gt;&lt;span&gt;NVIDIA Cosmos&lt;/span&gt;&lt;span&gt; Predict and Transfer produce unlimited novel situations for training and testing &lt;/span&gt;&lt;span&gt;autonomous vehicles&lt;/span&gt;&lt;span&gt;.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With these technologies, developers can use text prompts to generate new weather and road conditions, or change lighting and introduce obstacles to simulate new scenarios and test driving policies in novel conditions. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;5. Compute Power&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;None of these advances would be possible without sufficient computational power. The &lt;/span&gt;&lt;span&gt;NVIDIA DRIVE AGX&lt;/span&gt;&lt;span&gt; and &lt;/span&gt;&lt;span&gt;NVIDIA DGX&lt;/span&gt;&lt;span&gt; platforms have evolved through multiple generations, each designed for today’s AI workloads as well as those anticipated years down the road.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Co-optimization matters. Technology must be designed anticipating the computational demands of next-generation AI systems.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;6. AI Safety&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Safety is foundational for level 4 autonomy, where reliability is the defining characteristic distinguishing it from lower autonomy levels. Recent advances in physical AI safety enable the trustworthy deployment of AI-based autonomy stacks by introducing safety guardrails at the stages of design, deployment and validation.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;For example, NVIDIA’s safety architecture guardrails the end-to-end driving model with checks supported by a diverse modular stack, and validation is greatly accelerated by the latest advancements in neural reconstruction.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;These and other guardrails are part of &lt;/span&gt;&lt;span&gt;NVIDIA Halos&lt;/span&gt;&lt;span&gt;, a comprehensive safety system that unifies the NVIDIA DRIVE architecture, the safety-certified &lt;/span&gt;&lt;span&gt;NVIDIA DriveOS&lt;/span&gt;&lt;span&gt; operating system, and AI models, hardware, software, tools and services to help ensure the safe development and deployment of autonomous vehicles from cloud to car. NVIDIA partners can adopt individual components or the full stack, depending on their needs.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Why It Matters: Saving Lives and Resources&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;The stakes extend far beyond technological achievement. Improving vehicle safety can help save lives and conserve significant amounts of money and resources. Level 4 autonomy systematically removes human error, the cause of the vast majority of crashes.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;NVIDIA, as a full-stack autonomous vehicle company — from cloud to car — is enabling the broader automotive ecosystem to achieve level 4 autonomy, building on the foundation of its level 2+ stack already in production. In particular, NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Its three AI compute platforms critical for autonomy are:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;NVIDIA GTC Washington, D.C&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;., running Oct. 27-29, will feature a wide range of &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;sessions on autonomous vehicles and safety&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;, which will also be available on demand.&lt;/span&gt;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2018/01/automotive-key-visual-corp-blog-level4-av-og-1280x680-1.png" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;&lt;span&gt;When the Society of Automotive Engineers established its &lt;/span&gt;&lt;span&gt;framework for vehicle autonomy&lt;/span&gt;&lt;span&gt; in 2014, it created the industry-standard roadmap for self-driving technology.&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;&lt;br /&gt;&lt;/span&gt;&lt;span&gt;The levels of automation progress from level 1 (driver assistance) to level 2 (partial automation), level 3 (conditional automation), level 4 (high automation) and level 5 (full automation).&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Predicting when each level would arrive proved more challenging than defining them. This uncertainty created industry-wide anticipation, as breakthroughs seemed perpetually just around the corner.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;That dynamic has shifted dramatically in recent years, with more progress in autonomous driving in the past three to four years than in the previous decade combined. Below, learn about recent advancements that have made such rapid progress possible.&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;What Is Level 4 Autonomous Driving?&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;Level 4 autonomous driving enables vehicles to handle all driving tasks within specific operating zones — such as certain cities or routes — without the need for human intervention. This high automation level uses AI breakthroughs including foundation models, end-to-end architectures and reasoning models to navigate complex scenarios.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Today, level 4 “high automation” is bringing the vision of &lt;/span&gt;&lt;span&gt;autonomous driving&lt;/span&gt;&lt;span&gt; closer to a scalable, commercially viable reality.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Six AI Breakthroughs Advancing Autonomous Vehicles&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;Six major AI breakthroughs are converging to accelerate level 4 autonomy:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;1. Foundation Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Foundation models&lt;/span&gt;&lt;span&gt; can tap internet-scale knowledge, not just proprietary driving fleet data.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;When humans learn to drive at, say, 18 years old, they’re bringing 18 years of world experience to the endeavor. Similarly, foundation models bring a breadth of knowledge — understanding unusual scenarios and predicting outcomes based on general world knowledge.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With foundation models, a vehicle encountering a mattress in the road or a ball rolling into the street can now reason its way through scenarios it has never seen before, drawing on information learned from vast training datasets.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;2. End-to-End Architectures&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Traditional autonomous driving systems used separate modules for perception, planning and control — losing information at each handoff.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;End-to-end autonomy&lt;/span&gt;&lt;span&gt; architectures have the potential to change that. With end-to-end architectures, a single network processes sensor inputs directly into driving decisions, maintaining context throughout. While the concept of end-to-end architectures is not new, architectural advancements and improved training methodologies are finally making this paradigm viable, resulting in better autonomous decision-making with less engineering complexity.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;3. Reasoning Models&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Reasoning vision language action (VLA)&lt;/span&gt;&lt;span&gt; models integrate diverse perceptual inputs, language understanding and action generation with step-by-step reasoning. This enables them to break down complex situations, evaluate multiple possible outcomes and decide on the best course of action — much like humans do.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Systems powered by reasoning models deliver far greater reliability and performance, with explainable, step-by-step decision-making. For autonomous vehicles, this means the ability to flag unusual decision patterns for real-time safety monitoring, as well as post-incident debugging to reveal why a vehicle took a particular action. This improves the performance of autonomous vehicles while building user trust. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;4. Simulation&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With physical testing alone, it would take decades to test a driving policy in every possible driving scenario, if ever achievable at all. Enter &lt;/span&gt;&lt;span&gt;simulation&lt;/span&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Technologies like &lt;/span&gt;&lt;span&gt;neural reconstruction&lt;/span&gt;&lt;span&gt; can be used to create interactive simulations from real-world sensor data, while world models like &lt;/span&gt;&lt;span&gt;NVIDIA Cosmos&lt;/span&gt;&lt;span&gt; Predict and Transfer produce unlimited novel situations for training and testing &lt;/span&gt;&lt;span&gt;autonomous vehicles&lt;/span&gt;&lt;span&gt;.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;With these technologies, developers can use text prompts to generate new weather and road conditions, or change lighting and introduce obstacles to simulate new scenarios and test driving policies in novel conditions. &lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;5. Compute Power&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;None of these advances would be possible without sufficient computational power. The &lt;/span&gt;&lt;span&gt;NVIDIA DRIVE AGX&lt;/span&gt;&lt;span&gt; and &lt;/span&gt;&lt;span&gt;NVIDIA DGX&lt;/span&gt;&lt;span&gt; platforms have evolved through multiple generations, each designed for today’s AI workloads as well as those anticipated years down the road.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Co-optimization matters. Technology must be designed anticipating the computational demands of next-generation AI systems.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;6. AI Safety&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Safety is foundational for level 4 autonomy, where reliability is the defining characteristic distinguishing it from lower autonomy levels. Recent advances in physical AI safety enable the trustworthy deployment of AI-based autonomy stacks by introducing safety guardrails at the stages of design, deployment and validation.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;For example, NVIDIA’s safety architecture guardrails the end-to-end driving model with checks supported by a diverse modular stack, and validation is greatly accelerated by the latest advancements in neural reconstruction.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;These and other guardrails are part of &lt;/span&gt;&lt;span&gt;NVIDIA Halos&lt;/span&gt;&lt;span&gt;, a comprehensive safety system that unifies the NVIDIA DRIVE architecture, the safety-certified &lt;/span&gt;&lt;span&gt;NVIDIA DriveOS&lt;/span&gt;&lt;span&gt; operating system, and AI models, hardware, software, tools and services to help ensure the safe development and deployment of autonomous vehicles from cloud to car. NVIDIA partners can adopt individual components or the full stack, depending on their needs.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Why It Matters: Saving Lives and Resources&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;&lt;span&gt;The stakes extend far beyond technological achievement. Improving vehicle safety can help save lives and conserve significant amounts of money and resources. Level 4 autonomy systematically removes human error, the cause of the vast majority of crashes.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;NVIDIA, as a full-stack autonomous vehicle company — from cloud to car — is enabling the broader automotive ecosystem to achieve level 4 autonomy, building on the foundation of its level 2+ stack already in production. In particular, NVIDIA is the only company that offers an end-to-end compute stack for autonomous driving.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span&gt;Its three AI compute platforms critical for autonomy are:&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span&gt;Together, these platforms form a feedback loop for learning, testing and deployment that tightens the cycle of innovation while keeping safety front and center.&amp;nbsp;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;i&gt;&lt;span&gt;NVIDIA GTC Washington, D.C&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;., running Oct. 27-29, will feature a wide range of &lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;sessions on autonomous vehicles and safety&lt;/span&gt;&lt;/i&gt;&lt;i&gt;&lt;span&gt;, which will also be available on demand.&lt;/span&gt;&lt;/i&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/level-4-autonomous-driving-ai/</guid><pubDate>Mon, 20 Oct 2025 15:00:51 +0000</pubDate></item><item><title>[NEW] Fold your own tessellation (MIT Technology Review)</title><link>https://www.technologyreview.com/2025/10/20/1125594/technologyreview-com-tessellation/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;strong&gt;Download the pattern&lt;/strong&gt; for Dancing Ribbons here.&lt;/p&gt;  &lt;p&gt;Yoder recommends printing the pattern on paper in between normal printer paper and cardstock in weight, making sure it folds in straight lines (not too thick), folds back and forth easily on the same line (not too thin), and is crisp enough to make a satisfying snapping noise when you shake it. Her favorite paper isSkytone, which is commonly used to print certificates and fancy envelopes.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Watch the &lt;strong&gt;video tutorial&lt;/strong&gt; on folding Dancing Ribbons here.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Yoder’s detailed folding instructions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Once you have your crease pattern on a sheet of paper, cut out the hexagon that contains the pattern. Yoder recommends using a straightedge and blade on a cutting mat instead of scissors, whether that means an X-Acto knife and a ruler on a sheet of cardboard or a quilting ruler and rotary cutter on a fabric cutting mat.&lt;/p&gt;  &lt;p&gt;The next step is folding the background grid of black lines that the pattern uses as references. Assuming you’ve cut out your hexagon precisely, you can use the edge of the hexagon and the printed lines to make your creases, or you can fold as if there were no lines printed by folding the hexagon in half (edge to opposite edge) and then folding those edges in to the center to make quarter lines, first in one direction and then in the other two. After each set of folds, it’s a good idea to fold the new lines back the other way to make the paper easier to work with later. After folding the quarters, fold the eighths in each direction, and finally the 16ths. Yoder presses the creases with a bone folder to make them easier to work with and to minimize stress on her hands.&lt;/p&gt; 
 &lt;p&gt;You can choose at this point whether to fold the pattern one twist at a time or to precrease the off-grid creases (just crease the short segments that have been printed, folded as mountains on the printed side of the pattern) and collapse everything all at once. Beginning folders may find it helpful to precrease the triangle and rhombus twists, to make the squashing process easier, even if you plan to fold the pattern one twist at a time. Solid red lines in the crease pattern represent mountain folds, and dashed blue lines represent valley folds. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.&lt;/p&gt;  &lt;p&gt;The central closed hexagon twist will be the first twist folded, and it’ll be made on the blank side of the paper. All the mountain folds for this twist (as viewed on the blank side of the paper) will be on grid lines going to the corners of the hexagon, and the valley folds will be one grid spacing above the mountains on the right-hand side of the paper. To fold the twist, set up both the mountain and valley folds of one pleat; then pass that pleat counterclockwise into your other hand before setting up both folds of the next pleat. Keep all pleats folded and the center of the paper elevated as you work your way around the center, eventually folding all six pleats (use your table to keep the pleats folded, or use clips at the edge of the paper) and forming a hexagonal tower in the center of the paper. Make the pleats more flat, working from the edges in, until this hexagon tower is two grid spacings high. Then grab the tower and give it a sharp counterclockwise twist to get it to lie flat. This twist almost never lies down completely flat right away, so lift each pleat slightly to make sure the valley folds have stayed on grid lines to help the central hexagon to smooth out.&lt;/p&gt;  &lt;p&gt;Once the hexagon has been folded, flip the paper over to the printed side. Take the mountain fold of one pleat and split it into a three-way intersection of mountain folds evenly spaced around a point two grid spacings out from the closed hexagon hole. This point is the center of the closed triangle twist, which can be squashed to create the triangle of off-grid creases once the two new pleats are folded over in a clockwise direction (as printed). To squash the triangle twist, press gently on each of the three pleats just outside the point where the valley fold of one pleat contacts the mountain fold of the next pleat. This will start to flatten the central triangle, which can then be pressed from the top to smooth it out and finalize the new creases.&lt;/p&gt;  &lt;p&gt;Fold each of the triangle twists in the same way (causing pleats to overlap with pleats from other triangles), in a counterclockwise order around the central hexagon (this order makes the overlapping pleats easier to work with later).&lt;/p&gt;  &lt;p&gt;Once the triangles have all been folded, find a place where two pleats from triangle twists are overlapping and open up the overlap so you can see all the parts of the paper (leaving the triangle twists folded). Use the printed folds to set up a rhombus twist, and then press the twist flat from the top once all the folds in the pleats are set up.&lt;/p&gt;  &lt;p&gt;Repeat this step with all six of the pleat overlaps (if you followed the recommended sequence for the triangles, only one overlap will be in a different order from the rest) to complete the pattern.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0"&gt; &lt;p&gt;&lt;strong&gt;Download the pattern&lt;/strong&gt; for Dancing Ribbons here.&lt;/p&gt;  &lt;p&gt;Yoder recommends printing the pattern on paper in between normal printer paper and cardstock in weight, making sure it folds in straight lines (not too thick), folds back and forth easily on the same line (not too thin), and is crisp enough to make a satisfying snapping noise when you shake it. Her favorite paper isSkytone, which is commonly used to print certificates and fancy envelopes.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;Watch the &lt;strong&gt;video tutorial&lt;/strong&gt; on folding Dancing Ribbons here.&lt;strong&gt;&lt;/strong&gt;&lt;/p&gt;  &lt;p&gt;&lt;strong&gt;Yoder’s detailed folding instructions:&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;Once you have your crease pattern on a sheet of paper, cut out the hexagon that contains the pattern. Yoder recommends using a straightedge and blade on a cutting mat instead of scissors, whether that means an X-Acto knife and a ruler on a sheet of cardboard or a quilting ruler and rotary cutter on a fabric cutting mat.&lt;/p&gt;  &lt;p&gt;The next step is folding the background grid of black lines that the pattern uses as references. Assuming you’ve cut out your hexagon precisely, you can use the edge of the hexagon and the printed lines to make your creases, or you can fold as if there were no lines printed by folding the hexagon in half (edge to opposite edge) and then folding those edges in to the center to make quarter lines, first in one direction and then in the other two. After each set of folds, it’s a good idea to fold the new lines back the other way to make the paper easier to work with later. After folding the quarters, fold the eighths in each direction, and finally the 16ths. Yoder presses the creases with a bone folder to make them easier to work with and to minimize stress on her hands.&lt;/p&gt; 
 &lt;p&gt;You can choose at this point whether to fold the pattern one twist at a time or to precrease the off-grid creases (just crease the short segments that have been printed, folded as mountains on the printed side of the pattern) and collapse everything all at once. Beginning folders may find it helpful to precrease the triangle and rhombus twists, to make the squashing process easier, even if you plan to fold the pattern one twist at a time. Solid red lines in the crease pattern represent mountain folds, and dashed blue lines represent valley folds. The faded lines inside the twists are helper folds used to set up the twists; they will not be used in the final pattern.&lt;/p&gt;  &lt;p&gt;The central closed hexagon twist will be the first twist folded, and it’ll be made on the blank side of the paper. All the mountain folds for this twist (as viewed on the blank side of the paper) will be on grid lines going to the corners of the hexagon, and the valley folds will be one grid spacing above the mountains on the right-hand side of the paper. To fold the twist, set up both the mountain and valley folds of one pleat; then pass that pleat counterclockwise into your other hand before setting up both folds of the next pleat. Keep all pleats folded and the center of the paper elevated as you work your way around the center, eventually folding all six pleats (use your table to keep the pleats folded, or use clips at the edge of the paper) and forming a hexagonal tower in the center of the paper. Make the pleats more flat, working from the edges in, until this hexagon tower is two grid spacings high. Then grab the tower and give it a sharp counterclockwise twist to get it to lie flat. This twist almost never lies down completely flat right away, so lift each pleat slightly to make sure the valley folds have stayed on grid lines to help the central hexagon to smooth out.&lt;/p&gt;  &lt;p&gt;Once the hexagon has been folded, flip the paper over to the printed side. Take the mountain fold of one pleat and split it into a three-way intersection of mountain folds evenly spaced around a point two grid spacings out from the closed hexagon hole. This point is the center of the closed triangle twist, which can be squashed to create the triangle of off-grid creases once the two new pleats are folded over in a clockwise direction (as printed). To squash the triangle twist, press gently on each of the three pleats just outside the point where the valley fold of one pleat contacts the mountain fold of the next pleat. This will start to flatten the central triangle, which can then be pressed from the top to smooth it out and finalize the new creases.&lt;/p&gt;  &lt;p&gt;Fold each of the triangle twists in the same way (causing pleats to overlap with pleats from other triangles), in a counterclockwise order around the central hexagon (this order makes the overlapping pleats easier to work with later).&lt;/p&gt;  &lt;p&gt;Once the triangles have all been folded, find a place where two pleats from triangle twists are overlapping and open up the overlap so you can see all the parts of the paper (leaving the triangle twists folded). Use the printed folds to set up a rhombus twist, and then press the twist flat from the top once all the folds in the pleats are set up.&lt;/p&gt;  &lt;p&gt;Repeat this step with all six of the pleat overlaps (if you followed the recommended sequence for the triangles, only one overlap will be in a different order from the rest) to complete the pattern.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2025/10/20/1125594/technologyreview-com-tessellation/</guid><pubDate>Mon, 20 Oct 2025 15:46:58 +0000</pubDate></item><item><title>[NEW] NVIDIA and Google Cloud Accelerate Enterprise AI and Industrial Digitalization (NVIDIA Blog)</title><link>https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/partner-promo-google-cloudg4-blog-1920x1080-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA and Google Cloud are expanding access to accelerated computing to transform the full spectrum of enterprise workloads, from visual computing to agentic and physical AI.&lt;/p&gt;
&lt;p&gt;Google Cloud today announced the general availability of G4 VMs, powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Plus, NVIDIA Omniverse and NVIDIA Isaac Sim are now available as virtual machine images (VMIs) on the Google Cloud Marketplace to unlock physical AI-driven applications for key industries like manufacturing, automotive and logistics.&lt;/p&gt;
&lt;p&gt;This powerful combination creates a versatile, multi-workload platform for enterprises to accelerate their most demanding challenges on Google Cloud.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX PRO 6000 Blackwell GPUs excel at high-performance AI inference for multimodal, generative and agentic AI deployments, while also powering complex visual and simulation workloads ranging from computer-aided engineering and content creation to robotics simulation.&lt;/p&gt;
&lt;p&gt;Customers like WPP are using G4 VMs with NVIDIA Omniverse to instantly generate photorealistic 3D advertising environments at global scale, while Altair is using the platform within Altair One to accelerate demanding simulation and fluid dynamics workloads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The G4 VM: A Universal Platform for AI and Visual Computing&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At the core of the new G4 VM is the NVIDIA RTX PRO 6000 Blackwell Server Edition GPU, the ultimate data center GPU for AI and visual computing.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Blackwell architecture, it serves as a universal platform for a broad range of workloads. Its design uniquely combines two powerful engines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Fifth-Generation Tensor Cores&lt;/b&gt; that deliver a massive leap in AI performance, supporting new data formats like FP4 to enable faster performance with lower memory usage.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fourth-Generation RT Cores&lt;/b&gt; that provide over 2x the real-time ray-tracing performance over the previous generation, enabling cinematic-quality graphics and photorealistic simulations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Google Cloud, these G4 VMs are built for massive scale, configurable with up to eight RTX PRO 6000 GPUs — totaling 768 GB of GDDR7 memory — and supported by high-throughput local and network storage.&lt;/p&gt;
&lt;p&gt;As part of Google Cloud’s AI Hypercomputer architecture, G4 VMs natively integrate with services like Google Kubernetes Engine and Vertex AI to simplify containerized deployments and streamline machine learning operations for AI workloads. This flexibility extends to accelerating large-scale data analytics on Apache Spark and Hadoop with Dataproc.&lt;/p&gt;
&lt;p&gt;For design and simulation, the VMs also support a broad ecosystem of popular third-party engineering and graphics applications like Autodesk AutoCAD, Blender and Dassault SolidWorks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Industrial Digitalization at Scale With NVIDIA Omniverse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Google Cloud customers can now tap into NVIDIA Omniverse, a collection of integration-ready libraries and frameworks for developing industrial digitalization applications built on Universal Scene Description (OpenUSD).&lt;/p&gt;
&lt;p&gt;With the availability of the Omniverse VMI and G4 VMs on Google Cloud, enterprises can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Build and Operate Digital Twins&lt;/b&gt;: Easily create physically accurate, real-time virtual replicas of factories and products to simulate and optimize operations. These workflows are powered by the NVIDIA Cosmos world foundation model platform and NVIDIA Omniverse Blueprints for creating digital twins.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Accelerate Robotics Development&lt;/b&gt;: Use NVIDIA Isaac Sim, a reference application built on Omniverse, to train, simulate and validate AI-driven robots in physics-based virtual environments before deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA AI Accelerates Every Enterprise Workload&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In addition to Omniverse, Google Cloud customers can use the full NVIDIA software stack to accelerate a range of high-demand workloads, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Agentic AI&lt;/b&gt;: Developers can use the NVIDIA Nemotron family of open reasoning models and NVIDIA Blueprints to get started quickly with building sophisticated AI agents that can reason and act. For deployment, NVIDIA NIM — a set of easy-to-use microservices — provides optimized, high-performance inference for AI models with enterprise-grade security and support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Scientific and High-Performance Computing&lt;/b&gt;: Solve complex problems in fields like drug discovery and genomics using NVIDIA CUDA-X libraries and microservices. On the RTX PRO 6000 Blackwell GPU, core genomics algorithms used in sequence alignment can see up to 6.8x faster throughput compared with the previous generation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Design and Visual Computing&lt;/b&gt;: Power remote creative and design pipelines with NVIDIA RTX Virtual Workstation software, which delivers high-performance virtual workstation instances from G4 VMs to any device, anywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These latest announcements establish a complete, end-to-end platform built on the NVIDIA Blackwell platform — from NVIDIA GB200 NVL72 (A4X VMs) and NVIDIA HGX B200 (A4 VMs) for massive-scale AI training and inference, to the RTX PRO 6000 Blackwell for AI inference and visual computing on G4 VMs.&lt;/p&gt;
&lt;p&gt;This unified architecture provides a seamless experience for accelerating every workload, empowering enterprises to tackle complex, multistage pipelines from data analytics to physical AI within a single, consistent cloud ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about G4 VMs on Google Cloud, and deploy the NVIDIA Omniverse and Isaac Sim VMIs from the Google Cloud Marketplace. Explore NVIDIA Nemotron models and NVIDIA Blueprints&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://blogs.nvidia.com/wp-content/uploads/2025/10/partner-promo-google-cloudg4-blog-1920x1080-1.jpg" /&gt;&lt;/div&gt;&lt;span class="bsf-rt-reading-time"&gt;&lt;span class="bsf-rt-display-label"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-time"&gt;&lt;/span&gt; &lt;span class="bsf-rt-display-postfix"&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;NVIDIA and Google Cloud are expanding access to accelerated computing to transform the full spectrum of enterprise workloads, from visual computing to agentic and physical AI.&lt;/p&gt;
&lt;p&gt;Google Cloud today announced the general availability of G4 VMs, powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs. Plus, NVIDIA Omniverse and NVIDIA Isaac Sim are now available as virtual machine images (VMIs) on the Google Cloud Marketplace to unlock physical AI-driven applications for key industries like manufacturing, automotive and logistics.&lt;/p&gt;
&lt;p&gt;This powerful combination creates a versatile, multi-workload platform for enterprises to accelerate their most demanding challenges on Google Cloud.&lt;/p&gt;
&lt;p&gt;NVIDIA RTX PRO 6000 Blackwell GPUs excel at high-performance AI inference for multimodal, generative and agentic AI deployments, while also powering complex visual and simulation workloads ranging from computer-aided engineering and content creation to robotics simulation.&lt;/p&gt;
&lt;p&gt;Customers like WPP are using G4 VMs with NVIDIA Omniverse to instantly generate photorealistic 3D advertising environments at global scale, while Altair is using the platform within Altair One to accelerate demanding simulation and fluid dynamics workloads.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;The G4 VM: A Universal Platform for AI and Visual Computing&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;At the core of the new G4 VM is the NVIDIA RTX PRO 6000 Blackwell Server Edition GPU, the ultimate data center GPU for AI and visual computing.&lt;/p&gt;
&lt;p&gt;Built on the NVIDIA Blackwell architecture, it serves as a universal platform for a broad range of workloads. Its design uniquely combines two powerful engines:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Fifth-Generation Tensor Cores&lt;/b&gt; that deliver a massive leap in AI performance, supporting new data formats like FP4 to enable faster performance with lower memory usage.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Fourth-Generation RT Cores&lt;/b&gt; that provide over 2x the real-time ray-tracing performance over the previous generation, enabling cinematic-quality graphics and photorealistic simulations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On Google Cloud, these G4 VMs are built for massive scale, configurable with up to eight RTX PRO 6000 GPUs — totaling 768 GB of GDDR7 memory — and supported by high-throughput local and network storage.&lt;/p&gt;
&lt;p&gt;As part of Google Cloud’s AI Hypercomputer architecture, G4 VMs natively integrate with services like Google Kubernetes Engine and Vertex AI to simplify containerized deployments and streamline machine learning operations for AI workloads. This flexibility extends to accelerating large-scale data analytics on Apache Spark and Hadoop with Dataproc.&lt;/p&gt;
&lt;p&gt;For design and simulation, the VMs also support a broad ecosystem of popular third-party engineering and graphics applications like Autodesk AutoCAD, Blender and Dassault SolidWorks.&lt;/p&gt;
&lt;h2&gt;&lt;b&gt;Industrial Digitalization at Scale With NVIDIA Omniverse&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;Google Cloud customers can now tap into NVIDIA Omniverse, a collection of integration-ready libraries and frameworks for developing industrial digitalization applications built on Universal Scene Description (OpenUSD).&lt;/p&gt;
&lt;p&gt;With the availability of the Omniverse VMI and G4 VMs on Google Cloud, enterprises can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Build and Operate Digital Twins&lt;/b&gt;: Easily create physically accurate, real-time virtual replicas of factories and products to simulate and optimize operations. These workflows are powered by the NVIDIA Cosmos world foundation model platform and NVIDIA Omniverse Blueprints for creating digital twins.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Accelerate Robotics Development&lt;/b&gt;: Use NVIDIA Isaac Sim, a reference application built on Omniverse, to train, simulate and validate AI-driven robots in physics-based virtual environments before deployment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;b&gt;NVIDIA AI Accelerates Every Enterprise Workload&lt;/b&gt;&lt;/h2&gt;
&lt;p&gt;In addition to Omniverse, Google Cloud customers can use the full NVIDIA software stack to accelerate a range of high-demand workloads, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Agentic AI&lt;/b&gt;: Developers can use the NVIDIA Nemotron family of open reasoning models and NVIDIA Blueprints to get started quickly with building sophisticated AI agents that can reason and act. For deployment, NVIDIA NIM — a set of easy-to-use microservices — provides optimized, high-performance inference for AI models with enterprise-grade security and support.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Scientific and High-Performance Computing&lt;/b&gt;: Solve complex problems in fields like drug discovery and genomics using NVIDIA CUDA-X libraries and microservices. On the RTX PRO 6000 Blackwell GPU, core genomics algorithms used in sequence alignment can see up to 6.8x faster throughput compared with the previous generation.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Design and Visual Computing&lt;/b&gt;: Power remote creative and design pipelines with NVIDIA RTX Virtual Workstation software, which delivers high-performance virtual workstation instances from G4 VMs to any device, anywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These latest announcements establish a complete, end-to-end platform built on the NVIDIA Blackwell platform — from NVIDIA GB200 NVL72 (A4X VMs) and NVIDIA HGX B200 (A4 VMs) for massive-scale AI training and inference, to the RTX PRO 6000 Blackwell for AI inference and visual computing on G4 VMs.&lt;/p&gt;
&lt;p&gt;This unified architecture provides a seamless experience for accelerating every workload, empowering enterprises to tackle complex, multistage pipelines from data analytics to physical AI within a single, consistent cloud ecosystem.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Learn more about G4 VMs on Google Cloud, and deploy the NVIDIA Omniverse and Isaac Sim VMIs from the Google Cloud Marketplace. Explore NVIDIA Nemotron models and NVIDIA Blueprints&lt;/em&gt;&lt;/p&gt;

		&lt;footer class="entry-footer  " id="post-footer"&gt;
					&lt;/footer&gt;</content:encoded><guid isPermaLink="false">https://blogs.nvidia.com/blog/nvidia-google-cloud-enterprise-ai-industrial-digitalization/</guid><pubDate>Mon, 20 Oct 2025 16:00:53 +0000</pubDate></item><item><title>[NEW] FTC removes Lina Khan-era posts about AI risks and open source (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/ftc-removes-lina-khan-era-posts-about-ai-risks-and-open-source/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/06/GettyImages-1232440387.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Federal Trade Commission has removed three blog posts from the Lina Khan-era that addressed open-source AI and risks of AI to consumers, according to a Wired report. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One post, titled “On Open-Weights Foundation Models,” was published July 10, 2024. Another, titled “Consumers Are Voicing Concerns About AI,” came out in October 2023. A third, authored by Khan’s staff, was published on January 3, 2025 with the title “AI and the Risk of Consumer Harm.” That post noted the FTC was “taking note of AI’s potential for real-world instances of harm – from incentivizing commercial surveillance to enabling fraud and impersonation to perpetuating illegal discrimination.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to the FTC to learn why the posts were taken down. Khan declined to comment. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These removals are part of a broader pattern under the Trump administration, which began issuing executive orders to direct federal agencies to remove or modify substantial amounts of government content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After his inauguration, Trump also installed a new head of the FTC and removed several FTC commissioners, installing leadership that focused less on Khan’s aggressive antitrust agenda and more on deregulation for Big Tech.&amp;nbsp;In September, new FTC Chair Andrew Ferguson submitted recommendations for deleting or revising anticompetitive regulations across the entire federal government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The blog posts most recently removed by the FTC, which focused on consumer harm, don’t seem to align with the Trump administration’s AI Action Plan. That plan has reduced its focus on safety and guardrails, instead favoring fast growth and competition with China. However, the Trump administration has been vocal about backing open-source initiatives. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Former FTC public affairs director Douglas Farrar told TechCrunch: “I was shocked to see that Andrew Ferguson led FTC be so out of line with the Trump White House on this signal to the market.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This is not the first time this administration’s FTC has removed content. In March, Wired reported that the FTC removed around 300 posts related to AI, consumer protection, and the agency’s lawsuits against tech companies like Amazon and Microsoft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While hundreds of blog posts from Khan’s tenure and earlier remain on the agency’s Office of Technology Blog, Ferguson’s FTC has yet to publish any posts to the site, despite the feverish pace of the AI race, which has resulted in several business mergers and acquisitions — including acqui-hires — that could be seen as anticompetitive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The FTC blog culling follows the Trump administration’s removal or modification of thousands of government web pages and datasets, particularly content related to diversity, equity, and inclusion; gender identity; public health; and environmental policy. For example, the Centers for Disease Control and Prevention has removed data on topics ranging from chronic medical conditions to HIV/AIDS. The Justice Department has removed studies on hate crimes, and the National Oceanic and Atmospheric Administration has taken down the congressionally mandated National Climate Assessment reports. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The removal of content – including the blog posts from the FTC – could violate the Federal Records Act, which requires federal agencies to preserve records that properly document government activities, and the Open Government Data Act, which requires agencies to publish their data as “open data” by default.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Biden administration’s FTC leadership placed warning labels on content published during previous administrations that it disagreed with, according to Wired.&amp;nbsp;&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2021/06/GettyImages-1232440387.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;The Federal Trade Commission has removed three blog posts from the Lina Khan-era that addressed open-source AI and risks of AI to consumers, according to a Wired report. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;One post, titled “On Open-Weights Foundation Models,” was published July 10, 2024. Another, titled “Consumers Are Voicing Concerns About AI,” came out in October 2023. A third, authored by Khan’s staff, was published on January 3, 2025 with the title “AI and the Risk of Consumer Harm.” That post noted the FTC was “taking note of AI’s potential for real-world instances of harm – from incentivizing commercial surveillance to enabling fraud and impersonation to perpetuating illegal discrimination.”&amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;TechCrunch has reached out to the FTC to learn why the posts were taken down. Khan declined to comment. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;These removals are part of a broader pattern under the Trump administration, which began issuing executive orders to direct federal agencies to remove or modify substantial amounts of government content.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;After his inauguration, Trump also installed a new head of the FTC and removed several FTC commissioners, installing leadership that focused less on Khan’s aggressive antitrust agenda and more on deregulation for Big Tech.&amp;nbsp;In September, new FTC Chair Andrew Ferguson submitted recommendations for deleting or revising anticompetitive regulations across the entire federal government.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The blog posts most recently removed by the FTC, which focused on consumer harm, don’t seem to align with the Trump administration’s AI Action Plan. That plan has reduced its focus on safety and guardrails, instead favoring fast growth and competition with China. However, the Trump administration has been vocal about backing open-source initiatives. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Former FTC public affairs director Douglas Farrar told TechCrunch: “I was shocked to see that Andrew Ferguson led FTC be so out of line with the Trump White House on this signal to the market.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;This is not the first time this administration’s FTC has removed content. In March, Wired reported that the FTC removed around 300 posts related to AI, consumer protection, and the agency’s lawsuits against tech companies like Amazon and Microsoft. &amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;While hundreds of blog posts from Khan’s tenure and earlier remain on the agency’s Office of Technology Blog, Ferguson’s FTC has yet to publish any posts to the site, despite the feverish pace of the AI race, which has resulted in several business mergers and acquisitions — including acqui-hires — that could be seen as anticompetitive.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The FTC blog culling follows the Trump administration’s removal or modification of thousands of government web pages and datasets, particularly content related to diversity, equity, and inclusion; gender identity; public health; and environmental policy. For example, the Centers for Disease Control and Prevention has removed data on topics ranging from chronic medical conditions to HIV/AIDS. The Justice Department has removed studies on hate crimes, and the National Oceanic and Atmospheric Administration has taken down the congressionally mandated National Climate Assessment reports. &amp;nbsp;&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The removal of content – including the blog posts from the FTC – could violate the Federal Records Act, which requires federal agencies to preserve records that properly document government activities, and the Open Government Data Act, which requires agencies to publish their data as “open data” by default.&amp;nbsp;&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Biden administration’s FTC leadership placed warning labels on content published during previous administrations that it disagreed with, according to Wired.&amp;nbsp;&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/ftc-removes-lina-khan-era-posts-about-ai-risks-and-open-source/</guid><pubDate>Mon, 20 Oct 2025 16:49:58 +0000</pubDate></item><item><title>[NEW] Anthropic brings Claude Code to the web (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/anthropic-brings-claude-code-to-the-web/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/YouTube-Thumb-Text-2-3.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic launched a web app on Monday for its viral AI coding assistant, Claude Code, which lets developers create and manage several AI coding agents from their browser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claude Code for web is now rolling out to subscribers to Anthropic’s $20-per-month Pro plan, as well as its $100 and $200-per-month Max plans. Pro and Max users can access Claude Code on the web by navigating to claude.ai (the same website for Anthropic’s consumer chatbot) and clicking into the “Code” tab.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks Anthropic’s latest attempt to evolve Claude Code beyond a command-line interface (CLI) tool that developers access from a terminal. By putting Claude Code on the web, Anthropic hopes developers will spin up AI coding agents in more places.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s increasingly competitive for tech companies trying to make their AI coding tools stand out. While Microsoft’s GitHub Copilot once dominated the space, Cursor, Google, OpenAI, and Anthropic now have highly performant AI coding tools of their own — many of them already available on the web. That said, Claude Code is arguably one of the most popular. Anthropic’s flagship coding tool has grown 10x in users since its broader launch in May, and the product now accounts for more than $500 million of the company’s revenue on an annualized basis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic Product Manager Cat Wu tells TechCrunch in an interview that she attributes a large part of Claude Code’s success to the company’s AI models, which have become a favorite among developers in recent years. However, Wu also says the Claude Code team deliberately tries to “sprinkle in some fun” to the product wherever they can. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wu said that Anthropic will continue to put Claude Code in more places, but the terminal will likely remain the home base for their AI coding product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we look forward, one of our key focuses is making sure the CLI product is the most intelligent and customizable way for you to use coding agents,” said Wu. “But we’re continuing to put Claude Code everywhere, helping it meet developers wherever they are. Web and mobile are a big step in this direction.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic claims that 90% of the Claude Code product itself is written by the company’s AI models. Wu, who was previously an engineer, says that she rarely ever sits down at a keyboard to write code anymore, and mostly just reviews Claude Code’s outputs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early AI coding tools worked like an autocomplete tool, finishing lines of code as developers wrote them. But the agentic generation of AI coding tools — including Claude Code — allow developers to spin up agents that work autonomously. This shift has made millions of software engineers act more like managers of AI coding assistants in their day-to-day jobs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change has not been welcome to every developer. One recent study found that some engineers were actually slower when using AI coding tools like Cursor. Researchers suggested one factor could be that engineers in the study spent much of their time prompting and waiting for AI tools to finish, rather than working on other problems. AI coding tools also struggle in large, complex code bases, so engineers may have spent a lot of time working through incorrect responses from the AI model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nevertheless, companies like Anthropic are continuing to push ahead making AI coding agents. Anthropic CEO Dario Amodei predicted a few months ago that AI should soon write 90% of code for software engineers. While that may be true inside of Anthropic, the shift may taken longer to pan out in the broader economy.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/06/YouTube-Thumb-Text-2-3.png?resize=1200,675" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Anthropic launched a web app on Monday for its viral AI coding assistant, Claude Code, which lets developers create and manage several AI coding agents from their browser.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Claude Code for web is now rolling out to subscribers to Anthropic’s $20-per-month Pro plan, as well as its $100 and $200-per-month Max plans. Pro and Max users can access Claude Code on the web by navigating to claude.ai (the same website for Anthropic’s consumer chatbot) and clicking into the “Code” tab.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The launch marks Anthropic’s latest attempt to evolve Claude Code beyond a command-line interface (CLI) tool that developers access from a terminal. By putting Claude Code on the web, Anthropic hopes developers will spin up AI coding agents in more places.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;It’s increasingly competitive for tech companies trying to make their AI coding tools stand out. While Microsoft’s GitHub Copilot once dominated the space, Cursor, Google, OpenAI, and Anthropic now have highly performant AI coding tools of their own — many of them already available on the web. That said, Claude Code is arguably one of the most popular. Anthropic’s flagship coding tool has grown 10x in users since its broader launch in May, and the product now accounts for more than $500 million of the company’s revenue on an annualized basis.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic Product Manager Cat Wu tells TechCrunch in an interview that she attributes a large part of Claude Code’s success to the company’s AI models, which have become a favorite among developers in recent years. However, Wu also says the Claude Code team deliberately tries to “sprinkle in some fun” to the product wherever they can. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Wu said that Anthropic will continue to put Claude Code in more places, but the terminal will likely remain the home base for their AI coding product.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“As we look forward, one of our key focuses is making sure the CLI product is the most intelligent and customizable way for you to use coding agents,” said Wu. “But we’re continuing to put Claude Code everywhere, helping it meet developers wherever they are. Web and mobile are a big step in this direction.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Anthropic claims that 90% of the Claude Code product itself is written by the company’s AI models. Wu, who was previously an engineer, says that she rarely ever sits down at a keyboard to write code anymore, and mostly just reviews Claude Code’s outputs. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Early AI coding tools worked like an autocomplete tool, finishing lines of code as developers wrote them. But the agentic generation of AI coding tools — including Claude Code — allow developers to spin up agents that work autonomously. This shift has made millions of software engineers act more like managers of AI coding assistants in their day-to-day jobs.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The change has not been welcome to every developer. One recent study found that some engineers were actually slower when using AI coding tools like Cursor. Researchers suggested one factor could be that engineers in the study spent much of their time prompting and waiting for AI tools to finish, rather than working on other problems. AI coding tools also struggle in large, complex code bases, so engineers may have spent a lot of time working through incorrect responses from the AI model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Nevertheless, companies like Anthropic are continuing to push ahead making AI coding agents. Anthropic CEO Dario Amodei predicted a few months ago that AI should soon write 90% of code for software engineers. While that may be true inside of Anthropic, the shift may taken longer to pan out in the broader economy.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/anthropic-brings-claude-code-to-the-web/</guid><pubDate>Mon, 20 Oct 2025 18:00:00 +0000</pubDate></item><item><title>[NEW] Meta AI’s app downloads and daily users spiked after launch of ‘Vibes’ AI video feed (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2025/10/20/meta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;New data indicates that use of Meta AI’s mobile app for iOS and Android has seen a significant increase. According to a new analysis from market intelligence provider Similarweb, the app’s daily active users across both platforms jumped to 2.7 million as of October 17, up from around 775,000 just four weeks ago. In addition, Meta AI’s app installs are also up, reaching 300,000 new downloads per day, compared with under 200,000 daily downloads a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For comparison, Meta AI’s app had just 4,000 daily downloads a year ago, on October 17, 2024. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059538" height="384" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.29PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The firm says it haven’t seen any meaningful correlation in either search or advertising estimates, but notes Meta could be running Facebook or Instagram promotions that wouldn’t be captured in its model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, there’s also another possible explanation for the sharp rise: the launch of Meta’s new Vibes feed in September, which introduced short-form AI-generated videos to the Meta AI mobile app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta AI introduced the Vibes feed on September 25, which correlates with the sharp increase in the app’s daily active users on iOS and Android, as seen in the chart below. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059534" height="518" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.45PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Recently, OpenAI’s video generator Sora drew headlines as its app reached the top of the App Store when users rushed to try the new technology. However, Meta AI could have beneffited from this launch, as well. While Similarweb says its data doesn’t prove cause and effect, it’s possilbe that the attention to Sora drove some people to try Meta AI, in order to compare the two experiences. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another possibilty is that Meta could be benefitting from Sora’s invite-only status. That is, those who couldn’t try out the OpenAI app may have looked for an alternative to experiment with. This would be an interesting explanation, too, as it suggests that OpenAI’s decision to gatekeep Sora may have directly boosted its rivals. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Meta AI's Vibes feed, which showcases AI videos, may have driven a spike in app downloads and usage, " class="wp-image-3059536" height="537" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.36PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As of October 17, Meta AI’s app had seen a 15.58% increase in daily active users worldwide, while ChatGPT, Grok, and Perplexity saw declines of 3.51%, 7.35%, and 2.29%, respectively. &lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;New data indicates that use of Meta AI’s mobile app for iOS and Android has seen a significant increase. According to a new analysis from market intelligence provider Similarweb, the app’s daily active users across both platforms jumped to 2.7 million as of October 17, up from around 775,000 just four weeks ago. In addition, Meta AI’s app installs are also up, reaching 300,000 new downloads per day, compared with under 200,000 daily downloads a few weeks ago.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;For comparison, Meta AI’s app had just 4,000 daily downloads a year ago, on October 17, 2024. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059538" height="384" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.29PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;The firm says it haven’t seen any meaningful correlation in either search or advertising estimates, but notes Meta could be running Facebook or Instagram promotions that wouldn’t be captured in its model.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;However, there’s also another possible explanation for the sharp rise: the launch of Meta’s new Vibes feed in September, which introduced short-form AI-generated videos to the Meta AI mobile app. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meta AI introduced the Vibes feed on September 25, which correlates with the sharp increase in the app’s daily active users on iOS and Android, as seen in the chart below. &lt;/p&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="alt" class="wp-image-3059534" height="518" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.45PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;Recently, OpenAI’s video generator Sora drew headlines as its app reached the top of the App Store when users rushed to try the new technology. However, Meta AI could have beneffited from this launch, as well. While Similarweb says its data doesn’t prove cause and effect, it’s possilbe that the attention to Sora drove some people to try Meta AI, in order to compare the two experiences. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another possibilty is that Meta could be benefitting from Sora’s invite-only status. That is, those who couldn’t try out the OpenAI app may have looked for an alternative to experiment with. This would be an interesting explanation, too, as it suggests that OpenAI’s decision to gatekeep Sora may have directly boosted its rivals. &lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 27-29, 2025&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-large"&gt;&lt;img alt="Meta AI's Vibes feed, which showcases AI videos, may have driven a spike in app downloads and usage, " class="wp-image-3059536" height="537" src="https://techcrunch.com/wp-content/uploads/2025/10/Screenshot-2025-10-20-at-1.41.36PM.png?w=680" width="680" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Similarweb&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;As of October 17, Meta AI’s app had seen a 15.58% increase in daily active users worldwide, while ChatGPT, Grok, and Perplexity saw declines of 3.51%, 7.35%, and 2.29%, respectively. &lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2025/10/20/meta-ais-app-downloads-and-daily-users-spiked-after-launch-of-vibes-ai-video-feed/</guid><pubDate>Mon, 20 Oct 2025 18:22:38 +0000</pubDate></item></channel></rss>