<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>AI News Aggregator - Full Text</title><link>https://Tairon861.github.io/ai-news-aggregator/feed.xml</link><description>Recent AI News with Full Content</description><atom:link href="https://Tairon861.github.io/ai-news-aggregator/feed.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 07 Jan 2026 12:52:11 +0000</lastBuildDate><item><title>Meta’s Manus news is getting different receptions in Washington and Beijing (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/06/metas-manus-news-is-getting-different-receptions-in-washington-and-beijing/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1968119319.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta’s $2 billion acquisition of AI assistant platform Manus is unsurprisingly caught in a regulatory tug-of-war — but not because of U.S. regulators. They appear assured that the deal is legitimate despite earlier misgivings about Benchmark’s investment in Manus. China’s regulators, however, are reportedly not quite as sanguine, according to the Financial Times.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Benchmark led a financing round for Manus earlier this year, the investment sparked immediate controversy. U.S. Senator John Cornyn complained about the deal on X, and the investment prompted inquiries from the U.S. Treasury Department around new rules restricting American investment in Chinese AI companies.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The concerns were significant enough to spur Manus’s eventual relocation from Beijing to Singapore — part of what drove the company’s “step-by-step disentanglement from China,” as one Chinese professor described it on WeChat this past weekend.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now the tables have turned. Chinese officials are reportedly reviewing whether the Meta deal violates technology export controls, potentially giving Beijing leverage it wasn’t initially perceived as having. Specifically, they’re examining whether Manus needed an export license when it relocated its core team from China to Singapore — a move that’s apparently now so common it has earned the nickname “Singapore washing.” A recent Wall Street Journal article speculated that China has “few tools to influence the deal given Manus’s foothold in Singapore,” but that assessment may have been premature.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The concern in Beijing is that this deal could encourage more Chinese startups to physically relocate to dodge domestic oversight. Winston Ma, a professor at New York University School of Law and partner at Dragon Capital, told the Journal that if the deal closes smoothly, “It creates a new path for the young AI startups in China.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;History suggests Beijing could act. China previously used similar export control mechanisms to intervene in Trump’s attempted TikTok ban during his first term. The Chinese professor on WeChat even warned that Manus’ founders could face criminal liability if they exported restricted technology without authorization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, some U.S. analysts are calling the acquisition a win for Washington’s investment restrictions, arguing it shows Chinese AI talent is defecting to the American ecosystem. One expert told the FT that the deal demonstrates “the US AI ecosystem is currently more attractive.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s too early to know if this impacts Meta’s plans to integrate Manus’s AI agent software into its products, but this $2 billion deal may have gotten more complicated than anyone anticipated.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2024/10/GettyImages-1968119319.jpg?resize=1200,800" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Meta’s $2 billion acquisition of AI assistant platform Manus is unsurprisingly caught in a regulatory tug-of-war — but not because of U.S. regulators. They appear assured that the deal is legitimate despite earlier misgivings about Benchmark’s investment in Manus. China’s regulators, however, are reportedly not quite as sanguine, according to the Financial Times.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;When Benchmark led a financing round for Manus earlier this year, the investment sparked immediate controversy. U.S. Senator John Cornyn complained about the deal on X, and the investment prompted inquiries from the U.S. Treasury Department around new rules restricting American investment in Chinese AI companies.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;The concerns were significant enough to spur Manus’s eventual relocation from Beijing to Singapore — part of what drove the company’s “step-by-step disentanglement from China,” as one Chinese professor described it on WeChat this past weekend.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Now the tables have turned. Chinese officials are reportedly reviewing whether the Meta deal violates technology export controls, potentially giving Beijing leverage it wasn’t initially perceived as having. Specifically, they’re examining whether Manus needed an export license when it relocated its core team from China to Singapore — a move that’s apparently now so common it has earned the nickname “Singapore washing.” A recent Wall Street Journal article speculated that China has “few tools to influence the deal given Manus’s foothold in Singapore,” but that assessment may have been premature.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The concern in Beijing is that this deal could encourage more Chinese startups to physically relocate to dodge domestic oversight. Winston Ma, a professor at New York University School of Law and partner at Dragon Capital, told the Journal that if the deal closes smoothly, “It creates a new path for the young AI startups in China.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;History suggests Beijing could act. China previously used similar export control mechanisms to intervene in Trump’s attempted TikTok ban during his first term. The Chinese professor on WeChat even warned that Manus’ founders could face criminal liability if they exported restricted technology without authorization.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, some U.S. analysts are calling the acquisition a win for Washington’s investment restrictions, arguing it shows Chinese AI talent is defecting to the American ecosystem. One expert told the FT that the deal demonstrates “the US AI ecosystem is currently more attractive.”&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;It’s too early to know if this impacts Meta’s plans to integrate Manus’s AI agent software into its products, but this $2 billion deal may have gotten more complicated than anyone anticipated.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/06/metas-manus-news-is-getting-different-receptions-in-washington-and-beijing/</guid><pubDate>Wed, 07 Jan 2026 02:52:57 +0000</pubDate></item><item><title>McKinsey and General Catalyst execs say the era of ‘learn once, work forever’ is over (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/06/mckinsey-and-general-catalyst-execs-say-the-era-of-learn-once-work-forever-is-over/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-06-at-6.30.05-PM.png?resize=1200,684" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If there is one point of consensus among the CES 2026 keynote speakers, it is that AI is reshaping technology with a speed and scale unlike any previous technological revolution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a live taping on Tuesday of the All-In podcast, co-host Jason Calacanis interviewed Bob Sternfels, Global Managing Partner of McKinsey &amp;amp; Company, and Hemant Taneja, CEO of General Catalyst. Their discussion focused on how AI is transforming investment strategies and the workforce.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The world has completely changed,” Taneja said about the unprecedented growth of AI companies. He noted that while it took Stripe about 12 years to reach a $100 billion valuation, Anthropic, another General Catalyst portfolio company, soared from a $60 billion valuation last year to a “couple hundred billion dollars” this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taneja believes we are on the verge of seeing a new wave of trillion-dollar companies. “That’s not a pie-in-the-sky idea with Anthropic, OpenAI, and a couple of others,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calacanis pressed them on what’s driving this explosive growth. According to McKinsey’s Sternfels, while many companies are testing AI products, non-tech enterprises remain on the fence about full adoption. Sternfels says the question that McKinsey consultants often hear from CEOs is: “Do I listen to my CFO or my CIO right now?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CFOs, seeing little return on investment, argue for delaying implementation. Meanwhile, CIOs claim it’s “crazy” not to adopt AI because “we’ll be disrupted,” Sternfels said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another key concern is how AI is reshaping the labor force. “Some people are looking at AI and they’re scared,” Calacanis said, noting concerns that AI could replace entry-level jobs traditionally filled by recent graduates. He asked Sternfels and Taneja for advice on what young people should do in this new landscape.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Sternfels said that while AI models can handle many tasks, sound judgment and creativity remain the essential skills humans must bring to succeed in an AI-infused world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Taneja argued that people must recognize that “skilling and re-skilling” will be a lifelong endeavor. “This idea that we spend 22 years learning and then 40 years working is broken,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calacanis agreed that in a world where it may take less time to build an AI agent than to train a new worker, people must find ways to stay relevant. “To stand out, you’re going to have to show chutzpah, drive, passion,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sternfels provided a glimpse into that future. While he expects McKinsey to have as many “personalized” AI agents as employees by the end of 2026, he noted that headcount will not necessarily decrease. Instead, the firm is shifting its composition; it’s increasing employees who work directly with clients by 25% while reducing back-office roles by the same percentage.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://techcrunch.com/wp-content/uploads/2026/01/Screenshot-2026-01-06-at-6.30.05-PM.png?resize=1200,684" /&gt;&lt;/div&gt;&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;If there is one point of consensus among the CES 2026 keynote speakers, it is that AI is reshaping technology with a speed and scale unlike any previous technological revolution.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;In a live taping on Tuesday of the All-In podcast, co-host Jason Calacanis interviewed Bob Sternfels, Global Managing Partner of McKinsey &amp;amp; Company, and Hemant Taneja, CEO of General Catalyst. Their discussion focused on how AI is transforming investment strategies and the workforce.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;“The world has completely changed,” Taneja said about the unprecedented growth of AI companies. He noted that while it took Stripe about 12 years to reach a $100 billion valuation, Anthropic, another General Catalyst portfolio company, soared from a $60 billion valuation last year to a “couple hundred billion dollars” this year.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Taneja believes we are on the verge of seeing a new wave of trillion-dollar companies. “That’s not a pie-in-the-sky idea with Anthropic, OpenAI, and a couple of others,” he said. &lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calacanis pressed them on what’s driving this explosive growth. According to McKinsey’s Sternfels, while many companies are testing AI products, non-tech enterprises remain on the fence about full adoption. Sternfels says the question that McKinsey consultants often hear from CEOs is: “Do I listen to my CFO or my CIO right now?”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;CFOs, seeing little return on investment, argue for delaying implementation. Meanwhile, CIOs claim it’s “crazy” not to adopt AI because “we’ll be disrupted,” Sternfels said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Another key concern is how AI is reshaping the labor force. “Some people are looking at AI and they’re scared,” Calacanis said, noting concerns that AI could replace entry-level jobs traditionally filled by recent graduates. He asked Sternfels and Taneja for advice on what young people should do in this new landscape.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p class="wp-block-paragraph"&gt;Sternfels said that while AI models can handle many tasks, sound judgment and creativity remain the essential skills humans must bring to succeed in an AI-infused world.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Meanwhile, Taneja argued that people must recognize that “skilling and re-skilling” will be a lifelong endeavor. “This idea that we spend 22 years learning and then 40 years working is broken,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Calacanis agreed that in a world where it may take less time to build an AI agent than to train a new worker, people must find ways to stay relevant. “To stand out, you’re going to have to show chutzpah, drive, passion,” he said.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Sternfels provided a glimpse into that future. While he expects McKinsey to have as many “personalized” AI agents as employees by the end of 2026, he noted that headcount will not necessarily decrease. Instead, the firm is shifting its composition; it’s increasing employees who work directly with clients by 25% while reducing back-office roles by the same percentage.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/06/mckinsey-and-general-catalyst-execs-say-the-era-of-learn-once-work-forever-is-over/</guid><pubDate>Wed, 07 Jan 2026 03:09:04 +0000</pubDate></item><item><title>[NEW] Grab brings robotics in-house to manage delivery costs (AI News)</title><link>https://www.artificialintelligence-news.com/news/grab-brings-robotics-in-house-to-manage-delivery-costs/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Grab-brings-robotics-in-house-to-manage-delivery-costs-scaled-e1767752940871.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Rising labour costs and tighter delivery margins are pushing large platform operators like Grab to look at automation. It’s moved to bring robotics capability in-house by its acquisition of Infermove.&lt;/p&gt;&lt;p&gt;Grab operates at a scale where small efficiency gains can have out-sized effects. Its platform supports millions of deliveries in Southeast Asia, many of them carried out by riders on scooters and bicycles in dense urban areas, producing complexity that limits how much automation could replace human labour. By acquiring a company focused on robots designed for unstructured settings, Grab sees physical-world AI as mature enough to use in cases outside pilot programmes.&lt;/p&gt;&lt;h3&gt;Delivery automation close to core operations&lt;/h3&gt;&lt;p&gt;Rather than relying on off-the-shelf systems, Grab is opting to internalise the development loop. Infermove’s technology is designed to learn from real-world movement data, including information generated by non-motorised delivery vehicles. In practical terms, that means robots trained on how people actually navigate pavements, crossings, and crowded drop-off points, rather than how those spaces appear in simulations.&lt;/p&gt;&lt;p&gt;For a delivery operator like Grab, that distinction matters. Simulated environments can support early development, but they often struggle with the edge cases that define real cities. Bringing that learning process in-house allows Grab to shape how automation behaves under its own operating constraints, rather than adapting its delivery network to fit a third-party system.&lt;/p&gt;&lt;p&gt;From an enterprise perspective, the strategic value lies in control. Owning the technology gives Grab more influence over deployment pace, operating scope, and cost trade-offs. It also reduces long-term dependence on vendors whose priorities may not match Grab’s regional footprint or economic realities.&lt;/p&gt;&lt;p&gt;Automation, however, is not positioned as a replacement for human riders. Even as robots take on parts of the workflow, people remain central to service delivery. Grab’s interest appears focused on selective use, like structured first-mile or last-mile segments where tasks are repetitive and distances are short. In these areas, robots may help smooth demand spikes, reduce delays during peak hours, and ease pressure during labour shortages.&lt;/p&gt;&lt;h3&gt;Managing cost pressure without breaking service&lt;/h3&gt;&lt;p&gt;During an internal meeting in December, Grab’s chief technology officer Suthen Thomas described Infermove’s progress as “impressive,” highlighting both the technology and its early commercial use. He also said the company would continue to operate independently, with its founder reporting directly to him. The structure suggests Grab is prioritising execution and continuity rather than rapid organisational integration.&lt;/p&gt;&lt;p&gt;The approach reflects a broader shift among large digital platforms. Instead of treating AI as a layer added on top of existing systems, companies are embedding it deeper into core operations. In delivery and logistics, that often means moving beyond optimisation software into physical automation, where the risks and costs are higher but the potential gains are more structural.&lt;/p&gt;&lt;p&gt;The timing is also telling. On-demand delivery volumes continue to grow, but margins remain under pressure. Customers expect faster service and lower fees, while operators face rising wages, fuel costs, and tighter regulation. In that environment, automation becomes less about novelty and more about sustaining service levels without eroding profitability.&lt;/p&gt;&lt;p&gt;Bringing robotics development closer to operations may also help align incentives around data use. Training physical AI systems requires large amounts of real-world data, which delivery platforms already generate at scale. Keeping that feedback loop internal can speed iteration and reduce the need to share sensitive operational data externally.&lt;/p&gt;&lt;p&gt;There are still limits. Robots designed for pavements and short routes are unlikely to replace human couriers in an entire network anytime soon. Weather, local rules, and customer acceptance will continue to shape where automation can realistically operate. Expanding in multiple countries adds further complexity, as infrastructure and regulations vary widely.&lt;/p&gt;&lt;p&gt;Industry forecasts suggest rapid growth in last-mile delivery robotics, but those figures offer limited guidance for operators. The more immediate question is whether automation can lower cost per delivery without introducing new failure points. That depends less on market size and more on performance in live environments.&lt;/p&gt;&lt;p&gt;Seen through an enterprise lens, the acquisition of Infermove is not a bet on robotics as a product category. It is a move to tighten the link between AI, data, and physical operations. For platform companies built on logistics and mobility, that integration may become a key factor in managing growth under sustained cost pressure.&lt;/p&gt;&lt;p&gt;(Photo by Afif Ramdhasuma)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: The Law Society: Current laws are fit for the AI era&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://www.artificialintelligence-news.com/wp-content/uploads/2026/01/Grab-brings-robotics-in-house-to-manage-delivery-costs-scaled-e1767752940871.jpg" /&gt;&lt;/div&gt;&lt;p&gt;Rising labour costs and tighter delivery margins are pushing large platform operators like Grab to look at automation. It’s moved to bring robotics capability in-house by its acquisition of Infermove.&lt;/p&gt;&lt;p&gt;Grab operates at a scale where small efficiency gains can have out-sized effects. Its platform supports millions of deliveries in Southeast Asia, many of them carried out by riders on scooters and bicycles in dense urban areas, producing complexity that limits how much automation could replace human labour. By acquiring a company focused on robots designed for unstructured settings, Grab sees physical-world AI as mature enough to use in cases outside pilot programmes.&lt;/p&gt;&lt;h3&gt;Delivery automation close to core operations&lt;/h3&gt;&lt;p&gt;Rather than relying on off-the-shelf systems, Grab is opting to internalise the development loop. Infermove’s technology is designed to learn from real-world movement data, including information generated by non-motorised delivery vehicles. In practical terms, that means robots trained on how people actually navigate pavements, crossings, and crowded drop-off points, rather than how those spaces appear in simulations.&lt;/p&gt;&lt;p&gt;For a delivery operator like Grab, that distinction matters. Simulated environments can support early development, but they often struggle with the edge cases that define real cities. Bringing that learning process in-house allows Grab to shape how automation behaves under its own operating constraints, rather than adapting its delivery network to fit a third-party system.&lt;/p&gt;&lt;p&gt;From an enterprise perspective, the strategic value lies in control. Owning the technology gives Grab more influence over deployment pace, operating scope, and cost trade-offs. It also reduces long-term dependence on vendors whose priorities may not match Grab’s regional footprint or economic realities.&lt;/p&gt;&lt;p&gt;Automation, however, is not positioned as a replacement for human riders. Even as robots take on parts of the workflow, people remain central to service delivery. Grab’s interest appears focused on selective use, like structured first-mile or last-mile segments where tasks are repetitive and distances are short. In these areas, robots may help smooth demand spikes, reduce delays during peak hours, and ease pressure during labour shortages.&lt;/p&gt;&lt;h3&gt;Managing cost pressure without breaking service&lt;/h3&gt;&lt;p&gt;During an internal meeting in December, Grab’s chief technology officer Suthen Thomas described Infermove’s progress as “impressive,” highlighting both the technology and its early commercial use. He also said the company would continue to operate independently, with its founder reporting directly to him. The structure suggests Grab is prioritising execution and continuity rather than rapid organisational integration.&lt;/p&gt;&lt;p&gt;The approach reflects a broader shift among large digital platforms. Instead of treating AI as a layer added on top of existing systems, companies are embedding it deeper into core operations. In delivery and logistics, that often means moving beyond optimisation software into physical automation, where the risks and costs are higher but the potential gains are more structural.&lt;/p&gt;&lt;p&gt;The timing is also telling. On-demand delivery volumes continue to grow, but margins remain under pressure. Customers expect faster service and lower fees, while operators face rising wages, fuel costs, and tighter regulation. In that environment, automation becomes less about novelty and more about sustaining service levels without eroding profitability.&lt;/p&gt;&lt;p&gt;Bringing robotics development closer to operations may also help align incentives around data use. Training physical AI systems requires large amounts of real-world data, which delivery platforms already generate at scale. Keeping that feedback loop internal can speed iteration and reduce the need to share sensitive operational data externally.&lt;/p&gt;&lt;p&gt;There are still limits. Robots designed for pavements and short routes are unlikely to replace human couriers in an entire network anytime soon. Weather, local rules, and customer acceptance will continue to shape where automation can realistically operate. Expanding in multiple countries adds further complexity, as infrastructure and regulations vary widely.&lt;/p&gt;&lt;p&gt;Industry forecasts suggest rapid growth in last-mile delivery robotics, but those figures offer limited guidance for operators. The more immediate question is whether automation can lower cost per delivery without introducing new failure points. That depends less on market size and more on performance in live environments.&lt;/p&gt;&lt;p&gt;Seen through an enterprise lens, the acquisition of Infermove is not a bet on robotics as a product category. It is a move to tighten the link between AI, data, and physical operations. For platform companies built on logistics and mobility, that integration may become a key factor in managing growth under sustained cost pressure.&lt;/p&gt;&lt;p&gt;(Photo by Afif Ramdhasuma)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;See also: The Law Society: Current laws are fit for the AI era&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Want to learn more about AI and big data from industry leaders?&lt;/strong&gt; Check out AI &amp;amp; Big Data Expo taking place in Amsterdam, California, and London. The comprehensive event is part of TechEx and is co-located with other leading technology events, click here for more information.&lt;/p&gt;&lt;p&gt;AI News is powered by TechForge Media. Explore other upcoming enterprise technology events and webinars here.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://www.artificialintelligence-news.com/news/grab-brings-robotics-in-house-to-manage-delivery-costs/</guid><pubDate>Wed, 07 Jan 2026 10:00:00 +0000</pubDate></item><item><title>[NEW] The man who made India digital isn’t done yet (MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/07/1129748/aadhaar-nandan-nilekani-india-digital-biometric-identity-data/</link><description>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Nandan Nilekani can’t stop trying to push India into the future. He started nearly 30 years ago, masterminding an ongoing experiment in technological state capacity that started with Aadhaar—the world’s largest digital identity system. Aadhaar means “foundation” in Hindi, and on that bedrock Nilekani and people working with him went on to build a sprawling collection of free, interoperating online tools that add up to nothing less than a digital infrastructure for society. They cover government services, digital payments, banking, credit, and health care, offering convenience and access that would be eye-popping in wealthy countries a tenth of India’s size. In India those systems are called, collectively, “digital public infrastructure,” or DPI.&lt;/p&gt;  &lt;p&gt;At 70 years old, Nilekani should be retired. But he has a few more ideas. India’s electrical grid is creaky and prone to failure; Nilekani wants to add a layer of digital communication to stabilize it. And then there’s his idea to expand the financial functions in DPI to the rest of the world, creating a global digital backbone for commerce that he calls the “finternet.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“It sounds like some crazy stuff,” Nilekani says. “But I think these are all big ideas, which over the next five years will have demonstrable, material impact.” As a last act in public life, why not Aadhaarize the world?&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;India’s digital backbone&lt;/h3&gt;  &lt;p&gt;Today, a farmer in a village in India, hours from the nearest bank, can collect welfare payments or transfer money by simply pressing a thumb to a fingerprint scanner at the local store. Digitally authenticated copies of driver’s licenses, birth certificates, and educational records can be accessed and shared via a digital wallet that sits on your smartphone.&lt;/p&gt; 
 &lt;p&gt;In big cities, where cash is less and less common (just trying to break a bill can be a major headache), mobile payments are ubiquitous, whether you’re buying a TV from a high-street retailer or a coconut from a roadside cart. There are no fees, and any payment app or bank account can send money to any other. The country’s chaotic patchwork of public and private hospitals have begun digitizing all their medical records and uploading them to a nationwide platform. On the Open Network for Digital Commerce (ONDC), people can do online shopping searches on whatever app they want, and the results show sellers from an array of &lt;em&gt;other&lt;/em&gt; platforms, too. The idea is to liberate small merchants and consumers from the walled gardens of online shopping giants like Amazon and the domestic giant Flipkart.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;In the most populous nation on Earth—with 1.4 billion people—a large portion of the bureaucracy anyone encounters in daily life happens seamlessly and in the cloud.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;At the heart of all these tools is Aadhaar. The system gives every Indian a 12-digit number that, in combination with either a fingerprint scan or an SMS code, allows access to government services, SIM cards, basic bank accounts, digital signature services, and social welfare payments. The Indian government says that since its inception in 2009, Aadhaar has saved 3.48 trillion rupees ($39.2 billion) by boosting efficiency, bypassing corrupt officials, and cutting other types of fraud. The system is controversial and imperfect—a database with 1.4 billion people in it comes with inherent security and privacy concerns. Still, in the most populous nation on Earth, a big portion of the bureaucracy anyone might encounter in daily life just happens in the cloud.&lt;/p&gt; 
 &lt;p&gt;Nilekani was behind much of that innovation, marshaling an army of civil servants, tech companies, and volunteers. Now he sees it in action every day. “It reinforces that what you have done is not some abstract stuff, but real stuff for real people,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;By his own admission, Nilekani is entering the twilight of his career. But it’s not over yet. He’s now “chief mentor” for the India Energy Stack (IES), a government initiative to connect the fragmented data held by companies responsible for generating, transmitting, and distributing power. India’s grids are unstable and disparate, but Nilekani hopes an Aadhaar-like move will help. IES aims to give unique digital identities not only to power plants and energy storage facilities but even to rooftop solar panels and electric vehicles. All the data attached to those things—device characteristics, energy rating certifications, usage information—will be in a common, machine-readable format and shared on the same open protocols.&lt;/p&gt;  &lt;p&gt;Ideally, that’ll give grid operators a real-time view of energy supply and demand. And if it works, it might also make it simpler and cheaper for &lt;em&gt;anyone&lt;/em&gt; to connect to the grid—even everyday folks selling excess power from their rooftop solar rigs, says RS Sharma, the chair of the project and Nilekani’s deputy while building Aadhaar.&lt;/p&gt;  &lt;p&gt;Nilekani’s other side hustle is even more ambitious. His idea for a global “finternet” combines Aadhaarization with blockchains—creating digital representations called tokens for not only financial instruments like stocks or bonds but also real-world assets like houses or jewelry. Anyone from a bank to an asset manager or even a company could create and manage these tokens, but Nilekani’s team especially hopes the idea will help poor people trade their assets, or use them as loan collateral—expanding financial services to those who otherwise couldn’t access them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;It sounds almost wild-eyed. Yet the finternet project has 30 partners across four continents. Nilekani says it’ll launch next year.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A call to service&lt;/h3&gt;  &lt;p&gt;Nilekani was born in Bengaluru, in 1955. His family was middle class and, Nilekani says, “seized with societal issues and challenges.” His upbringing was also steeped in the kind of socialism espoused by the newish nation’s first prime minister, Jawaharlal Nehru.&lt;/p&gt;  &lt;p&gt;After studying electrical engineering at the Indian Institute of Technology, in 1981 Nilekani helped found Infosys, an information technology company that pioneered outsourcing and helped turned India into the world’s IT back office. In 1999, he was part of a government-appointed task force trying to upgrade the infrastructure and services in Bengaluru, then emerging as India’s tech capital. But Nilekani was at the time leery of being viewed as just another techno-optimist. “I didn’t want to be seen as naive enough to believe that tech could solve everything,” he says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Nilekani holds a device to one eye" class="wp-image-1129867" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-526255786.jpg?w=910" /&gt;&lt;figcaption class="wp-element-caption"&gt;Nilekani demonstrates the biometric technology at the heart of Aadhaar, the system he spearheaded that provides a unique digital identity number to all Indians.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;PALLAVA BAGLA/CORBIS/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Seeing the scope of the problem changed his mind—sclerotic bureaucracy, endemic corruption, and financial exclusion were intractable without technological solutions. In 2008 Nilekani published a book, &lt;em&gt;Imagining India: The Idea of a Renewed Nation&lt;/em&gt;. It was a manifesto for an India that could leapfrog into a networked future.&lt;/p&gt; 

 &lt;p&gt;And it got him a job. At the time more than half the births in the country were not recorded, and up to 400 million Indians had no official identity document. Manmohan Singh, the prime minister, asked Nilekani to put into action an ill-defined plan to create a national identity card.&lt;/p&gt;  &lt;p&gt;Nilekani’s team made a still-controversial decision to rely on biometrics. A system based on people’s fingerprints and retina scans meant nobody could sign up twice, and nobody had to carry paperwork. In terms of execution, it was like trying to achieve industrialization but skip a steam era. Deployment required a monumental data collection effort, as well as new infrastructure that could compare each new enrollment against hundreds of millions of existing records in seconds. At its peak, the Unique Identification Authority of India (UIDAI), the agency responsible for administering Aadhaar, was registering more than a million new users a day. That happened with a technical team of just about 50 developers, and in the end cost slightly less than half a billion dollars.&lt;/p&gt;  &lt;p&gt;Buoyed by their success, Nilekani and his allies started casting around for other problems they could solve using the same digitize-the-real-world playbook. “We built more and more layers of capability,” Nilekani says, “and then this became a wider-ranging idea. More grandiose.”&lt;/p&gt;  &lt;p&gt;While other countries were building digital backbones with full state control (as in China) or in public-private partnerships that favored profit-seeking corporate approaches (as in the US), Nilekani thought India needed something else. He wanted critical technologies in areas like identity, payments, and data sharing to be open and interoperable, not monopolized by either the state or private industry. So the tools that make up DPI use open standards and open APIs, meaning that anyone can plug into the system. No single company or institution controls access—no walled gardens.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;h3 class="wp-block-heading"&gt;A contested legacy&lt;/h3&gt;  &lt;p&gt;Of course, another way to look at putting financial and government services and records into giant databases is that it’s a massive risk to personal liberty. Aadhaar, in particular, has faced criticism from privacy advocates concerned about the potential for surveillance. Several high-profile data breaches of Aadhaar records held by government entities have shaken confidence in the system, most recently in 2023, when security researchers found hackers selling the records of more than 800 million Indians on the dark web.&lt;/p&gt;  &lt;p&gt;Technically, this shouldn’t matter—an Aadhaar number ought to be useless without biometric or SMS-based authentication. It’s “a myth that this random number is a very powerful number,” says Sharma, the onetime co-lead of UIDAI. “I don’t have any example where somebody’s Aadhaar disclosure would have harmed somebody.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;One problem is that in everyday use, Aadhaar users often bypass the biometric authentication system. To ensure that people use a genuine address at registration, Aadhaar administrators give people their numbers on an official-looking document. Indians co-opted this paperwork as a proof of identity on its own. And since the document—Indians even call it an “Aadhaar card”—doesn’t have an expiration date, it’s possible for people to get multiple valid cards with different details by changing their address or date of birth. That’s quite a loophole. In 2018 an NGO report found that 67% of people using Aadhaar to open a bank account relied on this verification document rather than digital authentication. That report was the last time anyone published data on the problem, so nobody knows how bad it is today. “Everybody’s living on anecdotes,” says Kiran Jonnalagadda, an anti-Aadhaar activist.&lt;/p&gt;  &lt;p&gt;In other cases, flaws in Aadhaar’s biometric technology have caused people to be denied essential government services. The government downplays these risks, but again, it’s impossible to tell how serious the problem is because the UIDAI won’t disclose numbers. “There needs to be a much more honest acknowledgment, documentation, and then an examination of how those exclusions can be mitigated,” says Apar Gupta, director of the Internet Freedom Foundation.&lt;/p&gt; 
 &lt;p&gt;Beyond the potential for fraud, it’s also true that the free and interoperable tools haven’t reached all the people who might find them useful, especially among India’s rural and poorer populations. Nilekani’s hopes for openness haven’t fully come to pass. Big e-commerce companies still dominate, and retail sales on ONDC have been dropping steadily since 2024, when financial incentives to participate began to taper off. The digital payments and government documentation services have hundreds of millions of users, numbers most global technology companies would love to see—but in a country as large as India, that leaves a lot of people out.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Going global&lt;/h3&gt;  &lt;p&gt;The usually calm Nilekani bristles at that criticism; he has heard it before. Detractors overlook the dysfunction that preceded these efforts, he says, and he remains convinced that technology was the only way forward. “How do you move a country of 1.4 billion people?” he asks. “There’s no other way you can fix it.”&lt;/p&gt; 
 &lt;p&gt;The proof is self-evident, he says. Indians have opened more than 500 million basic bank accounts using Aadhaar; before it came into use, millions of those people had been completely unbanked. Earlier this year, India’s Unified Payments Interface overtook Visa as the world’s largest real-time payments system. “There is no way Aadhaar could have worked but for the fact that people needed this thing,” Nilekani says. “There’s no way payments would have worked without people needing it. So the voice of the people—they’re voting with their feet.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129868" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-2234533815.jpg?w=1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;A street vendor in Kolkata displays a QR code that lets him get paid via India’s Unified Payments Interface, part of the digital public infrastructure Nilekani helped build. The Reserve Bank of India says more than 657 million people used the system in the financial year 2024–2025.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;DEBAJYOTI CHAKRABORTY/NURPHOTO/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;That need might be present in countries beyond India. “Many countries don’t have a proper birth registration system. Many countries don’t have a payment system. Many countries don’t have a way for data to be leveraged,” Nilekani says. “So this is a very powerful idea.” It seems to be spreading. Foreign governments regularly send delegations to Bengaluru to study India’s DPI tools. The World Bank and the United Nations have tried to introduce the concept to other developing countries equally eager to bring their economies into the digital age. The Gates Foundation has established projects to promote digital infrastructure, and Nilekani has set up and funded a network of think tanks, research institutes, and other NGOs aimed at, as he says, “propagating the gospel.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Still, he admits he might not live to see DPI go global. “There are two races,” Nilekani says. “My personal race against time and India’s race against time.” He worries that the economic potential of its vast young population—the so-called demographic dividend—could turn into a demographic disaster. Despite rapid growth, gains have been uneven. Youth unemployment remains stubbornly high—a particularly volatile problem in a large and economically turbulent country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Maybe I’m a junkie,” he says. “Why the hell am I doing all this? I think I need it. I think I need to keep curious and alive and looking at the future.” But that’s the thing about building the future: It never quite arrives.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Edd Gent is a journalist based in Bengaluru, India.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;Nandan Nilekani can’t stop trying to push India into the future. He started nearly 30 years ago, masterminding an ongoing experiment in technological state capacity that started with Aadhaar—the world’s largest digital identity system. Aadhaar means “foundation” in Hindi, and on that bedrock Nilekani and people working with him went on to build a sprawling collection of free, interoperating online tools that add up to nothing less than a digital infrastructure for society. They cover government services, digital payments, banking, credit, and health care, offering convenience and access that would be eye-popping in wealthy countries a tenth of India’s size. In India those systems are called, collectively, “digital public infrastructure,” or DPI.&lt;/p&gt;  &lt;p&gt;At 70 years old, Nilekani should be retired. But he has a few more ideas. India’s electrical grid is creaky and prone to failure; Nilekani wants to add a layer of digital communication to stabilize it. And then there’s his idea to expand the financial functions in DPI to the rest of the world, creating a global digital backbone for commerce that he calls the “finternet.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;“It sounds like some crazy stuff,” Nilekani says. “But I think these are all big ideas, which over the next five years will have demonstrable, material impact.” As a last act in public life, why not Aadhaarize the world?&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;India’s digital backbone&lt;/h3&gt;  &lt;p&gt;Today, a farmer in a village in India, hours from the nearest bank, can collect welfare payments or transfer money by simply pressing a thumb to a fingerprint scanner at the local store. Digitally authenticated copies of driver’s licenses, birth certificates, and educational records can be accessed and shared via a digital wallet that sits on your smartphone.&lt;/p&gt; 
 &lt;p&gt;In big cities, where cash is less and less common (just trying to break a bill can be a major headache), mobile payments are ubiquitous, whether you’re buying a TV from a high-street retailer or a coconut from a roadside cart. There are no fees, and any payment app or bank account can send money to any other. The country’s chaotic patchwork of public and private hospitals have begun digitizing all their medical records and uploading them to a nationwide platform. On the Open Network for Digital Commerce (ONDC), people can do online shopping searches on whatever app they want, and the results show sellers from an array of &lt;em&gt;other&lt;/em&gt; platforms, too. The idea is to liberate small merchants and consumers from the walled gardens of online shopping giants like Amazon and the domestic giant Flipkart.&amp;nbsp;&lt;/p&gt;  &lt;blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow"&gt; &lt;p&gt;&lt;strong&gt;In the most populous nation on Earth—with 1.4 billion people—a large portion of the bureaucracy anyone encounters in daily life happens seamlessly and in the cloud.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;At the heart of all these tools is Aadhaar. The system gives every Indian a 12-digit number that, in combination with either a fingerprint scan or an SMS code, allows access to government services, SIM cards, basic bank accounts, digital signature services, and social welfare payments. The Indian government says that since its inception in 2009, Aadhaar has saved 3.48 trillion rupees ($39.2 billion) by boosting efficiency, bypassing corrupt officials, and cutting other types of fraud. The system is controversial and imperfect—a database with 1.4 billion people in it comes with inherent security and privacy concerns. Still, in the most populous nation on Earth, a big portion of the bureaucracy anyone might encounter in daily life just happens in the cloud.&lt;/p&gt; 
 &lt;p&gt;Nilekani was behind much of that innovation, marshaling an army of civil servants, tech companies, and volunteers. Now he sees it in action every day. “It reinforces that what you have done is not some abstract stuff, but real stuff for real people,” he says.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;By his own admission, Nilekani is entering the twilight of his career. But it’s not over yet. He’s now “chief mentor” for the India Energy Stack (IES), a government initiative to connect the fragmented data held by companies responsible for generating, transmitting, and distributing power. India’s grids are unstable and disparate, but Nilekani hopes an Aadhaar-like move will help. IES aims to give unique digital identities not only to power plants and energy storage facilities but even to rooftop solar panels and electric vehicles. All the data attached to those things—device characteristics, energy rating certifications, usage information—will be in a common, machine-readable format and shared on the same open protocols.&lt;/p&gt;  &lt;p&gt;Ideally, that’ll give grid operators a real-time view of energy supply and demand. And if it works, it might also make it simpler and cheaper for &lt;em&gt;anyone&lt;/em&gt; to connect to the grid—even everyday folks selling excess power from their rooftop solar rigs, says RS Sharma, the chair of the project and Nilekani’s deputy while building Aadhaar.&lt;/p&gt;  &lt;p&gt;Nilekani’s other side hustle is even more ambitious. His idea for a global “finternet” combines Aadhaarization with blockchains—creating digital representations called tokens for not only financial instruments like stocks or bonds but also real-world assets like houses or jewelry. Anyone from a bank to an asset manager or even a company could create and manage these tokens, but Nilekani’s team especially hopes the idea will help poor people trade their assets, or use them as loan collateral—expanding financial services to those who otherwise couldn’t access them.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;p&gt;It sounds almost wild-eyed. Yet the finternet project has 30 partners across four continents. Nilekani says it’ll launch next year.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;A call to service&lt;/h3&gt;  &lt;p&gt;Nilekani was born in Bengaluru, in 1955. His family was middle class and, Nilekani says, “seized with societal issues and challenges.” His upbringing was also steeped in the kind of socialism espoused by the newish nation’s first prime minister, Jawaharlal Nehru.&lt;/p&gt;  &lt;p&gt;After studying electrical engineering at the Indian Institute of Technology, in 1981 Nilekani helped found Infosys, an information technology company that pioneered outsourcing and helped turned India into the world’s IT back office. In 1999, he was part of a government-appointed task force trying to upgrade the infrastructure and services in Bengaluru, then emerging as India’s tech capital. But Nilekani was at the time leery of being viewed as just another techno-optimist. “I didn’t want to be seen as naive enough to believe that tech could solve everything,” he says.&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="Nilekani holds a device to one eye" class="wp-image-1129867" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-526255786.jpg?w=910" /&gt;&lt;figcaption class="wp-element-caption"&gt;Nilekani demonstrates the biometric technology at the heart of Aadhaar, the system he spearheaded that provides a unique digital identity number to all Indians.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;PALLAVA BAGLA/CORBIS/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;Seeing the scope of the problem changed his mind—sclerotic bureaucracy, endemic corruption, and financial exclusion were intractable without technological solutions. In 2008 Nilekani published a book, &lt;em&gt;Imagining India: The Idea of a Renewed Nation&lt;/em&gt;. It was a manifesto for an India that could leapfrog into a networked future.&lt;/p&gt; 

 &lt;p&gt;And it got him a job. At the time more than half the births in the country were not recorded, and up to 400 million Indians had no official identity document. Manmohan Singh, the prime minister, asked Nilekani to put into action an ill-defined plan to create a national identity card.&lt;/p&gt;  &lt;p&gt;Nilekani’s team made a still-controversial decision to rely on biometrics. A system based on people’s fingerprints and retina scans meant nobody could sign up twice, and nobody had to carry paperwork. In terms of execution, it was like trying to achieve industrialization but skip a steam era. Deployment required a monumental data collection effort, as well as new infrastructure that could compare each new enrollment against hundreds of millions of existing records in seconds. At its peak, the Unique Identification Authority of India (UIDAI), the agency responsible for administering Aadhaar, was registering more than a million new users a day. That happened with a technical team of just about 50 developers, and in the end cost slightly less than half a billion dollars.&lt;/p&gt;  &lt;p&gt;Buoyed by their success, Nilekani and his allies started casting around for other problems they could solve using the same digitize-the-real-world playbook. “We built more and more layers of capability,” Nilekani says, “and then this became a wider-ranging idea. More grandiose.”&lt;/p&gt;  &lt;p&gt;While other countries were building digital backbones with full state control (as in China) or in public-private partnerships that favored profit-seeking corporate approaches (as in the US), Nilekani thought India needed something else. He wanted critical technologies in areas like identity, payments, and data sharing to be open and interoperable, not monopolized by either the state or private industry. So the tools that make up DPI use open standards and open APIs, meaning that anyone can plug into the system. No single company or institution controls access—no walled gardens.&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;h3 class="wp-block-heading"&gt;A contested legacy&lt;/h3&gt;  &lt;p&gt;Of course, another way to look at putting financial and government services and records into giant databases is that it’s a massive risk to personal liberty. Aadhaar, in particular, has faced criticism from privacy advocates concerned about the potential for surveillance. Several high-profile data breaches of Aadhaar records held by government entities have shaken confidence in the system, most recently in 2023, when security researchers found hackers selling the records of more than 800 million Indians on the dark web.&lt;/p&gt;  &lt;p&gt;Technically, this shouldn’t matter—an Aadhaar number ought to be useless without biometric or SMS-based authentication. It’s “a myth that this random number is a very powerful number,” says Sharma, the onetime co-lead of UIDAI. “I don’t have any example where somebody’s Aadhaar disclosure would have harmed somebody.”&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;aside class="related__wrap"&gt;&lt;/aside&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt; &lt;p&gt;One problem is that in everyday use, Aadhaar users often bypass the biometric authentication system. To ensure that people use a genuine address at registration, Aadhaar administrators give people their numbers on an official-looking document. Indians co-opted this paperwork as a proof of identity on its own. And since the document—Indians even call it an “Aadhaar card”—doesn’t have an expiration date, it’s possible for people to get multiple valid cards with different details by changing their address or date of birth. That’s quite a loophole. In 2018 an NGO report found that 67% of people using Aadhaar to open a bank account relied on this verification document rather than digital authentication. That report was the last time anyone published data on the problem, so nobody knows how bad it is today. “Everybody’s living on anecdotes,” says Kiran Jonnalagadda, an anti-Aadhaar activist.&lt;/p&gt;  &lt;p&gt;In other cases, flaws in Aadhaar’s biometric technology have caused people to be denied essential government services. The government downplays these risks, but again, it’s impossible to tell how serious the problem is because the UIDAI won’t disclose numbers. “There needs to be a much more honest acknowledgment, documentation, and then an examination of how those exclusions can be mitigated,” says Apar Gupta, director of the Internet Freedom Foundation.&lt;/p&gt; 
 &lt;p&gt;Beyond the potential for fraud, it’s also true that the free and interoperable tools haven’t reached all the people who might find them useful, especially among India’s rural and poorer populations. Nilekani’s hopes for openness haven’t fully come to pass. Big e-commerce companies still dominate, and retail sales on ONDC have been dropping steadily since 2024, when financial incentives to participate began to taper off. The digital payments and government documentation services have hundreds of millions of users, numbers most global technology companies would love to see—but in a country as large as India, that leaves a lot of people out.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;Going global&lt;/h3&gt;  &lt;p&gt;The usually calm Nilekani bristles at that criticism; he has heard it before. Detractors overlook the dysfunction that preceded these efforts, he says, and he remains convinced that technology was the only way forward. “How do you move a country of 1.4 billion people?” he asks. “There’s no other way you can fix it.”&lt;/p&gt; 
 &lt;p&gt;The proof is self-evident, he says. Indians have opened more than 500 million basic bank accounts using Aadhaar; before it came into use, millions of those people had been completely unbanked. Earlier this year, India’s Unified Payments Interface overtook Visa as the world’s largest real-time payments system. “There is no way Aadhaar could have worked but for the fact that people needed this thing,” Nilekani says. “There’s no way payments would have worked without people needing it. So the voice of the people—they’re voting with their feet.”&lt;/p&gt; &lt;div class="wp-block-image"&gt; &lt;figure class="wp-block-image size-large"&gt;&lt;img alt="&amp;quot;&amp;quot;" class="wp-image-1129868" src="https://wp.technologyreview.com/wp-content/uploads/2025/12/GettyImages-2234533815.jpg?w=1024" /&gt;&lt;figcaption class="wp-element-caption"&gt;A street vendor in Kolkata displays a QR code that lets him get paid via India’s Unified Payments Interface, part of the digital public infrastructure Nilekani helped build. The Reserve Bank of India says more than 657 million people used the system in the financial year 2024–2025.&lt;/figcaption&gt;&lt;div class="image-credit"&gt;DEBAJYOTI CHAKRABORTY/NURPHOTO/GETTY IMAGES&lt;/div&gt; &lt;/figure&gt; &lt;/div&gt; &lt;p&gt;That need might be present in countries beyond India. “Many countries don’t have a proper birth registration system. Many countries don’t have a payment system. Many countries don’t have a way for data to be leveraged,” Nilekani says. “So this is a very powerful idea.” It seems to be spreading. Foreign governments regularly send delegations to Bengaluru to study India’s DPI tools. The World Bank and the United Nations have tried to introduce the concept to other developing countries equally eager to bring their economies into the digital age. The Gates Foundation has established projects to promote digital infrastructure, and Nilekani has set up and funded a network of think tanks, research institutes, and other NGOs aimed at, as he says, “propagating the gospel.”&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_12"&gt;&lt;p&gt;Still, he admits he might not live to see DPI go global. “There are two races,” Nilekani says. “My personal race against time and India’s race against time.” He worries that the economic potential of its vast young population—the so-called demographic dividend—could turn into a demographic disaster. Despite rapid growth, gains have been uneven. Youth unemployment remains stubbornly high—a particularly volatile problem in a large and economically turbulent country.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;“Maybe I’m a junkie,” he says. “Why the hell am I doing all this? I think I need it. I think I need to keep curious and alive and looking at the future.” But that’s the thing about building the future: It never quite arrives.&lt;/p&gt;  &lt;p&gt;&lt;em&gt;Edd Gent is a journalist based in Bengaluru, India.&lt;/em&gt;&lt;svg class="monogramTLogo" viewBox="0 0 1091.84 1091.84" xmlns="http://www.w3.org/2000/svg"&gt;&lt;polygon fill="#6d6e71" points="363.95 0 363.95 1091.84 727.89 1091.84 727.89 363.95 363.95 0"&gt;&lt;polygon fill="#939598" points="363.95 0 728.24 365.18 1091.84 364.13 1091.84 0 363.95 0"&gt;&lt;polygon fill="#414042" points="0 0 0 0.03 0 363.95 363.95 363.95 363.95 0 0 0"&gt;&lt;/svg&gt; &lt;/p&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/07/1129748/aadhaar-nandan-nilekani-india-digital-biometric-identity-data/</guid><pubDate>Wed, 07 Jan 2026 11:00:00 +0000</pubDate></item><item><title>[NEW] LLMs contain a LOT of parameters. But what’s a parameter? (Artificial intelligence – MIT Technology Review)</title><link>https://www.technologyreview.com/2026/01/07/1130795/what-even-is-a-parameter/</link><description>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250825_parameter.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt; Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. &lt;/em&gt;&lt;em&gt;You can read more from the series here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;I am writing this because one of my editors woke up in the middle of the night and scribbled on a bedside notepad: “What is a parameter?” Unlike a lot of thoughts that hit at 4 a.m., it’s a really good question—one that goes right to the heart of how large language models work. And I’m not just saying that because he’s my boss. (Hi, Boss!)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A large language model’s parameters are often said to be the dials and levers that control how it behaves. Think of a planet-size pinball machine that sends its balls pinging from one end to the other via billions of paddles and bumpers set just so. Tweak those settings and the balls will behave in a different way.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI’s GPT-3, released in 2020, had 175 billion parameters. Google DeepMind’s latest LLM, Gemini 3, may have at least a trillion—some think it’s probably more like 7 trillion—but the company isn’t saying. (With competition now fierce, AI firms no longer share information about how their models are built.)&lt;/p&gt; 
 &lt;p&gt;But the basics of what parameters are and how they make LLMs do the remarkable things that they do are the same across different models. Ever wondered what makes an LLM really tick—what’s behind the colorful pinball-machine metaphors? Let’s dive in.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What is a parameter?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Think back to middle school algebra, like 2&lt;em&gt;a&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;. Those letters are parameters: Assign them values and you get a result. In math or coding, parameters are used to set limits or determine output. The parameters inside LLMs work in a similar way, just on a mind-boggling scale.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;How are they assigned their values?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Short answer: an algorithm. When a model is trained, each parameter is set to a random value. The training process then involves an iterative series of calculations (known as training steps) that update those values. In the early stages of training, a model will make errors. The training algorithm looks at each error and goes back through the model, tweaking the value of each of the model’s many parameters so that next time that error is smaller. This happens over and over again until the model behaves in the way its makers want it to. At that point, training stops and the values of the model’s parameters are fixed.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Sounds straightforward …&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In theory! In practice, because LLMs are trained on so much data and contain so many parameters, training them requires a huge number of steps and an eye-watering amount of computation. During training, the 175 billion parameters inside a medium-size LLM like GPT-3 will each get updated tens of thousands of times. In total, that adds up to quadrillions (a number with 15 zeros) of individual calculations. That’s why training an LLM takes so much energy. We’re talking about thousands of specialized high-speed computers running nonstop for months.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Oof. What are all these parameters for, exactly?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;There are three different types of parameters inside an LLM that get their values assigned through training: embeddings, weights, and biases. Let’s take each of those in turn.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Okay! So, what are embeddings?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;An embedding is the mathematical representation of a word (or part of a word, known as a token) in an LLM’s vocabulary. An LLM’s vocabulary, which might contain up to a few hundred thousand unique tokens, is set by its designers before training starts. But there’s no meaning attached to those words. That comes during training.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;When a model is trained, each word in its vocabulary is assigned a numerical value that captures the meaning of that word in relation to all the other words, based on how the word appears in countless examples across the model’s training data.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Each word gets replaced by a kind of code?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Yeah. But there’s a bit more to it. The numerical value—the embedding—that represents each word is in fact a &lt;em&gt;list&lt;/em&gt; of numbers, with each number in the list representing a different facet of meaning that the model has extracted from its training data. The length of this list of numbers is another thing that LLM designers can specify before an LLM is trained. A common size is 4,096.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Every word inside an LLM is represented by a list of 4,096 numbers?&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Yup, that’s an embedding. And each of those numbers is tweaked during training. An LLM with embeddings that are 4,096 numbers long is said to have 4,096 dimensions.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why 4,096?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;It might look like a strange number. But LLMs (like anything that runs on a computer chip) work best with powers of two—2, 4, 8, 16, 32, 64, and so on. LLM engineers have found that 4,096 is a power of two that hits a sweet spot between capability and efficiency. Models with fewer dimensions are less capable; models with more dimensions are too expensive or slow to train and run.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Using more numbers allows the LLM to capture very fine-grained information about how a word is used in many different contexts, what subtle connotations it might have, how it relates to other words, and so on.&lt;/p&gt;  &lt;p&gt;Back in February, OpenAI released GPT-4.5, the firm’s largest LLM yet (some estimates have put its parameter count at more than 10 trillion). Nick Ryder, a research scientist at OpenAI who worked on the model, told me at the time that bigger models can work with extra information, like emotional cues, such as when a speaker’s words signal hostility: “All of these subtle patterns that come through a human conversation—those are the bits that these larger and larger models will pick up on.”&lt;/p&gt;  &lt;p&gt;The upshot is that all the words inside an LLM get encoded into a high-dimensional space. Picture thousands of words floating in the air around you. Words that are closer together have similar meanings. For example, “table” and “chair” will be closer to each other than they are to “astronaut,” which is close to “moon” and “Musk.” Way off in the distance you can see “prestidigitation.” It’s a little like that, but instead of being related to each other across three dimensions, the words inside an LLM are related across 4,096 dimensions.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Yikes.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;It’s dizzying stuff. In effect, an LLM compresses the entire internet into a single monumental mathematical structure that encodes an unfathomable amount of interconnected information. It’s both why LLMs can do astonishing things and why they’re impossible to fully understand.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Okay. So that’s embeddings. What about weights?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;A weight is a parameter that represents the strength of a connection between different parts of a model—and one of the most common types of dial for tuning a model’s behavior. Weights are used when an LLM processes text.&lt;/p&gt;  &lt;p&gt;When an LLM reads a sentence (or a book chapter), it first looks up the embeddings for all the words and then passes those embeddings through a series of neural networks, known as transformers, that are designed to process sequences of data (like text) all at once. Every word in the sentence gets processed in relation to every other word.&lt;/p&gt;  &lt;p&gt;This is where weights come in. An embedding represents the meaning of a word without context. When a word appears in a specific sentence, transformers use weights to process the meaning of that word in that new context. (In practice, this involves multiplying each embedding by the weights for all other words.)&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;And biases?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Biases are another type of dial that complement the effects of the weights. Weights set the thresholds at which different parts of a model fire (and thus pass data on to the next part). Biases are used to adjust those thresholds so that an embedding can trigger activity even when its value is low. (Biases are values that are added to an embedding rather than multiplied with it.)&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;By shifting the thresholds at which parts of a model fire, biases allow the model to pick up information that might otherwise be missed. Imagine you’re trying to hear what somebody is saying in a noisy room. Weights would amplify the loudest voices the most; biases are like a knob on a listening device that pushes quieter voices up in the mix.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Here’s the TL;DR: Weights and biases are two different ways that an LLM extracts as much information as it can out of the text it is given. And both types of parameters are adjusted over and over again during training to make sure they do this.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Okay. What about neurons? Are they a type of parameter too?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;No, neurons are more a way to organize all this math—containers for the weights and biases, strung together by a web of pathways between them. It’s all very loosely inspired by biological neurons inside animal brains, with signals from one neuron triggering new signals from the next and so on.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Each neuron in a model holds a single bias and weights for every one of the model’s dimensions. In other words, if a model has 4,096 dimensions—and therefore its embeddings are lists of 4,096 numbers—then each of the neurons in that model will hold one bias and 4,096 weights.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Neurons are arranged in layers. In most LLMs, each neuron in one layer is connected to every neuron in the layer above. A 175-billion-parameter model like GPT-3 might have around 100 layers with a few tens of thousands of neurons in each layer. And each neuron is running tens of thousands of computations at a time.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Dizzy again. That’s a lot of math.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;That’s a lot of math.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;And how does all of that fit together? How does an LLM take a bunch of words and decide what words to give back?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;When an LLM processes a piece of text, the numerical representation of that text—the embedding—gets passed through multiple layers of the model. In each layer, the value of the embedding (that list of 4,096 numbers) gets updated many times by a series of computations involving the model’s weights and biases (attached to the neurons) until it gets to the final layer.&lt;/p&gt;  &lt;p&gt;The idea is that all the meaning and nuance and context of that input text is captured by the final value of the embedding after it has gone through a mind-boggling series of computations. That value is then used to calculate the next word that the LLM should spit out.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;It won’t be a surprise that this is more complicated than it sounds: The model in fact calculates, for every word in its vocabulary, how likely that word is to come next and ranks the results. It then picks the top word. (Kind of. See below …)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That word is appended to the previous block of text, and the whole process repeats until the LLM calculates that the most likely next word to spit out is one that signals the end of its output.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;That’s it?&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Sure. Well …&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Go on.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;LLM designers can also specify a handful of other parameters, known as hyperparameters. The main ones are called temperature, top-p, and top-k.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;You’re making this up.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Temperature is a parameter that acts as a kind of creativity dial. It influences the model’s choice of what word comes next. I just said that the model ranks the words in its vocabulary and picks the top one. But the temperature parameter can be used to push the model to choose the most probable next word, making its output more factual and relevant, or a less probable word, making the output more surprising and less robotic.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Top-p and top-k are two more dials that control the model’s choice of next words. They are settings that force the model to pick a word at random from a pool of most probable words instead of the top word. These parameters affect how the model comes across—quirky and creative versus trustworthy and dull.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;One last question! There has been a lot of buzz about small models that can outperform big models. How does a small model do more with fewer parameters?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;That’s one of the hottest questions in AI right now. There are a lot of different ways it can happen. Researchers have found that the amount of training data makes a huge difference. First you need to make sure the model sees enough data: An LLM trained on too little text won’t make the most of all its parameters, and a smaller model trained on the same amount of data could outperform it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another trick researchers have hit on is overtraining. Showing models far more data than previously thought necessary seems to make them perform better. The result is that a small model trained on a lot of data can outperform a larger model trained on less data. Take Meta’s Llama LLMs. The 70-billion-parameter Llama 2 was trained on around 2 trillion words of text; the 8-billion-parameter Llama 3 was trained on around 15 trillion words of text. The far smaller Llama 3 is the better model.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;A third technique, known as distillation, uses a larger model to train a smaller one. The smaller model is trained not only on the raw training data but also on the outputs of the larger model’s internal computations. The idea is that the hard-won lessons encoded in the parameters of the larger model trickle down into the parameters of the smaller model, giving it a boost.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In fact, the days of single monolithic models may be over. Even the largest models on the market, like OpenAI’s GPT-5 and Google DeepMind’s Gemini 3, can be thought of as several small models in a trench coat. Using a technique called “mixture of experts,” large models can turn on just the parts of themselves (the “experts”) that are required to process a specific piece of text. This combines the abilities of a large model with the speed and lower power consumption of a small one.&lt;/p&gt;  &lt;p&gt;But that’s not the end of it. Researchers are still figuring out ways to get the most out of a model’s parameters. As the gains from straight-up scaling tail off, jacking up the number of parameters no longer seems to make the difference it once did. It’s not so much how many you have, but what you do with them.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Can I see one?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;You want to &lt;em&gt;see&lt;/em&gt; a parameter? Knock yourself out: Here's an embedding. &lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><content:encoded>&lt;div&gt;&lt;img class="ff-og-image-inserted" src="https://wp.technologyreview.com/wp-content/uploads/2025/08/250825_parameter.jpg?resize=1200,600" /&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="class"&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_0 html_first"&gt; &lt;p&gt;MIT Technology Review&lt;em&gt; Explains: Let our writers untangle the complex, messy world of technology to help you understand what’s coming next. &lt;/em&gt;&lt;em&gt;You can read more from the series here&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;I am writing this because one of my editors woke up in the middle of the night and scribbled on a bedside notepad: “What is a parameter?” Unlike a lot of thoughts that hit at 4 a.m., it’s a really good question—one that goes right to the heart of how large language models work. And I’m not just saying that because he’s my boss. (Hi, Boss!)&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_2"&gt; &lt;p&gt;A large language model’s parameters are often said to be the dials and levers that control how it behaves. Think of a planet-size pinball machine that sends its balls pinging from one end to the other via billions of paddles and bumpers set just so. Tweak those settings and the balls will behave in a different way.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;p&gt;OpenAI’s GPT-3, released in 2020, had 175 billion parameters. Google DeepMind’s latest LLM, Gemini 3, may have at least a trillion—some think it’s probably more like 7 trillion—but the company isn’t saying. (With competition now fierce, AI firms no longer share information about how their models are built.)&lt;/p&gt; 
 &lt;p&gt;But the basics of what parameters are and how they make LLMs do the remarkable things that they do are the same across different models. Ever wondered what makes an LLM really tick—what’s behind the colorful pinball-machine metaphors? Let’s dive in.&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;What is a parameter?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Think back to middle school algebra, like 2&lt;em&gt;a&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;. Those letters are parameters: Assign them values and you get a result. In math or coding, parameters are used to set limits or determine output. The parameters inside LLMs work in a similar way, just on a mind-boggling scale.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;How are they assigned their values?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Short answer: an algorithm. When a model is trained, each parameter is set to a random value. The training process then involves an iterative series of calculations (known as training steps) that update those values. In the early stages of training, a model will make errors. The training algorithm looks at each error and goes back through the model, tweaking the value of each of the model’s many parameters so that next time that error is smaller. This happens over and over again until the model behaves in the way its makers want it to. At that point, training stops and the values of the model’s parameters are fixed.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Sounds straightforward …&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;In theory! In practice, because LLMs are trained on so much data and contain so many parameters, training them requires a huge number of steps and an eye-watering amount of computation. During training, the 175 billion parameters inside a medium-size LLM like GPT-3 will each get updated tens of thousands of times. In total, that adds up to quadrillions (a number with 15 zeros) of individual calculations. That’s why training an LLM takes so much energy. We’re talking about thousands of specialized high-speed computers running nonstop for months.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Oof. What are all these parameters for, exactly?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;There are three different types of parameters inside an LLM that get their values assigned through training: embeddings, weights, and biases. Let’s take each of those in turn.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Okay! So, what are embeddings?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;An embedding is the mathematical representation of a word (or part of a word, known as a token) in an LLM’s vocabulary. An LLM’s vocabulary, which might contain up to a few hundred thousand unique tokens, is set by its designers before training starts. But there’s no meaning attached to those words. That comes during training.&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_4"&gt; &lt;p&gt;When a model is trained, each word in its vocabulary is assigned a numerical value that captures the meaning of that word in relation to all the other words, based on how the word appears in countless examples across the model’s training data.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Each word gets replaced by a kind of code?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Yeah. But there’s a bit more to it. The numerical value—the embedding—that represents each word is in fact a &lt;em&gt;list&lt;/em&gt; of numbers, with each number in the list representing a different facet of meaning that the model has extracted from its training data. The length of this list of numbers is another thing that LLM designers can specify before an LLM is trained. A common size is 4,096.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Every word inside an LLM is represented by a list of 4,096 numbers?&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Yup, that’s an embedding. And each of those numbers is tweaked during training. An LLM with embeddings that are 4,096 numbers long is said to have 4,096 dimensions.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Why 4,096?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;It might look like a strange number. But LLMs (like anything that runs on a computer chip) work best with powers of two—2, 4, 8, 16, 32, 64, and so on. LLM engineers have found that 4,096 is a power of two that hits a sweet spot between capability and efficiency. Models with fewer dimensions are less capable; models with more dimensions are too expensive or slow to train and run.&amp;nbsp;&lt;/p&gt; 

 &lt;p&gt;Using more numbers allows the LLM to capture very fine-grained information about how a word is used in many different contexts, what subtle connotations it might have, how it relates to other words, and so on.&lt;/p&gt;  &lt;p&gt;Back in February, OpenAI released GPT-4.5, the firm’s largest LLM yet (some estimates have put its parameter count at more than 10 trillion). Nick Ryder, a research scientist at OpenAI who worked on the model, told me at the time that bigger models can work with extra information, like emotional cues, such as when a speaker’s words signal hostility: “All of these subtle patterns that come through a human conversation—those are the bits that these larger and larger models will pick up on.”&lt;/p&gt;  &lt;p&gt;The upshot is that all the words inside an LLM get encoded into a high-dimensional space. Picture thousands of words floating in the air around you. Words that are closer together have similar meanings. For example, “table” and “chair” will be closer to each other than they are to “astronaut,” which is close to “moon” and “Musk.” Way off in the distance you can see “prestidigitation.” It’s a little like that, but instead of being related to each other across three dimensions, the words inside an LLM are related across 4,096 dimensions.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Yikes.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;It’s dizzying stuff. In effect, an LLM compresses the entire internet into a single monumental mathematical structure that encodes an unfathomable amount of interconnected information. It’s both why LLMs can do astonishing things and why they’re impossible to fully understand.&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_6"&gt; &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Okay. So that’s embeddings. What about weights?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;A weight is a parameter that represents the strength of a connection between different parts of a model—and one of the most common types of dial for tuning a model’s behavior. Weights are used when an LLM processes text.&lt;/p&gt;  &lt;p&gt;When an LLM reads a sentence (or a book chapter), it first looks up the embeddings for all the words and then passes those embeddings through a series of neural networks, known as transformers, that are designed to process sequences of data (like text) all at once. Every word in the sentence gets processed in relation to every other word.&lt;/p&gt;  &lt;p&gt;This is where weights come in. An embedding represents the meaning of a word without context. When a word appears in a specific sentence, transformers use weights to process the meaning of that word in that new context. (In practice, this involves multiplying each embedding by the weights for all other words.)&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;And biases?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Biases are another type of dial that complement the effects of the weights. Weights set the thresholds at which different parts of a model fire (and thus pass data on to the next part). Biases are used to adjust those thresholds so that an embedding can trigger activity even when its value is low. (Biases are values that are added to an embedding rather than multiplied with it.)&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;By shifting the thresholds at which parts of a model fire, biases allow the model to pick up information that might otherwise be missed. Imagine you’re trying to hear what somebody is saying in a noisy room. Weights would amplify the loudest voices the most; biases are like a knob on a listening device that pushes quieter voices up in the mix.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Here’s the TL;DR: Weights and biases are two different ways that an LLM extracts as much information as it can out of the text it is given. And both types of parameters are adjusted over and over again during training to make sure they do this.&amp;nbsp;&lt;/p&gt; 
 &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Okay. What about neurons? Are they a type of parameter too?&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;No, neurons are more a way to organize all this math—containers for the weights and biases, strung together by a web of pathways between them. It’s all very loosely inspired by biological neurons inside animal brains, with signals from one neuron triggering new signals from the next and so on.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Each neuron in a model holds a single bias and weights for every one of the model’s dimensions. In other words, if a model has 4,096 dimensions—and therefore its embeddings are lists of 4,096 numbers—then each of the neurons in that model will hold one bias and 4,096 weights.&amp;nbsp;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_8"&gt; &lt;p&gt;Neurons are arranged in layers. In most LLMs, each neuron in one layer is connected to every neuron in the layer above. A 175-billion-parameter model like GPT-3 might have around 100 layers with a few tens of thousands of neurons in each layer. And each neuron is running tens of thousands of computations at a time.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Dizzy again. That’s a lot of math.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;That’s a lot of math.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;And how does all of that fit together? How does an LLM take a bunch of words and decide what words to give back?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;When an LLM processes a piece of text, the numerical representation of that text—the embedding—gets passed through multiple layers of the model. In each layer, the value of the embedding (that list of 4,096 numbers) gets updated many times by a series of computations involving the model’s weights and biases (attached to the neurons) until it gets to the final layer.&lt;/p&gt;  &lt;p&gt;The idea is that all the meaning and nuance and context of that input text is captured by the final value of the embedding after it has gone through a mind-boggling series of computations. That value is then used to calculate the next word that the LLM should spit out.&amp;nbsp;&lt;/p&gt; 
 &lt;p&gt;It won’t be a surprise that this is more complicated than it sounds: The model in fact calculates, for every word in its vocabulary, how likely that word is to come next and ranks the results. It then picks the top word. (Kind of. See below …)&amp;nbsp;&lt;/p&gt;  &lt;p&gt;That word is appended to the previous block of text, and the whole process repeats until the LLM calculates that the most likely next word to spit out is one that signals the end of its output.&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;That’s it?&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Sure. Well …&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Go on.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;LLM designers can also specify a handful of other parameters, known as hyperparameters. The main ones are called temperature, top-p, and top-k.&lt;/p&gt; 
&lt;/div&gt;&lt;/div&gt;&lt;div&gt;&lt;div class="gutenbergContent__content--109b03a769a11e8ae3acbab352a64269 html_10"&gt;&lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;You’re making this up.&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;Temperature is a parameter that acts as a kind of creativity dial. It influences the model’s choice of what word comes next. I just said that the model ranks the words in its vocabulary and picks the top one. But the temperature parameter can be used to push the model to choose the most probable next word, making its output more factual and relevant, or a less probable word, making the output more surprising and less robotic.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Top-p and top-k are two more dials that control the model’s choice of next words. They are settings that force the model to pick a word at random from a pool of most probable words instead of the top word. These parameters affect how the model comes across—quirky and creative versus trustworthy and dull.&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;One last question! There has been a lot of buzz about small models that can outperform big models. How does a small model do more with fewer parameters?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;That’s one of the hottest questions in AI right now. There are a lot of different ways it can happen. Researchers have found that the amount of training data makes a huge difference. First you need to make sure the model sees enough data: An LLM trained on too little text won’t make the most of all its parameters, and a smaller model trained on the same amount of data could outperform it.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;Another trick researchers have hit on is overtraining. Showing models far more data than previously thought necessary seems to make them perform better. The result is that a small model trained on a lot of data can outperform a larger model trained on less data. Take Meta’s Llama LLMs. The 70-billion-parameter Llama 2 was trained on around 2 trillion words of text; the 8-billion-parameter Llama 3 was trained on around 15 trillion words of text. The far smaller Llama 3 is the better model.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;A third technique, known as distillation, uses a larger model to train a smaller one. The smaller model is trained not only on the raw training data but also on the outputs of the larger model’s internal computations. The idea is that the hard-won lessons encoded in the parameters of the larger model trickle down into the parameters of the smaller model, giving it a boost.&amp;nbsp;&lt;/p&gt;  &lt;p&gt;In fact, the days of single monolithic models may be over. Even the largest models on the market, like OpenAI’s GPT-5 and Google DeepMind’s Gemini 3, can be thought of as several small models in a trench coat. Using a technique called “mixture of experts,” large models can turn on just the parts of themselves (the “experts”) that are required to process a specific piece of text. This combines the abilities of a large model with the speed and lower power consumption of a small one.&lt;/p&gt;  &lt;p&gt;But that’s not the end of it. Researchers are still figuring out ways to get the most out of a model’s parameters. As the gains from straight-up scaling tail off, jacking up the number of parameters no longer seems to make the difference it once did. It’s not so much how many you have, but what you do with them.&lt;/p&gt;  &lt;h3 class="wp-block-heading"&gt;&lt;strong&gt;Can I see one?&lt;/strong&gt;&lt;/h3&gt;  &lt;p&gt;You want to &lt;em&gt;see&lt;/em&gt; a parameter? Knock yourself out: Here's an embedding. &lt;/p&gt;    &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content:encoded><guid isPermaLink="false">https://www.technologyreview.com/2026/01/07/1130795/what-even-is-a-parameter/</guid><pubDate>Wed, 07 Jan 2026 11:23:47 +0000</pubDate></item><item><title>[NEW] Intel spinout Articul8 raises more than half of $70M round at $500M valuation (AI News &amp; Artificial Intelligence | TechCrunch)</title><link>https://techcrunch.com/2026/01/07/intel-spin-off-articul8-is-halfway-to-70m-ai-funding-round-at-500m-valuation/</link><description>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Articul8, an enterprise AI company spun out of Intel in early 2024, has secured more than half of a planned $70 million funding round at a $500 million pre-money valuation, according to its CEO, as it looks to capitalize on growing demand for AI systems in regulated industries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Series B funding round is structured in two installments, with the first led by Spain’s Adara Ventures, Articul8 founder and CEO Arun K. Subramaniyan (pictured above, center) said in an interview. He declined to disclose the size of the initial installment, but said the company expects to close the round in the first quarter of this year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Articul8’s valuation for its current funding round marks a roughly fivefold increase from the company’s $100 million post-money Series A valuation in January 2024. Since then, the Santa Clara-based company said it has surpassed $90 million in total contract value — the cumulative value of all signed customer contracts — from 29 paying customers, including Hitachi Energy, AWS, Franklin Templeton, and Intel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Subramaniyan told TechCrunch that Articul8 was not under pressure to raise capital, describing the company as revenue-positive following a series of large enterprise contracts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are not cash-strapped,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company expects to finish the year with annual recurring revenue of just over $57 million, Subramaniyan said, with roughly 45% to 50% of that already recognized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Articul8 develops specialized AI systems that operate within customers’ own IT environments, rather than relying on shared, general-purpose models. Instead of selling standalone models, the company packages its technology as software applications and AI agents tailored to specific business functions, targeting regulated industries such as energy, manufacturing, aerospace, financial services, and semiconductors, where accuracy, auditability, and data control are critical.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Articul8 knowledge graph" class="wp-image-3080549" height="1226" src="https://techcrunch.com/wp-content/uploads/2026/01/articul8-knowledge-graph.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Articul8’s knowledge graph view&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Articul8&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Our competition is pretty much everybody,” said Subramaniyan. “But today, the major competitors are the cloud service providers, because they have realized that their model, as the general-purpose [offerings], are all commodities.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that Articul8’s focus on specialized systems appeals to customers who need predictable results and clear audit trails, something that is harder to achieve with general-purpose models run on shared cloud platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Articul8 plans to use the Series B proceeds primarily to expand research and product development and to scale its operations internationally, with a focus on Europe and parts of Asia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adara Ventures’ participation will help speed-up the European expansion plan, as the European Investment Fund backs the Madrid-based VC firm’s energy fund, Subramaniyan said. The company is also looking to scale in markets including Japan and South Korea, where it has begun working with large enterprise customers, he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s Aditya Birla Ventures also participated in the ongoing round, Subramaniyan stated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Articul8 works with large tech groups including Nvidia and Google Cloud, Subramaniyan said, adding that Amazon Web Services is both a customer and a partner for the company on some deployments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company employs 75 people, with about 80% focused on R&amp;amp;D, and teams spread across the U.S., Brazil, and India.&lt;/p&gt;</description><content:encoded>&lt;p class="wp-block-paragraph" id="speakable-summary"&gt;Articul8, an enterprise AI company spun out of Intel in early 2024, has secured more than half of a planned $70 million funding round at a $500 million pre-money valuation, according to its CEO, as it looks to capitalize on growing demand for AI systems in regulated industries.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The Series B funding round is structured in two installments, with the first led by Spain’s Adara Ventures, Articul8 founder and CEO Arun K. Subramaniyan (pictured above, center) said in an interview. He declined to disclose the size of the initial installment, but said the company expects to close the round in the first quarter of this year.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Articul8’s valuation for its current funding round marks a roughly fivefold increase from the company’s $100 million post-money Series A valuation in January 2024. Since then, the Santa Clara-based company said it has surpassed $90 million in total contract value — the cumulative value of all signed customer contracts — from 29 paying customers, including Hitachi Energy, AWS, Franklin Templeton, and Intel.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Subramaniyan told TechCrunch that Articul8 was not under pressure to raise capital, describing the company as revenue-positive following a series of large enterprise contracts.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;“We are not cash-strapped,” he said.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company expects to finish the year with annual recurring revenue of just over $57 million, Subramaniyan said, with roughly 45% to 50% of that already recognized.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Articul8 develops specialized AI systems that operate within customers’ own IT environments, rather than relying on shared, general-purpose models. Instead of selling standalone models, the company packages its technology as software applications and AI agents tailored to specific business functions, targeting regulated industries such as energy, manufacturing, aerospace, financial services, and semiconductors, where accuracy, auditability, and data control are critical.&lt;/p&gt;
&lt;div class="wp-block-techcrunch-inline-cta"&gt;
	&lt;div class="inline-cta__wrapper"&gt;
		
		&lt;p&gt;Techcrunch event&lt;/p&gt;
		&lt;div class="inline-cta__content"&gt;
			
			&lt;p&gt;
									&lt;span class="inline-cta__location"&gt;San Francisco&lt;/span&gt;
													&lt;span class="inline-cta__separator"&gt;|&lt;/span&gt;
													&lt;span class="inline-cta__date"&gt;October 13-15, 2026&lt;/span&gt;
							&lt;/p&gt;
			
		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;figure class="wp-block-image aligncenter size-full"&gt;&lt;img alt="Articul8 knowledge graph" class="wp-image-3080549" height="1226" src="https://techcrunch.com/wp-content/uploads/2026/01/articul8-knowledge-graph.jpg" width="1920" /&gt;&lt;figcaption class="wp-element-caption"&gt;&lt;span class="wp-element-caption__text"&gt;Articul8’s knowledge graph view&lt;/span&gt;&lt;span class="wp-block-image__credits"&gt;&lt;strong&gt;Image Credits:&lt;/strong&gt;Articul8&lt;/span&gt;&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p class="wp-block-paragraph"&gt;“Our competition is pretty much everybody,” said Subramaniyan. “But today, the major competitors are the cloud service providers, because they have realized that their model, as the general-purpose [offerings], are all commodities.”&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;He added that Articul8’s focus on specialized systems appeals to customers who need predictable results and clear audit trails, something that is harder to achieve with general-purpose models run on shared cloud platforms.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Articul8 plans to use the Series B proceeds primarily to expand research and product development and to scale its operations internationally, with a focus on Europe and parts of Asia.&lt;/p&gt;







&lt;p class="wp-block-paragraph"&gt;Adara Ventures’ participation will help speed-up the European expansion plan, as the European Investment Fund backs the Madrid-based VC firm’s energy fund, Subramaniyan said. The company is also looking to scale in markets including Japan and South Korea, where it has begun working with large enterprise customers, he noted.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;India’s Aditya Birla Ventures also participated in the ongoing round, Subramaniyan stated.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;Articul8 works with large tech groups including Nvidia and Google Cloud, Subramaniyan said, adding that Amazon Web Services is both a customer and a partner for the company on some deployments.&lt;/p&gt;

&lt;p class="wp-block-paragraph"&gt;The company employs 75 people, with about 80% focused on R&amp;amp;D, and teams spread across the U.S., Brazil, and India.&lt;/p&gt;</content:encoded><guid isPermaLink="false">https://techcrunch.com/2026/01/07/intel-spin-off-articul8-is-halfway-to-70m-ai-funding-round-at-500m-valuation/</guid><pubDate>Wed, 07 Jan 2026 12:00:00 +0000</pubDate></item></channel></rss>